<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83382</article-id><article-id pub-id-type="doi">10.7554/eLife.83382</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Experience transforms crossmodal object representations in the anterior temporal lobes</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-290996"><name><surname>Li</surname><given-names>Aedan Yue</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0580-4676</contrib-id><email>aedanyue.li@utoronto.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-292752"><name><surname>Ladyka-Wojcik</surname><given-names>Natalia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1218-0080</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-292753"><name><surname>Qazilbash</surname><given-names>Heba</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-292754"><name><surname>Golestani</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-138026"><name><surname>Walther</surname><given-names>Dirk B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8585-9858</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96981"><name><surname>Martin</surname><given-names>Chris B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7014-4371</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-32294"><name><surname>Barense</surname><given-names>Morgan D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>Department of Psychology, University of Toronto</institution></institution-wrap><addr-line><named-content content-type="city">Toronto</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03yjb2x39</institution-id><institution>Department of Physics and Astronomy, University of Calgary</institution></institution-wrap><addr-line><named-content content-type="city">Calgary</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution>Rotman Research Institute, Baycrest Health Sciences</institution><addr-line><named-content content-type="city">North York</named-content></addr-line><country>Canada</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05g3dte14</institution-id><institution>Department of Psychology, Florida State University</institution></institution-wrap><addr-line><named-content content-type="city">Tallahassee</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>04</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e83382</elocation-id><history><date date-type="received" iso-8601-date="2022-09-09"><day>09</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-04-19"><day>19</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-09-01"><day>01</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.31.504599"/></event></pub-history><permissions><copyright-statement>© 2024, Li et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Li et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83382-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83382-figures-v3.pdf"/><abstract><p>Combining information from multiple senses is essential to object recognition, core to the ability to learn concepts, make new inferences, and generalize across distinct entities. Yet how the mind combines sensory input into coherent crossmodal representations – the <italic>crossmodal binding problem</italic> – remains poorly understood. Here, we applied multi-echo fMRI across a 4-day paradigm, in which participants learned three-dimensional crossmodal representations created from well-characterized unimodal visual shape and sound features. Our novel paradigm decoupled the learned crossmodal object representations from their baseline unimodal shapes and sounds, thus allowing us to track the emergence of crossmodal object representations as they were learned by healthy adults. Critically, we found that two anterior temporal lobe structures – temporal pole and perirhinal cortex – differentiated learned from non-learned crossmodal objects, even when controlling for the unimodal features that composed those objects. These results provide evidence for integrated crossmodal object representations in the anterior temporal lobes that were different from the representations for the unimodal features. Furthermore, we found that perirhinal cortex representations were by default biased toward visual shape, but this initial visual bias was attenuated by crossmodal learning. Thus, crossmodal learning transformed perirhinal representations such that they were no longer predominantly grounded in the visual modality, which may be a mechanism by which object concepts gain their abstraction.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>crossmodal binding problem</kwd><kwd>crossmodal object representations</kwd><kwd>concept learning</kwd><kwd>integrative coding</kwd><kwd>unimodal features</kwd><kwd>multi-echo fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Alexander Graham Bell Canada Graduate Scholarship-Doctoral</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Aedan Yue</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Discovery Grant (RGPIN-2020-05747)</award-id><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><award-id>Scholar Award</award-id><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001804</institution-id><institution>Canada Research Chairs</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan D</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100015668</institution-id><institution>Ontario Ministry of Research and Innovation</institution></institution-wrap></funding-source><award-id>Early Researcher Award</award-id><principal-award-recipient><name><surname>Barense</surname><given-names>Morgan D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Anterior temporal lobe structures differentiated learned from non-learned crossmodal objects, evidence of integrated crossmodal object representations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The world is a great blooming, buzzing confusion (<xref ref-type="bibr" rid="bib27">James, 1890</xref>) of the senses. Our ability to understand ‘what is out there’ depends on combining sensory features to form <italic>crossmodal object concepts</italic>. A child, for example, might form the concept ‘frog’ by learning that the visual appearance of a four-legged creature goes with the sound of its croaking. Consequently, this child has also learned that frogs do not produce barking sounds, as the child has created a unique object association for a frog from specific unimodal shape and sound features. Forming coherent crossmodal object representations is thus essential for human experience, allowing adaptive behavior under changing environments. Yet, how is it possible for the child to know that the sound of croaking is associated with the visual shape of a frog, even when she might be looking at a dog? How does the human mind form meaningful concepts from the vast amount of unimodal feature information that bombards the senses, allowing us to interpret our external world?</p><p>Known as the <italic>crossmodal binding problem</italic>, this unresolved question in the cognitive sciences concerns how the mind combines unimodal sensory features into coherent crossmodal object representations. Better characterization of how this computational challenge is solved will not only improve our understanding of the human mind but will also have important consequences for the design of future artificial neural networks. Current artificial machines do not yet reach human performance on tasks involving crossmodal integration (<xref ref-type="bibr" rid="bib20">Guo et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Fei et al., 2022</xref>) or generalization beyond previous experience, (<xref ref-type="bibr" rid="bib29">Keysers et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Hupkes et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Santoro et al., 2017</xref>) which are limitations thought to be in part driven by the inability of existing machines to resolve the binding problem (<xref ref-type="bibr" rid="bib19">Greff et al., 2020</xref>).</p><p>One theoretical view from the cognitive sciences suggests that crossmodal objects are built from component unimodal features represented across distributed sensory regions. (<xref ref-type="bibr" rid="bib4">Barsalou, 2008</xref>) Under this view, when a child thinks about ‘frog’, the visual cortex represents the appearance of the shape of the frog, whereas the auditory cortex represents the croaking sound. Alternatively, other theoretical views predict that multisensory objects are not only built from their component unimodal sensory features, but that there is also a crossmodal integrative code that is different from the sum of these parts (<xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Saksida and Bussey, 2010</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Kent et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Damasio, 1989</xref>). These latter views propose that anterior temporal lobe structures can act as a polymodal ‘hub’ that combines separate features into integrated wholes (<xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Suzuki and Naya, 2014</xref>; <xref ref-type="bibr" rid="bib43">Ralph et al., 2017</xref>).</p><p>Thus, a key theoretical challenge central to resolving the crossmodal binding problem is understanding how anterior temporal lobe structures form object representations. Are crossmodal objects entirely built from features distributed across sensory regions, or is there also integrative coding in the anterior temporal lobes? Furthermore, the existing literature has predominantly studied the neural representation of well-established object concepts from the visual domain alone (<xref ref-type="bibr" rid="bib4">Barsalou, 2008</xref>; <xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Saksida and Bussey, 2010</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Kent et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Damasio, 1989</xref>; <xref ref-type="bibr" rid="bib50">Suzuki and Naya, 2014</xref>; <xref ref-type="bibr" rid="bib43">Ralph et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Ferko et al., 2022</xref>; <xref ref-type="bibr" rid="bib5">Bausch et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Pagan et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Yee and Thompson-Schill, 2016</xref>; <xref ref-type="bibr" rid="bib1">Barense et al., 2012</xref>; <xref ref-type="bibr" rid="bib15">Erez et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Liang et al., 2020</xref>; <xref ref-type="bibr" rid="bib36">Martin et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Hebart et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Li et al., 2022</xref>), even though human experience is fundamentally crossmodal.</p><p>Here, we leveraged multi-echo fMRI (<xref ref-type="bibr" rid="bib30">Kundu et al., 2017</xref>) across a novel 4-day task in which participants learned to associate unimodal visual shape and sound features into 3D crossmodal object representations. First, we characterized shape (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>) and sound features in a separate validation experiment, ensuring that the unimodal features were well-matched in terms of their subjective similarity (<xref ref-type="fig" rid="fig1">Figure 1</xref>). On the learning task, participants independently explored the 3D-printed shapes and heard novel experimenter-constructed sounds. The participants then learned specific shape-sound associations (congruent objects), while other shape-sound associations were not learned (incongruent objects).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>3D-printed objects.</title><p>An independent validation experiment ensured that the similarity of the selected shapes and sounds were well-matched. (<bold>a</bold>) Three shapes were sampled from the <italic>Validated Circular Shape (VCS) Space</italic> (shown as black points on VCS space), (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>) a stimulus space whereby angular distance corresponds to subjective shape similarity. Three sounds were sampled from a set of five experimenter-created sounds. This independent validation experiment ensured that we could characterize the change in similarity structure following crossmodal learning, because we knew the baseline similarity structure that is, two triangular representational geometries visualized using multidimensional scaling (<xref ref-type="bibr" rid="bib48">Shepard, 1980</xref>; also see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Furthermore, this procedure ensured that the subjective similarity of the three features was equated within each modality. (<bold>b</bold>) The shapes were then 3D-printed with a hollow space and embedded with a button-activated speaker. (<bold>c</bold>) Participants could physically explore and palpate the 3D shape-sound objects. Critically, we manipulated whether the button-activated speaker was operational across learning days (see Methods/<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig1-v3.tif"/></fig><p>Critically, our 4-day learning task allowed us to isolate neural activity associated with integrative coding in anterior temporal lobe structures that emerges with experience and differs from the neural patterns recorded at baseline. The learned and non-learned crossmodal objects were constructed from the same set of three validated shape and sound features, ensuring that factors such as familiarity with the unimodal features, subjective similarity, and feature identity were tightly controlled (<xref ref-type="fig" rid="fig2">Figure 2</xref>). If the mind represented crossmodal objects entirely as the reactivation of unimodal shapes and sounds (i.e. objects are constructed from their parts) then there should be no difference between the learned and non-learned objects (because they were created from the same three shapes and sounds). By contrast, if the mind represented crossmodal objects as something over and above their component features (i.e. representations for crossmodal objects rely on integrative coding that is different from the sum of their parts) then there should be behavioral and neural differences between learned and non-learned crossmodal objects (because the only difference across the objects is the learned relationship between the parts). Furthermore, this design allowed us to determine the relationship between the object representation acquired <italic>after</italic> crossmodal learning and the unimodal feature representations acquired <italic>before</italic> crossmodal learning. That is, we could examine whether learning led to abstraction of the object representations such that it no longer resembled the unimodal feature representations.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Four-day crossmodal object learning task.</title><p>On Day 1 (behavior), participants heard sounds through a headset and explored 3D-printed shapes while the button-activated speakers were not operational. During a separate task (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), participants rated the similarity of the visual shapes and sound features. On Day 2 (neuroimaging), participants completed (<bold>i</bold>) 10 Unimodal Feature runs in which they performed a 1-back task involving the shape and sound features experienced separately and (ii) 5 Crossmodal Object runs in which they performed a 1-back task for the shapes and sounds experienced simultaneously. As participants at this point have not yet learned the congruent shape-sound pairings, the Day 2 neuroimaging session serves as a within-subject neural baseline for how the unimodal features were represented before crossmodal learning. On Day 3 (behavior), participants again explored the shape and sound features. Participants now learned to make crossmodal associations between the specific visual and sound features that composed the shape-sound object by pressing the button to play an embedded speaker, thus forming congruent object representations (i.e. crossmodal learning). Shape-sound associations were counterbalanced across participants, and we again collected similarity ratings between the shapes and sounds on a separate task. On Day 4 (neuroimaging), participants completed the same task as on Day 2. In summary, across 4 days, we characterized the neural and behavioral changes that occurred before and after shapes and sounds were paired together to form crossmodal object representations. As the baseline similarity structure of the shape and sound features were a priori defined (see <xref ref-type="fig" rid="fig1">Figure 1</xref>) and measured on the first day of learning (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), changes to the within-subject similarity structure provide insight into whether the crossmodal object representations (acquired after crossmodal learning) differed from component unimodal representations (acquired before crossmodal learning).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Pairwise similarity task and results.</title><p>In the initial stimulus validation experiment, participants provided pairwise ratings for five sounds and three shapes. The shapes were equated in their subjective similarity that had been selected from a well-characterized perceptually uniform stimulus space (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>) and the pairwise ratings followed the same procedure as described in <xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>. Based on this initial experiment, we then selected the three sounds from the that were most closely equated in their subjective similarity. (<bold>a</bold>) 3D-printed shapes were displayed as images, whereas sounds were displayed in a box that could be played when clicked by the participant. Ratings were averaged to produce a similarity matrix for each participant, and then averaged to produce a group-level similarity matrix. Shown as triangular representational geometries recovered from multidimensional scaling in the above, shapes (blue) and sounds (orange) were approximately equated in their subjective similarity. These features were then used in the 4-day crossmodal learning task. (<bold>b</bold>) Behavioral results from the 4-day crossmodal learning task paired with multi-echo fMRI described in the main text. Before crossmodal learning, there was no difference in similarity between shape and sound features associated with congruent objects compared to incongruent objects – indicating that similarity was controlled at the unimodal feature-level. After crossmodal learning, we observed a robust shift in the magnitude of similarity. The shape and sound features associated with congruent objects were now significantly more similar than the same shape and sound features associated with incongruent objects (p&lt;0.001), evidence that crossmodal learning changed how participants experienced the unimodal features (observed in 17/18 participants). (<bold>c</bold>) We replicated this learning-related shift in pattern similarity with a larger sample size (n=44; observed in 38/44 participants). *** denotes p&lt;0.001. Horizontal lines denote the comparison of congruent vs. incongruent conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig2-figsupp1-v3.tif"/></fig></fig-group><p>In brief, we found that crossmodal object concepts were represented as distributed sensory-specific unimodal features along the visual and auditory processing pathways, as well as integrative crossmodal combinations of those unimodal features in the anterior temporal lobes. Intriguingly, the perirhinal cortex – an anterior temporal lobe structure – was biased toward the visual modality before crossmodal learning at baseline, with greater activity toward shape over sound features. Pattern similarity analyses revealed that the shape representations in perirhinal cortex were initially unaffected by sound, providing evidence of a default visual shape bias. However, crossmodal learning transformed the object representation in perirhinal cortex such that it was no longer predominantly visual. These results are consistent with the idea that the object representation had become abstracted away from the component unimodal features with learning, such that perirhinal representations was no longer grounded in the visual modality.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Four-day crossmodal object learning task</title><sec id="s2-1-1"><title>Measuring within-subject changes after crossmodal learning</title><p>We designed a 4-day learning task where each participant learned a set of shape-sound associations that created crossmodal objects (<xref ref-type="fig" rid="fig2">Figure 2</xref>). There were two days involving only behavioral measures (<italic>Day 1</italic> and <italic>Day 3</italic>). Before crossmodal learning on Day 1, participants explored the 3D-printed shapes (<italic>Visual</italic>) and heard the sounds (<italic>Sound</italic>) separately. In blocks of trials interleaved with these exploration phases, participants rated the similarity of the shapes and sounds (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). During crossmodal learning on Day 3, participants explored specific shape-sound associations (<italic>Congruent</italic> objects) by pressing the button on each 3D-printed shape to play the associated sound, with pairings counterbalanced across observers. Again, the participants rated the similarity of the shapes and sounds. Notably, all participants could recognize their specific shape-sound associations at the end of Day 3, confirming that the congruent shape-sound objects were successfully learned (performance = 100% for all participants).</p><p>There were two neuroimaging days (<italic>Day 2</italic> and <italic>Day 4</italic>), during which we recorded brain responses to unimodal features presented separately and to unimodal features presented simultaneously using multi-echo fMRI (<xref ref-type="fig" rid="fig2">Figure 2</xref>). During Unimodal Feature runs, participants either viewed images of the 3D-printed shapes or heard sounds. During Crossmodal Object runs, participants experienced either the shape-sound associations learned on Day 3 (<italic>Congruent</italic>) or shape-sound associations that had not been learned on Day 3 (<italic>Incongruent</italic>). We were especially interested in neural differences between congruent and incongruent objects as evidence of crossmodal integration; experience with the unimodal features composing congruent and incongruent objects was equated and the only way to distinguish them was in terms of how the features were integrated.</p></sec></sec><sec id="s2-2"><title>Behavioral pattern similarity</title><sec id="s2-2-1"><title>Subjective similarity changes after crossmodal learning</title><p>To understand how crossmodal learning impacts behavior, we analyzed the within-subject change in subjective similarity of the unimodal features <italic>before</italic> (Day 1) and <italic>after</italic> (Day 3) participants learned their crossmodal pairings (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In other words, we determined whether the perceived similarity of the unimodal feature representations changed after participants had experienced those unimodal features combined into crossmodal objects.</p><p>We conducted a linear mixed model which included learning day (before vs. after crossmodal learning) and congruency (congruent vs. incongruent) as fixed effects. We observed a robust learning-related behavioral change in terms of how participants experienced the similarity of shape and sound features (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>): there was a main effect of learning day (before or after crossmodal learning: <italic>F</italic><sub>1,51</sub> = 24.45, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.32), a main effect of congruency (congruent or incongruent: <italic>F</italic><sub>1,51</sub> = 6.93, p=0.011, <italic>η</italic><sup>2</sup>=0.12), and an interaction between learning day and congruency (<italic>F</italic><sub>1,51</sub> = 15.33, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.23). Before crossmodal learning, there was no difference in similarity between congruent and incongruent shape-sound features (<italic>t</italic><sub>17</sub>=0.78, p=0.44), whereas after crossmodal learning, participants rated shapes and sounds associated with congruent objects to be more similar than shapes and sounds associated with incongruent objects (<italic>t</italic><sub>17</sub>=5.10, p&lt;0.001, <italic>Cohen’s d</italic>=1.28; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Notably, this learning-related change in similarity was observed in 17 out of 18 participants. We confirmed this experience-dependent change in similarity structure in a separate behavioral experiment with a larger sample size (observed in 38 out of 44 participants; learning day x congruency interaction: F<sub>1,129</sub> = 13.74, p&lt;0.001; <italic>η</italic><sup>2</sup>=0.096; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec></sec><sec id="s2-3"><title>Whole-brain univariate analysis</title><sec id="s2-3-1"><title>Unimodal shape and sound representations are distributed</title><p>In the first set of neuroimaging analyses<bold>,</bold> we examined whether distributed brain regions were involved in representing unimodal shapes and sounds. During unimodal runs (shapes and sounds presented separately), we observed robust bilateral modality-specific activity across the neocortex (<xref ref-type="fig" rid="fig3">Figure 3a–c</xref>). The ventral visual stream extending into the perirhinal cortex activated more strongly to unimodal visual compared to sound information, indicating that perirhinal cortex activity was by default biased toward visual information in the unimodal runs (i.e. toward complex visual shape configurations; <xref ref-type="fig" rid="fig3">Figure 3a</xref>). The auditory processing stream, from the primary auditory cortex extending into the temporal pole along the superior temporal sulcus, activated more strongly to unimodal sound compared to visual information (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). These results replicate the known representational divisions across the neocortex and show that regions processing unimodal shapes and sounds are distributed across visual and auditory processing pathways.<xref ref-type="bibr" rid="bib2">Barense et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Mishkin et al., 1983</xref>; <xref ref-type="bibr" rid="bib42">Poremba and Mishkin, 2007</xref>. Furthermore, the robust signal quality we observe in anterior temporal regions demonstrates the improved quality of the multi-echo ICA pipeline employed in the current study, as these anterior temporal regions are often susceptible to signal dropout with standard single echo designs due to magnetic susceptibility issues near the sinus air/tissue boundaries (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Univariate results.</title><p>(<bold>a–b</bold>) Univariate analyses superimposed on MNI-152 standard space. All contrasts were thresholded at voxel-wise p=0.001 and cluster-corrected at p=0.05 (random-effects, FSL FLAME; 6 mm spatial smoothing). Collapsing across learning days, robust modality-specific activity was observed across the neocortex. (<bold>c–d</bold>) Five ROIs were a priori selected based on existing theory:<xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref> temporal pole – TP, perirhinal cortex – PRC, lateral occipital complex – LOC, primary visual cortex – V1, and primary auditory cortex – A1. (<bold>c</bold>) Consistent with the whole-brain results, LOC was biased toward visual features whereas A1 and TP were biased toward sound features. Activation in PRC and LOC showed learning-related shifts, with the magnitude of visual bias decreasing after crossmodal learning. (<bold>d</bold>) TP was the only brain region to show an experience-dependent change in univariate activity to the learned shape-sound associations during crossmodal object runs. * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001. Asterisks above or below bars indicate a significant difference from zero. Horizontal lines within brain regions reflect an interaction between modality or congruency with learning day (e.g. reduction in visual bias after crossmodal learning in PRC). Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Signal quality comparison from a representative participant.</title><p>(<bold>a</bold>) The multi-echo sequence we used acquired three measurements after every radiofrequency pulse, compared to the standard single-echo EPI which acquires a single measurement (usually at a TE around 30ms). A multi-echo sequence with three echoes acquires three times as much data as the current standard single-echo approach, and accounts for differences in measured T2* across brain regions. For example, better signal is obtained at high TE values for the anterior temporal lobes, which would otherwise reveal substantial signal dropout due to susceptibility artifacts at TE = 30ms. (<bold>b</bold>) We optimally averaged the three echoes, using a method that weighs the combination of echoes based on the estimated <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> at each voxel for each echo, and then applied ICA decomposition to remove non-BOLD noise. We found that the multi-echo approach better recovered signal from the anterior temporal lobe structures compared to the standard single-echo approach (shown in the Echo 2 column).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig3-figsupp1-v3.tif"/></fig></fig-group></sec></sec><sec id="s2-4"><title>Region-of-interest univariate analysis</title><sec id="s2-4-1"><title>Anterior temporal lobes differentiate between congruent and incongruent conditions</title><p>We next examined univariate activity focusing on five a priori regions thought to be important for representing unimodal features and their integration (<xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>) temporal pole, perirhinal cortex, lateral occipital complex (LOC), primary visual cortex (V1), and primary auditory cortex (A1). For each ROI, we conducted a linear mixed model which included learning day (before vs. after crossmodal learning) and modality (visual vs. sound feature) as fixed factors. Collapsing across learning days, perirhinal cortex (<italic>t</italic><sub>67</sub>=5.53, p&lt;0.001, <italic>Cohen’s d</italic>=0.67) and LOC (<italic>t</italic><sub>63</sub>=16.02, p&lt;0.001, <italic>Cohen’s d</italic>=2.00) were biased toward visual information, whereas temporal pole (<italic>t</italic><sub>67</sub>=6.73, p&lt;0.001, <italic>Cohen’s d</italic>=0.82) and A1 (<italic>t</italic><sub>67</sub>=17.09, p&lt;0.001, <italic>Cohen’s d</italic>=2.07) were biased toward sound information (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Interestingly, we found a small overall bias toward sound in V1, consistent with past work (<xref ref-type="bibr" rid="bib53">Vetter et al., 2014</xref>; <italic>t</italic><sub>67</sub>=2.26, p=0.027, <italic>Cohen’s d</italic>=0.20). Next, we determined how neural responses in these regions changed following crossmodal learning. We observed an interaction between learning day and modality in perirhinal cortex (<italic>F</italic><sub>1,48</sub> = 5.24, p=0.027, <italic>η</italic><sup>2</sup>=0.098) and LOC (<italic>F</italic><sub>1,45</sub> = 25.89, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.37; <xref ref-type="fig" rid="fig3">Figure 3d</xref>). These regions activated more strongly to visual information at baseline before crossmodal learning compared to after crossmodal learning, indicative of a visual bias that was attenuated with experience.</p><p>As a central goal of our study was to identify brain regions that were influenced by the learned crossmodal associations, we next examined univariate differences between <italic>Congruent vs. Incongruent</italic> for crossmodal object runs as a function of whether the crossmodal association had been learned. We conducted a linear mixed model for each ROI which included learning day (before vs. after crossmodal learning) and congruency (congruent vs. incongruent objects) as fixed factors. We observed a significant interaction between learning day and congruency in the temporal pole (<italic>F</italic><sub>1,48</sub> = 7.63, p=0.0081, <italic>η</italic><sup>2</sup>=0.14). Critically, there was no difference in activity between congruent and incongruent objects at baseline before crossmodal learning (<italic>t</italic><sub>33</sub>=0.37, p=0.72), but there was more activation to incongruent compared to congruent objects after crossmodal learning (<italic>t</italic><sub>33</sub>=2.42, p=0.021, <italic>Cohen’s d</italic>=0.42). As the unimodal shape-sound <italic>features</italic> experienced by participants were the same before and after crossmodal learning (<xref ref-type="fig" rid="fig2">Figure 2</xref>), this finding reveals that the univariate signal in the temporal pole was differentiated between congruent and incongruent objects that had been constructed from the same unimodal features.</p><p>By contrast, we did not observe a univariate difference between the congruent and incongruent conditions in the perirhinal cortex, LOC, V1, or A1 (<italic>F</italic><sub>1,45-48</sub> between 0.088 and 2.34, <italic>p</italic> between 0.13 and 0.77). Similarly, the exploratory ROIs hippocampus (HPC: <italic>F</italic><sub>1,48</sub> = 0.32, p=0.58) and inferior parietal lobe (IPL: <italic>F</italic><sub>1,48</sub> = 0.094, p=0.76) did not distinguish between the congruent and incongruent conditions.</p></sec></sec><sec id="s2-5"><title>Neural pattern similarity</title><sec id="s2-5-1"><title>Congruent associations differ from incongruent associations in anterior temporal lobes</title><p>We next conducted a series of representational similarity analyses across Unimodal Feature and Crossmodal Object runs before and after crossmodal learning. Here, we investigated whether representations for unimodal features were changed after learning the crossmodal associations between those features (i.e. learning the crossmodal pairings that comprised the shape-sound objects). Such a finding could be taken as evidence that learning crossmodal <italic>object</italic> concepts transforms the original representation of the component unimodal <italic>features</italic>. More specifically, we compared the correlation between congruent and incongruent shape-sound features within Unimodal Feature runs before and after crossmodal learning (<xref ref-type="fig" rid="fig4">Figure 4a</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Pattern similarity analyses for unimodal feature runs.</title><p>(<bold>a</bold>) Contrast matrix comparing the effect of congruency on feature representations. The voxel-wise matrix averaged across unimodal runs were autocorrelated using the z-transformed Pearson’s correlation, creating a unimodal feature-level contrast matrix. We examined the average pattern similarity between unimodal features associated with congruent objects (green) compared to the same unimodal features associated with incongruent objects (yellow). (<bold>b</bold>) Pattern similarity analysis revealed an interaction between learning day and congruency in the temporal pole (TP). At baseline before crossmodal learning, there was no difference in neural similarity between unimodal features that paired to create congruent objects compared to the same unimodal features that paired to create incongruent objects. After crossmodal learning, however, there was <italic>less</italic> neural similarity between the unimodal features of pairs comprising congruent objects compared to the unimodal features of pairs comprising incongruent objects. Because congruent and incongruent objects were built from the same shapes and sounds, this result provides evidence that learning about crossmodal object associations influenced the representations of the component features in the temporal pole. There was no difference between the congruent and incongruent pairings in any other ROI (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). ** p&lt;0.01. Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Pattern similarity analyses between unimodal features associated with congruent objects and incongruent objects, before and after crossmodal learning (analysis visualized in <xref ref-type="fig" rid="fig4">Figure 4</xref> in the main text).</title><p>(<bold>a–c</bold>) Interestingly, the perirhinal cortex, LOC, and V1 – primarily visually biased regions (see main text) – reduced in pattern similarity after crossmodal learning. (<bold>d</bold>) By contrast, there was no change across learning days in A1. No region displayed a difference between congruent and incongruent feature pairings other than the temporal pole (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). * denotes p<italic>&lt;</italic>0.05, ** denotes p<italic>&lt;</italic>0.01, *** denotes p&lt;0.001. Horizontal lines denote the main effect of learning day. Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig4-figsupp1-v3.tif"/></fig></fig-group><p>We conducted a linear mixed model which included learning day (before vs. after crossmodal learning) and congruency (congruent vs. incongruent) as fixed effects for each ROI. Complementing the previous behavioral pattern similarity results (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), in the temporal pole we observed a main effect of learning day (before or after crossmodal learning: <italic>F</italic><sub>1,32</sub> = 4.63, p=0.039, <italic>η</italic><sup>2</sup>=0.13), a main effect of congruency (congruent or incongruent object: <italic>F</italic><sub>1,64</sub> = 7.60, p=0.0076, <italic>η</italic><sup>2</sup>=0.11), and an interaction between learning day and congruency (<italic>F</italic><sub>1,64</sub> = 6.09, p=0.016, <italic>η</italic><sup>2</sup>=0.087). At baseline before crossmodal learning, there was no difference in pattern similarity between congruent features compared to incongruent features in the temporal pole (<italic>t</italic><sub>33</sub>=0.22, p=0.82). After crossmodal learning, however, there was lower pattern similarity for shape and sound features associated with congruent compared to incongruent objects (<italic>t</italic><sub>33</sub>=3.47, p=0.0015, <italic>Cohen’s d</italic>=0.22; <xref ref-type="fig" rid="fig4">Figure 4</xref>). Thus, although in behavior we observed that learning the crossmodal associations led to greater pattern similarity between congruent compared to incongruent features (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), this <italic>greater behavioral similarity</italic> was related to <italic>reduced neural similarity</italic> following crossmodal learning in the temporal pole.</p><p>By contrast, the other four a priori determined ROIs (perirhinal cortex, LOC, V1, or A1) did not show an interaction between learning day and congruency (<italic>F</italic><sub>1,60-64</sub> between 0.039 and 1.30, <italic>p</italic> between 0.26 and 0.84; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Likewise, our two exploratory ROIs (hippocampus, inferior parietal lobe) did not show an interaction between learning day and congruency (<italic>F</italic><sub>1,64</sub> between 0.68 and 0.91, <italic>p</italic> between 0.34 and 0.41; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec></sec><sec id="s2-6"><title>The visually biased code in perirhinal cortex was attenuated with learning</title><p>The previous analyses found that the temporal pole differentiated between congruent and incongruent shape-sound pairs after participants learned the crossmodal pairings. Next, we characterized how the representations of these unimodal features changed after they had been paired with features from another stimulus modality to form the crossmodal objects. Our key question was whether learning crossmodal associations transformed the unimodal feature representations.</p><p>First, the voxel-wise activity for unimodal feature runs was correlated to the voxel-wise activity for crossmodal object runs at baseline before crossmodal learning (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Specifically, we quantified the similarity in the patterns for the visual <italic>shape features</italic> with the <italic>crossmodal objects</italic> that had that same shape, as well as between the <italic>sound features</italic> and the <italic>crossmodal objects</italic> that had that same sound. We then conducted a linear mixed model which included modality (visual vs. sound) as a fixed factor within each ROI. Consistent with the univariate results (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we observed greater pattern similarity when there was a match between sound features in the temporal pole (<italic>F</italic><sub>1,32</sub> = 15.80, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.33) and A1 (<italic>F</italic><sub>1,32</sub> = 145.73, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.82), and greater pattern similarity when there was a match in the visual shape features in the perirhinal cortex (<italic>F</italic><sub>1,32</sub> = 10.99, p=0.0023, <italic>η</italic><sup>2</sup>=0.26), LOC (<italic>F</italic><sub>1,30</sub> = 20.09, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.40), and V1 (<italic>F</italic><sub>1,32</sub> = 22.02, p&lt;0.001, <italic>η</italic><sup>2</sup>=0.41). Pattern similarity for each ROI was higher for one of the two modalities, indicative of a baseline modality-specific bias toward either visual or sound content.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Contrast matrices and pattern similarity analyses investigating the effect of crossmodal learning on modality-specific biases.</title><p>The voxel-wise matrix for unimodal feature runs on Day 2 were correlated to the voxel-wise matrix for crossmodal object runs on (<bold>a</bold>) Day 2 and (<bold>b</bold>) Day 4, creating a contrast matrix between visual and auditory unimodal features to crossmodal objects that contained those features. We compared the average pattern similarity (z-transformed Pearson correlation) between shape (blue) and sound (orange) features across learning days. (<bold>a</bold>) Robust modality-specific feature biases were observed in all examined regions before crossmodal learning. That is, pattern similarity for each brain region was higher for one of the two modalities, indicative of a modality-specific bias. For example, pattern similarity in perirhinal cortex (PRC) preferentially tracked the visual features of the crossmodal objects, evidence of a default visual shape bias <italic>before crossmodal learning</italic>. (<bold>b</bold>) Critically, we found that perirhinal representations were transformed with experience, such that the initial visual bias was attenuated <italic>after crossmodal learning</italic> (i.e. denoted by a significant interaction, shown by shaded green regions), evidence that representations were no longer predominantly grounded in the visual modality. * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001. Horizontal lines within brain regions indicate a significant main effect of modality. Vertical asterisks denote pattern similarity comparisons relative to 0. Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Analyses for the hippocampus (HPC) and inferior parietal lobe (IPL).</title><p>(<bold>a</bold>) In the visual vs. auditory univariate analysis, there was no visual or sound bias in HPC, but there was a bias toward sounds that increased numerically after crossmodal learning in the IPL. (<bold>b</bold>) Pattern similarity analyses between unimodal features associated with congruent objects and incongruent objects. Similar to <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, there was no main effect of congruency in either region. (<bold>c</bold>) When we looked at the pattern similarity between Unimodal Feature runs on Day 2 to Crossmodal Object runs on Day 2, we found that there was significant pattern similarity when there was a match between the unimodal feature and the crossmodal object (e.g. pattern similarity &gt;0). This pattern of results held when (<bold>d</bold>) correlating the Unimodal Feature runs on Day 2 to Crossmodal Object runs on Day 4, and (<bold>e</bold>) correlating the Unimodal Feature runs on Day 4 to Crossmodal Object runs on Day 4. Finally, (<bold>f</bold>) there was no significant pattern similarity between Crossmodal Object runs before learning correlated to Crossmodal Object after learning in HPC, but there was significant pattern similarity in IPL (p&lt;0.001). Taken together, these results suggest that both HPC and IPL are sensitive to visual and sound content, as the (<bold>c, d, e</bold>) unimodal feature-level representations were correlated to the crossmodal object representations irrespective of learning day. However, there was no difference between congruent and incongruent pairings in any analysis, suggesting that HPC and IPL did not represent crossmodal objects differently from the component unimodal features. For these reasons, HPC and IPL may represent the convergence of unimodal feature representations (i.e. because HPC and IPL were sensitive to both visual and sound features), but our results do not seem to support these regions in forming crossmodal integrative coding distinct from the unimodal features (i.e. because representations in HPC and IPL did not differentiate the congruent and incongruent conditions and did not change with experience). * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001. Asterisks above or below bars indicate a significant difference from zero. Horizontal lines within brain regions in (<bold>a</bold>) reflect an interaction between modality and learning day, whereas horizontal lines within brain regions in reflect main effects of (<bold>b</bold>) learning day, (<bold>c–e</bold>) modality, or (<bold>f</bold>) congruency. Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig5-figsupp1-v3.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>The voxel-wise matrix for Unimodal Feature runs on Day 4 were correlated to the voxel-wise matrix for Crossmodal Object runs on Day 4 (see <xref ref-type="fig" rid="fig5">Figure 5</xref> in the main text for an example).</title><p>We compared the average pattern similarity (z-transformed Pearson correlation) between shape (blue) and sound (orange) features specifically after crossmodal learning. Consistent with <xref ref-type="fig" rid="fig5">Figure 5b</xref>, perirhinal cortex was the only region without a modality-specific bias. Furthermore, perirhinal cortex was the only region where the representations of both the visual and sound features were not significantly correlated to the crossmodal objects. By contrast, every other region maintained a modality-specific bias for either the visual or sound features. These results suggest that perirhinal cortex representations were transformed with experience, such that the initial visual shape representations (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) were no longer grounded in a single modality after crossmodal learning. Furthermore, these results suggest that crossmodal learning formed an integrative code different from the unimodal features in perirhinal cortex, as the visual and sound features were not significantly correlated with the crossmodal objects. * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001. Horizontal lines within brain regions indicate a significant main effect of modality. Vertical asterisks denote pattern similarity comparisons relative to 0. Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig5-figsupp2-v3.tif"/></fig></fig-group><p>We then examined whether the original representations would change after participants learned how the features were paired together to make specific crossmodal objects, conducting the same analysis described above after crossmodal learning had taken place (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). With this analysis, we sought to measure the relationship between the representation for the learned crossmodal object and the original baseline representation for the unimodal features. More specifically, the voxel-wise activity for unimodal feature runs <italic>before</italic> crossmodal learning was correlated to the voxel-wise activity for crossmodal object runs <italic>after</italic> crossmodal learning (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). Another linear mixed model which included modality as a fixed factor within each ROI revealed that the perirhinal cortex was no longer biased toward visual shape after crossmodal learning (<italic>F</italic><sub>1,32</sub> = 0.12, p=0.73), whereas the temporal pole, LOC, V1, and A1 remained biased toward either visual shape or sound (<italic>F</italic><sub>1,30-32</sub> between 16.20 and 73.42, all p&lt;0.001, <italic>η</italic> <xref ref-type="bibr" rid="bib20">Guo et al., 2019</xref> between 0.35 and 0.70).</p><p>To investigate this effect in perirhinal cortex more specifically, we conducted a linear mixed model to directly compare the change in the visual bias of perirhinal representations from before crossmodal learning to after crossmodal learning (green regions in <xref ref-type="fig" rid="fig5">Figure 5a</xref> <italic>vs. 5b</italic>). Specifically, the linear mixed model included learning day (before vs. after crossmodal learning) and modality (visual feature match to crossmodal object vs. sound feature match to crossmodal object). Results revealed a significant interaction between learning day and modality in the perirhinal cortex (<italic>F</italic><sub>1,775</sub> = 5.56, p=0.019, <italic>η</italic><sup>2</sup>=0.071), meaning that the baseline visual shape bias observed in perirhinal cortex (green region of <xref ref-type="fig" rid="fig5">Figure 5a</xref>) was significantly attenuated with experience (green region of <xref ref-type="fig" rid="fig5">Figure 5b</xref>). After crossmodal learning, a given shape no longer invoked significant pattern similarity between objects that had the same shape but differed in terms of what they sounded like. Taken together, these results suggest that prior to learning the crossmodal objects, the perirhinal cortex had a default bias toward representing the visual shape information and was not representing sound information of the crossmodal objects. After crossmodal learning, however, the visual shape bias in perirhinal cortex was no longer present. That is, with crossmodal learning, the representations within perirhinal cortex started to look less like the visual features that comprised the crossmodal objects, providing evidence that the perirhinal representations were no longer predominantly grounded in the visual modality.</p><p>To examine whether these results differed by congruency (i.e. whether any modality-specific biases differed as a function of whether the object was congruent or incongruent), we conducted exploratory linear mixed models for each of the five a priori ROIs across learning days. More specifically, we correlated: (1) the voxel-wise activity for Unimodal Feature Runs <italic>before</italic> crossmodal learning to the voxel-wise activity for Crossmodal Object Runs <italic>before</italic> crossmodal learning (Day 2 vs. Day 2), (2) the voxel-wise activity for Unimodal Feature Runs <italic>before</italic> crossmodal learning to the voxel-wise activity for Crossmodal Object Runs <italic>after</italic> crossmodal learning (Day 2 vs Day 4), and (3) the voxel-wise activity for Unimodal Feature Runs <italic>after</italic> crossmodal learning to the voxel-wise activity for Crossmodal Object Runs <italic>after</italic> crossmodal learning (Day 4 vs Day 4). For each of the three analyses described, we then conducted separate linear mixed models which included modality (visual feature match to crossmodal object vs. sound feature match to crossmodal object) and congruency (congruent vs. incongruent).</p><p>There was no significant relationship between modality and congruency in any ROI between Day 2 and Day 2 (<italic>F</italic><sub>1,346-368</sub> between 0.00 and 1.06, <italic>p</italic> between 0.30 and 0.99), between Day 2 and Day 4 (<italic>F</italic><sub>1,346-368</sub> between 0.021 and 0.91, <italic>p</italic> between 0.34 and 0.89), or between Day 4 and Day 4 (<italic>F</italic><sub>1,346-368</sub> between 0.01 and 3.05, <italic>p</italic> between 0.082 and 0.93). However<italic>,</italic> exploratory analyses revealed that perirhinal cortex was the only region without a modality-specific bias and where the unimodal feature runs were not significantly correlated to the crossmodal object runs <italic>after crossmodal learning</italic> (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><p>Taken together, the overall pattern of results suggests that representations of the crossmodal objects in perirhinal cortex were heavily influenced by their consistent visual features <italic>before</italic> crossmodal learning. However, the crossmodal object representations were no longer influenced by the component visual features <italic>after</italic> crossmodal learning (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Additional exploratory analyses did not find evidence of experience-dependent changes in the hippocampus or inferior parietal lobes (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>Importantly, the change in pattern similarity in the perirhinal cortex across learning days (<xref ref-type="fig" rid="fig5">Figure 5</xref>) is unlikely to be driven by noise, poor alignment of patterns across sessions, or generally reduced responses. Other regions with numerically similar pattern similarity to perirhinal cortex did not change across learning days (e.g. visual features x crossmodal objects in A1 in <xref ref-type="fig" rid="fig5">Figure 5</xref>; the exploratory ROI hippocampus with numerically similar pattern similarity to perirhinal cortex also did not change in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><sec id="s2-6-1"><title>Representations in perirhinal cortex change with experience</title><p>So far, we have shown that the perirhinal cortex was by default biased toward visual shape features (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), and that this visual shape bias was attenuated with experience (<xref ref-type="fig" rid="fig5">Figure 5b</xref>; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). In the final analysis, we tracked how the <italic>individual crossmodal object representations</italic> themselves change after crossmodal learning.</p><p>We assessed the cross-day pattern similarity between Crossmodal Object Runs by correlating the congruent and incongruent runs across learning days (<xref ref-type="fig" rid="fig6">Figure 6</xref>). We then conducted a linear mixed model which included congruency (congruent vs. incongruent) as a fixed factor for each a priori ROI. Perirhinal cortex was the only region that differentiated between congruent and incongruent objects in this analysis (PRC: <italic>F</italic><sub>1,34</sub> = 4.67, p=0.038, <italic>η</italic><sup>2</sup>=0.12; TP, LOC, V1, A1: <italic>F</italic><sub>1,32-34</sub> between 0.67 and 2.83, <italic>p</italic> between 0.10 and 0.42). Pattern similarity in perirhinal cortex did not differ from 0 for congruent objects across learning days (<italic>t</italic><sub>35</sub>=0.39, p=0.70) but was significantly lower than 0 for incongruent objects (<italic>t</italic><sub>35</sub>=2.63, p=0.013, <italic>Cohen’s d</italic>=0.44). By contrast, pattern similarity in temporal pole, LOC, V1, and A1 was significantly correlated across learning days (pattern similarity &gt;0; t<sub>33-35</sub> between 4.31 and 6.92 all p&lt;0.001) and did not differ between congruent and incongruent objects (temporal pole, LOC, V1, and A1; <italic>F</italic><sub>1,32-34</sub> between 0.67 and 2.83, <italic>p</italic> between 0.10 and 0.42). Thus, perirhinal cortex was unique in that it not only differentiated between congruent and incongruent objects that were built from the same unimodal features (i.e. representations of the whole crossmodal object that was different than the unimodal features that composed it), but it also showed no significant pattern similarity above 0 for the same representations across learning days (i.e. suggesting that the object representations were transformed after crossmodal learning).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Contrast matrix shown on the left panel, with actual results shown on the right panel.</title><p>We compared the average pattern similarity across learning days between crossmodal object runs on Day 2 with crossmodal object runs on Day 4 (z-transformed Pearson correlation). We observed lower average pattern similarity for incongruent objects (yellow) compared to congruent (green) objects in perirhinal cortex (PRC). These results suggest that perirhinal cortex differentiated congruent and incongruent objects constructed from the same features. Furthermore, pattern similarity was never above 0 for the perirhinal cortex. By contrast, there was no significant difference between congruent and incongruent objects in any other examined region, and pattern similarity was always above 0. * denotes p&lt;0.05, ** denotes p&lt;0.01, *** denotes p&lt;0.001. Horizontal lines within brain regions denote a main effect of congruency. Vertical asterisks denote pattern similarity comparisons relative to 0. Error bars reflect the 95% confidence interval (n = 17).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83382-fig6-v3.tif"/></fig><p>No significant difference between the congruent and incongruent conditions were observed for the hippocampus (<italic>F</italic><sub>1,34</sub> = 0.34, p=0.56) or inferior parietal lobe (<italic>F</italic><sub>1,34</sub> = 0.00, p=0.96) in a follow-up exploratory analysis (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Known as the <italic>crossmodal binding problem</italic>, a long-standing question in the cognitive sciences has asked how the mind forms coherent concepts from multiple sensory modalities. To study this problem, we designed a 4-day task to decouple the learned crossmodal object representations (Day 3 and 4) from the baseline unimodal shape and sound features (Day 1 and 2). We equated the familiarity, subjective similarity, and identity of the unimodal feature representations composing the learned (congruent) and unlearned (incongruent) objects, ensuring that any differences between the two would not be driven by single features but rather by the integration of those features (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Paired with multi-echo fMRI to improve signal quality in the anterior temporal lobes (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), this novel paradigm tracked the emergence of crossmodal object concepts from component baseline unimodal features in healthy adults.</p><p>We found that the temporal pole and perirhinal cortex – two anterior temporal lobe structures – came to represent new crossmodal object concepts with learning, such that the acquired crossmodal object representations were different from the representation of the constituent unimodal features (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>). Intriguingly, the perirhinal cortex was by default biased toward visual shape, but that this initial visual bias was attenuated with experience (<xref ref-type="fig" rid="fig3">Figures 3c</xref> and <xref ref-type="fig" rid="fig5">5</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Within the perirhinal cortex, the acquired crossmodal object concepts (measured after crossmodal learning) became less similar to their original component unimodal features (measured at baseline before crossmodal learning); <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>. This is consistent with the idea that object representations in perirhinal cortex integrate the component sensory features into a whole that is different from the sum of the component parts, which might be a mechanism by which object concepts obtain their abstraction.</p><p>As one solution to the crossmodal binding problem, we suggest that the temporal pole and perirhinal cortex form unique crossmodal object representations that are different from the distributed features in sensory cortex (<xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). However, the nature by which the integrative code is structured and formed in the temporal pole and perirhinal cortex following crossmodal experience – such as through transformations, warping, or other factors – is an open question and an important area for future investigation. Furthermore, these distinct anterior temporal lobe structures may be involved with integrative coding in different ways. For example, the crossmodal object representations measured after learning were found to be related to the component unimodal feature representations measured before learning in the temporal pole but not the perirhinal cortex (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Moreover, pattern similarity for congruent shape-sound pairs were lower than the pattern similarity for incongruent shape-sound pairs after crossmodal learning in the temporal pole but not the perirhinal cortex (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). As one interpretation of this pattern of results, the temporal pole may represent new crossmodal objects by combining previously learned knowledge (<xref ref-type="bibr" rid="bib4">Barsalou, 2008</xref>; <xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Saksida and Bussey, 2010</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Damasio, 1989</xref>; <xref ref-type="bibr" rid="bib50">Suzuki and Naya, 2014</xref>; <xref ref-type="bibr" rid="bib43">Ralph et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Binder and Desai, 2011</xref>). Specifically, research into <italic>conceptual combination</italic> has linked the anterior temporal lobes to compound object concepts such as ‘hummingbird’ (<xref ref-type="bibr" rid="bib34">Lynott and Connell, 2010</xref>; <xref ref-type="bibr" rid="bib12">Coutanche et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Baron and Osherson, 2011</xref>). For example, participants during our task may have represented the sound-based ‘humming’ concept and visually based ‘bird’ concept on Day 1, forming the crossmodal ‘hummingbird’ concept on Day 3; <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>, which may recruit less activity in temporal pole than an incongruent pairing such as ‘barking-frog’. For these reasons, the temporal pole may form a crossmodal object code based on pre-existing knowledge, resulting in reduced neural activity (<xref ref-type="fig" rid="fig3">Figure 3d</xref>) and pattern similarity toward features associated with learned objects (<xref ref-type="fig" rid="fig4">Figure 4b</xref>).</p><p>By contrast, perirhinal cortex may be involved in pattern separation following crossmodal experience. In our task, participants had to differentiate congruent and incongruent objects constructed from the same three shape and sound features (<xref ref-type="fig" rid="fig2">Figure 2</xref>). An efficient way to solve this task would be to form distinct object-level outputs from the overlapping unimodal feature-level inputs such that congruent objects are made to be orthogonal from the representations before learning (i.e. measured as pattern similarity equal to 0 in the perirhinal cortex; <xref ref-type="fig" rid="fig5">Figures 5b</xref> and <xref ref-type="fig" rid="fig6">6</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), whereas non-learned incongruent objects could be made to be dissimilar from the representations before learning (i.e. anticorrelation, measured as patten similarity less than 0 in the perirhinal cortex; <xref ref-type="fig" rid="fig6">Figure 6</xref>). Because our paradigm could decouple neural responses to the learned object representations (on Day 4) from the original component unimodal features at baseline (on Day 2), these results could be taken as evidence of pattern separation in the human perirhinal cortex (<xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Kent et al., 2016</xref>). However, our pattern of results could also be explained by other types of crossmodal integrative coding. For example, incongruent object representations may be less stable than congruent object representations, such that incongruent objects representation are warped to a greater extent than congruent objects (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>Our results suggest that the temporal pole and perirhinal cortex are involved in representing crossmodal objects after a period of crossmodal learning. Although this observation is consistent with previous animal research (<xref ref-type="bibr" rid="bib26">Jacklin et al., 2016</xref>) finding that a period of experience is necessary for the perirhinal cortex to represent crossmodal objects, future work will need to determine whether our findings are driven by <italic>only</italic> experience or by experience <italic>combined with</italic> sleep-dependent consolidation (<xref ref-type="bibr" rid="bib47">Schapiro et al., 2017</xref>). Perhaps, a future study could explore how separate unimodal features and the integrative object representations change over the course of the same learning day compared to multiple learning days after sleep. Nevertheless, perirhinal cortex was critically influenced by experience, potentially explaining why findings in this literature have been at times mixed, as stimulus history was not always controlled across different experiments (<xref ref-type="bibr" rid="bib51">Taylor et al., 2006</xref>; <xref ref-type="bibr" rid="bib24">Holdstock et al., 2009</xref>). In our study, we explicitly controlled for stimulus history (<xref ref-type="fig" rid="fig2">Figure 2</xref>), ensuring that participants extensively explored individual features by the end of the first day and formed crossmodal objects by the end of the third day.</p><p>Complementing seminal patient work causally linking anterior temporal lobe damage to the loss of object concepts (<xref ref-type="bibr" rid="bib23">Hodges and Patterson, 1997</xref>), we show that the formation of new crossmodal concepts also recruits anterior temporal lobe structures like the temporal pole and perirhinal cortex. An important direction of future work will be to investigate the fine-grained functional divisions within the heterogeneous anterior temporal lobe region. One recent study has found that the anterior temporal lobe can be separated into 34 distinct functional regions (<xref ref-type="bibr" rid="bib41">Persichetti et al., 2021</xref>), suggesting that a simple temporal pole versus perirhinal cortex division may not fully capture the complexity of this region. Imaging the anterior temporal lobe has long been known to be challenging with functional neuroimaging due to signal dropout (<xref ref-type="bibr" rid="bib56">Visser et al., 2010</xref>). We show that a multi-echo fMRI sequence (<xref ref-type="bibr" rid="bib30">Kundu et al., 2017</xref>) may be especially useful in future work, as multi-echo fMRI mitigates signal dropout better than the standard single-echo fMRI (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for a visual comparison).</p><p>Importantly, the initial visual shape bias observed in the perirhinal cortex was attenuated by experience (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), suggesting that the perirhinal representations had become abstracted and were no longer predominantly grounded in a single modality after crossmodal learning. One possibility may be that the perirhinal cortex is by default visually driven as an extension to the ventral visual stream, (<xref ref-type="bibr" rid="bib45">Saksida and Bussey, 2010</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Kent et al., 2016</xref>) but can act as a polymodal ‘hub’ region for additional crossmodal input following learning. A complementary possibility may be that our visual features contained tactile information (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) that the perirhinal cortex may be sensitive to following the initial exploration phase on our task (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="bibr" rid="bib24">Holdstock et al., 2009</xref>) Critically, other brain regions like the LOC also reduced in visual bias (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), which may reflect visual imagery or feedback connectivity between the anterior temporal lobes. However, the perirhinal cortex was the only region where the visual bias was entirely attenuated following crossmodal learning (<xref ref-type="fig" rid="fig5">Figure 5b</xref>).</p><p>An interesting future line of investigation may be to explore whether there exist similar changes to the visual bias in artificial neural networks that aim to learn crossmodal object concepts (<xref ref-type="bibr" rid="bib20">Guo et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Fei et al., 2022</xref>; <xref ref-type="bibr" rid="bib19">Greff et al., 2020</xref>). Previous human neuroimaging has shown that the anterior temporal lobes are important for intra-object configural representations, (<xref ref-type="bibr" rid="bib59">Yeung et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Watson and Lee, 2013</xref>) such that damage to the perirhinal cortex (<xref ref-type="bibr" rid="bib1">Barense et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Bonnen et al., 2021</xref>) leads to object discrimination impairment. For example, human participants with perirhinal cortex damage are unable to resolve feature-level interference created by viewing multiple objects with overlapping features. Certain types of errors made by deep learning models (<xref ref-type="bibr" rid="bib21">Guo et al., 2022</xref>) also seem to resemble the kinds of errors made by human patients, (<xref ref-type="bibr" rid="bib1">Barense et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Taylor et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Hodges and Patterson, 1997</xref>; <xref ref-type="bibr" rid="bib7">Bonnen et al., 2021</xref>) whereby accurate object recognition can be disrupted by feature-level interference. Writing the word ‘iPod’ on an apple image, for instance, can lead to deep learning models falsely recognizing the apple as an actual iPod (<xref ref-type="bibr" rid="bib18">Goh et al., 2021</xref>). As certain limitations of existing neural networks may be driven by an inability to resolve the binding problem (<xref ref-type="bibr" rid="bib19">Greff et al., 2020</xref>), future work to mimic the coding properties of anterior temporal lobe structures may allow artificial machines to better mimic the remarkable human ability to learn concepts, make new inferences, and generalize across distinct entities.</p><p>Notably, our perirhinal cortex mask overlaps with a key region of the ventral anterior temporal lobe thought to be the central locus of crossmodal integration in the ‘hub and spokes’ model of semantic representations (<xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref><xref ref-type="bibr" rid="bib43">Ralph et al., 2017</xref>). However, additional work has also linked other brain regions to the convergence of unimodal representations, such as the hippocampus (<xref ref-type="bibr" rid="bib8">Butler and James, 2011</xref>; <xref ref-type="bibr" rid="bib9">Clouter et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Viganò and Piazza, 2020</xref>) and inferior parietal lobes (<xref ref-type="bibr" rid="bib55">Viganò et al., 2021</xref>; <xref ref-type="bibr" rid="bib6">Binder and Desai, 2011</xref>). This past work on the hippocampus and inferior parietal lobe does not necessarily address the crossmodal binding problem that was the main focus of our present study, as previous findings often do not differentiate between crossmodal integrative coding and the convergence of unimodal feature representations per se. Furthermore, previous studies in the literature typically do not control for stimulus-based factors such as experience with unimodal features, subjective similarity, or feature identity that may complicate the interpretation of results when determining regions important for crossmodal integration. Indeed, we found evidence consistent with the convergence of unimodal feature-based representations in both the hippocampus and inferior parietal lobes (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), but no evidence of crossmodal integrative coding different from the unimodal features. The hippocampus and inferior parietal lobes were both sensitive to visual and sound features before and after crossmodal learning (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Yet, the hippocampus and inferior parietal lobes did not differentiate between the congruent and incongruent conditions or change with experience (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>In summary, forming crossmodal object concepts relies on the representations for the whole crossmodal object in anterior temporal lobe structures different from the distributed unimodal feature representations in sensory regions. It is this hierarchical architecture that supports our ability to understand the external world, providing one solution to the age-old question of how crossmodal concepts can be constructed from their component features.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>The experiments described in this study were approved by the University of Toronto Ethics Review Board (protocols 37590 and 38856). Informed consent was obtained for all participants in the study prior to their participation.</p><sec id="s4-1"><title>Initial stimulus validation experiment</title><sec id="s4-1-1"><title>Participants</title><p>16 participants (Females = 11, <italic>M<sub>age</sub></italic> = 18.63 years) were recruited from the University of Toronto undergraduate participant pool and from the community. Course credit or $10 /hr CAD was provided as compensation.</p></sec><sec id="s4-1-2"><title>Stimuli</title><p>Three shape stimuli were sampled from the Validated Shape Space (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>) at equidistant positions, ensuring that the shapes were equated in their subjective similarity. The sound stimuli were manually generated in a similar procedure to how the shape stimuli from the Validated Shape Space (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>) were originally created. More specifically, distinct sounds were morphed together to create 5 complex, unrecognizable sounds that lasted for a duration of 2 s.</p></sec><sec id="s4-1-3"><title>Validation procedure</title><p>The stimulus validation procedure was based on previous work (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>; see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for an example of the task). Across nine trials, participants rated the similarity of each of the three shapes in the context of every other shape, as well as four control trials in which each shape was rated relative to itself. For this initial stimulus validation experiment, we used line drawings of the three shapes (for the 4-day crossmodal learning task we used images of the printed objects). Afterwards, participants completed 40 trials in which they rated the similarity of each of the 5 sounds in the context of every other sound, as well as 4 trials in which every sound was rated relative to itself. In a self-timed manner, participants viewed pictures of shapes or clicked icons to play the to-be-rated sounds from a headset.</p><p>For the shapes, we replicated the triangular geometry from participant similarity ratings obtained in our past work (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>) indicating that each shape was about as similar as every other shape (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We then selected the three sounds that were best equated in terms of their perceived similarity (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Thus, like the shapes, this procedure ensured that subjective similarity for the sounds was explicitly controlled but the underlying auditory dimensions could vary (e.g. timbre, pitch, frequency). This initial validation experiment ensured that the subjective similarity of the three features of each stimulus modality was equated within each modality prior to the primary 4-day learning task.</p></sec></sec><sec id="s4-2"><title>3D-printed shape-sound objects</title><p>The three validated shapes were 3D-printed using a DREMEL Digilab 3D Printer 3D45-01 with 1.75 mm gold-colored polymerized lactic acid filament. To create the 3D object models, the original 2D images were imported into Blender and elongated to add depth. The face of the shape image created a detachable lid, with a small circular opening to allow wiring to extend to a playable button positioned on the exterior of the shape. An empty space was formed inside the 3D shape for the battery-powered embedded speaker. To ensure that the objects were graspable, each shape was 3D-printed to be approximately the size of an adult hand (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). The lid of the shape was detached before each learning day (<xref ref-type="fig" rid="fig2">Figure 2</xref>), with the embedded speaker programmed to play either no sound (Day 1) or to play the paired sound that formed the congruent object (Day 3; <xref ref-type="fig" rid="fig1">Figure 1a</xref>). After the speaker was programmed, the lid of the shape was reattached using thermoplastic adhesive.</p><p>The sounds were played at an audible volume by the 3D-printed shapes during the learning task (see next section). During the scanning sessions, we individually tailored the volume until the participant could hear the sounds clearly when inside the MRI scanner.</p></sec><sec id="s4-3"><title>Four-day crossmodal object learning task</title><sec id="s4-3-1"><title>Participants</title><p>Twenty new participants (Females = 13, <italic>M<sub>age</sub></italic> = 23.15 years) were recruited and scanned at the Toronto Neuroimaging Facility. All participants were right-handed, with normal or corrected-to-normal vision, normal hearing, and no history of psychiatric illness. Of the 20 scanned participants, 1 participant dropped out after the first neuroimaging session. Severe distortion was observed in a second participant from a metal retainer and data from this participant was excluded from subsequent analyses. Due to technical difficulties, the functional localizer scans were not saved for one participant and most feature runs could not be completed for a second participant. Overall, the within-subject analyses described in the main text included data from a minimum of 16 participants, with most analyses containing data from 17 participants. Critically, this within-subject learning design increases power to detect an effect.</p><p>Compensation was $250 CAD for the two neuroimaging sessions and two behavioral sessions (~ 6 hr total, which included set-up, consent, and debriefing), with a $50 CAD completion bonus.</p></sec><sec id="s4-3-2"><title>Behavioral tasks</title><p>On each behavioral day (Day 1 and Day 3; <xref ref-type="fig" rid="fig2">Figure 2</xref>), participants completed the following tasks, in this order: Exploration Phase, one Unimodal Feature 1-back run (26 trials), Exploration Phase, one Crossmodal 1-back run (26 trials), Exploration Phase, Pairwise Similarity Task (24 trials), Exploration Phase, Pairwise Similarity Task (24 trials), Exploration Phase, Pairwise Similarity Task (24 trials), and finally, Exploration Phase. To verify learning on Day 3, participants also additionally completed a Learning Verification Task at the end of the session. Details on each task are provided below.</p><p>The overall procedure ensured that participants extensively explored the unimodal features on Day 1 and the crossmodal objects on Day 3. The Unimodal Feature and the Crossmodal Object 1-back runs administered on Day 1 and Day 3 served as practice for the neuroimaging sessions on Day 2 and Day 4, during which these 1-back tasks were completed. Each behavioral session required less than 1 hr of total time to complete.</p></sec><sec id="s4-3-3"><title>Day 1 exploration phase</title><p>On Day 1 (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), participants separately learned the shape and sound features in a random order. The 3D shapes were explored and physically palpated by the participants. We also encouraged participants to press the button on each shape, although the button was not operational on this day. Each 3D-printed shape was physically explored for 1 min and each sound was heard through a headset seven times. There were six exploration phases in total, interleaved between the 1-back and pairwise similarity tasks (order provided above). This procedure ensured that each individual stimulus was experienced extensively by the end of the first day.</p></sec><sec id="s4-3-4"><title>Day 3 exploration phase</title><p>On Day 3 (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), participants experienced the 3D-printed shape-sound objects in a random order. The sound was played over the embedded speakers by pressing the now-operational button on each object. Participants were allotted 1 min to physically explore and palpate each shape-sound object, as well as to listen to the associated sound by pressing the button. Like Day 1, there were six exploration phases in total, interleaved between the 1-back and pairwise similarity tasks.</p></sec><sec id="s4-3-5"><title>Pairwise similarity task</title><p>Using the same task as the stimulus validation procedure (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), participants provided similarity ratings for all combinations of the three validated shapes and three validated sounds (each of the six features were rated in the context of every other feature in the set, with four repeats of the same feature, for a total of 72 trials). More specifically, three stimuli were displayed on each trial, with one at the top and two at the bottom of the screen in the same procedure as we have used previously (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>). The 3D shapes were visually displayed as a photo, whereas sounds were displayed on screen in a box that could be played over headphones when clicked with the mouse. The participant made an initial judgment by selecting the more similar stimulus on the bottom relative to the stimulus on the top. Afterwards, the participant made a similarity rating between each bottom stimulus with the top stimulus from 0 being no similarity to five being identical. This procedure ensured that ratings were made relative to all other stimuli in the set.</p></sec><sec id="s4-3-6"><title>Unimodal feature and crossmodal object 1-back tasks</title><p>During fMRI scanning on Days 2 and 4, participants completed 1-back tasks in which the target was an exact sequential repeat of a feature (Unimodal Feature Task) or an exact sequential repeat of the shape-sound object (Crossmodal Object Task). In total, there were 10 Unimodal Feature runs and 5 Crossmodal Object runs for each scanning session. Two Unimodal Feature runs were followed by one Crossmodal Object run in an interleaved manner to participants until all 10 Unimodal Feature runs and 5 Crossmodal Object runs were completed. Each run lasted 3 min and had 26 trials.</p><p>Each Unimodal Feature and Crossmodal Object run began with a blank screen appearing for 6 s. For Unimodal Feature runs, either a shape or sound feature would then be presented for 2 s, followed by a fixation cross appearing for 2–8 s (sampled from the following probability distribution: 2 s=30%, 4 s=30%, 6 s=30%, and 8 s=10%). For Crossmodal Object runs, each shape appeared on the monitor at the same time as a sound was played through the headset for two seconds, followed by a fixation cross appearing for 2–8 s (sampled from the following probability distribution: 2 s=30%, 4 s=30%, 6 s=30%, and 8 s=10%). Ensuring equal trial numbers, three shape-sound pairings were congruent (learned by participants) and three shape-sound pairings were incongruent (not learned by participants). Congruent and incongruent pairings were built from different combinations of the same shape and sound features, with pairings counterbalanced across participants.</p><p>Overall, each stimulus was presented four times in a random order per run, with two repeats occurring at a random position for the corresponding 1-back task. The stimulus identity and temporal position of any given 1-back repeat was random.</p></sec><sec id="s4-3-7"><title>Learning verification task (Day 3 Only)</title><p>As the final task on Day 3, participants completed a task to ensure that participants successfully formed their crossmodal pairing. All three shapes and sounds were randomly displayed in six boxes on a display. Photos of the 3D shapes were shown, and sounds were played by clicking the box with the mouse cursor. The participant was cued with either a shape or sound, and then selected the corresponding paired feature. At the end of Day 3, we found that all participants reached 100% accuracy on this task (10 trials).</p></sec></sec><sec id="s4-4"><title>Behavioral pattern similarity analysis</title><p>The pairwise similarity ratings for each stimulus were averaged into a single feature-level RDM. We examined the magnitude of pattern similarity for congruent features compared to incongruent features across learning days (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><sec id="s4-4-1"><title>Neuroimaging procedures</title><p>Scanning was conducted using a 32-channel receiver head coil with the Siemens Magnetom Prisma 3T MRI scanner at the Toronto Neuroimaging Facility. To record responses, participants used a 4-button keypad (Current Designs, HHSC-1X4 CR). Stimulus materials were displayed using an MR compatible screen at high resolution (1920x1080) with zero-delay timing (32” BOLD screen) controlled by PsychToolbox-3 in MATLAB. At the start of each neuroimaging session, we performed a sound check with a set of modified in-ear MR-compatible headphones (Sensimetrics, model S14), followed by a functional localizer and then by the task-related runs.</p><p>While in the scanner, participants completed the following: After an initial functional localizer, we collected a resting state scan. After five 1-back runs, we acquired a whole-brain high-resolution T1-weighted structural image. After an additional five 1-back runs, we acquired a second resting-state scan, followed by the last five 1-back runs. The 15 total 1-back runs were interleaved such that 2 Unimodal Feature runs would be presented, followed by 1 Crossmodal Feature run until all 15 runs had been completed (see <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec></sec><sec id="s4-5"><title>Multi-echo fMRI</title><p>A 3D multi-echo echo-planer imaging (EPI) sequence with blipped-controlled aliasing in parallel imaging (CAIPI) sampling (<xref ref-type="bibr" rid="bib49">Stirnberg and Stöcker, 2021</xref>) was used to acquire fMRI data on Day 2 and Day 4. For task-related scans, the 3 echoes (TR = 2000ms, TE 1=11ms, TE 2=31.6ms, and TE 3=52.2ms) were each acquired with 90 images (210x210 field of view with a 100x100 matrix resize; anterior to posterior phase encoding, 78 slices, slice thickness: 2.10 mm, flip angle: 17°, interleaved multi-slice acquisition), resulting in an in-plane resolution of 2.10x2.10 mm. 3D distortion correction and pre-scan normalization was enabled, with acceleration factor PE = 2 and acceleration factor 3D=3. These parameters yielded coverage over the entire cortex, and a B0 field map was collected at the completion of the experiment.</p><sec id="s4-5-1"><title>1-back tasks (unimodal feature runs and crossmodal object runs)</title><p>Rather than collecting data from many different instances of a category as is common in a fMRI study using multivariate pattern analysis, we collected data from many repetitions of the <italic>same</italic> stimulus using a psychophysics-inspired approach. This paradigm ensured that the neural representations specific to each unimodal feature and each crossmodal object was well-powered for subsequent pattern similarity analyses (<xref ref-type="bibr" rid="bib11">Coutanche and Thompson-Schill, 2012</xref>). Excluding 1-back repeats, each unimodal feature was displayed four times per run for a total of 40 instances per scanning session (80 instances of each unimodal feature in total). Excluding 1-back repeats, each shape-sound pairing was displayed four times per run for a total of 20 instances per scanning session (40 instances of each shape-sound object in total). We designed our task-related runs to be 3 min in length, as ‘mini-runs’ have been shown to improve data quality in multivariate pattern analysis (<xref ref-type="bibr" rid="bib11">Coutanche and Thompson-Schill, 2012</xref>). Details of the task can be found in the section above.</p></sec><sec id="s4-5-2"><title>Standard functional localizer</title><p>Participants viewed intact visual features and phase scrambled versions of the same features in separate 24 s blocks (8 functional volumes; <xref ref-type="bibr" rid="bib35">Malach et al., 1995</xref>). Each of the 32 images within a block were presented for 400ms each with a 350ms ISI. There were two groups of four blocks, with each group separated by a 12 s fixation cross. Block order was counterbalanced across participants. All stimuli were presented in the context of an 1-back task, and the order of images within blocks was randomized with the 1-back repeat occurring once per block. The identity and temporal position of the 1-back repeat was random.</p></sec><sec id="s4-5-3"><title>Structural and resting state scans</title><p>A standard whole-brain high-resolution T1-weighted structural image was collected (TR = 2000ms, TE = 2.40ms, flip angle = 9°, field of view = 256 mm, 160 slices, slice thickness = 1.00 mm, acceleration factor PE = 2), resulting in an in-place resolution of 1.00 mm x 1.00.</p><p>Two 6 min 42 s resting state scans were also collected (TR = 2000ms, TE = 30ms; field of view: 220 mm, slice thickness: 2.00 mm; interleaved multi-slice acquisition, with acceleration factor PE = 2).</p></sec></sec><sec id="s4-6"><title>Neuroimaging analysis</title><sec id="s4-6-1"><title>ROI Definitions</title><p>We conducted region-of-interest univariate (<xref ref-type="fig" rid="fig3">Figure 3c and d</xref>) and multivariate pattern analysis (<xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref>) in five a priori masks: temporal pole, perirhinal cortex, lateral occipital complex (LOC), primary visual cortex (V1), and primary auditory cortex (A1). These regions were selected a priori given their hypothesized role in representing individual unimodal features as well as their integrated whole (<xref ref-type="bibr" rid="bib40">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Cowell et al., 2019</xref>). More specifically, we expected that the anterior temporal lobe structures – temporal pole and perirhinal cortex – would differentiate between the congruent and incongruent conditions. By contrast, we expected LOC, V1, and A1 to possess modality-specific biases for either the visual or sound features. Temporal pole, V1, and A1 masks were extracted from the Harvard-Oxford atlas. The perirhinal cortex mask was created from the average of 55 manually segmented T1 images from a previous publication (<xref ref-type="bibr" rid="bib44">Ritchey et al., 2015</xref> ). The LOC mask was extracted from the top 500 voxels in the lateral occipital region of each hemisphere that activated more strongly to intact than phase scrambled objects in the functional localizer (uncorrected voxel-wise p&lt;0.001; <xref ref-type="bibr" rid="bib35">Malach et al., 1995</xref>).</p><p>Additionally, we conducted region-of-interest univariate and multivariate pattern analysis in two <italic>exploratory</italic> masks: hippocampus and inferior parietal lobes (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). These regions were selected given their hypothesized role in the convergence of unimodal feature representations (<xref ref-type="bibr" rid="bib8">Butler and James, 2011</xref>; <xref ref-type="bibr" rid="bib9">Clouter et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Viganò and Piazza, 2020</xref>; <xref ref-type="bibr" rid="bib55">Viganò et al., 2021</xref>; <xref ref-type="bibr" rid="bib6">Binder and Desai, 2011</xref>).</p><p>Probabilistic masks were thresholded at.5 (i.e. voxels labelled in 50% of participants), with the masks transformed to subject space through the inverse warp matrix generated from FNIRT nonlinear registration (see <italic>Preprocessing</italic>) then resampled from 1mm<sup>3</sup> to 2.1 mm<sup>3</sup>. All subsequent analyses were conducted in subject space.</p></sec></sec><sec id="s4-7"><title>Multi-echo ICA-based denoising</title><p>For a detailed description of the overall ME-ICA pipeline, see the <italic>tedana</italic> Community (<xref ref-type="bibr" rid="bib52">tedana Community, 2021</xref>). The multi-echo ICA-based denoising approach was implemented using the function <italic>meica.py</italic> in AFNI. We optimally averaged the three echoes, which weights the combination of echoes based on the estimated <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> at each voxel for each echo. PCA then reduced the dimensionality of the optimally combined dataset and ICA decomposition was applied to remove non-BOLD noise. TE-dependent components reflecting BOLD-like signal for each run were used as the dataset for subsequent preprocessing in FSL (e.g. see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec><sec id="s4-8"><title>Preprocessing</title><p>First, the anatomical image was skull-stripped. Data were high-pass temporally filtered (50 s) and spatially smoothed (6 mm). Functional runs were registered to each participant’s high-resolution MPRAGE image using FLIRT boundary-based registration, with registration further refined using FNIRT nonlinear registration. The resulting data were analyzed using first-level FEAT Version 6.00 in each participant’s native anatomical space.</p></sec><sec id="s4-9"><title>Univariate analysis</title><p>To obtain participant-level contrasts, we averaged the run-level Unimodal Feature (<italic>Visual</italic> vs. <italic>Sound</italic>) and Crossmodal Object (<italic>Congruent</italic> vs. <italic>Incongruent</italic>) runs to produce the whole-brain group-level contrasts in FSL FLAME. Whole-brain analyses were thresholded at voxel-level p=0.001 with random field theory cluster correction at p=0.05.</p><p>For ROI-based analyses (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we estimated percent signal change using <italic>featquery</italic>. The parameter estimates (beta weight) were scaled by the peak height of the regressor, divided by the baseline intensity in the <italic>Visual</italic> vs. <italic>Sound</italic> and <italic>Congruent</italic> vs. <italic>Incongruent</italic> contrasts to obtain a difference score. Inferential statistical analyses were performed with these difference scores using a linear mixed model which included learning day (before vs. after crossmodal learning) and hemisphere (left or right) as fixed effects for each ROI, with participants modelled as random effects. All linear mixed model analyses were conducted using the <italic>nlme</italic> package in R version 3.6.1.</p></sec><sec id="s4-10"><title>Single-trial estimates</title><p>We used the least squares single approach (<xref ref-type="bibr" rid="bib38">Mumford et al., 2014</xref>) with 2 mm smoothing on the raw data in a separate set of analyses distinct from the univariate contrasts. Each individual stimulus, all other repetitions of the stimulus, and all other individual stimuli were modelled as covariates, allowing us to estimate whole-brain single-trial betas for each trial by run by mask by hemisphere by subject. All pattern similarity analyses described in the main text were conducted using the <italic>CoSMoMVPA</italic> package in MATLAB. After the single-trial betas were estimated, the voxel-wise activity across runs were averaged into a single overall matrix.</p></sec><sec id="s4-11"><title>Neuroimaging pattern similarity analysis</title><p>Four comparisons were conducted for each a priori ROI: (1) the autocorrelation of the average voxel-wise matrix during Unimodal Feature runs (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> ) the correlation between the RDM created from the Unimodal Feature runs before crossmodal learning to the RDM created from the Crossmodal Object runs before crossmodal learning (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), (3) the correlation between the RDM created from the Unimodal Feature runs before crossmodal learning to the RDM created from the Crossmodal Object runs after crossmodal learning (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), and (4) the correlation between the RDM created from the Crossmodal Object runs before crossmodal learning to the RDM created from the Crossmodal Object runs after crossmodal learning (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>The z-transformed Pearson’s correlation coefficient was used as the distance metric for all pattern similarity analyses. More specifically, each individual Pearson correlation was Fisher z-transformed and then averaged (see <xref ref-type="bibr" rid="bib10">Corey et al., 1998</xref>). Inferential statistical analyses were performed for each individual ROI using linear mixed models which could include congruency (congruent or incongruent), learning day (before or after crossmodal learning), modality (visual or sound), and hemisphere (left or right) as fixed factors, with participant modelled as random effects allowing intercepts to vary by learning day when appropriate. One-sample t-tests also compared the z-transformed pattern similarity scores relative to 0. All linear mixed model analyses were conducted using the <italic>nlme</italic> package in R version 3.6.1.</p></sec><sec id="s4-12"><title>Crossmodal object learning task: behavioral replication</title><sec id="s4-12-1"><title>Participants</title><p>Forty-four new participants (Females = 34, <italic>M<sub>age</sub></italic> = 23.95 years) were recruited from the University of Toronto undergraduate participant pool and from the community. Course credit or $10 /hr CAD was provided as compensation.</p></sec><sec id="s4-12-2"><title>Procedure</title><p>We conducted a same-day behavioral-only variant of the 4-day task described in the main text (<xref ref-type="fig" rid="fig2">Figure 2</xref>), excluding neuroimaging sessions. Participants first explored the 3D-printed shapes and heard the sounds separately (the button-activated speaker was not operational on this day). Each 3D-printed shape was physically explored for 1 min and each sound was heard through a headset seven times. On a separate pairwise similarity rating task, participants then provided similarity ratings for all combinations of the three shapes and three sounds (rated in the context of each other stimulus in the set, with four repeats of the same item; 72 total trials). Every 24 trials, participants again explored the same shapes and sounds (separately before crossmodal learning, in a counterbalanced order across participants).</p><p>Next, participants learned that certain shapes are associated with certain sounds, such that the 3D-printed shapes now played a sound when the button was pressed. Participants were allotted 1 min to physically explore and palpate each shape-sound object, as well as to listen to the associated sound by pressing the button. Participants repeated the pairwise similarity rating task, and every 24 trials, participants explored the 3D-printed shape-sound objects.</p><p>The behavioral similarity judgments before and after crossmodal learning were analyzed in the same pattern similarity approach described in the main text (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Validation, Investigation, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Software, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments described in this study were approved by the University of Toronto Ethics Review Board: 37590. Informed consent was obtained for all participants in the study.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83382-mdarchecklist1-v3.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Anonymized data are available on the Open Science Framework: <ext-link ext-link-type="uri" xlink:href="https://osf.io/vq4wj/">https://osf.io/vq4wj/</ext-link>. Univariate maps are available on NeuroVault: <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/LFDCGMAY/">https://neurovault.org/collections/LFDCGMAY/</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Li</surname><given-names>AY</given-names></name><name><surname>Ladyka-Wojcik</surname><given-names>N</given-names></name><name><surname>Qazilbash</surname><given-names>H</given-names></name><name><surname>Golestani</surname><given-names>A</given-names></name><name><surname>Bernhardt-Walther</surname><given-names>D</given-names></name><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Experience transforms crossmodal object representations in the anterior temporal lobes</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/VQ4WJ</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Li</surname><given-names>AY</given-names></name><name><surname>Ladyka-Wojcik</surname><given-names>N</given-names></name><name><surname>Qazilbash</surname><given-names>H</given-names></name><name><surname>Golestani</surname><given-names>A</given-names></name><name><surname>Bernhardt-Walther</surname><given-names>D</given-names></name><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Multimodal object representations rely on integrative coding</data-title><source>NeuroVault</source><pub-id pub-id-type="accession" xlink:href="https://identifiers.org/neurovault.collection:12807">12807</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to the Toronto Neuroimaging community for helpful feedback. In particular, the first author thanks Dr. Katherine Duncan and Dr. Massieh Moayedi for suggestions related to the experimental design, Dr. Michael Mack for initial guidance with 3D-printing, Dr. Rosanna Olsen for her tutorial on medial temporal lobe segmentation, as well as Dr. Andy Lee and Dr. Adrian Nestor for their neuroimaging and multivariate pattern analysis courses. We thank Annie Kim and Katarina Savel for their assistance with participant recruitment, as well as Priya Abraham for her assistance with MRI scanning. Finally, we thank Dr. Rüdiger Stirnberg for sharing with us the multi-echo fMRI sequence used in this manuscript. AYL is supported by an Alexander Graham Bell Canada Graduate Scholarship-Doctoral from the Natural Sciences and Engineering Research Council of Canada (NSERC CGS-D). This work is supported by a Scholar Award from the James S McDonnell Foundation, an Early Researcher Award from the Ontario Government, an NSERC Discovery grant, and a Canada Research Chair to MDB.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname><given-names>MD</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name><name><surname>Yeung</surname><given-names>LK</given-names></name><name><surname>Brady</surname><given-names>SM</given-names></name><name><surname>Gregori</surname><given-names>M</given-names></name><name><surname>Kapur</surname><given-names>N</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Henson</surname><given-names>RNA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Intact memory for irrelevant information impairs perception in amnesia</article-title><source>Neuron</source><volume>75</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.05.014</pub-id><pub-id pub-id-type="pmid">22794269</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barense</surname><given-names>MD</given-names></name><name><surname>Warren</surname><given-names>JD</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>The temporal lobes</chapter-title><person-group person-group-type="editor"><name><surname>Husain</surname><given-names>M</given-names></name><name><surname>Jonathan</surname><given-names>M</given-names></name></person-group><source>Oxford Textbook of Cognitive Neurology and Dementia</source><publisher-name>Schott, Oxford University Press</publisher-name><fpage>39</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1093/med/9780199655946.001.0001</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron</surname><given-names>SG</given-names></name><name><surname>Osherson</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evidence for conceptual combination in the left anterior temporal lobe</article-title><source>NeuroImage</source><volume>55</volume><fpage>1847</fpage><lpage>1852</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.066</pub-id><pub-id pub-id-type="pmid">21281723</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Grounded cognition</article-title><source>Annual Review of Psychology</source><volume>59</volume><fpage>617</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.59.103006.093639</pub-id><pub-id pub-id-type="pmid">17705682</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bausch</surname><given-names>M</given-names></name><name><surname>Niediek</surname><given-names>J</given-names></name><name><surname>Reber</surname><given-names>TP</given-names></name><name><surname>Mackay</surname><given-names>S</given-names></name><name><surname>Boström</surname><given-names>J</given-names></name><name><surname>Elger</surname><given-names>CE</given-names></name><name><surname>Mormann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Concept neurons in the human medial temporal lobe flexibly represent abstract relations between concepts</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6164</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26327-3</pub-id><pub-id pub-id-type="pmid">34697305</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neurobiology of semantic memory</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.10.001</pub-id><pub-id pub-id-type="pmid">22001867</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>T</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>When the ventral visual stream is not enough: a deep learning account of medial temporal lobe involvement in perception</article-title><source>Neuron</source><volume>109</volume><fpage>2755</fpage><lpage>2766</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.018</pub-id><pub-id pub-id-type="pmid">34265252</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>AJ</given-names></name><name><surname>James</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cross-modal versus within-modal recall: differences in behavioral and brain responses</article-title><source>Behavioural Brain Research</source><volume>224</volume><fpage>387</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2011.06.017</pub-id><pub-id pub-id-type="pmid">21723328</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clouter</surname><given-names>A</given-names></name><name><surname>Shapiro</surname><given-names>KL</given-names></name><name><surname>Hanslmayr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Theta phase synchronization is the glue that binds human associative memory</article-title><source>Current Biology</source><volume>27</volume><fpage>3143</fpage><lpage>3148</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.09.001</pub-id><pub-id pub-id-type="pmid">28988860</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corey</surname><given-names>DM</given-names></name><name><surname>Dunlap</surname><given-names>WP</given-names></name><name><surname>Burke</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Averaging correlations: expected values and bias in combined pearson rs and fisher’s transformations</article-title><source>Journal of General Psychology</source><volume>125</volume><fpage>245</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1080/00221309809595548</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The advantage of brief fMRI acquisition runs for multi-voxel pattern detection across runs</article-title><source>NeuroImage</source><volume>61</volume><fpage>1113</fpage><lpage>1119</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.03.076</pub-id><pub-id pub-id-type="pmid">22498658</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Solomon</surname><given-names>SH</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Conceptual combination</chapter-title><person-group person-group-type="editor"><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Gazzaniga</surname><given-names>MS</given-names></name></person-group><source>The Cognitive Neurosciences</source><edition>6th edition</edition><publisher-name>MIT Press</publisher-name><fpage>1</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.7551/mitpress/11442.001.0001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowell</surname><given-names>RA</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name><name><surname>Sadil</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A roadmap for understanding memory: decomposing cognitive processes into operations and representations</article-title><source>eNeuro</source><volume>6</volume><elocation-id>ENEURO.0122-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0122-19.2019</pub-id><pub-id pub-id-type="pmid">31189554</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damasio</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Time-locked multiregional retroactivation: a systems-level proposal for the neural substrates of recall and recognition</article-title><source>Cognition</source><volume>33</volume><fpage>25</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(89)90005-x</pub-id><pub-id pub-id-type="pmid">2691184</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname><given-names>J</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Kendall</surname><given-names>W</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Conjunctive coding of complex object features</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2271</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv081</pub-id><pub-id pub-id-type="pmid">25921583</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fei</surname><given-names>N</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Huo</surname><given-names>Y</given-names></name><name><surname>Wen</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Song</surname><given-names>R</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Xiang</surname><given-names>T</given-names></name><name><surname>Sun</surname><given-names>H</given-names></name><name><surname>Wen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Towards artificial general intelligence via a multimodal foundation model</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>3094</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-30761-2</pub-id><pub-id pub-id-type="pmid">35655064</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferko</surname><given-names>KM</given-names></name><name><surname>Blumenthal</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Minos</surname><given-names>AN</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Khan</surname><given-names>AR</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Activity in perirhinal and entorhinal cortex predicts perceived visual similarities among category exemplars with highest precision</article-title><source>eLife</source><volume>11</volume><elocation-id>e66884</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66884</pub-id><pub-id pub-id-type="pmid">35311645</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>G</given-names></name><name><surname>Cammarata</surname><given-names>N</given-names></name><name><surname>Voss</surname><given-names>C</given-names></name><name><surname>Carter</surname><given-names>S</given-names></name><name><surname>Petrov</surname><given-names>M</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multimodal neurons in artificial neural networks</article-title><source>Distill</source><volume>6</volume><elocation-id>0030</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00030</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Greff</surname><given-names>K</given-names></name><name><surname>Steenkiste</surname><given-names>SV</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the binding problem in artificial neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2012.05208">https://arxiv.org/abs/2012.05208</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep multimodal representation learning: a survey</article-title><source>IEEE Access</source><volume>7</volume><fpage>63373</fpage><lpage>63394</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2916887</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>MJ</given-names></name><name><surname>Leclerc</surname><given-names>G</given-names></name><name><surname>Dapello</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>Y</given-names></name><name><surname>Madry</surname><given-names>A</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Adversarially trained neural representations may already be as robust as corresponding biological neural representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2206.11228">https://arxiv.org/abs/2206.11228</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</article-title><source>Nature Human Behaviour</source><volume>4</volume><fpage>1173</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00951-3</pub-id><pub-id pub-id-type="pmid">33046861</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodges</surname><given-names>JR</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Semantic memory disorders</article-title><source>Trends in Cognitive Sciences</source><volume>1</volume><fpage>68</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(97)01022-X</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holdstock</surname><given-names>JS</given-names></name><name><surname>Hocking</surname><given-names>J</given-names></name><name><surname>Notley</surname><given-names>P</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Integrating visual and tactile information in the perirhinal cortex</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2993</fpage><lpage>3000</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp073</pub-id><pub-id pub-id-type="pmid">19386635</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hupkes</surname><given-names>D</given-names></name><name><surname>Dankers</surname><given-names>V</given-names></name><name><surname>Mul</surname><given-names>M</given-names></name><name><surname>Bruni</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Compositionality decomposed: how do neural networks generalise?</article-title><source>Journal of Artificial Intelligence Research</source><volume>67</volume><fpage>757</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1613/jair.1.11674</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacklin</surname><given-names>DL</given-names></name><name><surname>Cloke</surname><given-names>JM</given-names></name><name><surname>Potvin</surname><given-names>A</given-names></name><name><surname>Garrett</surname><given-names>I</given-names></name><name><surname>Winters</surname><given-names>BD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The dynamic multisensory engram: neural circuitry underlying crossmodal object recognition in rats changes with the nature of object experience</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1273</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3043-15.2016</pub-id><pub-id pub-id-type="pmid">26818515</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>James</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1890">1890</year><source>The Principles of Psychology</source><publisher-name>Henry Holt and Co</publisher-name><pub-id pub-id-type="doi">10.1037/10538-000</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kent</surname><given-names>BA</given-names></name><name><surname>Hvoslef-Eide</surname><given-names>M</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The representational-hierarchical view of pattern separation: not just hippocampus, not just space, not just memory?</article-title><source>Neurobiology of Learning and Memory</source><volume>129</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2016.01.006</pub-id><pub-id pub-id-type="pmid">26836403</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Keysers</surname><given-names>D</given-names></name><name><surname>Schärli</surname><given-names>N</given-names></name><name><surname>Scales</surname><given-names>N</given-names></name><name><surname>Buisman</surname><given-names>H</given-names></name><name><surname>Furrer</surname><given-names>D</given-names></name><name><surname>Kashubin</surname><given-names>S</given-names></name><name><surname>Momchev</surname><given-names>N</given-names></name><name><surname>Sinopalnikov</surname><given-names>D</given-names></name><name><surname>Stafiniak</surname><given-names>L</given-names></name><name><surname>Tihon</surname><given-names>T</given-names></name><name><surname>Tsarkov</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Zee</surname><given-names>MV</given-names></name><name><surname>Bousquet</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Measuring Compositional Generalization: A Comprehensive Method on Realistic Data</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.09713">https://arxiv.org/abs/1912.09713</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kundu</surname><given-names>P</given-names></name><name><surname>Voon</surname><given-names>V</given-names></name><name><surname>Balchandani</surname><given-names>P</given-names></name><name><surname>Lombardo</surname><given-names>MV</given-names></name><name><surname>Poser</surname><given-names>BA</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multi-echo fMRI: a review of applications in fMRI denoising and analysis of BOLD signals</article-title><source>NeuroImage</source><volume>154</volume><fpage>59</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.033</pub-id><pub-id pub-id-type="pmid">28363836</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>AY</given-names></name><name><surname>Liang</surname><given-names>JC</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The validated circular shape space: quantifying the visual similarity of shape</article-title><source>Journal of Experimental Psychology. General</source><volume>149</volume><fpage>949</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1037/xge0000693</pub-id><pub-id pub-id-type="pmid">31580102</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>AY</given-names></name><name><surname>Fukuda</surname><given-names>K</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Independent features form integrated objects: Using a novel shape-color “conjunction task” to reconstruct memory resolution for multiple object features simultaneously</article-title><source>Cognition</source><volume>223</volume><elocation-id>105024</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105024</pub-id><pub-id pub-id-type="pmid">35091259</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>JC</given-names></name><name><surname>Erez</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Experience transforms conjunctive object representations: neural evidence for unitization after visual expertise</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>2721</fpage><lpage>2739</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz250</pub-id><pub-id pub-id-type="pmid">32118259</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynott</surname><given-names>D</given-names></name><name><surname>Connell</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Embodied conceptual combination</article-title><source>Frontiers in Psychology</source><volume>1</volume><elocation-id>212</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2010.00212</pub-id><pub-id pub-id-type="pmid">21833267</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Benson</surname><given-names>RR</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Kennedy</surname><given-names>WA</given-names></name><name><surname>Ledden</surname><given-names>PJ</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>PNAS</source><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.18.8135</pub-id><pub-id pub-id-type="pmid">7667258</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Douglas</surname><given-names>D</given-names></name><name><surname>Newsome</surname><given-names>RN</given-names></name><name><surname>Man</surname><given-names>LL</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title><source>eLife</source><volume>7</volume><elocation-id>e31873</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31873</pub-id><pub-id pub-id-type="pmid">29393853</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishkin</surname><given-names>M</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Macko</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Object vision and spatial vision: two cortical pathways</article-title><source>Trends in Neurosciences</source><volume>6</volume><fpage>414</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(83)90190-X</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname><given-names>JA</given-names></name><name><surname>Davis</surname><given-names>T</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The impact of study design on pattern estimation for single-trial multivariate pattern analysis</article-title><source>NeuroImage</source><volume>103</volume><fpage>130</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.09.026</pub-id><pub-id pub-id-type="pmid">25241907</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Urban</surname><given-names>LS</given-names></name><name><surname>Wohl</surname><given-names>MP</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1132</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1038/nn.3433</pub-id><pub-id pub-id-type="pmid">23792943</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Nestor</surname><given-names>PJ</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Where do you know what you know? the representation of semantic knowledge in the human brain</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>976</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1038/nrn2277</pub-id><pub-id pub-id-type="pmid">18026167</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Persichetti</surname><given-names>AS</given-names></name><name><surname>Denning</surname><given-names>JM</given-names></name><name><surname>Gotts</surname><given-names>SJ</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A data-driven functional mapping of the anterior temporal lobes</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>6038</fpage><lpage>6049</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0456-21.2021</pub-id><pub-id pub-id-type="pmid">34083253</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poremba</surname><given-names>A</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Exploring the extent and function of higher-order auditory cortex in rhesus monkeys</article-title><source>Hearing Research</source><volume>229</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.01.003</pub-id><pub-id pub-id-type="pmid">17321703</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural and computational bases of semantic cognition</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id><pub-id pub-id-type="pmid">27881854</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Montchal</surname><given-names>ME</given-names></name><name><surname>Yonelinas</surname><given-names>AP</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delay-dependent contributions of medial temporal lobe regions to episodic memory retrieval</article-title><source>eLife</source><volume>4</volume><elocation-id>e05025</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05025</pub-id><pub-id pub-id-type="pmid">25584461</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The representational-hierarchical view of amnesia: translation from animal to human</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>2370</fpage><lpage>2384</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2010.02.026</pub-id><pub-id pub-id-type="pmid">20206190</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Barrett</surname><given-names>DG</given-names></name><name><surname>Malinowski</surname><given-names>M</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Battaglia</surname><given-names>PW</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Simple Neural Network Module for Relational Reasoning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1706.01427</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>McDevitt</surname><given-names>EA</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Mednick</surname><given-names>SC</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sleep benefits memory for semantic category structure while preserving exemplar-specific information</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>14869</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-12884-5</pub-id><pub-id pub-id-type="pmid">29093451</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Multidimensional scaling, tree-fitting, and clustering</article-title><source>Science</source><volume>210</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1126/science.210.4468.390</pub-id><pub-id pub-id-type="pmid">17837406</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stirnberg</surname><given-names>R</given-names></name><name><surname>Stöcker</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Segmented K-space blipped-controlled aliasing in parallel imaging for high spatiotemporal resolution EPI</article-title><source>Magnetic Resonance in Medicine</source><volume>85</volume><fpage>1540</fpage><lpage>1551</lpage><pub-id pub-id-type="doi">10.1002/mrm.28486</pub-id><pub-id pub-id-type="pmid">32936488</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>WA</given-names></name><name><surname>Naya</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The perirhinal cortex</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>39</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014207</pub-id><pub-id pub-id-type="pmid">25032492</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>KI</given-names></name><name><surname>Moss</surname><given-names>HE</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Binding crossmodal object features in perirhinal cortex</article-title><source>PNAS</source><volume>103</volume><fpage>8239</fpage><lpage>8244</lpage><pub-id pub-id-type="doi">10.1073/pnas.0509704103</pub-id><pub-id pub-id-type="pmid">16702554</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><collab>tedana Community</collab></person-group><year iso-8601-date="2021">2021</year><data-title>ME-ICA/Tedana</data-title><version designator="0.0.11">0.0.11</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5541689">https://doi.org/10.5281/zenodo.5541689</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>P</given-names></name><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding sound and imagery content in early visual cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>1256</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.04.020</pub-id><pub-id pub-id-type="pmid">24856208</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname><given-names>S</given-names></name><name><surname>Piazza</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distance and direction codes underlie navigation of a novel semantic space in the human brain</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>2727</fpage><lpage>2736</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1849-19.2020</pub-id><pub-id pub-id-type="pmid">32060171</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname><given-names>S</given-names></name><name><surname>Rubino</surname><given-names>V</given-names></name><name><surname>Buiatti</surname><given-names>M</given-names></name><name><surname>Piazza</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The neural representation of absolute direction during mental navigation in conceptual spaces</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>1294</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02806-7</pub-id><pub-id pub-id-type="pmid">34785757</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visser</surname><given-names>M</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Semantic processing in the anterior temporal lobes: a meta-analysis of the functional neuroimaging literature</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>1083</fpage><lpage>1094</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21309</pub-id><pub-id pub-id-type="pmid">19583477</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>HC</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The perirhinal cortex and recognition memory interference</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>4192</fpage><lpage>4200</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2075-12.2013</pub-id><pub-id pub-id-type="pmid">23447626</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yee</surname><given-names>E</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Putting concepts into context</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1015</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0948-7</pub-id><pub-id pub-id-type="pmid">27282993</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>LK</given-names></name><name><surname>Olsen</surname><given-names>RK</given-names></name><name><surname>Bild-Enkin</surname><given-names>HEP</given-names></name><name><surname>D’Angelo</surname><given-names>MC</given-names></name><name><surname>Kacollja</surname><given-names>A</given-names></name><name><surname>McQuiggan</surname><given-names>DA</given-names></name><name><surname>Keshabyan</surname><given-names>A</given-names></name><name><surname>Ryan</surname><given-names>JD</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Anterolateral entorhinal cortex volume predicted by altered intra-item configural processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>5527</fpage><lpage>5538</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3664-16.2017</pub-id><pub-id pub-id-type="pmid">28473640</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83382.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.31.504599" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.31.504599"/></front-stub><body><p>The fMRI study is important because it investigates fundamental questions about the neural basis of multimodal binding using an innovative multi-day learning approach. The results provide solid evidence for learning-related changes in the anterior temporal lobe. This paper is of interest to a broad audience of cognitive neuroscientists.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83382.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rogers</surname><given-names>Tim</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y2jtd41</institution-id><institution>University of Wisconsin-Madison</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.31.504599">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.31.504599v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Multimodal Object Representations Rely on Integrative Coding&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Chris Baker as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Sarah Solomon (Reviewer #2), Tim Rogers (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>As you will read below, the reviewers were enthusiastic about the research question and the design of the study. However, they were not convinced that the current set of results provide strong evidence for the main conclusions of an &quot;explicit integrative representation&quot;. They have suggested alternative interpretations and additional analyses that I would encourage you to consider.</p><p>Essential revisions (for the authors):</p><p>1) Clarify what is meant by an &quot;explicit integrative&quot; representation and improve the motivation for the individual analyses as tests of such a representation. For example, consider including, for each analysis, specific predictions that follow from the main hypothesis (and not alternative hypotheses) of an explicit integrative representation. A schematic illustration may also be helpful.</p><p>2) Discuss alternative explanations for the results; the reviewers were not convinced that all analyses provide evidence for an explicit integrative representation.</p><p>3) Run additional analyses that may provide more direct evidence for an integrative code.</p><p>4) Correct for multiple comparisons, where appropriate, and test the specificity of results to ROIs (e.g., in an ANOVA that includes ROI).</p><p>5) Consider including additional ROIs (e.g., ventral ATL/anterior fusiform, hippocampus, parietal cortex).</p><p>6) Include additional detail about the procedures and analyses.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The Introduction could discuss additional literature that is relevant to the current study. For example, previous studies investigated learning associations between modalities, and discussed several ways in which this could lead to neural changes; for example, unisensory stimulation could activate representations of the other modality after learning (Shams and Seitz, TiCS 2008). Previous fMRI work on conceptual/multimodal representations in ATL is not reviewed in much detail. Studies that seem particularly relevant are Vigano and Piazza (2020) and Vigano et al. (2021).</p><p>– The focus is specifically on the anterior temporal lobes. However, there are other regions that have been proposed (and shown) to similarly represent multimodal object representations, including the parietal cortex (Binder et al., TiCS 2011). Indeed, Binder et al. write: &quot;the most anterior parts of the temporal lobe, including the TP and anteromedial temporal regions, are unlikely to be a critical hub for retrieval of multimodal semantic knowledge.&quot; Discussing these alternative views would be relevant for a balanced article.</p><p>– Throughout the manuscript, the comparison is made with knowledge of a frog. While this is useful in the Introduction as general background, to set up the study, I found it somewhat confusing in the Results section. In the current study, the sounds and shapes are arbitrary and are learned for a short period. The resulting neural representation may well be different from the semantic representations of animal shapes and sounds, which have developed over many years.</p><p>– The results are interpreted as evidence for an integrative code already in the Results section. For example, on p.9, the incongruent&gt;congruent activation in TP is interpreted as &quot;the formation of an explicit integrative code in the temporal pole&quot;. However, the evidence for such a multimodal object code is very indirect here, and there are many alternative explanations for this (and other) findings (e.g., the novelty signal mentioned on p.11). I think it would be better to stay closer to the data when first presenting these individual results. Alternatively, to remain open to alternative interpretations, results can be described as being &quot;consistent with&quot; the integrative code interpretation.</p><p>– Relatedly, it was often not clear to me why specific results provide evidence for an integrative code. It might help if you could motivate the analyses in the context of searching for an integrative code, i.e., make explicit predictions based on the hypothesis of an integrative code and then present the results. In this way, the reader can follow the logic of the predictions before being presented with possibly complex results.</p><p>– The strongest effect of multimodal learning on univariate activity (Figure 3d) is the reduced visual bias in LOC. This result is not investigated further and does not feature in the interpretation and conclusions. However, this effect seems important to understand, also in relation to a similar (smaller) change in PRC, which is interpreted extensively. Do both these effects reflect the same underlying change in processing the stimuli after learning? Could it reflect visual imagery after multimodal learning? E.g., participants may imagine the sound-associated object after having learned the association, thus making the response evoked by the sound more visual.</p><p>– The analysis on page 12 (Figure 4) shows the reduced similarity of associated shapes and sounds in TP after learning. This is an interesting finding but I find it hard to interpret, also in light of the behavioral results showing that the shapes and sounds are perceived as more similar after learning – where is this similarity neurally represented? Could this reflect suppression of the associated sound/shape? Furthermore, I did not understand how this finding provides evidence for an explicit integrative code, with &quot;the whole being different from the parts&quot;, considering that this analysis only involved responses to individual modalities.</p><p>– The reporting of the analysis on page 13 (Figure 5) differs from how the other analyses are reported, starting by showing an interaction with ROI to motivate only testing one ROI. However, the interaction is between modality and ROI after learning but I suppose this interaction is equally strong before learning. This analysis does not test whether the visual bias in PRC (or other regions) was impacted by learning, yet this is what is concluded: &quot;the PRC was the only region that differed in its modality-specific bias across learning days&quot;. A subsequent analysis then tests this more directly, but only for PRC.</p><p>– I didn't understand why having a visual or sound bias (in Figure 5) is strong evidence for &quot;an explicit integrative object representation transformed from the original features&quot; (p.14). The specific change in PRC is hard to interpret, considering that the similarity is generally very low and after learning the PRC no longer resembled either of the components. This could reflect increased noise, for example, because patterns are now compared across sessions (see also next point).</p><p>– The analysis in Figure 6 shows pattern similarity across days. PRC shows a significant difference before vs after learning but the overall similarity is very low (and not above 0), and the effect is driven by below-zero similarity in the incongruent condition. I'm not sure how a below-zero similarity can be interpreted. The main effect of the region is not very informative here; it does not show that the learning-related difference was specific to PRC.</p><p>– A more direct test of &quot;the whole is different from the sum of the parts&quot; would be to model the combined response using the individual feature responses (e.g., see Baldassano et al., Cereb Cortex 2017). You would expect that a linear combination of sound and shape activity patterns (e.g., the average) is less similar to congruent objects than incongruent objects after learning. Including this test would be important to address the key hypothesis.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– The only explanation of the theory of &quot;explicit integrative representations&quot; is that the &quot;whole is different than the sum of its parts.&quot; This is not enough for the reader to fully understand the manuscript's theoretical claims. An explanation of what is meant by &quot;explicit&quot; and what is meant by &quot;integrative&quot; seems necessary since researchers in different fields and subfields might interpret those terms in meaningfully different ways. If the claim is that &quot;explicit integrative representations&quot; are abstracted away from featural information, the currently presented analyses are not convincing. If the claim is that object representations contain featural information that has been warped or transformed, this is more strongly supported by the presented data. I think the authors are arguing for the former rather than the latter, but the data seem to support the latter over the former. More clarity on the theoretical claims and how the data speak to those claims would be helpful.</p><p>– An explanation for the direction of the univariate effect in the temporal pole and acknowledgement of alternative interpretations is warranted. The authors mention in a later section that this result could be a novelty response, but acknowledging this possibility in the same section the data are reported feels pertinent, especially since other interpretations are provided (&quot;these neural changes imply the formation of an explicit integrative code in the temporal pole&quot;).</p><p>– The finding that congruent visual-sound pairs were more dissimilar after multimodal object learning should be contextualized within the theory of &quot;explicit integrative&quot; representations. First, why would this representational theory predict this direction of representational change? Second, if one's claim is that a new, explicit representation is formed to represent the learned multimodal object, a learning-evoked change in unimodal feature representations seems to contradict that theory. If the explicit object representation is distinct from the features themselves, wouldn't it follow that the unimodal features should remain unchanged? Explaining how the results help discriminate between the two representational theories raised in the introduction, and how it specifically supports the explicit integrative theory, would enable the reader to contextualize the reported findings.</p><p>– The finding that unimodal features correlate with their respective multimodal object representations before but not after learning in the perirhinal cortex provides the best support for the claim that object representations are independent of feature representations. However, there are two modifications to the current analysis that could make this argument more direct. First, readers would want to know that unimodal features no longer correlate with congruent objects in the perirhinal cortex, but that their correlations with incongruent objects are unaffected. Otherwise, we can't interpret this result as due to multimodal object learning per se. Second, there's the question of representational stability in the perirhinal cortex. If perirhinal feature representations are not stable across days, it is possible that featural content is actually present in the object representations, but this would only be evident if one used the Day 4 feature representations instead of the Day 2 feature representations. If the Day 4 feature representations do not correlate with the Day 4 congruent object representations, this would be the most direct evidence for explicit, integrative object representations that are distinct from feature representations in the perirhinal cortex.</p><p>– There is a conflation between &quot;visual features&quot; and &quot;objects&quot; throughout the manuscript which can be quite confusing. Sometimes the word &quot;object&quot; is used to represent multimodal visual-auditory objects (&quot;multimodal object&quot;), other times &quot;object&quot; refers to complex visual objects with multiple features (&quot;frog&quot;), and sometimes simple visual stimuli (in functional localizer). In particular, the frog example used throughout the manuscript doesn't feel appropriate, because the representation of &quot;frog&quot; is a lot more complex than one visual feature, and is already an integrated representation across different visual features and modalities (even if the &quot;croak&quot; feature is hypothetically removed). The frog example also muddies the theoretical claims-the authors want the &quot;frog&quot; representation to change when paired with &quot;croak&quot; because &quot;frog&quot; is already an integrated object representation that now needs to be modified. However, the authors should *not* want the representation of a single visual feature to change, since one visual feature is an ingredient that is fed into a separate, integrated object representation.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I would encourage the authors to provide open access to the data and analysis code.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83382.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions (for the authors):</p><p>1) Clarify what is meant by an &quot;explicit integrative&quot; representation and improve the motivation for the individual analyses as tests of such a representation. For example, consider including, for each analysis, specific predictions that follow from the main hypothesis (and not alternative hypotheses) of an explicit integrative representation. A schematic illustration may also be helpful.</p><p>2) Discuss alternative explanations for the results; the reviewers were not convinced that all analyses provide evidence for an explicit integrative representation.</p><p>3) Run additional analyses that may provide more direct evidence for an integrative code.</p><p>4) Correct for multiple comparisons, where appropriate, and test the specificity of results to ROIs (e.g., in an ANOVA that includes ROI).</p><p>5) Consider including additional ROIs (e.g., ventral ATL/anterior fusiform, hippocampus, parietal cortex).</p><p>6) Include additional detail about the procedures and analyses.</p></disp-quote><p>We thank the editor and reviewers for their careful reading of the manuscript and are grateful for the constructive feedback.</p><p>Overall, we have rewritten the manuscript based on the reviewer feedback. More specifically, we have now (1) elaborated on why our methodological design allows us to make claims about “crossmodal integrative coding” in anterior temporal lobe structures that is different from the representations of the unimodal features, as well as softened our claims substantially (e.g., new title, rewrote results and discussion), (2) discussed alternative interpretations of the data, (3) performed several additional analyses suggested by the reviewers, (4) clarified why we have not controlled for multiple comparisons, by removing a series of post hoc across-ROI comparisons that were irrelevant to the key questions of the present manuscript, (5), included additional ROIs as suggested by the reviewers, and (6) discussed key analytic choices and include substantially more detail about the procedures and analyses.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The Introduction could discuss additional literature that is relevant to the current study. For example, previous studies investigated learning associations between modalities, and discussed several ways in which this could lead to neural changes; for example, unisensory stimulation could activate representations of the other modality after learning (Shams and Seitz, TiCS 2008). Previous fMRI work on conceptual/multimodal representations in ATL is not reviewed in much detail. Studies that seem particularly relevant are Vigano and Piazza (2020) and Vigano et al. (2021).</p><p>– The focus is specifically on the anterior temporal lobes. However, there are other regions that have been proposed (and shown) to similarly represent multimodal object representations, including the parietal cortex (Binder et al., TiCS 2011). Indeed, Binder et al. write: &quot;the most anterior parts of the temporal lobe, including the TP and anteromedial temporal regions, are unlikely to be a critical hub for retrieval of multimodal semantic knowledge.&quot; Discussing these alternative views would be relevant for a balanced article.</p></disp-quote><p>We thank the reviewer for pointing us to this literature and now cite these papers in the manuscript where appropriate. Vigano and Piazza (2020) examined navigation along dimensions on a conceptual space, finding evidence of direction-based coding in the prefrontal cortex and entorhinal cortex. Interestingly, it is likely that our perirhinal cortex masks overlap with what Vigano and Piazza (2020) had defined as entorhinal cortex. Vigano et. al (2021) found that the parietal cortex may represent absolute direction along conceptual spaces. This is consistent with Binder (2011), who argued that the inferior parietal lobes are essential for the retrieval of multimodal semantic knowledge. Finally, Shams and Seitz described the importance of multisensory learning. Notably, and in contrast to this previous work, our study investigated whether crossmodal representations are entirely constructed from their unimodal features or whether crossmodal representations are distinct from the sum of their component features. While we agree that previous findings have linked multiple brain regions to multisensory representations, this previous work did not consider crossmodal object representations separately from their component unimodal feature-level representations, which was the central focus of our present study. Depending on the research question, factors such as complexity, familiarity with the unimodal features, and subjective similarity may also be uncontrolled, which may complicate the interpretation of results.</p><p>Given this, we preferred to keep our introduction focused on our specific question of the relationship between crossmodal object representations and the component unimodal features. However, we agree with both Reviewers 1 and 3 that these other brain regions are interesting as they relate to binding and now include them as exploratory analyses in Supplemental Figure 4. We did not find evidence of crossmodal integrative coding in the inferior parietal lobes or hippocampus. Instead, these regions better seemed to represent the <italic>convergence</italic> of unimodal feature representations.</p><p>We now discuss this at various points throughout the manuscript:</p><p>“Notably, our perirhinal cortex mask overlaps with a key region of the ventral anterior temporal lobe thought to be the central locus of crossmodal integration in the “hub and spokes” model of semantic representations.<sup>9,50</sup> However, additional work has also linked other brain regions to the convergence of unimodal representations, such as the hippocampus<sup>51,52,53</sup> and inferior parietal lobes.<sup>54,55</sup> This past work on the hippocampus and inferior parietal lobe does not necessarily address the crossmodal binding problem that was the main focus of our present study, as previous findings often do not differentiate between crossmodal integrative coding and the convergence of unimodal feature representations per se. Furthermore, previous studies in the literature typically do not control for stimulus-based factors such as experience with unimodal features, subjective similarity, or feature identity that may complicate the interpretation of results when determining regions important for crossmodal integration. Indeed, we found evidence consistent with the convergence of unimodal feature-based representations in both the hippocampus and inferior parietal lobes (Figure 5 —figure supplement 1), but no evidence of crossmodal integrative coding different from the unimodal features. The hippocampus and inferior parietal lobes were both sensitive to visual and sound features before and after crossmodal learning (see Figure 5 —figure supplement 1). Yet the hippocampus and inferior parietal lobes did not differentiate between the congruent and incongruent conditions or change with experience (see Figure 5 —figure supplement 1).” – pg. 15</p><p>“Analyses for the hippocampus (HPC) and inferior parietal lobe (IPL). (a) In the visual vs. auditory univariate analysis, there was no visual or sound bias in HPC, but there was a bias towards sounds that increased numerically after crossmodal learning in the IPL. (b) Pattern similarity analyses between unimodal features associated with congruent objects and incongruent objects. Similar to Figure 4 —figure supplement 1, there was no main effect of congruency in either region. (c) When we looked at the pattern similarity between Unimodal Feature runs on Day 2 to Crossmodal Object runs on Day 2, we found that there was significant pattern similarity when there was a match between the unimodal feature and the crossmodal object (e.g., pattern similarity &gt; 0). This pattern of results held when (d) correlating the Unimodal Feature runs on Day 2 to Crossmodal Object runs on Day 4, and (e) correlating the Unimodal Feature runs on Day 4 to Crossmodal Object runs on Day 4. Finally, (f) there was no significant pattern similarity between Crossmodal Object runs before learning correlated to Crossmodal Object after learning in HPC, but there was significant pattern similarity in IPL (p &lt; 0.001). Taken together, these results suggest that both HPC and IPL are sensitive to visual and sound content, as the (c, d, e) unimodal feature-level representations were correlated to the crossmodal object representations irrespective of learning day. However, there was no difference between congruent and incongruent pairings in any analysis, suggesting that HPC and IPL did not represent crossmodal objects differently from the component unimodal features. For these reasons, HPC and IPL may represent the convergence of unimodal feature representations (i.e., because HPC and IPL were sensitive to both visual and sound features), but our results do not seem to support these regions in forming crossmodal integrative coding distinct from the unimodal features (i.e., because HPC and IPL did not differentiate the congruent and incongruent conditions and did not change with experience). * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Asterisks above or below bars indicate a significant difference from zero. Horizontal lines within brain regions in (a) reflect an interaction between modality and learning day, whereas horizontal lines within brain regions in reflect main effects of (b) learning day, (c-e) modality, or (f) congruency.” – Figure 5 —figure supplement 1.</p><disp-quote content-type="editor-comment"><p>– Throughout the manuscript, the comparison is made with knowledge of a frog. While this is useful in the Introduction as general background, to set up the study, I found it somewhat confusing in the Results section. In the current study, the sounds and shapes are arbitrary and are learned for a short period. The resulting neural representation may well be different from the semantic representations of animal shapes and sounds, which have developed over many years.</p></disp-quote><p>We thank the reviewer for this suggestion. We have now removed the frog examples when describing the results.</p><disp-quote content-type="editor-comment"><p>– The results are interpreted as evidence for an integrative code already in the Results section. For example, on p.9, the incongruent&gt;congruent activation in TP is interpreted as &quot;the formation of an explicit integrative code in the temporal pole&quot;. However, the evidence for such a multimodal object code is very indirect here, and there are many alternative explanations for this (and other) findings (e.g., the novelty signal mentioned on p.11). I think it would be better to stay closer to the data when first presenting these individual results. Alternatively, to remain open to alternative interpretations, results can be described as being &quot;consistent with&quot; the integrative code interpretation.</p><p>– Relatedly, it was often not clear to me why specific results provide evidence for an integrative code. It might help if you could motivate the analyses in the context of searching for an integrative code, i.e., make explicit predictions based on the hypothesis of an integrative code and then present the results. In this way, the reader can follow the logic of the predictions before being presented with possibly complex results.</p></disp-quote><p>We agree with the reviewer that some of our evidence is indirect and have now tried to stay much closer to the data when presenting individual results. In addition, the Results section has now been completely rewritten to provide better motivation and a clearer description of our analyses.</p><p>Furthermore, we now clarify that our experimental task equated the unimodal feature representations in the congruent and incongruent conditions. Congruent and incongruent objects have the same features – the only difference between the conditions was the learned crossmodal representation that emerged with experience. Thus, neural differences between the congruent and incongruent conditions would suggest that the crossmodal object representation is sensitive to the combinations of those features, over and above the identity of those features themselves. Finally, we removed the original novelty signal mentioned on pg. 11, because this was referring to the univariate results and may be a source of confusion.</p><p>Overall, when presenting individual results we offer minimal interpretation, as well as better describe our key methodological design in the introduction. For example:</p><p>“Critically, our four-day learning task allowed us to isolate any neural activity associated with integrative coding in anterior temporal lobe structures that emerges with experience and differs from the neural patterns recorded at baseline. The learned and non-learned crossmodal objects were constructed from the same set of three validated shape and sound features, ensuring that factors such as familiarity with the unimodal features, subjective similarity, and feature identity were tightly controlled (Figure 2). If the mind represented crossmodal objects entirely as the reactivation of unimodal shapes and sounds (i.e., objects are constructed from their parts), then there should be no difference between the learned and non-learned objects (because they were created from the same three shapes and sounds). By contrast, if the mind represented crossmodal objects as something over and above their component features (i.e., representations for crossmodal objects rely on integrative coding that is different from the sum of their parts), then there should be behavioral and neural differences between learned and non-learned crossmodal objects (because the only difference across the objects is the learned relationship between the parts). Furthermore, this design allowed us to determine the relationship between the object representation acquired after crossmodal learning and the unimodal feature representations acquired before crossmodal learning. That is, we could examine whether learning led to abstraction of the object representations such that it no longer resembled the unimodal feature representations.” – pg. 5</p><p>“As a central goal of our study was to identify brain regions that were influenced by the learned crossmodal associations, we next examined univariate differences between Congruent vs. Incongruent for crossmodal object runs as a function of whether the crossmodal association had been learned. We conducted a linear mixed model for each ROI which included learning day (before vs. after crossmodal learning) and congruency (congruent vs. incongruent objects) as fixed factors. We observed a significant interaction between learning day and congruency in the temporal pole (F<sub>1,48</sub> = 7.63, p = 0.0081, η<sup>2</sup> = 0.14). Critically, there was no difference in activity between congruent and incongruent objects at baseline before crossmodal learning (t<sub>33</sub> = 0.37, p = 0.72), but there was more activation to incongruent compared to congruent objects after crossmodal learning (t<sub>33</sub> = 2.42, p = 0.021, Cohen’s d = 0.42). As the unimodal shape-sound features experienced by participants were the same before and after crossmodal learning (Figure 2), this finding reveals that the univariate signal in the temporal pole was differentiated between congruent and incongruent objects that had been constructed from the same unimodal features.” – pg. 8</p><p>“By contrast, we did not observe a univariate difference between the congruent and incongruent conditions in the perirhinal cortex, LOC, V1, or A1 (F<sub>1,45-48</sub> between 0.088 and 2.34, p between 0.13 and 0.77). Similarly, the exploratory ROIs hippocampus (HPC: F<sub>1,48</sub> = 0.32, p = 0.58) and inferior parietal lobe (IPL: F<sub>1,48</sub> = 0.094, p = 0.76) did not distinguish between the congruent and incongruent conditions.” – pg. 8</p><disp-quote content-type="editor-comment"><p>– The strongest effect of multimodal learning on univariate activity (Figure 3d) is the reduced visual bias in LOC. This result is not investigated further and does not feature in the interpretation and conclusions. However, this effect seems important to understand, also in relation to a similar (smaller) change in PRC, which is interpreted extensively. Do both these effects reflect the same underlying change in processing the stimuli after learning? Could it reflect visual imagery after multimodal learning? E.g., participants may imagine the sound-associated object after having learned the association, thus making the response evoked by the sound more visual.</p></disp-quote><p>We thank the reviewer for highlighting this issue. We primarily focused on the anterior temporal lobes due to prior theoretical work on the importance of this region to concept representations. As the reviewer mentions, there could be several factors that might be at play – these findings could reflect visual imagery or also feedback connectivity between the anterior temporal lobes to the LOC. We now make the rationale of our study and analyses clearer in the manuscript, for example:</p><p>“Thus, a key theoretical challenge central to resolving the crossmodal binding problem is understanding how anterior temporal lobe structures form object representations. Are crossmodal objects entirely built from features distributed across sensory regions, or is there also integrative coding in the anterior temporal lobes? Furthermore, the existing literature has predominantly studied the neural representation of well-established object concepts from the visual domain alone,<sup>8-25</sup> even though human experience is fundamentally crossmodal.” – pg. 4</p><p>“As a central goal of our study was to identify brain regions that were influenced by the learned crossmodal associations, we next examined univariate differences between Congruent vs. Incongruent for crossmodal object runs as a function of whether the crossmodal association had been learned.” – pg. 8</p><p>“Importantly, the initial visual shape bias observed in the perirhinal cortex was attenuated by experience (Figure 5, Figure 5 —figure supplement 2), suggesting that the perirhinal representations had become abstracted and were no longer predominantly grounded in a single modality after crossmodal learning. One possibility may be that the perirhinal cortex is by default visually driven as an extension to the ventral visual stream,<sup>10,11,12</sup> but can act as a polymodal “hub” region for additional crossmodal input following learning. A complementary possibility may be that our visual features contained tactile information (Figure 1c) that the perirhinal cortex may be sensitive to following the initial exploration phase on our task (Figure 2).<sup>40</sup> Critically, other brain regions like the LOC also reduced in visual bias (Figure 3c), which may reflect visual imagery or feedback connectivity between the anterior temporal lobes. However, the perirhinal cortex was the only region where the visual bias was entirely attenuated following crossmodal learning (Figure 5b).” – pg. 14</p><disp-quote content-type="editor-comment"><p>– The analysis on page 12 (Figure 4) shows the reduced similarity of associated shapes and sounds in TP after learning. This is an interesting finding but I find it hard to interpret, also in light of the behavioral results showing that the shapes and sounds are perceived as more similar after learning – where is this similarity neurally represented? Could this reflect suppression of the associated sound/shape? Furthermore, I did not understand how this finding provides evidence for an explicit integrative code, with &quot;the whole being different from the parts&quot;, considering that this analysis only involved responses to individual modalities.</p></disp-quote><p>We thank the reviewer for identifying this source of confusion. We now better clarify that such a finding could be taken as evidence that learning crossmodal object concepts transforms the original representation of the component unimodal features.</p><p>“We next conducted a series of representational similarity analyses across Unimodal Feature and Crossmodal Object runs before and after crossmodal learning. Here, we investigated whether representations for unimodal features were changed after learning the crossmodal associations between those features (i.e., learning the crossmodal pairings that comprised the shape-sound objects). Such a finding could be taken as evidence that learning crossmodal object concepts transforms the original representation of the component unimodal features. More specifically, we compared the correlation between congruent and incongruent shape-sound features within Unimodal Feature runs before and after crossmodal learning (Figure 4a).” – pg. 8</p><p>In the discussion, we suggest that the overall pattern of results could indicate that the representation in the anterior temporal lobes reflects crossmodal integrative coding, but remain open to alternative interpretations.</p><p>“As one solution to the crossmodal binding problem, we suggest that the temporal pole and perirhinal cortex form unique crossmodal object representations that are different from the distributed features in sensory cortex (Figure 4, 5, 6, Figure 5 —figure supplement 2). However, the nature by which the integrative code is structured and formed in the temporal pole and perirhinal cortex following crossmodal experience – such as through transformations, warping, or other factors – is an open question and an important area for future investigation. Furthermore, these anterior temporal lobe structures may be involved with integrative coding in different ways. For example, the crossmodal object representations measured after learning were found to be related to the component unimodal feature representations measured before learning in the temporal pole but not the perirhinal cortex (Figure 5, 6, Figure 5 —figure supplement 2). Furthermore, pattern similarity for congruent shape-sound pairs were lower than the pattern similarity for incongruent shape-sound pairs after crossmodal learning in the temporal pole but not the perirhinal cortex (Figure 4b, Figure 4 —figure supplement 1). As one interpretation of this pattern of results, the temporal pole may represent new crossmodal objects by combining previously learned knowledge. <sup>8,9,10,11,13,14,15,33</sup> Specifically, research into conceptual combination has linked the anterior temporal lobes to compound object concepts such as “hummingbird”.<sup>34,35,36</sup> For example, participants during our task may have represented the sound-based “humming” concept and visually-based “bird” concept on Day 1, forming the crossmodal “hummingbird” concept on Day 3; Figure 1, 2, which may recruit less activity in temporal pole than an incongruent pairing such as “barking-frog”. For these reasons, the temporal pole may form a crossmodal object code based on pre-existing knowledge, resulting in reduced neural activity (Figure 3d) and pattern similarity towards features associated with learned objects (Figure 4b).” – pg. 12</p><disp-quote content-type="editor-comment"><p>– The reporting of the analysis on page 13 (Figure 5) differs from how the other analyses are reported, starting by showing an interaction with ROI to motivate only testing one ROI. However, the interaction is between modality and ROI after learning but I suppose this interaction is equally strong before learning. This analysis does not test whether the visual bias in PRC (or other regions) was impacted by learning, yet this is what is concluded: &quot;the PRC was the only region that differed in its modality-specific bias across learning days&quot;. A subsequent analysis then tests this more directly, but only for PRC.</p></disp-quote><p>The reviewer is correct that the interaction is equally strong before and after learning. However, the perirhinal cortex was the only region to lose its visually biased coding across brain regions in a direct interaction before and after learning. Furthermore, the perirhinal cortex was an a priori selected brain region given previous literature relating the anterior temporal lobes to crossmodal integration. Moreover, we presented the results this way for clarity, and now further reword this section:</p><p>“To investigate this effect in perirhinal cortex more specifically, we conducted a linear mixed model to directly compare the change in the visual bias of perirhinal representations from before crossmodal learning to after crossmodal learning (green regions in Figure 5a vs. 5b). Specifically, the linear mixed model included learning day (before vs. after crossmodal learning) and modality (visual feature match to crossmodal object vs. sound feature match to crossmodal object). Results revealed a significant interaction between learning day and modality in the perirhinal cortex (F<sub>1,775</sub> = 5.56, p = 0.019, η<sup>2</sup> = 0.071), meaning that the baseline visual shape bias observed in perirhinal cortex (green region of Figure 5a) was significantly attenuated with experience (green region of Figure 5b). After crossmodal learning, a given shape no longer invoked significant pattern similarity between objects that had the same shape but differed in terms of what they sounded like. Taken together, these results suggest that prior to learning the crossmodal objects, the perirhinal cortex had a default bias toward representing the visual shape information and was not representing sound information of the crossmodal objects. After crossmodal learning, however, the visual shape bias in perirhinal cortex was no longer present. That is, with crossmodal learning, the representations within perirhinal cortex started to look less like the visual features that comprised the crossmodal objects, providing evidence that the perirhinal representations were no longer predominantly grounded in the visual modality.” – pg. 10</p><p>“One theoretical view from the cognitive sciences suggests that crossmodal objects are built from component unimodal features represented across distributed sensory regions.<sup>8</sup> Under this view, when a child thinks about “frog”, the visual cortex represents the appearance of the shape of the frog whereas the auditory cortex represents the croaking sound. Alternatively, other theoretical views predict that multisensory objects are not only built from their component unimodal sensory features, but that there is also a crossmodal integrative code that is different from the sum of these parts.<sup>9,10,11,12,13</sup> These latter views propose that anterior temporal lobe structures can act as a polymodal “hub” that combines separate features into integrated wholes.<sup>9,11,14,15</sup>” – pg. 4</p><disp-quote content-type="editor-comment"><p>– I didn't understand why having a visual or sound bias (in Figure 5) is strong evidence for &quot;an explicit integrative object representation transformed from the original features&quot; (p.14). The specific change in PRC is hard to interpret, considering that the similarity is generally very low and after learning the PRC no longer resembled either of the components. This could reflect increased noise, for example, because patterns are now compared across sessions (see also next point).</p></disp-quote><p>We thank the reviewer for highlighting this issue. We now better clarify that Figure 5 indicates that the originally visually-biased representations in perirhinal cortex was changed with experience. More specifically, we found that prior to learning the crossmodal objects, the perirhinal cortex had a default bias toward representing the visual shape information but there was no evidence that perirhinal cortex was tracking the unimodal sound features on the crossmodal objects. After crossmodal learning, the visual shape bias in perirhinal cortex is no longer present – that is, with crossmodal learning, the perirhinal cortex started to look less like the visual features that comprise the crossmodal objects and were no longer predominantly grounded in a single modality.</p><p>“Taken together, these results suggest that prior to learning the crossmodal objects, the perirhinal cortex had a default bias toward representing the visual shape information and was not representing sound information of the crossmodal objects. After crossmodal learning, however, the visual shape bias in perirhinal cortex was no longer present. That is, with crossmodal learning, the representations within perirhinal cortex started to look less like the visual features that comprised the crossmodal objects, providing evidence that the perirhinal representations were no longer predominantly grounded in the visual modality.” – pg. 10</p><p>“Importantly, the initial visual shape bias observed in the perirhinal cortex was attenuated by experience (Figure 5, Figure 5 —figure supplement 2), suggesting that the perirhinal representations had become abstracted and were no longer predominantly grounded in a single modality after crossmodal learning. One possibility may be that the perirhinal cortex is by default visually driven as an extension to the ventral visual stream,<sup>10,11,12</sup> but can act as a polymodal “hub” region for additional crossmodal input following learning.” – pg. 14</p><p>Notably, these results are unlikely to be driven by noise or poor alignment of patterns across sessions, as not all brain regions decreased in pattern similarity across days, even in regions with pattern similarity numerically like that of the PRC (e.g., TP and A1 did not change in Figure 5, nor did HPC and IPL in Figure 5 —figure supplement 1).</p><p>“Importantly, the change in pattern similarity in the perirhinal cortex across learning days (Figure 5) is unlikely to be driven by noise, poor alignment of patterns across sessions, or generally reduced responses. Other regions with numerically similar pattern similarity to perirhinal cortex did not change across learning days (e.g., visual features x crossmodal objects in A1 in Figure 5; the exploratory ROI hippocampus with numerically similar pattern similarity to perirhinal cortex also did not change in Figure 5 —figure supplement 1).” – pg. 11</p><disp-quote content-type="editor-comment"><p>– The analysis in Figure 6 shows pattern similarity across days. PRC shows a significant difference before vs after learning but the overall similarity is very low (and not above 0), and the effect is driven by below-zero similarity in the incongruent condition. I'm not sure how a below-zero similarity can be interpreted. The main effect of the region is not very informative here; it does not show that the learning-related difference was specific to PRC.</p></disp-quote><p>We thank the reviewer for this suggestion. In the discussion, we suggest that one possibility is that this below zero pattern similarity reflects a non-linear transformation, such that crossmodal representations in the incongruent condition are made to be dissimilar from the unimodal features before learning (e.g., anticorrelation), whereas crossmodal representations in the congruent condition are made to be orthogonal from the unimodal features before learning (e.g., no correlation) – suggestive of pattern separation. Another possibility – as Reviewer 2 suggested – is that incongruent representations are less stable, with the representation warped to a greater extent than the congruent objects. Future work should more directly explore the structure of the integrative code that emerges with experience (as previously discussed in the response to reviewers and on page 13).</p><p>“By contrast, perirhinal cortex may be involved in pattern separation following crossmodal experience. In our task, participants had to differentiate congruent and incongruent objects constructed from the same three shape and sound features (Figure 2). An efficient way to solve this task would be to form distinct object-level outputs from the overlapping unimodal feature-level inputs such that congruent objects are made to be orthogonal from the representations before learning (i.e., measured as pattern similarity equal to 0 in the perirhinal cortex; Figure 5b, 6, Figure 5 —figure supplement 2), whereas non-learned incongruent objects could be made to be dissimilar from the representations before learning (i.e., anticorrelation, measured as patten similarity less than 0 in the perirhinal cortex; Figure 6). Because our paradigm could decouple neural responses to the learned object representations (on Day 4) from the original component unimodal features at baseline (on Day 2), these results could be taken as evidence of pattern separation in the human perirhinal cortex.<sup>11,12</sup> However, our pattern of results could also be explained by other types of crossmodal integrative coding. For example, incongruent object representations may be less stable than congruent object representations, such that incongruent objects representation are warped to a greater extent than congruent objects (Figure 6).” – pg. 13</p><disp-quote content-type="editor-comment"><p>– A more direct test of &quot;the whole is different from the sum of the parts&quot; would be to model the combined response using the individual feature responses (e.g., see Baldassano et al., Cereb Cortex 2017). You would expect that a linear combination of sound and shape activity patterns (e.g., the average) is less similar to congruent objects than incongruent objects after learning. Including this test would be important to address the key hypothesis.</p></disp-quote><p>We thank the reviewer for this suggestion. We conducted this analysis, but there was no significant difference between congruent or incongruent objects when correlated to an average of sound and shape features in any brain region (within days or across days). However, this could be because participants are representing both the congruent and incongruent objects differently from the unimodal features. That is, both the congruent and incongruent objects were transformed with experience and different from the unimodal features in anterior temporal lobe structures. By contrast, a version of this analysis was significant in the temporal pole (shown in Figure 4b), whereby the features associated with congruent objects are less similar than the features associated with incongruent objects.</p><p>Moreover, we softened claims throughout the manuscript regarding the “whole is different from the sum of the parts”. However, we suggest that aspects of our results are consistent with this interpretation:</p><p>“Within the perirhinal cortex, the acquired crossmodal object concepts (measured after crossmodal learning) became less similar to their original component unimodal features (measured at baseline before crossmodal learning); Figure 5, 6, Figure 5 —figure supplement 2. This is consistent with the idea that object representations in perirhinal cortex integrate the component sensory features into a whole that is different from the sum of the component parts, which might be a mechanism by which object concepts obtain their abstraction.” – pg. 12</p><p>Although the current work provides evidence for crossmodal integrative coding in the anterior temporal lobes, the structure of how this integrative coding emerges with experience – through transformations, warping, or another factor – is an open question and an important area for future investigation.</p><p>“As one solution to the crossmodal binding problem, we suggest that the temporal pole and perirhinal cortex form unique crossmodal object representations that are different from the distributed features in sensory cortex (Figure 4, 5, 6, Figure 5 —figure supplement 2). However, the nature by which the integrative code is structured and formed in the temporal pole and perirhinal cortex following crossmodal experience – such as through transformations, warping, or other factors – is an open question and an important area for future investigation.” – pg. 12</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– The only explanation of the theory of &quot;explicit integrative representations&quot; is that the &quot;whole is different than the sum of its parts.&quot; This is not enough for the reader to fully understand the manuscript's theoretical claims. An explanation of what is meant by &quot;explicit&quot; and what is meant by &quot;integrative&quot; seems necessary since researchers in different fields and subfields might interpret those terms in meaningfully different ways. If the claim is that &quot;explicit integrative representations&quot; are abstracted away from featural information, the currently presented analyses are not convincing. If the claim is that object representations contain featural information that has been warped or transformed, this is more strongly supported by the presented data. I think the authors are arguing for the former rather than the latter, but the data seem to support the latter over the former. More clarity on the theoretical claims and how the data speak to those claims would be helpful.</p></disp-quote><p>We thank the reviewer for these suggestions. We have clarified that we mean “cross-modal integrative coding” to be a whole object representation that is different from the original unimodal features (pg. 5). We have also now removed the term “explicit”.</p><p>“Critically, our four-day learning task allowed us to isolate any neural activity associated with integrative coding in anterior temporal lobe structures that emerges with experience and differs from the neural patterns recorded at baseline. The learned and non-learned crossmodal objects were constructed from the same set of three validated shape and sound features, ensuring that factors such as familiarity with the unimodal features, subjective similarity, and feature identity were tightly controlled (Figure 2). If the mind represented crossmodal objects entirely as the reactivation of unimodal shapes and sounds (i.e., objects are constructed from their parts), then there should be no difference between the learned and non-learned objects (because they were created from the same three shapes and sounds). By contrast, if the mind represented crossmodal objects as something over and above their component features (i.e., representations for crossmodal objects rely on integrative coding that is different from the sum of their parts), then there should be behavioral and neural differences between learned and non-learned crossmodal objects (because the only difference across the objects is the learned relationship between the parts). Furthermore, this design allowed us to determine the relationship between the object representation acquired after crossmodal learning and the unimodal feature representations acquired before crossmodal learning. That is, we could examine whether learning led to abstraction of the object representations such that it no longer resembled the unimodal feature representations.” – pg. 5</p><p>Furthermore, we have removed our interpretations from the Results section, and instead provide our suggested interpretation in the Discussion section. We highlight that while we find crossmodal integrative coding that is different from the unimodal feature representations in anterior temporal lobe structures, the structure of how this integrative code emerges with experience is less clear and is an important avenue of future research (pg. 18).</p><p>“By contrast, perirhinal cortex may be involved in pattern separation following crossmodal experience. In our task, participants had to differentiate congruent and incongruent objects constructed from the same three shape and sound features (Figure 2). An efficient way to solve this task would be to form distinct object-level outputs from the overlapping unimodal feature-level inputs such that congruent objects are made to be orthogonal from the representations before learning (i.e., measured as pattern similarity equal to 0 in the perirhinal cortex; Figure 5b, 6, Figure 5 —figure supplement 2), whereas non-learned incongruent objects could be made to be dissimilar from the representations before learning (i.e., anticorrelation, measured as patten similarity less than 0 in the perirhinal cortex; Figure 6). Because our paradigm could decouple neural responses to the learned object representations (on Day 4) from the original component unimodal features at baseline (on Day 2), these results could be taken as evidence of pattern separation in the human perirhinal cortex.<sup>11,12</sup> However, our pattern of results could also be explained by other types of crossmodal integrative coding. For example, incongruent object representations may be less stable than congruent object representations, such that incongruent objects representation are warped to a greater extent than congruent objects (Figure 6).” – pg. 13</p><disp-quote content-type="editor-comment"><p>– An explanation for the direction of the univariate effect in the temporal pole and acknowledgement of alternative interpretations is warranted. The authors mention in a later section that this result could be a novelty response, but acknowledging this possibility in the same section the data are reported feels pertinent, especially since other interpretations are provided (&quot;these neural changes imply the formation of an explicit integrative code in the temporal pole&quot;).</p></disp-quote><p>We thank the reviewer for this suggestion and now temper our wording when describing the univariate results in the temporal pole.</p><p>“As a central goal of our study was to identify brain regions that were influenced by the learned crossmodal associations, we next examined univariate differences between Congruent vs. Incongruent for crossmodal object runs as a function of whether the crossmodal association had been learned. We conducted a linear mixed model for each ROI which included learning day (before vs. after crossmodal learning) and congruency (congruent vs. incongruent objects) as fixed factors. We observed a significant interaction between learning day and congruency in the temporal pole (F<sub>1,48</sub> = 7.63, p = 0.0081, η<sup>2</sup> = 0.14). Critically, there was no difference in activity between congruent and incongruent objects at baseline before crossmodal learning (t<sub>33</sub> = 0.37, p = 0.72), but there was more activation to incongruent compared to congruent objects after crossmodal learning (t<sub>33</sub> = 2.42, p = 0.021, Cohen’s d = 0.42). As the unimodal shape-sound features experienced by participants were the same before and after crossmodal learning (Figure 2), this finding reveals that the univariate signal in the temporal pole was differentiated between congruent and incongruent objects that had been constructed from the same unimodal features.” – pg. 8</p><disp-quote content-type="editor-comment"><p>– The finding that congruent visual-sound pairs were more dissimilar after multimodal object learning should be contextualized within the theory of &quot;explicit integrative&quot; representations. First, why would this representational theory predict this direction of representational change? Second, if one's claim is that a new, explicit representation is formed to represent the learned multimodal object, a learning-evoked change in unimodal feature representations seems to contradict that theory. If the explicit object representation is distinct from the features themselves, wouldn't it follow that the unimodal features should remain unchanged? Explaining how the results help discriminate between the two representational theories raised in the introduction, and how it specifically supports the explicit integrative theory, would enable the reader to contextualize the reported findings.</p></disp-quote><p>We hope that our changes throughout the manuscript now better clarify that we are looking for a crossmodal integrative code different from the unimodal features. We remain open to alternative structures of the integrative code, and so we have not predicted a direction of representational change (though we provide some interpretation in the Discussion section). As mentioned in a previous response, these changes are described on pages 12 and 13.</p><p>Notably, it is likely the case that the crossmodal integrative code can change <italic>in addition to</italic> the unimodal feature representations. For example, we have previously shown that unimodal visual feature representations are influenced by experience in parallel to the representation of the conjunction (e.g., Liang et al., 2020; <italic>Cerebral Cortex</italic>).</p><disp-quote content-type="editor-comment"><p>– The finding that unimodal features correlate with their respective multimodal object representations before but not after learning in the perirhinal cortex provides the best support for the claim that object representations are independent of feature representations. However, there are two modifications to the current analysis that could make this argument more direct. First, readers would want to know that unimodal features no longer correlate with congruent objects in the perirhinal cortex, but that their correlations with incongruent objects are unaffected. Otherwise, we can't interpret this result as due to multimodal object learning per se. Second, there's the question of representational stability in the perirhinal cortex. If perirhinal feature representations are not stable across days, it is possible that featural content is actually present in the object representations, but this would only be evident if one used the Day 4 feature representations instead of the Day 2 feature representations. If the Day 4 feature representations do not correlate with the Day 4 congruent object representations, this would be the most direct evidence for explicit, integrative object representations that are distinct from feature representations in the perirhinal cortex.</p></disp-quote><p>We thank the reviewer for this suggestion and have conducted the suggested analyses, splitting the congruent and incongruent conditions. There was no significant interaction between modality and congruency in any ROI across days or within days. In one possibility, it may be the case that both congruent and incongruent crossmodal objects are represented differently from their underlying unimodal features, and all of these representations can change with experience (e.g., are not stable across days).</p><p>Indeed, in an additional exploratory analysis, we found that perirhinal cortex was the only region where the Day 4 unimodal feature representations do not correlate with the Day 4 crossmodal object representations – suggestive of a crossmodal integrative code transformed from the unimodal features. Finally, we emphasize that unimodal feature representations can also change with learning in parallel to changes at the level of the conjunction (e.g., Liang et al., 2020; Cerebral Cortex).</p><p>“To examine whether these results differed by congruency (i.e., whether any modality-specific biases differed as a function of whether the object was congruent or incongruent), we conducted exploratory linear mixed models for each of the five a priori ROIs across learning days. More specifically, we correlated: (1) the voxel-wise activity for Unimodal Feature Runs before crossmodal learning to the voxel-wise activity for Crossmodal Object Runs before crossmodal learning (Day 2 vs. Day 2), (2) the voxel-wise activity for Unimodal Feature Runs before crossmodal learning to the voxel-wise activity for Crossmodal Object Runs after crossmodal learning (Day 2 vs Day 4), and (3) the voxel-wise activity for Unimodal Feature Runs after crossmodal learning to the voxel-wise activity for Crossmodal Object Runs after crossmodal learning (Day 4 vs Day 4). For each of the three analyses described, we then conducted separate linear mixed models which included modality (visual feature match to crossmodal object vs. sound feature match to crossmodal object) and congruency (congruent vs. incongruent).” – pg. 10</p><p>“There was no significant relationship between modality and congruency in any ROI between Day 2 and Day 2 (F<sub>1,346-368</sub> between 0.00 and 1.06, p between 0.30 and 0.99), between Day 2 and Day 4 (F<sub>1,346-368</sub> between 0.021 and 0.91, p between 0.34 and 0.89), or between Day 4 and Day 4 (F<sub>1,346-368</sub> between 0.01 and 3.05, p between 0.082 and 0.93). However, exploratory analyses revealed that perirhinal cortex was the only region without a modality-specific bias and where the unimodal feature runs were not significantly correlated to the crossmodal object runs after crossmodal learning (Figure 5 —figure supplement 2).” – pg. 11</p><p>“Taken together, the overall pattern of results suggests that representations of the crossmodal objects in perirhinal cortex were heavily influenced by their consistent visual features before crossmodal learning. However, the crossmodal object representations were no longer influenced by the component visual features after crossmodal learning (Figure 5, Figure 5 —figure supplement 2). Additional exploratory analyses did not find evidence of experience-dependent changes in the hippocampus or inferior parietal lobes (Figure 5 —figure supplement 1).” – pg. 11</p><p>“The voxel-wise matrix for Unimodal Feature runs on Day 4 were correlated to the voxel-wise matrix for Crossmodal Object runs on Day 4 (see Figure 5 in the main text for an example). We compared the average pattern similarity (z-transformed Pearson correlation) between shape (blue) and sound (orange) features specifically after crossmodal learning. Consistent with Figure 5b, perirhinal cortex was the only region without a modality-specific bias. Furthermore, perirhinal cortex was the only region where the representations of both the visual and sound features were not significantly correlated to the crossmodal objects. By contrast, every other region maintained a modality-specific bias for either the visual or sound features. These results suggest that perirhinal cortex representations were transformed with experience, such that the initial visual shape representations (Figure 5a) were no longer grounded in a single modality after crossmodal learning. Furthermore, these results suggest that crossmodal learning formed an integrative code different from the unimodal features in perirhinal cortex, as the visual and sound features were not significantly correlated with the crossmodal objects. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Horizontal lines within brain regions indicate a significant main effect of modality. Vertical asterisks denote pattern similarity comparisons relative to 0.” – Figure 5 —figure supplement 2</p><disp-quote content-type="editor-comment"><p>– There is a conflation between &quot;visual features&quot; and &quot;objects&quot; throughout the manuscript which can be quite confusing. Sometimes the word &quot;object&quot; is used to represent multimodal visual-auditory objects (&quot;multimodal object&quot;), other times &quot;object&quot; refers to complex visual objects with multiple features (&quot;frog&quot;), and sometimes simple visual stimuli (in functional localizer). In particular, the frog example used throughout the manuscript doesn't feel appropriate, because the representation of &quot;frog&quot; is a lot more complex than one visual feature, and is already an integrated representation across different visual features and modalities (even if the &quot;croak&quot; feature is hypothetically removed). The frog example also muddies the theoretical claims-the authors want the &quot;frog&quot; representation to change when paired with &quot;croak&quot; because &quot;frog&quot; is already an integrated object representation that now needs to be modified. However, the authors should *not* want the representation of a single visual feature to change, since one visual feature is an ingredient that is fed into a separate, integrated object representation.</p></disp-quote><p>We thank the reviewer for providing this suggestion and we now replace “features” with unimodal features and “object” with crossmodal object throughout the manuscript text, figures, and title where appropriate (which was also suggested by Reviewer 3). We also removed the “frog” example in the results.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I would encourage the authors to provide open access to the data and analysis code.</p></disp-quote><p>We thank the reviewer for the positive comments on our work. The preprocessed data files for the univariate and pattern similarity analyses are available on OSF. Given the non-standard format of the multi-echo pipeline and large storage requirements needed, we hope to make the data files readable and openly available in the future when file standards have evolved to include multi-echo ICA data (e.g., BIDS).</p></body></sub-article></article>