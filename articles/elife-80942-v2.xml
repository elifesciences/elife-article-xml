<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80942</article-id><article-id pub-id-type="doi">10.7554/eLife.80942</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>ProteInfer, deep neural networks for protein functional inference</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-282578"><name><surname>Sanderson</surname><given-names>Theo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4177-2851</contrib-id><email>theo.sanderson@crick.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-284023"><name><surname>Bileschi</surname><given-names>Maxwell L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6771-0590</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-284024"><name><surname>Belanger</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7115-9009</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-284025"><name><surname>Colwell</surname><given-names>Lucy J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3148-0337</contrib-id><email>lcolwell@google.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04tnbqb63</institution-id><institution>The Francis Crick Institute</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00njsd438</institution-id><institution>Google AI</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e80942</elocation-id><history><date date-type="received" iso-8601-date="2022-06-10"><day>10</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-02-24"><day>24</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-09-23"><day>23</day><month>09</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.20.461077"/></event></pub-history><permissions><copyright-statement>© 2023, Sanderson, Bileschi et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Sanderson, Bileschi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80942-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80942-figures-v2.pdf"/><abstract><p>Predicting the function of a protein from its amino acid sequence is a long-standing challenge in bioinformatics. Traditional approaches use sequence alignment to compare a query sequence either to thousands of models of protein families or to large databases of individual protein sequences. Here we introduce ProteInfer, which instead employs deep convolutional neural networks to directly predict a variety of protein functions – Enzyme Commission (EC) numbers and Gene Ontology (GO) terms – directly from an unaligned amino acid sequence. This approach provides precise predictions which complement alignment-based methods, and the computational efficiency of a single neural network permits novel and lightweight software interfaces, which we demonstrate with an in-browser graphical interface for protein function prediction in which all computation is performed on the user’s personal computer with no data uploaded to remote servers. Moreover, these models place full-length amino acid sequences into a generalised functional space, facilitating downstream analysis and interpretation. To read the interactive version of this paper, please visit <ext-link ext-link-type="uri" xlink:href="https://google-research.github.io/proteinfer/">https://google-research.github.io/proteinfer/</ext-link>.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>protein</kwd><kwd>function</kwd><kwd>learning</kwd><kwd>neural network</kwd><kwd>prediction</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006785</institution-id><institution>Google</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Sanderson</surname><given-names>Theo</given-names></name><name><surname>Bileschi</surname><given-names>Maxwell L</given-names></name><name><surname>Colwell</surname><given-names>Lucy J</given-names></name><name><surname>Belanger</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Cancer Research</institution></institution-wrap></funding-source><award-id>FC001043</award-id><principal-award-recipient><name><surname>Sanderson</surname><given-names>Theo</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>UK Medical Research Council</institution></institution-wrap></funding-source><award-id>FC001043</award-id><principal-award-recipient><name><surname>Sanderson</surname><given-names>Theo</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>FC001043</award-id><principal-award-recipient><name><surname>Sanderson</surname><given-names>Theo</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>598399</award-id><principal-award-recipient><name><surname>Colwell</surname><given-names>Lucy J</given-names></name></principal-award-recipient></award-group><funding-statement>The Wellcome Trust, CRUK, Simons Foundation and the UK MRC had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A machine learning model predicts the properties of proteins from their amino acid sequences and can run in a user's web browser.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Every day, more than a hundred thousand protein sequences are added to global sequence databases (<xref ref-type="bibr" rid="bib74">UniProt Consortium, 2019a</xref>). However, these entries are of limited use to practitioners unless they are accompanied by functional annotations. While curators diligently extract annotations from the literature, assessing more than 60,000 papers each year (<xref ref-type="bibr" rid="bib75">UniProt Consortium, 2019b</xref>), the time-consuming nature of this task means that only ~0.03% of publicly available protein sequences are manually annotated. The community has a long history of using computational tools to infer protein function directly from amino acid sequence. Starting in the 1980s, methods such as BLAST (<xref ref-type="bibr" rid="bib5">Altschul et al., 1990</xref>) relied on pairwise sequence comparisons, where a query protein is assumed to have the same function as highly similar sequences that have already been annotated. Signature-based approaches were later introduced, with the PROSITE database (<xref ref-type="bibr" rid="bib11">Bairoch, 1991</xref>) cataloguing short amino acid ‘motifs’ found in proteins that share a particular function. Subsequently, a crucial refinement of signature-based approaches was the development of profile hidden Markov models (HMMs) (<xref ref-type="bibr" rid="bib45">Krogh et al., 1994</xref>; <xref ref-type="bibr" rid="bib28">Eddy, 1998</xref>). These models collapse an alignment of related protein sequences into a model that provides likelihood scores for new sequences, which describe how well they fit the aligned set. Critically, profile HMMs allow for longer signatures and fuzzier matching and are currently used to update popular databases such as Interpro and Pfam (<xref ref-type="bibr" rid="bib13">Bateman et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Blum et al., 2021</xref>). Subsequent refinements have made these techniques more sensitive and computationally efficient (<xref ref-type="bibr" rid="bib6">Altschul et al., 1997</xref>; <xref ref-type="bibr" rid="bib29">Eddy, 2011</xref>; <xref ref-type="bibr" rid="bib13">Bateman et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Söding, 2005</xref>; <xref ref-type="bibr" rid="bib71">Steinegger et al., 2019</xref>), while their availability as web tools allows practitioners to easily incorporate them into workflows (<xref ref-type="bibr" rid="bib42">Johnson et al., 2008</xref>; <xref ref-type="bibr" rid="bib70">Soding et al., 2005</xref>; <xref ref-type="bibr" rid="bib56">Potter et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Bernhofer et al., 2021</xref>).</p><p>These computational modelling approaches have had great impact; however, one-third of bacterial proteins still cannot be annotated (even computationally) with a function (<xref ref-type="bibr" rid="bib57">Price et al., 2018</xref>). It is therefore worthwhile examining how new approaches might complement existing techniques. First, current approaches conduct entirely separate comparisons for each comparator sequence or model, and thus may not fully exploit the features shared across different functional classes. An ideal classification system, for example, might have a modular ATP-binding region detector used in detection of both kinases and ABC transporters (<xref ref-type="bibr" rid="bib59">Ramakrishnan et al., 2002</xref>). Separately modelling these targets in each family, like standard HMM approaches, increases the computational cost and may also be less accurate. In addition, the process of creating many of these signatures is not fully automated and requires considerable curatorial efforts (<xref ref-type="bibr" rid="bib31">El-Gebali et al., 2018b</xref>; <xref ref-type="bibr" rid="bib30">El-Gebali et al., 2018a</xref>), which at present are spread across an array of disparate but overlapping signature databases (<xref ref-type="bibr" rid="bib17">Blum et al., 2021</xref>).</p><p>Deep neural networks have recently transformed a number of labelling tasks, including image recognition – the early layers in these models build up an understanding of simple features such as edges, and later layers use these features to identify textures, and then entire objects. Edge-detecting filters can thus be trained with information from all the labelled examples, and the same filters can be used to detect, for instance, both oranges and lemons (<xref ref-type="bibr" rid="bib22">Carter et al., 2019</xref>).</p><p>In response, recent work has contributed a number of deep neural network models for protein function classification (<xref ref-type="bibr" rid="bib25">Dalkiran et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Kulmanov et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Cao et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Almagro Armenteros et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Schwartz et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Sureyya Rifaioglu et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Li et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Hou et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Littmann et al., 2021</xref>). These approaches train a single model to recognise multiple properties, building representations of different protein functions via a series of layers, which allow the same low-level features to be used for different high-level classifications. Of special note is the layer preceding the final layer of the network, which constructs an ‘embedding’ of the entire example in a high-dimensional vector space, and often captures semantic features of the input.</p><p>Beyond functional annotation, deep learning has enabled significant advances in protein structure prediction (<xref ref-type="bibr" rid="bib4">AlQuraishi, 2019</xref>; <xref ref-type="bibr" rid="bib66">Senior et al., 2020</xref>; <xref ref-type="bibr" rid="bib78">Yang et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Du et al., 2019</xref>; <xref ref-type="bibr" rid="bib61">Rao et al., 2021</xref>), predicting the functional effects of mutations (<xref ref-type="bibr" rid="bib62">Riesselman et al., 2018</xref>; <xref ref-type="bibr" rid="bib63">Rives et al., 2021</xref>; <xref ref-type="bibr" rid="bib60">Rao et al., 2019</xref>; <xref ref-type="bibr" rid="bib33">Frazer et al., 2021</xref>), and protein design (<xref ref-type="bibr" rid="bib77">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib51">Mazurenko et al., 2020</xref>; <xref ref-type="bibr" rid="bib16">Biswas et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Madani et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Elnaggar et al., 2020</xref>; <xref ref-type="bibr" rid="bib8">Anishchenko et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Bryant et al., 2021</xref>). A key departure from traditional approaches is that researchers have started to incorporate vast amounts of raw, uncurated sequence data into model training, an approach which also shows promise for functional prediction (<xref ref-type="bibr" rid="bib18">Brandes et al., 2022</xref>).</p><p>Of particular relevance to the present work is <xref ref-type="bibr" rid="bib15">Bileschi et al., 2022</xref>, where it is shown that models with residual layers (<xref ref-type="bibr" rid="bib37">He et al., 2015</xref>) of dilated convolutions (<xref ref-type="bibr" rid="bib79">Yu and Koltun, 2015</xref>) can precisely and efficiently categorise protein domains. <xref ref-type="bibr" rid="bib26">Dohan et al., 2021</xref> provide additional accuracy improvements using uncurated data. However, these models cannot infer functional annotations for full-length protein sequences since they are trained on pre-segmented domains and can only predict a single label. The full-sequence task is of primary importance to biological practitioners.</p><p>To address this challenge we employ deep dilated convolutional networks to learn the mapping between full-length protein sequences and functional annotations. The resulting ProteInfer models take amino acid sequences as input and are trained on the well-curated portion of the protein universe annotated by Swiss-Prot (<xref ref-type="bibr" rid="bib75">UniProt Consortium, 2019b</xref>). We find that (1) ProteInfer models reproduce curator decisions for a variety of functional properties across sequences distant from the training data, (2) attribution analysis shows that the predictions are driven by relevant regions of each protein sequence, and (3) ProteInfer models create a generalised mapping between sequence space and the space of protein functions, which is useful for tasks other than those for which the models were trained. We provide trained ProteInfer networks that enable other researchers to reproduce the analysis presented and explore embeddings of their proteins of interest via both a command line tool (<ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/proteinfer">https://github.com/google-research/proteinfer</ext-link>, copy archived at <xref ref-type="bibr" rid="bib64">Sanderson et al., 2023</xref>), and also via an in-browser JavaScript implementation that demonstrates the computational efficiency of deep learning approaches.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A neural network for protein function prediction</title><p>In a ProteInfer neural network (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>), a raw amino acid sequence is first represented numerically as a <italic>one-hot</italic> matrix and then passed through a series of convolutional layers. Each layer takes the representation of the sequence in the previous layer and applies a number of <italic>filters</italic>, which detect patterns of features. We use <italic>residual</italic> layers, in which the output of each layer is added to its input to ease the training of deeper networks (<xref ref-type="bibr" rid="bib37">He et al., 2015</xref>), and dilated convolutions (<xref ref-type="bibr" rid="bib79">Yu and Koltun, 2015</xref>), meaning that successive layers examine larger sub-sequences of the input sequence. After building up an embedding of each position in the sequence, the model collapses these down to a single <inline-formula><mml:math id="inf1"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional embedding of the sequence using average pooling. Since natural protein sequences can vary in length by at least three orders of magnitude, this pooling is advantageous because it allows our model to accommodate sequences of arbitrary length without imposing restrictive modelling assumptions or computational burdens that scale with sequence length. In contrast, many previous approaches operate on fixed sequence lengths: these techniques are unable to make predictions for proteins larger than this sequence length, and use unnecessary resources when employed on smaller proteins. Finally, a fully connected layer maps these embeddings to logits for each potential label, which are the input to an element-wise sigmoid layer that outputs per-label probabilities. We select all labels with predicted probability above a given confidence threshold, and varying this threshold yields a tradeoff between precision and recall. To summarise model performance as a single scalar, we compute the <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> score, the maximum <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> score (the geometric mean of precision and recall) across all thresholds (<xref ref-type="bibr" rid="bib58">Radivojac et al., 2013</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Three approaches for mapping from an amino acid sequence to inferred function: (1) finding similar sequences in a large database of sequences with known annotation (e.g. BLAST), (2) scoring against a large database of statistical models for each family of sequences with known function (e.g. InterProScan), and (3) applying a single deep neural network trained to predict multiple output categories (e.g. this work).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig1-v2.tif"/></fig><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A deep dilated convolutional architecture for protein function prediction.</title><p>Amino acids are one-hot encoded, then pass through a series of convolutions implemented within residual blocks. Successive filters are increasingly dilated, allowing the top residual layer of the network to build up a representation of high-order protein features. The positional embeddings in this layer are collapsed by mean-pooling to a single embedding of the entire sequence, which is converted into probabilities of each functional classification through a fully connected layer with sigmoidal activations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig2-v2.tif"/></fig><p>Each model was trained for about 60 hr using the Adam optimiser (<xref ref-type="bibr" rid="bib44">Kingma and Ba, 2015</xref>) on 8 NVIDIA P100 GPUs with data parallelism (<xref ref-type="bibr" rid="bib40">Jeffrey, 2012</xref>; <xref ref-type="bibr" rid="bib67">Shallue et al., 2018</xref>). We found that using more than one GPU for training improved training time by allowing an increased batch size, but did not have a substantial impact on accuracy compared to training for longer with a smaller learning rate and smaller batch size on one GPU. The models have a small set of hyperparameters, such as the number of layers and the number of filters in each layer, which were tuned using random sampling to maximise <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> on the random train-test split. Hyperparameter values are available in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Hyperparameters used in convolutional neural networks.</title><p>We note that hyperparameters for single-GPU training are available in <ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/proteinfer/blob/master/hparams_sets.py">github.com/google-research/proteinfer/blob/master/hparams_sets.py</ext-link>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">CNN</th></tr></thead><tbody><tr><td align="left" valign="bottom">Concurrent batches (data parallelism)</td><td align="left" valign="bottom">8</td></tr><tr><td align="left" valign="bottom">Batch size</td><td align="left" valign="bottom">40 (per each GPU)<break/>Dynamic based on sequence length</td></tr><tr><td align="left" valign="bottom">Dilation rate</td><td align="left" valign="bottom">3</td></tr><tr><td align="left" valign="bottom">Filters</td><td align="left" valign="bottom">1100</td></tr><tr><td align="left" valign="bottom">First dilated layer</td><td align="left" valign="bottom">2</td></tr><tr><td align="left" valign="bottom">Gradient clip</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">Kernel size</td><td align="left" valign="bottom">9</td></tr><tr><td align="left" valign="bottom">Learning rate</td><td align="left" valign="bottom">1.5E-3</td></tr><tr><td align="left" valign="bottom">Learning rate decay rate</td><td align="left" valign="bottom">0.997</td></tr><tr><td align="left" valign="bottom">Learning rate decay steps</td><td align="left" valign="bottom">1000</td></tr><tr><td align="left" valign="bottom">Learning rate warmup steps</td><td align="left" valign="bottom">3000</td></tr><tr><td align="left" valign="bottom">Adam <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">.9</td></tr><tr><td align="left" valign="bottom">Adam <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">.999</td></tr><tr><td align="left" valign="bottom">Adam <inline-formula><mml:math id="inf7"><mml:mi>ϵ</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">1E-8</td></tr><tr><td align="left" valign="bottom">Number of ResNet layers</td><td align="left" valign="bottom">5</td></tr><tr><td align="left" valign="bottom">Pooling</td><td align="left" valign="bottom">Mean</td></tr><tr><td align="left" valign="bottom">ResNet bottleneck factor</td><td align="left" valign="bottom">0.5</td></tr><tr><td align="left" valign="bottom">Train steps</td><td align="left" valign="bottom">500,000</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>A machine-learning-compatible dataset for protein function prediction</title><p>The UniProt database is the central global repository for information about proteins. The manually curated portion, Swiss-Prot, is constructed by assessing 60,000 papers each year to harvest 35% of the theoretically curatable information in the literature (<xref ref-type="bibr" rid="bib75">UniProt Consortium, 2019b</xref>). We focus on Swiss-Prot to ensure that our models learn from human-curated labels, rather than labels generated by a computational annotation pipeline. Each protein in Swiss-Prot goes through a six-stage process of sequence curation, sequence analysis, literature curation, family-based curation, evidence attribution, and quality assurance. Functional annotation is stored in UniProt largely through <italic>database cross-references</italic>, which link a specific protein with a label from a particular ontology. These cross-references include Enzyme Commission (EC) numbers, representing the function of an enzyme; Gene Ontology (GO) terms relating to the protein’s molecular function, biological process, or subcellular localisation; protein family information contained in the Pfam (<xref ref-type="bibr" rid="bib13">Bateman et al., 2019</xref>), SUPFAM (<xref ref-type="bibr" rid="bib55">Pandurangan et al., 2019</xref>), PRINTS (<xref ref-type="bibr" rid="bib10">Attwood et al., 2003</xref>), TIGR (<xref ref-type="bibr" rid="bib36">Haft et al., 2013</xref>), PANTHR (<xref ref-type="bibr" rid="bib53">Mi et al., 2016</xref>) databases, or the umbrella database InterPro (<xref ref-type="bibr" rid="bib39">Hunter et al., 2009</xref>), as well as other information including ortholog databases and references on PubMed. Here, we focus on EC and GO labels, though our model training framework can immediately extend to other label sets.</p><p>We use two methods to split data into training and evaluation sets. First, a random split of the data allows us to answer the following question: suppose that curators had randomly annotated only 80% of the sequences in Swiss-Prot. How accurately can ProteInfer annotate the remaining 20%? Second, we use UniRef50 (<xref ref-type="bibr" rid="bib73">Suzek et al., 2015</xref>) clustering to split the data to model a challenging use case in which an unseen sequence has low sequence similarity to anything that has been previously annotated. Note that there are alternative methods for splitting (<xref ref-type="bibr" rid="bib18">Brandes et al., 2022</xref>; <xref ref-type="bibr" rid="bib81">Zhou et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Gillis and Pavlidis, 2013</xref>), such as reserving the most recently annotated proteins for evaluating models. This approach, which is used in CAFA and CASP (<xref ref-type="bibr" rid="bib81">Zhou et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Gillis and Pavlidis, 2013</xref>), helps ensure a fair competition because labels for the evaluation data are not available to participants, or the scientific community at large, until after the competition submissions are due. Such a split is not available for EC classification, which is the primary focus of our analyses below. Finally, note that all of the above approaches typically lack reliable annotation for true negatives (<xref ref-type="bibr" rid="bib76">Warwick Vesztrocy and Dessimoz, 2020</xref>).</p><p>To facilitate further development of machine learning methods, we provide TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>) TFrecord files for Swiss-Prot (<ext-link ext-link-type="uri" xlink:href="https://console.cloud.google.com/storage/browser/brain-genomics-public/research/proteins/proteinfer/datasets/">https://console.cloud.google.com/storage/browser/brain-genomics-public/research/proteins/proteinfer/datasets/</ext-link>). Each example has three fields: the UniProt accession, the amino acid sequence, and a list of database cross-reference labels. UniProt annotations include only leaf nodes for hierarchical taxononomies such as EC and GO. To allow machine learning algorithms to model this hierarchy, we added all parental annotations to each leaf node during dataset creation.</p></sec><sec id="s2-3"><title>Prediction of catalysed reactions</title><p>We initially trained a model to predict enzymatic catalytic activities from amino acid sequence. This data is recorded as EC numbers, which describe a hierarchy of catalytic functions. For instance, <inline-formula><mml:math id="inf8"><mml:mi>β</mml:mi></mml:math></inline-formula> amylase enzymes have an EC number of EC:3.2.1.2, which represents the leaf node in the following hierarchy (<xref ref-type="fig" rid="S1">Scheme 1</xref>):</p><fig id="S1" position="anchor"><label>Scheme 1.</label><caption><title>The EC hierarchy.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-scheme1-v2.tif"/></fig><p>Individual protein sequences can be annotated with zero (non-enzymatic proteins), one (enzymes with a single function), or many (multi-functional enzymes) leaf-level EC numbers. These are drawn from a total of 8162 catalogued chemical reactions. Our best <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was achieved by a model containing five residual blocks with 1100 filters each (full details in ‘Materials and methods’). For the dev set, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> converged within 500,000 training steps. On the random split, the model achieves  <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.977 (0.976–0.978) on the held-out test data. At the corresponding confidence threshold, the model correctly predicts 96.7% of true labels, with a false-positive rate of 1.4%. Results from the clustered test set are discussed below. Performance was roughly similar across labels at the top of the EC hierarchy, with the highest <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> score observed for ligases (0.993), and the lowest for oxidoreductases (0.963). For all classes, the precision of the network was higher than the recall at the threshold maximising <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Precision and recall can be traded off against each other by adjusting the confidence threshold at which the network outputs a prediction, creating the curves shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>ProteInfer performance (<bold>A</bold>) for all seven top-level enzyme groups from a single CNN model (<bold>B</bold>) compared between methods: a single ProteInfer CNN, an ensemble of ProteInfer CNNs, a BLAST-based baseline, and an ensemble of BLAST predictions combined with ProteInfer CNNs.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Histogram of number of labels per sequence, including hierarchical labels, on the random dataset.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Histogram of number of labels per sequence, including hierarchical labels, on the random dataset.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Number of sequences annotated with a given functional label (Enzyme Commission [EC] class) in the random dataset.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Number of sequences annotated with a given functional label (Gene Ontology [GO] label) in the random dataset.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp4-v2.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Number of sequences annotated with a given functional label (Enzyme Commission [EC] class) in the clustered dataset.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp5-v2.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>Number of sequences annotated with a given functional label.</title><p>(Gene Ontology [GO] label) in the clustered dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp6-v2.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Bootstrapped precision–recall curves for Enzyme Commission (EC) number prediction and Gene Ontology term prediction for random and clustered splits for four methods: BLAST top pick, single ProteInfer CNN, ensembled ProteInfer CNNs, and ensembled ProteInfer CNNs scaled by BLAST score.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp7-v2.tif"/></fig><fig id="fig3s8" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 8.</label><caption><title>Full precision–recall curves for Enzyme Commission (EC) number prediction and Gene Ontology term prediction for random and clustered splits for four methods: BLAST top pick, single ProteInfer CNN, ensembled ProteInfer CNNs.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp8-v2.tif"/></fig><fig id="fig3s9" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 9.</label><caption><title>Enzyme Commission (EC) random task with different methods compared against a naive baseline where the predictor is simply the frequency in the training set.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp9-v2.tif"/></fig><fig id="fig3s10" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 10.</label><caption><title>Gene Ontology (GO) performance stratified by method and ontology type.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp10-v2.tif"/></fig><fig id="fig3s11" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 11.</label><caption><title>Performance of Enzyme Commission (EC) model stratified by number of training examples available for each test example.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp11-v2.tif"/></fig><fig id="fig3s12" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 12.</label><caption><title>The ProteInfer algorithm is set up to allow any desired training vocabulary to be used.</title><p>We demonstrated this by additionally training a model for predicting Pfam families from full-length protein sequences, which is available through our CLI-tool, and performs as shown here.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig3-figsupp12-v2.tif"/></fig></fig-group><p>We implemented an alignment-based baseline in which BLASTp is used to identify the closest sequence to a query sequence in the train set. Labels are then imputed for the query sequence by transferring those labels that apply to the annotated match from the train set. We produced a precision–recall curve by using the bit-score of the closest sequence as a measure of confidence, varying the cutoff above which we retain the imputed labels (<xref ref-type="bibr" rid="bib81">Zhou et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Eddy, 2011</xref>). We also considered an ensemble of neural networks (<xref ref-type="bibr" rid="bib15">Bileschi et al., 2022</xref>), where the average of the ensemble elements’ predicted probabilities is used as a confidence score, and a naive control, where the number of proteins annotated with a specific term in the training set plays this role (<xref ref-type="bibr" rid="bib58">Radivojac et al., 2013</xref>; see <xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7</xref>, <xref ref-type="fig" rid="fig3s8">Figure 3—figure supplement 8</xref>).</p><p>We found that BLASTp was able to achieve higher recall values than ProteInfer for lower precision values, while ProteInfer was able to provide greater precision than BLASTp at lower recall values. The high recall of BLAST is likely to reflect the fact that it has access to the entirety of the training set, rather than having to compress it into a limited set of neural network weights. In contrast, the lack of precision in BLAST could relate to reshuffling of sequences during evolution, which would allow a given protein to show high similarity to a trainining sequence in a particular subregion, despite lacking the core region required for that training sequence’s function. We wondered whether a combination of ProteInfer and BLASTp could synergise the best properties of both approaches. We found that even the simple ensembling strategy of rescaling the BLAST bit-score by the averages of the ensembled CNNs’ predicted probabilities gave a <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> score (0.991, 95% confidence interval [CI]: 0.990–0.992) that exceeded that of BLAST (0.984, 95% CI: 0.983–0.985) or the ensembled CNN (0.981, 95% CI: 0.980–0.982) alone (see ‘Materials and methods’ for more details on this method). On the clustered train-test split based on UniRef50 (see <italic>clustered</italic> in <xref ref-type="fig" rid="fig3">Figure 3B</xref>), we see a performance drop in all methods: this is expected, as remote homology tasks are designed to challenge methods to generalise farther in sequence space. The <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> score of a single neural network fell to 0.914 (95% CI: 0.913–0.915, precision: 0.959 recall: 0.875), substantially lower than BLAST (0.950, 95% CI: 0.950–0.951), though again an ensemble of both BLAST and ProteInfer outperformed both (0.979, 95% CI: 0.979–0.980). These patterns suggest that neural network methods learn different information about proteins to alignment-based methods, and so a combination of the two provides a synergistic result. All methods dramatically outperformed the naive frequency-based approach (<xref ref-type="fig" rid="fig3s9">Figure 3—figure supplement 9</xref>).</p><p>We also examined the relationship between the number of examples of a label in the training dataset and the performance of the model. In an image recognition task, this is an important consideration since one image of, say, a dog, can be utterly different to another. Large numbers of labels are therefore required to learn filters that are able to predict members of a class. In contrast, for sequence data we found that even for labels that occurred less than five times in the training set, 58% of examples in the test set were correctly recalled, while achieving a precision of 88%, for an F1 of 0.7 (<xref ref-type="fig" rid="fig3s12">Figure 3—figure supplement 12</xref>). High levels of performance are maintained with few training examples because of the evolutionary relationship between sequences, which means that one ortholog of a gene may be similar in sequence to another. The simple BLAST implementation described above also performs well, and better than a single neural network, likely again exploiting the fact that many sequence have close neighbours in sequence space with similar functions. We again find that ensembling the BLAST and ProteInfer outputs provides performance exceeding that of either technique used alone.</p></sec><sec id="s2-4"><title>Deep models link sequence regions to function</title><p>Proteins that use separate domains to carry out more than one enzymatic function are particularly useful in interpreting the behaviour of our model. For example, <italic>Saccharomyces cerevisiae</italic> fol1 (accession Q4LB35) catalyses three sequential steps of tetrahydrofolate synthesis using three different protein domains (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This protein is in our held-out test set, so no information about its labels was directly provided to the model.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Linking sequence regions to function with class activation mapping for C-1-tetrahydrofolate synthase (accession P11586).</title><p>(<bold>A</bold>) Ground-truth annotation of function on UniProt (<xref ref-type="bibr" rid="bib75">UniProt Consortium, 2019b</xref>). (<bold>B</bold>) The three horizontal bars are the sequence region ProteInfer predicts are most involved in each corresponding reaction. This concurs with the known function localisation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig4-v2.tif"/></fig><p>To investigate what sequence regions the neural network is using to make its functional predictions, we used class activation mapping (CAM) (<xref ref-type="bibr" rid="bib80">Zhou et al., 2015</xref>) to identify the sub-sequences responsible for the model predictions. We found that separate regions of sequence cause the prediction of each enzymatic activity, and that these regions correspond to the known functions of these regions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). This demonstrates that our network identifies relevant elements of a sequence in determining function.</p><p>We then assessed the ability of this method to more generally localise function within a sequence, even though the model was not trained with any explicit localisation information. We selected all enzymes from Swiss-Prot that had two separate leaf-node EC labels for which our model predicted known EC labels, and these labels were mappable to corresponding Pfam labels. For each of these proteins, we obtained coarse-grained functional localisation by using CAM to predict the order of the domains in the sequence and compared to the ground-truth Pfam domain ordering (see Methods). We found that in 296 of 304 (97%) of the cases, we correctly predicted the ordering, though we note that the set of bifunctional enzymes for which this analysis is applicable is limited in its functional diversity (see ‘Materials and methods’). Although we did not find that fine-grained, per-residue functional localisation arose from our application of CAM, we found that it reliably provided coarse-grained annotation of domains’ order, as supported by Pfam. This experiment suggests that this is a promising future area for research.</p></sec><sec id="s2-5"><title>Neural networks learn a general-purpose embedding space for protein function</title><p>Whereas InterProScan compares each sequence against more than 50,000 individual signatures and BLAST compares against an even larger sequence database, ProteInfer uses a single deep model to extract features from sequences that directly predict protein function. One convenient property of this approach is that in the penultimate layer of the network each protein is expressed as a single point in a high-dimensional space. To investigate to what extent this space is useful in examining enzymatic function, we used the ProteInfer EC model trained on the random split to embed each test set protein sequence into a 1100-dimensional vector. To visualise this space, we selected proteins with a single leaf-level EC number and used UMAP to compress their embeddings into two dimensions (<xref ref-type="bibr" rid="bib52">McInnes et al., 2018</xref>).</p><p>The resulting representation captures the hierarchical nature of EC classification, with the largest clusters in embedding space corresponding to top level EC groupings (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). These clusters in turn are further divided into sub-regions on the basis of subsequent levels of the EC hierarchy (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Exceptions to this rule generally recapitulate biological properties. For instance, <italic>Q8RUD6</italic> is annotated as Arsenate reductase (glutaredoxin) (EC:1.20.4.1) (<xref ref-type="bibr" rid="bib23">Chao et al., 2014</xref>) was not placed with other oxidoreductases (EC:1.-.-.-) but rather with sulfurtransferases (EC:2.8.1.-). <italic>Q8RUD6</italic> can, however, act as a sulfurtransferase (<xref ref-type="bibr" rid="bib12">Bartels et al., 2007</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Embedding reflects enzyme functional hierarchy.</title><p>UMAP projection of embeddings for the subset of test set sequences which have only one leaf-level Enzyme Commission (EC) classification. Points are colour-coded at successive levels of the EC hierarchy in each panel. (<bold>A</bold>) colours denote top level EC groups, (<bold>B</bold>) colours denote second level EC groups within EC2*, (<bold>C</bold>) colours denote third level EC groups within EC:2.7*, and (<bold>D</bold>) colours depict terminal EC groups within EC:2.7.4*.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig5-v2.tif"/></fig><p>Note that the model is directly trained with labels reflecting the EC hierarchy; the structure in <xref ref-type="fig" rid="fig5">Figure 5</xref> was not discovered automatically from the data. However, we can also ask whether the embedding captures more general protein characteristics, beyond those on which it was directly supervised. To investigate this, we took the subset of proteins in Swiss-Prot that are non-enzymes, and so lack any EC annotations. The network would achieve perfect accuracy on these examples if it, for example, mapped all of them to a single embedding that corresponds to zero predicted probability for every enzymatic label. Do these proteins therefore share the same representation in embedding space? The UMAP projection of these sequences’ embeddings revealed clear structure to the embedding space, which we visualised by highlighting several GO annotations which the network was never supervised on. For example, one region of the embedding space contained ribosomal proteins, while other regions could be identified containing nucleotide binding proteins, or membrane proteins (<xref ref-type="fig" rid="fig6">Figure 6</xref>). To quantitatively measure whether these embeddings capture the function of non-enzyme proteins, we trained a simple random forest classification model that used these embeddings to predict whether a protein was annotated with the <italic>intrinsic component of membrane</italic> GO term. We trained on a small set of non-enzymes containing 518 membrane proteins and evaluated on the rest of the examples. This simple model achieved a precision of 97% and recall of 60% for an F1 score of 0.74. Model training and data-labelling took around 15 s. This demonstrates the power of embeddings to simplify other studies with limited labelled data, as has been observed in recent work (<xref ref-type="bibr" rid="bib2">Alley et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Biswas et al., 2021</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>A neural network trained on enzyme function learns general protein properties, beyond enzymatic activity.</title><p>This figure shows Enzyme Commission (EC)-trained ProteInfer embeddings for all non-enzymatic sequences in the test set, projected using UMAP. To illustrate the structure contained in these embeddings, we highlight genes based on Gene Ontology (GO) labels (on which this network was never trained) - (<bold>a</bold>): Nucleotide binding, (<bold>b</bold>): Structural constituent of ribosome and (<bold>c</bold>): Intrinsic component of membrane .</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig6-v2.tif"/></fig></sec><sec id="s2-6"><title>Rapid client-side in-browser protein function prediction</title><p>Processing speed and ease of access are important considerations for the utility of biological software. An algorithm that takes hours or minutes is less useful than one that runs in seconds, both because of its increased computational cost, but also because it allows less immediate interactivity with a researcher. An ideal tool for protein function prediction would require minimal installation and would instantly answer a biologist’s question about protein function, allowing them to immediately act on the basis of this knowledge. Moreover, there may be intellectual property concerns in sending sequence data to remote servers, so a tool that does annotation completely client-side may also be preferable.</p><p>There is arguably room for improvement in this regard from classical approaches. For example, the online interface to InterProScan can take 147 s to process a 1500 amino acid sequence (Protein used was Q77Z83. It should be noted that the individual databases that make up InterProScan may return matches faster, with the online interface to Pfam taking 14–20 s for a 1500 amino acid sequence.), while running the tool may make the search faster, doing so requires downloading a 9 GB file, with an additional 14 GB for the full set of signatures, which when installed exceeds 51 GB. Meanwhile, conducting a BLAST search against Swiss-Prot takes 34 s for a 1500 amino acid sequence (a target database with decreased redundancy could be built to reduce this search time, and other optimisations of BLAST have been developed).</p><p>An attractive property of deep learning models is that they can be run efficiently using consumer graphics cards for acceleration. Indeed, recently, a framework has been developed to allow models developed in TensorFlow to be run locally using simply a user’s browser (<xref ref-type="bibr" rid="bib68">Smilkov et al., 2019</xref>), but to our knowledge this has never been deployed to investigate biological sequence data. We therefore built a tool to allow near-instantaneous prediction of protein functional properties in the browser. When the user loads the tool, lightweight EC (5 MB) and GO model (7 MB) prediction models are downloaded and all predictions are then performed locally, with query sequences never leaving the user’s computer. We selected the hyperparameters for these lightweight models by performing a tuning study in which we filtered results by the size of the model’s parameters and then selected the best performing models. This approach uses a single neural network rather than an ensemble. Inference in the browser for a 1500 amino acid sequence takes &lt;1.5 s for both models (see supplement).</p></sec><sec id="s2-7"><title>Comparison to experimental data</title><p>Despite its curated nature, SwissProt contains many proteins annotated only on the basis of electronic tools. To assess our model’s performance using an experimentally validated source of ground truth, we focused our attention on a large set of bacterial genes for which functions have recently been identified in a high-throughput experimental genetic study (<xref ref-type="bibr" rid="bib57">Price et al., 2018</xref>). In particular, this study listed newly identified EC numbers for 171 proteins, representing cases when there was previously either misannotation or inconsistent annotation in the SEED or KEGG databases. Therefore, this set of genes may be enriched for proteins whose functions are difficult to assess computationally. We examined how well our network was able to make predictions for this experimental dataset at each level of the EC hierarchy (<xref ref-type="fig" rid="fig7">Figure 7</xref>) using as a decision threshold the value that optimised F1 identified during tuning. The network had high accuracy for identification of broad enzyme class, with 90% accuracy at the top level of the EC hierarchy. To compute accuracy, we examined the subset of these 171 proteins for which there was a single enzymatic annotation from <xref ref-type="bibr" rid="bib57">Price et al., 2018</xref>, giving us predictions for 119 enzymes. At the second level of the hierarchy, accuracy was 90% and the network declined to make a prediction for 12% of classes. Even at the third level, accuracy was 86% with the network making a prediction in 77% of cases. At the finest level of classification, the proportion of examples for which a prediction was made fell to 28%, with 42% of these predictions correct.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>ProteInfer predictions for a set of genes recently experimentally reannotated by high-throughput phenotyping.</title><p>ProteInfer makes confident and largely accurate predictions at the earliest levels of the Enzyme Commission (EC) hierarchy. Accuracy falls at the finest levels of classification (for this set of challenging genes) but fortunately the network declines to make a prediction in most cases, with every label failing to meet the threshold for positive classification.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80942-fig7-v2.tif"/></fig><p>As an example, the <italic>Sinorhizobium meliloti</italic> protein Q92SI0 is annotated in UniProt as a Inosine-uridine nucleoside N-ribohydrolase (EC 3.2.2.1). Analysing the gene with InterProScan (<xref ref-type="bibr" rid="bib43">Jones et al., 2014</xref>) also gives this prediction, but our model instead predicts it to be a uridine nucleosidase (EC 3.2.2.3), and this was indeed the result found in this experimental work. Similarly, <italic>Pseudomonas fluorescens</italic> A0A166Q345 was correctly classified by our model as a D-galacturonate dehydrogenase (EC 1.1.1.203) as opposed to a misannotation on UniProt and with InterProScan.</p><p>It was notable that for many of these proteins the network declined to make a prediction at the finest level of the EC hierarchy. This suggests that by training on this hierarchical data, the network is able to appropriately make broad or narrow classification decisions. This is similar to the procedure employed with manual annotation: when annotators are confident of the general class of reaction that an enzyme catalyses but not its specific substrate, they may leave the third or fourth position of the EC number blank (e.g. EC:1.1.-.-). Due to training on hierarchical data, our network is able to reproduce these effects by being more confident (with higher accuracy) at earlier levels of classification.</p></sec><sec id="s2-8"><title>A model predicting the entire Gene Ontology</title><p>Given the high accuracy that our deep learning model was able to achieve on the more than 5000 enzymatic labels in Swiss-Prot, we asked whether our networks could learn to predict protein properties using an even larger vocabulary of labels, using a similar test-train setup. GO (<xref ref-type="bibr" rid="bib9">Ashburner et al., 2000</xref>; <xref ref-type="bibr" rid="bib24">Consortium, 2019</xref>; <xref ref-type="bibr" rid="bib21">Carbon et al., 2009</xref>) terms describe important protein functional properties, with 32,109 such terms in Swiss-Prot that cover the molecular functions of proteins (e.g. DNA-binding, amylase activity), the biological processes they are involved in (e.g. DNA replication, meiosis), and the cellular components to which they localise (e.g. mitochondrion, cytosol). These terms are arranged in a complex directed acyclic graph, with some nodes having as many as 12 ancestral nodes.</p><p>We note that there has been extensive work in GO label prediction evaluated on a temporally split dataset (constructing a test set with the most recently experimentally annotated proteins), for example, <xref ref-type="bibr" rid="bib81">Zhou et al., 2019</xref>, and stress that our comparison is based on the random and clustered splits of Swiss-Prot described above. This approach to splitting the data into train and test has advantages and disadvantages compared to a temporal split, which depend on the desired application for the method being evaluated.</p><p>We trained a single model to predict presence or absence for each of these terms and found that our network was able to achieve a precision of 0.918 and a recall of 0.854 for an F1 score of 0.885 (95% CI: 0.882–0.887).</p><p>An ensemble of multiple CNN elements was again able to achieve a slightly better result with an F1 score of 0.899 (95% CI: 0.897–0.901), which was exceeded by a simple transfer of the BLAST top pick at 0.902 (95% CI: 0.900–0.904), with an ensemble of both producing the best result of 0.908 (95% CI: 0.906–0.911).</p><p>The same trends for the relative performance of different approaches were seen for each of the direct-acyclic graphs that make up the GO (biological process, cellular component, and molecular function), but there were substantial differences in absolute performance (<xref ref-type="fig" rid="fig3s10">Figure 3—figure supplement 10</xref>). Performance was highest for molecular function (max F1: 0.94), followed by biological process (max F1:0.86) and then cellular component (max F1:0.84).</p><p>To benchmark against a common signature-based methodology, we used InterProScan to assign protein family signatures to each test sequence. We chose InterProScan for its coverage of labels as well as its use of multiple profile-based annotation methods, including HMMER and PROSITE, mentioned above. We note that while InterProScan predicts GO labels directly, it does not do so for EC labels, which is why we did not use InterProScan to benchmark our work on predicting EC labels. We found that InterProScan gave good precision, but within this UniProt data had lower recall, giving it a precision of 0.937 and recall of 0.543 for an F1 score of 0.688. ProteInfer’s recall at a precision of 0.937 is substantially higher (0.835) than InterProScan at assigning GO labels.</p><p>There are multiple caveats to these comparisons. One challenge is that the completeness of Swiss-Prot’s GO term annotations varies (<xref ref-type="bibr" rid="bib41">Jiang et al., 2014</xref>). As an extreme example, <italic>Pan paniscus</italic> (Pygmy Chimpanzee) and <italic>Pan troglodytes</italic> (Chimpanzee) have an identical Apolipoprotein A-II protein, (accessions P0DM95 and Q8MIQ5), where the first protein has 24 GO annotations, while the latter has 143 GO annotations (this count is done using not only the set of all labels that appear in Swiss-Prot, but also any parents of those labels). One way this is reflected in the performance of the models is that some BLAST matches that have extremely large bit-scores are not annotated identically, and thus reduce the precision of the BLAST model. It is also important to note that our model has the advantage of being specifically trained on the UniProt labelling schema upon which it is being evaluated. InterPro works quite differently, with GO terms being assigned to families, and so inconsistencies in terms of how these are assigned can explain reduced performance – for instance, InterPro families simply do not feature all of the GO terms found in UniProt. Thus these results should be seen as specific to the task of reproducing the curated results in UniProt.</p><p>We also tested how well our trained model was able to recall the subset of GO term annotations which are not associated with the ‘inferred from electronic annotation’ (IEA) evidence code, indicating either experimental work or more intensely curated evidence. We found that at the threshold that maximised F1 score for overall prediction, 75% of molecular function annotations could be successfully recalled, 61% of cellular component annotations, and 60% of biological process annotations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have shown that neural networks trained and evaluated on high-quality Swiss-Prot data accurately predict functional properties of proteins using only their raw, unaligned amino acid sequences. Further, our models make links between the regions of a protein and the function that they confer, produce predictions that agree with experimental characterisations, and place proteins into an embedding space that captures additional properties beyond those on which the models were directly trained. We have provided a convenient browser-based tool, where all computation runs locally on the user’s computer. To support follow-up research, we have also released our datasets, code for model training and evaluation, and a command-line version of the tool.</p><p>Using Swiss-Prot to benchmark our tool against traditional alignment-based methods has distinct advantages and disadvantages. It is desirable because the data has been carefully curated by experts, and thus it contains minimal false-positives. On the other hand, many entries come from experts applying existing computational methods, including BLAST and HMM-based approaches, to identify protein function. Therefore, the data may be enriched for sequences with functions that are easily ascribable using these techniques which could limit the ability to estimate the added value of using an alternative alignment-free tool. An idealised dataset would involved training only on those sequences that have themselves been experimentally characterised, but at present too little data exists than would be needed for a fully supervised deep-learning approach. Semi-supervised approaches that combine a smaller number of high-quality experimental labels with the vast set of amino acid sequences in TrEMBL may be a productive way forward.</p><p>Further, our work characterises proteins by assigning labels from a fixed, predefined set, but there are many proteins with functions that are not covered by this set. These categories of functions may not even be known to the scientific community yet. There is a large body of alternative work that identifies groups of related sequences (e.g. <xref ref-type="bibr" rid="bib47">Li et al., 2003</xref>), where a novel function could be discovered, for example, using follow-up experiments.</p><p>Finally, despite the successes of deep learning in many application domains, a number of troublesome behaviours have also been identified. For example, probabilities output by deep models are often over-confident, rather than well-calibrated (<xref ref-type="bibr" rid="bib35">Guo et al., 2017</xref>), and networks perform poorly on out-of-distribution data without being aware that they are outside their own range of expertise (<xref ref-type="bibr" rid="bib7">Amodei et al., 2016</xref>). Though these issues still need to be addressed and better understood by both the machine learning and bioinformatics communities, deep learning continues to make advances in a wide range of areas relating to the understanding protein function. We thus believe deep learning will have a central place in the future of this field.</p><p>Our code, data, and notebooks reproducing the analyses shown in this work are available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/proteinfer">https://github.com/google-research/proteinfer</ext-link> (<xref ref-type="bibr" rid="bib64">Sanderson et al., 2023</xref>) and <ext-link ext-link-type="uri" xlink:href="https://console.cloud.google.com/storage/browser/brain-genomics-public/research/proteins/proteinfer/datasets/">https://console.cloud.google.com/storage/browser/brain-genomics-public/research/proteins/proteinfer/datasets/</ext-link>.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Implementation details</title><sec id="s4-1-1"><title>Label inheritance</title><p>Our data processing pipeline takes as input UniProt XML entries and outputs training data for the neural network. Some types of annotations, such as GO terms and EC numbers, exist within directed acyclic graphs (which take the form of simple trees for EC numbers). Typically in such cases the annotation provided on UniProt is the most specific that is known. For example, if a protein is known to exhibit <italic>sequence-specific DNA binding</italic> (GO:0043565). It will not separately be annotated with the ancestral term <italic>DNA binding</italic> (GO:0003677); this is simply assumed from the ontology. Using such an annotation directly, however, is likely to be problematic in a deep learning setting. Failing to annotate an example with the parental term demands that the model predict that the example is negative for this term, which is not the effect we want. To address this, our datasets include labels for all ancestors of applied labels for EC, GO, and InterPro datasets. In the case of GO, we restrict these to <italic>is_a</italic> relationships.</p></sec><sec id="s4-1-2"><title>Class activation mapping</title><p>Many proteins have multiple functional properties. For example, we analyse the case of the bifunctional dhfr/ts of <italic>Toxoplasma gondii</italic>. Such bifunctional enzymes are often not unique – it is functionally advantageous for these enzymes to be fused, which facilitates channelling of substrate between their active sites. Since there are a number of such examples within Swiss-Prot, the mere existence of a TS domain in a protein is (mild) evidence for possible DHFR function. To increase the interpretability of the network, we normalise the results of class-activation mapping for proteins with multiple predicted functions. We initially calculate first-pass localisations for each predicted function using class-activation mapping. Then we make the localisation of each function more specific by taking the score for each residue and subtracting the scores at the same residues for all other functions. In the case of DHFR-TS, the TS activations in the TS domain are much greater than the DHFR activations in the TS domain and so this subtraction prevents the TS domain from being associated with DHFR function, increasing interpretability.</p></sec></sec><sec id="s4-2"><title>Model architecture</title><p>To create an architecture capable of receiving a wide range of input sequences, with computational requirements determined for each inference by the length of the individual input sequence, we employed a dilated convolutional approach (<xref ref-type="bibr" rid="bib79">Yu and Koltun, 2015</xref>). Computation for both training and prediction in such a model can be parallelised across the length of the sequence. By training on full-length proteins, in a multi-label training setting, we aimed to build networks that could extract functional information from raw amino acid sequences. One helpful feature of this architecture is its flexibility with regards to sequence length. Natural protein sequences can vary in length by at least three orders of magnitude, but some architectures have computational requirements that scale with the maximum sequence they are capable of receiving as input, rather than the sequence being currently examined. These fixed-length approaches reduce efficiency as well as place a hard limit on the length of sequences that can be examined.</p></sec><sec id="s4-3"><title>Hyperparameters</title><p>We tuned over batch size, dilation rate, filters, first dilated layer, kernel size, learning rate, number of layers, mean vs. max pooling, and Adam <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib44">Kingma and Ba, 2015</xref>) over a number of studies to determine the set of parameters that optimised <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">F</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We found, as in <xref ref-type="bibr" rid="bib15">Bileschi et al., 2022</xref>, that the network was relatively unresponsive to slight changes in hyperparameters and that many of the hyperparameters that performed well in <xref ref-type="bibr" rid="bib15">Bileschi et al., 2022</xref> also performed well for this task. We chose to keep identical hyperparameters for the EC and GO tasks across both the random and clustered splits for simplicity, and we note that parameters with good performance on the random task performed respectively well on the clustered split.</p></sec><sec id="s4-4"><title>Predicting coarse-grained functional localisation with CAM</title><p>The goal of this experimental methodology is to measure whether or not we correctly order the localisation of function in bifunctional enzymes. As such, first we have to identify a set of candidates for experimentation.</p><sec id="s4-4-1"><title>Candidate set construction</title><p>We note that no functional localisation information was available to our models during training, so we can consider not just the dev and test sets, but instead the entirety of Swiss-Prot for our experimentation. As such, we take all examples from Swiss-Prot that have an EC label and convert these labels to Pfam labels using a set of 1515 EC-Pfam manually curated label correlations from InterPro (<xref ref-type="bibr" rid="bib54">Mitchell et al., 2015</xref>), omitting unmapped labels. We then take the set of 3046 proteins where exactly two of their ground-truth labels map to corresponding Pfam labels. In our Swiss-Prot random test-train split test set, on bifunctional enzymes, we get 0.995 precision and 0.948 recall at a threshold of 0.5, so we believe this set is a reasonable test set for ordering analysis.</p><p>We then predict EC labels for these proteins with one of our trained convolutional neural network classifiers, considering only the most specific labels in the hierarchy. Then, we map these predicted EC labels to Pfam labels using the InterPro mapping again, and retain only the proteins on which we predict exactly two labels above a threshold of 0.5, and are left with 2679 proteins. In 2669 out of 2679 proteins, our predictions are identical to the Pfam-mapped ground-truth labels. We take these 2669 that have two true and predicted <italic>Pfam</italic> labels, and look at their current Pfam labels annotated in Swiss-Prot. Of these 2669 proteins, 304 of them contain both of the mapped labels. We note that this difference between 2669 and 304 is likely due in part to Pfam being conservative in calling family members, potential agreements at the Pfam clan vs. family level, as well as database version skew issues.</p></sec><sec id="s4-4-2"><title>Computation of domain ordering</title><p>On these 304 proteins, we have the same predicted-EC-to-Pfam labels and the same true-Pfam labels. For each of these proteins, we can get an ordering of their two enzymatic domains from Pfam, giving us a <italic>true</italic> ordering. It is now our task to produce a predicted ordering.</p><p>We use CAM to compute a confidence for each class at each residue for every protein in this set of 304. We then filter this large matrix of values and only consider the families for which our classifier predicted membership, giving us a matrix of shape sequence length by predicted classes (which is two in this case). For each class, we take the CAM output and compute a centre of mass. Then we order the two classes based on where their centre of mass lies. Further data is available in <xref ref-type="table" rid="table2">Table 2</xref>.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>In our random split of the training data, we allocate about 80% to the training fold, 10% to the development fold, and 10% to the test fold.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Fold</th><th align="center" valign="bottom">Number of sequences</th></tr></thead><tbody><tr><td align="left" valign="bottom">Train</td><td align="center" valign="bottom">438,522</td></tr><tr><td align="left" valign="bottom">Dev</td><td align="center" valign="bottom">55,453</td></tr><tr><td align="left" valign="bottom">Test</td><td align="center" valign="bottom">54,289</td></tr><tr><td align="left" valign="bottom">All together</td><td align="center" valign="bottom">548,264</td></tr></tbody></table></table-wrap><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>In our clustered split of the training data, we use UniRef50 and allocate approximately equal numbers of sequences to each fold.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Fold</th><th align="center" valign="bottom">Number of sequences</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">Train</td><td align="center" valign="bottom">182,965</td></tr><tr><td align="left" valign="bottom">Dev</td><td align="center" valign="bottom">180,309</td></tr><tr><td align="left" valign="bottom">Test</td><td align="center" valign="bottom">183,475</td></tr><tr><td align="left" valign="bottom">All together</td><td align="center" valign="bottom">546,749</td></tr></tbody></table></table-wrap><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Clustered dataset statistics for Enzyme Commission (EC) labels.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Type</th><th align="center" valign="bottom">Number</th></tr></thead><tbody><tr><td align="left" valign="bottom">Train labels</td><td align="center" valign="bottom">3411</td></tr><tr><td align="left" valign="bottom">Test labels</td><td align="center" valign="bottom">3414</td></tr><tr><td align="left" valign="bottom">Impossible test labels</td><td align="center" valign="bottom">1043</td></tr><tr><td align="left" valign="bottom">Train example-label pairs</td><td align="center" valign="bottom">348,105</td></tr><tr><td align="left" valign="bottom">Test example-label pairs</td><td align="center" valign="bottom">348,755</td></tr><tr><td align="left" valign="bottom">Impossible test example-label pairs</td><td align="center" valign="bottom">3415</td></tr></tbody></table></table-wrap><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Clustered dataset statistics for Gene Ontology (GO) labels.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Type</th><th align="center" valign="bottom">Number</th></tr></thead><tbody><tr><td align="left" valign="bottom">Train labels</td><td align="center" valign="bottom">26,538</td></tr><tr><td align="left" valign="bottom">Test labels</td><td align="center" valign="bottom">26,666</td></tr><tr><td align="left" valign="bottom">Impossible test labels</td><td align="center" valign="bottom">3739</td></tr><tr><td align="left" valign="bottom">Train example-label pairs</td><td align="center" valign="bottom">8,338,584</td></tr><tr><td align="left" valign="bottom">Test example-label pairs</td><td align="center" valign="bottom">8,424,299</td></tr><tr><td align="left" valign="bottom">Impossible test example-label pairs</td><td align="center" valign="bottom">11,137</td></tr></tbody></table></table-wrap><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Vocabulary sizes in models trained for Enzyme Commission (EC) and Gene Ontology (GO).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Vocabulary</th><th align="center" valign="bottom">Number of terms</th></tr></thead><tbody><tr><td align="left" valign="bottom">EC</td><td align="center" valign="bottom">5134</td></tr><tr><td align="left" valign="bottom">GO</td><td align="center" valign="bottom">32,109</td></tr></tbody></table></table-wrap><table-wrap id="table7" position="float"><label>Table 7.</label><caption><title>In Swiss-Prot, there are 16 candidate domain architectures available for our Enzyme Commission (EC) functional localisation experiment.</title><p>Among these, all domain architectures with more than three instances in Swiss-Prot (seven of them) are 100% correctly ordered by our class activation mapping (CAM) method.</p><p>Domain architecture diversity in bifunctional enzymes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">First domain</th><th align="left" valign="bottom">Second domain</th><th align="center" valign="bottom">Number ordered correctly</th><th align="center" valign="bottom">Number times seen</th><th align="center" valign="bottom">Percent correct</th></tr></thead><tbody><tr><td align="left" valign="bottom">EC:2.7.7.60</td><td align="left" valign="bottom">EC:4.6.1.12</td><td align="center" valign="bottom">94</td><td align="center" valign="bottom">94</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:4.1.99.12</td><td align="left" valign="bottom">EC:3.5.4.25</td><td align="center" valign="bottom">83</td><td align="center" valign="bottom">83</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:3.5.4.19</td><td align="left" valign="bottom">EC:3.6.1.31</td><td align="center" valign="bottom">59</td><td align="center" valign="bottom">59</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:1.8.4.11</td><td align="left" valign="bottom">EC:1.8.4.12</td><td align="center" valign="bottom">20</td><td align="center" valign="bottom">20</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:4.1.1.48</td><td align="left" valign="bottom">EC:5.3.1.24</td><td align="center" valign="bottom">18</td><td align="center" valign="bottom">18</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:5.4.99.5</td><td align="left" valign="bottom">EC:4.2.1.51</td><td align="center" valign="bottom">12</td><td align="center" valign="bottom">12</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:5.4.99.5</td><td align="left" valign="bottom">EC:1.3.1.12</td><td align="center" valign="bottom">4</td><td align="center" valign="bottom">4</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:4.2.1.10</td><td align="left" valign="bottom">EC:1.1.1.25</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:2.7.7.61</td><td align="left" valign="bottom">EC:2.4.2.52</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">EC:2.7.1.71</td><td align="left" valign="bottom">EC:4.2.3.4</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">2</td><td align="center" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">EC:1.1.1.25</td><td align="left" valign="bottom">EC:4.2.1.10</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">EC:2.7.2.3</td><td align="left" valign="bottom">EC:5.3.1.1</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:4.1.1.97</td><td align="left" valign="bottom">EC:1.7.3.3</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:4.1.3.1</td><td align="left" valign="bottom">EC:2.3.3.9</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">EC:5.1.99.6</td><td align="left" valign="bottom">EC:1.4.3.5</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">EC:1.8.4.12</td><td align="left" valign="bottom">EC:1.8.4.11</td><td align="center" valign="bottom">0</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">0</td></tr></tbody></table></table-wrap></sec></sec><sec id="s4-5"><title>Input data statistics</title><p>We use <ext-link ext-link-type="uri" xlink:href="https://ftp.uniprot.org/pub/databases/uniprot/previous_major_releases/release-2019_01/">Swiss-Prot version 2019_01</ext-link> in our analysis, which gives us 559,077 proteins, or 548,264 after filtering for only 20 standard amino acids and filtering fragments (<xref ref-type="table" rid="table3">Table 3</xref>). Because different protein functions have differing prevalence, we note the number of proteins that have a given function for Pfam, EC, and GO labels, as well as noting the number of labels per protein. Further statistics on dataset size for different data splits are provided in <xref ref-type="table" rid="table2 table4 table5 table6 table7">Tables 2, 4–7</xref>.</p><p>When assigning examples to folds in our clustered dataset, we note that there are test examples that have labels that are never seen in the training data. We report these cases below as ‘Impossible’ test example-label pairs (<xref ref-type="table" rid="table8">Table 8</xref>).</p><table-wrap id="table8" position="float"><label>Table 8.</label><caption><title>Clustered dataset statistics for EC labels.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Type</th><th align="center" valign="bottom">Number</th></tr></thead><tbody><tr><td align="left" valign="bottom">Train labels</td><td align="center" valign="bottom">3411</td></tr><tr><td align="left" valign="bottom">Test labels</td><td align="center" valign="bottom">3414</td></tr><tr><td align="left" valign="bottom">Impossible test labels</td><td align="center" valign="bottom">1043</td></tr><tr><td align="left" valign="bottom">Train example-label pairs</td><td align="center" valign="bottom">348,105</td></tr><tr><td align="left" valign="bottom">Test example-label pairs</td><td align="center" valign="bottom">348,755</td></tr><tr><td align="left" valign="bottom">Impossible test example-label pairs</td><td align="center" valign="bottom">3415</td></tr></tbody></table></table-wrap><sec id="s4-5-1"><title>Timing ProteInfer browser models</title><p>We timed the performance of the ProteInfer model by running the code found at <ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/proteinfer/blob/gh-pages/latex/timing_code.js">https://github.com/google-research/proteinfer/blob/gh-pages/latex/timing_code.js</ext-link> in the browser console.</p></sec><sec id="s4-5-2"><title>Combining CNN and BLAST</title><p>We noticed a clear difference between the performance of CNN models and BLAST models. The CNN models were able make predictions with higher precision than BLAST for a given recall value, but achieved lower overall recall, while BLAST gave high recall at the expense of precision. This suggested that combining the two methods would be beneficial. A CNN, or an ensemble of CNNs, produces a metric that is notionally a probability (though often imperfectly calibrated), while BLAST bit-score produces a bit-score metric indicating the significance of the match. We reasoned that one approach to combining the two would simply be to multiply the values together – improving the precision of BLAST predictions by reducing bit-scores in cases where the CNN model lacked confidence in a prediction. This approach performed well and was the best that we evaluated. To implement this method, all labels associated with the top hit from BLAST with a sequence are initially assigned identical scores, determined by the bit-score of the top match, but these are then rescaled according to the output of the ProteInfer command-line interface.</p><p>Whether this approach is worth the additional infrastructure required to maintain two different methods of prediction will depend on the scale of analysis being conducted. Further sophistication in how the CNN and BLAST are combined, perhaps with a learnt model, might further improve performance.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>performed research as part of their employment at Google LLC. Google is a technology company that sells machine learning services as part of its business. Portions of this work are covered by US patent WO2020210591A1, filed by Google</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Validation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Investigation, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80942-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Source code is available on GitHub from <ext-link ext-link-type="uri" xlink:href="https://github.com/google-research/proteinfer">https://github.com/google-research/proteinfer</ext-link> (copy archived at <xref ref-type="bibr" rid="bib64">Sanderson et al., 2023</xref>). Processed TensorFlow files are available from the indicated URLs. Raw training data is from UniProt.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Babak Alipanahi, Jamie Smith, Eli Bixby, Drew Bryant, Shanqing Cai, Cory McLean, and Abhinay Ramaprasad. The static version of this manuscript uses a template made by Ricardo Henriques. TS receives funding from the Wellcome Trust through a Sir Henry Wellcome Postdoctoral Fellowship (210918/Z/18/Z). LJC receives funding from the Simons Foundation (Award 598399). This work was also supported by the Francis Crick Institute which receives its core funding from Cancer Research UK (FC001043), the UK Medical Research Council (FC001043), and the Wellcome Trust (FC001043). This research was funded in whole, or in part, by the Wellcome Trust (FC001043). For the purpose of Open Access, the authors have applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: A system for large-scale machine learning</article-title><conf-name>In 12th USENIX symposium on operating systems design and implementation OSDI</conf-name><elocation-id>265</elocation-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alley</surname><given-names>EC</given-names></name><name><surname>Khimulya</surname><given-names>G</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>AlQuraishi</surname><given-names>M</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title><source>Nature Methods</source><volume>16</volume><fpage>1315</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id><pub-id pub-id-type="pmid">31636460</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almagro Armenteros</surname><given-names>JJ</given-names></name><name><surname>Sønderby</surname><given-names>CK</given-names></name><name><surname>Sønderby</surname><given-names>SK</given-names></name><name><surname>Nielsen</surname><given-names>H</given-names></name><name><surname>Winther</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DeepLoc: prediction of protein subcellular localization using deep learning</article-title><source>Bioinformatics</source><volume>33</volume><fpage>3387</fpage><lpage>3395</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx431</pub-id><pub-id pub-id-type="pmid">29036616</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>AlQuraishi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>End-To-End differentiable learning of protein structure</article-title><source>Cell Systems</source><volume>8</volume><fpage>292</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2019.03.006</pub-id><pub-id pub-id-type="pmid">31005579</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><name><surname>Gish</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Myers</surname><given-names>EW</given-names></name><name><surname>Lipman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Basic local alignment search tool</article-title><source>Journal of Molecular Biology</source><volume>215</volume><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/S0022-2836(05)80360-2</pub-id><pub-id pub-id-type="pmid">2231712</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><name><surname>Madden</surname><given-names>TL</given-names></name><name><surname>Schäffer</surname><given-names>AA</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Lipman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Gapped blast and psi-blast: a new generation of protein database search programs</article-title><source>Nucleic Acids Research</source><volume>25</volume><fpage>3389</fpage><lpage>3402</lpage><pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id><pub-id pub-id-type="pmid">9254694</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Steinhardt</surname><given-names>J</given-names></name><name><surname>Christiano</surname><given-names>PF</given-names></name><name><surname>Schulman</surname><given-names>J</given-names></name><name><surname>Mané</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Concrete Problems in AI Safety</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1606.06565</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Pellock</surname><given-names>SJ</given-names></name><name><surname>Chidyausiku</surname><given-names>TM</given-names></name><name><surname>Ramelot</surname><given-names>TA</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Hao</surname><given-names>J</given-names></name><name><surname>Bafna</surname><given-names>K</given-names></name><name><surname>Norn</surname><given-names>C</given-names></name><name><surname>Kang</surname><given-names>A</given-names></name><name><surname>Bera</surname><given-names>AK</given-names></name><name><surname>DiMaio</surname><given-names>F</given-names></name><name><surname>Carter</surname><given-names>L</given-names></name><name><surname>Chow</surname><given-names>CM</given-names></name><name><surname>Montelione</surname><given-names>GT</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>De novo protein design by deep network hallucination</article-title><source>Nature</source><volume>600</volume><fpage>547</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04184-w</pub-id><pub-id pub-id-type="pmid">34853475</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>M</given-names></name><name><surname>Ball</surname><given-names>CA</given-names></name><name><surname>Blake</surname><given-names>JA</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Cherry</surname><given-names>JM</given-names></name><name><surname>Davis</surname><given-names>AP</given-names></name><name><surname>Dolinski</surname><given-names>K</given-names></name><name><surname>Dwight</surname><given-names>SS</given-names></name><name><surname>Eppig</surname><given-names>JT</given-names></name><name><surname>Harris</surname><given-names>MA</given-names></name><name><surname>Hill</surname><given-names>DP</given-names></name><name><surname>Issel-Tarver</surname><given-names>L</given-names></name><name><surname>Kasarskis</surname><given-names>A</given-names></name><name><surname>Lewis</surname><given-names>S</given-names></name><name><surname>Matese</surname><given-names>JC</given-names></name><name><surname>Richardson</surname><given-names>JE</given-names></name><name><surname>Ringwald</surname><given-names>M</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Sherlock</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Gene ontology: tool for the unification of biology</article-title><source>Nature Genetics</source><volume>25</volume><fpage>25</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1038/75556</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwood</surname><given-names>TK</given-names></name><name><surname>Bradley</surname><given-names>P</given-names></name><name><surname>Flower</surname><given-names>DR</given-names></name><name><surname>Gaulton</surname><given-names>A</given-names></name><name><surname>Maudling</surname><given-names>N</given-names></name><name><surname>Mitchell</surname><given-names>AL</given-names></name><name><surname>Moulton</surname><given-names>G</given-names></name><name><surname>Nordle</surname><given-names>A</given-names></name><name><surname>Paine</surname><given-names>K</given-names></name><name><surname>Taylor</surname><given-names>P</given-names></name><name><surname>Uddin</surname><given-names>A</given-names></name><name><surname>Zygouri</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Prints and its automatic supplement, preprints</article-title><source>Nucleic Acids Research</source><volume>31</volume><fpage>400</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1093/nar/gkg030</pub-id><pub-id pub-id-type="pmid">12520033</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bairoch</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Prosite: a dictionary of sites and patterns in proteins</article-title><source>Nucleic Acids Research</source><volume>19</volume><fpage>2241</fpage><lpage>2245</lpage><pub-id pub-id-type="doi">10.1093/nar/19.suppl.2241</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartels</surname><given-names>A</given-names></name><name><surname>Mock</surname><given-names>HP</given-names></name><name><surname>Papenbrock</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differential expression of Arabidopsis sulfurtransferases under various growth conditions</article-title><source>Plant Physiology and Biochemistry</source><volume>45</volume><fpage>178</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.plaphy.2007.02.005</pub-id><pub-id pub-id-type="pmid">17408957</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Mistry</surname><given-names>J</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name><name><surname>Luciani</surname><given-names>A</given-names></name><name><surname>Potter</surname><given-names>SC</given-names></name><name><surname>Qureshi</surname><given-names>M</given-names></name><name><surname>Richardson</surname><given-names>LJ</given-names></name><name><surname>Salazar</surname><given-names>GA</given-names></name><name><surname>Smart</surname><given-names>A</given-names></name><name><surname>Sonnhammer</surname><given-names>ELL</given-names></name><name><surname>Hirsh</surname><given-names>L</given-names></name><name><surname>Paladin</surname><given-names>L</given-names></name><name><surname>Piovesan</surname><given-names>D</given-names></name><name><surname>Tosatto</surname><given-names>SCE</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The pfam protein families database in 2019</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D427</fpage><lpage>D432</lpage><pub-id pub-id-type="doi">10.1093/nar/gky995</pub-id><pub-id pub-id-type="pmid">30357350</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernhofer</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Karl</surname><given-names>T</given-names></name><name><surname>Satagopam</surname><given-names>V</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Littmann</surname><given-names>M</given-names></name><name><surname>Olenyi</surname><given-names>T</given-names></name><name><surname>Qiu</surname><given-names>J</given-names></name><name><surname>Schütze</surname><given-names>K</given-names></name><name><surname>Yachdav</surname><given-names>G</given-names></name><name><surname>Ashkenazy</surname><given-names>H</given-names></name><name><surname>Ben-Tal</surname><given-names>N</given-names></name><name><surname>Bromberg</surname><given-names>Y</given-names></name><name><surname>Goldberg</surname><given-names>T</given-names></name><name><surname>Kajan</surname><given-names>L</given-names></name><name><surname>O’Donoghue</surname><given-names>S</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Schafferhans</surname><given-names>A</given-names></name><name><surname>Schlessinger</surname><given-names>A</given-names></name><name><surname>Vriend</surname><given-names>G</given-names></name><name><surname>Mirdita</surname><given-names>M</given-names></name><name><surname>Gawron</surname><given-names>P</given-names></name><name><surname>Gu</surname><given-names>W</given-names></name><name><surname>Jarosz</surname><given-names>Y</given-names></name><name><surname>Trefois</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Schneider</surname><given-names>R</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>PredictProtein - predicting protein structure and function for 29 years</article-title><source>Nucleic Acids Research</source><volume>49</volume><fpage>W535</fpage><lpage>W540</lpage><pub-id pub-id-type="doi">10.1093/nar/gkab354</pub-id><pub-id pub-id-type="pmid">33999203</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bileschi</surname><given-names>ML</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>Bryant</surname><given-names>DH</given-names></name><name><surname>Sanderson</surname><given-names>T</given-names></name><name><surname>Carter</surname><given-names>B</given-names></name><name><surname>Sculley</surname><given-names>D</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>DePristo</surname><given-names>MA</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Using deep learning to annotate the protein universe</article-title><source>Nature Biotechnology</source><volume>40</volume><fpage>932</fpage><lpage>937</lpage><pub-id pub-id-type="doi">10.1038/s41587-021-01179-w</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>Khimulya</surname><given-names>G</given-names></name><name><surname>Alley</surname><given-names>EC</given-names></name><name><surname>Esvelt</surname><given-names>KM</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Low-N protein engineering with data-efficient deep learning</article-title><source>Nature Methods</source><volume>18</volume><fpage>389</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01100-y</pub-id><pub-id pub-id-type="pmid">33828272</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>M</given-names></name><name><surname>Chang</surname><given-names>H-Y</given-names></name><name><surname>Chuguransky</surname><given-names>S</given-names></name><name><surname>Grego</surname><given-names>T</given-names></name><name><surname>Kandasaamy</surname><given-names>S</given-names></name><name><surname>Mitchell</surname><given-names>A</given-names></name><name><surname>Nuka</surname><given-names>G</given-names></name><name><surname>Paysan-Lafosse</surname><given-names>T</given-names></name><name><surname>Qureshi</surname><given-names>M</given-names></name><name><surname>Raj</surname><given-names>S</given-names></name><name><surname>Richardson</surname><given-names>L</given-names></name><name><surname>Salazar</surname><given-names>GA</given-names></name><name><surname>Williams</surname><given-names>L</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><name><surname>Bridge</surname><given-names>A</given-names></name><name><surname>Gough</surname><given-names>J</given-names></name><name><surname>Haft</surname><given-names>DH</given-names></name><name><surname>Letunic</surname><given-names>I</given-names></name><name><surname>Marchler-Bauer</surname><given-names>A</given-names></name><name><surname>Mi</surname><given-names>H</given-names></name><name><surname>Natale</surname><given-names>DA</given-names></name><name><surname>Necci</surname><given-names>M</given-names></name><name><surname>Orengo</surname><given-names>CA</given-names></name><name><surname>Pandurangan</surname><given-names>AP</given-names></name><name><surname>Rivoire</surname><given-names>C</given-names></name><name><surname>Sigrist</surname><given-names>CJA</given-names></name><name><surname>Sillitoe</surname><given-names>I</given-names></name><name><surname>Thanki</surname><given-names>N</given-names></name><name><surname>Thomas</surname><given-names>PD</given-names></name><name><surname>Tosatto</surname><given-names>SCE</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The interpro protein families and domains database: 20 years on</article-title><source>Nucleic Acids Research</source><volume>49</volume><fpage>D344</fpage><lpage>D354</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa977</pub-id><pub-id pub-id-type="pmid">33156333</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Peleg</surname><given-names>Y</given-names></name><name><surname>Rappoport</surname><given-names>N</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>ProteinBERT: a universal deep-learning model of protein sequence and function</article-title><source>Bioinformatics</source><volume>38</volume><fpage>2102</fpage><lpage>2110</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btac020</pub-id><pub-id pub-id-type="pmid">35020807</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bryant</surname><given-names>DH</given-names></name><name><surname>Bashir</surname><given-names>A</given-names></name><name><surname>Sinai</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>NK</given-names></name><name><surname>Ogden</surname><given-names>PJ</given-names></name><name><surname>Riley</surname><given-names>PF</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name><name><surname>Kelsic</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep diversification of an AAV capsid protein by machine learning</article-title><source>Nature Biotechnology</source><volume>39</volume><fpage>691</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1038/s41587-020-00793-4</pub-id><pub-id pub-id-type="pmid">33574611</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>R</given-names></name><name><surname>Freitas</surname><given-names>C</given-names></name><name><surname>Chan</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>M</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>ProLanGO: protein function prediction using neural machine translation based on a recurrent neural network</article-title><source>Molecules</source><volume>22</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3390/molecules22101732</pub-id><pub-id pub-id-type="pmid">29039790</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carbon</surname><given-names>S</given-names></name><name><surname>Ireland</surname><given-names>A</given-names></name><name><surname>Mungall</surname><given-names>CJ</given-names></name><name><surname>Shu</surname><given-names>S</given-names></name><name><surname>Marshall</surname><given-names>B</given-names></name><name><surname>Lewis</surname><given-names>S</given-names></name><name><surname>Hub</surname><given-names>A</given-names></name><name><surname>Group</surname><given-names>WPW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>AmiGO: online access to ontology and annotation data</article-title><source>Bioinformatics</source><volume>25</volume><fpage>288</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btn615</pub-id><pub-id pub-id-type="pmid">19033274</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>S</given-names></name><name><surname>Armstrong</surname><given-names>Z</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name><name><surname>Johnson</surname><given-names>I</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Activation atlas</article-title><source>Distill</source><volume>4</volume><elocation-id>e15</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00015</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>DY</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Shi</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Danku</surname><given-names>JM</given-names></name><name><surname>Zhao</surname><given-names>FJ</given-names></name><name><surname>Salt</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Genome-wide association mapping identifies a new arsenate reductase enzyme critical for limiting arsenic accumulation in plants</article-title><source>PLOS Biology</source><volume>12</volume><elocation-id>e1002009</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002009</pub-id><pub-id pub-id-type="pmid">25464340</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Consortium</surname><given-names>GO</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The gene ontology resource: 20 years and still going strong</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D330</fpage><lpage>D338</lpage><pub-id pub-id-type="doi">10.1093/nar/gky1055</pub-id><pub-id pub-id-type="pmid">30395331</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalkiran</surname><given-names>A</given-names></name><name><surname>Rifaioglu</surname><given-names>AS</given-names></name><name><surname>Martin</surname><given-names>MJ</given-names></name><name><surname>Cetin-Atalay</surname><given-names>R</given-names></name><name><surname>Atalay</surname><given-names>V</given-names></name><name><surname>Doğan</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>ECPred: a tool for the prediction of the enzymatic functions of protein sequences based on the EC nomenclature</article-title><source>BMC Bioinformatics</source><volume>19</volume><elocation-id>334</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-018-2368-y</pub-id><pub-id pub-id-type="pmid">30241466</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dohan</surname><given-names>D</given-names></name><name><surname>Gane</surname><given-names>A</given-names></name><name><surname>Bileschi</surname><given-names>ML</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>Colwell</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Improving Protein Function Annotation via Unsupervised Pre-training: Robustness, Efficiency, and Insights</article-title><conf-name>KDD ’21</conf-name><conf-loc>Virtual Event Singapore</conf-loc><pub-id pub-id-type="doi">10.1145/3447548.3467163</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Energy-based models for atomic-resolution protein conformations</article-title><conf-name>In International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eddy</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Profile hidden markov models</article-title><source>Bioinformatics</source><volume>14</volume><fpage>755</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/14.9.755</pub-id><pub-id pub-id-type="pmid">9918945</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eddy</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Accelerated profile HMM searches</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002195</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002195</pub-id><pub-id pub-id-type="pmid">22039361</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>El-Gebali</surname><given-names>S</given-names></name><name><surname>Richardson</surname><given-names>L</given-names></name><name><surname>Finn</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Repeats in pfam</article-title><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/training/online/courses/repeats-in-pfam/">https://www.ebi.ac.uk/training/online/courses/repeats-in-pfam/</ext-link><date-in-citation iso-8601-date="2022-03-01">March 1, 2022</date-in-citation></element-citation></ref><ref id="bib31"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>El-Gebali</surname><given-names>S</given-names></name><name><surname>Richardson</surname><given-names>L</given-names></name><name><surname>Finn</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Creating protein families</article-title><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/training/online/courses/pfam-creating-protein-families/">https://www.ebi.ac.uk/training/online/courses/pfam-creating-protein-families/</ext-link><date-in-citation iso-8601-date="2022-03-01">March 1, 2022</date-in-citation></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Rehawi</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gibbs</surname><given-names>T</given-names></name><name><surname>Feher</surname><given-names>T</given-names></name><name><surname>Angerer</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Bhowmik</surname><given-names>D</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.1101/2020.07.12.199554</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazer</surname><given-names>J</given-names></name><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Dias</surname><given-names>M</given-names></name><name><surname>Gomez</surname><given-names>A</given-names></name><name><surname>Min</surname><given-names>JK</given-names></name><name><surname>Brock</surname><given-names>K</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Disease variant prediction with deep generative models of evolutionary data</article-title><source>Nature</source><volume>599</volume><fpage>91</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04043-8</pub-id><pub-id pub-id-type="pmid">34707284</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillis</surname><given-names>J</given-names></name><name><surname>Pavlidis</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Characterizing the state of the art in the computational assignment of gene function: lessons from the first critical assessment of functional annotation (cafA)</article-title><source>BMC Bioinformatics</source><volume>14 Suppl 3</volume><elocation-id>S15</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2105-14-s3-s15</pub-id><pub-id pub-id-type="pmid">23630983</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Pleiss</surname><given-names>G</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>On Calibration of Modern Neural Networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1706.04599</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haft</surname><given-names>DH</given-names></name><name><surname>Selengut</surname><given-names>JD</given-names></name><name><surname>Richter</surname><given-names>RA</given-names></name><name><surname>Harkins</surname><given-names>D</given-names></name><name><surname>Basu</surname><given-names>MK</given-names></name><name><surname>Beck</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>TIGRFAMs and genome properties in 2013</article-title><source>Nucleic Acids Research</source><volume>41</volume><fpage>D387</fpage><lpage>D395</lpage><pub-id pub-id-type="doi">10.1093/nar/gks1234</pub-id><pub-id pub-id-type="pmid">23197656</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>J</given-names></name><name><surname>Adhikari</surname><given-names>B</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepSF: deep convolutional neural network for mapping protein sequences to folds</article-title><source>Bioinformatics</source><volume>34</volume><fpage>1295</fpage><lpage>1303</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx780</pub-id><pub-id pub-id-type="pmid">29228193</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>S</given-names></name><name><surname>Apweiler</surname><given-names>R</given-names></name><name><surname>Attwood</surname><given-names>TK</given-names></name><name><surname>Bairoch</surname><given-names>A</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Binns</surname><given-names>D</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><name><surname>Das</surname><given-names>U</given-names></name><name><surname>Daugherty</surname><given-names>L</given-names></name><name><surname>Duquenne</surname><given-names>L</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name><name><surname>Gough</surname><given-names>J</given-names></name><name><surname>Haft</surname><given-names>D</given-names></name><name><surname>Hulo</surname><given-names>N</given-names></name><name><surname>Kahn</surname><given-names>D</given-names></name><name><surname>Kelly</surname><given-names>E</given-names></name><name><surname>Laugraud</surname><given-names>A</given-names></name><name><surname>Letunic</surname><given-names>I</given-names></name><name><surname>Lonsdale</surname><given-names>D</given-names></name><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>Madera</surname><given-names>M</given-names></name><name><surname>Maslen</surname><given-names>J</given-names></name><name><surname>McAnulla</surname><given-names>C</given-names></name><name><surname>McDowall</surname><given-names>J</given-names></name><name><surname>Mistry</surname><given-names>J</given-names></name><name><surname>Mitchell</surname><given-names>A</given-names></name><name><surname>Mulder</surname><given-names>N</given-names></name><name><surname>Natale</surname><given-names>D</given-names></name><name><surname>Orengo</surname><given-names>C</given-names></name><name><surname>Quinn</surname><given-names>AF</given-names></name><name><surname>Selengut</surname><given-names>JD</given-names></name><name><surname>Sigrist</surname><given-names>CJA</given-names></name><name><surname>Thimma</surname><given-names>M</given-names></name><name><surname>Thomas</surname><given-names>PD</given-names></name><name><surname>Valentin</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>D</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name><name><surname>Yeats</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>InterPro: the integrative protein signature database</article-title><source>Nucleic Acids Research</source><volume>37</volume><fpage>D211</fpage><lpage>D215</lpage><pub-id pub-id-type="doi">10.1093/nar/gkn785</pub-id><pub-id pub-id-type="pmid">18940856</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jeffrey</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Large scale distributed deep networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1223</fpage><lpage>1231</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Clark</surname><given-names>WT</given-names></name><name><surname>Friedberg</surname><given-names>I</given-names></name><name><surname>Radivojac</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The impact of incomplete knowledge on the evaluation of protein function prediction: a structured-output learning perspective</article-title><source>Bioinformatics</source><volume>30</volume><fpage>i609</fpage><lpage>i616</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu472</pub-id><pub-id pub-id-type="pmid">25161254</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>M</given-names></name><name><surname>Zaretskaya</surname><given-names>I</given-names></name><name><surname>Raytselis</surname><given-names>Y</given-names></name><name><surname>Merezhuk</surname><given-names>Y</given-names></name><name><surname>McGinnis</surname><given-names>S</given-names></name><name><surname>Madden</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Ncbi blast: a better web interface</article-title><source>Nucleic Acids Research</source><volume>36</volume><fpage>W5</fpage><lpage>W9</lpage><pub-id pub-id-type="doi">10.1093/nar/gkn201</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>P</given-names></name><name><surname>Binns</surname><given-names>D</given-names></name><name><surname>Chang</surname><given-names>H-Y</given-names></name><name><surname>Fraser</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>McAnulla</surname><given-names>C</given-names></name><name><surname>McWilliam</surname><given-names>H</given-names></name><name><surname>Maslen</surname><given-names>J</given-names></name><name><surname>Mitchell</surname><given-names>A</given-names></name><name><surname>Nuka</surname><given-names>G</given-names></name><name><surname>Pesseat</surname><given-names>S</given-names></name><name><surname>Quinn</surname><given-names>AF</given-names></name><name><surname>Sangrador-Vegas</surname><given-names>A</given-names></name><name><surname>Scheremetjew</surname><given-names>M</given-names></name><name><surname>Yong</surname><given-names>S-Y</given-names></name><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>Hunter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>InterProScan 5: genome-scale protein function classification</article-title><source>Bioinformatics</source><volume>30</volume><fpage>1236</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu031</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: A method for stochastic optimization</article-title><conf-name>The International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krogh</surname><given-names>A</given-names></name><name><surname>Brown</surname><given-names>M</given-names></name><name><surname>Mian</surname><given-names>IS</given-names></name><name><surname>Sjölander</surname><given-names>K</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Hidden markov models in computational biology applications to protein modeling</article-title><source>Journal of Molecular Biology</source><volume>235</volume><fpage>1501</fpage><lpage>1531</lpage><pub-id pub-id-type="doi">10.1006/jmbi.1994.1104</pub-id><pub-id pub-id-type="pmid">8107089</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kulmanov</surname><given-names>M</given-names></name><name><surname>Khan</surname><given-names>MA</given-names></name><name><surname>Hoehndorf</surname><given-names>R</given-names></name><name><surname>Wren</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title><source>Bioinformatics</source><volume>34</volume><fpage>660</fpage><lpage>668</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx624</pub-id><pub-id pub-id-type="pmid">29028931</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Stoeckert</surname><given-names>CJ</given-names></name><name><surname>Roos</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>OrthoMCL: identification of ortholog groups for eukaryotic genomes</article-title><source>Genome Research</source><volume>13</volume><fpage>2178</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1101/gr.1224503</pub-id><pub-id pub-id-type="pmid">12952885</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Umarov</surname><given-names>R</given-names></name><name><surname>Xie</surname><given-names>B</given-names></name><name><surname>Fan</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DEEPre: sequence-based enzyme EC number prediction by deep learning</article-title><source>Bioinformatics</source><volume>34</volume><fpage>760</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx680</pub-id><pub-id pub-id-type="pmid">29069344</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Littmann</surname><given-names>M</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Olenyi</surname><given-names>T</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Embeddings from deep learning transfer go annotations beyond homology</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>1160</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-80786-0</pub-id><pub-id pub-id-type="pmid">33441905</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Madani</surname><given-names>A</given-names></name><name><surname>McCann</surname><given-names>B</given-names></name><name><surname>Naik</surname><given-names>N</given-names></name><name><surname>Keskar</surname><given-names>NS</given-names></name><name><surname>Anand</surname><given-names>N</given-names></name><name><surname>Eguchi</surname><given-names>RR</given-names></name><name><surname>Huang</surname><given-names>PS</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ProGen: Language Modeling for Protein Generation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.07.982272</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazurenko</surname><given-names>S</given-names></name><name><surname>Prokop</surname><given-names>Z</given-names></name><name><surname>Damborsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Machine learning in enzyme engineering</article-title><source>ACS Catalysis</source><volume>10</volume><fpage>1210</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1021/acscatal.9b04321</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Saul</surname><given-names>N</given-names></name><name><surname>Großberger</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>UMAP: uniform manifold approximation and projection</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>861</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mi</surname><given-names>H</given-names></name><name><surname>Poudel</surname><given-names>S</given-names></name><name><surname>Muruganujan</surname><given-names>A</given-names></name><name><surname>Casagrande</surname><given-names>JT</given-names></name><name><surname>Thomas</surname><given-names>PD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Panther version 10: expanded protein families and functions, and analysis tools</article-title><source>Nucleic Acids Research</source><volume>44</volume><fpage>D336</fpage><lpage>D342</lpage><pub-id pub-id-type="doi">10.1093/nar/gkv1194</pub-id><pub-id pub-id-type="pmid">26578592</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>A</given-names></name><name><surname>Chang</surname><given-names>HY</given-names></name><name><surname>Daugherty</surname><given-names>L</given-names></name><name><surname>Fraser</surname><given-names>M</given-names></name><name><surname>Hunter</surname><given-names>S</given-names></name><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>McAnulla</surname><given-names>C</given-names></name><name><surname>McMenamin</surname><given-names>C</given-names></name><name><surname>Nuka</surname><given-names>G</given-names></name><name><surname>Pesseat</surname><given-names>S</given-names></name><name><surname>Sangrador-Vegas</surname><given-names>A</given-names></name><name><surname>Scheremetjew</surname><given-names>M</given-names></name><name><surname>Rato</surname><given-names>C</given-names></name><name><surname>Yong</surname><given-names>SY</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Punta</surname><given-names>M</given-names></name><name><surname>Attwood</surname><given-names>TK</given-names></name><name><surname>Sigrist</surname><given-names>CJA</given-names></name><name><surname>Redaschi</surname><given-names>N</given-names></name><name><surname>Rivoire</surname><given-names>C</given-names></name><name><surname>Xenarios</surname><given-names>I</given-names></name><name><surname>Kahn</surname><given-names>D</given-names></name><name><surname>Guyot</surname><given-names>D</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><name><surname>Letunic</surname><given-names>I</given-names></name><name><surname>Gough</surname><given-names>J</given-names></name><name><surname>Oates</surname><given-names>M</given-names></name><name><surname>Haft</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Natale</surname><given-names>DA</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name><name><surname>Orengo</surname><given-names>C</given-names></name><name><surname>Sillitoe</surname><given-names>I</given-names></name><name><surname>Mi</surname><given-names>H</given-names></name><name><surname>Thomas</surname><given-names>PD</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The interpro protein families database: the classification resource after 15 years</article-title><source>Nucleic Acids Research</source><volume>43</volume><fpage>D213</fpage><lpage>D221</lpage><pub-id pub-id-type="doi">10.1093/nar/gku1243</pub-id><pub-id pub-id-type="pmid">25428371</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandurangan</surname><given-names>AP</given-names></name><name><surname>Stahlhacke</surname><given-names>J</given-names></name><name><surname>Oates</surname><given-names>ME</given-names></name><name><surname>Smithers</surname><given-names>B</given-names></name><name><surname>Gough</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The superfamily 2.0 database: a significant proteome update and a new Webserver</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D490</fpage><lpage>D494</lpage><pub-id pub-id-type="doi">10.1093/nar/gky1130</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname><given-names>SC</given-names></name><name><surname>Luciani</surname><given-names>A</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>Finn</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>HMMER web server: 2018 update</article-title><source>Nucleic Acids Research</source><volume>46</volume><fpage>W200</fpage><lpage>W204</lpage><pub-id pub-id-type="doi">10.1093/nar/gky448</pub-id><pub-id pub-id-type="pmid">29905871</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>MN</given-names></name><name><surname>Wetmore</surname><given-names>KM</given-names></name><name><surname>Waters</surname><given-names>RJ</given-names></name><name><surname>Callaghan</surname><given-names>M</given-names></name><name><surname>Ray</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Kuehl</surname><given-names>JV</given-names></name><name><surname>Melnyk</surname><given-names>RA</given-names></name><name><surname>Lamson</surname><given-names>JS</given-names></name><name><surname>Suh</surname><given-names>Y</given-names></name><name><surname>Carlson</surname><given-names>HK</given-names></name><name><surname>Esquivel</surname><given-names>Z</given-names></name><name><surname>Sadeeshkumar</surname><given-names>H</given-names></name><name><surname>Chakraborty</surname><given-names>R</given-names></name><name><surname>Zane</surname><given-names>GM</given-names></name><name><surname>Rubin</surname><given-names>BE</given-names></name><name><surname>Wall</surname><given-names>JD</given-names></name><name><surname>Visel</surname><given-names>A</given-names></name><name><surname>Bristow</surname><given-names>J</given-names></name><name><surname>Blow</surname><given-names>MJ</given-names></name><name><surname>Arkin</surname><given-names>AP</given-names></name><name><surname>Deutschbauer</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mutant phenotypes for thousands of bacterial genes of unknown function</article-title><source>Nature</source><volume>557</volume><fpage>503</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0124-0</pub-id><pub-id pub-id-type="pmid">29769716</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radivojac</surname><given-names>P</given-names></name><name><surname>Clark</surname><given-names>WT</given-names></name><name><surname>Oron</surname><given-names>TR</given-names></name><name><surname>Schnoes</surname><given-names>AM</given-names></name><name><surname>Wittkop</surname><given-names>T</given-names></name><name><surname>Sokolov</surname><given-names>A</given-names></name><name><surname>Graim</surname><given-names>K</given-names></name><name><surname>Funk</surname><given-names>C</given-names></name><name><surname>Verspoor</surname><given-names>K</given-names></name><name><surname>Ben-Hur</surname><given-names>A</given-names></name><name><surname>Pandey</surname><given-names>G</given-names></name><name><surname>Yunes</surname><given-names>JM</given-names></name><name><surname>Talwalkar</surname><given-names>AS</given-names></name><name><surname>Repo</surname><given-names>S</given-names></name><name><surname>Souza</surname><given-names>ML</given-names></name><name><surname>Piovesan</surname><given-names>D</given-names></name><name><surname>Casadio</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Fang</surname><given-names>H</given-names></name><name><surname>Gough</surname><given-names>J</given-names></name><name><surname>Koskinen</surname><given-names>P</given-names></name><name><surname>Törönen</surname><given-names>P</given-names></name><name><surname>Nokso-Koivisto</surname><given-names>J</given-names></name><name><surname>Holm</surname><given-names>L</given-names></name><name><surname>Cozzetto</surname><given-names>D</given-names></name><name><surname>Buchan</surname><given-names>DWA</given-names></name><name><surname>Bryson</surname><given-names>K</given-names></name><name><surname>Jones</surname><given-names>DT</given-names></name><name><surname>Limaye</surname><given-names>B</given-names></name><name><surname>Inamdar</surname><given-names>H</given-names></name><name><surname>Datta</surname><given-names>A</given-names></name><name><surname>Manjari</surname><given-names>SK</given-names></name><name><surname>Joshi</surname><given-names>R</given-names></name><name><surname>Chitale</surname><given-names>M</given-names></name><name><surname>Kihara</surname><given-names>D</given-names></name><name><surname>Lisewski</surname><given-names>AM</given-names></name><name><surname>Erdin</surname><given-names>S</given-names></name><name><surname>Venner</surname><given-names>E</given-names></name><name><surname>Lichtarge</surname><given-names>O</given-names></name><name><surname>Rentzsch</surname><given-names>R</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Romero</surname><given-names>AE</given-names></name><name><surname>Bhat</surname><given-names>P</given-names></name><name><surname>Paccanaro</surname><given-names>A</given-names></name><name><surname>Hamp</surname><given-names>T</given-names></name><name><surname>Kaßner</surname><given-names>R</given-names></name><name><surname>Seemayer</surname><given-names>S</given-names></name><name><surname>Vicedo</surname><given-names>E</given-names></name><name><surname>Schaefer</surname><given-names>C</given-names></name><name><surname>Achten</surname><given-names>D</given-names></name><name><surname>Auer</surname><given-names>F</given-names></name><name><surname>Boehm</surname><given-names>A</given-names></name><name><surname>Braun</surname><given-names>T</given-names></name><name><surname>Hecht</surname><given-names>M</given-names></name><name><surname>Heron</surname><given-names>M</given-names></name><name><surname>Hönigschmid</surname><given-names>P</given-names></name><name><surname>Hopf</surname><given-names>TA</given-names></name><name><surname>Kaufmann</surname><given-names>S</given-names></name><name><surname>Kiening</surname><given-names>M</given-names></name><name><surname>Krompass</surname><given-names>D</given-names></name><name><surname>Landerer</surname><given-names>C</given-names></name><name><surname>Mahlich</surname><given-names>Y</given-names></name><name><surname>Roos</surname><given-names>M</given-names></name><name><surname>Björne</surname><given-names>J</given-names></name><name><surname>Salakoski</surname><given-names>T</given-names></name><name><surname>Wong</surname><given-names>A</given-names></name><name><surname>Shatkay</surname><given-names>H</given-names></name><name><surname>Gatzmann</surname><given-names>F</given-names></name><name><surname>Sommer</surname><given-names>I</given-names></name><name><surname>Wass</surname><given-names>MN</given-names></name><name><surname>Sternberg</surname><given-names>MJE</given-names></name><name><surname>Škunca</surname><given-names>N</given-names></name><name><surname>Supek</surname><given-names>F</given-names></name><name><surname>Bošnjak</surname><given-names>M</given-names></name><name><surname>Panov</surname><given-names>P</given-names></name><name><surname>Džeroski</surname><given-names>S</given-names></name><name><surname>Šmuc</surname><given-names>T</given-names></name><name><surname>Kourmpetis</surname><given-names>YAI</given-names></name><name><surname>van Dijk</surname><given-names>ADJ</given-names></name><name><surname>ter Braak</surname><given-names>CJF</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Gong</surname><given-names>Q</given-names></name><name><surname>Dong</surname><given-names>X</given-names></name><name><surname>Tian</surname><given-names>W</given-names></name><name><surname>Falda</surname><given-names>M</given-names></name><name><surname>Fontana</surname><given-names>P</given-names></name><name><surname>Lavezzo</surname><given-names>E</given-names></name><name><surname>Di Camillo</surname><given-names>B</given-names></name><name><surname>Toppo</surname><given-names>S</given-names></name><name><surname>Lan</surname><given-names>L</given-names></name><name><surname>Djuric</surname><given-names>N</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Vucetic</surname><given-names>S</given-names></name><name><surname>Bairoch</surname><given-names>A</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name><name><surname>Babbitt</surname><given-names>PC</given-names></name><name><surname>Brenner</surname><given-names>SE</given-names></name><name><surname>Orengo</surname><given-names>C</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name><name><surname>Mooney</surname><given-names>SD</given-names></name><name><surname>Friedberg</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A large-scale evaluation of computational protein function prediction</article-title><source>Nature Methods</source><volume>10</volume><fpage>221</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2340</pub-id><pub-id pub-id-type="pmid">23353650</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramakrishnan</surname><given-names>C</given-names></name><name><surname>Dani</surname><given-names>VS</given-names></name><name><surname>Ramasarma</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A conformational analysis of Walker motif a [ gxxxxgkt (S) ] in nucleotide-binding and other proteins</article-title><source>Protein Engineering, Design and Selection</source><volume>15</volume><fpage>783</fpage><lpage>798</lpage><pub-id pub-id-type="doi">10.1093/protein/15.10.783</pub-id><pub-id pub-id-type="pmid">12468712</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Bhattacharya</surname><given-names>N</given-names></name><name><surname>Thomas</surname><given-names>N</given-names></name><name><surname>Duan</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Canny</surname><given-names>J</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Song</surname><given-names>YS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evaluating protein transfer learning with tape</article-title><conf-name>Neural Information Processing Systems</conf-name><fpage>9689</fpage><lpage>9701</lpage><pub-id pub-id-type="pmid">33390682</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RM</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Canny</surname><given-names>J</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Tom sercu, and alexander rives</article-title><source>Msa Transformer</source><volume>139</volume><fpage>18</fpage><lpage>24</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesselman</surname><given-names>AJ</given-names></name><name><surname>Ingraham</surname><given-names>JB</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep generative models of genetic variation capture the effects of mutations</article-title><source>Nature Methods</source><volume>15</volume><fpage>816</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0138-4</pub-id><pub-id pub-id-type="pmid">30250057</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>PNAS</source><volume>118</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sanderson</surname><given-names>T</given-names></name><name><surname>Bileschi</surname><given-names>ML</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>ProteInfer</data-title><version designator="swh:1:rev:540773f988005cc5ed834210d1477e4db1f141e6">swh:1:rev:540773f988005cc5ed834210d1477e4db1f141e6</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:61ad36b062bad0a0a7ddd7435d0f3856c86fd18c;origin=https://github.com/google-research/proteinfer;visit=swh:1:snp:12a948e7197d60b08648c8f3041e11bd4f50b4ca;anchor=swh:1:rev:540773f988005cc5ed834210d1477e4db1f141e6">https://archive.softwareheritage.org/swh:1:dir:61ad36b062bad0a0a7ddd7435d0f3856c86fd18c;origin=https://github.com/google-research/proteinfer;visit=swh:1:snp:12a948e7197d60b08648c8f3041e11bd4f50b4ca;anchor=swh:1:rev:540773f988005cc5ed834210d1477e4db1f141e6</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>AS</given-names></name><name><surname>Hannum</surname><given-names>GJ</given-names></name><name><surname>Dwiel</surname><given-names>ZR</given-names></name><name><surname>Smoot</surname><given-names>ME</given-names></name><name><surname>Grant</surname><given-names>AR</given-names></name><name><surname>Knight</surname><given-names>JM</given-names></name><name><surname>Becker</surname><given-names>SA</given-names></name><name><surname>Eads</surname><given-names>JR</given-names></name><name><surname>LaFave</surname><given-names>MC</given-names></name><name><surname>Eavani</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Bansal</surname><given-names>AK</given-names></name><name><surname>Richardson</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep Semantic Protein Representation for Annotation, Discovery, and Engineering</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/365965</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senior</surname><given-names>AW</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Kirkpatrick</surname><given-names>J</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Qin</surname><given-names>C</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Nelson</surname><given-names>AWR</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Penedones</surname><given-names>H</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Crossan</surname><given-names>S</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Jones</surname><given-names>DT</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Improved protein structure prediction using potentials from deep learning</article-title><source>Nature</source><volume>577</volume><fpage>706</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1923-7</pub-id><pub-id pub-id-type="pmid">31942072</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shallue</surname><given-names>CJ</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Antognini</surname><given-names>JM</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name><name><surname>Frostig</surname><given-names>R</given-names></name><name><surname>Dahl</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Measuring the Effects of Data Parallelism on Neural Network Training</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1811.03600</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Smilkov</surname><given-names>D</given-names></name><name><surname>Thorat</surname><given-names>N</given-names></name><name><surname>Assogba</surname><given-names>Y</given-names></name><name><surname>Yuan</surname><given-names>A</given-names></name><name><surname>Kreeger</surname><given-names>N</given-names></name><name><surname>Yu</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Cai</surname><given-names>S</given-names></name><name><surname>Nielsen</surname><given-names>E</given-names></name><name><surname>Soergel</surname><given-names>D</given-names></name><name><surname>Bileschi</surname><given-names>S</given-names></name><name><surname>Terry</surname><given-names>M</given-names></name><name><surname>Nicholson</surname><given-names>C</given-names></name><name><surname>Gupta</surname><given-names>SN</given-names></name><name><surname>Sarah Sirajuddin</surname><given-names>DS</given-names></name><name><surname>Monga</surname><given-names>R</given-names></name><name><surname>Corrado</surname><given-names>G</given-names></name><name><surname>Viegas</surname><given-names>FB</given-names></name><name><surname>Wattenberg</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Tensorflow.Js: Machine Learning for the Web and Beyond</article-title><conf-name>Proceedings of Machine Learning and Systems</conf-name><fpage>309</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.48550/arXiv.1901.05350</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Protein homology detection by hmm-hmm comparison</article-title><source>Bioinformatics</source><volume>21</volume><fpage>951</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bti125</pub-id><pub-id pub-id-type="pmid">15531603</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soding</surname><given-names>J</given-names></name><name><surname>Biegert</surname><given-names>A</given-names></name><name><surname>Lupas</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The hhpred interactive server for protein homology detection and structure prediction</article-title><source>Nucleic Acids Research</source><volume>33</volume><fpage>W244</fpage><lpage>W248</lpage><pub-id pub-id-type="doi">10.1093/nar/gki408</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Mirdita</surname><given-names>M</given-names></name><name><surname>Vöhringer</surname><given-names>H</given-names></name><name><surname>Haunsberger</surname><given-names>SJ</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>HH-suite3 for fast remote homology detection and deep protein annotation</article-title><source>BMC Bioinformatics</source><volume>20</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1186/s12859-019-3019-7</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sureyya Rifaioglu</surname><given-names>A</given-names></name><name><surname>Doğan</surname><given-names>T</given-names></name><name><surname>Jesus Martin</surname><given-names>M</given-names></name><name><surname>Cetin-Atalay</surname><given-names>R</given-names></name><name><surname>Atalay</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DEEPred: automated protein function prediction with multi-task feed-forward deep neural networks</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>7344</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-43708-3</pub-id><pub-id pub-id-type="pmid">31089211</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzek</surname><given-names>BE</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>McGarvey</surname><given-names>PB</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name><name><surname>Consortium</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title><source>Bioinformatics</source><volume>31</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id><pub-id pub-id-type="pmid">25398609</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="web"><person-group person-group-type="author"><collab>UniProt Consortium</collab></person-group><year iso-8601-date="2019">2019a</year><article-title>Statistics: UniProtKB/TrEMBL 2019-02</article-title><ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/statistics/TrEMBL">https://www.uniprot.org/statistics/TrEMBL</ext-link><date-in-citation iso-8601-date="2019-03-30">March 30, 2019</date-in-citation></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>UniProt Consortium</collab></person-group><year iso-8601-date="2019">2019b</year><article-title>UniProt: a worldwide hub of protein knowledge</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D506</fpage><lpage>D515</lpage><pub-id pub-id-type="doi">10.1093/nar/gky1049</pub-id><pub-id pub-id-type="pmid">30395287</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warwick Vesztrocy</surname><given-names>A</given-names></name><name><surname>Dessimoz</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Benchmarking gene ontology function predictions using negative annotations</article-title><source>Bioinformatics</source><volume>36</volume><fpage>i210</fpage><lpage>i218</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa466</pub-id><pub-id pub-id-type="pmid">32657372</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Arnold</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine-learning-guided directed evolution for protein engineering</article-title><source>Nature Methods</source><volume>16</volume><fpage>687</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0496-6</pub-id><pub-id pub-id-type="pmid">31308553</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Anishchenko</surname><given-names>I</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Peng</surname><given-names>Z</given-names></name><name><surname>Ovchinnikov</surname><given-names>S</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Improved protein structure prediction using predicted interresidue orientations</article-title><source>PNAS</source><volume>117</volume><fpage>1496</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1073/pnas.1914677117</pub-id><pub-id pub-id-type="pmid">31896580</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>F</given-names></name><name><surname>Koltun</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multi-Scale Context Aggregation by Dilated Convolutions</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1511.07122</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Lapedriza</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning Deep Features for Discriminative Localization</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.319</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>N</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Bergquist</surname><given-names>TR</given-names></name><name><surname>Lee</surname><given-names>AJ</given-names></name><name><surname>Kacsoh</surname><given-names>BZ</given-names></name><name><surname>Crocker</surname><given-names>AW</given-names></name><name><surname>Lewis</surname><given-names>KA</given-names></name><name><surname>Georghiou</surname><given-names>G</given-names></name><name><surname>Nguyen</surname><given-names>HN</given-names></name><name><surname>Hamid</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The cafa challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens</article-title><source>Genome Biology</source><volume>20</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1186/s13059-019-1835-8</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80942.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.09.20.461077" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.20.461077"/></front-stub><body><p>The authors describe with the newly developed software, ProteInfer, an important new tool that analyses protein sequences to predict their functions. It is based on a single convolutional neural network scan for all known domains in parallel. This software provides a convincing approach for all computational scientists as well as experimentalists working near the interface of machine learning and molecular biology.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80942.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04cvxnb49</institution-id><institution>Goethe University</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Staller</surname><given-names>Max V</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an7q238</institution-id><institution>University of California, Berkeley</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.20.461077">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.09.20.461077v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Deep neural networks for protein functional inference&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Volker Dötsch as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Max Staller (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Outline a better method for setting up a validation set. This should be using the experimental evidence codes: http://geneontology.org/docs/guide-go-evidence-codes/ in some fashion, as those are the proteins for which the function is known. They can enrich the pool by adding close homologs.</p><p>2) Separate GO-based analyses to the different GO aspects: MFO, BPO, and CCO.</p><p>3) Figure 3: Add baseline methods (BLAST and Naive) to all analyses.</p><p>4) The difference between the single ProteInfer CNN and the ensemble ProteinInfer CNNs is unclear. Which one is being used on the website?</p><p>5) In addition, please provide more guidance on how to use the &quot;ProteinInfer CNN scaled by Blast Score&quot; model. Are there heuristics for when the scaling is worth the extra effort?</p><p>6) Figure S7 shows that combining Blast results with the CNN-Ensemble model was sometimes the best performing model, but it is unclear how the user could use the joint functionality.</p><p>7) It would be helpful to add a short paragraph explaining how a wet lab biologist might most efficiently combine ProteinInfer, BLAST, and ProtCNN. For which problems is each best suited? This discussion might be beyond the scope of this work.</p><p>8) How does this work differs from existing methods that seem very similar? This should be shown more clearly.</p><p>9) Recommendation: report results on highly similar (e.g. &gt;70% identical sequences between validation and training set) and less similar (&lt;70% identity between training and validation) sequences.</p><p>10) Recommendation: further integrate ProteInfer with Pfam-N (the ProtCNN model). It would be amazing to have both run in parallel with integrated results.</p><p>11) Recommendation: I would suggest moving the comparison of precision vs recall for CNN vs BLASTp from the supplemental material to the main text, as this is a crucial aspect of the study. It would be useful to also discuss or hypothesize why CNN has higher precision at lower recall values, whereas BLAST has higher recall at lower precision values (and especially why precision plateaus if you decrease recall in BLAST).</p><p>In the same vein, it would help to motivate the conceptual utility of the high-dimensional embedding for protein sequences, for example by providing functional or phylogenetic insight into a sub-category of enzymes.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80942.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Outline a better method for setting up a validation set. This should be using the experimental evidence codes: http://geneontology.org/docs/guide-go-evidence-codes/ in some fashion, as those are the proteins for which the function is known. They can enrich the pool by adding close homologs.</p></disp-quote><p>We performed this analysis, which is described in response to reviewer 1 public review.</p><disp-quote content-type="editor-comment"><p>2) Separate GO-based analyses to the different GO aspects: MFO, BPO, and CCO.</p></disp-quote><p>We performed this analysis, which is described in response to reviewer 1 public review.</p><disp-quote content-type="editor-comment"><p>3) Figure 3: Add baseline methods (BLAST and Naive) to all analyses.</p></disp-quote><p>We performed this analysis, which is described in response to reviewer 1 public review.</p><disp-quote content-type="editor-comment"><p>4) The difference between the single ProteInfer CNN and the ensemble ProteinInfer CNNs is unclear. Which one is being used on the website?</p></disp-quote><p>We clarified this in response to reviewer 2 public review.</p><disp-quote content-type="editor-comment"><p>5) In addition, please provide more guidance on how to use the &quot;ProteinInfer CNN scaled by Blast Score&quot; model. Are there heuristics for when the scaling is worth the extra effort?</p></disp-quote><p>We have now added a section on this approach to the Methods addressing these points.</p><disp-quote content-type="editor-comment"><p>6) Figure S7 shows that combining Blast results with the CNN-Ensemble model was sometimes the best performing model, but it is unclear how the user could use the joint functionality.</p></disp-quote><p>We now describe methodology better in this new Methods section. Essentially in this approach we see ProteInfer as an &quot;add-on&quot; to BLAST-based approaches, in order to increase the precision.</p><disp-quote content-type="editor-comment"><p>7) It would be helpful to add a short paragraph explaining how a wet lab biologist might most efficiently combine ProteinInfer, BLAST, and ProtCNN. For which problems is each best suited? This discussion might be beyond the scope of this work.</p></disp-quote><p>We are grateful for this point. We are excited about the fact that even a very basic heuristic multiplying the BLAST bit-score by the ProteInfer probability achieves a result better than either approach alone. We think that this suggests in particular potential for future more sophisticated approaches that use BLAST-results as a feature for models (encompassing more than simply the bit-score) to perform better still, but prefer to leave this for future work.</p><disp-quote content-type="editor-comment"><p>8) How does this work differs from existing methods that seem very similar? This should be shown more clearly.</p></disp-quote><p>We addressed this in response to reviewer 3 public review.</p><disp-quote content-type="editor-comment"><p>9) Recommendation: report results on highly similar ( e.g. &gt;70% identical sequences between validation and training set) and less similar (&lt;70% identity between training and validation) sequences.</p></disp-quote><p>We believe that we address this issue with our &quot;random&quot; and &quot;clustered&quot; splits, already included in the paper. This allows comparison of performance on a dataset where training data are forced to be more distant in sequence space (as assessed by UniRef50 clusters). Simply breaking down performance for a single network by sequence distance has some potential disadvantages compared to this technique as closeness in sequence space can also be confounded with the total number of examples for a particular label.</p><disp-quote content-type="editor-comment"><p>10) Recommendation: further integrate ProteInfer with Pfam-N (the ProtCNN model). It would be amazing to have both run in parallel with integrated results.</p></disp-quote><p>We agree that there is a lot of potential for applying deep learning methods that use the Pfam ontology to full-length proteins. The key motivation for developing the ProteInfer approach, which operates on full-length protein sequences, was that Pfam-N, trained on pre-segmented protein domains, performs relatively poorly when challenged with full length protein sequences. This means that direct integration is difficult as it would require the use of another algorithm to segment. However, when we built ProteInfer we did so in a flexible way that allowed any database cross-reference labels recorded in UniProt to be used as a vocabulary, which includes Pfam. While we did not focus the manuscript around this, the Pfam models have actually been available via the ProteInfer command-line interface throughout. We have now added a mention of this towards the end of the Methods, and provided a figure supplement to Figure 3 displaying performance.</p><disp-quote content-type="editor-comment"><p>11) Recommendation: I would suggest moving the comparison of precision vs recall for CNN vs BLASTp from the supplemental material to the main text, as this is a crucial aspect of the study. It would be useful to also discuss or hypothesize why CNN has higher precision at lower recall values, whereas BLAST has higher recall at lower precision values (and especially why precision plateaus if you decrease recall in BLAST).</p></disp-quote><p>We agree and have moved this graph to Figure 3B. We have added a sentence discussing how the need for the neural network to compress its knowledge into a limited set of weights may contribute to its reduced recall, and to a possible explanation for BLAST's reduced recall.</p><p>&quot;We found that BLASTp was able to achieve higher recall values than ProteInfer for lower precision values, while ProteInfer was able to provide greater precision than BLASTp at lower recall values. The high recall of BLAST is likely to reflect the fact that it has access to the entirety of the training set, rather than having to compress it into a limited set of neural network weights. In contrast, the lack of precision in BLAST could relate to reshuffling of sequences during evolution, which would allow a given protein to show high similarity to a trainining sequence in a particular subregion, despite lacking the core region required for that training sequence's function. We wondered whether..&quot;</p></body></sub-article></article>