<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72737</article-id><article-id pub-id-type="doi">10.7554/eLife.72737</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Meta-Research</subject></subj-group></article-categories><title-group><article-title>Investigating disagreement in the scientific literature</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-248275"><name><surname>Lamers</surname><given-names>Wout S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7176-9579</contrib-id><email>w.s.lamers@cwts.leidenuniv.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Wout S Lamers</bold> is in the Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands</p></bio></contrib><contrib contrib-type="author" id="author-128850"><name><surname>Boyack</surname><given-names>Kevin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7814-8951</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Kevin Boyack</bold> is at SciTech Strategies, Inc, Albuquerque, United States</p></bio></contrib><contrib contrib-type="author" id="author-196743"><name><surname>Larivière</surname><given-names>Vincent</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2733-0689</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Vincent Larivière</bold> is at École de bibliothéconomie et des sciences de l’information, Université de Montréal, Montréal, Canada</p></bio></contrib><contrib contrib-type="author" id="author-69651"><name><surname>Sugimoto</surname><given-names>Cassidy R</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Cassidy R Sugimoto</bold> is in the School of Public Policy, Georgia Institute of Technology, Atlanta, United States</p></bio></contrib><contrib contrib-type="author" id="author-131038"><name><surname>van Eck</surname><given-names>Nees Jan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8448-4521</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Nees Jan van Eck</bold> is in the Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands</p></bio></contrib><contrib contrib-type="author" id="author-41658"><name><surname>Waltman</surname><given-names>Ludo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8249-1752</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Ludo Waltman</bold> is in the Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands</p></bio></contrib><contrib contrib-type="author" corresp="yes" id="author-249745"><name><surname>Murray</surname><given-names>Dakota</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7119-0169</contrib-id><email>dakmurra@iu.edu</email><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Dakota Murray</bold> is in the School of Informatics, Computing, and Engineering, Indiana University, Bloomington, United States</p></bio></contrib><aff id="aff1"><label>1</label><institution>Centre for Science and Technology Studies, Leiden University</institution><addr-line><named-content content-type="city">Leiden</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>SciTech Strategies, Inc</institution><addr-line><named-content content-type="city">Albuquerque</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>École de bibliothéconomie et des sciences de l’information, Université de Montréal</institution><addr-line><named-content content-type="city">Montreal</named-content></addr-line><country>Canada</country></aff><aff id="aff4"><label>4</label><institution>School of Public Policy, Georgia Institute of Technology</institution><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>School of Informatics, Computing, and Engineering, Indiana University</institution><addr-line><named-content content-type="city">Bloomington</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>24</day><month>12</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e72737</elocation-id><history><date date-type="received" iso-8601-date="2021-08-03"><day>03</day><month>08</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-11-26"><day>26</day><month>11</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at arXiv.</event-desc><date date-type="preprint" iso-8601-date="2021-07-30"><day>30</day><month>07</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://arxiv.org/abs/2107.14641"/></event></pub-history><permissions><copyright-statement>© 2021, Lamers et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Lamers et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72737-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-72737-figures-v1.pdf"/><abstract><p>Disagreement is essential to scientific progress but the extent of disagreement in science, its evolution over time, and the fields in which it happens remain poorly understood. Here we report the development of an approach based on cue phrases that can identify instances of disagreement in scientific articles. These instances are sentences in an article that cite other articles. Applying this approach to a collection of more than four million English-language articles published between 2000 and 2015 period, we determine the level of disagreement in five broad fields within the scientific literature (biomedical and health sciences; life and earth sciences; mathematics and computer science; physical sciences and engineering; and social sciences and humanities) and 817 meso-level fields. Overall, the level of disagreement is highest in the social sciences and humanities, and lowest in mathematics and computer science. However, there is considerable heterogeneity across the meso-level fields, revealing the importance of local disciplinary cultures and the epistemic characteristics of disagreement. Analysis at the level of individual articles reveals notable episodes of disagreement in science, and illustrates how methodological artifacts can confound analyses of scientific texts.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>meta-research</kwd><kwd>disagreement</kwd><kwd>citation analysis</kwd><kwd>natural language processing</kwd><kwd>metascience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000181</institution-id><institution>Air Force Office of Scientific Research</institution></institution-wrap></funding-source><award-id>FA9550-19-1-039</award-id><principal-award-recipient><name><surname>Murray</surname><given-names>Dakota</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001804</institution-id><institution>Canada Research Chairs</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Larivière</surname><given-names>Vincent</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An approach based on cue phrases can be used to identify instances of disagreement in scientific articles and compare the level of disagreement in various disciplines.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Disagreement is a common phenomenon in science, and many of the most famous discoveries in the history of science were accompanied by controversy and disputes. Dialectic discourse emerged in ancient Greece, whereby the truth was thought to emerge from the arguments and counterarguments of scholars engaged in dialogue. The modern scientific method arose from a similar dialogue 350 years ago, as two individuals—Robert Boyle and Thomas Hobbes—debated over the meaning of experimental results obtained with the newly-invented air pump (<xref ref-type="bibr" rid="bib48">Shapin and Schaffer, 2011</xref>).</p><p>Disagreement also anchors much of the lore surrounding major scientific discoveries. For example, Alfred Wegener’s theory of plate tectonics was initially rejected by the scientific community; it took decades for the existence of gravitational waves to be confirmed in physics (<xref ref-type="bibr" rid="bib13">Collins, 2017</xref>) and the value of the Hubble constant is still disputed in cosmology (<xref ref-type="bibr" rid="bib8">Castelvecchi, 2020</xref>). Other conflicts are influenced by forces external to science, such as the controversies on the link between cigarette and lung cancer or between greenhouse gas and climate change (<xref ref-type="bibr" rid="bib43">Oreskes and Conway, 2011</xref>). Disagreement also features prominently in a number of influential theories in the philosophy and sociology of science, such as falsifiability (<xref ref-type="bibr" rid="bib44">Popper and Hudson, 1963</xref>), paradigm shifts (<xref ref-type="bibr" rid="bib28">Kuhn, 1996</xref>), and the scientific division of labor (<xref ref-type="bibr" rid="bib27">Kitcher, 1995</xref>).</p><p>Despite its importance to science, however, there is little empirical evidence of how much disagreement exists, where it is most common, and its consequences. Quantitative measures can be valuable tools to better understand the role and extent of disagreement across fields of science. Previous research has focused on consensus as evidenced by citation networks (<xref ref-type="bibr" rid="bib7">Bruggeman et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Shwed and Bearman, 2010</xref>; <xref ref-type="bibr" rid="bib50">Shwed and Bearman, 2012</xref>); on concepts related to disagreement in scientific texts such as negative citations, disputing citations, and uncertainty (<xref ref-type="bibr" rid="bib9">Catalini et al., 2015</xref>; <xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Nicholson et al., 2021</xref>); and on approaches based on word counts (<xref ref-type="bibr" rid="bib3">Bertin et al., 2016</xref>). Studying disagreement is challenging, given the lack of a widely accepted theoretical framework for conceptualizing disagreement combined with major challenges in its operationalization, for instance, the limited availability of large-scale collections of scientific texts.</p><p>This paper proposes an operationalization of disagreement in scientific articles that captures direct disagreement between two papers, as well as statements indicative of disagreement within the community. We describe a methodological approach to generate and manually-validate cue-phrases that reliably match to citation sentences (which we call &quot;citances&quot;) to represent valid instances of disagreement. We then use this approach to quantify the extent of disagreement across more than four million publications in the Elsevier <italic>ScienceDirect</italic> database, and investigate the rate of disagreement across fields of science.</p></sec><sec id="s2"><title>Literature review</title><p>It is widely acknowledged that disagreement plays a fundamental role in scientific progress (<xref ref-type="bibr" rid="bib1">Balietti et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Sarewitz, 2011</xref>; <xref ref-type="bibr" rid="bib40">Nature Methods, 2016</xref>). However, few studies have tried to quantify the level of disagreement in the scientific literature. Part of this may be explained by the fact that disagreement is difficult to both define and measure. There have been, however, attempts to assess consensus or uncertainty in the literature. Much of the early work on consensus attempted at characterizing differences between so-called hard and soft sciences. Cole described a series of experiments done in several fields, finding no evidence of differences in cognitive consensus along the &quot;hierarchy of sciences&quot; (<xref ref-type="bibr" rid="bib11">Cole, 1983</xref>). Hargens claimed that consensus was lower in fields having journals with higher rejection rates (<xref ref-type="bibr" rid="bib23">Hargens, 1988</xref>). This claim was contested by Cole, Simon and Cole, who argued that other variables accounted for the differences, and that reviewer’s assessments would be a better measure of consensus than rejection rates (<xref ref-type="bibr" rid="bib12">Cole et al., 1988</xref>). Fanelli found that positive results—support for the paper’s hypotheses—was far higher in the social sciences than the physical sciences, which is argued to reflect higher ambiguity, and thus lower consensus, in the social sciences (<xref ref-type="bibr" rid="bib20">Fanelli, 2010</xref>).</p><p>Recent studies on scientific consensus have made use of citations and text. Through a series of case studies, Shwed and Bearman used network modularity to claim that divisions in the citation network decreased over time, corresponding to increased consensus (<xref ref-type="bibr" rid="bib49">Shwed and Bearman, 2010</xref>). Nicolaisen and Frandsen used a Gini index calculated over bibliographic coupling count distributions to approximate consensus, and found that physics papers showed more consensus on average than psychology papers (<xref ref-type="bibr" rid="bib42">Nicolaisen and Frandsen, 2012</xref>). Using a corpus of nearly 168,000 papers, Evans, Gomez and McFarland calculated the Shannon entropy of language in a set of eight fields, and found more evidence that consensus was higher in the hard sciences than the social sciences (<xref ref-type="bibr" rid="bib19">Evans et al., 2016</xref>).</p><p>Other studies developed methods to identify <italic>uncertainty</italic> in text, a concept that is related to disagreement, and a potential indicator of consensus. For example, Szarvas and colleagues interpreted uncertainty as a &quot;lack of information&quot; and created a cue-word based uncertainty detection model based on three annotated datasets: BioScope, WikiWeasel, and Fact Bank (<xref ref-type="bibr" rid="bib55">Szarvas et al., 2012</xref>). Their results suggest that while domain-specific cues are useful, there remain cues that can reasonably identify uncertainty across domains. Similarly, Yang and colleagues developed a classifier based on manually annotated uncertainty cues and conditional random fields, and conducted a series of experiments to assess the performance of their method (<xref ref-type="bibr" rid="bib63">Yang et al., 2012</xref>). Chen, Song and Heo later extended these approaches and applied them to an empirical study of uncertainty across science (<xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref>). They first introduced a conceptual framework to study uncertainty that incorporates epistemic status and perturbation strength, and then measured uncertainty in 24 high-level scientific fields, and finally created an expanded set of uncertainty cues to support further analysis (Note that these rates included all types of uncertainty, whether they be theoretical, conceptual, or experimental, and within or between studies). The reported rate of uncertainty closely mirrored consensus, highest in the social sciences, followed by the medical sciences, environmental sciences, and engineering.</p><p>Many of the cues used as a starting point by <xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref> are hedging terms, which are commonly used in scientific writing to express possibility rather than certainty (<xref ref-type="bibr" rid="bib25">Hyland, 1998</xref>). In addition to being field-dependent, hedging rates have also been found to depend on whether a paper is primarily methodological. Recent work by Small and colleagues showed that citing sentences (i.e., citances) with the word &quot;may&quot; occur more frequently when citing method papers than non-method papers (<xref ref-type="bibr" rid="bib52">Small, 2018</xref>; <xref ref-type="bibr" rid="bib53">Small et al., 2019</xref>). More recently, Bornmann, Wray and Haunschild used a similar method to investigate uncertainty associated with specific concepts in the context of highly cited works (<xref ref-type="bibr" rid="bib5">Bornmann et al., 2019</xref>). While some might equate uncertainty or hedging with disagreement, they are not the same. As Small and colleagues have written, when citing another work, “hedging does not assert that the paper is wrong, but only suggests that uncertainty surrounds some aspect of the ideas put forward” (<xref ref-type="bibr" rid="bib53">Small et al., 2019</xref>). Here, we attempt to explicitly identify and measure scientific disagreement by using a large set of citances across all fields and by developing a set of cues validated by expert assessment.</p><p>Other studies of disagreement have been performed in the context of classification schemes of citation function. In an early attempt to categorize types of citations, disagreement was captured as “juxtapositional” and “negational” citations (<xref ref-type="bibr" rid="bib36">Moravcsik and Murugesan, 1975</xref>). However, this scheme was manually developed using a limited sample of papers and citations, and so the robustness and validity of the categories cannot be easily assessed. More recently, scholars have used larger datasets and machine learning techniques to scale citation classifications, often including categories of citations similar or inclusive of disagreement. For example, Teufel, Siddharthan and Tidhar developed a four-category scheme in which disagreement might be captured under their “weakness” or “contrast” citation types (<xref ref-type="bibr" rid="bib57">Teufel et al., 2006</xref>). Bertin and colleagues used n-grams to study location of negative and positive citations, and showed that that the word “<italic>disagree*</italic>” was much less likely to occur than the word “<italic>agree*</italic>”, irrespective of papers’ sections (<xref ref-type="bibr" rid="bib3">Bertin et al., 2016</xref>). In another study that aimed to identify meaningful citations, Valenzuela, Ha and Etzioni captured disagreement under the “comparison” citation type (<xref ref-type="bibr" rid="bib59">Valenzuela et al., 2015</xref>). Others have sought more coarse categories: Catalini, Lacetera and Oettl classified over 750,000 references made by papers published in the <italic>Journal of Immunology</italic> as either positive or negative, finding that negative references comprised about 2% of all references made (<xref ref-type="bibr" rid="bib9">Catalini et al., 2015</xref>). However, while these machine learning approaches are useful for analyzing large text data, they are also black boxes which can obfuscate issues and limit interpretation of their results.</p><p>Building on these studies, we propose a novel approach for the study of disagreement based on a set of manually-validated cue-phrases. We conduct one of the first empirical investigations into the specific notion of <italic>disagreement</italic> in science, and our inclusive definition allows us to capture explicit disagreement between specific papers as well as traces of disagreement within a field. Our cue-phrase based approach is more transparent and reproducible than black-box machine learning methodologies commonly employed in citation classification, and also extensively validated using over 3,000 citation sentences representing a range of fields. We extend the scale of past analyses, identifying instances of disagreement across more than four million scientific publications.</p></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Data</title><p>We sourced data from an Elsevier ScienceDirect corpus that was also used in a previous study (<xref ref-type="bibr" rid="bib6">Boyack et al., 2018</xref>) and that is hosted at the Centre for Science and Technology Studies (CWTS) at Leiden University. This corpus contains the full-text information of nearly five million English-language research articles, short communications, and review articles published in Elsevier journals between 1980 and 2016. The corpus comprises articles from nearly 3,000 Elsevier journals. Given that Elsevier is the largest publisher in the world, this corpus is one of the largest multidisciplinary sources of full-text scientific articles currently available, with coverage of both natural sciences, medical sciences, as well as the social sciences and humanities.</p><p>We focus our analysis on sentences containing in-text citations (citances). These citances were extracted from the full-text of articles following the procedure outlined in previous work (<xref ref-type="bibr" rid="bib6">Boyack et al., 2018</xref>). The Elsevier ScienceDirect corpus that was used was constructed in the following way. First, the Crossref REST API was used to identify all articles published by Elsevier. The full-text of these articles was subsequently downloaded from the Elsevier ScienceDirect API (Article Retrieval API) in XML format. Each XML full-text record was parsed to identify major sections and paragraphs (using XML tags), and sentences (using a sentence-splitting algorithm). In-text citations in the main text were identified by parsing the main text (excluding those in footnotes and figure and table captions). XML records without in-text citations were discarded, and publications from before 1998 were omitted from analysis due to poor availability of full-text records before that year. The resulting dataset consisted of 4,776,340 publications containing a total of 145,351,937 citances, ranging from 1998–2016.</p><p>To facilitate analysis at the level of scientific fields, articles in Elsevier ScienceDirect and references cited in these articles were matched with records in the Web of Science database based on their DOI (where available) and a combination of publication year, volume number, and first page number. (The Web of Science database used by CWTS includes the Science Citation Index Expanded, the Social Sciences Citation Index, and the Arts &amp; Humanities Citation Index; other Web of Science citation indices are not included). We used an existing classification of research articles and review articles in the Web of Science created at CWTS. In this hierarchical classification, each article published between 2000 and 2015 and indexed in the Web of Science was algorithmically assigned to a single micro-level scientific field, each of which are in turn members of one of 817 meso-level fields. It is at this meso-level that we perform our most detailed analyses, the categories being fine-grained enough to provide insights into local communities while also large enough to contain a sufficient number of citances. A further benefit of this approach to clustering is that each meso-level field, and each individual publication, can be directly grouped into one of five broad fields: biomedical and health sciences; life and earth sciences; mathematics and computer science; physical sciences and engineering; and social sciences and humanities. Linking our dataset to this classification system resulted in a subset of 3,883,563 papers containing 118,012,368 citances, spanning 2000–2015. The classification was created algorithmically based on direct citation links between articles, using the methodology introduced by <xref ref-type="bibr" rid="bib60">van Eck and Waltman, 2010</xref> and <xref ref-type="bibr" rid="bib58">Traag et al., 2019</xref>. A visualization of the meso-level classification was created using the VOSviewer software (<xref ref-type="bibr" rid="bib60">van Eck and Waltman, 2010</xref>).</p></sec><sec id="s3-2"><title>Operationalizing disagreement</title><p>Researchers can disagree for many reasons, sometimes over data and methodologies, but more often because of differences in interpretation (<xref ref-type="bibr" rid="bib17">Dieckmann and Johnson, 2019</xref>). Some of these disagreements are explicitly hostile and adversarial, whereas others are more subtle, such as contrasting findings with past results and theories. We introduce an inclusive definition of disagreement that captures explicit textual instances of disagreement, controversy, dissonance, or lack of consensus between scientific publications, including cases where citing authors are not taking an explicit stance themselves. Our definition distinguishes between two kinds of disagreement, which together capture the diversity of obvious and subtle disagreement in the scientific literature: <italic>paper-level disagreement</italic> and <italic>community-level disagreement</italic>.</p><p>The first, <italic>paper-level disagreement</italic>, occurs when one publication offers a finding or perspective that is (at least partly) incompatible with the perspective of another (even though there may be no explicit contradiction). Consider the following artificial example of a citation sentence explicitly disagreeing with the conclusion of a past study:</p><list list-type="simple"><list-item><p>&gt; We find that coffee does not cause cancer, contrary to the finding of &lt;ref&gt; that coffee does cause cancer.</p></list-item></list><p><italic>Paper-level disagreement</italic> can also be more subtle. For example, in the following two disagreement sentences, although they do not resolutely contradict one another, the citing and cited publications use models that are based on incompatible assumptions (first sentence), or observe different effects from different data (second sentence):</p><list list-type="simple"><list-item><p>&gt; Assuming that coffee increases the probability of cancer by 50%, the predicted life expectancy for the Dutch population is 80 years, in contrast to the 85 years proposed by models that assumed coffee does not increase the risk of cancer &lt;ref&gt;.</p></list-item><list-item><p>&gt; Contrary to previous studies that did not observe evidence to support the hypothesis that coffee causes cancer &lt;ref&gt;, our data suggests that drinking coffee increases the probability of cancer by 50%.</p></list-item></list><p><italic>Community-level disagreement,</italic> in contrast, refers to the situation in which a citing publication, without explicitly disagreeing with a cited publication, instead draws attention to a controversy or lack of consensus in the larger body of literature. Including community-level disagreement allows us to identify indirect traces of disagreement in a field, even in the absence of explicit disagreement between the referenced authors, or between the citing and cited papers. Consider the following examples of community-level disagreement; the first notes the disagreement between the referenced studies, and the second cites a single review article indicating disagreement within the field:</p><list list-type="simple"><list-item><p>&gt; There remains controversy in the scientific literature over whether or not coffee is associated with an increased risk of cancer &lt;refs&gt;.</p></list-item><list-item><p>&gt; A recent review of studies assessing the potential link between coffee consumption and cancer risk has observed continued controversy &lt;ref&gt;.</p></list-item></list><p>Here, we do not differentiate between paper-level or community-level disagreement, including both under our operationalization of disagreement.</p></sec><sec id="s3-3"><title>Signal and filter terms</title><p>We compose cue-phrases of <italic>signal</italic> terms and <italic>filter</italic> terms. A variety of approaches can be used to generate these terms, and our approach is not dependent on any particular strategy. Here, we create a preliminary set of signal terms through an intensive iterative process of manually identifying, classifying, validating, and deliberating on strategies for retrieving instances of disagreement. This took place over several meetings, utilizing multiple approaches to generate signal words, including sourcing cues used in related work (e.g., <xref ref-type="bibr" rid="bib3">Bertin et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref>), expanding this list with synonyms from online thesauruses, and ranking them by their frequency among citation sentences. This inductive process included several rounds of deliberation, manual annotation, and tests of inter-rater reliability in order to generate a robust list of candidate signal terms. The terms are intended to have high validity, but are not considered comprehensive.</p><p>We queried the database for citances containing each of these signal terms (case insensitive), using wildcards to provide for possible variants of terms (e.g., “challenge”, “challenged”, and “challenges”), excluding generic negation phrases (“no”, “not”, “cannot”, “nor” and “neither” to exclude phrases such as “no conflict”), and for some signal terms excluding citances containing words associated with disciplinary jargon or methods, such as for the signal term “disagreement”, which often appears with Likert-scale descriptions (e.g., “scale”, “agreement”, or “kappa”) for survey-heavy fields. The modifications for the signal terms were derived after several rounds of review and validation. In total, citances returned by signal phrase queries comprise 3.10% of the database (n=145,351,937), though their relative occurrence varied dramatically, with the most coming from the “<italic>differ*”</italic> signal term, and the least from “<italic>disprove*”</italic> (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Specific terms comprising each of the thirteen signal term sets and specific exceptions.</title><p>The “*” symbol (wildcard) captures possible variants.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Signal term</th><th align="left" valign="top">Variants</th><th align="left" valign="top">Exclusions</th><th align="left" valign="top">Results</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>challenge*</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">405,613</td></tr><tr><td align="left" valign="bottom"><bold>conflict*</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">212,246</td></tr><tr><td align="left" valign="bottom"><bold>contradict*</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">115,375</td></tr><tr><td align="left" valign="bottom"><bold>contrary</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">171,711</td></tr><tr><td align="left" valign="bottom"><bold>contrast*</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">1,257,866</td></tr><tr><td align="left" valign="bottom"><bold>controvers*</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">154,608</td></tr><tr><td align="left" valign="bottom"><bold>debat*</bold></td><td align="left" valign="top"/><td align="left" valign="top">“parliament* debat*”, “congress* debat*”, “senate* debat*”, “polic* debat*”, “politic* debat*”, “public* debat*”, “societ* debat*”</td><td align="left" valign="bottom">150,617</td></tr><tr><td align="left" valign="bottom"><bold>differ*</bold></td><td align="left" valign="top"/><td align="left" valign="top">“different*”</td><td align="left" valign="bottom">2,003,677</td></tr><tr><td align="left" valign="bottom"><bold>disagree*</bold></td><td align="left" valign="top">“not agree*”, “no agreement”</td><td align="left" valign="top">“range”, “scale”, “kappa”, “likert”, “agree*” and/or “disagree” within a ten-word range of each other.</td><td align="left" valign="bottom">52,615</td></tr><tr><td align="left" valign="bottom"><bold>disprov*</bold></td><td align="left" valign="top"/><td align="left" valign="top">“prove*” and “disprove*” within a ten-word range</td><td align="left" valign="bottom">2,938</td></tr><tr><td align="left" valign="bottom"><bold>no consensus</bold></td><td align="left" valign="top">“lack of consensus”</td><td align="left" valign="top">“consensus sequence”, “consensus site”</td><td align="left" valign="bottom">16,632</td></tr><tr><td align="left" valign="bottom"><bold>questionable</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="bottom">24,244</td></tr><tr><td align="left" valign="bottom"><bold>refut*</bold></td><td align="left" valign="top"/><td align="left" valign="top">“refutab*”</td><td align="left" valign="bottom">10,322</td></tr><tr><td align="left" valign="top"><bold>total</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">4,578,464</td></tr></tbody></table></table-wrap><p>In order to more precisely capture valid instances of disagreement and to understand their function within the literature, we also queried for citances containing both the signal terms along with at least one of four sets of <italic>filter</italic> terms, with no more than four words separating signal and filter in the same sentence. As with signal terms, filter terms were derived from iterative manual efforts of the authors to identify terms most associated with valid instances of disagreement. Four distinct sets of terms were identified, corresponding to explicit mentions of terms relating to past studies, ideas, methods, and results (see <xref ref-type="table" rid="table2">Table 2</xref>). As with signal phrases alone, the relative incidence of signal and filter phrase combinations varies widely (see Table S1 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Queries were constructed for each combination of signal term (13 total) and filter term (four total sets), producing 52 combined queries, alongside 13 queries consisting only of standalone signal terms unrestricted by filter terms, for a total of 65 queries.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Specific terms comprising each of the four filter term sets.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom"><bold>studies</bold></td><td align="left" valign="bottom">studies; study; previous work; earlier work; literature; analysis; analyses; report; reports</td></tr><tr><td align="left" valign="bottom"><bold>ideas</bold></td><td align="left" valign="bottom">idea*; theory; theories; assumption*; hypothesis; hypotheses</td></tr><tr><td align="left" valign="bottom"><bold>methods</bold></td><td align="left" valign="bottom">model*, method*, approach*; technique*</td></tr><tr><td align="left" valign="bottom"><bold>results</bold></td><td align="left" valign="bottom">result*; finding*; outcome*; evidence; data; conclusion*; observation*</td></tr></tbody></table></table-wrap></sec><sec id="s3-4"><title>Query validation</title><p>From each set of results returned by the 65 queries, we selected 50 sentences for validation using simple random sampling without replacement (only 40 citances existed for “no consensus” +”ideas”), resulting in over 3,000 queried sentences. For each query, two coders were randomly selected from among the seven authors on this paper to manually annotate each citance as a valid or invalid instance of disagreement. The label was chosen based only on the text in the citation sentence, without knowledge on the citing paper’s title, authors, field of study, or the surrounding text.</p><p>Consider the following four example sentences listed below (where (…) indicates the position of cited references and […] indicates additional text not quoted here). The first is invalid because the signal term, “conflict”, refers to an object of study, and not a scientific dispute; the second sentence is also invalid because the term “conflicting” refers to results within a single study, not between studies; the third sentence is invalid because “challenge” appears while quoting the cited study; the fourth and fifth sentence are both examples of sentences that would be marked as valid. Similar patterns can be observed for other signal terms, such as <italic>challenge*</italic> (see Table S2 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><list list-type="order"><list-item><p>Invalid: “To facilitate <underline>conflict</underline> management and analysis in Mcr (…), the Graph Model for Conflict Resolution (GMCR) (…) was used.”</p></list-item><list-item><p>Invalid: “The 4 year extension study provided ambiguous […] and <underline>conflicting</underline> post hoc […] results.”</p></list-item><list-item><p>Invalid: “Past studies (...) review the theoretical literature and concludes that future empirical research should ‘<underline>challenge</underline> the assumptions and analysis of the theory’.”</p></list-item><list-item><p>Valid: “These observations are rather in <underline>contradiction</underline> with Smith et al.’s […].”</p></list-item><list-item><p>Valid: “Although there is substantial evidence supporting this idea, there are also recent <underline>conflicting</underline> reports (…).”</p></list-item></list><p>We assessed the labels for each signal/filter term combination with two measures: percent agreement (% agree) and percent valid (% valid). Percent agreement is the proportion of annotated citances in which both coders agreed on the same label of valid or invalid; this measure provides a simple measure of coder’s consensus. Here, percent agreement is justified over more complicated measures (such as Cohen’s kappa) due to the small sample of data per signal/filter term combination, and given that there are only two categories and coders.</p><p>Most signal/filter term combinations had high agreement (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The overall percentage agreement between coders was high, at 85.5%. Given the difficulty of interpreting academic texts, this high percentage agreement demonstrates the robustness of our operationalization of disagreement. The signal term with the highest average agreement was <italic>no consensus</italic> (95.8%). There were only a few combinations with very low percentage agreement, mostly regarding the signal term <italic>questionable,</italic> which had an average lowest average percent agreement (64%); the nature of sentences returned from the <italic>questionable</italic> keyword tended to constitute marginal cases of disagreement. There was virtually no variance between the average percent agreement aggregated across filter terms. However, certain combinations of signal and filter terms were notable in resulting in higher or lower performance. For example, the difference between the highest agreement, <italic>differ* _standalone_</italic> (100%)<italic>,</italic> and <italic>differ* + methods</italic> (74%) is 26 perecentage points—the addition of filter terms can dramatically impact the kinds of citances returned by the query.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Agreement and validity of different combinations of signal term and filter term.</title><p>Measures calculated from 50 randomly-sampled citances for each combination of signal term (vertical axis) and filter term (horizontal axis), annotated as valid or invalid instances of disagreement by two independent coders. (<bold>a</bold>) Percentage agreement, or the proportion of citances for which coders independently agreed on the label. (<bold>b</bold>) Percentage validity, or the proportion of citances which both coders labeled as valid. Averages for the various signal terms are shown in the left-most column; averages for the various filter terms are shown in the bottom row. (<bold>c</bold>) Percentage agreement (blue circles) and validity (red diamonds) of each signal/filter term combination, ordered from highest percent validity (top) to lowest percent validity (bottom). Numbers on the right are the total number of citances returned by querying using the signal/filter term combination, and are colored according to their log-transformed value. (<bold>d</bold>) Log-transformed count of citances returned by each query combination, colored by the (log-transformed) number of citances. Citance counts are non-exclusive, meaning that citances of the form <italic>debat* + studies</italic> will also be counted towards <italic>debat* _standalone_</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Distribution of citances returned by signal/filter term queries.</title><p>Callouts (<bold>I</bold>, <bold>II</bold>, …, <bold>VIII</bold>) map to examples in Table S3. <bold>a</bold>. Distribution of all disagreement citances appearing in papers across five fields: Biomedical and Health Sciences, Life and Earth Sciences, Physical Sciences and Engineering, Social Sciences and Humanities, and Math and Computer Science. <bold>b–d</bold>. Percentage change between the actual number of citances per field and signal/filter term combination compared to the expected given the disciplinary distribution (from <bold>a</bold>). The red line corresponds to 0 percent increase between the actual and expected. White dots indicate that the citances for that signal/filter term are under-represented (lower than expected, ratio less than zero), whereas black dots indicate that citances are over-represented (more than expected). Shown aggregated across signal terms (<bold>b</bold>), filter terms (<bold>c</bold>), and for all signal/filter term combinations (<bold>d</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig1-figsupp1-v1.tif"/></fig></fig-group><p>We calculate the percent valid as the percentage of citances annotated as valid by both coders; this provides an intuitive measure of the validity and reliability of a query. Signal/filter term combinations that best capture disagreement should have both high percent agreement and high percent validity. Not all signal/filter term combinations were found to be sufficiently valid (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Overall, 61.6% of all coded citances were labeled as valid, with large variance between the most (100%), and the least valid (0%) combinations. The signal term with the highest average validity regardless of filter term was <italic>no consensus</italic> (94.9%), followed by <italic>controvers*</italic> (88.8%) and <italic>debat*</italic> (82.4%). Unlike with percent agreement, average validity differs drastically between filter terms, with all having higher average validity than <italic>_standalone_</italic>. The combinations with highest validity are <italic>no consensus + studies</italic> (98%), <italic>no consensus + methods</italic> (98%), and <italic>no consensus _standalone_</italic> (94%) For specific signal terms, the presence of a filter term can have a drastic impact of coded validity; for example, the validity of <italic>contrast*</italic> + ideas (80%) is four times greater than of <italic>contrast* _standalone_</italic> and <italic>contrast + methods (20</italic>%).</p><p>The queries that best capture instances of disagreement are those with the highest validity. We choose a validity threshold of 80% and exclude queries with lower validity from subsequent analysis. We also consider several adjustments to the threshold to assess the robustness of our empirical findings. 23 queries sit above this the 80% threshold (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), including all five <italic>no consensus</italic> and <italic>controvers*</italic> queries, four <italic>debat*</italic> queries, two <italic>disagree*</italic> and <italic>contradict*</italic> queries, and one query each for <italic>contrary*</italic>, <italic>contrast</italic>*, <italic>conflict*</italic>, <italic>disprove*,</italic> and <italic>questionable</italic>. Because we prioritized precision, these 23 queries comprise only a fraction of total citances: 455,625, representing 0.31% of all citances in our dataset. We note that citances returned by queries are not exclusive; for example, a citance containing both <italic>controvers*</italic> and <italic>no consensus*</italic> would count towards both signal phrases. Similarly, a citance returned with the query <italic>controvers* + methods</italic> would also be returned by the <italic>controvers*</italic>. Naturally, more general queries, such as <italic>differ*</italic> and <italic>contrast*</italic> returned a much greater number of citances. Among queries above the 80% threshold, <italic>controvers*</italic> and <italic>debat*</italic> produce the highest number of citances (154,608 and 150,617 respectively, <xref ref-type="fig" rid="fig1">Figure 1d</xref>). The 455,625 citances returned by our queries as well as relevant publication and query details are available in Zenodo (<xref ref-type="bibr" rid="bib29">Lamers and Van Eck, 2021</xref>).</p></sec></sec><sec id="s4" sec-type="results"><title>Results</title><p>Instances of disagreement, operationalized using the 23 validated queries, accounted for approximately 0.31% of all citation sentences (citances) extracted from indexed papers published between 2000 and 2015 (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Disagreement was highest in the social sciences and humanities (Soc &amp; Hum; 0.61%), followed by biomedical and health sciences (Bio &amp; Health; 0.41%), life and earth sciences (Life &amp; Earth; 0.29%); physical sciences and engineering (Phys &amp; Engr; 0.15%), and mathematics and computer science (Math &amp; Comp; 0.06%).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Disagreement reflects a hierarchy of fields.</title><p>(<bold>a</bold>) Percent of all citances in each field that contain signals of disagreement, meaning they were returned by one of the 23 queries with validity of 80% or higher. Fields marked by lower consensus, such as in Soc &amp; Hum, had a greater proportion of disagreement. (<bold>b</bold>) Percent of disagreement by field and over time, showing little change overall, but some changes by field. Text indicates the average percentage-point change per-year by field.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Percent of all citances returned by each of the 23 queries with validity over 80%.</title><p>Each panel corresponds to the signal phrase, and lines within each panel to filter phrases.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig2-figsupp1-v1.tif"/></fig></fig-group><p>Our measure shows that disagreement has been relatively constant over time (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), decreasing at an average rate of about 0.0005 percentage points per year. This is driven by falling disagreement in Phys &amp; Engr (–0.0045 points per year), Soc &amp; Hum (–0.0033 points per year), and Math &amp; Comp (–0.0019 points per year). Phys &amp; Engr stands out not only for its stable decrease each year, but also for its relative size; given a starting rate of one disagreement signal per 529 citances in 2000, by 2015 the rate of disagreement in Phys &amp; Engr fell to one disagreement per 809 citances, a 35% decrease, compared to a 24% decrease for Math &amp; Comp and only a 5% decrease in Soc &amp; Hum. In contrast, disagreement has tended to increase somewhat in Bio &amp; Health ( + 0.0017 points per year) and Life &amp; Earth ( + 0.0018 points per year). These trends are likely not the result of uses of individual queries; for example, <italic>disagree*</italic> queries are over-represented in Phys &amp; Engr (see VI in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), yet the incidence of these terms is falling or remaining stable (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Similarly, <italic>debat*</italic> was over-represented in Soc &amp; Hum and has increased in usage despite slight falling disagreement in the field. That these changes are not confined to any single query suggests that field-level differences represent changes in the level of disagreement within a field rather than linguistic or methodological artifacts.</p><sec id="s4-1"><title>Heterogeneity in disagreement across scientific fields</title><p>The more fine-grained meso-fields reveal heterogeneity within the larger fields (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Overall, meso-field disagreement followed the same pattern as <xref ref-type="fig" rid="fig2">Figure 2</xref>, with higher scores in Soc &amp; Hum and lower in Math &amp; Comp. However, some meso-fields stand out. For example, some of the highest rates of disagreement found in Bio &amp; Health meso-fields were in more socially-oriented journals such as Quality of Life Research, Value in Health, and Pharmacoeconomics. Similarly, in Math &amp; Comp, the meso-field with the most disagreement contained journals relating to transportation science, a technical field which draws on management studies and other social science literature. This pattern held in Life &amp; Earth, in which a meso-field with a relatively high share of disagreement contained papers in journals such as <italic>Marine Policy</italic>, <italic>Ecology &amp; Society</italic>, and <italic>Forest Policy &amp; Economics</italic>. The high disagreement in these meso-fields lends support to the hypothesis that regardless of the high-level field, more socially-oriented topics generate a higher level of disagreement. Also notable is that, in Life &amp; Earth, several large fields with relatively high disagreement study the distant geological past or other inaccessible objects of studies, comprised of papers in journals such as the <italic>Journal of Vertebrate Paleontology</italic>, <italic>Cretaceous Research</italic>, and <italic>Sedimentary Research</italic>. A similar observation can be made in Phys &amp; Engr, where astronomy-related fields featuring journals such as <italic>Planetary and Space Science</italic> and <italic>Theoretical Biology</italic> exhibit above-average rates of disagreement, along with fields pertaining to research into superconductivity. Field-level results must be interpreted cautiously, however, as our signal terms may misclassify citances based on disciplinary keywords and jargon (see Table S3 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Heterogeneity in disagreement across meso-fields.</title><p>Fine-grained view across 817 meso-level fields, each a cluster of publications grouped and positioned based on their citation links derived from the Web of Science database (see Materials and methods), 2000–2015. The area of each point is proportional to the number of disagreement citances in that field. Overlapping points are an artifact of their position and size, and bear no additional meaning. Color maps to the log ratio of the share of disagreement citances given the mean share across all fields, truncated at 4 x greater and 4 x lower than the mean. Soc &amp; Hum tends to have a greater proportion of disagreement citances, and Math &amp; Comp the least. Other panels show the same data, but highlight the meso-fields in each high-level field. Meso-fields of interest are highlighted, and labels show a selection of journals in which papers in each field are published. Journals listed in labels are representative of each meso-field in the Web of Science, and is not limited to those represented in the Elsevier ScienceDirect data. An interactive version of this visualization is available online at <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/disagreement-meso-fields">https://tinyurl.com/disagreement-meso-fields</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>On average, older papers are less likely to receive a disagreement citance, though this trend does not hold for the “hard” sciences.</title><p>Percentage of disagreement citances by the relative age of the citing to the cited paper, in years, and high-level field, for papers published between 2000 and 2015. Intensity of color corresponds to the age category of the cited paper.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Distribution of citances by their position in the text of the manuscript, and by field.</title><p>Shown for all citances (solid line) and disagreement citances (dotted line). For example, about 15% of disagreement citances in Physical Sciences and Engineering appear in the first 0%–5% of the sentences in documents.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Little difference in disagreement between men and women.</title><p>Percentage of disagreement citances by gender of the citing-paper author, their authorship position (first or last), and the high-level field. Numbers above each bar corresponds to the ratio difference between the percentage of disagreement between women and men. The number below each bar corresponds to the number of disagreement citances. we infer a gender for the first and last authors of papers with a disagreement citance published after 2008, determined based on the author’s first name as in past work (<xref ref-type="bibr" rid="bib32">Larivière et al., 2013</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Authors disagree less when citing their own work.</title><p>Percentage of disagreement citances among instances of non-self and self-citation, 2000–2015. A citance is defined as a self-citation when the citing and cited paper have at least one name in common. Results are shown by field. Numbers below each bar are the number of disagreement citances. Overall, disagreement is 2.4 times more common for non-self citation than for self-citation, with variance between major fields.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig3-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s4-2"><title>Disagreement by contextual factors</title><p>We also investigated the extent to which other factors, including paper age, citance position, author demographics, and self-citation, relate to a paper’s being cited in the context of disagreement.</p><sec id="s4-2-1"><title>Paper age</title><p>Authors may disagree with more recent papers at different rates than older ones. We quantify disagreement based on the age of a cited paper papers (relative to the citing paper) and find that, on average, younger papers are more likely to feature in a disagreement citance than older ones, which may indicate that the role of cited literature varies based on its age (<xref ref-type="bibr" rid="bib24">He and Chen, 2018</xref>). Following a brief <italic>bump,</italic> or increase in disagreement (at 05–09 years), older papers tend to be receive fewer disagreement citances (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), a pattern driven by field differences. Low consensus, high complexity fields such as Soc &amp; Hum and Bio &amp; Health both exhibit a clear decreasing pattern, with falling disagreement as the paper ages. Life &amp; Earth, in the middle of the hierarchy, repeats this pattern, but only after a period of stability in disagreement in the first ten years. Disagreement instead steadily increases over time in high consensus and low complexity fields such as Phys &amp; Engr and Math &amp; Comp.</p></sec><sec id="s4-2-2"><title>Position in the paper</title><p>Disagreement is not equally likely to occur throughout a paper. Investigating the distribution of disagreement citances across papers, we find that they are far more likely to occur in the beginning of a paper, likely in the introduction, and then towards the end, likely the discussion section (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), corresponding to previous observations of disagreement cue phrases in PLoS journals (Bertin et a., 2016), and likely indicating a unique argumentative role of disagreement. The precise patterns differ by field. For example, in Soc &amp; Hum, disagreement citances are more evenly distributed through the first 40% of the paper, whereas in Bio &amp; Health and Life &amp; Earth disagreement citations are more likely to appear near the end of a paper. While these field level differences may reflect differences in how fields use citations, they are more likely the result of distinct article formats across fields (i.e., long literature reviews in Soc &amp; Hum).</p></sec><sec id="s4-2-3"><title>Gender of Citing-Paper Author</title><p>Men and women authors may issue disagreement citances at different rates. To investigate this, we infer a gender for the first and last authors of papers issuing a disagreement citance and published after 2008, determined based on the author’s first name as in past work (<xref ref-type="bibr" rid="bib32">Larivière et al., 2013</xref>). Overall, there is little difference in the rate at which disagreement is introduced by men and women first and last authors (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). The one exception is Math &amp; Comp, in which women last authors issue 1.2 times more disagreement citations than men, though the rate of disagreement is small, and driven by a small number of instances.</p></sec><sec id="s4-2-4"><title>Self-citation</title><p>As a confirmation of overall validity, we measure the rate of disagreement by instances of self-citation and non-self-citation. We expect that authors will be less likely to cite their own work within the context of disagreement. Indeed, we find that the rate of disagreement for non-self-citations is 2.4 times greater than for self-citations (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>), demonstrating that our indicator of disagreement affirms expectations. The field with the largest difference is Bio &amp; Health (2.5 times greater), followed by Phys &amp; Engr (2.2 times greater), Math &amp; Comp (2.2 times greater), Life &amp; Earth (1.9 times greater), and finally, Soc &amp; Hum (1.6 times greater).</p></sec></sec><sec id="s4-3"><title>Robustness</title><p>To test the robustness of our results, we compare findings using the 23 queries with greater than 80% validity to those using the 36 queries with greater than 70% validity. The new queries include c<italic>ontradict* _standalone_, contrary + studies, contrary + methods, conflict* + results, disagree* + methods, disagree* + ideas, disprov* + methods, disprov* + ideas, refut* + studies, refut* + results, refut* + ideas, debat* + ideas,</italic> and <italic>questionable + ideas</italic>. Queries above the 80% validity cutoff account for 455,625 citances; the addition of 13 queries above the 70% cutoff bring this total to 574,020.</p><p>We find that our findings are robust whether using an 80% or 70% validity cutoff. Relaxing the validity cutoff results in including more citances, inflating the share of disagreement across all results. However, the qualitative interpretation of these results does not change (Table S4). The 80% and 70% cutoffs both produce the same ordering of fields from most to least disagreement. Similarly, the ordering of fields from high-to-low disagreement holds between the 80% and 70% cutoff for all quantities presented here, including the average change per year, the ratio of disagreement between non-self-citation and self-citation, and the average change in disagreement per age bin. Some fields gain more from these new queries than others, manifesting in more or less intense field differences. For example, Soc &amp; Hum gains a full 17 percentage points in overall disagreement with the 70% threshold, with the increase across all fields at only eight points. Similarly, the ratio of non-self-citation to self-citation is 2.2 x for Math &amp; Comp with the 80% cutoff, but only 1.3 x for the 70% cutoff. Future work may find that different thresholds are more appropriate across fields, depending on their distinct patterns of discourse.</p></sec><sec id="s4-4"><title>Disciplinary differences in query results</title><p>It is worthile to consider how specific queries manifest across fields, which can give insights into their unique uses of language and disagreement. We consider the incidence of each query compared to an expected value, given the distribution of citances across all high-level fields. This is necessary as the number of publications varies across fields. For example there are far more publications from Bio &amp; Health than in other fields, accounting for a total of 47.5% of all publications indexed in the Web of Science Database; in contrast, publications in Math &amp; Comp comprise a far smaller proportion of the database, accounting for only 3.1%.</p><p>Even accounting for the different number of publications per field, we still observe that some signal terms appear more in certain fields than expected, often as a result of differences in disciplinary jargon, topics, and norms (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>). For example, there are more <italic>conflict*</italic> citances than expected in Soc &amp; Hum, where it often appears in relation to conflict as a topic of study, such as the study of international conflict, conflict theory, or other interpersonal conflicts (line I in Table S3). Similarly, <italic>disprov*</italic> citances appear more often in Math &amp; Comp, where disprove is often used in relation to proving or disproving theorems and other mathematical proofs (line II in Table S3). Other notable differences are <italic>controvers*</italic> citances appearing more often in Bio &amp; Health, <italic>debat*</italic> appearing most often in Life &amp; Earth, and <italic>disagree*</italic> appearing most in Phys &amp; Engr.</p><p>Filter terms are also not randomly distributed across fields (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>). For example, the +<italic>ideas</italic> filter term appears more often than expected in Soc &amp; Hum, possibly as a result of disciplinary norms around use and discussion of abstract theories and concepts (line III in Table S3). In contrast, + <italic>methods</italic> is over-represented in Phys &amp; Engr and Math &amp; Comp, likely a result of these field’s focus on methods and technique (line IV in Table S3). Notably, + <italic>studies</italic> and + <italic>results</italic> are under-represented among Math &amp; Comp publications, whereas + <italic>ideas</italic> and + <italic>methods</italic> are underrepresented among papers in the Bio &amp; Health.</p><p>The complexity of disciplinary differences between queries is made apparent when examining combinations of signal and filter phrases (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>). While there are no obvious or consistent patterns across fields, there are notable cases. For example, compared to all other fields, <italic>controvers*</italic> citances are over-represented in Bio &amp; Health (line V in Table S3), except for <italic>controvers + ideas,</italic> which is instead slightly over-represented in Life &amp; Earth. In contrast, <italic>disagree*</italic> citances are under-represented in Bio &amp; Health, but over-represented in Life &amp; Earth and Phys &amp; Engr (line VI in Table S3). In some cases, the specific signal + filter term combination has a massive effect, such as <italic>no consensus + ideas</italic>, which is heavily over-represented in Soc &amp; Hum (line VII in Table S3), whereas all other signal and filter term combinations are under-represented. Similarly, <italic>contradict* + ideas</italic> and <italic>contradict* + methods</italic> are over-represented in Math &amp; Comp (line VIII in Table S3), whereas + <italic>results</italic> and + <italic>studies</italic> are underrepresented. Similar intricacies can be found across the 325 combinations of cue-phrase and field, demonstrating the importance that field plays in the utility and significance of our signal and filter terms.</p><p>Especially at the fine-grained field level, methodological artifacts can drive differences we observe between meso-fields. For example, in Soc &amp; Hum, one of the meso-fields with the most disagreement was composed of papers from journals such as <italic>Political Studies</italic> and <italic>International Relations</italic>—journals and fields for which “debates” and “conflicts” are objects of study, which could confound the <italic>debat*</italic> and <italic>conflict*</italic> signal terms. This is demonstrated by the following invalid citation sentences:</p><list list-type="order"><list-item><p>“Since the late-1990s, there has been even less room for <bold><underline>debate</underline></bold> within the party (...).”</p></list-item><list-item><p>“Indeed, this whole idea harkens back to the badges of slavery of the 13th Amendment and the <bold><underline>debate</underline></bold> in (...).”</p></list-item><list-item><p>“In political behaviour literature, we refer to such <bold><underline>conflictive</underline></bold> opinions as “ambivalence” (...).”</p></list-item><list-item><p>“In politics as usual, people often do not like to see the <bold><underline>conflicts</underline></bold> and disagreements common to partisan debate (...).”</p></list-item></list><p>Even though terms such as “public debate”, and “parliamentary debate” were excluded (<xref ref-type="table" rid="table1">Table 1</xref>), the <italic>debat*</italic> signal terms were over-represented in Soc &amp; Hum (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>); <italic>conflict*</italic> was also overrepresented to a lesser extent. Interpretation of the results for main and meso-fields needs to be moderated by these, and other confounding artifacts.</p></sec><sec id="s4-5"><title>Assessment of individual papers</title><p>We perform a qualitative investigation of the individual papers that issued the most disagreement citations, and which were cited most often in the context of disagreement. First, we examine the citing paper perspective, that is those papers that issued the most citances (Table S5 in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). These top papers demonstrate how methodological artifacts can contribute to these more extreme examples. For example, one of these papers considers the pedagogical and evaluative potential of debates in the classroom (<xref ref-type="bibr" rid="bib18">Doody and Condon, 2012</xref>); the <italic>“debat*”</italic> signal term incorrectly classifies several citations as evidence of scientific disagreement. However, other papers offer interesting instances of disagreement, and exemplify lessons that should be considered in its study. For instance, one such paper concerns meteorite impact structures (<xref ref-type="bibr" rid="bib22">French and Koeberl, 2010</xref>) and includes discussion on the controversies in the field. Another is a review article arguing for multi-target agents for treating depressive states (<xref ref-type="bibr" rid="bib34">Millan, 2006</xref>), and catalogs the controversies around the topic. Yet another is a book on <italic>Neurotoxicology and Teratology</italic>, misclassified as a research article in the database, which illustrates how the length of an article can contribute to its likelihood of issuing a disagreement citation (<xref ref-type="bibr" rid="bib26">Kalter, 2003</xref>).</p><p>Considering the cited paper perspective—those papers that received the most paper-level disagreement citations or were referenced the most in the context of community disagreement—reveals clear instances of disagreement in the literature. Many of the studies receiving the most disagreement citances (Table S6 in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>) relate to a single longstanding scientific controversy in the earth sciences concerning the formation of the <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/w/index.php?title=North_China_Craton&amp;oldid=1056978275">North China Craton</ext-link>, a tectonic structure spanning Northern China, Inner Mongolia, the Yellow Sea, and North Korea. This list of most-disagreed-with papers also includes a literature review that is cited as an exemplar of controversy, here regarding the existence of “lipid rafts” in cells (<xref ref-type="bibr" rid="bib37">Munro, 2003</xref>), and a paper on fMRI research that is heralded as a methodological improvement in the field, and is often cited to draw a contrast with other methods (<xref ref-type="bibr" rid="bib38">Murphy et al., 2009</xref>). A more thorough discussion of papers that issue and receive the most disagreement can be found in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>.</p></sec><sec id="s4-6"><title>Disagreement and citation impact</title><p>We also explore whether disagreement relates to citation impact; whereas previous analysis revealed a positive relationship between conflict and citation (<xref ref-type="bibr" rid="bib45">Radicchi, 2012</xref>), our preliminary results do not find evidence of increased citation, at least in the years immediately following the disagreement (<xref ref-type="table" rid="table3">Table 3</xref>). We arrived at this observation by comparing the number of citations received in year <italic>t + 1</italic> for papers that featured in a disagreement citance for the first time in year <italic>t</italic>, with the average number of citations received in year <italic>t + 1</italic> by papers that received the exact same number of citations in year <italic>t</italic>. This over- or under-citation of individual papers that encountered disagreement can then be aggregated to arrive at the average over- or under-citation following disagreement.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Being cited in the context of disagreement has little impact on citations in the year following.</title><p>For each field, shown are the number of cited papers, as well as for t + 1, t + 2 and t + 3 with t being the year in which a cited paper first featured in the context of disagreement, its average number of received citations, expected number of received citations, and d the ratio between these two values. When d is greater than one, papers cited in the context of disagreement receive more citations in the following year than expected. When d is less than one, they receive fewer citations than expected.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Scientific field</th><th align="left" valign="bottom">Number of records</th><th align="left" valign="bottom">Avg. citations, t + 1 following disagreement</th><th align="left" valign="bottom">Expected citations, t + 1 following disagreement</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom">Avg. citations, t + 2</th><th align="left" valign="bottom">Expected citations, t + 2</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom">Avg. citations, t + 3</th><th align="left" valign="bottom">Expected citations, t + 3</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom">All</td><td align="char" char="." valign="bottom">109,545</td><td align="char" char="." valign="bottom">3.03</td><td align="char" char="." valign="bottom">3.08</td><td align="char" char="." valign="bottom">0.983</td><td align="char" char="." valign="bottom">3.02</td><td align="char" char="." valign="bottom">3.05</td><td align="char" char="." valign="bottom">0.990</td><td align="char" char="." valign="bottom">2.96</td><td align="char" char="." valign="bottom">2.98</td><td align="char" char="." valign="bottom">0.993</td></tr><tr><td align="left" valign="bottom">Bio &amp; Health</td><td align="char" char="." valign="bottom">60,707</td><td align="char" char="." valign="bottom">2.73</td><td align="char" char="." valign="bottom">2.81</td><td align="char" char="." valign="bottom">0.969</td><td align="char" char="." valign="bottom">2.68</td><td align="char" char="." valign="bottom">2.75</td><td align="char" char="." valign="bottom">0.974</td><td align="char" char="." valign="bottom">2.56</td><td align="char" char="." valign="bottom">2.65</td><td align="char" char="." valign="bottom">0.966</td></tr><tr><td align="left" valign="bottom">Life &amp; Earth</td><td align="char" char="." valign="bottom">20,581</td><td align="char" char="." valign="bottom">3.43</td><td align="char" char="." valign="bottom">3.35</td><td align="char" char="." valign="bottom">1.023</td><td align="char" char="." valign="bottom">3.55</td><td align="char" char="." valign="bottom">3.42</td><td align="char" char="." valign="bottom">1.038</td><td align="char" char="." valign="bottom">3.63</td><td align="char" char="." valign="bottom">3.44</td><td align="char" char="." valign="bottom">1.056</td></tr><tr><td align="left" valign="bottom">Math &amp; Comp</td><td align="char" char="." valign="bottom">770</td><td align="char" char="." valign="bottom">3.36</td><td align="char" char="." valign="bottom">3.34</td><td align="char" char="." valign="bottom">1.005</td><td align="char" char="." valign="bottom">3.54</td><td align="char" char="." valign="bottom">3.28</td><td align="char" char="." valign="bottom">1.080</td><td align="char" char="." valign="bottom">3.29</td><td align="char" char="." valign="bottom">2.97</td><td align="char" char="." valign="bottom">1.109</td></tr><tr><td align="left" valign="bottom">Phys &amp; Engr</td><td align="char" char="." valign="bottom">18,011</td><td align="char" char="." valign="bottom">3.55</td><td align="char" char="." valign="bottom">3.52</td><td align="char" char="." valign="bottom">1.006</td><td align="char" char="." valign="bottom">3.48</td><td align="char" char="." valign="bottom">3.44</td><td align="char" char="." valign="bottom">1.010</td><td align="char" char="." valign="bottom">3.43</td><td align="char" char="." valign="bottom">3.34</td><td align="char" char="." valign="bottom">1.027</td></tr><tr><td align="left" valign="bottom">Soc &amp; Hum</td><td align="char" char="." valign="bottom">9,476</td><td align="char" char="." valign="bottom">3.04</td><td align="char" char="." valign="bottom">3.11</td><td align="char" char="." valign="bottom">0.979</td><td align="char" char="." valign="bottom">3.20</td><td align="char" char="." valign="bottom">3.28</td><td align="char" char="." valign="bottom">0.975</td><td align="char" char="." valign="bottom">3.30</td><td align="char" char="." valign="bottom">3.40</td><td align="char" char="." valign="bottom">0.971</td></tr></tbody></table></table-wrap><p>We define <italic>t</italic> as the time in years since publication and <italic>c</italic> as the number of citations a paper received at time <italic>t</italic>. We calculate for each combination of <italic>t</italic> and <italic>c</italic> the number of papers <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that were first cited in the context of disagreement at time <italic>t</italic> when they held <italic>c</italic> citations. Using these, we calculate the number of citations received by these papers in the year following publication, averaged across all combinations of <italic>t</italic> and <italic>c</italic>,<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mover><mml:mi>c</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>∗</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mover><mml:mi>c</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>In the same way, we also calculate the expected number of citations, defined using the average number of citations received by papers that received <italic>c</italic> citations in year <italic>t</italic>, regardless of whether they were cited by a disagreement citation.<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mover><mml:mi>c</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>∗</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mover><mml:mi>c</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>We calculated <italic>d</italic> as the ratio of these two values. When greater than one, it indicates that papers received more citations than expected in the year after having been cited in the context of disagreement. A value less than one indicates that papers with a disagreement citation received fewer citations in the year following.<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mover><mml:mi>c</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi>c</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The results of this analysis (<xref ref-type="table" rid="table3">Table 3</xref>) show that being cited in a context of disagreement has little to no effect on the citations received by papers in the year following their citation (or not) in the context of disagreement. Extending the analysis to citations received in year <italic>t + 2</italic> and <italic>t + 3</italic> yielded similar null results.</p><p>Papers that themselves contain disagreement citances, however, tend to receive more citations over their lifespan. To demonstrate this, we examine the 3.5% (n = 126,250) of publications that contain at least one disagreement citance in their text. Across all publications, those with at least one disagreement citance tended to receive more citations than those without disagreement in the first four years, beginning with one additional citation in the first year following publication, and expanding to a difference of about 4.7 citations by the fourth year (<xref ref-type="fig" rid="fig4">Figure 4</xref>), an effect that varies, yet is qualitatively consistent across all fields. This effect may be confounded by article type—for example, review articles are over-represented in terms of disagreement—24.6% of all review articles contain a disagreement citance—and review articles are also known to be more highly cited (<xref ref-type="bibr" rid="bib35">Miranda and Garcia-Carpintero, 2018</xref>). While excluding review articles does shrink this gap, the citation count for full research articles (85% of all publications) remains 2.5 citations higher for those with a disagreement citance than for those without.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Full research articles with a disagreement citance are cited more.</title><p>The y-axis shows the difference in average citation counts for papers containing at least one disagreement citance, and for papers without. Positive values indicate that publications with disagreement received more citations than those without. Values are shown for the population of publications in each year following publication (x-axis). Shown here for only articles labeled in the Web of Science database as full research articles.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72737-fig4-v1.tif"/></fig><p>We note that these results are confounded by our umbrella definition of disagreement, which does not differentiate between paper-level and community-level disagreement. Paper-level disagreement—when the author of the citing paper explicitly contrasts their study with another—is a straightforward example of issuing (by the citing paper) and receiving (by the cited paper) disagreement. Community-level disagreement, in contrast, either involves a citing author rhetorically positioning two or more papers as being in disagreement, or citing a single past paper such as a review, as evidence of the controversy surrounding a topic. While these two cases offer evidence of disagreement in the field, their potential for identifying specific, controversial papers is less clear (<xref ref-type="bibr" rid="bib45">Radicchi, 2012</xref>). Future research should aim to disentangle paper-level and community-level disagreement, and understand their varying relationship to citation impact.</p></sec></sec><sec id="s5" sec-type="discussion"><title>Discussion</title><p>When it comes to defining scientific disagreement, scholars disagree. Rather than staking out a specific definition, we adopt a broad operationalization of disagreement that incorporates elements of Kuhn’s accumulation of anomalies and paradigm shifts (<xref ref-type="bibr" rid="bib28">Kuhn, 1996</xref>), Latour’s controversies (<xref ref-type="bibr" rid="bib33">Latour, 1988</xref>), and more recent notions of uncertainty (<xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref>) and negative citations (<xref ref-type="bibr" rid="bib9">Catalini et al., 2015</xref>). By bridging these past theories, we quantify the rate of disagreement across science. Roughly 0.31% of all citances in our dataset are instances of disagreement, a share that has remained relatively stable over time. However, this number is much smaller than in past studies—such as the 2.4% for so-called “negative” references (<xref ref-type="bibr" rid="bib9">Catalini et al., 2015</xref>), and the estimated 0.8% for “disputing” citations (<xref ref-type="bibr" rid="bib41">Nicholson et al., 2021</xref>). This is explained by our operationalization of disagreement, which although conceptually broader than negative or disputing citations, is narrowed to only 23 queries to prioritize precision. Moreover, studies differ in corpus used, most often covering only one journal or field, compared to our large multidisciplinary corpus. The strength of our analysis is not the absolute incidence of disagreement, but its relative differences across disciplinary and social contexts.</p><p>Disagreement across fields can be interpreted using several theoretical frameworks. Differences in disagreement might stem from the <italic>epistemic</italic> characteristics of fields and their topics of study. For example, Auguste Comte proposed that fields are organized based on the inherent complexity of their subject matter (<xref ref-type="bibr" rid="bib14">Comte, 1856</xref>). We reinforce this &quot;hierarchy of sciences&quot; model, finding that disagreement is highest in fields at the top of the hierarchy, such as the social sciences and humanities, and lowest in fields at the bottom of the hierarchy, such as physics and mathematics.</p><p>While the hierarchy of sciences model is well-grounded theoretically (<xref ref-type="bibr" rid="bib11">Cole, 1983</xref>) and bibliometrically (<xref ref-type="bibr" rid="bib20">Fanelli, 2010</xref>; <xref ref-type="bibr" rid="bib21">Fanelli and Glänzel, 2013</xref>), other frameworks may be equally useful in understanding disagreement across fields. For example, the structural characteristics of fields may explain their differences in disagreement. One such characteristic is how reliant the field is on Kuhnian paradigms (<xref ref-type="bibr" rid="bib28">Kuhn, 1996</xref>); so-called “hard” sciences, such as physics, may have strong theoretical paradigms and greater consensus (less disagreement) than “soft” sciences such as those in the social sciences and humanities (<xref ref-type="bibr" rid="bib4">Biglan, 1973</xref>).</p><p>Changes in these structural characteristics may also contribute to the temporal evolution of disagreement. For instance, the decrease of disagreement in physics and engineering may be due to a transition into a period of “normal” science (<xref ref-type="bibr" rid="bib28">Kuhn, 1996</xref>), as it has been previously argued for certain sub-fields (<xref ref-type="bibr" rid="bib54">Smolin, 2007</xref>). Increase in collaboration (<xref ref-type="bibr" rid="bib62">Wuchty et al., 2007</xref>) may also affect the trends, as consensus has to be reached among a larger body of individuals during the research process.</p><p>Social sciences and humanities have other characteristics that might be associated with more common or more intense conflicts, including low centralization of resources and control over research agendas, high diversity in their audiences and stakeholders, and limited standardization of methods and theories (<xref ref-type="bibr" rid="bib61">Whitley, 2000</xref>). A field’s <italic>cultural</italic> characteristics also play a role in its norms of disagreement. Fields have different norms when it comes to consensus formation and the settling of disputes (<xref ref-type="bibr" rid="bib15">de Cetina and Reyes, 1999</xref>), and some fields even value disagreement as an important element of scholarship. For instance, a cultural norm of “agonism”, or ritualized adversarialism, is common in many humanities fields, wherein one’s arguments are framed in direct opposition to past arguments (<xref ref-type="bibr" rid="bib56">Tannen, 2002</xref>).</p><p>Fields also have distinct cultures of evaluation, which shapes how they judge each other’s work and impacts whether they are likely to reach consensus (<xref ref-type="bibr" rid="bib30">Lamont, 2009</xref>). Of course, epistemic, structural, and cultural characteristics of fields are all inter-related—cultural practices emerge in part from structural characteristics of a field, such as access to expensive instruments, which in turn are related to the epistemic aspects of the object of study. Our data does not allow us to disentangle these relationships or argue which is most appropriate, but each offers a useful lens for understanding why disagreement might differ between fields.</p><p>Expanding our analysis into a more fine-grained classification of fields reveals greater detail into where disagreement happens in science. We observed that socially-oriented meso-level fields tended to have a higher rate of disagreement, no matter their main field. For example, meso-fields concerning healthcare policy had higher rates of disagreement than others in the biomedical and health sciences whereas the meso-field concerning transportation science had a higher rate of disagreement than all others mathematics and computer science. Though these fields draw on the expertise of the &quot;hard&quot; sciences, they do so in order to study social processes and address social questions.</p><p>In the life and earth sciences, disagreement was especially high in meso-fields that study the earth’s geological and paleontological history. In these fields, much like in the social sciences, researchers cannot easily design experiments, and so progress instead comes from debate over competing theories using limited evidence and reconstructed historical records. This is exemplified by paleontology, in which a 2017 paper sparked controversy and forced a re-interpretation of the fossil record and a 130-year-old theory of dinosaur evolution (<xref ref-type="bibr" rid="bib2">Baron et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Langer et al., 2017</xref>). Similarly, our approach identified a major controversy in the earth sciences—the formation of the North China Craton—again illustrating how reliance on historical records might exacerbate disagreement. These cases illustrate the heterogeneity of disagreement in science, and illustrate that existing theoretical frameworks, such as the hierarchy of science, can oversimplify the diversity of cultural norms and epistemic characteristics that manifest at more fine-grained levels of analysis.</p><p>Our approach comes with limitations. First, our method captures only a fraction of textual disagreements in science. This is partly due to our prioritization of precision over recall, having removed cue-phrases with low validity. Our lists of signal and filter terms are also non-exhaustive, and so their extension in future research would identify more instance of disagreement. Given our focus on citances, we are not able to identify traces of disagreements that occur without explicit reference to past literature, or those that can only be classified as disagreement with surrounding sentences as context. Some disagreements may also be too subtle, or rely on technical jargon, such that they cannot be identified with our general signal terms. Moreover, our measure does not capture non-explicit disagreements, or scientific disagreements occurring outside of citances, such as in conferences, books, social media, or in interpersonal interactions. For these reasons, our measure of disagreement may over- or under-represent disagreement in particular fields, and should be considered when evaluating results.</p><p>Second, in spite of its overall precision, our approach returns many false positives in particular disciplinary contexts. For example, the signal term <italic>conflict*</italic> matches to topics of study and theories in the fields of sociology and international relations (e.g., “ethnic conflicts”, “Conflict theory”). In other instances, a signal term can even match an author’s name (as in the surname “Debat”, as in <xref ref-type="bibr" rid="bib16">Debat et al., 2003</xref>). We also find that these artifacts are over-represented among the papers that issued the most disagreement citances, and those that were most often cited in the context of disagreement (see Appendix 6). However, given our extensive validation, these artifacts remain a small minority of all disagreement sentences identified, though they should be considered when interpreting disagreement in small sub-fields.</p><p>Finally, our inclusive definition of disagreement homogenizes disagreement into a single category, whereas there are many kinds of disagreement in science. For example, the ability to differentiate between paper-level and community-level disagreement could lend insight into how conflict and controversy manifest in different fields. This definition could also be developed to differentiate further between types of disagreement: for example, past citation classification schemes have differentiated between “juxtaposition” and “negational” citations (<xref ref-type="bibr" rid="bib36">Moravcsik and Murugesan, 1975</xref>), or between “weakness” and “contrast” citations (<xref ref-type="bibr" rid="bib57">Teufel et al., 2006</xref>).</p><p>Despite these limitations, our framework and study have several advantages. First, in contrast to keyword-based analyses, our approach provides a nuanced view of disagreement in science, revealing the differences in disagreement not only between signal terms, but also based on filter terms. This drives the second advantage of our approach—that its inherent <italic>transparency</italic> allows us to easily identify confounding artifacts such as when a signal term is an object of study (i.e., “international <italic>conflicts</italic>”, “public <italic>debate”</italic>), when it relates to disciplinary jargon (i.e., “<italic>disproving</italic> theorems” in Mathematics, or <italic>“strongly disagree”</italic> in survey studies that use Likert scales) or when the keyword is part of a proper name (i.e., “work by <italic>Debatin</italic> et al.,”). These issues are a concern for any automated analysis of scientific texts across disciplines—the usage and meaning of words varies across fields. In contrast to black-box style machine learning approaches, ours is transparent and can easily be validated, interpreted, replicated, and extended.</p><p>Finally, by being open and transparent, our approach is easily adjustable to different contexts. Our initial identification of keywords was the result of an iterative process of exploration and validation, which eventually resulted in a non-exhaustive set of signal terms, filter terms, exclusions, and then a final set of validated queries. Any step of this process can be tuned, extended, and improved to facilitate further studies of scientific disagreement—new signals or filters can be introduced, queries can be modified to be even more precise, and the threshold of validity changed; here, for example, we assessed our results by setting a more inclusive threshold for which queries constitute disagreement, and find the results remain robust (see Table S1 in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). To assist in further efforts to validate and extend our work, we have made annotated sentences and code that can reproduce this analysis publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/murrayds/sci-text-disagreement">github.com/murrayds/sci-text-disagreement</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:44641d8369477d44432fdf50b2eae38e5d079742;origin=https://github.com/murrayds/sci-text-disagreement;visit=swh:1:snp:5695398f6bd0811d67792e16a2684052abe9dc37;anchor=swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3">swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3</ext-link>, <xref ref-type="bibr" rid="bib39">Murray, 2021</xref>).</p><p>Whereas black-box machine learning approach have many strengths (e.g. <xref ref-type="bibr" rid="bib46">Rife et al., 2021</xref>), ours is transparent and intuitive. Its transparency allows to easily identify terms that have field-specific meanings, which may be obfuscated in black-box approaches. Our approach is also reproducible and can be refined and extended with additional signal and filter terms. The portability of our queries also mean that they can readily be applied to other full-text data. The general method of generating and manually validating signal and filter terms can also be applied to other scientific phenomena, such as detecting uncertainty (<xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref>), negativity (<xref ref-type="bibr" rid="bib9">Catalini et al., 2015</xref>), discovery (<xref ref-type="bibr" rid="bib51">Small et al., 2017</xref>), or an expanded framework of disagreement (<xref ref-type="bibr" rid="bib36">Moravcsik and Murugesan, 1975</xref>).</p><p>Future research could refine and extend these existing queries and link them to different conceptual perspectives on disagreement in science. Such work could build on our analyses of the factors that may affect disagreement, including gender, paper age, and citation impact. Disagreement is an essential aspect of knowledge production, and understanding its social, cultural, and epistemic characteristics will reveal fundamental insights into science in the making.</p></sec></body><back><sec id="s6" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Formal analysis, Investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec id="s7" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-72737-transrepform1-v1.pdf"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Tables S1 – S4.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-72737-supp1-v1.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Papers with most disagreement citances and papers most often cited in the context of disagreement.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-72737-supp2-v1.docx"/></supplementary-material></sec><sec id="s8" sec-type="data-availability"><title>Data availability</title><p>The Elsevier full-text data and the Web of Science bibliographic data used in this study were obtained from proprietary data sources. We are not allowed to share the raw data on which our study is based. We do have permission from Elsevier to share a data set containing the 455,625 citing sentences identified using our disagreement queries. This data set is available in Zenodo (Lamers &amp; Van Eck, 2021, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5148058">https://doi.org/10.5281/zenodo.5148058</ext-link>). Source data and visualization code for all figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/murrayds/sci-text-disagreement">github.com/murrayds/sci-text-disagreement</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:44641d8369477d44432fdf50b2eae38e5d079742;origin=https://github.com/murrayds/sci-text-disagreement;visit=swh:1:snp:5695398f6bd0811d67792e16a2684052abe9dc37;anchor=swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3">swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Lamers</surname><given-names>WS</given-names></name><name><surname>Van Eck</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Measuring Disagreement in Science</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5148058</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Yong-Yeol Ahn, Staša Milojević, Alessandro Flammini, Filippo Menczer, Dashun Wang, Lili Miao, participants of the &quot;A scientometric analysis of disagreement in science&quot; seminar held at CWTS at Leiden University, and the editor and reviewers for their helpful comments. We are grateful to Elsevier for making the full-text data available to us.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balietti</surname><given-names>S</given-names></name><name><surname>Mäs</surname><given-names>M</given-names></name><name><surname>Helbing</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On disciplinary fragmentation and scientific progress</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0118747</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0118747</pub-id><pub-id pub-id-type="pmid">25790025</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron</surname><given-names>MG</given-names></name><name><surname>Norman</surname><given-names>DB</given-names></name><name><surname>Barrett</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A new hypothesis of dinosaur relationships and early dinosaur evolution</article-title><source>Nature</source><volume>543</volume><fpage>501</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1038/nature21700</pub-id><pub-id pub-id-type="pmid">28332513</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertin</surname><given-names>M</given-names></name><name><surname>Atanassova</surname><given-names>I</given-names></name><name><surname>Sugimoto</surname><given-names>CR</given-names></name><name><surname>Lariviere</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The linguistic patterns and rhetorical structure of citation context: an approach using n-grams</article-title><source>Scientometrics</source><volume>109</volume><fpage>1417</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.1007/s11192-016-2134-8</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biglan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>The characteristics of subject matter in different academic areas</article-title><source>Journal of Applied Psychology</source><volume>57</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1037/h0034701</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornmann</surname><given-names>L</given-names></name><name><surname>Wray</surname><given-names>KB</given-names></name><name><surname>Haunschild</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Citation Concept Analysis (CCA): A new form of citation analysis revealing the usefulness of concepts for other researchers illustrated by exemplary case studies including classic books by Thomas S Kuhn and Karl R Popper</article-title><source>Scientometrics</source><volume>122</volume><fpage>1051</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1007/s11192-019-03326-2</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyack</surname><given-names>KW</given-names></name><name><surname>van Eck</surname><given-names>NJ</given-names></name><name><surname>Colavizza</surname><given-names>G</given-names></name><name><surname>Waltman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Characterizing in-text citations in scientific articles: A large-scale analysis</article-title><source>Journal of Informetrics</source><volume>12</volume><fpage>59</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2017.11.005</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruggeman</surname><given-names>J</given-names></name><name><surname>Traag</surname><given-names>VA</given-names></name><name><surname>Uitermark</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Detecting communities through network data</article-title><source>American Sociological Review</source><volume>77</volume><fpage>1050</fpage><lpage>1063</lpage><pub-id pub-id-type="doi">10.1177/0003122412463574</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelvecchi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mystery over universe’s expansion deepens with fresh data</article-title><source>Nature</source><volume>583</volume><fpage>500</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1038/d41586-020-02126-6</pub-id><pub-id pub-id-type="pmid">32669728</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catalini</surname><given-names>C</given-names></name><name><surname>Lacetera</surname><given-names>N</given-names></name><name><surname>Oettl</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The incidence and role of negative citations in science</article-title><source>PNAS</source><volume>112</volume><fpage>13823</fpage><lpage>13826</lpage><pub-id pub-id-type="doi">10.1073/pnas.1502280112</pub-id><pub-id pub-id-type="pmid">26504239</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Heo</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A scalable and adaptive method for finding semantically equivalent cue words of uncertainty</article-title><source>Journal of Informetrics</source><volume>12</volume><fpage>158</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2017.12.004</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The hierarchy of the sciences?</article-title><source>American Journal of Sociology</source><volume>89</volume><fpage>111</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1086/227835</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>S</given-names></name><name><surname>Simon</surname><given-names>G</given-names></name><name><surname>Cole</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Do journal rejection rates index consensus?</article-title><source>American Sociological Review</source><volume>53</volume><elocation-id>152</elocation-id><pub-id pub-id-type="doi">10.2307/2095740</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Gravity’s Kiss: The Detection of Gravitational Waves</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Comte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1856">1856</year><source>The Positive Philosophy of Auguste Comte</source><publisher-name>Calvin Blanchard</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>de Cetina</surname><given-names>TC</given-names></name><name><surname>Reyes</surname><given-names>LP</given-names></name></person-group><year iso-8601-date="1999">1999</year><chapter-title>Skin reactions to transdermal estrogen replacement therapy in a tropical climate</chapter-title><person-group person-group-type="editor"><name><surname>Geary</surname><given-names>M</given-names></name></person-group><source>International Journal of Gynaecology and Obstetrics</source><publisher-name>Harvard University Press</publisher-name><fpage>71</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/s0020-7292(98)00150-7</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Debat</surname><given-names>P</given-names></name><name><surname>Nikiéma</surname><given-names>S</given-names></name><name><surname>Mercier</surname><given-names>A</given-names></name><name><surname>Lompo</surname><given-names>M</given-names></name><name><surname>Béziat</surname><given-names>D</given-names></name><name><surname>Bourges</surname><given-names>F</given-names></name><name><surname>Roddaz</surname><given-names>M</given-names></name><name><surname>Salvi</surname><given-names>S</given-names></name><name><surname>Tollon</surname><given-names>F</given-names></name><name><surname>Wenmenga</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A new metamorphic constraint for the Eburnean orogeny from Paleoproterozoic formations of the Man shield (Aribinda and Tampelga countries, Burkina Faso)</article-title><source>Precambrian Research</source><volume>123</volume><fpage>47</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/S0301-9268(03)00046-9</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dieckmann</surname><given-names>NF</given-names></name><name><surname>Johnson</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Why do scientists disagree? Explaining and improving measures of the perceived causes of scientific disputes</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0211269</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0211269</pub-id><pub-id pub-id-type="pmid">30730902</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doody</surname><given-names>O</given-names></name><name><surname>Condon</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Increasing student involvement and learning through using debate as an assessment</article-title><source>Nurse Education in Practice</source><volume>12</volume><fpage>232</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.nepr.2012.03.002</pub-id><pub-id pub-id-type="pmid">22475508</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>E</given-names></name><name><surname>Gomez</surname><given-names>C</given-names></name><name><surname>McFarland</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Measuring paradigmaticness of disciplines using text</article-title><source>Sociological Science</source><volume>3</volume><fpage>757</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.15195/v3.a32</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>“Positive” results increase down the hierarchy of the sciences</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e10068</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0010068</pub-id><pub-id pub-id-type="pmid">20383332</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname><given-names>D</given-names></name><name><surname>Glänzel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bibliometric evidence for a hierarchy of the sciences</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e66938</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0066938</pub-id><pub-id pub-id-type="pmid">23840557</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>BM</given-names></name><name><surname>Koeberl</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The convincing identification of terrestrial meteorite impact structures: What works, what doesn’t, and why</article-title><source>Earth-Science Reviews</source><volume>98</volume><fpage>123</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.earscirev.2009.10.009</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hargens</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Scholarly consensus and journal rejection rates</article-title><source>American Sociological Review</source><volume>53</volume><elocation-id>139</elocation-id><pub-id pub-id-type="doi">10.2307/2095739</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Temporal representations of citations for understanding the changing roles of scientific pCitations for Understanding the Changing Roles of Scientific Publications</article-title><source>Frontiers in Research Metrics and Analytics</source><volume>3</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/frma.2018.00027</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hyland</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Hedging in Scientific Research Articles</source><publisher-name>John Benjamins Publishing Company</publisher-name><pub-id pub-id-type="doi">10.1075/pbns.54</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalter</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Teratology in the 20th century: Environmental causes of congenital malformations in humans and how they were established</article-title><source>Neurotoxicology and Teratology</source><volume>25</volume><fpage>131</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1016/S0892-0362(03)00010-2</pub-id><pub-id pub-id-type="pmid">12748001</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kitcher</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Advancement of Science: Science Without Legend, Objectivity Without Illusions</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/0195096533.001.0001</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>The Structure of Scientific Revolutions</source><publisher-name>University of Chicago Press</publisher-name><pub-id pub-id-type="doi">10.7208/chicago/9780226458106.001.0001</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lamers</surname><given-names>WS</given-names></name><name><surname>Van Eck</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Measuring disagreement in science</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5148058">https://doi.org/10.5281/zenodo.5148058</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lamont</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>How Professors Think: Inside the Curious World of Academic Judgment</source><publisher-name>Harvard University Press</publisher-name><pub-id pub-id-type="doi">10.4159/9780674054158</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langer</surname><given-names>MC</given-names></name><name><surname>Ezcurra</surname><given-names>MD</given-names></name><name><surname>Rauhut</surname><given-names>OWM</given-names></name><name><surname>Benton</surname><given-names>MJ</given-names></name><name><surname>Knoll</surname><given-names>F</given-names></name><name><surname>McPhee</surname><given-names>BW</given-names></name><name><surname>Novas</surname><given-names>FE</given-names></name><name><surname>Pol</surname><given-names>D</given-names></name><name><surname>Brusatte</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Untangling the dinosaur family tree</article-title><source>Nature</source><volume>551</volume><fpage>E1</fpage><lpage>E3</lpage><pub-id pub-id-type="doi">10.1038/nature24011</pub-id><pub-id pub-id-type="pmid">29094688</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larivière</surname><given-names>V</given-names></name><name><surname>Ni</surname><given-names>C</given-names></name><name><surname>Gingras</surname><given-names>Y</given-names></name><name><surname>Cronin</surname><given-names>B</given-names></name><name><surname>Sugimoto</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bibliometrics: Global gender disparities in science</article-title><source>Nature</source><volume>504</volume><fpage>211</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1038/504211a</pub-id><pub-id pub-id-type="pmid">24350369</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Latour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Science in Action: How to Follow Scientists and Engineers Through Society</source><publisher-name>Harvard University Press</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Millan</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Multi-target strategies for the improved treatment of depressive states: Conceptual foundations and neuronal substrates, drug discovery and therapeutic application</article-title><source>Pharmacology &amp; Therapeutics</source><volume>110</volume><fpage>135</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/j.pharmthera.2005.11.006</pub-id><pub-id pub-id-type="pmid">16522330</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miranda</surname><given-names>R</given-names></name><name><surname>Garcia-Carpintero</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Overcitation and overrepresentation of review papers in the most cited papers</article-title><source>Journal of Informetrics</source><volume>12</volume><fpage>1015</fpage><lpage>1030</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2018.08.006</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moravcsik</surname><given-names>MJ</given-names></name><name><surname>Murugesan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Some results on the function and quality of citations</article-title><source>Social Studies of Science</source><volume>5</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1177/030631277500500106</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munro</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Lipid rafts: Elusive or illusive?</article-title><source>Cell</source><volume>115</volume><fpage>377</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/s0092-8674(03)00882-1</pub-id><pub-id pub-id-type="pmid">14622593</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Birn</surname><given-names>RM</given-names></name><name><surname>Handwerker</surname><given-names>DA</given-names></name><name><surname>Jones</surname><given-names>TB</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The impact of global signal regression on resting state correlations: Are anti-correlated networks introduced?</article-title><source>NeuroImage</source><volume>44</volume><fpage>893</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.036</pub-id><pub-id pub-id-type="pmid">18976716</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>sci-text-disagreement</data-title><version designator="swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3">swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:44641d8369477d44432fdf50b2eae38e5d079742;origin=https://github.com/murrayds/sci-text-disagreement;visit=swh:1:snp:5695398f6bd0811d67792e16a2684052abe9dc37;anchor=swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3">https://archive.softwareheritage.org/swh:1:dir:44641d8369477d44432fdf50b2eae38e5d079742;origin=https://github.com/murrayds/sci-text-disagreement;visit=swh:1:snp:5695398f6bd0811d67792e16a2684052abe9dc37;anchor=swh:1:rev:b361157a9cfeb536ca255422280e154855b4e9a3</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Nature Methods</collab></person-group><year iso-8601-date="2016">2016</year><article-title>The power of disagreement</article-title><source>Nature Methods</source><volume>13</volume><elocation-id>185</elocation-id><pub-id pub-id-type="doi">10.1038/nmeth.3798</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>JM</given-names></name><name><surname>Mordaunt</surname><given-names>M</given-names></name><name><surname>Lopez</surname><given-names>P</given-names></name><name><surname>Uppala</surname><given-names>A</given-names></name><name><surname>Rosati</surname><given-names>D</given-names></name><name><surname>Rodrigues</surname><given-names>NP</given-names></name><name><surname>Grabitz</surname><given-names>P</given-names></name><name><surname>Rife</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>scite: a smart citation index that displays the context of citations and classifies their intent using deep learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.15.435418</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolaisen</surname><given-names>J</given-names></name><name><surname>Frandsen</surname><given-names>TF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Consensus formation in science modeled by aggregated bibliographic coupling</article-title><source>Journal of Informetrics</source><volume>6</volume><fpage>276</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2011.08.001</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oreskes</surname><given-names>N</given-names></name><name><surname>Conway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Climate Change</source><publisher-name>Bloomsbury Publishing</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popper</surname><given-names>KR</given-names></name><name><surname>Hudson</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Conjectures and refutations</article-title><source>Physics Today</source><volume>16</volume><fpage>80</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1063/1.3050617</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radicchi</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>In science “there is no bad publicity”: Papers criticized in comments have high scientific impact</article-title><source>Scientific Reports</source><volume>2</volume><elocation-id>815</elocation-id><pub-id pub-id-type="doi">10.1038/srep00815</pub-id><pub-id pub-id-type="pmid">23139864</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rife</surname><given-names>SC</given-names></name><name><surname>Rosati</surname><given-names>D</given-names></name><name><surname>Nicholson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>scite: The next generation of citations</article-title><source>Learned Publishing</source><volume>34</volume><fpage>454</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1002/leap.1379</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarewitz</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The voice of science: Let’s agree to disagree</article-title><source>Nature</source><volume>478</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1038/478007a</pub-id><pub-id pub-id-type="pmid">21979007</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shapin</surname><given-names>S</given-names></name><name><surname>Schaffer</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Leviathan and the Air-Pump: Hobbes, Boyle, and the Experimental Life</source><publisher-name>Princeton University Press</publisher-name><pub-id pub-id-type="doi">10.1515/9781400838493</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shwed</surname><given-names>U</given-names></name><name><surname>Bearman</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The temporal structure of scientific consensus formation</article-title><source>American Sociological Review</source><volume>75</volume><fpage>817</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1177/0003122410388488</pub-id><pub-id pub-id-type="pmid">21886269</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shwed</surname><given-names>U</given-names></name><name><surname>Bearman</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Symmetry is beautiful</article-title><source>American Sociological Review</source><volume>77</volume><fpage>1064</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1177/0003122412463018</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Small</surname><given-names>H</given-names></name><name><surname>Tseng</surname><given-names>H</given-names></name><name><surname>Patek</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Discovering discoveries: Identifying biomedical discoveries using citation contexts</article-title><source>Journal of Informetrics</source><volume>11</volume><fpage>46</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2016.11.001</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Small</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Characterizing highly cited method and non-method papers using citation contexts: The role of uncertainty</article-title><source>Journal of Informetrics</source><volume>12</volume><fpage>461</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2018.03.007</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Small</surname><given-names>H</given-names></name><name><surname>Boyack</surname><given-names>KW</given-names></name><name><surname>Klavans</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Citations and certainty: A new interpretation of citation counts</article-title><source>Scientometrics</source><volume>118</volume><fpage>1079</fpage><lpage>1092</lpage><pub-id pub-id-type="doi">10.1007/s11192-019-03016-z</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smolin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>The Trouble with Physics: The Rise of String Theory, the Fall of a Science, and What Comes Next</source><publisher-name>Mariner Books</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szarvas</surname><given-names>G</given-names></name><name><surname>Vincze</surname><given-names>V</given-names></name><name><surname>Farkas</surname><given-names>R</given-names></name><name><surname>Móra</surname><given-names>G</given-names></name><name><surname>Gurevych</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cross-genre and cross-domain detection of semantic uncertainty</article-title><source>Computational Linguistics</source><volume>38</volume><fpage>335</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1162/COLI_a_00098</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tannen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Agonism in academic discourse</article-title><source>Journal of Pragmatics</source><volume>34</volume><fpage>1651</fpage><lpage>1669</lpage><pub-id pub-id-type="doi">10.1016/S0378-2166(02)00079-6</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Teufel</surname><given-names>S</given-names></name><name><surname>Siddharthan</surname><given-names>A</given-names></name><name><surname>Tidhar</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><conf-name>Automatic classification of citation function</conf-name><article-title>Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</article-title><fpage>103</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.3115/1610075.1610091</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Traag</surname><given-names>VA</given-names></name><name><surname>Waltman</surname><given-names>L</given-names></name><name><surname>van Eck</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From Louvain to Leiden: Guaranteeing well-connected communities</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>5233</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-41695-z</pub-id><pub-id pub-id-type="pmid">30914743</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Valenzuela</surname><given-names>M</given-names></name><name><surname>Ha</surname><given-names>V</given-names></name><name><surname>Etzioni</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Identifying meaningful citations</article-title><source>AAAI workshop scholarly big data</source><ext-link ext-link-type="uri" xlink:href="https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/download/10185/10244">https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/download/10185/10244</ext-link><date-in-citation iso-8601-date="2015-12-11">December 11, 2015</date-in-citation></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Eck</surname><given-names>NJ</given-names></name><name><surname>Waltman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Software survey: VOSviewer, a computer program for bibliometric mapping</article-title><source>Scientometrics</source><volume>84</volume><fpage>523</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1007/s11192-009-0146-3</pub-id><pub-id pub-id-type="pmid">20585380</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Whitley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>The Intellectual and Social Organization of the Sciences</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wuchty</surname><given-names>S</given-names></name><name><surname>Jones</surname><given-names>BF</given-names></name><name><surname>Uzzi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The increasing dominance of teams in production of knowledge</article-title><source>Science</source><volume>316</volume><fpage>1036</fpage><lpage>1039</lpage><pub-id pub-id-type="doi">10.1126/science.1136099</pub-id><pub-id pub-id-type="pmid">17431139</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>De Roeck</surname><given-names>A</given-names></name><name><surname>Gervasi</surname><given-names>V</given-names></name><name><surname>Willis</surname><given-names>A</given-names></name><name><surname>Nuseibeh</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><conf-name>Speculative requirements: Automatic detection of uncertainty in natural language requirements</conf-name><article-title>20th IEEE International Requirements Engineering Conference</article-title><fpage>11</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/RE.2012.6345795</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72737.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Measuring Disagreement in Science&quot; to <italic>eLife</italic> for consideration as a Feature Article. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by the <italic>eLife</italic> Features Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Chaomei Chen; Iana Atanassova.</p><p>The reviewers and editors have discussed the reviews and we have drafted this decision letter to help you prepare a revised submission.</p><p>Summary:</p><p>The paper focuses on a study of citation instances associated with disagreements in scientific publications. Qualified citation instances (citances) are identified based on two sets of terms: signal terms and filter terms. Signal terms indicate some forms of disagreement such as controversies and debates, whereas filter terms characterize common elements of scientific publications such as studies, ideas, methods, and results. Citances are validated by two coders from the authors. ScienceDirect articles are matched to the Web of Science articles so that matched articles can be classified into 800+ meso-level fields. The distribution and the rate of appearances of valid citances are presented among other quantitative and qualitative analyses.</p><p>The topic is important, and one important strength of the paper is the size of the dataset that is processed (over 4M papers in full text) and also the fact that the papers have been already classified into disciplinary fields and 817 meso-level fields by the previous work of the authors. This constitutes a standpoint that allows to perform large scale analyses and draw conclusions for the different disciplines.</p><p>The background of the work is presented generally well, although more detailed comparisons and differentiations are desirable to position the study in the literature in terms of the scope and depth with reference to relevant works. The method and the execution of the study is generally in good order. On the other hand, the presentation, especially some of the interpretations and some of the claims, needs attention in a number of places.</p><p>Essential revisions:</p><p>1) The title needs attention for a number of reasons. &quot;Measuring Disagreement in Science&quot; is an overstatement because, as the authors state in the Discussion section, their study is rather &quot;an attempt at measuring&quot; or &quot;a proposal for a method of measurement&quot;.</p><p>Also, &quot;scientific literature&quot; would be more appropriate than &quot;science&quot; because the study is limited to scientific literature and other forms of scientific inquiries are beyond the scope of the study.</p><p>Similarly, &quot;disagreement in citation contexts&quot; would be more appropriate than &quot;disagreement&quot; because many other forms of disagreement are outside the scope of the study.</p><p>2) More details are needed in some places to allow other researchers to replicate the procedure. For example:</p><p>Line 138: Why was the Crossref API used to identify articles when the ScienceDirect database is hosted at CWTS?</p><p>Line 152: Why are these meso-level fields used? Are there alternatives?</p><p>Line 193: What are the sources of the preliminary signal terms? What are the specific steps for obtaining synonyms, e.g., using any standard and widely available resources such as WordNet?</p><p>Line 213. &quot;filter terms within a four-word window of the signal&quot; This expression seems a bit of ambiguous. Does it mean a filter term must be found no more than two words away from the signal term as illustrated by the following scenarios?</p><p>Some four-word windows of {signal}:</p><p>{filter} word1 word2 {signal}</p><p>{signal} word1 word2 {filter}</p><p>Presumably such windows do not contain sentence boundaries, e.g., &quot;… word1 {filter}. {signal} word2 …&quot; (since the citances are single sentences, correct?). If that is the case, it should be helpful if you can clarify this explicitly.</p><p>Line 222: How exactly was the selection of citances randomized?</p><p>3) The authors have cleverly shifted the issue of negative citation to the broader concept of disagreement. They have analyzed a corpus of sentences that all contain a citation, and their aim is to capture both paper-level and community-level disagreement. However, the example of community-level disagreement given Line 188 is not necessarily an instance of disagreement between the cited works/authors, as the authors state it. It might be (it is, actually, most of the times) a list of works that do mention the existence of a debate/controversy about a topic. I think it is a typical example of a citing author agreeing with the cited authors that there is disagreement on a topic ( = agreeing on disagreement). Choosing sentences with citations and cue terms marking disagreement does not in any way guarantee that the authors are expressing disagreement with the works they cite, but merely that there is disagreement in the literature on a topic at a given time. One may therefore ask what is the point of restricting the analysis to sentences with citations and not addressing the problem from the point of view of citation polarity or function, especially if the aim is to group together paper-level and community-level disagreement. I think the citations have the same function here as filter terms, but cannot be seen as the targets of the disagreement, as they are indeed line 373 and in some sections of the Supporting Information part.</p><p>4) I understand that precision is the priority in this study. Nevertheless, some equally useful vocabularies are not included in the list in Table 2. For example, commonly used terms that are missing from the list for 'studies' include research, investigation, inquiry, just to name a few. A more extensive list would certainly improve the hit rate. Similarly, some important omissions from the list for 'ideas' include concept and claim.</p><p>Please either redo this part of the analysis with more terms, or discuss how this is a limitation of the study.</p><p>5) Related to point 4, a quick review of the available dataset and the limitations provided by the authors show that the precision may not be good either. The authors claim that false positives are marginal. They may be right. But when it comes to making analyses at the journal level, this can distort the result somewhat. I will take just one example for lack of space: Figure 3, Soc and Hum, Electoral studies (which is emphasized in the figure) -&gt; with a manual check, I identify 37% of the citations as false positives for this journal. The use of syntactic parsing could have filtered out the false positives from all the examples citing &quot;presidential debates&quot; for example. The problem is that we don't know the amount of false positives at all.</p><p>6) Is there an underlying principle that leads to the distinction between the paper- and community-level disagreements?</p><p>7) Line 260: Percent valid is defined as the percentage of citances labeled as valid by both coders.</p><p>This seems very subjective. If different individuals were chosen as the coders, then we may end up with possibly quite different results because it is quite conceivable that we can find another pair of coders who may make different judgements. Are there alternatives to make this part of the procedure more systematic and reproducible?</p><p>8) Line 402-403: prioritize precision … relatively rare.</p><p>If the methodology prioritizes precision, then your results cannot be used to support a conclusion that it is relatively rare because one of the many possible consequences of your priority choice is to lower the rate of appearances of qualified instances. If you expand the signal terms and filter terms, then the rate is likely to increase.</p><p>9) Line 420: a field is increasingly becoming consensual … (Smolin, 2007).</p><p>This interpretation is not particularly convincing. A consensual field is a dying field unless it finds new driving forces. It is essential to maintain the healthy growth of a field as well as to the career and social dynamics of individual researchers who choose the work in the field. On the other hand, consensus at lower levels such as topic levels can be reached without fundamentally damaging the growth of a field. I wonder what you would see if you normalize instances by article and/or by all citances (including non-disagreement citances).</p><p>10) Line 474. The framework needs more clarification. For example, how is it related to previous works on negations and uncertainties? For example, manually crafted signal terms vs. black-box approaches, especially for potentially subsequent expansions, coder-based validity vs information theoretic approaches, citer-citee disagreement vs incommensurable paradigms (one may never cite the other from an incommensurable paradigm), and the prevalence of disagreements expressed outside citances.</p><p>11) Please revise the manuscript at appropriate places to address the following points:</p><p>i) the study was restricted to English-language articles.</p><p>ii) the absence of a citation can be an even stronger marker of disagreement than a negative citation.</p><p>iii) disagreements in science can take place in other venues, such as conferences, and in other article types (such as book reviews).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72737.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The title needs attention for a number of reasons. &quot;Measuring Disagreement in Science&quot; is an overstatement because, as the authors state in the Discussion section, their study is rather &quot;an attempt at measuring&quot; or &quot;a proposal for a method of measurement&quot;.</p><p>Also, &quot;scientific literature&quot; would be more appropriate than &quot;science&quot; because the study is limited to scientific literature and other forms of scientific inquiries are beyond the scope of the study.</p><p>Similarly, &quot;disagreement in citation contexts&quot; would be more appropriate than &quot;disagreement&quot; because many other forms of disagreement are outside the scope of the study.</p></disp-quote><p>Thank you for this suggestion. We agree that the title of our project needs to be changed to better reflect the nature of our work. However, we also believe that our paper it relevant to the broad and diverse audience at <italic>eLife</italic>, and that an overly-technical title would limit its appeal.</p><p>Recognizing this tension, we have updated our title to read “Investigating Disagreement in the Scientific Literature”, which we believe appropriately balances the interests of the reviewers and the potential readers of our work.</p><disp-quote content-type="editor-comment"><p>2) More details are needed in some places to allow other researchers to replicate the procedure. For example:</p><p>Line 138: Why was the Crossref API used to identify articles when the ScienceDirect database is hosted at CWTS?</p><p>Line 152: Why are these meso-level fields used? Are there alternatives?</p><p>Line 193: What are the sources of the preliminary signal terms? What are the specific steps for obtaining synonyms, e.g., using any standard and widely available resources such as WordNet?</p><p>Line 213. &quot;filter terms within a four-word window of the signal&quot; This expression seems a bit of ambiguous. Does it mean a filter term must be found no more than two words away from the signal term as illustrated by the following scenarios?</p><p>Some four-word windows of {signal}:</p><p>{filter} word1 word2 {signal}</p><p>{signal} word1 word2 {filter}</p><p>Presumably such windows do not contain sentence boundaries, e.g., &quot;… word1 {filter}. {signal} word2 …&quot; (since the citances are single sentences, correct?). If that is the case, it should be helpful if you can clarify this explicitly.</p><p>Line 222: How exactly was the selection of citances randomized?</p></disp-quote><p>Thank you for these suggestions. We have made the following edits to our manuscript, which have improved the clarity of our procedure</p><p>1. We have updated the text to clarify the nature of our database and the procedure used to originally collect the data and build our database: “We sourced data from an Elsevier ScienceDirect corpus that was also used in a previous study (Boyack et al., 2018) and that is hosted at the Centre for Science and Technology Studies (CWTS) at Leiden University. (…) The Elsevier ScienceDirect corpus that was used was constructed in the following way. First, the Crossref REST API was used to identify all articles published by Elsevier. The full-text of these articles was subsequently downloaded from the Elsevier ScienceDirect API (Article Retrieval API) in XML format.”</p><p>2. We have expanded on the selection of the meso-level fields: “In this hierarchical classification, each article published between 2000 and 2015 and indexed in the Web of Science was algorithmically assigned to a single micro-level scientific field, each of which are in turn members of one of 817 meso-level fields. It is at this meso-level that we perform our most detailed analyses, the categories being fine-grained enough to provide insights into local communities while also large enough to contain a sufficient number of citances. A further benefit of this approach to clustering is that each meso-level field, and each individual publication, can be directly grouped into one of five broad fields: <italic>Biomedical and Health Sciences</italic>, <italic>Life and Earth Sciences</italic>, <italic>Mathematics and Computer Science</italic>, <italic>Physical Sciences and Engineering</italic>, and <italic>Social Sciences and Humanities</italic>. Linking our dataset to this classification system resulted in a subset of 3,883,563 papers containing 118,012,368 citances, spanning 2000 to 2015.”</p><p>3. We have edited the text in this section to read “A variety of approaches can be used to generate these terms, and our approach is not dependent on any particular strategy. Here, we create a preliminary set of signal terms through an intensive iterative process of manually identifying, classifying, validating, and deliberating on strategies for identifying instances of disagreement. This took place over several meetings, utilizing multiple strategies to generate signal words, including sourcing cues used in related work (e.g., Bertin et al., 2016; Chen et al., 2018), expanding this list with synonyms from online thesauruses, and ranking them by their frequency among citation sentences. This inductive process included several rounds of deliberation, manual annotation, and tests of inter-rater reliability in order to generate a robust list of candidate signal terms. The terms are intended to have high validity, but are not considered comprehensive.”. In addition to this clarification, we note that a key feature of our analysis involves <italic>evaluating</italic> each generated cue word, and setting a specific threshold for their inclusion on our analysis, which ensures a high-degree of precision in our results.</p><p>4. We have adjusted the line to read “we also queried for citances containing both the signal terms along with at least one of four sets of filter terms, with no more than four words separating signal and filter.”</p><p>5. We have adjusted the line to read “From each set of results returned by the 65 queries, we selected 50 sentences for validation using simple random sampling without replacement (only 40 citances existed for “no consensus” +”ideas”), resulting in over 3,000 queried sentences.” which makes it clear that we are sampling from all queried results.</p><disp-quote content-type="editor-comment"><p>3) The authors have cleverly shifted the issue of negative citation to the broader concept of disagreement. They have analyzed a corpus of sentences that all contain a citation, and their aim is to capture both paper-level and community-level disagreement. However, the example of community-level disagreement given Line 188 is not necessarily an instance of disagreement between the cited works/authors, as the authors state it. It might be (it is, actually, most of the times) a list of works that do mention the existence of a debate/controversy about a topic. I think it is a typical example of a citing author agreeing with the cited authors that there is disagreement on a topic ( = agreeing on disagreement). Choosing sentences with citations and cue terms marking disagreement does not in any way guarantee that the authors are expressing disagreement with the works they cite, but merely that there is disagreement in the literature on a topic at a given time. One may therefore ask what is the point of restricting the analysis to sentences with citations and not addressing the problem from the point of view of citation polarity or function, especially if the aim is to group together paper-level and community-level disagreement. I think the citations have the same function here as filter terms, but cannot be seen as the targets of the disagreement, as they are indeed line 373 and in some sections of the Supporting Information part.</p></disp-quote><p>Thank you for your feedback. We attempted to summarize your points as follows, and respond to each of them in turn:</p><p>1. The example provided for <italic>community disagreement</italic> is not a valid instance of disagreement, as the authors have stated it</p><p>2. That “agreeing on disagreeing” can confound the author’s analysis</p><p>3. Given (1) and (2), that there is no reason to restrict to citation sentences only</p><p>4. That this presents challenges for understanding citations as <italic>targets</italic> of disagreement, as presented in the results and in the supporting information</p><p>1) The example provided for <italic>community disagreement</italic> is not a valid instance of disagreement, as the authors have stated it</p><p>The goal of our paper is not strictly to identify instances of targeted disagreement, but rather to assess, to a reasonable extent, the total amount of disagreement across scientific fields, based on traces within the scientific literature. <italic>Paper-level</italic> disagreements, represent direct, targeted disagreement between a citing and a cited author, which is what most people may envision when they conceive of scientific disagreements.</p><p>However, in our early manual analysis of citation data, we recognized the common case in which articles were rhetorically positioned as being in disagreement with one another by a citing author, even if the articles themselves did not explicitly disagree with one another. This kind of <italic>community disagreement</italic> is an indirect trace of disagreement within a field. Consider the toy example used in our paper:</p><p>“There remains controversy in the scientific literature over whether or not coffee is associated with an increased risk of cancer [1,2].”</p><p>In this case, both references [1] and [2] are <italic>in disagreement</italic> with one another, though they may not have even cited one another. However, the citing author has positioned them as being in disagreement, and thus indicates the presence of controversy in their research topic.</p><p>Our definition of disagreement can of course be expanded in future work, as well as differentiation made between the categories. However, we argue that community disagreement, of the type shown in our example, operates as a valid instance of disagreement, as we define it in our paper. We have edited our definition of community-level disagreement, as follows, in an attempt to clarify our position,</p><p>“Community-level disagreement, in contrast, refers to the situation in which a citing publication, without explicitly disagreeing with a cited publication, instead draws attention to a controversy or lack of consensus in the larger body of literature. Including community-level disagreement allows us to identify indirect traces of disagreement in a field, even in the absence of explicit disagreement between the referenced authors, or between the citing and cited papers.”</p><p>2) That “agreeing on disagreeing” can confound the author’s analysis</p><p>You offer an important concern, however we also argue that this fits within our notion of disagreement, as we define it in our manuscript. To demonstrate, we have slightly modified our toy example of community disagreement, as follows,</p><p>“A recent review of studies assessing the potential link between coffee consumption and cancer risk has observed continued controversy.”</p><p>In this modified example, the sentence is explicitly citing a single review article, and aggreging with its finding concerning “controversy in the literature…”. You are correct that this is not an instance of direct, <italic>paper-level</italic> disagreement, as the citing author is agreeing with cited paper. However, still fits within our definition of <italic>community-level disagreement,</italic> as it still indicates the presence of disagreement surrounding the association of coffee and cancer.</p><p>To better clarify our definition, we have added the above example citation sentence to our definition of community-level disagreement.</p><p>(3) Given (1) and (2), that there is no reason to restrict to citation sentences only</p><p>We believe that our response to (1) and (2) in this comment demonstrates that typical cases of community disagreement involve the citing author either (a) illustrating disagreement between multiple cited papers, or (b) citing a disagreement in a field as evidenced in another paper, such as a review article. Both (a) and (b) involve the citing author making use of past literature, and so it is natural to limit to in-text citations.</p><p>However, you are correct that these traces of community disagreement may not be limited to citation sentences. For example, consider the following (entirely artificial) paragraph that highlights two different ways in which relevant disagreement information appears <italic>outside</italic> of a citation sentence, which I label (c) and (d) respectively.</p><p>“(c) There is controversy over the link between tea and cancer, partially due to a lack of studies in the area. (d) The link between coffee and cancer, in contrast, has received more intensive study. The findings of these studies, however, are often contradictory and conflicting. “</p><p>Here, (c) demonstrates the case where controversy is remarked upon without any citations. We agree with you, that these could be legitimate instances of disagreement that are not picked up by our method. However, we maintain that valid instances of disagreement are far more likely to occur within a citation context than without. Viewed in this way, we agree that citations function similar to a filter term, providing more precise instances of disagreement. Future work should build upon our results utilizing <italic>all</italic> the text to identify instances of community disagreement. However, in our present study we argue that our emphasis of the non-exhaustiveness of our method, focusing instead on precision and generating a reasonable “first estimation” of disagreement. Towards this goal, restricting to only those sentences containing citations is reasonable.</p><p>In contrast, the two sentences labeled by (d) demonstrate the possibility that surrounding sentences could change the meaning of the citation sentence, signaling disagreement and controversy that is not apparent in the citance alone. We chose to limit our analysis to only a single citation sentence to simplify annotation and analysis. Early on in this project, we did assess the potential of the sentences immediately before and following the citation sentence, however we found that doing so introduced technical and conceptual challenges. We also did not find these wider citation contexts to benefit classification of disagreement to a significant extent. More sophisticated tools, such as utilizing co-reference identification, could be used in the future to automatically delineate relevant citation sentences.</p><p>With these considerations, we added the following text to our limitations section,</p><p>“Given our focus on citation sentences, we are not able to identify traces of disagreements that occur without explicit reference to past literature, or those that can only be classified as disagreement with surrounding sentences as context.”</p><p>4) That this presents challenges for understanding citations as <italic>targets</italic> of disagreement, as presented in the results and in the supporting information</p><p>Thank you for this point. The majority of our analysis concerns only assessing the extent of disagreement in different fields of science. However, you are correct that our notion of <italic>community-level</italic> disagreement confounds our analysis of <italic>issued</italic> and <italic>received</italic> disagreement citations, and section S6 of our supporting information.</p><p>It is an acknowledged limitation of our work that we are not able to discriminate between <italic>paper-level</italic> and <italic>community-level</italic> disagreement. We do not believe that this limitation significantly impacts our main results. In our discussion of the papers that most issued or received disagreement citations, we find most results, and their citation sentences, to be reasonable, apart from noted methodological artifacts.</p><p>In light of this limitation, we have made the following changes to our manuscript:</p><p>– In our Results section, we have rephrased the paragraph discussing the cited paper perspective to state “Considering the cited paper perspective—those papers that received the most paper-level disagreement citations or were referenced the most in the context of community disagreement”, which draws attention to the difference in disagreement type when interpreting these results.</p><p>– Throughout the paper, we have replaced “received disagreement citations” with the phrase “cited in the context of disagreement”, which is a phrase more inclusive of both paper and community-level disagreement.</p><p>– We have added text to the end of section S6 in our supporting information, highlighting the potential for community-level disagreement to confound results of the citation analysis, and noting the need for future research to disentangle these definitions.</p><p>– In our conclusion section, we have edited the second-to-last paragraph to read “The general method of generating and manually validating signal and filter terms can also be applied to other scientific phenomena, such as detecting uncertainty (Chen et al., 2018), negativity (Catalini et al., 2015), discovery (Small et al., 2017), or an expanded framework of disagreement (e.g., Moravcsik and Murugesan, 1975; Teufel et al., 2006).”. This mention of an “expanded framework of disagreement” is intended to draw attention to the need for further work to not only disentangle paper- and community-level citation, but also create a more expanded taxonomy of disagreement, all types of which may be relevant to understanding how and why controversy and consensus manifest in science.</p><disp-quote content-type="editor-comment"><p>4) I understand that precision is the priority in this study. Nevertheless, some equally useful vocabularies are not included in the list in Table 2. For example, commonly used terms that are missing from the list for 'studies' include research, investigation, inquiry, just to name a few. A more extensive list would certainly improve the hit rate. Similarly, some important omissions from the list for 'ideas' include concept and claim.</p><p>Please either redo this part of the analysis with more terms, or discuss how this is a limitation of the study.</p></disp-quote><p>Thank you for this suggestion. We have added the following line into our limitation section to highlight the incompleteness of the list, and also the potential for future refinement:</p><p>“Our lists of signal and filter terms are also non-exhaustive, and so their extension in future research would identify more instance of disagreement.”</p><disp-quote content-type="editor-comment"><p>5) Related to point 4, a quick review of the available dataset and the limitations provided by the authors show that the precision may not be good either. The authors claim that false positives are marginal. They may be right. But when it comes to making analyses at the journal level, this can distort the result somewhat. I will take just one example for lack of space: Figure 3, Soc and Hum, Electoral studies (which is emphasized in the figure) -&gt; with a manual check, I identify 37% of the citations as false positives for this journal. The use of syntactic parsing could have filtered out the false positives from all the examples citing &quot;presidential debates&quot; for example. The problem is that we don't know the amount of false positives at all.</p></disp-quote><p>We appreciate your bringing this to our attention. We agree that while our method is precise at the aggregate level, certain field-level results are confounded by false-positives. We acknowledge this as a limitation, and provide an extensive discussion of such disciplinary artifacts in the supporting information. We also argue that a unique advantage of our method, compared to black-box methods commonly used in citance classification, is that phrases like “presidential debates”, can be trivially excluded, mitigating common artifacts in sub-topics. This task is saved for future research, so that our empirical results reflect the exact queries that were manually validated.</p><p>To further emphasize this point, we have included the following line in our Results section, at the end of our paragraph introducing the meso-level field analysis</p><p>“Field-level results must be interested cautiously, however, as our signal terms may mis-classify citances based on disciplinary keywords and jargon (see Supporting Information).”</p><disp-quote content-type="editor-comment"><p>6) Is there an underlying principle that leads to the distinction between the paper- and community-level disagreements?</p></disp-quote><p>Thank you for this question. We introduced these concepts of paper-level and community-level disagreement following our extensive annotation and deliberation of citation data. In our observations, we found that a strict focus on paper-level disagreement, alone, was excluding many valuable traces of disagreement in the literature, that is, those cases of <italic>community-level</italic> disagreement in which an author highlighted and brought attention to an existing disagreement in their field. We have altered the text in the section “Operationalizing disagreement” to use the phrase “We <italic>introduce</italic> an inclusive definition of disagreement…”, to make it clear that this distinction was our own contribution.</p><p>While we do not do so here, we encourage future work to distinguish between paper- and community-level disagreement in their analyses. Based on our experience with these data, <italic>community-level</italic> disagreement is most often associated with the <italic>+studies</italic> filter term, which we find over-represented among the Biomedical and Health Sciences. (Figure SI 2) Further investigation may reveal other notable trends.</p><disp-quote content-type="editor-comment"><p>7) Line 260: Percent valid is defined as the percentage of citances labeled as valid by both coders.</p><p>This seems very subjective. If different individuals were chosen as the coders, then we may end up with possibly quite different results because it is quite conceivable that we can find another pair of coders who may make different judgements. Are there alternatives to make this part of the procedure more systematic and reproducible?</p></disp-quote><p>Thank you for pointing this out. You are correct that this procedure has the potential to introduce subjectivity that could confound results. However we note that our annotation criteria was developed collaboratively among all of the authors on our manuscript, involving extensive deliberation and an iterative process of smaller-scale validation, allowing for a high degree of agreement among coders. This is evidenced in the high degree of % agreement (85.5 percent) and Cohen’s kappa (0.66) among the coders.</p><p>Moreover, for each query, a different combination of coders was randomly selected from among the authors on this paper, such that the effect of any particular combination was mitigated. We edited the first paragraph of the “Query Validation” subsection, to better clarify this process:</p><p>“For each query, two coders were randomly selected from among the seven authors on this paper to manually annotate each citance as a valid or invalid instance of disagreement”</p><p>The study of disagreement in science often necessitates a degree of manual annotation. Ideally, we could recruit and train a greater number of coders, such that each citance could be judged by more than two people. However, given the intensive labor required in this manual annotation, such an ideal is beyond the reach of our team. Still, we maintain that we followed social science best practices to mitigate such biases, including (1) carefully deliberating over our forming a consensus over annotation criteria; (2) mitigating potential subjectivity by randomly assigning combinations of coders to each query, and (3) calculating standard measures of inter-rater reliability.</p><p>We do welcome others to take advantage of our data, including the large dataset of manually-annotated citation sentences that has been made available alongside this manuscript. Readers are encouraged to review our own team’s judgements, or extend their scope.</p><disp-quote content-type="editor-comment"><p>8) Line 402-403: prioritize precision … relatively rare.</p><p>If the methodology prioritizes precision, then your results cannot be used to support a conclusion that it is relatively rare because one of the many possible consequences of your priority choice is to lower the rate of appearances of qualified instances. If you expand the signal terms and filter terms, then the rate is likely to increase.</p></disp-quote><p>Thank you for pointing this out. We agree that our evidence does not support this claim, and so have removed this line from our Discussion section.</p><disp-quote content-type="editor-comment"><p>9) Line 420: a field is increasingly becoming consensual … (Smolin, 2007).</p><p>This interpretation is not particularly convincing. A consensual field is a dying field unless it finds new driving forces. It is essential to maintain the healthy growth of a field as well as to the career and social dynamics of individual researchers who choose the work in the field. On the other hand, consensus at lower levels such as topic levels can be reached without fundamentally damaging the growth of a field. I wonder what you would see if you normalize instances by article and/or by all citances (including non-disagreement citances).</p></disp-quote><p>Thank you for this valuable point. There are many potential interpretations of this finding. However, we also believe that the interpretation we provide is valid, and rests on discussions and concerns provided by authors within the Physics community. We have clarified this section in an attempt to both hedge and expand upon this interpretation,</p><p>“Changes in these structural characteristics may also contribute to the temporal evolution of disagreement. For instance, the decrease of disagreement in physics and engineering may be due to a transition into a period of “normal” science (Kuhn, 1996), as it has been previously argued for certain sub-fields (Smolin, 2007). Increase in collaboration (Wuchty et al., 2007) may also affect the trends, as consensus has to be reached among a larger body of individuals during the research process.”</p><p>Our results predominantly report percentage of disagreement citances over all citances, including non-disagreement citances, which is already normalized. Exceptions are Figure 3 and Figure SI 2, which report observed/expected ratios of the percentage of disagreement per meso-level field or per query, which is in turn based on the earlier (normalized) percentage of disagreement.</p><disp-quote content-type="editor-comment"><p>10) Line 474. The framework needs more clarification. For example, how is it related to previous works on negations and uncertainties? For example, manually crafted signal terms vs. black-box approaches, especially for potentially subsequent expansions, coder-based validity vs information theoretic approaches, citer-citee disagreement vs incommensurable paradigms (one may never cite the other from an incommensurable paradigm), and the prevalence of disagreements expressed outside citances.</p></disp-quote><p>Thank you, we welcome the chance to further clarify our framework. We have edited the closing paragraph of our literature review to read as follows, which we believe helps to clarify the novelty of our work compared to related studies,</p><p>“Building on these studies, we propose a novel approach for the study of disagreement based on a set of manually-validated cue-phrases. We conduct one of the first empirical investigations into the specific notion of <italic>disagreement</italic> in science, and our inclusive definition allows us to capture both explicit disagreement between papers, as well as traces of disagreement in a field. Our cue-phrase based approach is also more transparent and reproducible than black-box machine learning methodologies commonly employed in citation classification, and also extensively validated using over 3,000 citation sentences representing a range of fields. We also extend the scale of past analyses, identifying instances of disagreement across more than four million scientific publications. “</p><p>Additionally, we reflect on the possibility that we miss non-explicit disagreements (i.e, disagreement signaled by the lack of citation between papers) in our response to essential revision 11, below.</p><disp-quote content-type="editor-comment"><p>11) Please revise the manuscript at appropriate places to address the following points:</p><p>i) the study was restricted to English-language articles.</p><p>ii) the absence of a citation can be an even stronger marker of disagreement than a negative citation.</p><p>iii) disagreements in science can take place in other venues, such as conferences, and in other article types (such as book reviews).</p></disp-quote><p>Thank you for these points. In regards to the point (i), our Methods section clarifies that the documents we considered were English-language; additionally, we have clarified this in our abstract.</p><p>In regards to points (ii) and (iii), we have included the following line as part of our first limitation:</p></body></sub-article></article>