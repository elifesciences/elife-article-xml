<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80280</article-id><article-id pub-id-type="doi">10.7554/eLife.80280</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Coding of latent variables in sensory, parietal, and frontal cortices during closed-loop virtual navigation</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-244598"><name><surname>Noel</surname><given-names>Jean-Paul</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5297-3363</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-280922"><name><surname>Balzani</surname><given-names>Edoardo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3702-5856</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-281725"><name><surname>Avila</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-211035"><name><surname>Lakshminarasimhan</surname><given-names>Kaushik J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-281727"><name><surname>Bruni</surname><given-names>Stefania</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-255556"><name><surname>Alefantis</surname><given-names>Panos</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-250788"><name><surname>Savin</surname><given-names>Cristina</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-138210"><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9650-8962</contrib-id><email>da93@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York City</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Center for Theoretical Neuroscience, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>25</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e80280</elocation-id><history><date date-type="received" iso-8601-date="2022-05-14"><day>14</day><month>05</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-24"><day>24</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-10-24"><day>24</day><month>10</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.10.22.465526"/></event></pub-history><permissions><copyright-statement>© 2022, Noel et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Noel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80280-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80280-figures-v2.pdf"/><abstract><p>We do not understand how neural nodes operate and coordinate within the recurrent action-perception loops that characterize naturalistic self-environment interactions. Here, we record single-unit spiking activity and local field potentials (LFPs) simultaneously from the dorsomedial superior temporal area (MSTd), parietal area 7a, and dorsolateral prefrontal cortex (dlPFC) as monkeys navigate in virtual reality to ‘catch fireflies’. This task requires animals to actively sample from a closed-loop virtual environment while concurrently computing continuous latent variables: (i) the distance and angle travelled (i.e., path integration) and (ii) the distance and angle to a memorized firefly location (i.e., a hidden spatial goal). We observed a patterned mixed selectivity, with the prefrontal cortex most prominently coding for latent variables, parietal cortex coding for sensorimotor variables, and MSTd most often coding for eye movements. However, even the traditionally considered sensory area (i.e., MSTd) tracked latent variables, demonstrating path integration and vector coding of hidden spatial goals. Further, global encoding profiles and unit-to-unit coupling (i.e., noise correlations) suggested a functional subnetwork composed by MSTd and dlPFC, and not between these and 7a, as anatomy would suggest. We show that the greater the unit-to-unit coupling between MSTd and dlPFC, the more the animals’ gaze position was indicative of the ongoing location of the hidden spatial goal. We suggest this MSTd-dlPFC subnetwork reflects the monkeys’ natural and adaptive task strategy wherein they continuously gaze toward the location of the (invisible) target. Together, these results highlight the distributed nature of neural coding during closed action-perception loops and suggest that fine-grain functional subnetworks may be dynamically established to subserve (embodied) task strategies.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>navigation</kwd><kwd>active sensing</kwd><kwd>latent variables</kwd><kwd>eye movements</kwd><kwd>multi-area</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1U19 NS118246</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01 NS120407</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01 DC004260</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01MH125571</award-id><principal-award-recipient><name><surname>Savin</surname><given-names>Cristina</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1922658</award-id><principal-award-recipient><name><surname>Savin</surname><given-names>Cristina</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Google faculty award</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Savin</surname><given-names>Cristina</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Primates use their eyes to keep track of spatial goals, and this strategy is reflected by the functional connectivity between a traditionally considered optic flow area (dorsomedial superior temporal area) and prefrontal cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Despite the closed-loop interplay that exists between action and perception in the real world, our understanding of sensory encoding and the neural architectures supporting goal-directed behavior is largely derived from tasks segregating action from perception, and only sporadically requiring motor output. Arguably, the limited purview of this traditional approach has hindered our ability to understand neural coding for dynamic and flexible behaviors (see, e.g., <xref ref-type="bibr" rid="bib18">Cisek and Kalaska, 2010</xref>; <xref ref-type="bibr" rid="bib29">Gomez-Marin et al., 2014</xref>; <xref ref-type="bibr" rid="bib61">Pitkow and Angelaki, 2017</xref>; <xref ref-type="bibr" rid="bib74">Yoo et al., 2021</xref>, for similar arguments).</p><p>First, severing the loop between action and perception disrupts the natural timing that exists between sensory events and internal neural dynamics. In natural vision, for example, eye movements dictate the content, relative resolution, and timing of visual input. Indeed, work from active sensing, for example, <xref ref-type="bibr" rid="bib65">Schroeder et al., 2010</xref>; <xref ref-type="bibr" rid="bib73">Yang et al., 2016</xref>, has shown that neural excitability in primary visual cortex (<xref ref-type="bibr" rid="bib10">Barczak et al., 2019</xref>) and the anterior nuclei of the thalamus (<xref ref-type="bibr" rid="bib43">Leszczynski et al., 2020</xref>) are enhanced at saccade offset – precisely when new observations are made. This enhancement is likely mediated by phase-reset of neural oscillations (<xref ref-type="bibr" rid="bib40">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Rajkai et al., 2008</xref>) caused by the shifting gaze. In turn, physical movements of the eyes during sensory processing – an aspect absent in most laboratory, binary decision-making tasks – may not only enhance low-level visual encoding but may also favor certain channels of inter-area communication via local field potential (LFP) phase alignment or other coupling mechanisms (e.g., <xref ref-type="bibr" rid="bib35">Jutras et al., 2013</xref>).</p><p>Second, the emphasis on tasks with poor dynamics, together with a technology-limited focus on studying one area at a time, has limited our ability to explore how within- and across-area communication enables flexible behavior. This has possibly led to a degeneracy in the number of functions ascribed to each neural node. For instance, notorious redundancy has been observed between parietal and frontal cortices with both areas showing similar properties during visual feature categorization (<xref ref-type="bibr" rid="bib71">Swaminathan and Freedman, 2012</xref>), target selection (<xref ref-type="bibr" rid="bib70">Suzuki and Gottlieb, 2013</xref>), visuo-spatial memory (<xref ref-type="bibr" rid="bib14">Chafee and Goldman-Rakic, 1998</xref>), and working memory (<xref ref-type="bibr" rid="bib58">Olesen et al., 2004</xref>), among others (see <xref ref-type="bibr" rid="bib37">Katsuki and Constantinidis, 2012</xref>, for a review). While this redundancy is certainly an adaptive feature (<xref ref-type="bibr" rid="bib52">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Driscoll et al., 2017</xref>), the joint characterization of sensory, parietal, and frontal areas during tasks requiring a flow of cognitive operations typical of daily life (e.g., sensing, memorizing, acting) may offer the possibility to study how these regions differ, and how they interact.</p><p>To tackle these limitations, we have developed a task requiring observers to catch memorized ‘fireflies’ in virtual reality (<xref ref-type="bibr" rid="bib41">Lakshminarasimhan et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Noel et al., 2020</xref>; <xref ref-type="bibr" rid="bib56">Noel et al., 2021</xref>). This goal-directed virtual navigation task addresses many of the limitations of the traditional approach, while remaining rooted in well-established neural elements: motion detection (<xref ref-type="bibr" rid="bib54">Newsome and Paré, 1988</xref>), optic flow processing (<xref ref-type="bibr" rid="bib13">Britten, 2008</xref>), and navigation (<xref ref-type="bibr" rid="bib26">Ekstrom et al., 2018</xref>). Animals first detect a briefly flashed target, much like the blinking of a firefly. Then, they use a joystick controlling linear and angular velocity to navigate via path integration to the memorized location of this target (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Importantly, the observers’ eyes are free to sample from their sensory surroundings, and trials last on the order of 2–3 s. In turn, this task requires integration of sensory evidence over protracted time periods, allows for active sensing, and engages the real-world loop between observations, beliefs, actions, and environmental states (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Importantly, the critical task variables are latent. Namely, observers must dynamically estimate their self-location (by accumulating velocity flow vectors) and that of the target (i.e., a hidden spatial goal). This dynamic computation of latent variables may offer a window into the mechanisms of internal computation (e.g., see <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task and behavioral results.</title><p>(<bold>A</bold>) Behavioral task. Top left: Egocentric perspective of the monkey during the firefly task, emphasizing the ground plane optic flow elements (flashing triangles), the fact that the target disappears (target off), and eye positions (following target position perfectly when the target is on, and then continuing on the correct path but deviating with time). Bottom left: Overhead view of the monkey, starting location (star), accelerating and progressively re-orienting to face the firefly, before de-accelerating and stopping at (left; rewarded) or near (right: unrewarded) the location of the now invisible target. Right: This task involves making observation of the sensory environment (composed of optic flow), using these observations to generate a dynamic belief (of the relative distance to target), and producing motor commands based on the current belief, which in turn updates the state of the environment. Right: Continuous variables are shown for three example trials. (<bold>B</bold>) Top: spatial distribution of target positions across trials. Bottom: monkey trajectories. The orange dot and trajectory are an example trial, maintained throughout the figure. (<bold>C</bold>) Example session. Radial (top) and angular (bottom) endpoint (y-axis) as a function of target (x-axis). Gray dots are individual trials, black line is unity, and blue line is the regression between response and target (slope of 1 indicates no bias). (<bold>D</bold>) Radial (top) and angular (bottom) bias (slope,=1 means no bias, &gt;1 means overshooting, and &lt;1 means undershooting) for every session (transparent blue circles) and monkey (x-axis, dark blue circles are average for each monkey, Q, S, and M). Rightmost circle is the average across monkeys and error bars represent ±1 SEM. Overall, monkeys are fairly accurate but undershoot targets, both radially and in eccentricity. (<bold>E</bold>) Eye trajectories (green = eye position at firefly offset) for an example session, projected onto a two-dimensional ‘screen’. Eyes start in the upper field and gradually converge in the lower center (where the firefly ought to be when they stop). (<bold>F</bold>) Target-tracking index (variance in eye position explained by prediction of fixating on firefly) for an example session as a function of time since firefly onset and offset. (<bold>G</bold>) Average target-tracking index within 1 s for all sessions (light blue) and monkeys (dark blue) showing the monkeys attempt to track the invisible target.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig1-v2.tif"/></fig><p>Here, we leverage this ‘firefly task’ to simultaneously characterize the encoding profiles of single units in sensory (dorsomedial superior temporal area, MSTd), parietal (area 7a), and frontal (dorsolateral prefrontal cortex, dlPFC) areas during closed-loop goal-directed behavior (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We record from these regions (MSTd, 7a, and dlPFC) because they form a series of reciprocally interconnected areas (<xref ref-type="bibr" rid="bib3">Andersen et al., 1990</xref>; <xref ref-type="bibr" rid="bib63">Rozzi et al., 2006</xref>) that are well established in the processing of optic flow for self-motion, sensorimotor transformations, and belief formation/working memory, all critical elements of the task (see, e.g., <xref ref-type="bibr" rid="bib39">Kravitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Constantinidis and Klingberg, 2016</xref>; <xref ref-type="bibr" rid="bib17">Christophel et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Noel and Angelaki, 2022</xref>, for reviews). We observe that all areas probed – including MSTd, a classically considered sensory area – encode latent task variables. Further, global encoding profiles and unit-to-unit couplings suggested a functional subnetwork composed by dlPFC and MSTd (and not area 7a, as would be predicted from anatomy). We suggest this MSTd-dlPFC subnetwork reflects the natural task strategy wherein monkeys continuously gaze toward the location of the (invisible) target.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Monkeys navigate to remembered targets employing natural task strategies</title><p>Targets (‘fireflies’) were displayed for 300 ms on a ground plane composed of triangular optic flow elements that appeared transiently (250 ms) with random orientations. The density of these optic flow elements was 5 elements/m<sup>2</sup>. A single target was presented per trial. The firefly targets were uniformly distributed (across trials) within a radial range of 1–4 m, and an angular eccentricity spanning from –40° (leftward) to 40° (rightward) of visual angle (<xref ref-type="fig" rid="fig1">Figure 1A and B</xref>). Performance feedback was provided at the end of each trial in the form of juice reward for correctly stopping within a reward boundary (see <italic>Methods</italic> for further details).</p><p>Visualizing example trials shows that monkeys followed curvilinear trajectories and stopped near the latent targets (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To quantify their performance, we expressed the monkeys’ trajectory endpoints and firefly locations in polar coordinates, with an eccentricity from straight-ahead (<inline-formula><mml:math id="inf1"><mml:mi>θ</mml:mi></mml:math></inline-formula>) and a radial distance (<inline-formula><mml:math id="inf2"><mml:mi>r</mml:mi></mml:math></inline-formula>). <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows radial (top; <inline-formula><mml:math id="inf3"><mml:mi>r</mml:mi></mml:math></inline-formula> vs. <inline-formula><mml:math id="inf4"><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> slope = 0.90; <italic>R</italic><sup>2</sup>=0.55) and angular (bottom; <inline-formula><mml:math id="inf5"><mml:mi>θ</mml:mi></mml:math></inline-formula> vs. <inline-formula><mml:math id="inf6"><mml:mover accent="true"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> slope = 0.95; <italic>R</italic><sup>2</sup>=0.78) responses as a function of target location for an example session (omitting 18% of trials the animals aborted early or never stopped, see <italic>Methods</italic>). As shown by this example session, and confirmed across three animals and all sessions (Monkey Q, 27 sessions; S, 38 sessions; M, 17 sessions; <xref ref-type="fig" rid="fig1">Figure 1D</xref>), monkeys were on average accurate, but tended to undershoot targets (<inline-formula><mml:math id="inf7"><mml:mi>r</mml:mi></mml:math></inline-formula> vs. <inline-formula><mml:math id="inf8"><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> slope, mean = 0.892, 95% CI=[0.860, 0.914]; <inline-formula><mml:math id="inf9"><mml:mi>θ</mml:mi></mml:math></inline-formula> vs. <inline-formula><mml:math id="inf10"><mml:mover accent="true"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> slope, M=0.792, 95% CI=[0.762, 0.842]). Undershooting targets is, in fact, the optimal strategy given that the uncertainty over one’s own and the firefly’s position scales supra-linearly with time and distance travelled (see <xref ref-type="bibr" rid="bib41">Lakshminarasimhan et al., 2018</xref>).</p><p>Most interestingly, this task affords observers with degrees of freedom that may reveal their innate task strategy. For instance, they are not required to fixate and instead we may use their eye movements as an indicator of their internal beliefs (<xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>). Indeed, over the course of individual trajectories, the monkeys’ eyes move downward and become less eccentric (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), as if tracking the (invisible) target progressively becoming nearer and aligned with straight ahead (<xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>; also see <xref ref-type="bibr" rid="bib34">Ilg and Thier, 2003</xref>; <xref ref-type="bibr" rid="bib33">Ilg and Thier, 1999</xref>). This behavior was quantified by deriving predictions for the binocular position of the observer’s eyes, assuming the monkeys maintained fixation at the center of the target throughout the trial. Then, we expressed a target-tracking index as the square root of the fraction of variance in the observed eye position that was explained by this prediction (see <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>, for further details). An index of 1 implies that the subject consistently looked at the center of the firefly, while a value of 0 denotes a lack of correspondence between target and gaze locations. The target-tracking index was high at firefly onset, highest at firefly offset, and remained above chance throughout the course of the trial (<xref ref-type="fig" rid="fig1">Figure 1F</xref> shows an example session). Across all animals and sessions, the target-tracking index averaged 0.73 (95% CI = [0.72, 0.74]) during the first second of each trial. This suggest that, while not a task requirement, animals developed the strategy of fixating on the visible firefly and then attempted to maintain their gaze on it even after the target had disappeared (<xref ref-type="fig" rid="fig1">Figure 1G</xref>, see <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>, for a full description of this phenomenon and demonstration that pursuing the invisible firefly with our eyes results in improved task performance).</p><p>Altogether, the animals accumulated optic flow velocity signals (<xref ref-type="bibr" rid="bib2">Alefantis et al., 2021</xref>) into an evolving (and largely accurate) estimate of self-position. They used this estimate to path integrate to the location of latent targets over prolonged periods of time. In addition, they seemingly sampled from their environment in an intelligent manner, tracking the (invisible) target with their eyes in what is seemingly an embodied mnemonic strategy.</p></sec><sec id="s2-2"><title>Patterned mixed selectivity across sensory, parietal, and frontal cortices</title><p>We recorded single-unit activity from a large number of neurons in MSTd (231 units), area 7a (3200 units), and dlPFC (823 units) across 82 sessions while monkeys performed the firefly task (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Dorsomedial superior temporal area (MSTd), area 7a, and dorsolateral prefrontal cortex (dlPFC) encode a heterogeneous mixture of task variables.</title><p>(<bold>A</bold>) Schematic of brain areas recorded. (<bold>B</bold>) Raster plots of spiking activity in MSTd (green), 7a (blue), and dlPFC (red). Shaded yellow area represents the time of the target being visible, and black dots represent the timing of movement offset. Trials are sorted from shortest (bottom) to longest (top) duration of the trial. (<bold>C</bold>) Schematic of the Poisson generalized additive model (P-GAM) used to fit spike trains. The task and neural variables used as input to the model were: linear and angular velocity and acceleration, time of movement onset and offset, time of firefly onset, distance and angle from origin and to target, time of reward, vertical and horizontal position of the eyes, and ongoing phase of LFP at theta, alpha, and beta bands. (<bold>D</bold>) Top: random snippet of spiking activity of simultaneously recorded neurons (green = MSTd; blue = 7a; red = dlPFC). Bottom: corresponding cross-validated prediction reconstructed from the P-GAM. The average cross-validated pseudo-<italic>R</italic><sup>2</sup> was 0.072 (see <xref ref-type="bibr" rid="bib19">Colin Cameron and Windmeijer, 1997</xref>). (<bold>E</bold>) Responses from an example MSTd, 7a, and dlPFC neuron (black), aligned to temporal task variables (e.g., time of movement onset and offset), or binned according to their value in a continuous task variable (e.g., linear velocity). Colored (respectively green, blue, and red for MSTd, 7a, and dlPFC) lines are the reconstruction from the reduced P-GAM. The responses are opaque (p&lt;0.01) or transparent (p&gt;0.01), according to whether the P-GAM kernel for the specific task variable is deemed to significantly contribute to the neuron’s firing rate. (<bold>F</bold>) Fraction of neurons tuned to the given task variable, according to brain area. Error bars are 99% CIs, and thus non-overlapping bars indicate a pair-wise significant difference.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Recoding sites.</title><p>Magnetic resonance imaging (MRI) reconstruction of recording sites and pictures from Utah array implants. 3D rendering of the brain is from Monkey S, and all recording sites have been placed on the common reference (dorsomedial superior temporal area [MSTd] in green, area 7a in blue, dorsolateral prefrontal cortex [dlPFC] in red). Location of acute recordings are indicated by spheres, color coded per animal (Monkey S, orange; Monkey Q, purple; Monkey M in green). Location of Utah arrays are indicated by squares. The pictures of the Utah arrays are framed in the color corresponding to the monkey. In turn, Monkey S had recordings performed from Utah arrays in dlPFC and area 7a on the left hemisphere, and from a linear probe in MSTd on the right hemisphere. Monkey Q had recordings performed from a Utah array in 7a on the left hemisphere and from a linear probe in MSTd on the right hemisphere. Monkey M had recordings performed from a linear probe in dlPFC on the right hemisphere. In total, therefore, each area was sampled twice. All 7a recordings were on the left hemisphere and all MSTd recordings were on the right hemisphere (note, given that MSTd is directly ventral to 7a, see <xref ref-type="fig" rid="fig2">Figure 2A</xref>, these cannot be recorded from the same hemisphere if the former area is implanted with an array). From the 823 neurons recorded in dlPFC, 55 were recorded on the right hemisphere in Monkey M. All inter-area coupling analyses (which requires simultaneous recordings) were based on left-hemisphere recordings in 7a and dlPFC, and from right-hemisphere recordings in MSTd. All monkeys were right-handed and used this hand to manipulate the joystick. AS, arcuate sulcus; IPS, intraparietal sulcus; PS, principal sulcus; STS, superior temporal sulcus; LF, lateral fissure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Poisson generalized additive model (P-GAM) controls.</title><p>(<bold>A</bold>) Feature inclusion in the ‘reduced’ model. We generated synthetic spike trains according to real input statistics (e.g., linear and angular velocity profiles, correlated inputs) and tuning functions derived in dorsomedial superior temporal area (MSTd) by the P-GAM (all 17 variables, as in the main text). Next, we attempted to recover these tuning functions, while including or excluding (i.e., zeroing) the tuning to radial distance to target. Across all simulation (n=1344), the P-GAM never excluded the distance to target from its selected model when this variable had not been zeroed (0.0% false negative), and only once failed to exclude this variable from its final selection when the variable had been zeroed (0.0007% false positive). Top row are significant tuning functions to radial distance in MSTd (black) and their recovered shape by the P-GAM (blue) when not excluded. Bottom row are the same units when radial distance (but not other variables) was zeroed. Columns show seven example cells. (<bold>B</bold>) Number of parameters in the ‘full’ and ‘reduced’ model. The full model is the model with all parameters, before any selection is applied. The reduced model is the model where only parameters relating to features that significantly contribute to a neuron’s spiking are kept. By design, the reduced model ought to have an equal or lower number of parameters than the full model. The full model number of parameters varies, according to how many simultaneously recorded neurons there were in a recording session, given that we include unit-to-unit coupling terms. (<bold>C</bold>) Cross-validated pseudo-<italic>R</italic><sup>2</sup> in the full and reduced model. While the reduced model has on average 18.9% the number of parameters as in the full model (<bold>B</bold>) its ability to account for neural activity is comparable to the full model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Effect sizes in the firing rate space for neurons deemed to code for sensorimotor variables.</title><p>Rows are the different sensorimotor variables, in the same order as in <xref ref-type="fig" rid="fig2">Figure 2E and F</xref>. The left column is the difference in evoked firing rate between the population of neurons deemed to significantly code for, or not, a given task variable. Namely, for each neuron we compute the difference in firing rate between the peak and the trough of its tuning function. The populations of significant and non-significant neurons are then log-transformed (to render normally distributed) and Cohen’s <italic>d</italic> is computed (indicated as the title of each subplot). Right column, as for the left column, but while contrasting the mutual information present between firing rates and the given task variable. For reference, Cohen’s <italic>d</italic> &lt; ~0.2 are typically considered weak effects, ~0.5 are considered moderate, and &gt;~0.8 are considered strong effects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Effect sizes in the firing rate space for neurons deemed to code for latent variables.</title><p>Format follows that of <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Effect sizes in the firing rate space for neurons deemed to phase lock to local field potential (LFP).</title><p>Format follows that of <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Effect sizes in the firing rate space for neurons deemed to code for reward and eye position.</title><p>Format follows that of <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 7.</label><caption><title>Speed and direction discrimination index for neurons in dorsomedial superior temporal area (MSTd), 7a, and dorsolateral prefrontal cortex (dlPFC).</title><p>To allow for direct comparison with prior studies, we compute the discrimination index (see <italic>Methods</italic>) for speed (i.e., linear velocity) and direction (i.e., angular velocity) in MSTd (green), 7a (blue), and dlPFC (red). Full triangles at the top indicate the mean of each population recorded from here (in their corresponding color), while the empty blue triangles show the mean in <xref ref-type="bibr" rid="bib4">Avila et al., 2019</xref> (7a recording) and the empty green triangles show the mean in <xref ref-type="bibr" rid="bib15">Chen et al., 2008</xref> (MSTd recordings).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp7-v2.tif"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 8.</label><caption><title>Illustration of spike-local field potential (LFP) phase locking.</title><p>(<bold>A</bold>) Example trials. For each of four example trials (different sessions as well) we show the raw LFP (top), as well as the band-passed version (transparent) and extracted phase (opaque) in theta (green, second row), alpha (orange, third row), and beta (blue, fourth row) ranges. Spikes are represented by dots, and they are placed on the y-axis according to the phase of the ongoing LFP. That is, across rows spikes occur at the same time along the x-axis, but are at different y-locations. If a neuron is phased-locked to LPF in a given range, spikes should predominantly occur at the same y location (as is seen in these examples for the beta band). (<bold>B</bold>) Example sessions. For six example neurons, we show the distribution of phases (x-axis, in radians) at which spikes occurred, throughout the entire session. We show two example neurons that were not modulated by LFP phase (first and second column), two example neurons that were modulated solely by beta frequency phases (third and fourth column), and finally two example neurons that were modulated by phases at theta, alpha, and beta frequencies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp8-v2.tif"/></fig><fig id="fig2s9" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 9.</label><caption><title>Pairwise phase consistency.</title><p>The findings related to spike-local field potential (LFP) phase coupling reported in the main text are based on the Poisson generalized additive model (P-GAM), as are the rest tuning properties reported. However, detecting a correlation between when spikes occur and the phase of LFPs may be biased by a number of factors, for example the firing rate of neurons or the amplitude of LFP oscillations. In turn, we corroborated the spike-LFP phase coupling results by computing the pairwise phase consistency (PPC0), a bias-free estimator (see <xref ref-type="bibr" rid="bib72">Vinck et al., 2010</xref>, for details). This analysis confirmed the findings from the main text in demonstrating greater spike-LFP phase coupling in area 7a, particularly within the beta and alpha ranges. Mean PPC across all neurons and sessions are reported separately by brain region and frequency range. Error bars are SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp9-v2.tif"/></fig><fig id="fig2s10" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 10.</label><caption><title>Stability in the fraction of neurons tuned to different task variables.</title><p>To test the stability in the Poisson generalized additive model (P-GAMs) estimates, we divided our datasets in even and odd trials. We fitted separate P-GAMs and plot the fraction of neurons tuned (y-axis) to each task variable (x-axis) according to brain area (dorsomedial superior temporal area [MSTd], 7a, and dorsolateral prefrontal cortex [dlPFC]) and whether these were odd or even trials. Error bars are 99% CI. Estimates in the fraction of neurons tuned were very stable.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp10-v2.tif"/></fig><fig id="fig2s11" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 11.</label><caption><title>Task engagement drives neural tuning.</title><p>(<bold>A</bold>) Example trials. To demonstrate that the fraction of neurons tuned to different task variables reported in <xref ref-type="fig" rid="fig2">Figure 2F</xref> are driven by task engagement and not purely low-level visual input, in a control experiment (two sessions) we recorded from area 7a (117 neurons) as a monkey first actively engaged in the task (top), and then passively viewed replayed the exact visual input (bottom). We show seven example trials, demonstrating that the linear velocity during active and passive trials matched (same for other task variables, not shown). Instead, the population evoked responses (one trial, average across the entire population of simultaneously recorded neurons) was evident during active but not passive trials. (<bold>B</bold>) Average firing rate. Firing rate (averaged over the entire recording) did not differ between active (x-axis) and passive (y-axis) viewing (blue dots are single cells in 7a, black dot is the mean, dashed black line is identity). (<bold>C</bold>) Fraction of neurons tuned and coupled. The fraction of neurons tuned to different task variables, and the fraction of neurons coupled to each other in area 7a, were blunted (but not entirely absent) during passive viewing. The exceptions were variables related to internal neural dynamics, notably the phase locking of spiking activity to LFP phase in theta, alpha, and beta band.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig2-figsupp11-v2.tif"/></fig></fig-group><p>To quantitatively account for the encoding properties of neurons within this task – wherein no two trials are alike (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> for rasters demonstrating heterogeneity in spike times and trial durations) – we fit spike trains to a Poisson generalized additive model (P-GAM). The P-GAM we leveraged was purposely developed to efficiently and robustly estimate encoding profiles during closed-loop and naturalistic tasks such as the ‘firefly task’ (<xref ref-type="bibr" rid="bib6">Balzani et al., 2020b</xref>). Namely, beyond capturing arbitrary non-linearities and handling collinear predictors (<xref ref-type="bibr" rid="bib23">Dormann et al., 2013</xref>), this encoding model has the strong advantage of inferring marginal confidence bounds for the contribution of each feature to neural responses (see <xref ref-type="bibr" rid="bib6">Balzani et al., 2020b</xref>, for details). This property allows us to identify the minimal set of task variables that each neuron is responsive to (setting statistical significance to p&lt;0.01), while circumventing the need for computationally demanding (and often unstable) model selection procedures – particularly given the large number and time-varying nature of the variables in this task. Indeed, beyond the numerical results in <xref ref-type="bibr" rid="bib6">Balzani et al., 2020b</xref>, demonstrating the ability to recover the ground-truth in artificial data, here we show that the P-GAM will capture the simplest possible interpretation of the neural responses in the statistical regime of our data, that is, even when neurons are weakly firing and predictors are correlated (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><p>In addition to continuous sensorimotor (e.g., linear and angular velocity and acceleration) and latent variables (e.g., distance from origin and to target, <xref ref-type="fig" rid="fig1">Figure 1A</xref>), as well as discrete task events (e.g., time of target onset, as well as movement onset and offset), we included elements of brain dynamics in the encoding model. These internal dynamics are most often not considered in accounting for task-relevant neural responses, yet they fundamentally shape spiking activity. These latter variables included the phase of LFP in different frequency bands (theta: 4–8 Hz; alpha: 8–12 Hz; beta: 12–30 Hz), and causal unit-to-unit coupling filters within (i.e., spike history, 36 ms wide temporal filter) and between units, both within (36 ms wide temporal filter) and across cortical areas (600 ms wide temporal filters, <xref ref-type="fig" rid="fig2">Figure 2C</xref>, see <italic>Methods</italic>). These coupling filters capture noise correlations, that is, the signal-independent relationship between one neuron firing and the likelihood that this same (i.e., spike history) or another neuron (i.e., coupling filter) will fire at a given time delay (see <xref ref-type="bibr" rid="bib31">Hart and Huk, 2020</xref>). In total, the encoding model had 17 (analog or digital) task inputs (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), in addition to hundreds of potential lateral and across region coupling filters (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref> for the distribution of the number parameters in the full and reduced encoding models, and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref> for demonstration that while the reduced model had a fifth of the parameters present in the full model, its cross-validated ability to account for observed spiking was unchanged). The fitted model accounts well for observed spiking activity (see raw and model reconstructed spiking activity of simultaneously recorded units in <xref ref-type="fig" rid="fig2">Figure 2D</xref>, mean pseudo-<italic>R</italic><sup>2</sup>=0.072).</p><p><xref ref-type="fig" rid="fig2">Figure 2E</xref> shows the empirical (black) and P-GAM reconstructed (colored) firing rate to task variables in an example MSTd, 7a, and dlPFC neuron. The striking feature of these examples is that all units responded to, or were modulated by, a multitude of task variables. For instance, each of these example neurons responded at the time of firefly onset. The 7a neuron showed a strong dependency not only to sensorimotor variables, but also to LFP phases (in all frequency ranges). Finally, and perhaps most surprisingly, the MSTd neuron reflected not only sensorimotor and eye position variables, but also latent ones, such as the angular distance to the (invisible) target. We used the P-GAM to factorize the contribution of different elements of the task to a neuron’s overall firing rate (i.e., perform credit assignment). The example neurons in <xref ref-type="fig" rid="fig2">Figure 2E</xref> are opaque (contributes) or transparent (does not contribute) for different task variables, according to the P-GAMs estimate of their factorized contribution. Thus, there may be cases where there are clear evoked responses (e.g., time of movement onset in the dlPFC example neuron, <xref ref-type="fig" rid="fig2">Figure 2E</xref>, bottom) yet the P-GAM estimated the neuron to not be tuned to this variable, likely due to the correlated nature of input statistics during this naturalistic task (i.e., this neuron appears tuned to linear acceleration, which correlates with the time of movement onset). In <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplements 3</xref>–<xref ref-type="fig" rid="fig2s6">6</xref> we quantify effect sizes by comparing the modulation in firing rates, as well as the mutual information between these and a given variable, for the population of neurons significantly coding or not for a given task variable. Similarly, to provide a point of comparison with prior work studying optic flow processing, in <xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref>, we quantify the speed (i.e., liner velocity) and direction (i.e., angular velocity) discrimination index (see <italic>Methods</italic> and e.g., <xref ref-type="bibr" rid="bib15">Chen et al., 2008</xref>; <xref ref-type="bibr" rid="bib4">Avila et al., 2019</xref>) for neurons in MSTd, 7a, and dlPFC.</p><p>The fraction of neurons tuned to different task variables demonstrated a patterned mixed selectivity (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Namely, the fraction of neurons tuned to sensorimotor variables was highest in area 7a (linear and angular velocity, linear and angular acceleration, time of movement onset and offset, all p&lt;0.01; Cohen’s <italic>d</italic> linear acceleration <italic>d</italic>=0.2, all the rest <italic>d</italic>&gt;0.76), while most neurons in MSTd coded for eye position (58.8% and 63.3% respectively for vertical and horizontal eye position). Interestingly, a large fraction of dlPFC neurons also coded for eye position (dlPFC vs. 7a, vertical and horizontal eye position, p&lt;6.4 × 10<sup>–17</sup>, all Cohen’s <italic>d</italic>&gt;3), putatively reflecting the fact that the gaze indexes the internal belief over firefly position (<xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>). Neurons in area 7a showed a strong dependency to the phase of ongoing LFP fluctuations (see <xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref> for further illustrations of this effect as quantified by the P-GAM, and <xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref> for corroborative evidence by pairwise phase consistency, <xref ref-type="bibr" rid="bib72">Vinck et al., 2010</xref>), a fact that was less observed in dlPFC or MSTd (all p&lt;7.1 × 10<sup>–8</sup>, all <italic>d</italic>&gt;1.02). Lastly, and most relevant to the task of path integrating to the location of invisible fireflies, we observed that the greatest fraction of neurons encoding for both path integration (i.e., travelled distance and angle turned) and the distance and angle to the target (i.e., spatial goal) were in dlPFC (ranging from 30.6% to 57.5%). Interestingly, while 7a had more neurons coding for path integration than MSTd (all p&lt;1.6 × 10<sup>–5</sup>, all Cohen’s <italic>d</italic>&gt;0.54), the latter area had a greater fraction of neurons coding for the latent distance and angle to target than 7a did (all p&lt;6.1 × 10<sup>–5</sup>, all Cohen’s <italic>d</italic>&gt;0.41). In the supplement we demonstrate that this coding was stable (contrasting odd vs. even trials; <xref ref-type="fig" rid="fig2s10">Figure 2—figure supplement 10</xref>) and task-relevant (<xref ref-type="fig" rid="fig2s11">Figure 2—figure supplement 11</xref>), in that passive viewing of the same stimuli did not elicit a comparable fraction of neurons tuned to task variables in 7a (passive viewing data in MSTd and dlPFC were unavailable). The fraction of neurons aligned with the phase of LFP in different frequency bands remained stable across passive and active viewing conditions, particularly in the beta band (all frequencies, active vs. passive, p=0.13; beta band, p=0.51). Altogether, the encoding pattern across areas may suggest that while dlPFC is critically involved in estimating the relative distance between self and target, 7a may be preferentially involved in the process of path integration, while somewhat unexpectedly, MSTd may play an important role in keeping track of the latent spatial goal.</p></sec><sec id="s2-3"><title>Cortical codes for path integration and vector coding of spatial goals</title><p>Beyond the frequency with which we observe neurons tuned to the angle and distance from the origin (i.e., path integration) and to the target (i.e., vector coding of spatial goals), we may expect the distributions of preferred distances and angles to also be informative. Of note, distance/angle from origin and to the target are not the reciprocal of one another given that the target location varies on a trial-by-trial fashion. In other words, the travelled distance and the distance to target may correlate within a trial (but need not, given under- vs. overshooting) but certainly do not across trials (e.g., a distance of, say, 100 cm from the origin could corresponding to a whole host of distances from target). In <xref ref-type="fig" rid="fig3">Figure 3A</xref> we show rasters of representative neurons tuned to the distance from origin (example neuron 1) and to target (example neuron 2). The neuron tuned to the distance to target (example neurons 2) is not tuned to a particular distance from origin, but does demonstrate a patterned firing rate, discharging at further distances as the animal travels further. The third example (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) is tuned to movement stopping, and demonstrates a pattern similar to the neuron tuned to distance to target when plotted as a function of distance from origin (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, third vs. fifth panel), but not when visualized as a function of distance to target. The distributions shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref> illustrate that the preferred distances/angles from origin and to target spanned the entire range of angles and distances visited, demonstrating a full basis set covering the feature space.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Preferred angle and distance from origin and to target in dorsomedial superior temporal area (MSTd), 7a, and dorsolateral prefrontal cortex (dlPFC).</title><p>(<bold>A</bold>) Rasters and average firing rate of three example neurons, sorted by their maximal distance from origin and to target. The first example neuron (left) responds at a distance of ~100 cm from origin and is not modulated by distance to target. The second example (middle) responds to a close distance to target (~30 cm). Arrows at the top of these rasters indicate the preferred distance from origin (example 1) and to target (example 2). We include a third example (tuned to movement stop) as a control, demonstrating that responding to a distance near the target and to stopping behavior are distinguishable. (<bold>B</bold>) Heatmaps showing neural responses (y-axis) sorted by preferred angles from origin (top), angle to target (second row), distance from origin (third row), and distance to target (bottom row) for MSTd (green), 7a (blue) and dlPFC (red) in Monkey S (data simultaneously recorded). Darker color indicates higher normalized firing rate. Neurons were sorted based on their preferred distances/angles in even trials and their responses during odd trials is shown (i.e., sorting is cross-validated, see Methods). (<bold>C</bold>) Histograms showing the probability of observing a given preferred angle or distance across all three monkeys. Inverted triangles at the top of each subplot indicate the median. Of note, however, medians may not appropriately summarize distributions in the case of bimodality. (<bold>D</bold>) We clustered the kernels driving the response to angle or distance to origin and from the target. Here, we show 10 representatives from each cluster (thin and transparent lines), as well as the mean of the cluster as a whole (ticker line). The inset quantifies the percentage of tunings within a particular area and for the particular variable that were deemed to belong within the cluster depicted (the most frequent one).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Latent tuning functions (radial and angular distance to target and from origin) encountered in dorsomedial superior temporal area (MSTd) (green), area 7a (blue), and dorsolateral prefrontal cortex (dlPFC) (red).</title><p>Tuning function clusters were determined by Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Ten example tuning functions are plotted per cluster (in semi-transparent), as well as the average of all tuning functions within the given cluster type (opaque and thicker line). Y-axis are firing rates (in Hz) z-scored and x-axis spans the state space of the particular variable, in cm for radial distances and degrees for angles (see top row).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Sensorimotor tuning functions to continuous variables (linear and angular velocity and acceleration) encountered in dorsomedial superior temporal area (MSTd) (green), area 7a (blue), and dorsolateral prefrontal cortex (dlPFC) (red).</title><p>Tuning function clusters were determined by Density-Based Spatial Clustering of Applications with Noise (DBSCAN). For each cluster, 10 example tuning functions are plotted (semi-transparent and background), as well as the average of all tuning functions within the given cluster type (opaque and thicker line). Y-axis are firing rates (in Hz) z-scored and x-axis spans the state space of the particular variable, in cm/s for linear velocity, deg/s for angular velocity, cm/s<sup>2</sup> for linear acceleration, and deg/ s<sup>2</sup> for angular acceleration (see top row).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Temporal kernels (time of movement onset and offset, time of target onset and reward presentation) encountered in dorsomedial superior temporal area (MSTd) (green), area 7a (blue), and dorsolateral prefrontal cortex (dlPFC) (red).</title><p>Tuning function clusters were determined by Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Ten example tuning functions are plotted per cluster (transparent), as well as the average of all tuning functions within the given cluster type (opaque and thicker line). Y-axis are firing rates (in Hz) z-scored and x-axis is time relative to the discrete event, in seconds (see top row).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Tuning functions to eye position variables (vertical and horizontal) encountered in dorsomedial superior temporal area (MSTd) (green), area 7a (blue), and dorsolateral prefrontal cortex (dlPFC) (red).</title><p>Tuning function clusters were determined by Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Ten example tuning functions are plotted per cluster (semi-transparent), as well as the average of all tuning functions within the given cluster type (opaque and thicker line). Y-axis are firing rates (in Hz) z-scored, and x-axis eye position, also in z-score.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig3-figsupp4-v2.tif"/></fig></fig-group><p>Across all animals, angles specifying a 0° offset from the heading at origin were over-represented (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, top row, proportions across all animals), while the 0° offset from target location was under-represented (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, second row). Instead, particularly in area 7a and dlPFC, the distribution of preferred angles to target was bimodal (bimodality coefficients, null = 0.55, MST = 0.57, 7a=0.89, dlPFC = 0.80, 7a and dlPFC p&lt;0.05), with many neurons showing a preference for ~45–90° away from target, either leftward or rightward. This pattern is in stark contrast with observations from the bat’s hippocampus, where vector coding of hidden spatial goals exists and a goal angle of 0° is over-represented (<xref ref-type="bibr" rid="bib64">Sarel et al., 2017</xref>). Speculatively, this reversed pattern of preferences between cortex (here) and hippocampus (<xref ref-type="bibr" rid="bib64">Sarel et al., 2017</xref>) may suggest that while the former plays a role in guiding heading toward the desired state (akin to an error or correction signal), the hippocampus maintains this heading.</p><p>In terms of radial distance, MSTd, area 7a, and dlPFC all showed an over-representation of units coding for distances near to, as opposed to far from, the starting location (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, third row, &lt;~150 cm). On the other hand, we observed a clear differentiation between cortical areas regarding their preferred distance to target. Area 7a showed a strong preference for nearby targets with approximately 50% of units coding for targets within ~30 cm. Neurons in MSTd and dlPFC, on the other hand, responded primarily at intermediary and far distances from the target (~200–400 cm from target; <xref ref-type="fig" rid="fig3">Figure 3C</xref>, fourth row). The preference for distances further from the target in MSTd and dlPFC seemingly concords with their frequent tuning to eye position, and the fact that the eyes attempt to pursue the hidden target (i.e., as if these areas were involved in planning and sampling from a distance).</p><p>Lastly, to depict the full shape of the kernels encoding the distance and angle from the origin and to the target, we performed statistical clustering of these kernels, separately for each area and task variable. <xref ref-type="fig" rid="fig3">Figure 3D</xref> shows the mean and 10 example tuning functions for the most frequently present cluster within each area. The most noticeable difference between areas is that while MSTd and dlPFC prefer distances far from the target, area 7a responds mostly to distances near it (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, bottom row). In <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s4">4</xref> we depict all the different types (i.e., high-dimensional clusters) of tuning functions that exist for all the different task variables (e.g., linear and angular velocity, horizontal and vertical eye position, etc.) in MSTd, area 7a, and dlPFC.</p></sec><sec id="s2-4"><title>Single-cell properties and unit-to-unit coupling suggest two distinct functional subnetworks</title><p>Both the fraction of neurons tuned to different task variables (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and the distribution of preferred angles and distances from origin and to target (<xref ref-type="fig" rid="fig3">Figure 3</xref>) show a surprising degree of coding similarity between MSTd and dlPFC. In turn, to systematically examine how the encoding of all task variables, and not solely a select few, varied across cortical areas, we employed a statistical clustering approach (see, e.g., <xref ref-type="bibr" rid="bib51">Minderer et al., 2019</xref>, for a similar approach).</p><p>First, we leveraged the knowledge of which variables each neuron was significantly tuned to (e.g., <xref ref-type="fig" rid="fig2">Figure 2F</xref>), and attempted clustering neurons based on this binary data, each neuron being defined by a size 17 binary vector (i.e., tuned or not tuned) corresponding to the 17 task variables defined in <xref ref-type="fig" rid="fig2">Figure 2</xref>. This approach (see <italic>Methods</italic> for details) showed that nearly all MSTd (89%) and dlPFC (94%) neurons were categorized as belonging within the same cluster (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, cluster number 1), one that was defined by true mixed selectivity (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top left). In contrast, area 7a neurons appeared in three distinct clusters (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, cluster numbers 1–3, respectively, 36%, 22%, and 31%). Cluster 2 was characterized by a strong selectivity for sensorimotor variables and firing in alignment with specific LFP phases (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top center), while Cluster 3 was characterized by a near complete lack of tuning to latent variables and eye position (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top right). Other cluster types existed, for instance composed of neurons selectively tuned to the ongoing phase in LFP bands but no other task variable (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>, Cluster 4), or driven also by motor onset and offset (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>, Cluster 5). These remaining clusters were, however, less common (~1–5%). This analysis was conducted with the full dataset (4254 neurons in total), yet in the supplement (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>) we confirm that the results are unchanged when subsampling from areas with more neurons (7a and dlPFC) to match the number present in MSTd (231 neurons). Together, this pattern of clustering results based on whether neurons were tuned to different task variables demonstrated a surprising degree of coding similarity between MSTd and dlPFC, which are in turn different from area 7a.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Global single-unit encoding profiles and unit-to-unit coupling properties suggest a common functional role for dorsomedial superior temporal area (MSTd) and dorsolateral prefrontal cortex (dlFPC).</title><p>(<bold>A</bold>) Proportion of neurons being classified into distinct cluster (x-axis, seven total) according to which task parameters they were significantly tuned to. (<bold>B</bold>) Fraction of neurons tuned to each of the 17 task variables (order is the same as in <xref ref-type="fig" rid="fig2">Figure 2C, E, F</xref>) according to cluster. (<bold>C</bold>) Uniform Manifold Approximation and Projection (UMAP) of the tuning function shapes, color coded by brain area (first column) or Density-Based Spatial Clustering of Applications with Noise (DBSCAN) cluster (second, third, and forth column). (<bold>D</bold>) Fraction of neurons whose spiking activity is phase-locked to local field potential (LFP) phases in other areas, in theta (leftmost), alpha (center) and beta (rightmost) bands. An arrow projecting from, for example, MSTd to 7a (center, 0.24), indicates that the neuron in area 7a is influenced by ongoing LFP phase in MSTd. Width of arrows and associated number indicate the proportion of neurons showing the particular property. (<bold>E</bold>) As (<bold>D</bold>) but for unit-to-unit coupling. An arrow projecting from, for example, MSTd to dlPFC indicates that the firing of a neuron in MSTd will subsequently influence spiking activity in dlPFC. (<bold>F</bold>) Left: Cross-validated pseudo-<italic>R</italic><sup>2</sup> of the full encoding model (y-axis) and a reduced model without within and across area unit-to-unit coupling (x-axis). Right: Probability distribution of the change in pseudo-<italic>R</italic><sup>2</sup> from the reduced to the full model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Clustering results, subsampling from neurons in dorsolateral prefrontal cortex (dlPFC) and 7a to match the number of units recorded from in dorsomedial superior temporal area (MSTd).</title><p>(<bold>A</bold>) We performed spectral (Jaccard) clustering of neurons based on their 1×17 vector of Booleans, indicating whether they were tuned or not to particular task variables (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Here, we perform this operation 10,000 times, while randomly selecting (without replacement) 231 neurons from 7a and dlPFC. Thus, the full matrix clustered was 693 (231 × 3 brain areas) × 17 (task variables). Given that on each run clusters are assigned an arbitrary cluster number, for each run we compute the ratio of the largest cluster size to the total number of units per area (231). Namely, in the main text we report that MSTd and dlPFC are predominantly represented by one mixed-selective cluster, while 7a is represented in three approximately equal sized clusters. The results here concord with those in the main text, demonstrating that approximately 70% of neurons in MSTd and dlPFC belong to a single cluster, while approximately 35% of neurons belong to the largest cluster in area 7a. Circles are the mean across 10,000 iteration, error bars are 95% CI. (<bold>B</bold>) 100 iterations of Uniform Manifold Approximation and Projection (UMAP) while randomly subsampling from 7a and dlPFC to match the number of units in MSTd. On each run, we compute the distance in UMAP space between MSTd and dlPFC, and between MSTd and 7a. Then we compute their ratio (MSTd-to-dlPFC/MSTd-to-7a, thus &gt;1 indicating closer MSTd-to-dlPFC distances). The figure shows the full distribution of ratios, concurring with the main text that MSTd and dlPFC are approximately six times closer in UMAP space, than MSTd and 7a are.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Uniform Manifold Approximation and Projection (UMAP) (<xref ref-type="bibr" rid="bib46">McInnes et al., 2020</xref>) space color coded by mutual information with each task variable.</title><p>Each subplot is normalized between its minimum and maximum mutual information (MI). Results demonstrate that the UMAP finds meaningful clusters – neurons with high MI for a particular variable clustering together – even though the algorithm is not known about task variables or MI. The plots also highlight the distribution in MI for the different variables; for example, most neurons having an intermediary MI for angular acceleration, while a subset of neurons have a very strong MI for the timing of reward.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Characteristics of coupling functions within and across areas.</title><p>(<bold>A</bold>) Coupling probability (y-axis) between two neurons as a function of the distance between them (x-axis). This illustration is for data from a single monkey (Monkey S) with recordings in dorsomedial superior temporal area (MSTd) (green, number of coupling pairs = 1714), area 7a (blue, number of coupling pairs = 97,711), and dorsolateral prefrontal cortex (dlPFC) (red, number of coupling pairs = 41,665). As expected, neurons that are closer to each other are more likely to be coupled. Given this effect and that multiple recording techniques were used (with different spacing between electrodes, 400µm in Utah array and 100µm in linear probes), we used these estimates in correcting for coupling probability for a single distance (500µm). (<bold>B</bold>) Coupling filters across cortical areas. Coupling functions were clustered by Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and are depictured from each of the possible emitters (top) to each of the possible receivers (bottom). The coupling filters are ordered from top to bottom, according to their maximal occurrence for any pair of emitter and receiver. Ten examples (if available) are plotted in semi-transparent and in the background, and the average across all coupling filters of the particular type are plotted in opaque and with a thicker line. Percentages refer to the percent of coupling filters between an emitter and a receiver that were clustered within the given category. X-axis is a z-score of firing rate (in Hz) and y-axis is time in seconds (maximum = 600 ms). Importantly, these coupling filters are not sinusoidal. (<bold>C</bold>) Fraction of neurons tuned (y-axis) to the phase of theta-, alpha-, and beta-band local field potentials (LFPs) (x-axis) as a function of brain area (top = 7a, bottom = dlPFC) and whether the unit was phase-locked to LFP in the other area (left column) or coupled to a neuron in the other area (right column). Neurons who were tuned to the LFP in a different area (dark blue) were significantly more likely to be tuned to the LFP in their own area (left column) than neurons that were not tuned to the phase of LFP in another area (light blue). On the other hand, whether units were coupled or not with neurons in another area (uncoupled in light blue and coupled in dark blue) did not impact their likelihood of being tuned to the ongoing phase of LFPs in their own area (right column). MSTd is not depicted, as there was no phase locking of dlPFC neurons to the ongoing LFP phase in MSTd. This analysis is based on 5929 coupling pairs in area 7a (blue) and 4190 coupling pairs in dlPFC (red).(<bold>D</bold>) Coupling probability (CP) between any two units given their tuning similarity to sensorimotor, latent, or other (reward and eye positions) variables. Tuning similarity is computed as the correlation in tuning functions, then these are discretized by their tuning similarity (<italic>r</italic>2 = [0–0.25; 0.25–0.5; 0.5–0.75; 0.75–1]) and averaged within each category (e.g., sensorimotor) and tuning similarity bin. CP is expressed as a ratio, normalized to the bin with lowest tuning similarity (leftmost), such that a CP ratio of~3 (e.g., sensorimotor variables in dlPFC) indicates that coupling is three times more likely given high vs. low tuning similarity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Fraction of neurons coupled in 7a and dorsolateral prefrontal cortex (dlPFC) as a function of probe (Utah array or linear probe).</title><p>We questioned whether the type of probe utilized during recording had an impact on the fraction of units the Poisson generalized additive model (P-GAM) estimated as coupled. To address this question, we examined a new set of recordings in 7a (5 sessions, 32 neurons in Monkey B, not reported in the main text), as well as the recordings in dlPFC in Monkey M (55 neurons), both of which were conducted with linear probes. For each area, we computed the fraction of neurons coupled during linear probe recordings, given that the distance between these neurons was 400 µm, the minimal distance between electrodes in the Utah array recordings. Then, we performed 500 iterations where we randomly subsampled from simultaneously recorded neurons in Utah arrays to match the number of simultaneously recorded neurons with the linear probe. We computed the fraction of neurons coupled within this subsample, again only for neurons at a distance of 400 µm. We plot the iterations (from array data) as a histogram (blue for 7a and red for dlPFC), demonstrating that the fraction of neurons tuned did not significantly depend on type of probe used (7a, mean of linear probe = 0.17, 95% CI of array = [0.10 0.49]; dlPFC, mean of linear probe = 0.10, 95% CI of array = [0.03 0.22]).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig4-figsupp4-v2.tif"/></fig></fig-group><p>Subsequently, we questioned whether utilizing knowledge of the shape of these tuning functions (as opposed to simply whether tuning to a variable was significant or not) would dissociate between neurons in each area. Tuning functions for all task variables of a given neuron were stacked, then all neurons were projected onto a low-dimensional manifold via Uniform Manifold Approximation and Projection (UMAP; <xref ref-type="bibr" rid="bib46">McInnes et al., 2020</xref>), and clustered on this projection via DBSCAN (Density-Based Spatial Clustering of Applications with Noise; <xref ref-type="bibr" rid="bib27">Ester, 1996</xref>). Area 7a and dlPFC neatly segregated from one another, while neurons from MSTd could be found along a continuum from area 7a to dlPFC. Notably, however, the centroid of MSTd was 6.49 times closer to the centroid of dlPFC than area 7a (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, top row. Note that <xref ref-type="bibr" rid="bib11">Becht et al., 2018</xref>, have shown UMAP to conserve global structure and thus allows for a meaningful interpretation of distances). This finding also holds when subsampling from 7a and dlPFC to match the number of units present in MSTd (100 iterations, MSTd-dlPFC distance was 5.56 times closer than MSTd-7a, 95% CI = [4.07, 7.33]; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). DBSCAN clustering highlighted three main clusters within this low-dimensional projection of tuning functions. Cluster 1 (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, second row) contained 81.5% of all dlPFC neurons, 58.9% of all MSTd neurons, and 3.6% of all neurons from 7a. Clusters 2 and 3 exclusively contained neurons from area 7a (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, third and fourth row). Thus, just as the clustering based on what variables were neurons tuned to, clustering based on tuning shapes also highlighted a stronger similarity in the encoding properties of MSTd and dlPFC, as opposed to either of the former and area 7a (see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for UMAPs color coded by the mutual information between neural responses and a particular task variable).</p><p>Given the similarity in encoding profiles between MSTd and dlPFC, we next examined inter-area global synchronization and unit-to-unit coupling to question whether these two areas formed an interacting functional subnetwork. We examined inter-area coordination both from the standpoint of coarse grain LFP-to-spike phase locking, and via finer resolution unit-to-unit couplings as estimated from the P-GAM (<xref ref-type="bibr" rid="bib6">Balzani et al., 2020b</xref>).</p><p>Spiking activity in some units was well explained by the ongoing LFP phase in different regions (above and beyond the ongoing LFP phase in their local area, see <xref ref-type="fig" rid="fig2">Figure 2F</xref> and <xref ref-type="fig" rid="fig2s8">Figure 2—figure supplements 8</xref> and <xref ref-type="fig" rid="fig2s9">9</xref>). Most notably, 37% of neurons in dlPFC were tuned to the ongoing LFP phase within the beta range in area 7a. An even greater proportion of units showed the reciprocal property, with 59% of neurons in area 7a being modulated by the ongoing phase of beta-band LFP in dlPFC. A considerable proportion of 7a units were also modulated by the ongoing phase of alpha-band LFP in MSTd (24%, <xref ref-type="fig" rid="fig4">Figure 4D</xref>). Globally, therefore, phase locking of spiking activity to LFP phase in different areas reflected known anatomical connections, with reciprocal projections between MSTd and 7a, as well as between 7a and dlPFC (<xref ref-type="bibr" rid="bib3">Andersen et al., 1990</xref>; <xref ref-type="bibr" rid="bib63">Rozzi et al., 2006</xref>). Interestingly, the putative ‘feedback’ influence from dlPFC to area 7a (potentially reflecting the ‘belief’ guiding motor action) was stronger than the putative ‘feedforward’ influence of 7a onto dlPFC. The lowest frequency we indexed (theta, 4–8 Hz) seemed to be reserved for putative ‘feedback’ influence of dlPFC onto both MSTd (12%) and area 7a (18%).</p><p>Finer grain unit-to-unit coupling was sparse, and within each area the probability of two units being functionally connected decreased as a function of the distance between neurons (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref>). The overall likelihood of two units being coupled within a given area changed as a function of brain area and was modulated by task engagement (active vs. passive viewing in area 7a; <xref ref-type="fig" rid="fig2s11">Figure 2—figure supplement 11C</xref>), but not as a function of probe type used (Utah array or linear probe, see <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). There were more units coupled in MSTd (13%, corrected for distance between electrodes, see <italic>Methods</italic>) and area 7a (22%) than in dlPFC (7%, <xref ref-type="fig" rid="fig4">Figure 4E</xref>), potentially reflecting an increase in the dimensionality of the neural code (i.e., decrease in redundancy) as we move from sensorimotor to cognitive areas. More importantly, the across area unit-to-unit coupling did not reflect global anatomical trends, and instead supported the functional association between MSTd and dlPFC, as suggested by the encoding profiles of these areas. Twelve percent of neurons in dlPFC were coupled to activity in MSTd, while 7% of neurons in MSTd were coupled to activity in dlPFC. Importantly, neither of these regions showed either a ‘feedforward’ or ‘feedback’ likelihood of being coupled with area 7a that exceeded 4% (<xref ref-type="fig" rid="fig4">Figure 4E</xref>).</p><p>Four arguments support the fact that unit-to-unit coupling as estimated by the P-GAM reflect a property of neural coding beyond global volume conductance. First, they significantly improved fits of the encoding model. <xref ref-type="fig" rid="fig4">Figure 4F</xref> shows the cross-validated pseudo-<italic>R</italic><sup>2</sup> for the encoding model, including all task variables, LFP phases, and unit-to-unit coupling (‘coupled model’, y-axis), relative to a reduced model lacking the unit-to-unit coupling (‘uncoupled model’, x-axis). Second, inter-area coupling filters did not show oscillatory behavior. Instead of sinusoidal patterns, we frequently observed gradual increases or decreases in the likelihood of the examined neuron to spike given the activity of another neurons (see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B</xref> for a ‘dictionary’ of coupling filters across areas and their relative frequency). Third, while units phase-locked to the LFP in a different region were likely to be phase-locked to LFP in their own area (reflecting the utility of LFPs in coordinating spiking activity across areas), there was no significant change in the likelihood of coupled vs. uncoupled units to be phased-locked to LFPs (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3C</xref>). Lastly, and most importantly, the likelihood of two units being coupled was not random or purely depending on their physical distance, but instead varied as a function of their tuning similarity. <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3D</xref> shows that the more similar the tuning functions were between two neurons, the more likely these were to be coupled. The P-GAM was able to reflect this well-established ‘like-to-like connectivity’ (<xref ref-type="bibr" rid="bib21">Cossell et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">Chettih and Harvey, 2019</xref>) within a naturalistic closed-loop task, even for latent variables, and even in MSTd.</p><p>Altogether, both the global encoding profiles and fine-grain unit-to-unit coupling across areas (<xref ref-type="fig" rid="fig4">Figure 4</xref>) suggested the presence of an MSTd-dlPFC functional subnetwork within this closed-loop virtual navigation task. Given the high probability to encountering neurons in MSTd and dlPFC tuned to the distance and angle from the target, as well as to eye position (<xref ref-type="fig" rid="fig2">Figure 2</xref>), and given the animals’ tendency to keep track of the firefly location with their eyes (<xref ref-type="fig" rid="fig1">Figure 1</xref>, and <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>), we hypothesized that this MSTd-dlPFC network may reflect the monkey’s embodied mnemonic strategy in navigating to hidden targets.</p></sec><sec id="s2-5"><title>MST-dlPFC coupling reflect the animals’ strategy of tracking hidden targets with their eyes</title><p>The monkeys’ gaze reflected their belief about the firefly location. Indeed, within a session and across trials, the mean absolute difference between a monkey’s eye position and where they ought to be looking if they were perfectly tracking the firefly correlated with steering endpoint error (<italic>r</italic><sup>2</sup> ± standard deviation across all datasets, 0.36±0.22; for shuffled data, 0.16±0.09, p=5.6 × 10<sup>–3</sup>, paired t-test; see <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>, for the original description of this effect). Further, this relationship also held across sessions, with better target tracking correlating with less bias (slopes closer to 1, see <xref ref-type="fig" rid="fig1">Figure 1C</xref>), particularly in the angular domain (<italic>r</italic><sup>2</sup>=0.43, p=0.004; radial: <italic>r</italic><sup>2</sup>=0.26, p=0.04; <xref ref-type="fig" rid="fig5">Figure 5A</xref>), and with an increasing proportion of rewarded trials (<italic>r</italic><sup>2</sup>=0.24, p=0.042). Thus, we may question whether the likelihood of observing unit-to-unit coupling (defined within a session but not within a trial) relates to session-by-session changes in target tracking and/or steering performance.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Increased dorsomedial superior temporal area (MSTd)-dorsolateral prefrontal cortex (dlPFC) coupling correlated with an increased likelihood of animals keeping track of the invisible fireflies with their eyes.</title><p>(<bold>A</bold>) Correlation between the target tracking index (x-axis, i.e., tracking the hidden target with their eyes) and the radial (top) or angular (bottom) bias (defined as the slope relating responses and targets, as in <xref ref-type="fig" rid="fig1">Figure 1</xref>). Only sessions included in the neural analysis in Panel B were included in Panel A. (<bold>B</bold>) Correlation between the target tracking index (x-axis) and the fraction of neurons coupled within the ‘sender’ region (y-axis). The diagonal shows within area couplings (MSTd-MSTd, 7a-7a, dlPFC-dlPFC), while off-diagonals show across area couplings. <italic>R</italic><sup>2</sup> and p-values are shown as insets for the significant correlations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Correlations between fraction of neurons coupled in a ‘sender’ region and steering behavior.</title><p>Correlations between the radial (<bold>A</bold>) or angular (<bold>B</bold>) bias within a session (defined as the slope relating responses and targets, as in <xref ref-type="fig" rid="fig1">Figure 1</xref>) and the fraction of neurons coupled within and across areas. The diagonals show within area couplings (MSTd-MSTd, 7a-7a, dlPFC-dlPFC), while off-diagonals show across area couplings. There was no correlation between fraction of units coupled and steering behavior.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80280-fig5-figsupp1-v2.tif"/></fig></fig-group><p>For well-fit sessions (mean pseudo-<italic>r</italic><sup>2</sup> &gt;0.05) with at least two units in each of two different areas, we computed the monkey’s target tracking index (averaged across the entire trial and then across trials), as well as the probability of units being coupled with others within and across areas, as estimated by the P-GAM. As shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref> (top right), we observed a strong association between the fraction of units showing MST-to-dlPFC coupling and the monkeys’ ability to track the hidden firefly with their eyes (<italic>r</italic><sup>2</sup>=0.63, p=0.003). This association, with more coupling indexing better tracking of the firefly, was also true for the ‘feedback’ projection from dlPFC-to-MSTd (<italic>r</italic><sup>2</sup>=0.41, p=0.035, <xref ref-type="fig" rid="fig5">Figure 5B</xref>, bottom left), as well as for dlPFC-to-dlPFC coupling (<italic>r</italic><sup>2</sup>=0.43, p=0.047, <xref ref-type="fig" rid="fig5">Figure 5B</xref>, bottom right). The other sender-receiver pairings, including MSTd-to-MSTd, did not show a correlation with the ability of the monkey to track the hidden firefly with their eyes (all p&gt;0.08). As shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, there was no correlation between the unit-to-unit couplings within a session, and either radial or angular biases (7a-7a coupling vs. angular bias p=0.06; all other p&gt;0.12). There was similarly no correlation between the proportion of rewarded trials in a session and unit-to-unit coupling probability (all p&gt;0.11, Bonferroni corrected). Overall, therefore, the functional subnetwork between MSTd and dlPFC (<xref ref-type="fig" rid="fig4">Figure 4</xref>) seemingly reflects the animals’ strategy in keeping track of the hidden target with their eyes. In turn, the eye movements (but not MSTd-dlPFC coupling directly) aid in successfully navigating to the location of the hidden target.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The lion’s share of our understanding linking spiking activity to goal-directed behavior originates from tasks that bear little resemblance to real-world self-environment interactions. Major differences include the prevalence of closed loops between action and perception in natural behaviors, as well as the necessity to concurrently accomplish multiple sensory and cognitive operations over protracted periods of time. To examine how the brain operates and coordinates among multiple neural nodes under these naturalistic conditions, we recorded simultaneously from three interconnected cortical areas (MSTd, 7a, and dlPFC) while non-human primates detected a target, remembered its location, and path integrated to it by actively sampling from their environment. We focused on the encoding profiles of single units and described four main findings.</p><p>First, we found that all task variables were present across all brain areas probed. This multiplexing, however, was not random. The prefrontal cortex showed the greatest proportion of neurons tuned to latent variables. Posterior parietal cortex showed a strong selectivity for sensorimotor variables and the ongoing phase of LFPs. MSTd was prominently tuned to eye position (<xref ref-type="bibr" rid="bib38">Komatsu and Wurtz, 1988</xref>; <xref ref-type="bibr" rid="bib53">Nadler et al., 2009</xref>). Most notoriously, even MSTd, an area traditionally considered a sensory node, showed a considerable number of neurons encoding latent variables, such as the angle or radial distance from origin (i.e., path integration) and to target (i.e., vector coding of spatial goals). In fact, the global encoding profile of MSTd was strikingly akin to that of dlPFC, suggesting this area need not be purely sensory (see <xref ref-type="bibr" rid="bib48">Mendoza-Halliday et al., 2014</xref>, for a similar argument).</p><p>Second, we found that MSTd, area 7a, and dlPFC all show evidence for vector-based coding of spatial goals (<xref ref-type="bibr" rid="bib64">Sarel et al., 2017</xref>, also see <xref ref-type="bibr" rid="bib25">Ekstrom et al., 2003</xref>; <xref ref-type="bibr" rid="bib28">Gauthier and Tank, 2018</xref>; <xref ref-type="bibr" rid="bib45">Marigold and Drew, 2017</xref>, for similar evidence in humans, rodents, and cats, respectively). Interestingly, we observed that area 7a showed a majority of neurons coding for locations near the origin or near the target. That is, 7a is likely involved in body-centered state transitions (e.g., from static to moving) and approaching behavior (see <xref ref-type="bibr" rid="bib47">Medendorp and Heed, 2019</xref>, for a similar argument and <xref ref-type="bibr" rid="bib67">Serino, 2019</xref>, for a review on the role of posterior parietal cortex in peri-personal space encoding). dlPFC, on the other hand (and to a lesser extent MSTd), showed a preponderance of units coding for locations near the origin and far from the target. Interestingly, human fMRI work (<xref ref-type="bibr" rid="bib32">Howard et al., 2014</xref>) has suggested that the posterior hippocampus encodes path distance to goals, while the anterior hippocampus encodes a more abstract Euclidean distance. Together with the current findings, and the fact that anterior and posterior hippocampus respectively project to the prefrontal and parietal cortex (<xref ref-type="bibr" rid="bib69">Strange et al., 2014</xref>) suggests that a circuit including the anterior hippocampus and prefrontal cortex may abstractly plan goal-directed trajectories, while a circuit including the posterior hippocampus and posterior parietal cortex may compute body-centric sensorimotor affordances.</p><p>The fact that all areas probed showed a preference for distances near (as opposed to far) from where the monkey started path integrating (i.e., the origin) may suggest a change in the computations undertaken early vs. late within a single trajectory. Namely, early on observers may compute their ongoing distance from the origin, and then switch to computing distance not from origin, but from the target. This switch from first coding displacement from origin to then coding location relative to a target has been previously suggested empirically (<xref ref-type="bibr" rid="bib30">Gothard et al., 1996</xref>) and in recent computational accounts (<xref ref-type="bibr" rid="bib36">Kanitscheider and Fiete, 2017</xref>).</p><p>The third major finding relates to the functional organization across MSTd, area 7a, and dlPFC, and again suggests that navigating to a remembered target may involve two interdependent but separable computations: (i) estimating own’s own location relative to a starting location (i.e., a landmark) and (ii) estimating the position of the target and discounting self-location from target location. Indeed, anatomical tracer studies suggest a distributed hierarchy, with recurrent connections between MSTd and 7a, and then recurrent connections between 7a and dlPFC (<xref ref-type="bibr" rid="bib3">Andersen et al., 1990</xref>; <xref ref-type="bibr" rid="bib63">Rozzi et al., 2006</xref>). As expected from anatomy, here we observed that spiking activity in 7a is influenced by LFP fluctuations in MSTd (particularly in the alpha range). In turn, spiking activity in dlPFC and 7a are influenced by LFP fluctuations, respectively, in 7a and dlPFC (particularly in the beta range). In other words, we observed that the coarse channels of communication riding on LFPs are in line with anatomy. Further, the strong tuning of area 7a to sensorimotor variables and the strong tuning of both area 7a and dlPFC (but not MSTd) to the distance and angle from origin suggest that this pathway may be primarily involved in path integration as opposed to the vector coding of spatial goals. In addition to this MSTd-7a-dlPFC pathway, we also observed significant functional coupling between MSTd and dlPFC, independent from 7a. Namely, the likelihood of MSTd neurons being coupled to dlPFC, and of dlPFC neurons being coupled to MSTd, was three and almost two times as large as the unit-to-unit coupling among either of these areas with 7a. These results suggest that despite the lack of direct anatomical connections, MSTd and dlPFC may form a functional subnetwork within this closed-loop navigate-to-target task.</p><p>Lastly, we examined the putative functional role of this MSTd-dlPFC subnetwork. As our group has previously reported, monkeys naturally track with their eyes the location of hidden goals during navigation (<xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>). Given this task strategy, as well as the prominent tuning of both MSTd and dlPFC to eye position and the distance and angle to the hidden target, we hypothesized that the MSTd-dlPFC subnetwork may reflect the monkeys’ innate task strategy. Indeed, we found that the more units were coupled between MSTd and dlPFC (either ‘feedforward’ or ‘feedback’), the better the monkeys tracked the hidden goal with their eyes. Thus, we suggest that the dynamic functional coupling between MSTd and dlPFC may reflect the innate task strategy monkeys adapted in pursuing the goal location with their eyes.</p><p>We must acknowledge a number of limitations and areas of ongoing or future experimentation. First, while the task we employed here is more naturalistic than most, further improvements are possible. For instance, virtual and real-world navigation may rely on partially distinct neural codes (<xref ref-type="bibr" rid="bib1">Aghajan et al., 2015</xref>). Thus, it will be interesting to replicate the current experiment while macaques move freely in a 3D environment (e.g., <xref ref-type="bibr" rid="bib44">Mao et al., 2021</xref>). This would also allow for independent eye and head movements (head was restrained here) and thus we could estimate whether eye movements in the current experiment partially reflected intended head movements (as they seemingly do in rodents; <xref ref-type="bibr" rid="bib50">Michaiel et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Meyer et al., 2020</xref>). Performing a ‘firefly task’ in a real environment would also suppose a more complex set of visual inputs (e.g., corners, textures, shadows) that could be leveraged in an expanded P-GAM taking visual features as input (see <xref ref-type="bibr" rid="bib60">Parker et al., 2022</xref>, for recent work taking this approach). The second limitation relates to the (necessarily incomplete) sampling of neural areas, and the focus on single units as opposed to population dynamics. We report a functional subnetwork between MSTd and dlPFC based on the similarity of their encoding profiles (though they are of course not identical) and the likelihood of encountering unit-to-unit couplings across these areas. But this functional connection must be subserved by structure (e.g., perhaps a third area we did not record from fluctuating with both MSTd and dlPFC). Thus, in ongoing experiments we have trained rodents to perform the ‘firefly task’. This will allow recording from a wider array of neural areas and cortical layers (most of the recordings reported here being from Utah arrays and hence likely from a single layer and of limited independence). Similarly, to further corroborate the functional subnetwork between MST and dlPFC, it will be interesting to examine population dynamics and the possibility that these areas form a functional ‘communication subspace’ (<xref ref-type="bibr" rid="bib66">Semedo et al., 2019</xref>), adapted to the naturalistic setting of this task (see <xref ref-type="bibr" rid="bib8">Balzani et al., 2022b</xref>). The last limitation relates to causality. The results we report here amount to detecting a correlation (i.e., spikes occurring most often at a given LFP phase and this correlation not being accounted by other sensorimotor, latent, or internal covariates). In future experiments it will be interesting to test for causality within this network, by either demanding observers to fixate elsewhere (<xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>), or by directly perturbing this network, for instance by micro-stimulation or optogenetic manipulations.</p><p>In conclusion, we demonstrate what may be broad principles of neural computation during closed action-perception loops: (i) mixed yet patterned single cell selectivity, (ii) coding of latent variables even in areas traditionally considered as purely sensory, and (iii) differential coarse (e.g., LFP-spike phase alignment) and fine-grain connectivity between task-relevant areas. Most notoriously, here we indexed the presence of significant noise correlations between MSTd and dlPFC, independently from 7a. We suggest that this coupling between sensory and prefrontal areas may reflect (embodied) task strategies.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Animals and animal preparation</title><p>We recorded extracellularly from areas MSTd, area 7a, and dlPFC in three adult male rhesus macaques (<italic>Macaca mulatta</italic>; 9.8–13.1 kg). We collected behavioral and neural data from 27 recording sessions from Monkey Q, 38 sessions from Monkey S, and 17 sessions from Monkey M (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for additional detail regarding the recording locations from each animal). All animals were chronically implanted with a lightweight polyacetal ring for head restraint. Further, for acute recordings, animals were outfitted with a removable grid to guide electrode penetrations. For eye tracking, a subset of monkeys (Monkey Q) were implanted with scleral coils (CNC Engineering, Seattle WA, USA), while eye tracking was performed using a video-based system (ISCAN Inc, Woburn, MA, USA) in the remaining animals (Monkeys S and M). Monkeys were trained using standard operant conditioning procedures to use a joystick to navigate in virtual reality and stop at the location of briefly presented targets, ‘fireflies’. All surgeries and procedures were approved by the Institutional Animal Care and Use Committee at Baylor College of Medicine (Protocol A3317-01) and New York University (Protocol number 18-1502) and were in accordance with National Institutes of Health guidelines.</p></sec><sec id="s4-2"><title>Experimental setup</title><p>Monkeys were head-fixed and secured in a primate chair. A three-chip DLP projector (Christie Digital Mirage 2000, Cypress, CA, USA) rear-projected images onto a 60×60 cm<sup>2</sup> screen that was attached to the front of the field coil frame, ~30 cm in front of the monkey. To navigate, the animals used an analog joystick (M20U9T-N82, CTI electronics) with two degrees of freedom to control their linear and angular speeds in a virtual environment. This virtual world comprised a ground plane whose textural elements had limited lifetime (~250 ms) to avoid serving as landmarks. The ground plane was circular with a radius of 70 m (near and far clipping planes at 5 cm and 40 m, respectively), with the subject positioned at its center at the beginning of each trial. Each texture element was an isosceles triangle (base × height: 8.5 × 18.5 cm<sup>2</sup>) that was randomly repositioned and reoriented at the end of its lifetime, making it impossible to use as a landmark. The maximum linear and angular speeds were fixed to 2 m/s and 90 deg/s. The density of the ground plane was 5.0 elements/m<sup>2</sup>. All stimuli were generated and rendered using C++ Open Graphics Library (OpenGL; Nvidia Quadro FX 3000G accelerator board) by continuously repositioning a virtual camera based on joystick inputs to update the visual scene at 60 Hz. The virtual camera was positioned at a height of 0.1 m above the ground plane. Given the OpenGL approach in re-positioning a virtual camera within a 3D space, depth cues included linear perspective, texture gradient, and motion parallax. Further, the stimulus was rendered as a red-green anaglyph and monkeys wore goggles fitted with Kodak Wratten filters (red #29 and green #61) to view the stimulus. This additionally provided binocular parallax. The binocular crosstalk for the green and red channels was 1.7% and 2.3%, respectively. Spike2 software (Cambridge Electronic Design Ltd., Cambridge, UK) was used to record and store the timeseries of target and animal’s location, animal linear and angular velocity, as well as eye positions. All behavioral data were recorded along with the neural event markers at a sampling rate of 833.33 Hz.</p></sec><sec id="s4-3"><title>Behavioral task</title><p>Monkeys steered to a target location (circular disc of radius 20 cm) that was cued briefly (300 ms) at the beginning of each trial. Each trial was programmed to start after a variable random delay (truncated exponential distribution, range: 0.2–2.0 s; mean: 0.5 s) following the end of the previous trial. The target appeared at a random location between –40 and 40 deg of visual angle, and between 1 and 4 m relative to where the subject was stationed at the beginning of the trial. The joystick was always active, and thus monkeys were free to start moving before the target vanished, or before it appeared. Monkeys typically performed blocks of 750 trials before being given a short break. In a session, monkeys would perform two or three blocks. Feedback in the form of juice reward was given following a variable waiting period after stopping (truncated exponential distribution, range: 0.1–0.6 s; mean: 0.25 s). They received a drop of juice if their stopping position was within 0.6 m from the center of the target. No juice was provided otherwise.</p></sec><sec id="s4-4"><title>Neural recording and pre-processing</title><p>We recorded extracellularly, either acutely using a 24- or 36-channel linear electrode array (100 µm spacing between electrodes; U-Probe, Plexon Inc, Dallas, TX, USA; MSTd in Monkeys Q and S, and dlPFC in Monkey M) or chronically with multi-electrode arrays (Blackrock Microsystems, Salt Lake City, UT, USA; 96 electrodes in area 7a in Monkey Q, and 48 electrodes in both area 7a and dlPFC in Monkey S). During acute recordings with the linear arrays, the probes were advanced into the cortex through a guide-tube using a hydraulic microdrive. Spike detection thresholds were manually adjusted separately for each channel to facilitate real-time monitoring of action potential waveforms. Recordings began once the waveforms were stable. The broadband signals were amplified and digitized at 20 kHz using a multichannel data acquisition system (Plexon Inc, Dallas, TX, USA) and were stored along with the action potential waveforms for offline analysis. Additionally, for each channel, we also stored low-pass filtered (–3 dB at 250 Hz) LFP signals. For the chronic array recordings, broadband neural signals were amplified and digitized at 30 kHz using a digital headstage (Cereplex E, Blackrock Microsystems, Salt Lake City, UT, USA), processed using the data acquisition system (Cereplex Direct, Blackrock Microsystems) and stored for offline analysis. Additionally, for each channel, we also stored low-pass filtered (–6 dB at 250 Hz) LFP signals sampled at 500 Hz. Finally, copies of event markers were received online from the stimulus acquisition software (Spike2) and saved alongside the neural data.</p><p>Spike detection and sorting were performed on the raw (broadband) neural signals using KiloSort 2.0 software (<xref ref-type="bibr" rid="bib59">Pachitariu et al., 2016</xref>) on an NVIDIA Quadro P5000 GPU. The software uses a template matching algorithm both for detection and for clustering of spike waveforms. The spike clusters produced by KiloSort were visualized in Phy2 and manually refined by a human observer, by either accepting or rejecting KiloSort’s label for each unit. In addition, we computed three isolation quality metrics: inter-spike interval violations (ISIv), waveform contamination rate (cR), and presence rate (PR). ISIv is the fraction of spikes that occur within 1 ms of the previous spike. cR is the proportion of spikes inside a high-dimensional cluster boundary (by waveform) that are not from the cluster (false positive rate) when setting the cluster boundary at a Mahalanobis distance such that there are equal false positives and false negatives. PR is 1 minus the fraction of 1 min bins in which there is no spike. We set the following thresholds in qualifying a unit as a single unit: ISIv &lt;20%, cR &lt;0.02, and PR &gt;90%.</p></sec><sec id="s4-5"><title>Analyses</title><sec id="s4-5-1"><title>Behavior</title><p>The location of targets and monkey’s endpoints were expressed in polar coordinates, with a radial distance (target = <inline-formula><mml:math id="inf11"><mml:mi>r</mml:mi></mml:math></inline-formula>, response = <inline-formula><mml:math id="inf12"><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>) and eccentricity from straight ahead (target = <inline-formula><mml:math id="inf13"><mml:mi>θ</mml:mi></mml:math></inline-formula>; response = <inline-formula><mml:math id="inf14"><mml:mover accent="true"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>). On a subset of trials (~5%) animals stopped within 0.5 m of the origin (range of targets, 1–4 m). Similarly, on a subset of trials (~13%) animals did not stop during the course of a trial (max duration = 7 s). These trials were discarded before further behavioral analyses. As we have observed before (<xref ref-type="bibr" rid="bib41">Lakshminarasimhan et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Lakshminarasimhan et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Noel et al., 2020</xref>; <xref ref-type="bibr" rid="bib56">Noel et al., 2021</xref>), a linear model with multiplicative gain accounted well for the observed data (average <italic>R</italic><sup>2</sup>=0.72). Thus, we used the slopes of the corresponding linear regressions as a measure of bias. Note that in this scheme a slope of one indicates no bias (i.e., targets and endpoints lie along the identity line), whereas slopes smaller than one indicate a bias wherein animals undershoot targets (either in radial or angular distance).</p></sec><sec id="s4-5-2"><title>Poisson generalized additive model</title><p>The P-GAM (<ext-link ext-link-type="uri" xlink:href="https://github.com/savin-lab">https://github.com/savin-lab</ext-link>; <xref ref-type="bibr" rid="bib5">Balzani, 2020a</xref>) defines a non-linear mapping between spike counts of a unit <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and a set of continuous covariates <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (angular and linear velocity and acceleration, angular and linear distance travelled, angular and linear distance to target, and LFP instantaneous phase across different frequency ranges), and discrete events <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (time of movement onset/offset, target onset, reward delivery, and the spike counts from simultaneously recorded units). As such, inputs to the P-GAM were of three types. First, the spike counts of the unit to be modeled, at a 6 ms resolution (i.e., the number of spikes within 6 ms windows, no baseline correction). Second, the continuous, discrete, and neural covariates, which were also sampled at a 6 ms resolution. The last input type were a set of 15 ‘knots’ per covariate, defining the nodes of eventual tuning functions. The location of knots were defined as to (i) cover the range of a given input variable from the 2nd to the 98th percentile with (ii) equi-probable knots (each knot covering the same probability mass). See (<ext-link ext-link-type="uri" xlink:href="https://github.com/BalzaniEdoardo/PGAM/blob/master/PGAM%20Tutorial.ipynb">https://github.com/BalzaniEdoardo/PGAM/blob/master/PGAM%20Tutorial.ipynb</ext-link>; <xref ref-type="bibr" rid="bib9">Balzani, 2022c</xref>) for a comprehensive tutorial.</p><p>The unit log-firing rate is modeled as a linear combination of arbitrary non-linear functions of the covariates,<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where * is the convolution operator, and the unit spike counts are generated as Poisson random variables with rate specified by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.</p><p>Input specific non-linearities <inline-formula><mml:math id="inf18"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> are expressed in terms of flexible B-splines, <inline-formula><mml:math id="inf19"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:mi>b</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> and are associated with a smoothness enforcing penalization term controlled by a scale parameter <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:math></disp-formula></p><p>The larger <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the smoother the model. This penalization terms can be interpreted as Gaussian priors over model parameters. The resulting log-likelihood of the model takes the form,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf22"><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> being the spike counts of the unit, <inline-formula><mml:math id="inf23"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> being the continuous task variables, <inline-formula><mml:math id="inf24"><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> being the discrete task events, <inline-formula><mml:math id="inf25"><mml:mi>T</mml:mi></mml:math></inline-formula> being the time points, <inline-formula><mml:math id="inf26"><mml:mi>β</mml:mi></mml:math></inline-formula> being the collection of all B-spline coefficients, and <inline-formula><mml:math id="inf27"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> the Poisson likelihood. Both parameters <inline-formula><mml:math id="inf28"><mml:mi>β</mml:mi></mml:math></inline-formula> and the hyperparameters <inline-formula><mml:math id="inf29"><mml:mi>λ</mml:mi></mml:math></inline-formula> are learned from the data by an iterative optimization procedure that switches between maximizing <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> as a function of the parameters and minimizing a cross-validation score as a function of the hyperparameters (see <xref ref-type="bibr" rid="bib6">Balzani et al., 2020b</xref>, for further details).</p><p>The probabilistic interpretation of the penalization terms allowed us to compute a posterior distribution for the model parameters, derive confidence intervals with desirable frequentist coverage properties, and implement a statistical test for input variable inclusion that selects a minimal subset of variables explaining most of the variance. In <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, we demonstrate that this approach will appropriately select variables contributing to the spike trials of individual neurons. Further, we show that including all variables in the model (hundreds to thousands of parameters, given the cell-to-cell coupling) does not outperform the selected (i.e., ‘reduced’) model, which typically has an order of magnitude less parameters. The approach circumvents traditional model-comparison-based variable selection, which would be unfeasible in a fully coupled model with hundreds of covariates.</p><p>To show the stability in the estimated tuning functions, <xref ref-type="fig" rid="fig2s10">Figure 2—figure supplement 10</xref> shows the fraction of units tuned to a given task variable as a function of brain area, and as a function of whether odd or even trials were fit to the P-GAM. Namely, we fit half of the dataset each time and show that the fraction of neurons tuned to a given task variable was the same regardless of whether we fit the odd numbered trials or the even numbered trials. Similarly, we index the ‘preferred’ distances and angles from origin and to target (<xref ref-type="fig" rid="fig3">Figure 3</xref>) as defined by the peak of tuning functions. In <xref ref-type="fig" rid="fig3">Figure 3B</xref> we sort neurons according to their preferred distances or angles in one subset of trials (i.e., ‘even’ trials) and plot the normalized responses in the other subset of trials (‘odd’ trials). <xref ref-type="fig" rid="fig3">Figure 3B</xref> therefore demonstrates that not only the fraction of neurons tuned to different variables was stable, but the estimated tuning functions were as well.</p></sec><sec id="s4-5-3"><title>Pseudo-<italic>R</italic><sup>2</sup></title><p>Fit quality was assessed via the pseudo-<italic>R</italic><sup>2</sup> on subset of held-out test trials (20% of the total trials, not used for inferring model parameters). Pseudo-<italic>R</italic><sup>2</sup> is a goodness of fit measure that is suitable for models with Poisson observation noise (<xref ref-type="bibr" rid="bib19">Colin Cameron and Windmeijer, 1997</xref>). The score is computed as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">p</mml:mi><mml:mi mathvariant="italic">s</mml:mi><mml:mi mathvariant="italic">e</mml:mi><mml:mi mathvariant="italic">u</mml:mi><mml:mi mathvariant="italic">d</mml:mi><mml:mi mathvariant="italic">o</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf30"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> being the likelihood of the true spike counts, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> being the likelihood of the P-GAM model prediction, and <inline-formula><mml:math id="inf32"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>´</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:math></inline-formula> being the likelihood of a Poisson null-model (constant mean rate). It can be interpreted as the fraction of the maximum possible likelihood (normalized by the null model) that the fit achieves. The score is 0 when the GAM fits are no more likely than the null model, 1 when it perfectly matches the data, and rarely can be negative (for test-set data, 3% of the recorded units) when overfitting occurs. In this latter case we excluded the unit from analysis. Empirically, the pseudo-<italic>R</italic><sup>2</sup> is a stringent metric and ranges in values that are substantially lower than the standard <italic>R</italic><sup>2</sup> when both are applicable (<xref ref-type="bibr" rid="bib22">Domencich and McFadden, 1975</xref>) and depends on the firing statistics of the area recorded (such as mean firing rates and variance), the type of covariates, and their number (<xref ref-type="bibr" rid="bib12">Benjamin et al., 2018</xref>). Our average score of 0.072 is about three times better than standard GLM performance even in areas such as primary somatosensory and motor areas (<xref ref-type="bibr" rid="bib12">Benjamin et al., 2018</xref>).</p></sec></sec><sec id="s4-6"><title>Speed and direction discrimination index</title><p>To allow for comparison with prior reports studying optic flow processing within the cadre of two-alternative forced-choice tasks, we compute the discrimination index for speed (i.e., linear velocity) and direction (i.e., angular velocity) in MSTd, 7a, and dlPFC. The discrimination index (DDI) was defined as:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>D</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic>R<sub>max</sub></italic> and <italic>R<sub>min</sub></italic> are the maximum and minimum response from a tuning function, <italic>SSE</italic> is the sum of squared errors around the mean responses, <italic>M</italic> is the number of stimulus directions, and <italic>N</italic> is the total number of observations. In the context of the current naturalistic experiment, we may bin linear and angular velocities and compute tuning functions defining <italic>R<sub>max</sub></italic> and <italic>R<sub>min</sub></italic>. The number of bins (here, we used 15 nodes, as defined by the P-GAM) defines <italic>M</italic>. To estimate <italic>SSE</italic> and <italic>N</italic> we must define trials wherein the full gamut of linear and angular velocities are experienced. To facilitate direct comparison with <xref ref-type="bibr" rid="bib15">Chen et al., 2008</xref>, we divided our recordings in 80 segments, the mean number of trials (<italic>N</italic>) in <xref ref-type="bibr" rid="bib15">Chen et al., 2008</xref>. Lastly, we computed DDI according to <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>.</p></sec><sec id="s4-7"><title>Clustering</title><p>For <xref ref-type="fig" rid="fig4">Figure 4A and B</xref>, clustering was performed by employing a spectral clustering algorithm based on the Jaccard distance metric (<xref ref-type="bibr" rid="bib68">Shi and Malik, 2000</xref>). This approach finds the <inline-formula><mml:math id="inf33"><mml:mi>k</mml:mi></mml:math></inline-formula> eigenvectors to split Jaccard distances <inline-formula><mml:math id="inf34"><mml:mi>k</mml:mi></mml:math></inline-formula> ways. Different values of <inline-formula><mml:math id="inf35"><mml:mi>k</mml:mi></mml:math></inline-formula> (≥3) resulted in conceptually identical findings. The results reported in the main text were generated for the full dataset, and thus a 4254 (231 MSTd neuron + 3200 7a neurons + 823 dlPFC neurons) × 17 (task variables) input matrix (where each entry denotes whether a particular neuron was significant or not for a particular task variable). In <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref> we confirm that the results reported in the main text were not driven by the fact that we had a considerably larger number of 7a neurons than other. Namely, we perform 10,000 iterations of spectral clustering while subsampling from 7a and dlPFC (without replacement) to match the number of units in MSTd (231). At each iteration, we compute the ratio of the size of the largest cluster, to the total number of units for a given area (i.e., 231). Results confirm that MSTd and dlPFC neurons were most often assigned to a single cluster, while neurons in 7a were most readily assigned to one of three clusters.</p><p>For <xref ref-type="fig" rid="fig4">Figure 4C</xref> all tuning functions of a neuron were stacked together, so each neuron was represented by a vector of dimension <inline-formula><mml:math id="inf36"><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the dimension of the <inline-formula><mml:math id="inf38"><mml:mi>i</mml:mi></mml:math></inline-formula> th tuning function. Then, the high-dimensional matrix (neurons × <inline-formula><mml:math id="inf39"><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) was reduced to a matrix (neurons × <inline-formula><mml:math id="inf40"><mml:mi>m</mml:mi></mml:math></inline-formula>) by PCA projection, where <inline-formula><mml:math id="inf41"><mml:mi>m</mml:mi></mml:math></inline-formula> is the number of PCs that explains &gt;90% of the variance. Finally, the matrix was further projected onto a 2D manifold using UMAP (<xref ref-type="bibr" rid="bib46">McInnes et al., 2020</xref>) and clustered using DBSCAN (<xref ref-type="bibr" rid="bib27">Ester, 1996</xref>). This clustering method automatically determined the number of clusters on the basis of the spatial density of the UMAPs. Clusters depicted in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s4">4</xref>, and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B</xref> were equally determined by DBSCAN. As for <xref ref-type="fig" rid="fig4">Figure 4A and B</xref>, for <xref ref-type="fig" rid="fig4">Figure 4C</xref> we confirmed that the results reported in the main text were not driven by the unequal number of neurons recorded from in MSTd, 7a, and dlPFC. Namely, we perform 500 iterations of the procedure described above (i.e., stacking of tuning functions, then PCA, then UMAP) while subsampling from 7a and dlPFC to match the number of units present in MSTd. At each iteration, we compute the distance between the MSTd and dlPFC centroid in UMAP space, and between the MSTd and 7a centroid. We then compute the ratio of these distances, which are depicted in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>.</p><p>For <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, mutual information was computed as the difference of the entropy of spike counts <italic>H</italic>(<italic>Y</italic>), and the entropy of the counts conditioned on the stimulus <italic>H</italic>(<italic>Y</italic>|<italic>S</italic>),<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To estimate these quantities, we assumed that the counts <inline-formula><mml:math id="inf42"><mml:mi>Y</mml:mi></mml:math></inline-formula> are Poisson distributed with rate parameter <italic>λ</italic> equal to the mean firing rate of the unit. The stimulus was discretized and its distribution approximated as a binomial, while the conditional distribution of the counts given the stimuli <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by the P-GAM generative model. Finally, we computed <italic>Y</italic> as:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-8"><title>Coupling filters</title><p>Coupling filters (and the corresponding inferential statistics) were determined via the P-GAM. Within area coupling filters were set to a duration of 36 ms, and across area filters were set to a duration of 600 ms. For the within-area coupling probability reported in <xref ref-type="fig" rid="fig4">Figure 4E</xref>, we corrected for the effects of unit distance (i.e., <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref>) by first fitting a brain region-specific logistic regression. Specifically, we expressed coupling probability as a non-linear function of electrode distance as follows:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>with <italic>c</italic> being a binary variable taking value 1 for significant coupling and 0 otherwise, <italic>d</italic> being the electrode distance, and <italic>f</italic> being a non-linear function expressed in terms of B-splines. Each brain area was fit independently, and the coupling probability in <xref ref-type="fig" rid="fig4">Figure 4E</xref> was set as the model prediction for a distance of 500 µm.</p></sec><sec id="s4-9"><title>Data and code availability</title><p>Data and code are available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/d7wtz/">https://osf.io/d7wtz/</ext-link>. Code and tutorials for utilizing the P-GAM are additionally available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/BalzaniEdoardo/PGAM">https://github.com/BalzaniEdoardo/PGAM</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:8d606e5b50563882c1cf8d5ce80bf8da5c8aeb89;origin=https://github.com/BalzaniEdoardo/PGAM;visit=swh:1:snp:c23a77d800c6056f62df9cfe21808ad2180d6e8a;anchor=swh:1:rev:deaaef66ccff5e667fcfbbc11c3de75dafea5be4">swh:1:rev:deaaef66ccff5e667fcfbbc11c3de75dafea5be4</ext-link>; <xref ref-type="bibr" rid="bib7">Balzani, 2022a</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation</p></fn><fn fn-type="con" id="con5"><p>Methodology</p></fn><fn fn-type="con" id="con6"><p>Methodology</p></fn><fn fn-type="con" id="con7"><p>Supervision, Validation, Investigation, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All surgeries and procedures were approved by the Institutional Animal Care and Use Committee at Baylor College of Medicine and New York University and were in accordance with National Institutes of Health guidelines.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80280-mdarchecklist1-v2.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code are available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/d7wtz/">https://osf.io/d7wtz/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>J </given-names></name><name><surname>Balzani</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>latent variables in sensory, parietal, and frontal cortices</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/d7wtz">d7wtz</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank Jing Lin and Jian Chen for programming the experimental stimulus. We also thank Roozbeh Kiani for his surgical expertise during the Utah array implantations. The work was funded by a 1U19 NS118246, 1R01 NS120407, and 1R01 DC004260 from NIH to DEA, as well as by 1R01MH125571 from NIH, the National Science Foundation under NSF Award No. 1922658, and a Google faculty award to CS.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aghajan</surname><given-names>ZM</given-names></name><name><surname>Acharya</surname><given-names>L</given-names></name><name><surname>Moore</surname><given-names>JJ</given-names></name><name><surname>Cushman</surname><given-names>JD</given-names></name><name><surname>Vuong</surname><given-names>C</given-names></name><name><surname>Mehta</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Impaired spatial selectivity and intact phase precession in two-dimensional virtual reality</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>121</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/nn.3884</pub-id><pub-id pub-id-type="pmid">25420065</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Alefantis</surname><given-names>P</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensory Evidence Accumulation Using Optic Flow in a Naturalistic Navigation Task</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.26.441532</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Asanuma</surname><given-names>C</given-names></name><name><surname>Essick</surname><given-names>G</given-names></name><name><surname>Siegel</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Corticocortical connections of anatomically and physiologically defined subdivisions within the inferior parietal lobule</article-title><source>The Journal of Comparative Neurology</source><volume>296</volume><fpage>65</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1002/cne.902960106</pub-id><pub-id pub-id-type="pmid">2358530</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visual and vestibular selectivity for self-motion in macaque posterior parietal area 7A</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3932</fpage><lpage>3947</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy272</pub-id><pub-id pub-id-type="pmid">30365011</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Balzani</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020a</year><data-title>Poisson generalized additive model (PGAM)</data-title><version designator="1.0">1.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/BalzaniEdoardo/PGAM">https://github.com/BalzaniEdoardo/PGAM</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Balzani</surname><given-names>E</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>K</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Savin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Efficient estimation of neural tuning during naturalistic behavior</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Balzani</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022a</year><data-title>PGAM</data-title><version designator="swh:1:rev:deaaef66ccff5e667fcfbbc11c3de75dafea5be4">swh:1:rev:deaaef66ccff5e667fcfbbc11c3de75dafea5be4</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:8d606e5b50563882c1cf8d5ce80bf8da5c8aeb89;origin=https://github.com/BalzaniEdoardo/PGAM;visit=swh:1:snp:c23a77d800c6056f62df9cfe21808ad2180d6e8a;anchor=swh:1:rev:deaaef66ccff5e667fcfbbc11c3de75dafea5be4">https://archive.softwareheritage.org/swh:1:dir:8d606e5b50563882c1cf8d5ce80bf8da5c8aeb89;origin=https://github.com/BalzaniEdoardo/PGAM;visit=swh:1:snp:c23a77d800c6056f62df9cfe21808ad2180d6e8a;anchor=swh:1:rev:deaaef66ccff5e667fcfbbc11c3de75dafea5be4</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Balzani</surname><given-names>E</given-names></name><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Herrero-Vidal</surname><given-names>P</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Savin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>A Probabilistic Framework for Task-Aligned Intra- and Inter-Area Neural Manifold Estimation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2209.02816">https://arxiv.org/abs/2209.02816</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Balzani</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022c</year><data-title>P-GAM tutorial</data-title><version designator="1.0">1.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/BalzaniEdoardo/PGAM/blob/master/PGAM%20Tutorial.ipynb">https://github.com/BalzaniEdoardo/PGAM/blob/master/PGAM%20Tutorial.ipynb</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barczak</surname><given-names>A</given-names></name><name><surname>Haegens</surname><given-names>S</given-names></name><name><surname>Ross</surname><given-names>DA</given-names></name><name><surname>McGinnis</surname><given-names>T</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dynamic modulation of cortical excitability during visual active sensing</article-title><source>Cell Reports</source><volume>27</volume><fpage>3447</fpage><lpage>3459</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.05.072</pub-id><pub-id pub-id-type="pmid">31216467</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becht</surname><given-names>E</given-names></name><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Dutertre</surname><given-names>CA</given-names></name><name><surname>Kwok</surname><given-names>IWH</given-names></name><name><surname>Ng</surname><given-names>LG</given-names></name><name><surname>Ginhoux</surname><given-names>F</given-names></name><name><surname>Newell</surname><given-names>EW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dimensionality reduction for visualizing single-cell data using UMAP</article-title><source>Nature Biotechnology</source><volume>37</volume><fpage>38</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1038/nbt.4314</pub-id><pub-id pub-id-type="pmid">30531897</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>AS</given-names></name><name><surname>Fernandes</surname><given-names>HL</given-names></name><name><surname>Tomlinson</surname><given-names>T</given-names></name><name><surname>Ramkumar</surname><given-names>P</given-names></name><name><surname>VerSteeg</surname><given-names>C</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Modern machine learning as a benchmark for fitting neural responses</article-title><source>Frontiers in Computational Neuroscience</source><volume>12</volume><elocation-id>56</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2018.00056</pub-id><pub-id pub-id-type="pmid">30072887</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of self-motion perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>389</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112953</pub-id><pub-id pub-id-type="pmid">18558861</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chafee</surname><given-names>MV</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Matching patterns of activity in primate prefrontal area 8a and parietal area 7ip neurons during a spatial working memory task</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>2919</fpage><lpage>2940</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.6.2919</pub-id><pub-id pub-id-type="pmid">9636098</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Takahashi</surname><given-names>K</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Deangelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Clustering of self-motion selectivity and visual response properties in macaque area mstd</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>2669</fpage><lpage>2683</lpage><pub-id pub-id-type="doi">10.1152/jn.90705.2008</pub-id><pub-id pub-id-type="pmid">18753323</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-Neuron perturbations reveal feature-specific competition in V1</article-title><source>Nature</source><volume>567</volume><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0997-6</pub-id><pub-id pub-id-type="pmid">30842660</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The distributed nature of working memory</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>111</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.12.007</pub-id><pub-id pub-id-type="pmid">28063661</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural mechanisms for interacting with a world full of action choices</article-title><source>Annual Review of Neuroscience</source><volume>33</volume><fpage>269</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135409</pub-id><pub-id pub-id-type="pmid">20345247</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colin Cameron</surname><given-names>A</given-names></name><name><surname>Windmeijer</surname><given-names>FAG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>An R-squared measure of goodness of fit for some common nonlinear regression models</article-title><source>Journal of Econometrics</source><volume>77</volume><fpage>329</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1016/S0304-4076(96)01818-0</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Klingberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neuroscience of working memory capacity and training</article-title><source>Nature Reviews. Neuroscience</source><volume>17</volume><fpage>438</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.43</pub-id><pub-id pub-id-type="pmid">27225070</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cossell</surname><given-names>L</given-names></name><name><surname>Iacaruso</surname><given-names>MF</given-names></name><name><surname>Muir</surname><given-names>DR</given-names></name><name><surname>Houlton</surname><given-names>R</given-names></name><name><surname>Sader</surname><given-names>EN</given-names></name><name><surname>Ko</surname><given-names>H</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional organization of excitatory synaptic strength in primary visual cortex</article-title><source>Nature</source><volume>518</volume><fpage>399</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1038/nature14182</pub-id><pub-id pub-id-type="pmid">25652823</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Domencich</surname><given-names>TA</given-names></name><name><surname>McFadden</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="1975">1975</year><source>Urban Travel Demand-A Behavioral Analysis. A Behavioral Analysis</source><publisher-loc>Amsterdam</publisher-loc><publisher-name>North-Holland</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dormann</surname><given-names>CF</given-names></name><name><surname>Elith</surname><given-names>J</given-names></name><name><surname>Bacher</surname><given-names>S</given-names></name><name><surname>Buchmann</surname><given-names>C</given-names></name><name><surname>Carl</surname><given-names>G</given-names></name><name><surname>Carré</surname><given-names>G</given-names></name><name><surname>Marquéz</surname><given-names>JRG</given-names></name><name><surname>Gruber</surname><given-names>B</given-names></name><name><surname>Lafourcade</surname><given-names>B</given-names></name><name><surname>Leitão</surname><given-names>PJ</given-names></name><name><surname>Münkemüller</surname><given-names>T</given-names></name><name><surname>McClean</surname><given-names>C</given-names></name><name><surname>Osborne</surname><given-names>PE</given-names></name><name><surname>Reineking</surname><given-names>B</given-names></name><name><surname>Schröder</surname><given-names>B</given-names></name><name><surname>Skidmore</surname><given-names>AK</given-names></name><name><surname>Zurell</surname><given-names>D</given-names></name><name><surname>Lautenbach</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Collinearity: a review of methods to deal with it and a simulation study evaluating their performance</article-title><source>Ecography</source><volume>36</volume><fpage>27</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1111/j.1600-0587.2012.07348.x</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>LN</given-names></name><name><surname>Pettit</surname><given-names>NL</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic reorganization of neuronal activity patterns in parietal cortex</article-title><source>Cell</source><volume>170</volume><fpage>986</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.07.021</pub-id><pub-id pub-id-type="pmid">28823559</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>AD</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name><name><surname>Caplan</surname><given-names>JB</given-names></name><name><surname>Fields</surname><given-names>TA</given-names></name><name><surname>Isham</surname><given-names>EA</given-names></name><name><surname>Newman</surname><given-names>EL</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cellular networks underlying human spatial navigation</article-title><source>Nature</source><volume>425</volume><fpage>184</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1038/nature01964</pub-id><pub-id pub-id-type="pmid">12968182</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>A</given-names></name><name><surname>Spiers</surname><given-names>H</given-names></name><name><surname>Bohbot</surname><given-names>V</given-names></name><name><surname>Rosenbaum</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Human Spatial Navigation</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Density-Based spatial clustering of applications with noise (DBSCAN</article-title><conf-name>Proc. of the Second International Conference on Knowledge Discovery and Data Mining</conf-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dedicated population for reward coding in the hippocampus</article-title><source>Neuron</source><volume>99</volume><fpage>179</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.008</pub-id><pub-id pub-id-type="pmid">30008297</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Big behavioral data: psychology, ethology and the foundations of neuroscience</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1455</fpage><lpage>1462</lpage><pub-id pub-id-type="doi">10.1038/nn.3812</pub-id><pub-id pub-id-type="pmid">25349912</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gothard</surname><given-names>KM</given-names></name><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dynamics of mismatch correction in the hippocampal ensemble code for space: interaction between path integration and environmental cues</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>8027</fpage><lpage>8040</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-24-08027.1996</pub-id><pub-id pub-id-type="pmid">8987829</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>E</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent circuit dynamics underlie persistent activity in the macaque frontoparietal network</article-title><source>eLife</source><volume>9</volume><elocation-id>e52460</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.52460</pub-id><pub-id pub-id-type="pmid">32379044</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>LR</given-names></name><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Mill</surname><given-names>RD</given-names></name><name><surname>Morrison</surname><given-names>LC</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Loftus</surname><given-names>MM</given-names></name><name><surname>Staskute</surname><given-names>L</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The hippocampus and entorhinal cortex encode the path and euclidean distances to goals during navigation</article-title><source>Current Biology</source><volume>24</volume><fpage>1331</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.001</pub-id><pub-id pub-id-type="pmid">24909328</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ilg</surname><given-names>UJ</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Eye movements of rhesus monkeys directed towards imaginary targets</article-title><source>Vision Research</source><volume>39</volume><fpage>2143</fpage><lpage>2150</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(98)00321-6</pub-id><pub-id pub-id-type="pmid">10343796</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ilg</surname><given-names>UJ</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visual tracking neurons in primate area MST are activated by smooth-pursuit eye movements of an “ imaginary ” target</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>1489</fpage><lpage>1502</lpage><pub-id pub-id-type="doi">10.1152/jn.00272.2003</pub-id><pub-id pub-id-type="pmid">12736240</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jutras</surname><given-names>MJ</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Buffalo</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Oscillatory activity in the monkey hippocampus during visual exploration and memory formation</article-title><source>PNAS</source><volume>110</volume><fpage>13144</fpage><lpage>13149</lpage><pub-id pub-id-type="doi">10.1073/pnas.1302351110</pub-id><pub-id pub-id-type="pmid">23878251</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of Dynamically Reconfigurable Hippocampal Responses by Learning to Perform Probabilistic Spatial Reasoning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/231159</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katsuki</surname><given-names>F</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Unique and shared roles of the posterior parietal and dorsolateral prefrontal cortex in cognitive functions</article-title><source>Frontiers in Integrative Neuroscience</source><volume>6</volume><elocation-id>17</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2012.00017</pub-id><pub-id pub-id-type="pmid">22563310</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komatsu</surname><given-names>H</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Relation of cortical areas MT and MST to pursuit eye movements. I. localization and visual properties of neurons</article-title><source>Journal of Neurophysiology</source><volume>60</volume><fpage>580</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1152/jn.1988.60.2.580</pub-id><pub-id pub-id-type="pmid">3171643</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A new neural framework for visuospatial processing</article-title><source>Nature Reviews. Neuroscience</source><volume>12</volume><fpage>217</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1038/nrn3008</pub-id><pub-id pub-id-type="pmid">21415848</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Karmos</surname><given-names>G</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection</article-title><source>Science</source><volume>320</volume><fpage>110</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1126/science.1154735</pub-id><pub-id pub-id-type="pmid">18388295</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Petsalis</surname><given-names>M</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dynamic Bayesian observer model reveals origins of bias in visual path integration</article-title><source>Neuron</source><volume>99</volume><fpage>194</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.040</pub-id><pub-id pub-id-type="pmid">29937278</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Neyhart</surname><given-names>E</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Tracking the mind’s eye: primate gaze behavior during virtual visuomotor navigation reflects belief dynamics</article-title><source>Neuron</source><volume>106</volume><fpage>662</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.02.023</pub-id><pub-id pub-id-type="pmid">32171388</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Leszczynski</surname><given-names>M</given-names></name><name><surname>Staudigl</surname><given-names>T</given-names></name><name><surname>Chaieb</surname><given-names>L</given-names></name><name><surname>Enkirch</surname><given-names>SJ</given-names></name><name><surname>Fell</surname><given-names>J</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Eye Movements Modulate Neural Activity in the Human Anterior Thalamus during Visual Active Sensing</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.30.015628</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>D</given-names></name><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Caziot</surname><given-names>B</given-names></name><name><surname>Laurens</surname><given-names>J</given-names></name><name><surname>Dickman</surname><given-names>JD</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spatial modulation of hippocampal activity in freely moving macaques</article-title><source>Neuron</source><volume>109</volume><fpage>3521</fpage><lpage>3534</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.09.032</pub-id><pub-id pub-id-type="pmid">34644546</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marigold</surname><given-names>DS</given-names></name><name><surname>Drew</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Posterior parietal cortex estimates the relationship between object and body location during locomotion</article-title><source>eLife</source><volume>6</volume><elocation-id>e28143</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.28143</pub-id><pub-id pub-id-type="pmid">29053442</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.1802.03426</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medendorp</surname><given-names>WP</given-names></name><name><surname>Heed</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>State estimation in posterior parietal cortex: distinct poles of environmental and bodily states</article-title><source>Progress in Neurobiology</source><volume>183</volume><elocation-id>101691</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2019.101691</pub-id><pub-id pub-id-type="pmid">31499087</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendoza-Halliday</surname><given-names>D</given-names></name><name><surname>Torres</surname><given-names>S</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sharp emergence of feature-selective sustained activity along the dorsal visual pathway</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1255</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1038/nn.3785</pub-id><pub-id pub-id-type="pmid">25108910</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>AF</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two distinct types of eye-head coupling in freely moving mice</article-title><source>Current Biology</source><volume>30</volume><fpage>2116</fpage><lpage>2130</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.042</pub-id><pub-id pub-id-type="pmid">32413309</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaiel</surname><given-names>AM</given-names></name><name><surname>Abe</surname><given-names>ET</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamics of gaze control during prey capture in freely moving mice</article-title><source>eLife</source><volume>9</volume><elocation-id>e57458</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57458</pub-id><pub-id pub-id-type="pmid">32706335</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Brown</surname><given-names>KD</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The spatial structure of neural encoding in mouse posterior cortex during navigation</article-title><source>Neuron</source><volume>102</volume><fpage>232</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.029</pub-id><pub-id pub-id-type="pmid">30772081</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Nawrot</surname><given-names>M</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mt neurons combine visual motion with a smooth eye movement signal to code depth-sign from motion parallax</article-title><source>Neuron</source><volume>63</volume><fpage>523</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.029</pub-id><pub-id pub-id-type="pmid">19709633</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Paré</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A selective impairment of motion perception following lesions of the middle temporal visual area (MT)</article-title><source>The Journal of Neuroscience</source><volume>8</volume><fpage>2201</fpage><lpage>2211</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.08-06-02201.1988</pub-id><pub-id pub-id-type="pmid">3385495</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Increased variability but intact integration during visual navigation in autism spectrum disorder</article-title><source>PNAS</source><volume>117</volume><fpage>11158</fpage><lpage>11166</lpage><pub-id pub-id-type="doi">10.1073/pnas.2000216117</pub-id><pub-id pub-id-type="pmid">32358192</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Caziot</surname><given-names>B</given-names></name><name><surname>Bruni</surname><given-names>S</given-names></name><name><surname>Fitzgerald</surname><given-names>NE</given-names></name><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Supporting generalization in non-human primate behavior by tapping into structural knowledge: examples from sensorimotor mappings, inference, and decision-making</article-title><source>Progress in Neurobiology</source><volume>201</volume><elocation-id>101996</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2021.101996</pub-id><pub-id pub-id-type="pmid">33454361</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cognitive, systems, and computational neurosciences of the self in motion</article-title><source>Annual Review of Psychology</source><volume>73</volume><fpage>103</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-021021-103038</pub-id><pub-id pub-id-type="pmid">34546803</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olesen</surname><given-names>PJ</given-names></name><name><surname>Westerberg</surname><given-names>H</given-names></name><name><surname>Klingberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Increased prefrontal and parietal activity after training of working memory</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>75</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nn1165</pub-id><pub-id pub-id-type="pmid">14699419</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Kenneth D.</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>PRL</given-names></name><name><surname>Abe</surname><given-names>ETT</given-names></name><name><surname>Leonard</surname><given-names>ESP</given-names></name><name><surname>Martins</surname><given-names>DM</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Joint coding of visual input and eye/head position in V1 of freely moving mice</article-title><source>Neuron</source><volume>16</volume><elocation-id>029</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.08.029</pub-id><pub-id pub-id-type="pmid">36137549</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inference in the brain: statistics flowing in redundant population codes</article-title><source>Neuron</source><volume>94</volume><fpage>943</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.028</pub-id><pub-id pub-id-type="pmid">28595050</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajkai</surname><given-names>C</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>CM</given-names></name><name><surname>Pincze</surname><given-names>Z</given-names></name><name><surname>Karmos</surname><given-names>G</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Transient cortical excitation at the onset of visual fixation</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>200</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm046</pub-id><pub-id pub-id-type="pmid">17494059</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rozzi</surname><given-names>S</given-names></name><name><surname>Calzavara</surname><given-names>R</given-names></name><name><surname>Belmalih</surname><given-names>A</given-names></name><name><surname>Borra</surname><given-names>E</given-names></name><name><surname>Gregoriou</surname><given-names>GG</given-names></name><name><surname>Matelli</surname><given-names>M</given-names></name><name><surname>Luppino</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical connections of the inferior parietal cortical convexity of the macaque monkey</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1389</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj076</pub-id><pub-id pub-id-type="pmid">16306322</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarel</surname><given-names>A</given-names></name><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Vectorial representation of spatial goals in the hippocampus of bats</article-title><source>Science</source><volume>355</volume><fpage>176</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1126/science.aak9589</pub-id><pub-id pub-id-type="pmid">28082589</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name><name><surname>Radman</surname><given-names>T</given-names></name><name><surname>Scharfman</surname><given-names>H</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dynamics of active sensing and perceptual selection</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>172</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id><pub-id pub-id-type="pmid">20307966</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semedo</surname><given-names>JD</given-names></name><name><surname>Zandvakili</surname><given-names>A</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical areas interact through a communication subspace</article-title><source>Neuron</source><volume>102</volume><fpage>249</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.026</pub-id><pub-id pub-id-type="pmid">30770252</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serino</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Peripersonal space (PPS) as a multisensory interface between the individual and the environment, defining the space of the self</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>99</volume><fpage>138</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.01.016</pub-id><pub-id pub-id-type="pmid">30685486</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Malik</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Normalized cuts and image segmentation</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>22</volume><fpage>888</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1109/34.868688</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strange</surname><given-names>BA</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Lein</surname><given-names>ES</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional organization of the hippocampal longitudinal axis</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>655</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1038/nrn3785</pub-id><pub-id pub-id-type="pmid">25234264</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>M</given-names></name><name><surname>Gottlieb</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distinct neural mechanisms of distractor suppression in the frontal and parietal lobe</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>98</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1038/nn.3282</pub-id><pub-id pub-id-type="pmid">23242309</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swaminathan</surname><given-names>SK</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Preferential encoding of visual categories in parietal cortex compared with prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>315</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1038/nn.3016</pub-id><pub-id pub-id-type="pmid">22246435</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>van Wingerden</surname><given-names>M</given-names></name><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The pairwise phase consistency: a bias-free measure of rhythmic neuronal synchronization</article-title><source>NeuroImage</source><volume>51</volume><fpage>112</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.01.073</pub-id><pub-id pub-id-type="pmid">20114076</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>SCH</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Theoretical perspectives on active sensing</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.06.009</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>SBM</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Pearson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Continuous decisions</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>376</volume><elocation-id>20190664</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0664</pub-id><pub-id pub-id-type="pmid">33423634</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80280.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.10.22.465526" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.22.465526"/></front-stub><body><p>This important study investigates distributed neural coding across the three brain areas MST, 7a, and dlPFC in monkeys carrying out a novel behavioural paradigm with a naturalistic closed action-perception-loop developed by the same group previously. The convincing model-based analysis discerns potential influences (e.g. task variables, hidden variables) on firing rates and supports the claim of task-specific sub-networks being formed. The authors provide an important first step to unravel potential drivers of dynamic activity in distributed networks during recurrent action-perception-loops, which should be augmented by future analyses of, for instance, the contribution of changing visual input, especially as the recordings stem from areas involved in processing optical flow, and of signals across different circuit elements like cortical layers.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80280.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Schölvinck</surname><given-names>Marieke</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ygt2y02</institution-id><institution>Ernst Strungmann Institute for Neuroscience</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.22.465526">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.22.465526v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Coding of latent variables in sensory, parietal, and frontal cortices during virtual closed-loop navigation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Marieke Schölvinck (Reviewer #2) and Sujaya Neupane (#3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this letter to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Heat maps of preferred angle and distance in Figure 3 must be cross-validated across trials to show data reliability. Maybe the authors have done that and these are cross-validated plots. If so, it is not mentioned in the methods anywhere. Please verify the ordering of preferred angle and distance hold up with cross-validation. One can get a spurious, evenly tiled coding of any continuous variable if one takes a random matrix (say mean FR x variable value), normalizes each row (FR) and sorts the column by peak location (i.e. preferred variable value) for each row.</p><p>2) We are a bit confused by the distribution of preferred latent variables (Figure 3). For e.g. travelled distance and distance to target are anti-correlated. Isn't it trivial that the preferred coding would appear bimodal across a neural population if some neurons are coding for one and some for the other? For a travelled distance coding neuron, there is nothing to code at the onset since distance travelled is 0 and vice versa for distance to the target coding neuron at the offset.</p><p>Related to #2, it would be helpful to see PSTH examples of single neurons that code for travelled distance and those that code for distance to target. PSTH would be obtained by averaging across trials, binned over a range of trial-lengths (e.g. bin1: short trial length, bin2: medium trial length, bin3: long trial length). We would expect clear differences in firing rate at the onset for distance to target coders and at the offset for travelled distance coders. It is difficult to see this in the presented rasters, although according to P-GAM results, that should be the case.</p><p>3) It would be helpful to provide a few examples of LFP traces and their filtered form along with spike times to appreciate the phase modulations apparent in their statistical modelling results (Figure 2F).</p><p>4) Tuning strength.</p><p>From the manuscript it is difficult to judge how representative the different neuronal populations for each area are and to what extent their selectivity differs. The analysis and it variables are quite complex to follow. It would be helpful for the reader to understand how some of them relate to more traditional measures. It is great that the focus on single neurons allows this comparison.</p><p>What does &quot;tuned&quot; in this context mean in terms of strength and selectivity?</p><p>How many neurons would pass a minimal response criterion like 10 spikes/s. Would these show stronger tuning or correlations?</p><p>Figure 2E: &quot;E. Responses from an example MSTd, 7a, and dlPFC neuron (black), aligned to temporal task variables (e.g., time of movement onset and offset), or binned according to their value in a continuous task variable (e.g., linear velocity).&quot; It would be helpful to give the x-axis for each line for the reader to be able to ascertain what the nature of the scale and the range of variables are over which the firing rate changes are depicted.</p><p>Could the authors derive from their data one of the traditional measures used for MST, like a direction tuning index? A direct comparison with previous studies could help understand the nature of the sampled pool, particularly (but not exclusively) for areas when smaller neuronal samples, like MST.</p><p>5) Eye movements and visual input.</p><p>Another issue is the extent that it seems difficult to distinguish the effect of eye position from that of the background stimulus flow patterns, which of course must differ in direction and element size when animals fixate at different locations on the screen. To what extent was this visual input to neurons correlated with &quot;latent variables&quot; like latent distance and angle to target (latent spatial goal)?</p><p>In order to dissociate the contribution of eye position and task from visual input, do the authors have data on a passive viewing control condition, in which the animal fixates and the visual pattern is played back to animals exactly as if in an active one? How do neural responses compare across the three areas?</p><p>Could the authors discuss in the paper how the visual input is (or not) included in the model?</p><p>6) MSTd and dlPFC coupling.</p><p>a) As the animals were head-fixed, eye position would compensate in some cases the animals might have moved its head position (for instance to keep track of the target). Both, MSTd and dlPFC encoded eye position. Could the close coupling of MSTd and dlPFC be linked to this element of the task?</p><p>b) The authors claim that areas MSTd and dlPFC form a functional sub-network together, on the basis of similarity in the fractions of neurons tuned to certain variables, and the distribution of the preferred value of some of these variables. However, the fractions of neurons tuned to the latent variables in MSTd and dlPFC (see Figure 2F) are actually quite different. Perhaps the authors could comment on this.</p><p>c) When there was stronger MSTd-to-dlPFC coupling and better tracking of the hidden firefly with the eyes (Figure 5B), was the performance of the monkey also better (i.e. more hits)?</p><p>7) Sampling of areas.</p><p>a) Area 7a was exclusively sampled with chronic rather than moveable probes. It has also the largest number of &quot;single units&quot;.</p><p>To what extent are these single units independent?</p><p>Could a sampling bias in these probes (part of 7a; layers) affect the results, especially when it comes to coupling. Please include in the discussion.</p><p>b) The number of recorded neurons in the three areas differs greatly: 231 units in MSTd, 823 units in dlPFC, and 3200 units in area 7a. Yet many conclusions in the paper rely on neuronal numbers: the fractions of neurons tuned to certain sensorimotor and latent variables differ between the areas, the variables explaining the firing rates cluster differently in the neurons of the three areas, and both the coarse LFP connectivity and the fine unit-to-unit coupling within areas differ. Especially the clustering results might depend on the number of recorded neurons: the fact that almost all MSTd and dlPFC neurons are categorized as belonging to the same cluster, whereas the area 7a neurons appear in three distinct clusters, could be caused by the much larger number of recorded neurons in area 7a. Also unit-to-unit coupling is more likely to show up in the data with a much larger number of recorded neurons. The data could be corrected for these differences in number of recorded neurons.</p><p>8) Lateralisation.</p><p>To what extent played the lateralization of the recording and task a role for neuronal response? This applies relative to brain hemisphere, body and eye position? Where in each monkey did the recordings take place? Which hand(s) did each monkeys use for the choice stick?</p><p>How was the lateralisation included in the model?</p><p>Please comment with regards to responses in MST, 7A, and dPFC and add information to the manuscript.</p><p>Specifically, it is unclear from Suppl Figure 1 whether within a particular monkey, some recording sites were interhemispheric, or whether within one monkey, all recordings were done in the same hemisphere. This of course has significant consequences for the effects of ongoing LFP and unit-to-unit coupling.</p><p>9) Data fed into the P-GAM model.</p><p>a) The P-GAM model is a great analysis tool for these kinds of data. However, the variables that the authors put into it are conceptually very different from each other. There are purely external task variables such as target onset and offset, latent variables such as distance to target that require knowledge of one's own position in space, and purely internal brain dynamics variables such as coupling to the LFP in another area. In that light, the finding of 'many variables contributing to the responses' is not surprising; all neurons in the brain are probably influenced both by external variables and internal brain dynamics. Maybe the authors could comment on the different nature of their variables and how that impacts their results.</p><p>b) Given that the sensorimotor and latent variables going into the G-PAM model are so crucial for the story, could you make a figure where you visualize them? This could maybe be added to Figure 1A. Also, 'radial bias' and 'angular bias' (in Fig1D) could be visualized here.</p><p>c) Quantification of electrophysiological activity processing that is fed into the P-GAM model is not entirely clear.</p><p>More details about the preprocessing of these data are required, for example, are the SUA baselined using pre-stimulus presentation activity? Are the LFP baselined as well? And how similar are the pooled responses within each area and across? This would allow the reader to spot possible problems when computing further neuronal properties, that could bias the main paper result:</p><p>An example is the tuning of the neurons to the phase of ongoing oscillation (Β, Α, Theta). There are a number of papers attempting to optimize methods to measure spike field coherency, e.g. the PPC pairwise phase consistency (Vinck et al., 2010). This method gives an estimation independent of spike count and LFP amplitudes (both parameter vary of course widely across time, tasks, subjects, areas…).</p><p>Here, it seems these two parameters are not considered and could lead to artefacts in the coupling results presented. The authors use temporal correlations to approximate coupling between spike/spike, and spike/LFP-phase. Correlation methods can potentially lead to artefacts and overestimations of coupling strength.</p><p>In their methods, the author state to 'bin spiking activity across 8ms window' prior to feeding this activity to the P-GAM. It means that 1 spike corresponds to an averaged 8ms time window. If you now try to calculate the dependency of this single spike to a specific phase of a β (30Hz) , the α (12Hz) and theta (4Hz) oscillation, it means that the chance level of assigning the binned spike to a particular phase differs considerably. Therefore, the statistical power of this analysis would decrease for higher frequency. It seems that the authors do not apply any correction.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) From the manuscript it is difficult to judge how representative the different neuronal populations for each area are and to what extent their selectivity differs. The analysis and its variables are quite complex to follow. It would be helpful for the reader to understand how some of them relate to more traditional measures. It is great that the focus on single neurons allows this comparison.</p><p>What does &quot;tuned&quot; in this context mean in terms of strength and selectivity?</p><p>How many neurons would pass a minimal response criterion like 10 spikes/s. Would these show stronger tuning or correlations?</p><p>Figure 2E: &quot;E. Responses from an example MSTd, 7a, and dlPFC neuron (black), aligned to temporal task variables (e.g., time of movement onset and offset), or binned according to their value in a continuous task variable (e.g., linear velocity).&quot; It would be helpful to give the x-axis for each line for the reader to be able to ascertain what the nature of the scale and the range of variables are over which the firing rate changes are depicted.</p><p>Could the authors derive from their data one of the traditional measures used for MST, like a direction tuning index? A direct comparison with previous studies could help understand the nature of the sampled pool, particularly (but not exclusively) for areas when smaller neuronal samples, like MST.</p><p>2) Another issue is the extent that it seems difficult to distinguish the effect of eye position from that of the background stimulus flow patterns, which of course must differ in direction and element size when animals fixate at different locations on the screen. To what extent was this visual input to neurons correlated with &quot;latent variables&quot; like latent distance and angle to target (latent spatial goal)?</p><p>In order to dissociate the contribution of eye position and task from visual input, do the authors have data on a passive viewing control condition, in which the animal fixates and the visual pattern is played back to animals exactly as if in an active one? How do neural responses compare across the three areas?</p><p>Could the authors discuss in the paper how the visual input is (or not) included in the model?</p><p>3) As the animals were head-fixed, eye position would compensate in some cases the animals might have moved its head position (for instance to keep track of the target). Both, MSTd and dlPFC encoded eye position. Could the close coupling of MSTd and dlPFC be linked to this element of the task?</p><p>4) Area 7a was exclusively sampled with chronic rather than moveable probes. It has also the largest number of &quot;single units&quot;.</p><p>To what extent are these single units independent?</p><p>Could a sampling bias in these probes (part of 7a; layers) affect the results, especially when it comes to coupling. Please include in the discussion.</p><p>5) To what extent played the lateralization of the recording and task a role for neuronal response? This applies relative to brain hemisphere, body and eye position? Where in each monkey did the recordings take place? Which hand(s) did each monkeys use for the choice stick?</p><p>How was the lateralisation included in the model?</p><p>Please comment with regards to responses in MST, 7A, and dPFC.</p><p>6) Quantification of task parameters are quite clear, this is not entirely the case for electrophysiological activity processing that they feed into their P-GAM model.</p><p>More details about the preprocessing of these data are required, for example, are the SUA baselined using pre-stimulus presentation activity? Are the LFP baselined as well? And how similar are the pooled responses within each area and across? This would allow the reader to spot possible problems when computing further neuronal properties, that could bias the main paper result:</p><p>An example is the tuning of the neurons to the phase of ongoing oscillation (Β, Α, Theta). There are a number of papers attempting to optimize methods to measure spike field coherency, e.g. the PPC pairwise phase consistency (Vinck et al., 2010). This method gives an estimation independent of spike count and LFP amplitudes (both parameter vary of course widely across time, tasks, subjects, areas…).</p><p>Here, it seems these two parameters are not considered and could lead to artefacts in the coupling results presented. The authors use temporal correlations to approximate coupling between spike/spike, and spike/LFP-phase. Correlation methods can potentially lead to artefacts and overestimations of coupling strength.</p><p>In their methods, the author state to 'bin spiking activity across 8ms window' prior to feeding this activity to the P-GAM. It means that 1 spike correspond to an averaged 8ms time window. If you now try to calculate the dependency of this single spike to a specific phase of a β (30Hz) , the α (12Hz) and theta (4Hz) oscillation, it means that the chance level of assigning the binned spike to a particular phase differs considerably. Therefore, the statistical power of this analysis would decrease for higher frequency. It seems that the authors do not apply any correction.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– I am missing a clear motivation for recording in the three areas that you chose. Could you maybe elaborate on this a bit more in the introduction?</p><p>– Given that the sensorimotor and latent variables going into the G-PAM model are so crucial for the story, could you make a figure where you visualize them? This could maybe be added to Figure 1A. Also, 'radial bias' and 'angular bias' (in Fig1D) could be visualized here.</p><p>– In Figure 1C, you have added 'slope=bias', whereas technically, it is 'deviation from slope=bias'.</p><p>– The legend of Figure 2 is extremely long and contains a lot of information that does not pertain directly to the figure. I suggest that the part '(The direct comparison of the goodness-of-fit….the complexity of their areas and tasks, reaches)' in Fig2D is taken out and added to the text somewhere else.</p><p>– It is unclear from Suppl Figure 1 whether within a particular monkey, some recording sites were interhemispheric, or whether within one monkey, all recordings were done in the same hemisphere. This of course has significant consequences for the effects of ongoing LFP and unit-to-unit coupling.</p><p>– In Fig2F, you show fractions of neurons tuned to the several variables of the G-PAM model, and in Fig4D, you show proportions of neurons phase-locked to LFP phases in other areas. I might have missed it, but I didn't see any quantification of how strong the tuning was, and how strong the phase-locking.</p><p>– When there was stronger MSTd-to-dlPFC coupling and better tracking of the hidden firefly with the eyes (Figure 5B), was the performance of the monkey also better (i.e. more hits)?</p><p>– There are a few spelling mistakes throughout the paper (psueudo-R on p.6; tunning on p.7)</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. Heat maps of preferred angle and distance in Figure 3 must be cross-validated across trials to show data reliability. Maybe the authors have done that and these are cross-validated plots. If so, it is not mentioned in the methods anywhere. Please verify the ordering of preferred angle and distance hold up with cross-validation. One can get a spurious, evenly tiled coding of any continuous variable if one takes a random matrix (say mean FR x variable value), normalizes each row (FR) and sorts the column by peak location (i.e. preferred variable value) for each row.</p><p>2. I am a bit confused by the distribution of preferred latent variables (Figure 3). For e.g. travelled distance and distance to target are anti-correlated. Isn't it trivial that the preferred coding would appear bimodal across a neural population if some neurons are coding for one and some for the other? For a travelled distance coding neuron, there is nothing to code at the onset since distance travelled is 0 and vice versa for distance to the target coding neuron at the offset.</p><p>3. Related to #2 above, it would be helpful to see PSTH examples of single neurons that code for travelled distance and those that code for distance to target. PSTH would be obtained by averaging across trials, binned over a range of trial-lengths (e.g. bin1: short trial length, bin2: medium trial length, bin3: long trial length). I would expect clear differences in firing rate at the onset for distance to target coders and at the offset for travelled distance coders. It is difficult to see this in the presented rasters, although according to P-GAM results, that should be the case.</p><p>4. It would be helpful to provide a few examples of LFP traces and their filtered form along with spike times to appreciate the phase modulations apparent in their statistical modelling results (Figure 2F).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80280.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Heat maps of preferred angle and distance in Figure 3 must be cross-validated across trials to show data reliability. Maybe the authors have done that and these are cross-validated plots. If so, it is not mentioned in the methods anywhere. Please verify the ordering of preferred angle and distance hold up with cross-validation. One can get a spurious, evenly tiled coding of any continuous variable if one takes a random matrix (say mean FR x variable value), normalizes each row (FR) and sorts the column by peak location (i.e. preferred variable value) for each row.</p></disp-quote><p>We thank the reviewer for this suggestion. We had previously not cross-validated those plots, but have now. We fit the P-GAM to half the dataset (either “odd” or “even” trials). Then, for the figure referenced by the reviewer, we find the distance and angle that most strongly drives a particular neuron (given that the neuron is significantly tuned to the variable), as indicated by their tuning function (i.e., peak of the tuning function). Neurons were then sorted based on their preferred angle/distance in “even” trials, and the tuning functions generated by the “odd” trials are plotted as a heatmap. As shown in Figure 3, the results originally reported hold after cross-validation.</p><p>We have amended the figure caption and the methods section to include this information.</p><p>Figure caption:</p><p>“Heatmaps showing neural responses (y-axis) sorted by preferred angles from origin (top), angle to target (2<sup>nd</sup> row), distance from origin (3<sup>rd</sup> row), and distance to target (bottom row) for MSTd (green), 7a (blue) and dlPFC (red) in monkey S (data simultaneously recorded). Darker color indicates higher normalized firing rate. Neurons were sorted based on their preferred distances/angles in even trials and their responses during odd trials is shown (i.e., sorting is cross-validated, see Methods).”</p><p>Methods:</p><p>“To show the stability in the estimated tuning functions, Figure 2 – supplement 10 shows the fraction of units tuned to a given task-variable as a function of brain area, and as a function of whether odd or even trials were fit to the P-GAM. Namely, we fit half of the dataset each time and show that the fraction of neurons tuned to a given task variable was the same regardless of whether we fit the odd numbered trials, or the even numbered trials. Similarly, we index the “preferred” distances and angles from origin and to target (Figure 3) as defined by the peak of tuning functions. In Figure 3B we sort neurons according to their preferred distances or angles in one subset of trials (i.e., “even” trials) and plot the normalized responses in the other subset of trials (“odd” trials). Figure 3B, therefore demonstrates that not only the fraction of neurons tuned to different variables was stable, but the estimated tuning functions were as well.”</p><disp-quote content-type="editor-comment"><p>2) We are a bit confused by the distribution of preferred latent variables (Figure 3). For e.g. travelled distance and distance to target are anti-correlated. Isn't it trivial that the preferred coding would appear bimodal across a neural population if some neurons are coding for one and some for the other? For a travelled distance coding neuron, there is nothing to code at the onset since distance travelled is 0 and vice versa for distance to the target coding neuron at the offset.</p><p>Related to #2, it would be helpful to see PSTH examples of single neurons that code for travelled distance and those that code for distance to target. PSTH would be obtained by averaging across trials, binned over a range of trial-lengths (e.g. bin1: short trial length, bin2: medium trial length, bin3: long trial length). We would expect clear differences in firing rate at the onset for distance to target coders and at the offset for travelled distance coders. It is difficult to see this in the presented rasters, although according to P-GAM results, that should be the case.</p></disp-quote><p>We apologize for the confusion and recognize this deserved a more detailed explanation. The distance from origin and to target are not correlated (in fact, if they were fully correlated the P-GAM would be under-specified). Within a trial, these distances could be correlated (e.g., if the target were straight ahead and the animal traveled in a perfectly straight line), but need not. For instance, if the target were at 200 cm straight ahead, and the animal travelled in a perfect line 100 cm. Now the animal is 100cm from the origin and from the target. But if the animal now overshoots the target by 100cm, they are now 300cm from the origin, and still 100 cm from the target. A similar (more realistic) example happens when the animal does not take an optimal path, but increases their distance from origin without decreasing their distance from the target (e.g., think of a target that is 200cm from the origin / the animal at trial onset, the animal could navigate to form a perfect isosceles triangle, where they are now 200cm from target and origin). More importantly, on different trials the target appears at a random distance between 100cm and 400cm. Thus, if the animal travelled 100cm (imagine in a perfect line to the target), on different trials they could be 0cm from the target (first case), or 300cm from the target (second case). Thus, particularly across trials, there is absolutely no correlation between distance travelled and distance to target, and hence why we can distinguish between these.</p><p>We agree with the reviewers that illustrating this point and providing PSTHs (and even more strikingly, rasters) would go a long way to clarifying this issue. We did not perform the exact analysis suggested by the reviewers (because it is not necessarily the case that firing rates vary monotonically with distance, as their suggestion implies), but have added raster plots for example neurons tuned to either the distance from origin or to target. We also show an example neuron that responds to movement stop. This latter neuron, thus, shows a pattern similar to that of Example 2 when sorted from starting location (c.f., Figure 3A third and fifth panel), but critically, when aligned to target location it is evident there is no relation between its firing pattern and the distance to target. Hence, we can differentiate between the distance from origin, to target, and simply responding to moments of starting or stopping movements.</p><p>Further, we have modified the main text:</p><p>“Beyond the frequency with which we observe neurons tuned to the angle and distance from the origin (i.e., path integration) and to the target (i.e., vector coding of spatial goals), we may expect the distributions of preferred distances and angles to also be informative. Of note, distance/angle from origin and to the target are not the reciprocal of one another given that the target location varies on a trial-by-trial fashion. In other words, the travelled distance and the distance to target may correlate within a trial (but need not, given under- vs. overshooting) but certainly do not across trials (e.g., a distance of, say, 100cm from the origin could corresponding to a whole host of distances from target). In Figure 3A we show rasters of representative neurons tuned to the distance from origin (example neuron 1) and to target (example neuron 2). The neuron tuned to the distance to target (example neurons 2) is not tuned to a particular distance from origin, but does demonstrate a patterned firing rate, discharging at further distances as the animal travels further. The third example (Figure 3A) is tuned to movement stopping, and demonstrates a pattern similar to the neuron tuned to distance to target when plotted as a function of distance from origin (Figure 3A, 3<sup>rd</sup> vs. 5<sup>th</sup> panel), but not when visualized as a function of distance to target.</p><p>And the figure caption:</p><p>“A. Rasters and average firing rate of three example neurons, sorted by their maximal distance from origin and to target. The first example neuron (left) responds at a distance of ~100cm from origin and is not modulated by distance to target. The second example (middle) responds to a close distance to target (~30cm). Arrows at the top of these rasters indicate the preferred distance from origin (example 1) and to target (example 2). We include a third example (tuned to movement stop) as a control, demonstrating that responding to a distance near the target and to stopping behavior are distinguishable.”</p><disp-quote content-type="editor-comment"><p>3) It would be helpful to provide a few examples of LFP traces and their filtered form along with spike times to appreciate the phase modulations apparent in their statistical modelling results (Figure 2F).</p></disp-quote><p>We agree with the reviewers. We have added a new supplement figure (Figure 2 —figure supplement 8), where we show a few example LFP traces, their band-passed version, and the phases. We also show the spikes and how they align to the ongoing phase. Finally, we have also added histograms showing the phase at which a few example neurons fired throughout the course of a recording.</p><p>The accompanying figure captions is:</p><p>Figure 2 – supplement figure 8. Illustration of spike-LFP phase locking. A. Example trials. For each of four example trials (different sessions as well) we show the raw LFP (top), as well as the band-passed version (transparent) and extracted phase (opaque) in theta (green, second row), α (orange, third row), and β (blue, fourth row) ranges. Spikes are represented by dots, and they are placed on the y-axis according to the phase of the ongoing LFP. That is, across rows spikes occur at the same time along the x-axis, but are at different y-locations. If a neuron is phased-locked to LPF in a given range, spikes should predominantly occur at the same y location (as is seen in these examples for the Β band). B. Example sessions. For 6 example neurons, we show the distribution of phases (x-axis, in radians) at which spikes occurred, throughout the entire session. We show 2 example neurons that were not modulated by LFP phase (1<sup>st</sup> and 2<sup>nd</sup> column, uniformly distributed), 2 example neurons that were modulated solely by Β frequency phases (3<sup>rd</sup> and 4<sup>th</sup> column), and finally 2 example neurons that were modulated by phases at Theta, Α, and Β frequencies.</p><disp-quote content-type="editor-comment"><p>4) Tuning strength.</p><p>From the manuscript it is difficult to judge how representative the different neuronal populations for each area are and to what extent their selectivity differs. The analysis and it variables are quite complex to follow. It would be helpful for the reader to understand how some of them relate to more traditional measures. It is great that the focus on single neurons allows this comparison.</p><p>What does &quot;tuned&quot; in this context mean in terms of strength and selectivity?</p><p>How many neurons would pass a minimal response criterion like 10 spikes/s. Would these show stronger tuning or correlations?</p></disp-quote><p>We thank the reviewers for this question and consider that in essence they are asking a question about effect sizes. We have added 4 supplementary figures addressing this question (explained below), but must first add the caveat that when examining “raw” firing rates (e.g., response criterion of 10 spikes/s) in a naturalistic task where variables are continuous, dynamic, and correlated, it is not entirely clear what drives the response. This is exactly why we must use a method such as the P-GAM, attempting to factorize variance (i.e., perform credit assignment).</p><p>The 4 supplement figures we add (Figure 2 —figure supplement 3, 4, 5, and 6) follow the same format and provide 2 new analyses regarding effect sizes. First, for each sensorimotor (supplement 3), latent (supplement 4), LFP phase (supplement 5) or “other” (supplement 6) variable, we find the neurons that are and are not significantly tuned to that variable. Then, we compute a selectivity index, by computing the difference between the maximum and minimum response of a tuning function (in firing rate space). This gives a more traditional “evoked response”. Then for each population of neurons (significant vs. not) we log transform their evoked response (to render the population Gaussian) and compute Cohen’s d (the distance between means of the distributions normalized by their variance). As it can be observed in only Figure 2 —figure supplement 3, the effect sizes are considerable.</p><p>The percentage of neurons showing an increase in firing rate above 10spikes/s for linear velocity, linear acceleration, angular velocity, angular acceleration, the timing of movement onset, offset, and the timing of target presentation are respectively,</p><p>For significant units: 10.7%, 34.4%,10.7%, 19.8%, 21.4%, 21.0%, and 3.1%. Non-significant units: 1.5%, 6.0%, 1.1%, 2.3%, 2.2%, 0.5%, and 2.0%.</p><p>The second analysis performed (second column) is similar to the first, but contrasting mutual information (a measure of correlation) between evoked responses in the firing rate space and task variables. Findings are conceptually the same as for the first approach.</p><p>Associated caption:</p><p>“Figure 2 – supplement figure 3. Effect sizes in the firing rate space for neurons deemed to code for sensorimotor variables. Rows are the different sensorimotor variables, in the same order as in Figure 2E and F. The left column is the difference in evoked firing rate between the population of neurons deemed to significantly code for, or not, a given task variable. Namely, for each neuron we compute the difference in firing rate between the peak and the trough of its tuning function. The populations of significant and non-significant neurons are then log-transformed (to render normally distributed) and Cohen’s d is computed (indicated as the title of each subplot). Right column, as for the left column, but while contrasting the mutual information present between firing rates and the given task variable. For reference, Cohen’s d &lt; 0.2 are typically considered weak effects, ~0.5 are considered moderate, and &gt; ~0.8 are considered strong effects.”</p><disp-quote content-type="editor-comment"><p>Figure 2E: &quot;E. Responses from an example MSTd, 7a, and dlPFC neuron (black), aligned to temporal task variables (e.g., time of movement onset and offset), or binned according to their value in a continuous task variable (e.g., linear velocity).&quot; It would be helpful to give the x-axis for each line for the reader to be able to ascertain what the nature of the scale and the range of variables are over which the firing rate changes are depicted.</p></disp-quote><p>We thank the reviewer for this suggestion. An x-axis has been added. The figure is as follows:</p><disp-quote content-type="editor-comment"><p>Could the authors derive from their data one of the traditional measures used for MST, like a direction tuning index? A direct comparison with previous studies could help understand the nature of the sampled pool, particularly (but not exclusively) for areas when smaller neuronal samples, like MST.</p></disp-quote><p>We thank the reviewers for this suggestion. The traditional measure our group (e.g., Chen et al., 2008 in MSTd, and Avila et al., 2019 in 7a) has employed is the “discrimination index”, defined as,<inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> Where Rmax and Rmin are the maximum and minimum response from a tuning function, SSE is the sum of squared errors around the mean responses, M is the number of stimulus directions, and N is the total number of observations. In the context of the current naturalistic experiment, we may bin linear and angular velocities and compute tuning functions defining Rmax and Rmin. The number of bins (here we used 15) defines M. To estimate SSE and N we must define trials wherein the full gamut of linear and angular velocities are experienced. To facilitate direct comparison with Chen et al., 2008, we divided our recordings in 80 segments, the mean number of trials (N) in Chen et al., 2008. Lastly, we computed DDI.</p><p>Importantly, we plot the mean estimates for MSTd (green), area 7a (blue), and dlPFC (red) from the current dataset in filled triangles, as well as the mean estimates for 7a (from Avila et al., 2019) and MSTd (from Chen et al., 2008) from “traditional experiments”. In general, there is good agreement between studies with MSTd, 7a, and finally dlPFC (in that order) showing the strongest discrimination for optic flow. We must note, however, that there are of course also coarse-grain differences, particularly for MSTd. This is expected given that in contrast to the P-GAM estimate, simply binning data and averaging within these bins does not factorize for the contribution of other experimental factors.</p><p>We have included this figure as Figure 2 —figure supplement 7. We have amended the text in the following manners:</p><p>Results:</p><p>“Similarly, to provide a point of comparison with prior work studying optic flow processing, in Figure 2 —figure supplement 7 we quantify the speed (i.e., liner velocity) and direction (i.e., angular velocity) discrimination index (see Methods and e.g., Chen et al., 2008; Avila et al., 2019) for neurons in MSTd, 7a, and dlPFC.”</p><p>Methods:</p><p>“To allow for comparison with prior reports studying optic flow processing within the cadre of two-alternative forced-choice tasks, we compute the discrimination index for speed (i.e., linear velocity) and direction (i.e., angular velocity) in MSTd, 7a, and dlPFC. The discrimination index (DDI) was defined as,<inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> Where Rmax and Rmin are the maximum and minimum response from a tuning function, SSE is the sum of squared errors around the mean responses, M is the number of stimulus directions, and N is the total number of observations. In the context of the current naturalistic experiment, we may bin linear and angular velocities and compute tuning functions defining Rmax and Rmin. The number of bins (here we used 15 nodes, as defined by the P-GAM) defines M. To estimate SSE and N we must define trials wherein the full gamut of linear and angular velocities are experienced. To facilitate direct comparison with Chen et al., 2008, we divided our recordings in</p><p>80 segments, the mean number of trials (N) in Chen et al., 2008. Lastly, we computed DDI according to Equation 5.”</p><p>Figure caption:</p><p>“Figure 2 – supplement figure 7. Speed and direction discrimination index for neurons in MSTd, 7a, and dlPFC. To allow for direct comparison with prior studies, we compute the discrimination index (see <italic>Methods</italic>) for speed (i.e., linear velocity) and direction (i.e., angular velocity) in MSTd (green), 7a (blue), and dlPFC (red). Full triangles at the top indicate the mean of each population recorded from here (in their corresponding color), while the empty blue triangles show the mean in Avila et al., 2019 (7a recording) and the empty green triangles show the mean in Chen et al., 2008 (MSTd recordings).”</p><disp-quote content-type="editor-comment"><p>5) Eye movements and visual input.</p><p>Another issue is the extent that it seems difficult to distinguish the effect of eye position from that of the background stimulus flow patterns, which of course must differ in direction and element size when animals fixate at different locations on the screen. To what extent was this visual input to neurons correlated with &quot;latent variables&quot; like latent distance and angle to target (latent spatial goal)?</p><p>In order to dissociate the contribution of eye position and task from visual input, do the authors have data on a passive viewing control condition, in which the animal fixates and the visual pattern is played back to animals exactly as if in an active one? How do neural responses compare across the three areas?</p></disp-quote><p>These are excellent questions; we address them both together.</p><p>In principle there is no correlation between the visual input the animals receive and the latent variables. As it can be observed in Figure 1B (bottom) the animals most often initially move forward and rotate to face the target straight-ahead. Then, after approximately the first 500-1000ms of the trial, they move forward at a constant speed (maximal linear velocity but little to no angular component). Thus, while distance to target is continuously changing (decreasing if the animal is performing the task well), their linear velocity (driving much of the visual stimulation) is held constant.</p><p>Nonetheless, the reviewer is correct that the manuscript would be substantially bolstered by providing empirical evidence for the fact that the neural properties we report are in fact related to the animal’s computing taskrelevant metrics and not purely sensory. To address this, we perform exactly what the reviewer suggested. We recorded 2 sessions from area 7a (117 neurons) while the animal first engaged in the task, and then passively viewed the same stimuli replayed (unfortunately recordings in MSTd and dlPFC were not possible at this time). Figure 2 —figure supplement 11 (panel A) shows 7 example trials, showing that stimuli (linear velocity shown, but applies to all task variables) were matched between active and passive conditions. Panel A also shows single trial evoked responses (mean for all simultaneously recorded neurons in the first session, 56 neurons). As it can be observed, there are (single trial) evoked responses when the animal is activity performing the task, but not when passively viewing stimuli. Panel B shows that the mean firing rate (across the entire recording) was unchanged between the active version of the task and passive viewing (dashed line is the diagonal and blue circle is the mean across all neurons). Lastly, Panel C shows summary statistics: the fraction of neurons tuned to different task variables and the fraction of neurons coupled, in active (blue) and passive (black) conditions. All variables except for the phase alignment with LFP bands was significantly blunted in the passive viewing condition. Interestingly, still ~28% of neurons showed tuning to the sensory variables (“latent” and “other” variables average in the passive viewing conditions = 14.8%). Coupling filters were also less prominent in the passive viewing condition (mean ± sem: active = 20.62% ± 0.0047; passive = 15.30% ± 0.0041 ; p = 4.03 x 10<sup>-18</sup>).</p><p>The text has been amended in the following manner:</p><p>Results:</p><p>“In the supplement we demonstrate that this coding was stable (contrasting odd vs. even trials; Figure 2 —figure supplement 10) and task-relevant (Figure 2 —figure supplement 11), in that passive viewing of the same stimuli did not elicit a comparable fraction of neurons tuned to task variables in 7a (passive viewing data in MSTd and dlPFC were unavailable). The fraction of neurons aligned with the phase of LFP in different frequency bands remained stable across passive and active viewing conditions, particularly in the Β band (all frequencies, active vs. passive, p = 0.13; Β band, p = 0.51). Altogether, the encoding pattern across areas may suggest that while dlPFC is critically involved in estimating the relative distance between self and target, 7a may be preferentially involved in the process of path integration, while somewhat unexpectedly, MSTd may play an important role in keeping track of the latent spatial goal.”</p><p>And:</p><p>“Finer grain unit-to-unit coupling was sparse, and within each area the probability of two units being functionally connected decreased as a function of the distance between neurons (Figure 4 —figure supplement 3A). The overall likelihood of two units being coupled within a given area changed as a function of brain area and was modulated by task engagement (active vs. passive viewing in area 7a; Figure 2 —figure supplement 11C), but not as a function of probe type used (Utah array or linear probe, see Figure 4 —figure supplement 4).”</p><p>Figure caption:</p><p>“Figure 2 – supplement figure 11. Task engagement drives neural tuning. A. Example trials. To demonstrate that the fraction of neurons tuned to different task variables reported in Figure 2F are driven by task engagement and not purely low-level visual input, in a control experiment (2 sessions) we recorded from area 7a (117 neurons) as a monkey first actively engaged in the task (top), and then passively viewed replayed the exact visual input (bottom). We show 7 example trials, demonstrating that the linear velocity during active and passive trials matched (same for other task variables, not shown). Instead, the population evoked responses (1 trial, average across the entire population of simultaneously recorded neurons) was evident during active but not passive trials. B. Average firing rate. Firing rate (averaged over the entire recording) did not differ between active (x-axis) and passive (y-axis) viewing (blue dots are single cells in 7a, black dot is the mean, dashed black line is identity). C. Fraction of neurons tuned and coupled. The fraction of neurons tuned to different taskvariables, and the fraction of neurons coupled to each other in area 7a, were blunted (but not entirely absent) during passive viewing. The exceptions were variables related to internal neural dynamics, notably the phase locking of spiking activity to LFP phase in Theta, Α, and Β band.”</p><p>Further, in the discussion we had previously stated:</p><p>“To the best of our knowledge, the striking difference between the posterior parietal node and other areas (here MSTd and dlPFC) vis-à-vis their dependency on the LFP phase has not been previously reported and may have been acutely evident here given the natural timing between sensory inputs, motor outputs, and ongoing neural dynamics that exists within this closed-loop setting”.</p><p>This sentence has been removed given the new analysis here demonstrating that even during passive viewing, there was a stronger spike-LFP phase locking in 7a (particularly in theta and α bands) than dlPFC.</p><disp-quote content-type="editor-comment"><p>Could the authors discuss in the paper how the visual input is (or not) included in the model?</p></disp-quote><p>We thank the reviewers for this question. The visual input (in particular the task-relevant feature, being optic flow) was determined by (1) the linear and angular velocity of the animals, and (2) their eye movements. Both of these were included in the P-GAM. However, it is true that the characteristics of each pixel, and how these pixels varied over time, was not included in the model. We consider both of these approaches (ours: in a sense assuming an abstraction of self-velocity; vs. fitting the visual input itself: more a question of representation) to be interesting, yet different. Even complementary. We have modified the discussion in the following manner:</p><p>“Performing a “firefly task” in a real environment would also suppose a more complex set of visual inputs (e.g., corners, textures, shadows) that could be leveraged in an expanded P-GAM taking visual features as input (see Parker et al., 2022, for recent work taking this approach).”</p><disp-quote content-type="editor-comment"><p>6) MSTd and dlPFC coupling.</p><p>a) As the animals were head-fixed, eye position would compensate in some cases the animals might have moved its head position (for instance to keep track of the target). Both, MSTd and dlPFC encoded eye position. Could the close coupling of MSTd and dlPFC be linked to this element of the task?</p></disp-quote><p>We believe that the functional subnetwork established between MSTd and dlPFC reflects the fact that the animals are moving their eyes to keep track of the hidden latent variable. This is shown in Figure 5, demonstrating that the more MSTd and dlPFC were coupled, the more the eyes moved as to keep track of the firefly. In turn, it may be (though this is an empirical question) that if the animals employed another strategy (e.g., moving their heads instead, if freely moving), there would be no (or a reduced) coupling between MSTd and dlPFC. We have amended the discussion in the following manner:</p><p>“For instance, virtual and real-world navigation may rely on partially distinct neural codes (Aghajan et al., 2015). Thus, it will be interesting to replicate the current experiment while macaques move freely in a 3D environment (e.g., Mao et al., 2021). This would also allow for independent eye- and head-movements (head was restrained here) and thus we could estimate whether eye movements in the current experiment partially reflected intended head movements (as they seemingly do in rodents; Michaiel et al., 2020, Meyer et al., 2020).”</p><disp-quote content-type="editor-comment"><p>b) The authors claim that areas MSTd and dlPFC form a functional sub-network together, on the basis of similarity in the fractions of neurons tuned to certain variables, and the distribution of the preferred value of some of these variables. However, the fractions of neurons tuned to the latent variables in MSTd and dlPFC (see Figure 2F) are actually quite different. Perhaps the authors could comment on this.</p></disp-quote><p>The reviewer is correct that the fraction of neurons tuned to different variables is not identical in MSTd and dlPFC. However, these areas were in general less tuned to velocity, acceleration, and the timing of different sensorimotor variables than 7a was. MSTd and dlPFC were often tuned to eye position, and these areas were more tuned to the distance to target than 7a. Thus, overall, there were strong similarities between MSTd and dlPFC that did not exist with 7a. Most importantly, we do not make the claim that these areas form a functional subnetwork only because their tuning properties are similar, but because there was a greater likelihood of seeing unit-to-unit coupling between these areas, than between these areas and 7a. We are confident in these results, and in fact initial results regarding population dynamics also supports this claim. Namely, we have seen that the communication subspace (defined as in Semedo et al., 2019, Neuron) is largest between MSTd and dlPFC than between these areas and 7a. And the variable that is most readily decodable from the communication manifold between MSTd and dlPFC is related to eye movements (c.f. Balzani et al., 2022, Arxiv), as would be suggested by the correlation between coupling likelihood and the monkey’s eyes tracking the hidden target.</p><p>We consider that it would not be appropriate to include these population-level findings to the current manuscript, as they rely on sophisticated methodology and address a slightly different question: single units vs. population dynamics. Nevertheless, we amend the discussion to explicitly acknowledge that the fraction of units tuned to different variables is not identical in MSTd and dlPFC. The text has been modified in the following manner:</p><p>“Similarly, to further corroborate the functional subnetwork between MST and dlPFC it will be interesting to examine population dynamics and the possibility that these areas form a functional “communication subspace” (Semedo et al., 2019), adapted to the naturalistic setting of this task (see Balzani et al., 2022). “</p><disp-quote content-type="editor-comment"><p>c) When there was stronger MSTd-to-dlPFC coupling and better tracking of the hidden firefly with the eyes (Figure 5B), was the performance of the monkey also better (i.e. more hits)?</p></disp-quote><p>We thank the reviewers for this question. Indeed, the better the tracking of the hidden firefly, the better is performance. This is true both when quantified as bias (as we had reported in the original paper and as is reported in Lakshminarasimhan et al., 2020), and when quantified as hit rate (added to the manuscript following this question). Coupling probability (MSTs-to-dlPFC, dlPFC-to-MSTd, and dlPFC-to-dlPFC) does correlate with better gazing toward the firefly. However, this coupling is not predictive of bias (as originally reported in Figure 5 —figure supplement 1), nor of hit rate (as reported now following this question). Together, it appears that (1) coupling within dlPFC or across MSTd and dlPFC correlates with (2) better gazing toward the invisible firefly, and this latter one correlates with (3) better task-performance, but the association between (1) and (3) is not (directly, without (2)) true.</p><p>The text has been modified to include the correlations (or lack thereof) with hit rate:</p><p>“Further, this relationship also held across sessions, with better target tracking correlating with less bias (slopes closer to 1, see Figure 1C), particularly in the angular domain (r<sup>2</sup> = 0.43, p = 0.004; radial: r<sup>2</sup> = 0.26, p = 0.04; Figure 5A), and with an increasing proportion of rewarded trials (r<sup>2</sup> = 0.24, p = 0.042).”</p><p>And:</p><p>“As shown in Figure 5 —figure supplement 1, there was no correlation between the unit-to-unit couplings within a session, and either radial or angular biases (7a-7a coupling v. angular bias p = 0.06; all other p &gt; 0.12). There was similarly no correlation between the proportion of rewarded trials in a session and unit-to-unit coupling probability (all p &gt; 0.11, Bonferroni corrected). Overall, therefore, the functional subnetwork between MSTd and dlPFC (Figure 4) seemingly reflects the animals’ strategy in keeping track of the hidden target with their eyes. In turn, the eye movements (but not MSTd-dlPFC coupling directly) aid in successfully navigating to the location of the hidden target.”</p><disp-quote content-type="editor-comment"><p>7) Sampling of areas.</p><p>a) Area 7a was exclusively sampled with chronic rather than moveable probes. It has also the largest number of &quot;single units&quot;.</p><p>To what extent are these single units independent?</p><p>Could a sampling bias in these probes (part of 7a; layers) affect the results, especially when it comes to coupling. Please include in the discussion.</p></disp-quote><p>This is an excellent question. We address it by examining the existent recording from Monkey M (dlPFC recordings with linear probes), and by incorporating in the supplementary materials three new recordings (from a new monkey, Monkey B) in 7a, all with linear probes. We fit these sessions to the P-GAM and compute the fraction of neurons coupled within area. Importantly, we only perform this analysis for neurons that were exactly 400 micrometers apart, the minimum distance in Utah probe recordings. Then, we take the array recordings, and compute a new distribution. Namely, we perform 500 iterations in which we first subsample neurons from a randomly selected session to match the number of simultaneously recorded neurons with the linear probes. Then, given that these neurons were at the distance of 400 micrometers, we compute the fraction of neurons coupled. That is, we match the distance (400 micrometers) and number of neurons between array and linear probe recordings. Figure 4 —figure supplement 4 shows, most importantly, that there was no significant effect of probe: the mean of the probe data (dashed black line) fell within the distribution of the array data. Interestingly, it also corroborates Figure 4E, showing that coupling among 7a was more common than among dlPFC.</p><p>Figure caption:</p><p>“Figure 4 – supplement figure 4. Fraction of neurons coupled in 7a and dlPFC as a function of probe (Utah array or linear probe). We questioned whether the type of probed utilized during recording had an impact on the fraction of units the P-GAM estimated as coupled. To address this question, we examined a new set of recordings in 7a (5 sessions, 32 neurons in monkey B, not reported in the main text), as well as the recordings in dlPFC in monkey M (55 neurons), both of which were conducted with linear probes. For each area, we computed the fraction of neurons coupled during linear probe recordings, given that the distance between these neurons was 400 micrometers, the minimal distance between electrodes in the Utah array recordings. Then, we performed 500 iterations where we randomly subsampled from simultaneously recorded neurons in Utah arrays to match the number of simultaneously recorded neurons with the linear probe. We computed the fraction of neurons coupled within this subsample, again only for neurons at a distance of 400 micrometers. We plot the iterations as a histogram (blue for 7a and red for dlPFC), demonstrating that the fraction of neurons tuned did not significantly depend on type of probe used (7a, mean of linear probe = 0.17, 95%CI of array = [0.10 0.49]; dlPFC, mean of linear probe = 0.10, 95%CI of array = [0.03 0.22]).“</p><p>Results:</p><p>“Finer grain unit-to-unit coupling was sparse, and within each area the probability of two units being functionally connected decreased as a function of the distance between neurons (Figure 4 —figure supplement 3A). The overall likelihood of two units being coupled within a given area changed as a function of brain area and was modulated by task engagement (active vs. passive viewing in area 7a; Figure 2 —figure supplement 11C), but not as a function of probe type used (Utah array or linear probe, see Figure 4 —figure supplement 4).”</p><p>Lastly, we also amend the discussion to make reference to the issues of sampling from a single layer in 7a and the potential of non-independence of single-units (across sessions) during Utah array recordings. The text has been modified as follows:</p><p>“The second limitation relates to the (necessarily limited) sampling of neural areas, and the focus on single units as opposed to population dynamics. We report a functional subnetwork between MSTd and dlPFC based on the similarity of their encoding profiles (though they are of course not identical) and the likelihood of encountering unit-to-unit couplings across these areas. But this functional connection must be subserved by structure (e.g., perhaps a third area we did not record from fluctuating with both MSTd and dlPFC). Thus, in ongoing experiments we have trained rodents to perform the “firefly task”. This will allow recording from a wider array of neural areas and cortical layers (most of the recordings reported here being from Utah arrays and hence likely from a single layer and of limited independence).”</p><disp-quote content-type="editor-comment"><p>b) The number of recorded neurons in the three areas differs greatly: 231 units in MSTd, 823 units in dlPFC, and 3200 units in area 7a. Yet many conclusions in the paper rely on neuronal numbers: the fractions of neurons tuned to certain sensorimotor and latent variables differ between the areas, the variables explaining the firing rates cluster differently in the neurons of the three areas, and both the coarse LFP connectivity and the fine unit-to-unit coupling within areas differ. Especially the clustering results might depend on the number of recorded neurons: the fact that almost all MSTd and dlPFC neurons are categorized as belonging to the same cluster, whereas the area 7a neurons appear in three distinct clusters, could be caused by the much larger number of recorded neurons in area 7a. Also unit-to-unit coupling is more likely to show up in the data with a much larger number of recorded neurons. The data could be corrected for these differences in number of recorded neurons.</p></disp-quote><p>We thank the reviewers for this question. Respectfully, we disagree that the fraction of neurons tuned to a particular variable, or the fraction of neurons coupled with others, depends on the number of neurons (these are proportions and thus corrected for the number of neurons). On the hand, we do agree that clustering analyses could be impacted by the relative number of units per area included in the analysis. Thus, we have performed these analyses again, while subsampling from 7a and dlPFC to match the number of neurons present in MSTd.</p><p>For the clustering based on whether or not a neuron was tuned to a particular variable (Figure 4A and B), we performed 10k iterations. As highlighted by the reviewers, the striking finding originally reported was that most neurons in MSTd and dlPFC were mixed selective and belonged to a single cluster, while neurons from 7a belonged to a wider variety of clusters. Thus, the metric we compute here is the size of the largest cluster relative to the total number of units. As shown in Figure 4 —figure supplement 1 (panel A, means and 95%CI shown) even when matching the number of units across areas, ~70% of units in MSTd and dlPFC come from the most populous cluster, while only ~35% do so in 7a. This results thus confirm the original finding.</p><p>Regarding the clustering based on tuning function shapes and UMAP projections (Figure 4C), the key metric originally reported was that the centroid of the MSTd UMAP projection was 6.49 times closer to the centroid of dlPFC than to the centroid of 7a. We computed this metric again, while performing 100 iterations. Figure 4 —figure supplement 1 (Panel B) we depict the ratio of MSTd-to-dlPFC UMAP distance to MSTd-to-7a distance, for all iterations. As it can be observed, the finding holds, with the boundaries of the 95%CI being 4.07 and 7.33.</p><p>The results and figure caption have been amended:</p><p>Results:</p><p>“Other cluster types existed, for instance composed of neurons selectively tuned to the ongoing phase in LFP bands but no other task variable (Figure 4A and B, Cluster 4), or driven also by motor onset and offset (Figure 4A and B, Cluster 5). These remaining clusters were, however, less common (~1-5%). This analysis was conducted with the full dataset (4254 neurons in total), yet in the supplement (Figure 4 —figure supplement 1A) we confirm that the results are unchanged when subsampling from areas with more neurons (7a and dlPFC) to match the number present in MSTd (231 neurons). Together, this pattern of clustering results based on whether neurons were tuned to different task variables demonstrated a surprising commonality between MSTd and dlPFC, which are in turn different from area 7a.”</p><p>And:</p><p>“Notably, however, the centroid of MSTd was 6.49 times closer to the centroid of dlPFC than area 7a (Figure 4C, top row. Note that Becht et al., 2018, have shown UMAP to conserve global structure and thus allows for a meaningful interpretation of distances). This finding also holds when subsampling from 7a and dlPFC to match the number of units present in MSTd (100 iterations, MSTd-dlPFC distance was 5.56 times closer than MSTd7a, 95%CI = [4.07, 7.33]; Figure 4 —figure supplement 1B).”</p><p>Figure caption:</p><p>“Figure 4 – supplement figure 1. Clustering results, subsampling from neurons in dlPFC and 7a to match the number of units recorded from in MSTd. A. We performed spectral (Jaccard) clustering of neurons based on their 1 x 17 vector of Booleans, indicating whether they were tuned or not to particular task-variables (Figure 4A). Here, we perform this operation 10000 times, while randomly selecting (without replacement) 231 neurons from 7a and dlPFC. Thus, the full matrix clustered was 693 (231 x 3 brain areas) x 17 (task variables). Given that on each run clusters are assigned an arbitrary cluster number, for each run we compute the ratio of the largest cluster size to the total number of units per area (231). Namely, in the main text we report that MSTd and dlPFC are predominantly represented by 1 mixed-selective cluster, while 7a is represented in 3 approximately equal sized clusters. The results here concord with those in the main text, demonstrating that approximately 70% of neurons in MSTd and dlPFC belong to a single cluster, while approximately 35% of neurons belong to the largest cluster in area 7a. Circles are the mean across 10000 iteration, error bars are 95%CI. B. 100 iterations of UMAP while randomly subsampling from 7a and dlPFC to match the number of units in MSTd. On each run, we compute the distance in UMAP space between MSTd and dlPFC, and between MSTd and 7a. Then we compute their ratio (MSTd-to-dlPFC/MSTd-to-7a, thus &gt;1 indicating closer MSTd-to-dlPFC distances). The figure shows the full distribution of ratios, concurring with the main text that MSTd and dlPFC are approximately 6 times closer in UMAP space, than MSTd and 7a are. “</p><disp-quote content-type="editor-comment"><p>8) Lateralisation.</p><p>To what extent played the lateralization of the recording and task a role for neuronal response? This applies relative to brain hemisphere, body and eye position? Where in each monkey did the recordings take place? Which hand(s) did each monkeys use for the choice stick?</p><p>How was the lateralisation included in the model?</p><p>Please comment with regards to responses in MST, 7A, and dPFC and add information to the manuscript.</p><p>Specifically, it is unclear from Suppl Figure 1 whether within a particular monkey, some recording sites were interhemispheric, or whether within one monkey, all recordings were done in the same hemisphere. This of course has significant consequences for the effects of ongoing LFP and unit-to-unit coupling.</p></disp-quote><p>We thank the reviewers for highlighting that Figure 1 —figure supplement 1 did not provide enough information as to lateralization. We have significantly expanded this figure caption to add the information requested by the reviewers. The figure caption reads:</p><p>“Location of acute recordings are indicated by spheres, color coded per animal (Monkey S, orange; Monkey Q, purple; Monkey M in green). Location of Utah arrays are indicated by squares. The pictures of the Utah arrays are framed in the color corresponding to the monkey. In turn, Monkey S had recordings performed from Utah arrays in dlPFC and area 7a on the left hemisphere, and from a linear probe in MSTd on the right hemisphere. Monkey Q had recordings performed from a Utah array in 7a on the left hemisphere and from a linear probe in MSTd on the right hemisphere. Monkey M had recordings performed from a linear probe in dlPFC on the right hemisphere. In total, therefore, each area was sampled twice. All 7a recordings were on the left hemisphere and all MSTd recordings were on the right hemisphere (note, given that MSTd is directly ventral to 7a, see Figure 2A, these cannot be recorded from the same hemisphere if the former area is implanted with an array). From the 823 neurons recorded in dlPFC, 55 were recorded on the right hemisphere in Monkey M. All inter-area coupling analysis (which requires simultaneous recordings) were based on left-hemisphere recordings in 7a and dlPFC, and from right-hemisphere recordings in MSTd. All monkeys were right-handed and used this hand to manipulate the joystick. AS, arcuate sulcus; IPS, intraparietal sulcus; PS, principal sulcus; STS, superior temporal sulcus; LF, lateral fissure.”</p><p>In short, all animals were right-handed. All recordings in area 7a were on the left-hemisphere, while all recordings in MSTd were on the right hemisphere. It is impossible to record from these two areas simultaneously in the same hemisphere, if recording high-density with a Utah array in 7a (the area on the surface). The vast majority of neural recordings in dlPFC (93%) were conducted via a Utah array on the left hemisphere. All data analysis requiring simultaneous recordings (i.e., inter-area coupling, inter-area LFP phase coupling) were conducted with MSTd data on the right hemisphere, and 7a and dlPFC on the left hemisphere. Thus, 7a and dlPFC were on equal footing with regard MSTd, in that they were both inter-hemispheric.</p><disp-quote content-type="editor-comment"><p>9) Data fed into the P-GAM model.</p><p>a) The P-GAM model is a great analysis tool for these kinds of data. However, the variables that the authors put into it are conceptually very different from each other. There are purely external task variables such as target onset and offset, latent variables such as distance to target that require knowledge of one's own position in space, and purely internal brain dynamics variables such as coupling to the LFP in another area. In that light, the finding of 'many variables contributing to the responses' is not surprising; all neurons in the brain are probably influenced both by external variables and internal brain dynamics. Maybe the authors could comment on the different nature of their variables and how that impacts their results.</p></disp-quote><p>We agree with the reviewer that neurons are very likely influenced by both external task variables and internal brain dynamics. In fact, we consider that the inclusion of both these types of variables to be a strong asset of the current manuscript. We have amended the text to more explicitly mention the difference between these types of variables. The text reads:</p><p>“In addition to continuous sensorimotor (e.g., linear and angular velocity and acceleration) and latent variables (e.g., distance from origin and to target, Figure 1A), as well as discrete task events (e.g., time of target onset, as well as movement onset and offset), we included elements of brain dynamics in the encoding model. These internal dynamics are most often not considered in accounting for task-relevant neural responses, yet they fundamentally shape spiking activity. These latter variables included the phase of LFP in different frequency bands (theta: 4-8 Hz; α: 8-12 Hz; β: 12-30 Hz), and causal unit-to-unit coupling filters within (i.e., spikehistory, 36 ms wide temporal filter) and between units, both within (36 ms wide temporal filter) and across cortical areas (600 ms wide temporal filters, Figure 2C, see <italic>Methods</italic>).”</p><disp-quote content-type="editor-comment"><p>b) Given that the sensorimotor and latent variables going into the G-PAM model are so crucial for the story, could you make a figure where you visualize them? This could maybe be added to Figure 1A. Also, 'radial bias' and 'angular bias' (in Fig1D) could be visualized here.</p></disp-quote><p>We thank the reviewers for the suggestion and agree that depicting these time-courses could be helpful to readers. We have modified the figure.</p><disp-quote content-type="editor-comment"><p>c) Quantification of electrophysiological activity processing that is fed into the P-GAM model is not entirely clear.</p><p>More details about the preprocessing of these data are required, for example, are the SUA baselined using pre-stimulus presentation activity? Are the LFP baselined as well? And how similar are the pooled responses within each area and across? This would allow the reader to spot possible problems when computing further neuronal properties, that could bias the main paper result:</p><p>An example is the tuning of the neurons to the phase of ongoing oscillation (Β, Α, Theta). There are a number of papers attempting to optimize methods to measure spike field coherency, e.g. the PPC pairwise phase consistency (Vinck et al., 2010). This method gives an estimation independent of spike count and LFP amplitudes (both parameter vary of course widely across time, tasks, subjects, areas…).</p><p>Here, it seems these two parameters are not considered and could lead to artefacts in the coupling results presented. The authors use temporal correlations to approximate coupling between spike/spike, and spike/LFP-phase. Correlation methods can potentially lead to artefacts and overestimations of coupling strength.</p><p>In their methods, the author state to 'bin spiking activity across 8ms window' prior to feeding this activity to the P-GAM. It means that 1 spike corresponds to an averaged 8ms time window. If you now try to calculate the dependency of this single spike to a specific phase of a β (30Hz) , the α (12Hz) and theta (4Hz) oscillation, it means that the chance level of assigning the binned spike to a particular phase differs considerably. Therefore, the statistical power of this analysis would decrease for higher frequency. It seems that the authors do not apply any correction.</p></disp-quote><p>We thank the reviewers for this question. We have added information regarding the pre-processing of neural data in the following manner:</p><p>“As such, inputs to the P-GAM were of three types. First, the spike counts of the unit to be modelled, at a 6ms resolution (i.e., the number of spikes within 6ms windows, no baseline correction). Second, the continuous, discrete, and neural co-variates, which were also sampled at a 6ms resolution. The last input type were a set of 15 “knots” per co-variate, defining the nodes of eventual tuning functions. The location of knots were defined as to (i) cover the range of a given input variable from the second to the 98<sup>th</sup> percentile with (ii) equi-probable knots (each knot covering the same probability mass). See</p><p>https://github.com/BalzaniEdoardo/PGAM/blob/master/PGAM%20Tutorial.ipynb for a comprehensive tutorial.“</p><p>Regarding the spike-LFP phase coupling, we thank the reviewers for highlighting the pairwise phase consistency (PPC0) measure. We have applied this measure and report the findings in Figure 2 —figure supplement 9.</p><p>As it can be observed, the main finding that neurons in area 7a are considerably more phase locked to LFPs (particularly in the Β range, but also in Α) holds. We have added Figure 2 —figure supplement 9 and the following text:</p><p>Results:</p><p>“Neurons in area 7a showed a strong dependency to the phase of ongoing LFP fluctuations (see Figure 2 —figure supplement 8 for further illustrations of this effect as quantified by the P-GAM, and Figure 2 —figure supplement 9 for corroborative evidence by pairwise phase consistency, Vinck et al., 2010), a fact that was less observed in dlPFC or MSTd (all p &lt; 7.1 x 10<sup>-8</sup>, all d &gt; 1.02) “</p><p>Figure caption:</p><p>“Figure 2 – supplement figure 9. Pairwise phase consistency. The findings related to spike-LFP phase coupling reported in the main text are based on the P-GAM, as are the rest tuning properties reported. However, detecting a correlation between when spikes occur and the phase of LFPs may be biased by a number of factors, for example the firing rate of neurons or the amplitude of LFP oscillations. In turn, we corroborated the spikeLFP phase coupling results by computing the pairwise phase consistency (PPC0), a bias-free estimator (see Vinck et al., 2010 for detail). This analysis confirmed the findings from the main text in demonstrating greater spike-LFP phase coupling in area 7a, particularly within the Β and Α ranges. Mean PPC across all neurons and sessions are reported separately by brain region and frequency range. Error bars are S.E.M.”</p></body></sub-article></article>