<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53045</article-id><article-id pub-id-type="doi">10.7554/eLife.53045</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Robust and distributed neural representation of action values</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-163886"><name><surname>Shin</surname><given-names>Eun Ju</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3901-498X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-163887"><name><surname>Jang</surname><given-names>Yunsil</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5736-214X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163888"><name><surname>Kim</surname><given-names>Soyoun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1348-6401</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163889"><name><surname>Kim</surname><given-names>Hoseok</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163890"><name><surname>Cai</surname><given-names>Xinying</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163891"><name><surname>Lee</surname><given-names>Hyunjung</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163892"><name><surname>Sul</surname><given-names>Jung Hoon</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-163893"><name><surname>Lee</surname><given-names>Sung-Hyun</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-189328"><name><surname>Chung</surname><given-names>Yeonseung</given-names></name><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-10628"><name><surname>Lee</surname><given-names>Daeyeol</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3474-019X</contrib-id><email>daeyeol@jhu.edu</email><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-17321"><name><surname>Jung</surname><given-names>Min Whan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4145-600X</contrib-id><email>mwjung@kaist.ac.kr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Center for Synaptic Brain Dysfunctions, Institute for Basic Science</institution><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff2"><label>2</label><institution>Department of Biological Sciences, Korea Advanced Institute of Science and Technology</institution><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff3"><label>3</label><institution>Center for Neuroscience Imaging Research, Institute for Basic Science</institution><addr-line><named-content content-type="city">Suwon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff4"><label>4</label><institution>Department of Neuroscience, Biomedicum, Karolinska Institutet</institution><addr-line><named-content content-type="city">Stockholm</named-content></addr-line><country>Sweden</country></aff><aff id="aff5"><label>5</label><institution>New York University Shanghai, NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai, and Shanghai Key Laboratory of Brain Functional Genomics (Ministry of Education), School of Psychology and Cognitive Science, East China Normal University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution>Department of Anatomy, Kyungpook National University School of Medicine</institution><addr-line><named-content content-type="city">Daegu</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff7"><label>7</label><institution>Neuroscience Graduate Program, Ajou University School of Medicine</institution><addr-line><named-content content-type="city">Suwon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff8"><label>8</label><institution>Department of Mathematical Sciences, Korea Advanced Institute of Science and Technology</institution><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff9"><label>9</label><institution>The Zanvyl Krieger Mind/Brain Institute, Kavli Neuroscience Discovery Institute, Department of Neuroscience, and Department of Psychological and Brain Sciences, Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>04</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e53045</elocation-id><history><date date-type="received" iso-8601-date="2019-10-28"><day>28</day><month>10</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2021-04-19"><day>19</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Shin et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Shin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53045-v2.pdf"/><abstract><p>Studies in rats, monkeys, and humans have found action-value signals in multiple regions of the brain. These findings suggest that action-value signals encoded in these brain structures bias choices toward higher expected rewards. However, previous estimates of action-value signals might have been inflated by serial correlations in neural activity and also by activity related to other decision variables. Here, we applied several statistical tests based on permutation and surrogate data to analyze neural activity recorded from the striatum, frontal cortex, and hippocampus. The results show that previously identified action-value signals in these brain areas cannot be entirely accounted for by concurrent serial correlations in neural activity and action value. We also found that neural activity related to action value is intermixed with signals related to other decision variables. Our findings provide strong evidence for broadly distributed neural signals related to action value throughout the brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>action value</kwd><kwd>chosen value</kwd><kwd>reinforcement learning</kwd><kwd>striatum</kwd><kwd>rat</kwd><kwd>monkey</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010446</institution-id><institution>Institute for Basic Science</institution></institution-wrap></funding-source><award-id>IBS-R002-A1</award-id><principal-award-recipient><name><surname>Jung</surname><given-names>Min Whan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>DA 029330</award-id><principal-award-recipient><name><surname>Lee</surname><given-names>Daeyeol</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Action-value signals previously found in many brain areas can be accounted for neither by concurrent serial correlations in neural activity and action value nor by signals for other decision variables.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The reinforcement learning theory provides a general theoretical framework for understanding the neural basis of value-based decision making (<xref ref-type="bibr" rid="bib4">Corrado and Doya, 2007</xref>; <xref ref-type="bibr" rid="bib5">Dayan and Niv, 2008</xref>; <xref ref-type="bibr" rid="bib8">Glimcher, 2011</xref>; <xref ref-type="bibr" rid="bib19">Lee et al., 2012a</xref>; <xref ref-type="bibr" rid="bib22">Mars et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">O'Doherty et al., 2007</xref>). In algorithms based on this theory, an agent selects an action based on a set of action values (i.e., values associated with potential actions) in a given state (<xref ref-type="bibr" rid="bib30">Sutton and Barto, 1998</xref>). Human and animal choice behaviors are parsimoniously accounted for by such algorithms. Furthermore, a large body of studies in rats, monkeys, and humans have found neural or hemodynamic signals correlated with action value in multiple regions of the brain, especially in the frontal cortex-basal ganglia loop (<xref ref-type="bibr" rid="bib3">Chase et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Ito and Doya, 2011</xref>; <xref ref-type="bibr" rid="bib18">Lee, 2006</xref>; <xref ref-type="bibr" rid="bib19">Lee et al., 2012a</xref>; <xref ref-type="bibr" rid="bib27">Rushworth et al., 2009</xref>). These findings led to the view that multiple brain structures contribute to biasing choices toward relatively valuable ones during decision making by representing a set of action values.</p><p>It is often difficult to rigorously demonstrate that neural activity is genuinely correlated with action value, because both neural activity and action value tend to fluctuate slowly over time and thus are serially correlated. Recently, for example, <xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref> made two lines of argument to suggest that action-value neurons had not been clearly demonstrated in the striatum. First, with a permutation test in which behavioral data from different sessions are used to determine significance of action-value-related neural activity, they found that the number of neurons encoding action value in the ventral striatum (VS) and ventral pallidum (VP; <xref ref-type="bibr" rid="bib12">Ito and Doya, 2009</xref>) was reduced to a chance level. A more recent simulation study also has shown that naïve applications of the conventional F-test for multiple linear regression can suffer from a false positive and hence a ‘nonsense correlation’ between a behavioral variable and autocorrelated neural activity (<xref ref-type="bibr" rid="bib9">Harris, 2020</xref>). Second, <xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref> argued that neural activity related to action value may reflect other decision variables correlated with action value, such as a choice probability or policy. Even though <xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref> focused on striatal action-value signals, these problems might be also relevant to action-value signals reported in other brain areas.</p><p>Given the significance of these statistical issues concerning value-related signals throughout the brain, we decided to reanalyze the data we have collected in our previous studies using the methods designed to strictly account for temporal correlations in the data. In addition to the permutation test used in <xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref>, we also used surrogate behavioral and neural data to determine the statistical significance of value signals. We also tested whether action-value neurons identified in our previous studies merely encode policy or state value rather than action value. Overall, the results from these analyses demonstrate that neural activity in many areas of the brain, including the striatum, robustly encode action values.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Neuronal and behavioral database</title><p>We analyzed neural activity related to action value as well as chosen value (value of the chosen action in a given trial). Included in this analysis are the neural data recorded from the dorsomedial striatum (DMS, 466 neurons), dorsolateral striatum (DLS, 206 neurons), VS (165 neurons), lateral orbitofrontal cortex (OFC, 1148 neurons), anterior cingulate cortex (ACC, 673 neurons), medial prefrontal cortex (mPFC, 854 neurons), secondary motor cortex (M2, 411 neurons), and dorsal CA1 region (508 neurons) in rats (n = 27; 383 sessions) performing a dynamic foraging task in a modified T-maze (<xref ref-type="fig" rid="fig1">Figure 1</xref>, see Materials and methods; <xref ref-type="bibr" rid="bib15">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Lee et al., 2012b</xref>; <xref ref-type="bibr" rid="bib21">Lee et al., 2017</xref>). We also analyzed neural data recorded from the dorsolateral prefrontal cortex (DLPFC, 164 neurons), caudate nucleus (CD, 93 neurons), and VS (90 neurons) in three monkeys performing an intertemporal choice task (see Materials and methods; <xref ref-type="bibr" rid="bib14">Kim et al., 2008</xref>; <xref ref-type="bibr" rid="bib2">Cai et al., 2011</xref>). In these monkey experiments, temporally discounted values (DVs) of alternative choices were randomized across trials, so that all decision variables were devoid of temporal correlation. We included in the analysis only those neurons with mean firing rates ≥1 Hz during a given analysis time window. To assess action-value-related neural activity in rats, we analyzed neural spike data during the last 2 s of the delay period, immediately before the central bridge is lowered so that the animal is allowed to run forward and head toward the left or right goal location (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; 196 DMS, 123 DLS, 68 VS, 782 OFC, 405 ACC, 431 mPFC, 301 M2, and 307 CA1 neurons). To assess action-value-related neural activity in monkeys, we analyzed neural spike data during the 1 s time window before the onset of sensory cues signaling two choice options (75 CD, 66 VS, and 105 DLPFC neurons). To assess chosen-value-related neural activity in rats, we analyzed neural spike data during the 2 s time period centered around the outcome onset (±1 s since the choice outcome was revealed; 241 DMS, 139 DLS, 80 VS, 808 OFC, 401 ACC, 446 mPFC, 334 M2, and 326 CA1 neurons). In the following, we first describe the results from simulations to test false positive rates of several different statistical tests used in the present study in identifying action-value and chosen-value neurons. We then show the results of these tests applied to the actual neural data collected from rats performing the block-designed dynamic foraging task. We then address the issue of potentially misidentifying other decision-variable signals as action-value signals using the data from both rats and monkeys.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Dynamic foraging task.</title><p>(<bold>A</bold>) Modified T-maze. Rats chose freely between two targets (orange circles) to obtain water reward. Rats navigated from the central stem to either target and returned to the central stem via the lateral alley to start a new trial. A delay (2–3 s) was imposed at the beginning of a new trial by raising the central bridge. Green arrows, photobeam sensors. Scale bar, 10 cm. (<bold>B</bold>) Behavioral data from a sample session (<xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>). The black curve shows the probability to choose the left target (<italic>P<sub>L</sub></italic>) in moving average of 10 trials. The gray curve denotes the probability to choose the left target predicted by the Q-learning model. Tick marks denote trial-by-trial choices of the rat (upper, left choice; lower, right choice; long, rewarded trial; short, unrewarded trial). Vertical gray lines denote block transitions and numbers above indicate reward probabilities of the left and right targets in each block. (<bold>C</bold>) Trial-by-trial action values of the sample session computed with the Q-learning model. Blue, left-choice action value (<italic>Q<sub>L</sub></italic>); Red, right-choice action value (<italic>Q<sub>R</sub></italic>). (<bold>D</bold>) An example DMS unit showing activity correlated with left-choice action value. Trials were grouped into quartiles of left-choice action value. Delay onset is when the rat broke the photobeam sensor on the central stem.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53045-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Validation of permutation and surrogate data-based tests</title><p>We first assessed false positive rates of different analysis methods using actual behavioral data and simulated null neural data whose autocorrelation was chosen to match that of the actual neural data. The simulated neural data was generated as the following:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>x(t)</italic> is the firing rate of the simulated neuron at trial <italic>t</italic>, <italic>α</italic> is the autoregressive (AR) coefficient, and ε is a standard normal deviate. We then generated time series for spike counts assuming the simulated neuron is a Poisson process. We set <italic>α</italic> = 0.8 and 0.83 to test false positive rates of action-value and chosen-value signals, respectively, which were chosen to match the distributions of the first-order AR coefficient, AR(1), and mean firing rate to those of the actual neural data used to analyze action-value signals (neural activity during the last 2 s of the delay period; AR(1) = 0.19 ± 0.18 and mean firing rate = 6.14 ± 7.61 Hz, n = 2613 neurons) and chosen-value signals (neural activity during the 2 s time period centered around the outcome onset; AR(1) = 0.21±0.20 and mean firing rate = 5.90 ± 6.72 Hz, n = 2775 neurons; mean ± SD).</p><p>We used these simulated neural data to test false positive rates of different analysis methods. Throughout the study, we identified action-value neurons as those whose activity is significantly related to either of the left and right action values (p&lt;0.025 for <italic>Q<sub>L</sub></italic> or <italic>Q<sub>R</sub></italic>). A conventional <italic>t</italic>-test (linear regression analysis, model 1, <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) yielded &gt;10% action-value neurons, which is significantly greater than expected by chance (binomial test, p=3.7 × 10<sup>−22</sup>). Adding potentially confounding variables to the regression (choice and chosen value; model 2, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) reduced the number of action-value neurons, but it was still significantly greater than expected by chance (binomial test, p=2.6 × 10<sup>−9</sup>; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). We used two different methods to handle false positive identification of action-value neurons in our previous studies. One method (within-block permutation; see Materials and methods; <xref ref-type="bibr" rid="bib15">Kim et al., 2009</xref>) reduced the false positive rate further, but it was still significantly above the chance level (binomial test, p=5.9 × 10<sup>−5</sup>). The other method (adding AR terms to the regression; see Materials and methods; <xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Lee et al., 2012b</xref>; <xref ref-type="bibr" rid="bib21">Lee et al., 2017</xref>) reduced the number of action-value neurons to the chance level (binomial test, p=0.191; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). The use of the same tests was less problematic for the analysis of chosen-value signals. A conventional <italic>t</italic>-test (model 4, <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>) yielded ~9% chosen-value neurons and it was significantly greater than expected by chance (binomial test, p=3.7 × 10<sup>−8</sup>). However, the number of chosen-value neurons was reduced to the chance level by adding confounding variables to the regression (model 5, <xref ref-type="disp-formula" rid="equ10">Equation 9</xref>) and also by other methods used in our previous studies (applying within-block permutation or adding AR terms to model 5; <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Performances of different statistical tests for action-value and chosen-value signals.</title><p>(<bold>A</bold>) Left, cumulative density functions (CDFs) of p-values for the neural activity related to action value were determined with different analysis methods using null neural data to assess false positive rates. Results obtained with the methods used in previous studies (top), including the <italic>t</italic>-test in two different regression models (models 1 and 2), within-block permutation applied to model 2 (model 2 + WB), and model 2 with autoregressive terms (model 2 + AR), as well as those obtained with the methods based on surrogate data (bottom) are shown. Right, fractions of neurons significantly responsive to either action value (p&lt;0.025 for <italic>Q<sub>L</sub></italic> or <italic>Q<sub>R</sub></italic>). Horizontal dotted lines denote 5%. Significant fractions (binomial test) are indicated by black filled circles. (<bold>B</bold>) Left, CDFs of p-values for the neural activity related to chosen value. Right, fractions of neurons significantly responsive to chosen value (p&lt;0.05 for <italic>Q<sub>c</sub></italic>). The same format as in <bold>B</bold>, but models 4 and 5 were used instead of models 1 and 2, respectively. (<bold>C</bold>) Correlation between action values (left) or chosen values (right) calculated from the original and resampled behavioral data either from other sessions (session permutation, n = 382) or simulated behavioral sessions (pseudosession, n = 500). Filled bars indicate significant (<italic>t</italic>-test, p&lt;0.05) correlations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53045-fig2-v2.tif"/></fig><p>We then tested four different methods based on surrogate data to determine statistical significance of value terms in multiple regression models (models 2 and 5; <xref ref-type="disp-formula" rid="equ6 equ10">Equation 6 and 9)</xref>. The first two methods used surrogate behavioral data. Specifically, we tested session permutation (<xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref>) and pseudosession (<xref ref-type="bibr" rid="bib9">Harris, 2020</xref>) methods. In the session permutation test, surrogate behavioral data was drawn from other behavioral sessions. In the pseudosession test, surrogate behavioral data for a given session was generated based on a reinforcement learning model using the model parameters estimated for the same animal (see Materials and methods). The other two methods used surrogate neural data generated with Fourier phase randomization (FPR). For this, we tested the conventional FPR method and the amplitude adjusted Fourier transformation (AAFT) method (<xref ref-type="bibr" rid="bib31">Theiler et al., 1992</xref>; see Materials and methods). Both methods generate surrogate neural data with the same amplitude, but randomized phase of the Fourier transform as the original data. The two methods differ in that the surrogate neural data has a normalized spike count distribution (FPR) or maintains the original spike count distribution (AAFT). In all of these methods, the p-value for a regression coefficient was determined by the frequency in which the magnitude of <italic>t</italic>-value obtained using surrogate data exceeds that of the original <italic>t</italic>-value. When tested using the simulated neural data, all of these four methods yielded ~5% of false positive action-value and chosen-value neurons, and none of them was significantly higher than expected by chance (binomial test, p&gt;0.05; <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>) Therefore, these tests are unlikely to suffer from an inflated false positive rate when applied to our actual neural data.</p><p>For the session permutation method, we found that trial-by-trial action values are substantially correlated between the original and resampled behavioral sessions. We used four different combinations of reward probabilities (left:right = 0.72:0.12, 0.63:0.21, 0.21:0.63 and 0.12:0.72) in our previous studies and, even though their sequence was randomized, there was a constraint that the option with the higher-reward probability always changes its location at the beginning of a new block. The number of trials per block was also similar across studies (40.1 ± 3.1; mean ± SD). Hence, the relative reward probability tended to be correlated or anti-correlated between two different sessions depending on whether the first blocks of the two sessions had the same or different locations for the higher-reward-probability target (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Likewise, in the pseudosession method, which generates simulated behavioral data according to the same block structure of a given behavioral session, trial-by-trial action values tended to be positively correlated between actual and simulated behavioral sessions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This raises the possibility that for the neural data collected during the experiments with a block design, the session permutation and pseudosession methods might be too stringent (high false negative rate) for the identification of action-value neurons. Unlike action values, trial-by-trial chosen values were only weakly correlated between the original and resampled behavioral sessions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p></sec><sec id="s2-3"><title>Activity related to action value and chosen value</title><p>We applied the above methods to the actual neural data obtained from rats. We analyzed the neural data during the last 2 s of the delay period to assess action-value-related neural activity. As expected, the conventional <italic>t</italic>-tests yielded high levels of action-value signals and they were reduced substantially by employing the within-block permutation procedure or adding AR terms. All of these methods yielded significant (binomial test, p&lt;0.05) fractions of action-value neurons in all tested brain structures except the DLS (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top). The pseudosession, FPR, and AAFT methods also yielded significant action-value signals in all of these brain structures except the DLS. The proportion of action value-coding neurons tended to be somewhat lower when they were determined with the session permutation method, but this was still significantly above the chance level in several brain areas, including the striatum, OFC, and hippocampus (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom). When applied to neural data during the 2 s time period centered around the outcome onset, all of these methods yielded significant chosen-value signals in all tested brain structures (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). These results show significant encoding of action-value and chosen-value signals in multiple areas of the rat brain that cannot be explained by slowly drifting and serially correlated neural activity.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Action-value and chosen-value signals in multiple brain regions.</title><p>Action-value and chosen-value neurons were determined based on actual behavioral data and actual neural data recorded from several different areas of the rat brain. Shown are fractions of neurons significantly responsive to either action value (p&lt;0.025 for <italic>Q<sub>L</sub></italic> or <italic>Q<sub>R</sub></italic>; <bold>A</bold>) or chosen value (p&lt;0.05 for <italic>Q<sub>c</sub></italic>; <bold>B</bold>) determined with the previous methods (top) or resampling-based methods (bottom). Significant fractions (binomial test) are indicated by black filled circles.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53045-fig3-v2.tif"/></fig></sec><sec id="s2-4"><title>Transformation of value signals</title><p>In reinforcement learning theory, action values are monotonically related to the probability of choosing the corresponding actions, referred to as policy, making it hard to distinguish the neural activity related to either of these quantities. In addition, the activity of individual neurons is likely to encode multiple variables simultaneously (<xref ref-type="bibr" rid="bib26">Rigotti et al., 2013</xref>). Despite these difficulties, it has been argued that neural signals related to action value might actually represent policy exclusively (<xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref>). To address this issue quantitatively, we used the difference in action values (<italic>ΔQ</italic>) and their sum (<italic>ΣQ</italic>) as proxies for policy and state value, respectively, and tested how signals for action value, policy, and state value are related in a population of neurons in different brain structures.</p><p>If the activity of a given neuron is strongly related to policy, then its activity would be related to the difference in action values, <italic>ΔQ</italic>, but not their sum, <italic>ΣQ</italic>. To test whether this is the case, we analyzed the same neural data used to assess action-value-related neural activity in rats (neural spikes during the last 2 s of the delay period). As shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, some of the action value-responsive neurons showed activity correlated with <italic>ΣQ</italic> (25.8–62.5% across different brain areas), some with <italic>ΔQ</italic> (13.2–42.9%), and others with both <italic>ΣQ</italic> and <italic>ΔQ</italic> (6.3–25%). There were also neurons that were exclusively responsive to action value (0–22.7%). Conversely, some of <italic>ΣQ</italic>- and <italic>ΔQ</italic>-responsive neurons were also responsive to action value (<italic>ΣQ</italic>, 59.1–100%; <italic>ΔQ</italic>, 11.5–38.5%) and some were exclusively responsive to <italic>ΣQ</italic> (0–40.9%) or <italic>ΔQ</italic> (61.5–88.5%). These results indicate that a population of neurons in many brain areas tend to represent all of these variables rather than exclusively representing only one type of decision variable. For comparison, we also analyzed neural activity recorded during the 1 s time window before cue onset from the CD (a part of the DS), VS, and DLPFC of monkeys performing an intertemporal choice task (<xref ref-type="bibr" rid="bib2">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Kim et al., 2008</xref>). The results from this analysis were similar to those obtained from rats (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), suggesting that DLPFC and striatal neurons in monkeys also represent all of these variables rather than exclusively representing only one type of value signals.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural signals related to action value, policy, and state value.</title><p>(<bold>A</bold>) Transformations applied to the angle defined by the original regression coefficients (θ) to examine multiple types of value signals (θ<sub>2</sub> for signals related to policy vs. state value; θ<sub>4</sub> for action values vs. other value signals). (<bold>B</bold>) The scatter plots show <italic>t</italic>-values for the left and right action values (abscissa and ordinate, respectively) estimated from neural activity recorded in different areas of the rat brain. Filled circles denote those neurons significantly responsive to one or more of the decision variables tested (<italic>Q<sub>L</sub></italic>, <italic>Q<sub>R</sub></italic>, <italic>ΔQ,</italic> and <italic>ΣQ</italic>). <italic>Q</italic>, those neurons significantly responsive to either action value (p&lt;0.025 for <italic>Q<sub>L</sub></italic> or <italic>Q<sub>R</sub></italic>). The vectors on the right panel for each area show mean vectors computed after doubling (2θ, red) or quadrupling (4θ, blue) the angle of each data point in the scatter plots. Red filled circles, the Y-component of the mean vector is significantly different from 0; blue filled circles, the X-component of the mean vector is significantly different from 0 (Wilcoxon rank-sum test, p&lt;0.05). (<bold>C</bold>) The same scatter and vector plots for monkey striatal and DLPFC neurons. <italic>DV<sub>L</sub></italic>, left action value; <italic>DV<sub>R</sub></italic>, right action value.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53045-fig4-v2.tif"/></fig><p>Even though all the brain regions tested in this study represented multiple types of value signals in parallel, their relative signal strengths varied across brain regions. If multiple types of value signals are represented equally often and strongly, then the points in <xref ref-type="fig" rid="fig4">Figure 4</xref> would be rotationally invariant. By contrast, the pattern of anisotropy in these plots would change if the neurons in a given brain area tend to encode a specific type of value signals. For example, if neurons in a given brain area mostly encode <italic>ΣQ</italic>, the points representing individual neurons would be clustered along the identity line, since the regression coefficients for <italic>Q<sub>L</sub></italic> and <italic>Q<sub>R</sub></italic> would be similar for such neurons (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). As shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, the distribution pattern of <italic>Q<sub>L</sub></italic>-versus-<italic>Q<sub>R</sub></italic> regression coefficients varied substantially across regions.</p><p>To quantify this further, we computed the mean resultant vectors after multiplying the angle of the vector defined by the regression coefficients for action values, θ (see Materials and methods), by a specific factor. First, we compared the vertical component of the mean resultant vector calculated after doubling these angles to test whether neurons in each area might be biased for coding <italic>ΣQ</italic> or <italic>ΔQ</italic> (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The results from this analysis showed that the vertical component of the mean resultant vector was significantly positive in all regions in the rat (Wilcoxon rank-sum test, statistical test results summarized in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>; see also <xref ref-type="fig" rid="fig4">Figure 4B</xref>), indicating stronger encoding of <italic>ΣQ</italic> than <italic>ΔQ</italic> signals. In addition, the vertical component of this resultant varied in magnitude across regions. In the striatum, it was significantly larger in the VS than in the DMS and DLS. In the cortical areas, it was significantly larger in the OFC, mPFC, and ACC than in the M2 (one-way ANOVA followed by Bonferroni post hoc tests, statistical test results summarized in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). In the monkey, the vertical component of the mean resultant vector was significantly negative in the DLPFC (Wilcoxon rank-sum test; <xref ref-type="fig" rid="fig4">Figure 4C</xref>), indicating stronger encoding of <italic>ΔDV</italic> than <italic>ΣDV</italic> signals in the DLPFC. In addition, the vertical component of the resultant in the VS was significantly different from those in the DLPFC and CD, suggesting that the VS neurons tended to encode <italic>ΣDV</italic> signals more strongly than DLPFC and CD neurons (statistical test results summarized in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Collectively, these results showed that <italic>ΣQ</italic> signals are generally stronger than <italic>ΔQ</italic> signals in the rat brain areas examined in this study, and <italic>ΔDV</italic> signals are particularly strong in the monkey DLPFC.</p><p>Next, we examined the horizontal component of the mean resultant vector after multiplying the angles of the regression coefficient vector by four in order to test whether neurons in each brain area tend to favor coding action values of individual choices or whether they tend to combine action values to encode policy or state value. After this transformation, <italic>Q<sub>L</sub></italic>- and <italic>Q<sub>R</sub></italic>-coding neurons together would form vectors along the X-axis with positive horizontal components, whereas <italic>ΣQ</italic>- and <italic>ΔQ</italic>-coding neurons together would form vectors along the X-axis in the negative domain (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The results from this analysis showed that the horizontal component of the mean resultant vector was significantly negative in the VS, OFC, mPFC, ACC, and M2, but not in the other regions of the rat brain (Wilcoxon rank-sum test; <xref ref-type="fig" rid="fig4">Figure 4B and C</xref>; statistical test results summarized in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>), indicating that signals related to <italic>ΣQ</italic> and <italic>ΔQ</italic> were more strongly represented than action-value signals of individual choices in the rat VS, OFC, mPFC, ACC, and M2.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Neural signals related to action value have been found in widespread regions of the brain, especially in the frontal cortex-basal ganglia loop (<xref ref-type="bibr" rid="bib3">Chase et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Ito and Doya, 2011</xref>; <xref ref-type="bibr" rid="bib18">Lee, 2006</xref>; <xref ref-type="bibr" rid="bib19">Lee et al., 2012a</xref>; <xref ref-type="bibr" rid="bib27">Rushworth et al., 2009</xref>), suggesting the involvement of multiple brain structures in value-based decision making. However, the potential confounding of concurrent autocorrelations in value signals and neural activity and the possible superposition of different types of value signals have not been clearly resolved. The results in the present study confirm significant action-value signals in most of the brain regions tested previously. We also found that action-value-related neural activity coexists with that related to policy and state values. These results support previous conclusions that action values are represented in many regions of the brain. Below, we discuss these two issues along with the significance of anatomical variation in value signals.</p><sec id="s3-1"><title>Concurrence of autocorrelation in behavioral and neural data</title><p>Neural spikes are often correlated across trials, as we demonstrated for all brain structures examined in the present study. Note that serial correlation in neural activity could be due to intrinsic non-stationarity and/or its relationship with action value. In the present study, we reanalyzed our previous neural data to rigorously test whether action-value neurons identified in previous block-design studies might result from serial correlation in neural spikes unrelated to action value. Recently, this issue was examined with simulated neural and behavioral data (<xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref>; <xref ref-type="bibr" rid="bib9">Harris, 2020</xref>), but the nature of serial correlations in simulated neural spikes and action value might deviate substantially from those of actual neural and behavioral data. In the present study, using actual behavioral data and simulated neural data whose level of autocorrelation was matched to that of the actual neural data, we first established that four different analysis methods, namely the session permutation, pseudosession, FPR and AAFT methods (<xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref>; <xref ref-type="bibr" rid="bib9">Harris, 2020</xref>; <xref ref-type="bibr" rid="bib31">Theiler et al., 1992</xref>), do not inflate action-value signals. Applying these methods to actual neural data, we still found significant action-value signals in multiple areas of the rat brain. These findings indicate that action-value signals in our previous block-design studies cannot be entirely attributed to concurring autocorrelations in behavioral data and neural spikes unrelated to action value.</p><p>It should be noted that the optimal methods to assess value-related neural activity might vary depending on exact structures of neural and behavioral data. In our studies, because of similarity in block structure across sessions, trial-by-trial action values were substantially correlated across sessions. This suggests that session permutation might be excessively stringent for testing action-value-related neural activity. This is similarly problematic for the pseudosession test because simulated behavioral sessions have the same block structure as the original behavioral session. Indeed, both methods yielded somewhat lower fractions of action-value neurons compared to the FPR and AAFT tests. The pseudosession method yielded somewhat higher fractions of action-value neurons than the session permutation test in most tested regions, which suggests that some variability in the animal’s behavior shared across different sessions (e.g., a slow change in motivation) might not be captured by the models used to estimate action values. For blocked behavioral sessions, therefore, the FPR and AAFT methods are likely to estimate action-value signals more accurately than the session permutation and pseudosession methods. Our results also suggest that the problem arising from serial correlation in neural activity can be ameliorated by adding AR terms in the regression model. In our study, the results obtained with the FPR and AAFT methods were similar. Nevertheless, the simulated neural data obtained with the FPR lose their discrete properties and become normally distributed, whereas the AAFT maintains the original distribution of spike counts. Therefore, the AAFT method might be more reliable when the neural signals of interest are influenced by a non-Gaussian or discrete nature of neural data. Neural activity is almost always serially correlated, and this makes it difficult to select appropriate statistical methods to identify how sensory, motor, or other cognitive variables are encoded in the brain when they are also serially correlated. For each candidate analysis method, therefore, it would be prudent to examine the rates of potential false positivity and negativity using a null data set that captures important features of the data set under investigation.</p></sec><sec id="s3-2"><title>Multiple types of value signals</title><p>Neural activity seemingly representing action value might in fact represent other decision variables, such as policy, that are correlated with action value (<xref ref-type="bibr" rid="bib7">Elber-Dorozko and Loewenstein, 2018</xref>). To test this, we compared neuronal responses to action value with those related to the sum of two action values and their difference as proxies for neuronal responses related to state value and policy, respectively. We found neurons carrying diverse combinations of value-related signals in the striatum, frontal cortical areas, and hippocampus. The majority of action-value coding neurons also coded state value and/or policy and, conversely, the majority of state value- and/or policy-coding neurons also coded action value as well. Also, a small number of neurons encoded action value without state value or policy, and some neurons encoded state value or policy without action value. Similarly, using a task in which values associated with specific colors and locations of sensory cues can be dissociated, we have shown previously that partially overlapping populations of neurons represent values associated with target colors and locations in the striatum and DLPFC in monkeys (<xref ref-type="bibr" rid="bib16">Kim et al., 2012</xref>). Collectively, these results suggest that neurons in the striatum, frontal cortical areas, and hippocampus might not represent multiple types of value signals categorically, but instead show random mixed selectivity (<xref ref-type="bibr" rid="bib10">Hirokawa et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Raposo et al., 2014</xref>). Namely, the results from our analyses suggest that relatively weights given to different types of value signals vary continuously across individual neurons in most brain areas.</p><p>In addition to this heterogeneity in value coding within each brain region, how different types of value signals are encoded by individual neurons also varied across the brain structures examined in the present study. In the rat, all the tested regions, especially the OFC, mPFC, ACC, and VS, tended to over-represent signals related to the sum of action values, but this tendency was weaker in the M2. These results suggest that the OFC, mPFC, ACC, and VS might mainly process signals related to the expected rewards that can be obtained in a given state (<xref ref-type="bibr" rid="bib1">Bari et al., 2019</xref>), whereas the M2 might be concerned more with policy and action selection (<xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>). The primate DLPFC conveyed relatively strong signals correlated with the difference between action values, suggesting its function might be more strongly related to policy and action selection than state value. These findings are at odds with functional homology between the rodent mPFC and monkey DLPFC (<xref ref-type="bibr" rid="bib32">Uylings et al., 2003</xref>; <xref ref-type="bibr" rid="bib33">Vertes, 2006</xref>). As in the rat striatum, we found a stronger representation of signals related to the sum of action values in the VS than in the CD in monkeys. This is consistent with the proposal that subdivisions of the striatum correspond to distinct cortico-basal ganglia loops serving different functions (<xref ref-type="bibr" rid="bib6">Devan et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Ito and Doya, 2011</xref>; <xref ref-type="bibr" rid="bib25">Redgrave et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Yin and Knowlton, 2006</xref>). Further studies are needed to clarify relative strengths of different decision variables in different brain structures and how they are related to the functions served by individual brain structures.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Behavioral and neural data</title><p>We analyzed single-neuron activity recorded from the dorsomedial (DMS, n = 466), dorsolateral (DLS, n = 206), and ventral (VS, n = 165) striatum of six rats performing a dynamic foraging task (a total of 81 sessions) in our previous studies (<xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Kim et al., 2009</xref>), as well as activity recorded from the lateral OFC (n = 1148, three rats), ACC (n = 673, five rats), mPFC (n = 854, six rats), M2 (n = 411, three rats), and dorsal CA1 (n = 508, 11 rats) in our previous studies (total 302 sessions; <xref ref-type="bibr" rid="bib28">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Lee et al., 2012b</xref>; <xref ref-type="bibr" rid="bib21">Lee et al., 2017</xref>). For the analysis of action-value signals, we focused on neural activity during the last 2 s interval of the delay period and included only the neurons with mean discharge rates ≥1 Hz during the analysis window. For the analysis of chosen-value signals, we analyzed the activity during the 2 s time period centered around the outcome onset for the neurons with mean discharge rates ≥1 Hz during the analysis window. We also analyzed neural activity previously recorded in the CD, VS, and DLPFC of three monkeys performing an intertemporal choice task (<xref ref-type="bibr" rid="bib2">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Kim et al., 2008</xref>). This analysis was based on the activity during the 1 s cue period of the neurons with mean discharge rates ≥1 Hz.</p></sec><sec id="s4-2"><title>Behavioral task</title><p>Details of behavioral tasks have been published previously (<xref ref-type="bibr" rid="bib2">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Kim et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">Lee et al., 2012a</xref>; <xref ref-type="bibr" rid="bib21">Lee et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Sul et al., 2010</xref>). Briefly, each rat performed one of two different dynamic foraging tasks. Each trial began as the rat returned to the central stem (detected by a photobeam sensor; green arrow in <xref ref-type="fig" rid="fig1">Figure 1A</xref>) of a modified T-maze from either target location (orange circles in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). After a delay of 2–3 s, the central bridge was lowered (delay offset) allowing the rat to navigate forward and choose freely between the two goal locations to obtain water reward. The rats performed four blocks of trials with each block associated with one of four different reward probability pairs (left:right = 0.72:0.12, 0.63:0.21, 0.21:0.63 or 0.12:0.72). The sequence of block was randomly determined with the constraint that the higher-probability target changes its location at the beginning of each block. In the two-armed bandit (TAB) task (n = 215 sessions, n = 17 rats; <xref ref-type="bibr" rid="bib15">Kim et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Lee et al., 2012b</xref>; <xref ref-type="bibr" rid="bib28">Sul et al., 2010</xref>), water was delivered probabilistically only at the chosen location in a given trial, whereas in the dual assignment with hold (DAWH) task (n = 168 sessions, n = 10 rats; <xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib21">Lee et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>), water was delivered probabilistically at both locations according to a concurrent variable-ratio reinforcement schedule. Water delivered at the unvisited goal remained available until the rat’s next visit without additional water delivery. This implies that reward probability for a given target increases with the number of consecutive choices for the other target during the DAWH task. Mean (± SD) trial duration was 17.64 ± 13.35 s in the TAB task and 16.25 ± 14.82 s in the DAWH task.</p><p>Monkeys performed an intertemporal choice task (<xref ref-type="bibr" rid="bib2">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Kim et al., 2008</xref>). A trial began with the monkey’s fixation of gaze on a white square presented at the center of a computer screen. Following a 1 s fore-period, two peripheral targets were presented. One target was green and delivered a small reward (0.26 ml of apple juice) when it was chosen, whereas the other target was red and delivered a large reward (0.4 ml of apple juice). The number of yellow disks (n = 0, 2, 4, 6, or 8) around each target indicated the delay (1 s/disk) between the animal’s choice and reward delivery (0 or 2 s for a small reward; 0, 2, 4, 6, or 8 s for a large reward). Each of the 10-possible delay pairs for the two targets was displayed four times in alternating blocks of 40 trials in a pseudo-random manner with the position of the large-reward target counterbalanced.</p></sec><sec id="s4-3"><title>Reinforcement learning models</title><p>We used the Q-learning model (<xref ref-type="bibr" rid="bib30">Sutton and Barto, 1998</xref>) to calculate the action values (<italic>Q<sub>L</sub></italic> and <italic>Q<sub>R</sub></italic> for left-target and right-target choices, respectively) for the TAB task, and the stacked-probability model (<xref ref-type="bibr" rid="bib11">Huh et al., 2009</xref>) for the DAWH task, respectively. In the Q-learning model, action values (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>) were computed in each trial as follows:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where α is the learning rate, <inline-formula><mml:math id="inf2"><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> denotes the reward in the <italic>t</italic>-th trial (1 if rewarded and 0 otherwise), and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates the selected action (left or right goal choice). In the stacked-probability model, values were computed considering that reward probability of the unchosen target increases as a function of the number of consecutive alternative choices (see <xref ref-type="bibr" rid="bib11">Huh et al., 2009</xref> for details).</p><p>For the intertemporal choice task (<xref ref-type="bibr" rid="bib2">Cai et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Kim et al., 2008</xref>), the temporally DV was computed using a hyperbolic discount function as the following:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicate the magnitude and the delay of the reward from target <italic>x</italic>, and the parameter <italic>k</italic> determines the steepness of the discount function. We indicate action value as <italic>DV<sub>x</sub></italic> instead of <italic>Q<sub>a</sub></italic> to denote temporally DV in the monkey studies. Actions were chosen according to the softmax action selection rule in all models as the following:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the probability to choose the left goal, <inline-formula><mml:math id="inf7"><mml:mi>β</mml:mi></mml:math></inline-formula> is the inverse temperature that defines the degree of randomness in action selection, <italic>b</italic> is a bias term for selecting the left target, and <italic>Q<sub>L</sub></italic> and <italic>Q<sub>R</sub></italic> (or <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) are values associated with two alternative actions of choosing left and right targets, respectively, in trial <italic>t</italic>. All the model parameters were estimated using a maximum likelihood method.</p></sec><sec id="s4-4"><title>Regression analysis</title><p>We used multiple linear regression models to identify neurons related to action value or chosen value. For action-value-related neural activity, we analyzed neural spikes during the delay period (before action selection) using several different regression models. The simplest contained only the left and right action values as explanatory variables as follows:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>S(t)</italic> is the spike count in a given analysis time window in trial <italic>t</italic>, <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> are the action values for the left and right target choices, respectively, and <inline-formula><mml:math id="inf12"><mml:mi>ε</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the error. The majority of the analysis was based on the following model that contained the animal’s choice (<italic>C</italic>, 1 if left and 0 if right) and chosen value (<italic>Q<sub>c</sub></italic>) as additional explanatory variables to control for effects of these variables on action values:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We subjected this model to various resampling-based tests to identify action-value neurons. To compare the results from our previous analysis method (<xref ref-type="bibr" rid="bib28">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Sul et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Lee et al., 2012b</xref>; <xref ref-type="bibr" rid="bib17">Kim et al., 2013</xref>; <xref ref-type="bibr" rid="bib21">Lee et al., 2017</xref>), we added AR terms, namely neural spikes during the same analysis time window in the previous three trials, to model 2.<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To investigate how multiple types of value signals are encoded in the activity of neurons across different brain areas, we tested a regression model that includes the sum of action values, <italic>ΣQ(t)=Q<sub>L</sub>(t)+Q<sub>R</sub>(t)</italic>, and their difference, Δ<italic>Q(t)=Q<sub>L</sub>(t)−Q<sub>R</sub>(t)</italic>, which roughly correspond to state value and policy, respectively.<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This regression model would fit the data equally well compared to the model containing action values (<italic>Q<sub>L</sub></italic> and <italic>Q<sub>R</sub></italic>) because <italic>ΔQ</italic> and <italic>ΣQ</italic> are linear combinations of action values. For chosen-value-related neural activity recorded in rats at the time choice outcome was revealed, the following two regression models were used:<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>R(t)</italic> is reward (1 if reward and 0 if unrewarded) and <italic>X(t)</italic> is the interaction between choice and reward.</p><p>Action-value-related neural activity in the monkey was analyzed using the following regression model:<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>D</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>D</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>6</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> are temporally DVs for the left and right target choices, respectively, and <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> are temporally DVs for the chosen and unchosen target choices, respectively. Neural activity related to the sum of and difference between temporally DVs (<inline-formula><mml:math id="inf17"><mml:mo>∑</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>D</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively) was assessed with the following regression model:<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:mi>D</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>D</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ε</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mn>7</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-5"><title>Permutation and surrogate data-based tests</title><p>For the session permutation and pseudosession tests, value-related neural activity was assessed using spike data of the original session. In the session permutation test, the original neural data was paired with 382 remaining behavioral sessions. The results did not differ qualitatively when we paired the neural data only with the same type of behavioral sessions as the original one (214 TAB and 167 DAWH remaining sessions). In the pseudosession test, we generated 500 simulated behavioral sessions based on the Q-learning (for TAB-task sessions) or stack probability (for DAWH-task sessions) model using model parameters estimated for a given animal. For the FPR and AAFT tests (<xref ref-type="bibr" rid="bib31">Theiler et al., 1992</xref>), value-related neural activity was assessed using the original behavioral data and 1000 samples of surrogate neural data. In the FPR test, each surrogate neural data was generated with the same amplitude of the Fourier transform as the original data but with random phase. In the AAFT test, the same number of elements as the number of trials in the original neural data was drawn randomly from a Gaussian distribution, and these elements were then sorted according to the rank of the neural data (Gaussianization). All zero values (no spikes) of the neural data were replaced with small (&lt;1) randomly chosen nonzero values in order to avoid artifacts in sorting consecutive zero values. The FPR method was then applied to the sorted Gaussian data. Finally, the original neural data was reordered according to the rank of the phase-randomized Gaussian data (de-Gaussianization), and this reordered neural data was used as surrogate neural data. For comparison, we also tested the within-block permutation procedure we used in our previous study (<xref ref-type="bibr" rid="bib15">Kim et al., 2009</xref>). For this, we randomly shuffled spike data 1000 times across different trials within each block while preserving the original block sequence.</p></sec><sec id="s4-6"><title>Statistical analysis</title><p>Significance (p-value) of a regression coefficient was determined with the <italic>t</italic>-test or by the frequency in which the absolute magnitude of <italic>t</italic>-value for the regression coefficient obtained using a permutation test or surrogate data exceeds that of the original <italic>t</italic>-value (resampling-based tests). Statistical significance of the fraction of action-value or chosen-value neurons in a given brain area was determined based on the binomial test.</p><p>To examine how different types of value signals are represented across different brain areas, we exploited the fact that the neurons encoding specific types of value signals, such as action values or policy, would be distributed along an oriented line through the origin in a complex plane defined by z = R⋅e<sup>-iθ</sup> = <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>R</italic> = <inline-formula><mml:math id="inf20"><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:math></inline-formula>, θ = atan(<inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are regression coefficients for left and right action values, respectively (namely, <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ6 equ11">Equationd 6 and 10</xref>). In this plane, neurons encoding state value or policy would display twofold rotational symmetry, since they would be distributed mainly along the lines defined by <italic>y = x</italic> or <italic>y = -x</italic>. When the angles are doubled, <italic>Q<sub>L</sub></italic>- and <italic>Q<sub>R</sub></italic>-coding neurons would form vectors along the x-axis (<italic>Q<sub>L</sub></italic>, positive; <italic>Q<sub>R</sub></italic>, negative) while <italic>ΣQ</italic>- and <italic>ΔQ</italic>-coding neurons would form vectors along the y-axis (<italic>ΣQ</italic>, positive; <italic>ΔQ</italic>, negative). Therefore, we examined the vertical component of the mean resultant vector after multiplying the angle of the vector z by a factor of 2 in order to test whether neurons in a given area tended to encode policy or state value more strongly. By contrast, the neurons encoding action values would show fourfold rotational symmetry since they would be clustered around <italic>x = 0</italic> or <italic>y = 0</italic>. Therefore, we examined the horizontal component of the mean resultant vector after multiplying the angle of z by a factor of 4 in order to test whether neurons tended to encode action values of individual choices or combine them for policy or state value. We used Wilcoxon rank-sum test to determine whether the horizontal or vertical component of the mean vector was significantly different from 0, and one-way ANOVA and Bonferroni post hoc tests to test whether they significantly varied across regions.</p><p>Throughout the paper, p=0.05 was used as the criterion for a significant statistical difference unless noted otherwise. Data are expressed as mean ± SEM unless noted otherwise. Raw data of this work is archived at Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.gtht76hj0">https://doi.org/10.5061/dryad.gtht76hj0</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Research Center Program of Institute for Basic Science (IBS-R002-A1; MWJ) and the National Institute of Health grants (DA 029330; DL).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis</p></fn><fn fn-type="con" id="con4"><p>Data curation</p></fn><fn fn-type="con" id="con5"><p>Data curation</p></fn><fn fn-type="con" id="con6"><p>Data curation</p></fn><fn fn-type="con" id="con7"><p>Data curation</p></fn><fn fn-type="con" id="con8"><p>Data curation</p></fn><fn fn-type="con" id="con9"><p>Formal analysis, Supervision</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Supervision, Validation</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Supervision, Validation</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Statistical test results for 2θ and 4θ plots.</title><p>Top, statistical test results for 2θ plots. Orange shading, Y-component of the mean vector was tested for significant deviation from 0 (Wilcoxon rank-sum test, red indicates p-values &lt;0.05). No shading, Y-component of the mean vector was compared across regions using one-way ANOVA (rat, F(7,2587) = 12.64, p=4.9 × 10<sup>−16</sup>; monkey, F(2,247) = 10.75, p=3.4 × 10<sup>−5</sup>) followed by Bonferroni post hoc tests. Significant differences (p-values &lt;0.05) between regions are indicated in red. Bottom, statistical test results for 4θ plots. Orange shading, X-component of the mean vector was tested for significant deviation from 0 (Wilcoxon rank-sum test, red indicates p-values&lt;0.05). No shading, X-component of the mean vector was compared across regions using one-way ANOVA (rat, F(7,2587) = 3.79, p=4.3 × 10<sup>−4</sup>; monkey, F(2,247) = 0.95, p=0.387) followed by Bonferroni post hoc tests. Significant differences (p-values &lt;0.05) between regions are indicated in red.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-53045-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53045-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analyzed during this study are included in the manuscript and supporting files. Raw data to reproduce this work is archived at Dryad <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.gtht76hj0">https://doi.org/10.5061/dryad.gtht76hj0</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>EJ</given-names></name><name><surname>Jang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Sul</surname><given-names>JH</given-names></name><name><surname>Chung</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data from: Robust and distributed neural representation of action values</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.gtht76hj0</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bari</surname> <given-names>BA</given-names></name><name><surname>Grossman</surname> <given-names>CD</given-names></name><name><surname>Lubin</surname> <given-names>EE</given-names></name><name><surname>Rajagopalan</surname> <given-names>AE</given-names></name><name><surname>Cressy</surname> <given-names>JI</given-names></name><name><surname>Cohen</surname> <given-names>JY</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stable representations of decision variables for flexible behavior</article-title><source>Neuron</source><volume>103</volume><fpage>922</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.001</pub-id><pub-id pub-id-type="pmid">31280924</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Heterogeneous coding of temporally discounted values in the dorsal and ventral striatum during intertemporal choice</article-title><source>Neuron</source><volume>69</volume><fpage>170</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.11.041</pub-id><pub-id pub-id-type="pmid">21220107</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chase</surname> <given-names>HW</given-names></name><name><surname>Kumar</surname> <given-names>P</given-names></name><name><surname>Eickhoff</surname> <given-names>SB</given-names></name><name><surname>Dombrovski</surname> <given-names>AY</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reinforcement learning models and their neural correlates: an activation likelihood estimation meta-analysis</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>15</volume><fpage>435</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.3758/s13415-015-0338-7</pub-id><pub-id pub-id-type="pmid">25665667</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corrado</surname> <given-names>G</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Understanding neural coding through the model-based analysis of decision making</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>8178</fpage><lpage>8180</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1590-07.2007</pub-id><pub-id pub-id-type="pmid">17670963</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reinforcement learning: the good, the bad and the ugly</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>185</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.08.003</pub-id><pub-id pub-id-type="pmid">18708140</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devan</surname> <given-names>BD</given-names></name><name><surname>Hong</surname> <given-names>NS</given-names></name><name><surname>McDonald</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Parallel associative processing in the dorsal striatum: segregation of stimulus-response and cognitive control subregions</article-title><source>Neurobiology of Learning and Memory</source><volume>96</volume><fpage>95</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2011.06.002</pub-id><pub-id pub-id-type="pmid">21704718</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elber-Dorozko</surname> <given-names>L</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Striatal action-value neurons reconsidered</article-title><source>eLife</source><volume>7</volume><elocation-id>e34248</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34248</pub-id><pub-id pub-id-type="pmid">29848442</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glimcher</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</article-title><source>PNAS</source><volume>108 Suppl 3</volume><fpage>15647</fpage><lpage>15654</lpage><pub-id pub-id-type="doi">10.1073/pnas.1014269108</pub-id><pub-id pub-id-type="pmid">21389268</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nonsense correlations in neuroscience</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.29.402719</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirokawa</surname> <given-names>J</given-names></name><name><surname>Vaughan</surname> <given-names>A</given-names></name><name><surname>Masset</surname> <given-names>P</given-names></name><name><surname>Ott</surname> <given-names>T</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Frontal cortex neuron types categorically encode single decision variables</article-title><source>Nature</source><volume>576</volume><fpage>446</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1816-9</pub-id><pub-id pub-id-type="pmid">31801999</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Jo</surname> <given-names>S</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Sul</surname> <given-names>JH</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Model-based reinforcement learning under concurrent schedules of reinforcement in rodents</article-title><source>Learning &amp; Memory</source><volume>16</volume><fpage>315</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1101/lm.1295509</pub-id><pub-id pub-id-type="pmid">19403794</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Validation of decision-making models and analysis of decision variables in the rat basal ganglia</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>9861</fpage><lpage>9874</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6157-08.2009</pub-id><pub-id pub-id-type="pmid">19657038</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiple representations and algorithms for reinforcement learning in the cortico-basal ganglia circuit</article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>368</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.04.001</pub-id><pub-id pub-id-type="pmid">21531544</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Hwang</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Prefrontal coding of temporally discounted values during intertemporal choice</article-title><source>Neuron</source><volume>59</volume><fpage>161</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.05.010</pub-id><pub-id pub-id-type="pmid">18614037</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Sul</surname> <given-names>JH</given-names></name><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Role of striatum in updating values of chosen actions</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>14701</fpage><lpage>14712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2728-09.2009</pub-id><pub-id pub-id-type="pmid">19940165</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>S</given-names></name><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Hwang</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prefrontal and striatal activity related to values of objects and locations</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>108</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00108</pub-id><pub-id pub-id-type="pmid">22822390</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals for previous goal choice persist in the dorsomedial, but not dorsolateral striatum of rats</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>52</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2422-12.2013</pub-id><pub-id pub-id-type="pmid">23283321</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural basis of quasi-rational decision making</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>191</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.02.001</pub-id><pub-id pub-id-type="pmid">16531040</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Seo</surname> <given-names>H</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Neural basis of reinforcement learning and decision making</article-title><source>Annual Review of Neuroscience</source><volume>35</volume><fpage>287</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150512</pub-id><pub-id pub-id-type="pmid">22462543</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Ghim</surname> <given-names>JW</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Hippocampal neural correlates for values of experienced events</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>15053</fpage><lpage>15065</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2806-12.2012</pub-id><pub-id pub-id-type="pmid">23100426</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>SH</given-names></name><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Lee</surname> <given-names>JW</given-names></name><name><surname>Ghim</surname> <given-names>JW</given-names></name><name><surname>Lee</surname> <given-names>I</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural signals related to outcome evaluation are stronger in CA1 than CA3</article-title><source>Frontiers in Neural Circuits</source><volume>11</volume><elocation-id>40</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2017.00040</pub-id><pub-id pub-id-type="pmid">28638322</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Shea</surname> <given-names>NJ</given-names></name><name><surname>Kolling</surname> <given-names>N</given-names></name><name><surname>Rushworth</surname> <given-names>MFS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Model-based analyses: promises, pitfalls, and example applications to the study of cognitive control</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>65</volume><fpage>252</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1080/17470211003668272</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Hampton</surname> <given-names>A</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model-based fMRI and its application to reward learning and decision making</article-title><source>Annals of the New York Academy of Sciences</source><volume>1104</volume><fpage>35</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1196/annals.1390.022</pub-id><pub-id pub-id-type="pmid">17416921</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname> <given-names>D</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redgrave</surname> <given-names>P</given-names></name><name><surname>Rodriguez</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>Y</given-names></name><name><surname>Rodriguez-Oroz</surname> <given-names>MC</given-names></name><name><surname>Lehericy</surname> <given-names>S</given-names></name><name><surname>Bergman</surname> <given-names>H</given-names></name><name><surname>Agid</surname> <given-names>Y</given-names></name><name><surname>DeLong</surname> <given-names>MR</given-names></name><name><surname>Obeso</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Goal-directed and habitual control in the basal ganglia: implications for parkinson's disease</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>760</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1038/nrn2915</pub-id><pub-id pub-id-type="pmid">20944662</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname> <given-names>M</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>MR</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushworth</surname> <given-names>MF</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>General mechanisms for making decisions?</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>75</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.02.005</pub-id><pub-id pub-id-type="pmid">19349160</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sul</surname> <given-names>JH</given-names></name><name><surname>Kim</surname> <given-names>H</given-names></name><name><surname>Huh</surname> <given-names>N</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Distinct roles of rodent orbitofrontal and medial prefrontal cortex in decision making</article-title><source>Neuron</source><volume>66</volume><fpage>449</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.033</pub-id><pub-id pub-id-type="pmid">20471357</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sul</surname> <given-names>JH</given-names></name><name><surname>Jo</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Role of rodent secondary motor cortex in value-based action selection</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1202</fpage><lpage>1208</lpage><pub-id pub-id-type="doi">10.1038/nn.2881</pub-id><pub-id pub-id-type="pmid">21841777</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-loc>Cambridge, US</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theiler</surname> <given-names>J</given-names></name><name><surname>Eubank</surname> <given-names>S</given-names></name><name><surname>Longtin</surname> <given-names>A</given-names></name><name><surname>Galdrikian</surname> <given-names>B</given-names></name><name><surname>Doyne Farmer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Testing for nonlinearity in time series: the method of surrogate data</article-title><source>Physica D: Nonlinear Phenomena</source><volume>58</volume><fpage>77</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(92)90102-S</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uylings</surname> <given-names>HB</given-names></name><name><surname>Groenewegen</surname> <given-names>HJ</given-names></name><name><surname>Kolb</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Do rats have a prefrontal cortex?</article-title><source>Behavioural Brain Research</source><volume>146</volume><fpage>3</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2003.09.028</pub-id><pub-id pub-id-type="pmid">14643455</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vertes</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Interactions among the medial prefrontal cortex, Hippocampus and midline thalamus in emotional and cognitive processing in the rat</article-title><source>Neuroscience</source><volume>142</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.06.027</pub-id><pub-id pub-id-type="pmid">16887277</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname> <given-names>HH</given-names></name><name><surname>Knowlton</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of the basal ganglia in habit formation</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>464</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nrn1919</pub-id><pub-id pub-id-type="pmid">16715055</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53045.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Action values are important components in reinforcement learning. Single neutrons in the brain have been reported to signal these values, but recent work has suggested that problems with these analyses bring these data into question. This paper performs rigorous analysis to show the action value signals are robust. In doing so it contributes to an important line of technical research that is finding safer ways to analyse neuronal data.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Further evidence for neural representation of action value&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Timothy Behrens as the Senior Editor and Reviewing Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript is a response to recent work (Elber-Dorozko and Loewenstein 2018) showing that inferences about action value coding in neural population can be distorted by two mechanisms: (1) Serial correlation from trial to trial in both neural activity and action values, that causes statistical analyses that assume independence of each trial to overestimate significance (2) Correlation between action values and other behavioural/decision variables which can cause incorrect inference that neurons coding for other variables represent action values.</p><p>The present study uses simulations and reanalysis of neuronal recordings to address methodological and scientific questions raised by Elber-Dorozko and Loewenstein's work. Broadly, they do so convincingly (although not elegantly) with respect to serial correlations. The portion of the manuscript that deals with correlated variables is less convincing, but this is an issue of narrower significance.</p><p>The manuscript is of interest to <italic>eLife</italic> for the following reasons. The previous paper raised an important methodological concern that had been uniformally ignored in the analysis of neuronal activity across many fields. In doing so, however, it also cast doubts on previous results that had studied one particular computation performed by neuronal activity – the computation of action value. These were important studies revealing a fundamental computation underlying how the brian controls behaviour. The current manuscript acknowledges the methodological concern but allays the doubts over action-value computations. It is therefore of potential significant interest.</p><p>Essential revisions:</p><p>The methods used to investigate and account for serial autocorrelations are coarse, ad-hoc, and little is presented to verify that they do what they say they do. This raises 2 issues. The first relates to how robust the findings will be in the face of future related criticisms. The second relates to setting methodological standards for how the field should deal with serial correlations in the future.</p><p>We will deal with the second issue first, as dealing with it properly may make the first problem redundant.</p><p>Strategy for accounting for autocorrelations:</p><p>The issue is that the authors propose 3 techniques for dealing with serial autocorrelations in the noise (simulating neurons with serial correlations, permuting &quot;within blocks&quot; only, and including a few trials back as co-regressors). None of them are elegant or general techniques, and in all cases the behaviour of these techniques in the null case is poorly characterised (see below for more on this point). This is particularly surprising because this is an old problem in the statistics literature, and there are off-the-shelf techniques for addressing the problem elegantly and rigorously. We acknowledge the requirement to adhere to the spirit of the EDL paper, but we think it would be extremely advantageous to move the argument forward using general well-validated techniques.</p><p>Options include</p><p>1. Estimating a whitening kernel for the residuals and refitting the model using this kernel (technically nontrivial). For an example from fMRI, see Woolrich et al. Neuroimage 2001.</p><p>2. Fitting autoregressive noise models using standard off-the-shelf software (eg gls in r – see review comment below ).</p><p>3. Permutation tests after transformations that render the data exchangeable.</p><p>Eg fourier transform the data and permute only the phases then reconstruct the data.</p><p>Eg Wavelet transform the data and permute wavelet coefficients then reconstruct the data.</p><p>Here are the reviewer comments that led to this discussion, which contain other related comments that maybe useful. A similar discussion was had at triage. We note that, whilst the reviewer suggests an auto-regressive model, which would be fine, it would also be fine to use one of the other techniques above, which may be more appropriate if the residuals are not well described by a limited AR model such as AR(1). The permutation strategies described above are trivial to implement and effective.</p><p>&quot;I think the paper could do a better job technically unpacking the issues with temporal correlations, which in my view weren't diagnosed as precisely as they could have been in the original article either. The senior author knows more econometrics than I do, but as I understand it, all of the estimation issues in OLS here are due to the assumption of uncorrelated <italic>errors</italic>. Autocorrelation in the dependent variables, and the explanatory variables, is perfectly OK (indeed, one would result from the other) so long as they cancel each other out when the model is fit, in which case the residuals would be uncorrelated. Changing the autocorrelation in either y or X affects this only indirectly, since both appear in the residual. I think the article's focus on the autocorrelation in the explanatory variables and spike rates – both in rhetoric and analysis and results – is a piece of the puzzle but tends to obscure this deeper point. It would be helpful to also focus on visualizing and decorrelating the residuals. For the same reason, the lack of <italic>regressor</italic> autocorrelation in the monkey experiment is less of a solution than it is made out to be, I think.&quot;</p><p>&quot;I find the estimation strategy of Figure 3 and sporadically onward a bit frustrating and less convincing (or at least more roundabout) than it could be. I'm sympathetic to the overall conclusion, but the overall strategy comes off as piling up multiple fixes, even though they are shown not to work adequately on simulated data. To compensate for this, the simulations are used to define a new, inflated false positive rate that is, finally, in a followup test, compared to the obtained rate of nominal positives. Frankly: yuck. How about figuring out why the fixes don't work, and finding a test that does work? For the nonparametric fix (bootstrap) the issue is presumably within session correlations, as discussed later; but for the lagged AR terms, I assume the problem is there aren't enough of them to handle longer-timescale correlations. But this is itself kind of a hack; a more orthodox parametric approach would be to use a nonlinear, generalized least squares (eg gls() in R) to estimate a full AR(1) model or whatever other error covariance form is supported by the actual data. (Note that even an AR(1) process predicts correlations at arbitrary lags so adding individual lag terms is not sufficient.)&quot;</p><p>Characterisation of performance in the null case.</p><p>If the authors change their strategy as recommended above, this section of the review may be rendered redundant. However, given the current approach the review team did not think that the paper did a good job in presenting diagnostics that adequately evaluate the performance of their strategies.</p><p>Minimally, given how much work the random walk neuron model does, we think that the authors should try harder to evaluate the performance with a model that looks more like the data. The model was setup only to match the neuronal autocorrelations at lag 1 trial and likely has a very different autocorrelation structure from real neurons at lags greater than 1. The autocorrelation structure of the control 'random' neuron model should be matched to that of the neuronal data. This may need a generative model that is more expressive than AR(1). Without this, the authors are susceptible to future criticism that simply shows that the authors techniques do not do well in the face of realistic data.</p><p>We also think that instead of simply reporting the number of false positives at p&lt;0.05 threshold, the authors should construct the p-p plot (Wikipedia), which plots observed false positives in empirical data against the nominal threshold. This will make it useful to future researchers who would like to use the same techniques with different threshold.</p><p>All three reviewers made the same point. I include all 3 here to encourage the authors that it is an important point that will likely be shared by many readers.</p><p>&quot;Generation of random-walk neurons. How is it possible to create the same autocorrelation kernel as the one observed in the neural data (essentially flat – at least for the shown scale of 5 trials) through a random-walk process – for which the correlation should intrinsically decrease over time? The authors mentioned that they have matched autocorrelation at lag 1 only, which may be good enough as an approximation for what the authors intend to do with random-walk neurons, but it is not a tight match and the authors may want to mention this somewhere in the manuscript.&quot;</p><p>&quot;The random walk neuron model does a lot of work as a control against which real neurons are compared. However, the model was setup only to match the neuronal autocorrelations at lag 1 trial and likely has a very different autocorrelation structure from real neurons at lags greater than 1. The autocorrelation structure of the control 'random' neuron model should be matched to that of the neuronal data. &quot;</p><p>&quot;Either way, everything comes down here to the simulated spike trains under the null model, and it would be good to have more argument that these are actually a good simulation for the data. Among other things, I wasn't clear if their timescale is individually fit per brain area or experiment or just roughly chosen; if multiple timescales of correlation are detectable in the actual data, rather than just rectified AR(1) as here; and again if the autocorrelative structure of the residuals is similar between data and simulation. &quot;</p><p>Action values vs policy etc.</p><p>There was broad scepticism amongst the reviewers as to whether it was possible to dissociate policies from values, and whether it was really relevant to do so, particularly if policy is (confusingly) used to refer to a difference in Q-values. This is reflected in the comments below. Whilst we acknowledge the authors' ambitions to address the critiques raised in EDL, we encourage great care in the interpretation of this whole section. Again, related points were made by all 3 reviewers, highlighting that this Is likely also to be an issue for many readers.</p><p>&quot;I find the second half of the article, on alternative decision variables, a little bit of a red herring. One thing is that the relationship between a Q value and a policy (as the term is normally used in RL, and was used by Elber-Doroko) is nonlinear. Calling the difference in Q values a &quot;policy&quot; is just not using the term accurately. On the other hand, my view is that this example shows that the whole critique is ill founded, and the only useful question is what is the (linear or nonlinear) relationship between decision variables and brain activity. Neural representations of values are likely to be nonlinear for reasons other than policy (eg, there is plenty of work by Glimcher and others on gain control or divisive normalization) and may also be differential (eg, activity which is related to the relative value, chosen minus unchosen, which is nevertheless in units of value and not normalized/softmaxed etc into a policy). Telling the difference between divisive and subtractive normalization is not really viable, especially in the linear setting; and even so, the same (softmax) algebraic form could describe either policy or (gain controlled) value. There's just not a meaningful categorical distinction to be made. I suppose there might be some way of recasting this section to focus on the distinction between summation vs difference as being representative (in a linear framework) of state values vs. relativized, or normalized, or post-choice policy values. But I think it's giving too much away to frame this as actually distinct variables confounding one another; and also unfair to call a difference a policy.&quot;</p><p>&quot;Correlation with sum(Q) and diff(Q). I don't understand the exact graphical description on Figure 8 and Figure 8. The authors label gray neurons as 'only Q', but many of them are probably not coding anything (non-significant, corresponding to black neurons in Figure 6 of the article by EDL). Also, I expected that the neurons coding selectively for one action value (QL or QR) should be found on Figure 8A for |x| &gt; threshold and |y| ~ 0 and vice versa. However, it is clearly not the case given the labelling of neurons provided by the authors for this graph. Could the authors clarify this and explain the apparent discrepancy with the analyses performed by EDL (Figure 6 from their article). I have a similar concern regarding Figure 8B: pure action-value neurons seem to be located only at the center of the graphs (for |x| ~ 0 and |y| ~ 0), which is where non-selective neurons should be found.&quot;</p><p>&quot;Figures 7 – 9 attempt to dissociate action value coding from coding of policy (difference in action values) and state value (approximated as sum of action values). As these variables are linearly dependent, it is formally impossible to say whether a neuron represents one of them, or a linear combination of the others. Mixed linear selectivity is ubiquitous (e.g. Kobak et al. <italic>eLife</italic> 2016;5:e10989), so it not that interesting to ask which of this degenerate set of variables is most 'purely' represented by each neuron. This said, having chosen a given non-degenerate pair of these variables to work with, it is interesting to know how representation of one variable correlates with representation of the other across the population, and how this varies across regions. This is shown nicely in figure 7A and the top two panels of 9A, but I felt the remaining panels of figures 7-9 did not add additional value.&quot;</p><p>When the authors assess the extent of chosen-value coding (Figure 5, 6B) , they include the individual action values in their regression model, which is important as these variables are correlated. However, when they assess action value coding (Figures 4, 6A) they do not include chosen value in the models. I think the rationale is that the analyses are different trial epochs, pre-choice for 4, 6A, post outcome for 5, 6B. However, chosen-value coding is certainly possible before the choice is executed and hence chosen value should be included in the model when assessing action value coding.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Further evidence for neural representation of action value&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Timothy Behrens as the Senior Editor and Reviewing Editor. The reviewers have opted to remain anonymous.</p><p>I have written a summary of our opinion after the discussion directly below here. I have also left the reviews below for emphasis and detail, but please don't feel that you need to address all the points in the reviews. If you can address the central issues in the summary directly below here, we will be happy.</p><p>This revision has raised some complications in the reviewers' minds that have led to a lot of discussion. In brief, we are not happy with the 2-stage approach that does not lead to p-values for individual neurons that can be trusted.</p><p>Whilst we agree that this approach goes some way to rebutting the EDL finding, it is a narrow rebuttal which does not provide a good way forward for scientists faced with similar problems in the future. The combination of GLS modelling that does not accurately deal with the autocorrelations, with non-standard application of circular permutations to demonstrate control performance is, in our view, dangerous, and not useful to the community. We are not keen on publishing such an approach in <italic>eLife</italic>.</p><p>If the GLS approach does not lead to good corrections for autocorrelations, then we think it incumbent upon you guys to find a non-parametric approach that does.</p><p>We are slightly bemused by your assertion that you cannot do the Fourier permutation test because the timepoints are not equally sampled. There are two reasons we are bemused. The first is that the same argument applies to the circular permutation that you do use. The second is that there are well-established methods for computing Fourier transforms for non-uniformly sampled data. However, we think that you don't even need to use them. We are happy for you to line the trials up in a matrix and do the Fourier transform across trials. We think the danger of introducing errors by this approximation is small.</p><p>It is also possible that a correct application of the circular method you propose would work (i.e. ignore GLS and just do the full circular permutation test). The three likely issues with this are (1) the circular permutations might correlate with the original design matrix leading to low sensitivity, (2) edge effects will mean that the circular-shifted test will have different autocorrelation properties than the original test. This can lead to false positives (and actually may be a problem in your control analysis in the paper). (3) The small number of possible permutations will prevent accurate inference in the tail (and possibly prevent eg corrections for multiple comparisons).</p><p>Note that although reviewer 2 below has an issue with the &gt;10 trial threshold for the circular test, this was not thought a problem after discussion.</p><p>In order to assess these effects, one of the reviewers prepared a Jupyter Notebook comparing the different approaches. It looks as though I can only attach a single file to the letter and the notebook comes in both.html and.py forms. I have attached the.html. If you email me when you get this, I will forward the (anonymised).py.</p><p>You can see that, whilst both Fourier and Circular approaches suffer slightly from edge effects, the circular test is much more severely affected. There are readily available techniques in the literature for removing edge effects. For example you could window the data before the regression (eg using a Tukey window). We think it would likely be profitable for you to investigate these approaches whichever permutation method you choose. You can see, however, that even without dealing with these details, the Fourier method does a pretty good job.</p><p>You can see in the reviews below, that the reviewers also remain concerned about the distinction in the second half of the paper between action values, chosen values, policies etc, which interacts heavily with questions about the linearity of neuronal responses. We remain concerned that there is a danger this will confuse more than clarify the issue for the community. However, we realise that there is a similar section in the EDL paper, and that you guys need to address this. This may be TB's fault (along with the original reviewers) for not flagging this in the original paper. We would appreciate a clear statement in this section in the paper that states that this section is a narrow rebuttal of the EDL paper, and discusses the difficulties in differentiating policy from action values etc. (see reviews below).</p><p><italic>Reviewer #1:</italic></p><p>The authors have substantially modified their manuscript, including the general statistical approach for assessing the neural encoding of value signals. Doing so, they have addressed several of the concerns I have regarding the original manuscript. The GLS regression approach appears to support the main claims made in the original manuscript. Although I found at times the description of the results (including their illustration) to be less clear than before, I do not have important concerns that remain to be addressed.</p><p><italic>Reviewer #2:</italic></p><p>The authors have taken some welcome steps to address reviewer comments. They now use a regression model which explicitly models autocorrelation in the residuals (GLS model), they compare models with different order auto-regressive structure using BIC, and show P-P plots for real and circularly permuted data. These steps do improve the manuscript, but unfortunately the statistical approach still has real problems.</p><p>The main request of the reviews was: 1. Use a principled method which would fix the problem of P value inflation due to correlations. 2. Show that it works (i.e. gives correct P values) using P-P plots on simulated or otherwise generated 'null' data. 3. Use P values derived from this method directly as the statistics reported.</p><p>Doing this properly solves the issue once and for all, and both answers the specific question (do neurons encode action values) and provides a method the field can use to avoid problems in future.</p><p>What the authors have actually done is; 1. Use a principled method (GLS model with autocorrelated residuals) to obtain P values. 2. Provide diagnostics based on null data generated via a poorly implemented circular permutation approach (see below), which none the less suggests that the GLS model has not fixed the problem (P values for null data are still inflated). 3. Test whether the fraction of significant neurons in the original data is significantly different from the average fraction of significant neurons across the permuted data.</p><p>This is really not good, as it does not provide a way of calculating accurate P values for individual neurons, nor does it take into account variability across permutations when asking if the fraction of significant neurons in the real data is significantly higher than that expected by chance.</p><p>Circular permutation of neuronal data relative to behavioral data by a random number of trials is a good way of generating data under the null hypothesis that there is no relationship between activity and behavior, while preserving the autocorrelations in both. However, the authors did their permutations &quot;with the constraint that the minimum difference of trial number between the original and shifted data is &gt; 10&quot;. This makes the permuted data meaningless as it no longer comes from any well-defined null distribution. Additionally, to accurately estimate the distribution of the measure of interest under the null hypothesis, a large number of permutations is needed (thousands), while the current work used only 10. As the authors only used the mean across permutations rather than the distribution when calculating their statistics, the number of permutations is perhaps less problematic, but this is a very non-standard way to use permutations.</p><p>In my understanding, the correct permutation test to generate accurate P values in a regression analysis of neuronal activity is as follows: 1. Calculate the measure of interest on the real data. This could be a β weight for a particular neuron, or a summary measure across the population (e.g. average β squared or coefficient of partial determination across neurons). Summary measures across the population will have much more statistical power once you have more than a few neurons. 2. Generate an ensemble of e.g. 5000 permutated datasets. To make each permuted dataset, circularly permute the neuronal data relative to the behavioral data by a random number of trials between 1 and the number of trials in the session. If multiple sessions contribute to your measure of interest, draw the circular shift separately for each session for each permuted dataset. 3. Calculate the measure of interest for each permuted datasets. The distribution of the measure across permuted datasets is an estimate of it's distribution under the null hypothesis that there is no relationship between behavior and neural activity. Calculate a P value by comparing the value of the measure for the real data with its distribution across the permutations. For a two tailed test the P value is min(X, 1-X) where X if the fraction of permutations for which the measure on true data is greater than that on permuted data. By construction, the P values generated by this method for circularly permuted data are uniformly distributed between 0 and 1.</p><p><italic>Reviewer #3:</italic></p><p>This article is improved but many of the core problems we identified in the original are unchanged, and overall I still feel it has promise but is not yet in publishable shape.</p><p>On the two sets of results separately:</p><p>Serial autocorrelation: The rhetoric is much more precise and improved, and I appreciate the move toward GLS and toward de-emphasizing the problematic 'null' simulations (which, though not improved are probably sufficient for the specific use to which they are now put).</p><p>However, I just think the methods presented here still haven't convincingly solved the problem at hand. The bottom line (from what I can see) is they still don't have a test for significance that actually produces correct p values, as shown by all the p/p plots. In particular, GLS with the chosen AR structure also produces inflated false positives, when run on the shifted null control data. (That said, it is hard to rule out that the problem might be due at least in part to the shifted null being overconservative for the same reason session-level permutations are, i.e. the block structure of conditions.)</p><p>The article thus continues to resort to the two-stage procedure of testing significance, criticized previously: using a demonstrably flawed method, then testing whether the proportion of significant neurons exceeds a measure of the proportion expected due to inflation. This is arguably valid, for the narrow job of rejecting the claim that previous results, in the aggregate, are due to p inflation, but it simply isn't a viable procedure going forward for conducting inference neuron by neuron. Just for instance, the very next section of the paper (on policy and value coding) contains extensive discussion counting and comparing the number of nominally significant neurons of different types, but none of these numbers can be taken seriously given what immediately preceded.</p><p>My view is that for the work to be useful to the field, it needs to present a method that gives a demonstrably trustworthy p value at the single neuron level. If GLS doesn't work, and they must resort to augmenting it with a further nonparametric stage, then they may as well just instead go ahead and define a proper nonparametric test, e.g. a full permutation test based on the circular shift. In this case the GLS is moot and correlation would work fine as the test statistic. The main question to my eye here is the validity / independence / sensitivity of the circular shifts as the unit of permutation. Given very long autocorrelation, many such shifts will be non-independent from each other, and furthermore I don't completely understand why same the issue argued to plague the across-session permutation control (i.e. nonindependence due to structure in the trial blocking) doesn't also apply here. Thus, I think to go this way would ideally require more work to validate the control, but I fear this means they are back with the problem of designing convincing null simulations.</p><p>Value vs policy coding: I continue to think this chunk of the paper is mostly built on confusing conceptual foundations, admittedly mostly inherited from the earlier paper. First, I still think the whole motivating framing that activity related to action value isn't bona fide action value activity if it is negatively modulated by the alternative value (&quot;policy&quot;) is just plain wrong, for many reasons I rehearsed before: related for instance to normalization and efficient coding. Second, this linear approach neglects chosen value altogether (but is confounded by it), which other results in the paper suggest are a key factor.</p><p>Finally, apart from the fact that the p values themselves are dubious (see above) the exercise of counting neurons that are significant or nonsignificant on different sets of correlated tests does not clearly lend itself to any formal conclusion or even informal interpretation. What results would be expected under different hypotheses? What are the hypotheses? The idea of &quot;mixed selectivity&quot; is neither defined nor tested, and I don't see that this would be a viable way to do it: ultimately, interpreting the Venn diagram of positive and negative results on correlated tests flirts with a combination of the fallacies of frequentist reasoning including affirming nulls and double dipping. The analysis of angles is much less problematic in this respect (since it represents a single test with a well-defined null hypothesis rather than a family of non-independent tests), though again it is a bit less than one might hope for since it is at the population, rather than neuron level. I'd still vote to axe this section entirely, or narrow it way down to the angle thing if necessary.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53045.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The methods used to investigate and account for serial autocorrelations are coarse, ad-hoc, and little is presented to verify that they do what they say they do. This raises 2 issues. The first relates to how robust the findings will be in the face of future related criticisms. The second relates to setting methodological standards for how the field should deal with serial correlations in the future.</p></disp-quote><p>Two major concerns raised by the reviewers were (1) the lack of systematic approach to evaluate statistical significance of value-signals in neural activity in the presence of residual autocorrelation, and (2) conflation of signals related to the contrast of action values and policy. To address these concerns, we have performed an almost completely new series of analyses on the neural data and re-wrote much of the entire manuscript. In particular, based on the suggestions from the reviewers and advice from an expert on applied statistics (a new co-author in the manuscript), we have adopted the generalized least square (GLS) regression model to evaluate the nature of residual autocorrelation in neural activity, and dramatically simplified the permutation tests to evaluate the statistical significance of value signals in neural activity. We also added the results obtained from the monkey dorsolateral prefrontal cortex. These and other major changes in the revised manuscript are summarized below.</p><disp-quote content-type="editor-comment"><p>We will deal with the second issue first, as dealing with it properly may make the first problem redundant.</p><p>Strategy for accounting for autocorrelations:</p><p>The issue is that the authors propose 3 techniques for dealing with serial autocorrelations in the noise (simulating neurons with serial correlations, permuting &quot;within blocks&quot; only, and including a few trials back as co-regressors). None of them are elegant or general techniques, and in all cases the behaviour of these techniques in the null case is poorly characterised (see below for more on this point). This is particularly surprising because this is an old problem in the statistics literature, and there are off-the-shelf techniques for addressing the problem elegantly and rigorously. We acknowledge the requirement to adhere to the spirit of the EDL paper, but we think it would be extremely advantageous to move the argument forward using general well-validated techniques.</p><p>Options include</p><p>1. Estimating a whitening kernel for the residuals and refitting the model using this kernel (technically nontrivial). For an example from fMRI, see Woolrich et al. Neuroimage 2001.</p><p>2. Fitting autoregressive noise models using standard off-the-shelf software (eg gls in r – see review comment below ).</p><p>3. Permutation tests after transformations that render the data exchangeable.</p><p>Eg fourier transform the data and permute only the phases then reconstruct the data.</p><p>Eg Wavelet transform the data and permute wavelet coefficients then reconstruct the data.</p><p>Here are the reviewer comments that led to this discussion, which contain other related comments that maybe useful. A similar discussion was had at triage. We note that, whilst the reviewer suggests an auto-regressive model, which would be fine, it would also be fine to use one of the other techniques above, which may be more appropriate if the residuals are not well described by a limited AR model such as AR(1). The permutation strategies described above are trivial to implement and effective.</p><p>&quot;I think the paper could do a better job technically unpacking the issues with temporal correlations, which in my view weren't diagnosed as precisely as they could have been in the original article either. The senior author knows more econometrics than I do, but as I understand it, all of the estimation issues in OLS here are due to the assumption of uncorrelated errors. Autocorrelation in the dependent variables, and the explanatory variables, is perfectly OK (indeed, one would result from the other) so long as they cancel each other out when the model is fit, in which case the residuals would be uncorrelated. Changing the autocorrelation in either y or X affects this only indirectly, since both appear in the residual. I think the article's focus on the autocorrelation in the explanatory variables and spike rates – both in rhetoric and analysis and results – is a piece of the puzzle but tends to obscure this deeper point. It would be helpful to also focus on visualizing and decorrelating the residuals. For the same reason, the lack of regressor autocorrelation in the monkey experiment is less of a solution than it is made out to be, I think.&quot;</p><p>&quot;I find the estimation strategy of Figure 3 and sporadically onward a bit frustrating and less convincing (or at least more roundabout) than it could be. I'm sympathetic to the overall conclusion, but the overall strategy comes off as piling up multiple fixes, even though they are shown not to work adequately on simulated data. To compensate for this, the simulations are used to define a new, inflated false positive rate that is, finally, in a followup test, compared to the obtained rate of nominal positives. Frankly: yuck. How about figuring out why the fixes don't work, and finding a test that does work? For the nonparametric fix (bootstrap) the issue is presumably within session correlations, as discussed later; but for the lagged AR terms, I assume the problem is there aren't enough of them to handle longer-timescale correlations. But this is itself kind of a hack; a more orthodox parametric approach would be to use a nonlinear, generalized least squares (eg gls() in R) to estimate a full AR(1) model or whatever other error covariance form is supported by the actual data. (Note that even an AR(1) process predicts correlations at arbitrary lags so adding individual lag terms is not sufficient.)&quot;</p></disp-quote><p>We agree with the editor and reviewers that we should have dealt with the statistical issues resulting from serial correlation in neural data more systematically. We are also grateful to the reviewers for suggesting 3 specific approaches we could take, including the use of whitening kernel, generalized least square method, and appropriate transformation (e.g., wavelet). As the reviewers pointed out, some of these methods have been well established in the field of fMRI and are broadly applied. Unfortunately, it is difficult to adapt these methods to the analysis of spike data in our current study, because unlike the fMRI data (which is sampled periodically) spike counts in successive trials during our experiment are separated by highly variable inter-trial interval. Also, the number of trials in our experiments is much smaller than the number of data points analyzed in typical fMRI experiments. Accordingly, we have decided to combine the generalized least square (GLS) regression analysis models with a circular permutation test to better assess the nature of residual autocorrelation and to develop the most robust method to circumvent the difficulty in evaluating the statistical significance of value-related signals in neural activity. We agree with the sentiment expressed by the reviewers that this would be most beneficial for future studies facing the same statistical issues. For comparison with the previous study (i.e., Elber-Dorozko and Loewenstein, 2018, EDL), however, we also kept the results obtained with the OLS method and the permutation test proposed by EDL but moved them to Figure 3 supplements (Figure 3-supplement 1 and Figure 3-supplement 2). We also show residual autocorrelation in Figure 2 of the revised manuscript as suggested by the reviewers.</p><disp-quote content-type="editor-comment"><p>Characterisation of performance in the null case.</p><p>If the authors change their strategy as recommended above, this section of the review may be rendered redundant. However, given the current approach the review team did not think that the paper did a good job in presenting diagnostics that adequately evaluate the performance of their strategies.</p><p>Minimally, given how much work the random walk neuron model does, we think that the authors should try harder to evaluate the performance with a model that looks more like the data. The model was setup only to match the neuronal autocorrelations at lag 1 trial and likely has a very different autocorrelation structure from real neurons at lags greater than 1. The autocorrelation structure of the control 'random' neuron model should be matched to that of the neuronal data. This may need a generative model that is more expressive than AR(1). Without this, the authors are susceptible to future criticism that simply shows that the authors techniques do not do well in the face of realistic data.</p><p>We also think that instead of simply reporting the number of false positives at p&lt;0.05 threshold, the authors should construct the p-p plot (Wikipedia), which plots observed false positives in empirical data against the nominal threshold. This will make it useful to future researchers who would like to use the same techniques with different threshold.</p><p>All three reviewers made the same point. I include all 3 here to encourage the authors that it is an important point that will likely be shared by many readers.</p><p>&quot;Generation of random-walk neurons. How is it possible to create the same autocorrelation kernel as the one observed in the neural data (essentially flat – at least for the shown scale of 5 trials) through a random-walk process – for which the correlation should intrinsically decrease over time? The authors mentioned that they have matched autocorrelation at lag 1 only, which may be good enough as an approximation for what the authors intend to do with random-walk neurons, but it is not a tight match and the authors may want to mention this somewhere in the manuscript.&quot;</p><p>&quot;The random walk neuron model does a lot of work as a control against which real neurons are compared. However, the model was setup only to match the neuronal autocorrelations at lag 1 trial and likely has a very different autocorrelation structure from real neurons at lags greater than 1. The autocorrelation structure of the control 'random' neuron model should be matched to that of the neuronal data. &quot;</p><p>&quot;Either way, everything comes down here to the simulated spike trains under the null model, and it would be good to have more argument that these are actually a good simulation for the data. Among other things, I wasn't clear if their timescale is individually fit per brain area or experiment or just roughly chosen; if multiple timescales of correlation are detectable in the actual data, rather than just rectified AR(1) as here; and again if the autocorrelative structure of the residuals is similar between data and simulation. &quot;</p></disp-quote><p>As we mentioned above, we now use the GLS regression to better address the problems resulting from autocorrelation in the residual from the regression model. In particular, we have used the BIC to determine the order of residual autocorrelation in the GLS method. In addition, we have performed extensive analysis to develop high-order random-walk neuron models to reproduce the shape of the autocorrelation function. Unfortunately, despite extensive simulation and data analyses, we failed to develop robust models that could accurately reproduce the main features of autocorrelation in the neural data. There are at least two reasons for this difficulty. First, we came to hypothesize that autocorrelation in neural data occurs in many different time scales, which is a topic we are currently pursuing in another manuscript. Therefore, to reproduce the observed autocorrelation, we were frequently required to include a large number of autoregressive terms, which made our analysis of value signals unnecessarily complicated. Second, there is a potential gap between the “non-linear” random-walk neuron model and the framework of linear models, because the first-order autocorrelation in the latent variable (rate parameter) can potentially be disguised as displaying higher-order autocorrelation when only the outputs (counts) of such models are considered. We think these are important problems for future investigations and we are planning to pursue them, but they are clearly beyond the scope of our current study. Therefore, in the revised manuscript, we now use circular permutation and trial-shifted data as a way to generate the null data and bypass the potential issues mentioned above. We show the results obtained with AR(1) random-walk neurons only in Figure 3-supplement 2 for comparison with the results shown in EDL paper. We also show P-P plots in Figures 3-5 as suggested by the reviewers.</p><disp-quote content-type="editor-comment"><p>Action values vs policy etc.</p><p>There was broad scepticism amongst the reviewers as to whether it was possible to dissociate policies from values, and whether it was really relevant to do so, particularly if policy is (confusingly) used to refer to a difference in Q-values. This is reflected in the comments below. Whilst we acknowledge the authors' ambitions to address the critiques raised in EDL, we encourage great care in the interpretation of this whole section. Again, related points were made by all 3 reviewers, highlighting that this Is likely also to be an issue for many readers.</p><p>&quot;I find the second half of the article, on alternative decision variables, a little bit of a red herring. One thing is that the relationship between a Q value and a policy (as the term is normally used in RL, and was used by Elber-Doroko) is nonlinear. Calling the difference in Q values a &quot;policy&quot; is just not using the term accurately. On the other hand, my view is that this example shows that the whole critique is ill founded, and the only useful question is what is the (linear or nonlinear) relationship between decision variables and brain activity. Neural representations of values are likely to be nonlinear for reasons other than policy (eg, there is plenty of work by Glimcher and others on gain control or divisive normalization) and may also be differential (eg, activity which is related to the relative value, chosen minus unchosen, which is nevertheless in units of value and not normalized/softmaxed etc into a policy). Telling the difference between divisive and subtractive normalization is not really viable, especially in the linear setting; and even so, the same (softmax) algebraic form could describe either policy or (gain controlled) value. There's just not a meaningful categorical distinction to be made. I suppose there might be some way of recasting this section to focus on the distinction between summation vs difference as being representative (in a linear framework) of state values vs. relativized, or normalized, or post-choice policy values. But I think it's giving too much away to frame this as actually distinct variables confounding one another; and also unfair to call a difference a policy.&quot;</p></disp-quote><p>As the reviewers pointed out, the relationship between differential action values and policy is non linear. Nevertheless, in the literature (inclu ding EDL), neurons with significant effects of value difference ( ΔQ) have been frequently referred to as policy coding neurons. We have clarified the text throughout the revised manuscript to avoid unnecessary confusion on this issue.</p><disp-quote content-type="editor-comment"><p>&quot;Correlation with sum(Q) and diff(Q). I don't understand the exact graphical description on Figure 8 and Figure 8. The authors label gray neurons as 'only Q', but many of them are probably not coding anything (non-significant, corresponding to black neurons in Figure 6 of the article by EDL). Also, I expected that the neurons coding selectively for one action value (QL or QR) should be found on Figure 8A for |x| &gt; threshold and |y| ~ 0 and vice versa. However, it is clearly not the case given the labelling of neurons provided by the authors for this graph. Could the authors clarify this and explain the apparent discrepancy with the analyses performed by EDL (Figure 6 from their article). I have a similar concern regarding Figure 8B: pure action-value neurons seem to be located only at the center of the graphs (for |x| ~ 0 and |y| ~ 0), which is where non-selective neurons should be found.&quot;</p><p>&quot;Figures 7 – 9 attempt to dissociate action value coding from coding of policy (difference in action values) and state value (approximated as sum of action values). As these variables are linearly dependent, it is formally impossible to say whether a neuron represents one of them, or a linear combination of the others. Mixed linear selectivity is ubiquitous (e.g. Kobak et al. eLife 2016;5:e10989), so it not that interesting to ask which of this degenerate set of variables is most 'purely' represented by each neuron. This said, having chosen a given non-degenerate pair of these variables to work with, it is interesting to know how representation of one variable correlates with representation of the other across the population, and how this varies across regions. This is shown nicely in figure 7A and the top two panels of 9A, but I felt the remaining panels of figures 7-9 did not add additional value.&quot;</p><p>When the authors assess the extent of chosen value coding (Figure 5, 6B) , they include the individual action values in their regression model, which is important as these variables are correlated. However, when they assess action value coding (Figures 4, 6A) they do not include chosen value in the models. I think the rationale is that the analyses are different trial epochs, pre-choice for 4, 6A, post outcome for 5, 6B. However, chosen value coding is certainly possible before the choice is executed and hence chosen value should be included in the model when assessing action value coding.</p></disp-quote><p>We apologize for the confusion caused by some of the figures in the original manuscript. Some of this was due to the fact that gray circles in the original manuscript represented those neurons coding only Q (Figure 7A) or DV (Figure 9) and empty squares represented those that do not encode any of these value terms, but these two symbols could not be clearly distinguished in the figures of the original manuscript. To avoid this confusion, we replaced gray circles in Figure 6 (Figure 7A and 9 in the original manuscript) with red circles. We also agree that some of the results shown in these original figures are not essential for the main conclusion of our manuscript, so the results other than the scatter plots in original Figure 7A and Figure 9 were either removed or moved to a supplementary figure (Figure 6-supplement 1). Instead, we added a new analysis result (examining distributions of action-value coefficients after doubling or quadrupling their angles to examine how relative strengths of different value signals vary across different brain regions; Figure 6 in the revised manuscript). With respect to the possibility of chosen value signals before action selection, we obtained essentially the same conclusions with and without including chosen value in the regression model assessing action-value signals, which is consistent with our previous finding that chosen-value signals are generally weak before action selection. As suggested, we show the results obtained with the model including chosen value (model 3, Equation 5) in the revised manuscript.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>I have written a summary of our opinion after the discussion directly below here. I have also left the reviews below for emphasis and detail, but please don't feel that you need to address all the points in the reviews. If you can address the central issues in the summary directly below here, we will be happy.</p><p>This revision has raised some complications in the reviewers' minds that have led to a lot of discussion. In brief, we are not happy with the 2-stage approach that does not lead to p-values for individual neurons that can be trusted.</p><p>Whilst we agree that this approach goes some way to rebutting the EDL finding, it is a narrow rebuttal which does not provide a good way forward for scientists faced with similar problems in the future. The combination of GLS modelling that does not accurately deal with the autocorrelations, with non-standard application of circular permutations to demonstrate control performance is, in our view, dangerous, and not useful to the community. We are not keen on publishing such an approach in eLife.</p><p>If the GLS approach does not lead to good corrections for autocorrelations, then we think it incumbent upon you guys to find a non-parametric approach that does.</p></disp-quote><p>We fully agree that a better method to evaluate the statistical significance of value-signals at the level of individual neurons was needed in order to firmly establish the extent to which neurons in different anatomical areas encode value signals throughout the brain. Thus, we very much appreciate the reviewer’s effort to provide us with a set of sample codes and illustrate how more appropriate analyses should be carried out. We have followed these suggestions closely in the revised manuscript. Specifically, we evaluated the extent of false positivity for several different methods by using actual behavioral and simulated neural data while maintaining the same distribution of firing rates and AR(1) coefficients as in the actual neural data. In the revised manuscript, we accordingly report the results from the four procedures that yield chance-level rate of false positivity (~5%) for action-value neurons (Figure 2 of the revised manuscript) when tested using the simulated null data. Two of these methods are based on resampling of behavioral data, and include ‘session permutation’ proposed by the previous paper published in <italic>eLife</italic> by Elber-Dorozko and Loewenstein (2018) and the ‘pseudosession’ method proposed in a recent bioRxiv paper (Harris, 2020). The other two are based on Fourier phase randomization of neural data suggested by the reviewer, namely the conventional Fourier phase randomization (FPR) method and a modified version of the amplitude adjusted Fourier transformation method (Theiler 1992 Physica D). Using these 4 new methods, we still found significant action-value and chosen-value signals in multiple areas of the rat brain (Figure 3 of the revised manuscript). Therefore, we believe that our findings robustly demonstrate the presence of action-value signals in the brain.</p><disp-quote content-type="editor-comment"><p>You can see in the reviews below, that the reviewers also remain concerned about the distinction in the second half of the paper between action values, chosen values, policies etc, which interacts heavily with questions about the linearity of neuronal responses. We remain concerned that there is a danger this will confuse more than clarify the issue for the community. However, we realise that there is a similar section in the EDL paper, and that you guys need to address this. This may be TB's fault (along with the original reviewers) for not flagging this in the original paper. We would appreciate a clear statement in this section in the paper that states that this section is a narrow rebuttal of the EDL paper, and discusses the difficulties in differentiating policy from action values etc. (see reviews below).</p></disp-quote><p>We also agree with the reviewer that approaches based on a simple linear regression model cannot adequately be used to classify individual neurons coding different types of value signals, such as action values, policy, and chosen values. Therefore, as suggested by the reviewers and editor, we shortened this section significantly, but kept it as a rebuttal to the previous <italic>eLife</italic> paper published by Elber-Dorozko and Loewenstein. In particular, we added the following text in the revised manuscript: “In reinforcement learning theory, action values are monotonically related to the probability of choosing the corresponding actions, referred to as policy, making it hard to distinguish the neural activity related to either of these quantities. In addition, the activity of individual neurons is likely to encode multiple variables simultaneously (Rigotti et al., 2013). Despite these difficulties, it has been argued that neural signals related to action value might actually represent policy exclusively (Elber-Dorozko and Loewenstein, 2018). To address this issue quantitatively, using the difference in action values (∆Q) and their sum (ΣQ) as proxies for policy and state value, respectively, we tested how signals for action values, policy and state value are related in a population of neurons in different brain structures.”</p></body></sub-article></article>