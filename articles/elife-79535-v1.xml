<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79535</article-id><article-id pub-id-type="doi">10.7554/eLife.79535</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Ultrafast simulation of large-scale neocortical microcircuitry with biophysically realistic neurons</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-260877"><name><surname>Oláh</surname><given-names>Viktor J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2069-7525</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-115772"><name><surname>Pedersen</surname><given-names>Nigel P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8494-0635</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-259854"><name><surname>Rowan</surname><given-names>Matthew JM</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0955-0706</contrib-id><email>mjrowan@emory.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Department of Cell Biology, Emory University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Department of Neurology, Emory University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bhalla</surname><given-names>Upinder Singh</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ht1xw27</institution-id><institution>Tata Institute of Fundamental Research</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>07</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e79535</elocation-id><history><date date-type="received" iso-8601-date="2022-04-16"><day>16</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-23"><day>23</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-02-23"><day>23</day><month>02</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.02.22.432356"/></event></pub-history><permissions><copyright-statement>© 2022, Oláh et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Oláh et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79535-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-79535-figures-v1.pdf"/><abstract><p>Understanding the activity of the mammalian brain requires an integrative knowledge of circuits at distinct scales, ranging from ion channel gating to circuit connectomics. Computational models are regularly employed to understand how multiple parameters contribute synergistically to circuit behavior. However, traditional models of anatomically and biophysically realistic neurons are computationally demanding, especially when scaled to model local circuits. To overcome this limitation, we trained several artificial neural network (ANN) architectures to model the activity of realistic multicompartmental cortical neurons. We identified an ANN architecture that accurately predicted subthreshold activity and action potential firing. The ANN could correctly generalize to previously unobserved synaptic input, including in models containing nonlinear dendritic properties. When scaled, processing times were orders of magnitude faster compared with traditional approaches, allowing for rapid parameter-space mapping in a circuit model of Rett syndrome. Thus, we present a novel ANN approach allowing for rapid, detailed network experiments using inexpensive and commonly available computational resources.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational model</kwd><kwd>artificial neural net</kwd><kwd>NMDA</kwd><kwd>cortex</kwd><kwd>deep learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R56-AG072473</award-id><principal-award-recipient><name><surname>Rowan</surname><given-names>Matthew JM</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006528</institution-id><institution>Emory Alzheimer's Disease Research Center</institution></institution-wrap></funding-source><award-id>00100569</award-id><principal-award-recipient><name><surname>Rowan</surname><given-names>Matthew JM</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>CURE Epilepsy and the NIH</institution></institution-wrap></funding-source><award-id>K08NS105929</award-id><principal-award-recipient><name><surname>Pedersen</surname><given-names>Nigel P</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RF1-AG079269</award-id><principal-award-recipient><name><surname>Rowan</surname><given-names>Matthew JM</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Emory/Georgia Tech I3 Computational and Data analysis to Advance Single Cell Biology Research Award</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Rowan</surname><given-names>Matthew JM</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Artificial neural networks can faithfully recapitulate realistic neuronal behavior with dramatically accelerated simulation runtimes, empowering accessible large-scale realistic network simulations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding the behavior of complex neural circuits like the human brain is one of the fundamental challenges of this century. Predicting mammalian circuit behavior is difficult due to several underlying mechanisms at distinct organizational levels, ranging from molecular-level interactions to large-scale connectomics. Computational modeling has become a cornerstone technique for deriving and testing new hypotheses about brain organization and function (<xref ref-type="bibr" rid="bib157">Sejnowski et al., 1988</xref>; <xref ref-type="bibr" rid="bib184">Wolpert and Ghahramani, 2000</xref>; <xref ref-type="bibr" rid="bib46">Dayan and Abbott, 2001</xref>; <xref ref-type="bibr" rid="bib101">Kriegeskorte and Douglas, 2018</xref>). In little more than 60 years, our mechanistic understanding of neural function has evolved from describing action potential (AP)-related ion channel gating (<xref ref-type="bibr" rid="bib85">Hodgkin and Huxley, 1952</xref>) to constructing models that can simulate the activity of whole-brain regions (<xref ref-type="bibr" rid="bib175">Traub et al., 2005</xref>; <xref ref-type="bibr" rid="bib187">Yu et al., 2013</xref>; <xref ref-type="bibr" rid="bib135">Neymotin et al., 2016b</xref>; <xref ref-type="bibr" rid="bib36">Chavlis et al., 2017</xref>; <xref ref-type="bibr" rid="bib177">Turi et al., 2019</xref>). Although tremendous advancements have been made in the development of computational resources, the lack of available or affordable hardware for neural simulations currently represents a significant barrier to entry for most neuroscientists and renders many questions intractable. This is particularly well illustrated by large-scale neural circuit simulations. In contrast to detailed single-cell models, which have been a regular occurrence in publications since the 1990s (<xref ref-type="bibr" rid="bib50">De Schutter and Bower, 1994</xref>; <xref ref-type="bibr" rid="bib110">Mainen et al., 1995</xref>; <xref ref-type="bibr" rid="bib124">Migliore et al., 1995</xref>; <xref ref-type="bibr" rid="bib111">Mainen and Sejnowski, 1996</xref>; <xref ref-type="bibr" rid="bib52">Destexhe et al., 1998</xref>; <xref ref-type="bibr" rid="bib167">Stuart and Spruston, 1998</xref>; <xref ref-type="bibr" rid="bib6">Aradi and Holmes, 1999</xref>; <xref ref-type="bibr" rid="bib125">Migliore et al., 1999</xref>), parallel simulation of thousands, or even hundreds of thousands of detailed neurons have only become a possibility with the advent of supercomputers (<xref ref-type="bibr" rid="bib115">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Arkhipov et al., 2018</xref>; <xref ref-type="bibr" rid="bib90">Joglekar et al., 2018</xref>; <xref ref-type="bibr" rid="bib154">Schmidt et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Antolík et al., 2019</xref>; <xref ref-type="bibr" rid="bib156">Schwalger and Chizhov, 2019</xref>; <xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>). As these resources are still not widely accessible, several attempts have been made to mitigate the immense computational load of large-scale neural simulations by judicious simplification (<xref ref-type="bibr" rid="bib183">Wang and Buzsáki, 1996</xref>; <xref ref-type="bibr" rid="bib15">Bartos et al., 2002</xref>; <xref ref-type="bibr" rid="bib150">Santhakumar et al., 2005</xref>; <xref ref-type="bibr" rid="bib61">Eppler, 2008</xref>, <xref ref-type="bibr" rid="bib42">Cutsuridis et al., 2010</xref>; <xref ref-type="bibr" rid="bib137">Nowotny et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>; <xref ref-type="bibr" rid="bib186">Yavuz et al., 2016</xref>; <xref ref-type="bibr" rid="bib173">Teeter et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Amsalem et al., 2020</xref>; <xref ref-type="bibr" rid="bib97">Knight et al., 2021</xref>; <xref ref-type="bibr" rid="bib98">Knight and Nowotny, 2021</xref>, <xref ref-type="bibr" rid="bib185">Wybo et al., 2021</xref>). However, simplification inevitably results in feature or information loss, such as sacrificing multicompartmental information for simulation speed (<xref ref-type="bibr" rid="bib183">Wang and Buzsáki, 1996</xref>; <xref ref-type="bibr" rid="bib15">Bartos et al., 2002</xref>; <xref ref-type="bibr" rid="bib150">Santhakumar et al., 2005</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>). Thus, there is a critical need for new approaches to enable efficient large-scale neural circuit simulations on widely available computational resources without surrendering biologically relevant information.</p><p>To counteract the increasing computational burden of ever-growing datasets on more traditional models, many fields have recently adopted various machine learning algorithms (<xref ref-type="bibr" rid="bib159">Sharma et al., 2011</xref>; <xref ref-type="bibr" rid="bib128">Montavon et al., 2013</xref>; <xref ref-type="bibr" rid="bib121">Meredig et al., 2014</xref>; <xref ref-type="bibr" rid="bib122">Merembayev et al., 2018</xref>; <xref ref-type="bibr" rid="bib155">Schütt et al., 2020</xref>). Specifically, artificial neural networks (ANNs) are superior to conventional model systems both in terms of speed and accuracy when dealing with complex systems such as those governing global financial markets or weather patterns (<xref ref-type="bibr" rid="bib86">Holmstrom, 2016</xref>; <xref ref-type="bibr" rid="bib65">Ghoddusi et al., 2019</xref>). Due to their accelerated processing speed, ANNs are ideal candidates for modeling large-scale biological systems. The idea that individual neural cells could be represented by ANNs was proposed almost two decades ago (<xref ref-type="bibr" rid="bib142">Poirazi et al., 2003</xref>); however, current ANN solutions are still unfit to replace traditional modeling systems as they cannot generate gradational neuronal dynamics needed for network simulations. Therefore, we aimed to develop an ANN that can (1) accurately replicate various features of biophysically detailed neuron models, (2) efficiently generalize for previously unobserved input conditions, and (3) significantly accelerate large-scale network simulations.</p><p>Here, we investigated the ability of several ANN architectures to represent membrane potential dynamics, in both simplified point neurons and multicompartment neurons. Among the selected ANNs, we found that a convolutional recurrent architecture can accurately simulate both subthreshold and suprathreshold voltage dynamics. Furthermore, this ANN could generalize to a wide range of input conditions and reproduce neuronal features following different input patterns beyond membrane potential responses, such as ionic current waveforms. Next, we demonstrated that this ANN could also accurately predict multicompartmental information by fitting this architecture to a biophysically detailed layer 5 (L5) pyramidal cell (PC; <xref ref-type="bibr" rid="bib74">Hallermann et al., 2012</xref>) model. Importantly, we found that ANN representations could drastically accelerate large network simulations, as demonstrated by network parameter space mapping of a cortical L5 recurrent microcircuit model of Rett syndrome, a neurodegenerative disorder associated with cortical dysfunction and seizures (<xref ref-type="bibr" rid="bib73">Hagberg et al., 1985</xref>; <xref ref-type="bibr" rid="bib10">Armstrong, 2005</xref>, <xref ref-type="bibr" rid="bib66">Glaze, 2005</xref>; <xref ref-type="bibr" rid="bib33">Chahrour and Zoghbi, 2007</xref>). Thus, we provide a detailed description of an ANN architecture suitable for large-scale simulations of anatomically and biophysically complex neurons, applicable to human disease modeling. Most importantly, our ANN simulations are accelerated to the point where detailed network experiments can now be carried out using inexpensive, readily available computational resources.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To create a deep learning platform capable of accurately representing the full dynamic membrane potential range of neuronal cells, we focused on model systems proven to be suitable for multivariate time-series forecasting (MTSF). To compare the ability of different ANNs to reproduce the activity of an excitable cell, we designed five distinct architectures (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The first two models were a simple linear model with one hidden layer (linear model, <xref ref-type="fig" rid="fig1">Figure 1A</xref>, blue) and a similar model equipped with nonlinear processing (nonlinear model, <xref ref-type="fig" rid="fig1">Figure 1A</xref>, cyan), as even relatively simple model architectures can explain the majority of subthreshold membrane potential variance (<xref ref-type="bibr" rid="bib178">Ujfalussy et al., 2018</xref>). The third and fourth models consist of recently constructed time-series forecasting architectures, including a recurrent ANN (convolutional neural network-long short-term memory [CNN-LSTM], <xref ref-type="fig" rid="fig1">Figure 1A</xref>, magenta) consisting of convolutional layers (<xref ref-type="bibr" rid="bib39">Collobert and Weston, 2008</xref>), long short-term memory (LSTM; <xref ref-type="bibr" rid="bib84">Hochreiter and Schmidhuber, 1997</xref>; <xref ref-type="bibr" rid="bib56">Donahue et al., 2015</xref>) layers, and fully connected layers, termed the CNN-LSTM network (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, <xref ref-type="bibr" rid="bib160">Shi, 2015</xref>), and a more recently developed architecture relying on dilated temporal convolutions (convolutional net, <xref ref-type="fig" rid="fig1">Figure 1A</xref>, orange) (based on the WaveNet architecture; <xref ref-type="bibr" rid="bib141">Oord, 2016</xref>; <xref ref-type="bibr" rid="bib19">Beniaguev et al., 2021</xref>), which is superior to the CNN-LSTM in several MTSF tasks. The CNN-LSTM has the distinct advantage of having almost two orders of magnitude more adjustable parameters compared to the aforementioned ANNs. Finally, we selected a fifth architecture (deep neural net, <xref ref-type="fig" rid="fig1">Figure 1A</xref>, green) with a comparable number of free parameters to the CNN-LSTM, composed of 10 hidden layers, which operates solely on linear and nonlinear transformations. Before moving to neural cell data, each of the five selected architectures were evaluated using a well-curated weather time-series dataset (see ‘Methods’). Each model performed similarly (0.070/0.069, 0.059/0.06, 0.089/0.094, 0.07/0.069, 0.092/0.095, mean absolute error on the validation/testing datasets for linear, nonlinear, convolutional net and CNN-LSTM, deep neural net architectures, respectively), demonstrating their suitability for MTSF problems.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Single-compartmental neuronal simulations using artificial neural networks (ANNs).</title><p>(<bold>A</bold>) Representative diagrams of the tested architectures, outlining the ordering of the specific functional blocks of the ANNs. (<bold>B</bold>) Continuous representative trace of a point-by-point fit of passive membrane potential. (<bold>C</bold>) Point-by-point fit plotted against ground truth data (n = 45,000). (<bold>D</bold>) Mean squared error of ANN fits corresponds to the entire training dataset (n = 2.64 * 10<sup>6</sup> datapoints). Single quantal inputs arrive stochastically with a fixed quantal size: 2.5 nS for excitatory, 8 nS for inhibitory inputs, sampling is 1 kHz. Red and green bars below membrane potential traces denote the arrival of inhibitory and excitatory events, respectively. (<bold>E</bold>) Representative trace of a continuous passive membrane potential prediction (left) created by relying on past model predictions. Explained variance (right) was calculated from 500-ms-long continuous predictions (n = 50). (<bold>F</bold>) Representative active membrane potential prediction by ANNs. (<bold>G</bold>) Explained variance (box chart) and Pearson’s r (circles) of model predictions and ground truth data for the five ANNs from 50 continuous predictions, 500 ms long each. (<bold>H</bold>) Spike timing of the convolutional neural network-long short-term memory (CNN-LSTM) model calculated from the same dataset as panel (<bold>G</bold>). Color coding is the same as in panel (<bold>A</bold>). (<bold>I</bold>) Representative continuous, 25-s-long simulation of subthreshold and spiking activity. (<bold>J</bold>) Explained variance as a function of time during the 25-s-long simulation depicted in panel (<bold>I</bold>). Red line and r-value correspond to the best linear fit. (<bold>K</bold>) Difference between voltage traces produced by NEURON and ANN simulations. Red line and r-value correspond to the best linear fit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Convolutional neural network-long short-term memory (CNN-LSTM) architecture for time-series forecasting.</title><p>The input of the artificial neural network (ANN) consisted of a membrane potential vector (V<sub>m</sub>) and the weights and onsets of synaptic inputs (a representative inhibitory synapse, inh in red; and an excitatory synapse, exc in green). The first layer of the ANN (Conv1D) creates a temporally aligned convolved representation (colored bars) of the input by sliding a convolutional kernel (gray box) along the input. The second functional block (LSTM layers) processes the output of the convolutional layers through recurrent connections to weigh information temporally. The last functional block consisting of fully connected layers provides additional nonlinear information processing power. The output of the network in this case is the first subsequent V<sub>m</sub> value (t<sub>n+1</sub>). The number of layers belonging to specific functional blocks of the CNN-LSTM architecture may vary. Red and green bars below membrane potential traces denote the arrival of inhibitory and excitatory events, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig1-figsupp1-v1.tif"/></fig></fig-group><sec id="s2-1"><title>Prediction of point neuron membrane potential dynamics by ANNs</title><p>To test the ability of the five ANNs to represent input–output transformations of a neural cell, we next fitted these architectures with data from passive responses of a single-compartmental point-neuron model (NEURON simulation environment; <xref ref-type="bibr" rid="bib81">Hines and Carnevale, 1997</xref>) using the standard backpropagation learning algorithm for ANNs (<xref ref-type="bibr" rid="bib147">Rumelhart et al., 1986</xref>). Each model was tasked with predicting a single-membrane potential value based on 64 ms (a time window that yielded the best results both in terms of speed and accuracy) of preceding membrane potentials and synaptic inputs (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). ANN fitting and query were run on a single-core central processing unit (CPU). We found that both the linear and nonlinear models predicted subsequent membrane potential values with low error rates (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) with similar behavior in both the CNN-LSTM and convolutional architectures (2.16 * 10<sup>–4</sup> ± 1.18 * 10<sup>–3</sup>, 2.07 * 10<sup>–4</sup> ± 1.11 * 10<sup>–3</sup>, 1.43 * 10<sup>–4</sup> ± 9.31 * 10<sup>–4</sup>, 1.29 * 10<sup>–4</sup> ± 9.42 * 10<sup>–4</sup> mean error for linear, nonlinear, CNN-LSTM, and convolutional models, respectively). However, the deep neural network performed considerably worse than all other tested models (3.94 * 10<sup>–4</sup> ± 1.56 * 10<sup>–3</sup> mean error), potentially due to the nonlinear correspondence of its predicted values to the ground truth data (<xref ref-type="fig" rid="fig1">Figure 1C and D</xref>).</p><p>Next, we tested ANNs in simulation conditions similar to the traditional models. To this end, we initialized ANNs with ground truth data followed by a continuous query period in which forecasted membrane potential values were fed back to the ANNs to observe continuous unconstrained predictions. As expected from the fit error rates of single-membrane potential forecasting (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), continuous predictions of the linear, convolutional, and CNN-LSTM models could explain the ground truth signal variance at high accuracy. At the same time, the deep neural net performed slightly worse (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, 97.1 ± 1.2, 99.3 ± 1.4, 97.2 ± 2.2, 84.0 ± 3.2 variance explained for linear, convolutional, CNN-LSTM, and deep neural net architectures, respectively, n = 50). Surprisingly, the nonlinear model produced the worst prediction for passive membrane potential traces (0.82 ± 0.03 variance explained, n = 50) despite performing the best on the benchmark dataset. Together, these results indicate that even simple linear ANNs can capture subthreshold membrane potential behavior accurately (<xref ref-type="bibr" rid="bib178">Ujfalussy et al., 2018</xref>).</p><p>Next, we tested how these models perform on the full dynamic range of neural cells, which due to AP firing (which can also be viewed as highly relevant outlier data points) constitutes a non-normally distributed and thus demanding dataset for ANNs. Interestingly, we found that only the CNN-LSTM architecture could precisely reproduce both subthreshold membrane potential dynamics and spiking activity, while all other tested ANNs converged to the mean of the training dataset (<xref ref-type="fig" rid="fig1">Figure 1F and G</xref>, 4.4 ± 7.2%, 4.1 ± 6.9%, 0.5 ± 3.9%, 78.9 ± 6.7%, 4.4 ± 2.8% variance explained for linear, nonlinear, convolutional net and CNN-LSTM, deep neural net architectures, respectively, n = 50). We found that although the CNN-LSTM model explained substantially less variance for the active membrane potential traces (<xref ref-type="fig" rid="fig1">Figure 1G</xref>) than for subthreshold voltages alone (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), the predictions showed high linear correlation with the ground truth signals (Pearson’s <italic>r</italic> = 0.76793 ± 0.10003, n = 50). For the four remaining ANN architectures, it is unlikely that convergence to the mean is caused by settling in local minima on the fitting error surface as ANNs have a large number of free parameters (2.07 * 10<sup>4</sup>, 2.07 * 10<sup>4</sup>, 2.47 * 10<sup>6</sup>, 3.64 * 10<sup>5</sup>, 1.95 * 10<sup>6</sup> free parameters for linear, nonlinear, deep, convolutional ANNs, and CNN-LSTM, respectively). Therefore, the chance of having a zero derivative for each parameter at the same point is extremely low (<xref ref-type="bibr" rid="bib91">Kawaguchi, 2016</xref>), suggesting that erroneous fitting is the consequence of the limitations of these ANN architectures. Consequently, of the tested ANN architectures, the CNN-LSTM is the only model that could depict the full dynamic range of a biophysical neural model.</p><p>Closer inspection of the timing of the predicted APs revealed that the CNN-LSTM models correctly learned thresholding as the occurrence of the APs matched the timing of the testing dataset (<xref ref-type="fig" rid="fig1">Figure 1H</xref>; 83.94 ± 16.89% precision and 90.94 ± 12.13% recall, 0.24 ± 0.79 ms temporal shift for true-positive spikes compared to ground truth, n = 283), thus CNN-LSTM predictions yielded voltage traces with good initial agreement to NEURON signals. To test the long-term stability of these predictions, we next performed a longer (25 s) ANN simulation (<xref ref-type="fig" rid="fig1">Figure 1I</xref>). During this extended simulation, we did not observe significant deviation from the ground truth signal in terms of explained variance (<xref ref-type="fig" rid="fig1">Figure 1J</xref>) or absolute difference (<xref ref-type="fig" rid="fig1">Figure 1K</xref>) and these metrics even improved slightly. Taken together, we developed an ANN architecture that is ideally suited for predicting both subthreshold membrane potential fluctuations and the precise timing of APs on a millisecond timescale.</p></sec><sec id="s2-2"><title>Generalization of the CNN-LSTM architecture</title><p>To test the applicability of the CNN-LSTM for predicting physiological cellular behavior, we assessed the generalization capability of the architecture built for active behavior prediction (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Generalization is the ability of an ANN to respond accurately to novel data (<xref ref-type="bibr" rid="bib76">Hassoun, 1995</xref>; <xref ref-type="bibr" rid="bib71">Graupe, 2013</xref>). According to our hypothesis, if the CNN-LSTM correctly learned the mechanistic operations of a neural cell, then the architecture should behave appropriately when tasked with responding to novel quantal amplitudes and input patterns.</p><p>We first challenged the CNN-LSTM by administering excitatory inputs with variable quantal sizes (0.1–3.5 nS, 0.1 nS increment). Similar to the control NEURON model, the CNN-LSTM responded linearly in subthreshold voltage regimes (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, Pearson’s <italic>r</italic> = 0.99, n = 35) and elicited an AP after reaching threshold. Independent evaluation of the NEURON model control revealed a surprisingly similar I/V relationship for the same quantal inputs (intercept, –0.003 ± 8.53 and –0.003 ± 0.001; slope for subthreshold linear I/V, 22.2 ± 0.41 and 23.31 ± 0.62; CNN-LSTM and NEURON model, respectively) and similar AP threshold (–58.03 mV and –56.64 mV for CNN-LSTM and NEURON model, respectively). Next, we tested temporal summation of excitatory inputs (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We found that the independently simulated NEURON model displayed similar temporal summation patterns to the CNN-LSTM for both sub- and suprathreshold events (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Finally, we combined the previous two tests and delivered unique temporal patterns of synaptic inputs with variable synaptic conductances randomly chosen from a normal distribution (mean: 2.5 nS; variance: 0.001 nS, <xref ref-type="fig" rid="fig2">Figure 2C</xref>). Again, the predictions of the CNN-LSTM architecture closely matched traces obtained from the NEURON model (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, Pearson’s <italic>r</italic> = 0.81, n = 5000 ms) and the timing of the majority of the APs agreed with the ground truth data (91.02 ± 16.03% recall and 69.38 ± 22.43% precision, n = 50).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Ideal generalization of the convolutional neural network-long short-term memory (CNN-LSTM).</title><p>(<bold>A</bold>) CNN-LSTM models predict similar subthreshold event amplitudes and action potential threshold (break in y-axis) for increasing input weight, compared to NEURON models. (<bold>B</bold>) CNN-LSTM models correctly represent temporal summation of synaptic events. Representative traces for different inter-event intervals (range: 2–10 ms, 1 ms increment) on the left, comparison of individual events in a stimulus train, relative to the amplitude of unitary events on the right. (<bold>C</bold>) Single-simulated active membrane potential trace in CNN-LSTM (purple) and NEURON (black) with variable synaptic input weights (left). The inset shows the distribution of synaptic weights used for testing generalization, with the original trained synaptic weight in purple. CNN-LSTM predicted membrane potential values plotted against NEURON model ground truth (right). Plotted values correspond to continuously predicted CNN-LSTM traces. (<bold>D</bold>) CNN-LSTM model predictions are accurate in various synaptic environments. Firing frequency was quantified upon two different excitation–inhibition ratios (2:1 – representative top trace on the left and bright magenta circles on the right, 1:2 – representative bottom trace on the left and dark magenta circles on the right). (<bold>E</bold>) Subthreshold effects of potassium conductance biophysical alterations are correctly depicted by the CNN-LSTM. Voltage dependence of the delayed rectifier conductances is illustrated on the left and their effect on subthreshold membrane potential is shown on the right (control conditions are shown in blue, 10 mV left-shifted delayed rectifier conditions in navy blue and 10 mV right-shifted conditions are shown in teal). (<bold>F</bold>) CNN-LSTM membrane potential predictions for left- (navy) or right-shifted potassium conditions are compared to control conditions., Membrane potential responses below and above –67 mV are quantified for the two altered potassium conductances in NEURON simulation and CNN-LSTM predictions. The effects of biophysical changes of potassium channels were only apparent at membrane potentials above their activation threshold (–67 mV). (<bold>G</bold>) Artificial neural networks (ANNs) fitting NEURON models with left-shifted (dark blue) and right-shifted (light blue) KDR conductances are plotted against membrane potential responses of ANNs with control KDR conductances. The separation of the two responses shows voltage response modulation of KDR at subthreshold membrane potentials. (<bold>H</bold>) Membrane potential responses of NEURON and ANN models below and above resting membrane potential (–67 mV).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig2-v1.tif"/></fig><p>In the initial training dataset for the CNN-LSTM, the ratio of excitatory and inhibitory events (8:3) was preserved while the total number of synaptic inputs was varied. We noticed that the firing rate of this model did not scale linearly with the number of synapses as initially expected in the presence of inhibitory inputs (<xref ref-type="bibr" rid="bib60">Enoki et al., 2001</xref>). Thus, we systematically mapped AP firing of model cells in two different excitatory-inhibitory ratios with at a wide range of synaptic input frequencies (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). We noted that varying excitation and inhibition could interact with each other in various ways, creating arithmetic operations like subtraction, division, or normalization (<xref ref-type="bibr" rid="bib29">Carandini and Heeger, 2011</xref>). We approximated the resulting firing rates with two different models (see ‘Methods’). We found that the logistic function representing divisive normalization best fit our results (<xref ref-type="bibr" rid="bib22">Bhatia et al., 2019</xref>) (Akaike information criterion [AIC] for linear models representing subtractive and divisive inhibition versus AIC for logistic function: 983.3 ± 231.66 and 905.87 ± 200.92, respectively, n = 700 each). Notably, the CNN-LSTM model was able to replicate firing responses to these variable synaptic conditions (R<sup>2</sup> values when comparing logistic fits for NEURON and CNN-LSTM models in 2:1 excitation–inhibition ratio: 0.996, for 1:2 excitation–inhibition ratio: 0.9, n = 700), further demonstrating the ability of the neuronal net to reproduce key features of neuronal excitability without prior entrainment.</p><p>Due to the opaque nature of neural net operations (<xref ref-type="bibr" rid="bib31">Castelvecchi, 2016</xref>), it is reasonable to assume that instant modification of the trained architecture to account for specific biophysical alterations may not be feasible, highlighting a potentially significant shortcoming of our approach. However, the complexity of encoded features is correlated with the depth of the encoding layer in hierarchically constructed neural networks (<xref ref-type="bibr" rid="bib59">Egmont-Petersen et al., 2002</xref>), which can be exploited through partial retraining. To test whether the ANN could accurately handle a specific biophysical change, we constructed a simple NEURON model equipped with a delayed rectifier K<sup>+</sup> conductance with variable voltage dependences (<xref ref-type="bibr" rid="bib139">Oláh et al., 2021</xref>; <xref ref-type="fig" rid="fig2">Figure 2F</xref>). Nonlinear signal summation at different subthreshold voltages was noted after shifting the steady-state activation and inactivation of the K<sup>+</sup> conductance (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). From this model, a single CNN-LSTM model was fitted to the control K<sup>+</sup> condition. Subsequently, the CNN-LSTM model layers were frozen, with the exception of the (upper) fully connected layers, which were trained for 10 min on NEURON traces with either a 10 mV leftward or rightward shift introduced to the voltage dependence of the potassium conductance. All three models were in good agreement with the NEURON simulation results and provided similar deviations in subthreshold membrane potential regimes compared to control conditions (<xref ref-type="fig" rid="fig2">Figure 2G and H</xref>, below resting membrane potential: –0.13 ± 0.36, –0.11 ± 0.03, –0.01 ± 0.23, 0.01 ± 0.06; above resting membrane potential: –0.4 ± 0.43, –0.22 ± 0.55, 0.35 ± 0.27, 0.2 ± 0.1 for CNN-LSTM right-shift, NEURON right-shift, CNN-LSTM left-shift, and NEURON left-shift, respectively, n = 270), indicating that CNN-LSTM can be rapidly adapted to account for biophysical alterations.</p><p>NEURON models can calculate and display several features of neuronal behavior in addition to membrane potential, including ionic current flux. To test how our CNN-LSTMs perform in predicting ionic current changes, we supplemented ANN inputs with sodium (I<sub>Na</sub>) and potassium currents (I<sub>K</sub>) and tasked the models to predict these values as well. The accuracy of the CNN-LSTM prediction for these ionic currents was similar to membrane potential predictions (<xref ref-type="fig" rid="fig3">Figure 3</xref>, Pearson’s <italic>r</italic> = 0.999 and 0.99 for fitting, n = 5000, variance explained: 15.1 ± 11.6% and 82 ± 6.1%; prediction correlation coefficient: 0.85 ± 0.08 and 0.81 ± 0.1, n = 5, for I<sub>K</sub> and I<sub>Na</sub>, respectively) while the other ANNs again regressed to the mean.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Convolutional neural network-long short-term memory (CNN-LSTM) prediction of neuronal mechanisms beyond somatic membrane potential.</title><p>(<bold>A</bold>) Representative membrane potential (V<sub>m</sub>, top) and ionic current (I<sub>K</sub>, potassium current; I<sub>na</sub>, sodium current, bottom) dynamics prediction upon arriving excitatory (green, middle) and inhibitory (red, middle) events. Enlarged trace shows subthreshold voltage and current predictions. Color coding is same as for <xref ref-type="fig" rid="fig1">Figure 1</xref>. (black, NEURON model traces; magenta, CNN-LSMT; blue, linear model; teal, nonlinear model; green, deep neural net; orange, convolutional net). Notice the smooth vertical line corresponding to predictions by artificial neural networks (ANNs), with the exception of CNN-LSTM. On bottom left, magnified view illustrates the subthreshold correspondence of membrane potential and ionic current traces. (<bold>B</bold>) CNN-LSTM models accurately predict ionic current dynamics. Normalized ANN predictions are plotted against normalized neuron signals for sodium (dark gray, left) and potassium currents (light gray). (<bold>C</bold>) Variance of suprathreshold traces is largely explained by CNN-LSTM predictions (right, color coding is same as in panel [<bold>B</bold>], left). Correlation coefficients are superimposed in black.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig3-v1.tif"/></fig><p>Finally, we explored whether ANNs could represent nonlinear synaptic responses. Thus, we constructed single-compartmental models with two-component AMPA-NMDA containing synapses and inhibitory synapses. AMPA-NMDA model responses were voltage-dependent and produced nonlinear response curves with respect to AMPA alone (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Importantly, the CNN-LSTM architecture recreated the nonlinear response amplitude and time-course characteristic of AMPA-NMDA synapse activation (<xref ref-type="bibr" rid="bib153">Schiller et al., 2000</xref>; <xref ref-type="bibr" rid="bib113">Major et al., 2008</xref>; <xref ref-type="bibr" rid="bib24">Branco and Häusser, 2011</xref>; <xref ref-type="bibr" rid="bib102">Kumar et al., 2018</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Accurate representation of nonlinear synaptic activation by convolutional neural network-long short-term memory (CNN-LSTM).</title><p>(<bold>A</bold>) Representative synaptic responses with variable synaptic activation, CNN-simulated AMPA receptors (light magenta) and AMPA + NMDA receptors (magenta), on the left. AMPA + NMDA response amplitudes nonlinearly depend on the activated synaptic conductance (magenta, CNN-LSTM; black, NEURON), compared to AMPA responses (light magenta, CNN-LSTM; gray, NEURON), on the right. (<bold>B</bold>) NMDA response nonlinearity enables coincidence detection in a narrow time window, resulting in action potential (AP) generation at short stimulus intervals. (<bold>C</bold>). Neuronal output modulation is dependent on synaptic NMDA receptor content in a naturalistic network condition. Representative traces on the left (CNN-LSTM, magenta; NEURON, black). Summary depiction of firing frequencies with varying amounts of NMDA receptor activation (percentages denominate the synaptic NMDA-AMPA fraction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig4-v1.tif"/></fig><p>A well-defined functional role of NMDA receptors is coincidence detection, which allows boosting of consecutive subthreshold signals well beyond passive integration (<xref ref-type="bibr" rid="bib171">Takahashi and Magee, 2009</xref>; <xref ref-type="bibr" rid="bib158">Shai et al., 2015</xref>). To test whether our ANN could reliably perform coincidence detection, we simulated two excitatory inputs in NMDA-AMPA or AMPA-alone models. Closely spaced stimuli could generate significantly boosted EPSPs in models with NMDA-AMPA (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). We found that both NEURON and ANN models exhibited strongly boosted excitatory signals within a well-defined ISI time window (±12 ms) when NMDA-AMPA receptors were activated, which could produce APs. Under physiological conditions, NMDA receptors have been reported to critically influence the AP output of neuronal cells (<xref ref-type="bibr" rid="bib165">Smith et al., 2013</xref>). Thus, we subjected NEURON models to a barrage of excitatory and inhibitory inputs, such that AP generation was limited in the absence of NMDA (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Adding NMDA resulted in increased spike output (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Across several NMDA conditions, output in the NEURON and ANN models was indistinguishable (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, 12.42 ± 1.36 and 12.39 ± 2.3 Hz firing, respectively, in condition where 100% synapses contained NMDA receptors, n = 50). Together, these results demonstrate that the CNN-LSTM correctly learned several highly specialized aspects of neuronal behavior.</p></sec><sec id="s2-3"><title>Predicting the activity of morphologically realistic neurons using ANNs</title><p>Neurons multiply their adaptive properties by segregating different conductances into separate subcellular compartments (<xref ref-type="bibr" rid="bib109">Magee and Cook, 2000</xref>; <xref ref-type="bibr" rid="bib99">Kole et al., 2008</xref>; <xref ref-type="bibr" rid="bib107">Losonczy et al., 2008</xref>; <xref ref-type="bibr" rid="bib93">Kim et al., 2012</xref>; <xref ref-type="bibr" rid="bib146">Rowan et al., 2014</xref>; <xref ref-type="bibr" rid="bib168">Stuart and Spruston, 2015</xref>; <xref ref-type="bibr" rid="bib26">Brunner and Szabadics, 2016</xref>; <xref ref-type="bibr" rid="bib169">Stuart et al., 2016</xref>). Thus, in addition to simplified input integrating point neurons, a substantial portion of neuronal models developed in recent decades intended to address subcellular signal processing via detailed multicompartmental biophysical cellular representations (<xref ref-type="bibr" rid="bib112">Major et al., 1994</xref>; <xref ref-type="bibr" rid="bib111">Mainen and Sejnowski, 1996</xref>; <xref ref-type="bibr" rid="bib179">Vetter et al., 2001</xref>; <xref ref-type="bibr" rid="bib74">Hallermann et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Brunner and Szabadics, 2016</xref>; <xref ref-type="bibr" rid="bib138">Oláh et al., 2020</xref>). Therefore, our next aim was to examine how well ANNs describe multicompartmental information. To this end, a training dataset of synaptic inputs and corresponding somatic voltage responses was generated in NEURON from a morphologically and biophysically detailed in vivo-labeled neocortical L5 PC (<xref ref-type="bibr" rid="bib74">Hallermann et al., 2012</xref>). The NEURON model included synapses placed at 200 synaptic locations along the dendritic tree. Although this number of synaptic sites is significantly lower compared to what has been established in biological neurons (<xref ref-type="bibr" rid="bib119">Megías et al., 2001</xref>), this amount of discretization has proven to yield low errors compared to nondiscretized synaptic placements, with fast simulation runtimes and negligible memory consumption (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). It is noted that each synaptic location can be contacted by multiple presynaptic cells; therefore, the number of the synaptic locations does not constrain the connectivity. As the computational resource requirements for modeling such complex cells are much higher than in single-compartmental neurons, all NEURON models, data preprocessing, and ANN fitting and query were carried out on single graphical processing units (GPUs) and tensor processing units (TPUs) (‘Methods,’ <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). We found that the trained CNN-LSTM performed in near-perfect accordance with the NEURON simulation (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, Pearson’s <italic>r</italic> = 0.999, n = 45,000 ms). The continuous self-reliant prediction yielded lower yet adequate AP fidelity (<xref ref-type="fig" rid="fig5">Figure 5G</xref>, 68.28 ± 18.97% and 66.52 ± 25.37% precision and recall, 0.439 ± 4.181 ms temporal shift for true-positive spikes compared to ground truth, n = 205) compared to the point neuron, and the accuracy of subthreshold membrane potential fluctuations remained high (Pearson’s <italic>r</italic> = 0.83, n = 37).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Multicompartmental simulation representation by convolutional neural network-long short-term memory (CNN-LSTM).</title><p>(<bold>A</bold>) CNN-LSTM can accurately predict membrane potential of a multicompartmental neuron upon distributed synaptic stimulation. Representative figure depicts the placement of synaptic inputs (150 excitatory inputs: 100 inputs on apical, oblique, and tuft dendrites and 50 inputs on the basal dendrite, randomly distributed; and 50 inhibitory inputs: 30 inputs on apical, oblique, and tuft dendrites and 20 inputs on the basal dendrite, randomly distributed) of a reconstructed level 5 (L5) pyramidal cell (PC) (left). Point-by-point forecasting of L5 PC membrane potential by a CNN-LSTM superimposed on biophysically detailed NEURON simulation (left). CNN-LSTM prediction accuracy of multicompartmental membrane dynamics is comparable to single-compartmental simulations (right, L5 PC in black, single-compartmental simulation of <xref ref-type="fig" rid="fig1">Figure 1D</xref> in gray, n = 45,000 and 50,000, respectively). (<bold>B</bold>) Convolutional filter information was gathered from the first convolutional layer (middle, color scale depicts the different weights of the filter), which directly processes the input (membrane potential in magenta, excitatory and inhibitory synapse onsets in green and red, respectively), providing convolved inputs to upper layers (gray bars, showing the transformed 1D outputs). (<bold>C</bold>) Distribution of filter weights from 512 convolutional units (n = 102,400) with double Gaussian fit (red). (<bold>D</bold>) Filter weight is independent of the somatic amplitude of the input (circles are averages from 512 filters, n = 200, linear fit in red). (<bold>E</bold>) Each synapse has a dedicated convolutional unit, shown by plotting the filter weights of the 200 most specific units against 200 synapses. Notice the dark diagonal illustrating high filter weights. (<bold>F</bold>) Excitatory and inhibitory synapse information is convolved by filters with opposing weights (n = 51,200, 25,600, 15,360, and 10,240 for apical excitatory, basal excitatory, apical inhibitory, and basal inhibitory synapses, respectively). (<bold>G</bold>) Representative continuous prediction of L5 PC membrane dynamics by CNN-LSTM (magenta) compared to NEURON simulation (black) upon synaptic stimulation (left, excitatory input in green, inhibitory input in red). Spike timing is measured on subthreshold traces (right, n = 50 for variance explained, precision and recall). (<bold>H</bold>) Artificial neural networks (ANNs) constrained on cortical layer 2/3 (top), layer 4 (middle), and layer 6 (bottom) PCs selected from the Allen Institute model database.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Increased spatial discretization causes reduced errors at the cost of computational overhead.</title><p>(<bold>A</bold>) Methodological illustration of a ball and stick model supplemented with synaptic locations placed at different locations. First, synapses were randomly placed along the dendritic tree (right side, light gray) and the somatic voltage response was recorded. Next, the number of synaptic locations was restricted to a low amount of evenly placed locations, and the previously randomly placed synapses were assigned to the nearest location. The resulting voltage trace was compared to the first arrangement. (<bold>B</bold>) Mean squared error of voltage traces recorded from models with randomly placed and spatially subsampled synaptic locations as a function of the number of evenly placed synaptic locations. (<bold>C</bold>) Simulation runtime of artificial neural networks (ANNs) with different input matrix dimensions (number of columns correspond to the number of synaptic locations). (<bold>D</bold>) Memory requirements of 5000 input matrices with different dimensions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Artificial neural network (ANN) fitting workflow.</title><p>As detailed in the ‘Methods’ section, (1) NEURON models were acquired from publicly available, well-curated databases. (2) Synaptic input stream was established at varying number of synaptic locations (200 synaptic locations on level 5 [L5] pyramidal cell [PC] in <xref ref-type="fig" rid="fig5">Figure 5</xref>). (3) Voltage traces with varying input frequencies were recorded. (4) Input/output matrices were created from synaptic activations and corresponding voltage recordings. (5) Datasets were normalized to better suit ANN fitting algorithms. Different normalizations were used for different models, based on trial and error (‘Methods’). (6) ANN training consisted of two consecutive steps. First (6.1), ANNs were supplemented with input matrices corresponding to low input frequency recordings, to obtain proper fitting of isolated inputs, and to learn resting membrane potential. Next (6.2), the resulting ANNs received input matrices with higher input frequencies, to learn action potential dynamics, and the spatiotemporal dynamics of distinct synaptic locations. (7) ANNs were evaluated, and further bias terms were established for long-term prediction stability.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Convolutional neural network-long short-term memory (CNN-LSTM) predictions of dendritic voltage and current fluctuations of a level 2/3 (L2/3) pyramidal cell (PC).</title><p>Morphology and biophysical features were obtained from the Cell Types Database of the Allen Institute for Brain Science. (<bold>A</bold>) Artificial neural network (ANN) signal generated through a point-by-point fit, plotted against NEURON ground truth signal of somatic voltage (left). Representative somatic voltage predictions (magenta) and ground truth NEURON signal (black) in the middle, quantification of explained variance on the right. (<bold>B</bold>) Representative prediction of membrane voltage (top) and calcium current (bottom) fluctutation at a basal dendritic location. CNN-LSTM predictions in magenta, NEURON signal in black. (<bold>C</bold>) Basal fit accuracy of CNN-LSTM plotted against NEURON signal. Dendritic membrane voltage on the left, calcium current on the right. (<bold>D</bold>) Variance explained by CNN-LSTM predictions of basal membrane voltage (left) and calcium currents (right). (<bold>E</bold>) Representative voltage (top) and calcium current (bottom) fluctuations at an apical dendritic location, quantification of fit accuracy and explained variance in panels (<bold>F</bold>) and (<bold>G</bold>) similar as in panels (<bold>C</bold>) and (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig5-figsupp3-v1.tif"/></fig></fig-group><p>We previously demonstrated that CNN-LSTMs could accurately predict various neuronal mechanisms beyond somatic voltage fluctuations in single-compartmental cells (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To investigate whether this architecture is sufficient to describe complex features of neuronal behavior in morphologically and biophysically realistic neurons as well, we tasked the ANN with simultaneously predicting membrane potentials from the soma and two dendritic locations (one apical and one basal) together with calcium current dynamics in the same locations (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). We found that CNN-LSTMs can accurately describe the selected aspects of neuronal activity, further demonstrating the versatility of this ANN architecture.</p><p>Establishing a proper multicompartmental representation of a neural system by relying solely on the somatic membrane potential is a nontrivial task due to complex signal processing mechanisms taking place in distal subcellular compartments (<xref ref-type="bibr" rid="bib152">Schiller et al., 1997</xref>; <xref ref-type="bibr" rid="bib77">Häusser and Mel, 2003</xref>; <xref ref-type="bibr" rid="bib88">Jarsky et al., 2005</xref>; <xref ref-type="bibr" rid="bib75">Harnett et al., 2015</xref>; <xref ref-type="bibr" rid="bib172">Takahashi et al., 2016</xref>). This is especially true for signals arising from more distal synapses (<xref ref-type="bibr" rid="bib164">Sjöström and Häusser, 2006</xref>; <xref ref-type="bibr" rid="bib104">Larkum et al., 2009</xref>; <xref ref-type="bibr" rid="bib171">Takahashi and Magee, 2009</xref>). To examine whether the CNN-LSTM considered distal inputs or neglected these in favor of more robust proximal inputs, we inspected the weights of the first layer of the neural network architecture (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). This convolutional layer consists of 512 filters, which directly processes the input matrix (64 ms of 201 input vectors corresponding to the somatic membrane potential and vectorized timing information of 200 synapses). Despite the random initialization of these filters from a uniform distribution (<xref ref-type="bibr" rid="bib79">He et al., 2015</xref>), only a small fraction of optimized filter weights were selected for robust information representation (13.83% of all weights were larger than 0.85), while the majority of them were closer to zero (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), suggesting relevant feature selection. In order to demonstrate that this feature selection was not biased against distal inputs, the 512 convolutional filters were ranked by their selectivity for distinct synapses. We found that each synaptic input was assigned an independent selectivity filter (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Next, we compared the mean weights of each synapse with the somatic amplitude of the elicited voltage response as a proxy for input distance from the soma (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). This comparison revealed a flat linear correspondence (Pearson’s <italic>r</italic> = 0.06), which combined with the filter specificity (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) confirmed that distal and proximal synaptic inputs carry equally relevant information for the CNN-LSTM.</p><p>When comparing the weights of excitatory and inhibitory inputs, we found that even at the first layer the CNN-LSTM could determine that these inputs have opposing effects on subsequent membrane potential (5.91 * 10<sup>–6</sup>, 2.66 * 10<sup>–5</sup>, –6.22 * 10<sup>–6</sup>, and –1.34 * 10<sup>–5</sup> mean weights for apical excitatory, basal excitatory, apical inhibitory, and basal inhibitory synapses, respectively, n = 51,200, 25,600, 15,360, and 10,240) even though these vectors only contain synaptic conductance information (comparable positive values for both excitatory and inhibitory synapses, <xref ref-type="fig" rid="fig5">Figure 5F</xref>). Taken together, the feature selectivity and prediction accuracy confirm that the CNN-LSTM architecture is well suited for representing multicompartmental information.</p><p>The recent surge in readily available cellular model datasets has significantly reduced the entry barrier for neuronal simulations as researchers no longer need to gather ground truth data individually. Therefore, we aimed to establish a pipeline to constrain ANNs on neuronal models from a publicly available, well-curated database (<xref ref-type="bibr" rid="bib69">Gouwens et al., 2018</xref>) without developer involvement. Using this pipeline, we constrained ANNs on the remaining major cortical PC types; layer 2/3, layer 4, and layer 6 PCs (<xref ref-type="fig" rid="fig5">Figure 5H</xref>). We found that the resulting ANNs were fit adequately to the NEURON simulations (<xref ref-type="fig" rid="fig5">Figure 5I</xref>, 94.2 ± 14.2%, 74.5 ± 23.5%, and 67 ± 14.5% variance explained, 86.6 ± 23.1%, 70.1 ± 25.8%, and 63.2 ± 33.2% precision, 90.7 ± 18%, 74.5 ± 25.8%, and 63.5 ± 32.7% recall for layer 2/3, layer 4, and layer 6 PCs, respectively, n = 50), and the fitting procedure was devoid of ambiguities. Together, we developed an ANN architecture appropriate for multicompartmental neuronal simulations of diverse cell types and a user-friendly methodology for their construction.</p></sec><sec id="s2-4"><title>Current injection-induced firing responses</title><p>The neuronal firing pattern upon direct current injection is one of the most prevalent means of establishing neuronal class and describing the cell’s potential in vivo behavior (<xref ref-type="bibr" rid="bib13">Ascoli et al., 2008</xref>). Therefore, these recordings often serve as ground truth data during single-neuronal model constraining (<xref ref-type="bibr" rid="bib87">Izhikevich, 2003</xref>; <xref ref-type="bibr" rid="bib132">Naud et al., 2008</xref>; <xref ref-type="bibr" rid="bib58">Druckmann et al., 2011</xref>; <xref ref-type="bibr" rid="bib173">Teeter et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Gouwens et al., 2020</xref>). Firing patterns are modulated by several ionic mechanisms in concert, several of which operate on much longer timescales than what the dimensions of our ANN input matrices allow us to observe. However, even complex firing patterns can be approximated by much simpler, biologically plausible, and computationally efficient single-cell models (<xref ref-type="bibr" rid="bib51">Destexhe, 1997</xref>; <xref ref-type="bibr" rid="bib87">Izhikevich, 2003</xref>; <xref ref-type="bibr" rid="bib25">Brette and Gerstner, 2005</xref>; <xref ref-type="bibr" rid="bib148">Sacerdote and Giraudo, 2013</xref>). Therefore, we created a custom ANN layer that can be inserted on top of CNN-LSTMs (for either single- and multicompartmental models) with its internal logic hard-coded based on the governing equations of the eloquent simple spiking model (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) described by <xref ref-type="bibr" rid="bib87">Izhikevich, 2003</xref>. In addition to the original variables of this model, we set the ‘time step’ parameter as a variable to account for differences in membrane time constant across cell types.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Firing pattern representation with custom artificial neural network (ANN) layer.</title><p>(<bold>A</bold>) Representative figure depicting the custom ANN layer (termed custom Izhikevich layer) placed on the output of the fully connected layers of the convolutional neural network-long short-term memory (CNN-LSTM). This layer represents the final signal integration step, analogous to the soma of biological neurons. (<bold>B</bold>) Four firing patterns with different activity dynamics, produced by the custom ANN layer. (<bold>C</bold>) Firing pattern of a NEURON model (black, top) and the constrained ANN counterpart (magenta, bottom). The ANN model accurately reproduced the input–output relationship of the NEURON model. (<bold>D</bold>) Continuous subthreshold membrane potential fluctuations of the NEURON model (black trace) and faithfully captured by the custom ANN layer (magenta trace). (<bold>E</bold>) Relationship of membrane potential values predicted step-by-step by the ANN layer compared to the ground truth NEURON model. (<bold>F</bold>) The custom ANN layer continuous predictions explain the majority of the variance occurring in voltage signals produces by the NEURON simulation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Custom ANN layers for encoding and decoding popular indicators neuronal activity.</title><p>(<bold>A</bold>) Depiction of custom artificial neural network (ANN) layers placed on top of the convolutional neural network-long short-term memory (CNN-LSTM) architecture. (<bold>B</bold>) Fitting results of the ANN plotted against ground truth signal. (<bold>C</bold>) Representative simulated calcium indicator traces of ANN predictions (magenta) and NEURON ground truth (black) and the corresponding membrane potential (gray, bottom). Quantification of explained variance, precision, and recall on the right. (<bold>D</bold>) Encoder ANN fit of voltage indicator signal (ANN, purple; target trace, black), representative trace on the left, fitted values plotted against target signal on the right. (<bold>E</bold>) Continuous ANN predictions (magenta) are well correlated with target signal (black). Input voltage trace in gray, explained variance quantification on the right. (<bold>F</bold>) Decoder ANN fit of voltage indicator signal (ANN, purple; target trace, black), representative trace on the left, fitted values plotted against target signal on the right. (<bold>G</bold>) Continuous ANN predictions (magenta) are in good agreement with target signal (black). Input voltage indicator signal in gray, explained variance quantification on the right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig6-figsupp1-v1.tif"/></fig></fig-group><p>The custom ANN layer (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) could reproduce a wide range of naturally occurring firing patterns (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). In contrast to the millions of free parameters in CNN-LSTMs, this custom layer has only five trainable parameters and thus can be constrained using conventional optimization algorithms (<xref ref-type="bibr" rid="bib162">Singer and Nelder, 2009</xref>). We created a single-compartmental NEURON model, equipped with Hodgkin–Huxley conductances based on a fast-spiking phenotype (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) to generate a ground truth dataset of firing activity and subthreshold membrane potential fluctuations. We found that the custom ANN layer could reliably capture the input–output characteristics of the NEURON model (Pearson’s r: 0.982). We next fitted the ANN layer on randomly distributed synaptic inputs (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). The custom ANN layer produced voltage responses in good agreement with the NEURON model (<xref ref-type="fig" rid="fig6">Figure 6E and F</xref>, Pearson’s r: 0.999, 96.9 ± 0.4% variance explained, n = 17). Together, this custom ANN layer approach imbues CNN-LSTMs with the ability to reproduce firing responses faithfully and also provides added flexibility allowing for the instantaneous alteration of firing behavior while preserving synaptic representations.</p><p>Generating diverse custom top layers operating on the output of CNN-LSTMs (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>) also creates opportunities to predict convoluted signals used to report neuronal activity in vivo, such as fluorescently reported calcium and voltage signals. To illustrate this possibility, we created custom ANN layers fitted to the dynamics of the GCamp6f fluorescent calcium indicator (<xref ref-type="bibr" rid="bib38">Chen et al., 2013</xref>) and a recently developed fluorescent voltage indicator (<xref ref-type="bibr" rid="bib180">Villette et al., 2019</xref>). Although these indicators severely distorted the underlying neuronal signals (i.e., membrane potential), we found that a custom recurrent encoder can accurately predict these characteristic waveforms (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), and importantly, stand-alone use of these layers can deconvolve even severely distorted ground truth signals.</p></sec><sec id="s2-5"><title>Ultra-rapid simulation of multiple cells using CNN-LSTM</title><p>One of the main benefits of this machine learning approach as a substitute for traditional modeling environments is the potential for markedly reduced simulation runtimes. Simulation environments such as NEURON rely on compartment-specific mathematical abstractions of active and passive biophysical mechanisms (<xref ref-type="bibr" rid="bib81">Hines and Carnevale, 1997</xref>), which results in high computational load in increasingly complex circuit models. In the case of small-sized (<xref ref-type="bibr" rid="bib136">Nikolic, 2006</xref>; <xref ref-type="bibr" rid="bib126">Migliore and Shepherd, 2008</xref>; <xref ref-type="bibr" rid="bib41">Cutsuridis and Wennekers, 2009</xref>; <xref ref-type="bibr" rid="bib32">Chadderdon et al., 2014</xref>; <xref ref-type="bibr" rid="bib78">Hay and Segev, 2015</xref>) and mid-sized networks (<xref ref-type="bibr" rid="bib115">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>; <xref ref-type="bibr" rid="bib161">Shimoura et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>) this hinders the possibility of running these models on nonspecialized computational resources. Although several attempts have been made to reduce the demanding computational load of neuronal simulations (<xref ref-type="bibr" rid="bib27">Bush and Sejnowski, 1993</xref>; <xref ref-type="bibr" rid="bib53">Destexhe and Sejnowski, 2001</xref>; <xref ref-type="bibr" rid="bib80">Hendrickson et al., 2011</xref>; <xref ref-type="bibr" rid="bib114">Marasco et al., 2012</xref>; <xref ref-type="bibr" rid="bib145">Rössert, 2016</xref>; <xref ref-type="bibr" rid="bib4">Amsalem et al., 2020</xref>; <xref ref-type="bibr" rid="bib185">Wybo et al., 2021</xref>), the most commonly used approach is parallelization, both at the level of single cells (<xref ref-type="bibr" rid="bib83">Hines et al., 2008</xref>) and network models (<xref ref-type="bibr" rid="bib82">Hines and Carnevale, 2008</xref>; <xref ref-type="bibr" rid="bib108">Lytton et al., 2016</xref>). However, ANNs offer a unique solution to this problem. Contrary to traditional modeling environments, graph-based ANNs are designed explicitly for parallel information processing. This means that ANN simulation runtimes on hardware that enables parallel computing, such as modern GPUs, do not increase linearly after additional cells are integrated into the simulated circuit (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), resulting in better scaling for large networks where an immense number of similar cells are simulated.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Orders of magnitude faster simulation times with convolutional neural network-long short-term memory (CNN-LSTM).</title><p>(<bold>A</bold>) An illustration demonstrating that CNN-LSTMs (top, magenta) handle both single-cell (left) and multiple-cell (right) simulations with a single graph, while the set of equations to solve increases linearly for NEURON simulations (bottom, black). (<bold>B</bold>) 100 ms simulation runtimes of 1-, 50-, and 5000-point neurons on four different resources. Bar graphs represent the average of five simulations. (<bold>C</bold>) Same as in panel (B), but for level 5 (L5) pyramidal cell(PC) simulations. Teal borders represent extrapolated datapoints.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig7-v1.tif"/></fig><p>To verify the efficiency of our CNN-LSTM, we compared single cells and small- to mid-sized simulation runtimes against NEURON models used in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig5">5</xref>. NEURON simulations were performed on a single CPU as this is the preferred and most widely used method (but see ; <xref ref-type="bibr" rid="bib18">Ben-Shalom et al., 2022</xref>), while neural nets were run on both CPU and GPU because these calculations are optimized for GPUs. Although GPUs are inherently faster in numerical calculations, NEURON simulations are currently not suitable for this resource; therefore, simulation runtimes were compared using CPUs as well. NEURON simulations were repeated with custom initialization, during which simulations were pre-run to allow time-dependent processes, such as conductance inactivation, to reach steady-state values. Simulation of multiple cells was carried out without the implementation of synaptic connections to establish baseline runtimes, without additional runtime impeding factors. For point neurons, single-cell simulations ran significantly faster in NEURON than their CNN-LSTM counterparts when the optional initialization step was omitted (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, 3.68 ± 0.24 s, 0.65 ± 0.03 s, 2.19 ± 0.69 ms, and 0.72 ± 0.04 s, 100 ms cellular activity by NEURON with initialization, NEURON without initialization, CNN-LSTM on CPU, and CNN-LSTM on GPU, respectively, n = 5). However, when increasing the number of cells, the predicted optimal scaling of CNN-LSTM models resulted in faster runtimes compared to NEURON models (e.g., for 50 cells, 24.23 ± 1.12 s, 7.45 ± 0.37 s, 4.42 ± 0.77 s, and 0.71 ± 0.05 s for a 100 ms simulation by NEURON with initialization, NEURON without initialization, CNN-LSTM on CPU, and CNN-LSTM on GPU, respectively, n = 5). These results show that while in NEURON the runtimes increased by approximately 6.6 times, CNN-LSTM runtimes on a GPU did not increase.</p><p>To demonstrate the practicality of ANNs for typical large-scale network simulations, we repeated these experiments with 5000 cells (representing the number of cells in a large-scale network belonging to the same cell type; <xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>). In these conditions, the NEURON simulation was ~148 times slower than a single-cell simulation. Notably, this large-scale CNN-LSTM simulation was only four times slower than that of a single cell (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, 546.85 ± 4.61 ms, 407.2 ± 9 ms, 222.15458 ± 19.02 ms, and 2.97 ± 0.02ms for simulating 100 ms activity by NEURON with initialization, NEURON without initialization, CNN-LSTM on CPU, and CNN-LSTM on GPU, respectively, n = 5).</p><p>We next compared runtime disparities for NEURON and CNN-LSTM simulations of detailed biophysical models (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). We found that the single-cell simulation of the L5 PC model ran significantly slower than the CNN-LSTM abstraction (2.08 * 10<sup>3</sup> ± 84.66 s, 185.5 ± 3.7 s, 4.73 ± 0.13 s, and 1.02 ± 0.05 s for simulating 100 ms activity by NEURON with initialization, NEURON without initialization, CNN-LSTM on CPU, and CNN-LSTM on GPU, respectively, n = 5). This runtime disparity was markedly amplified in simulations with multiple cells (50 cells: 6.3 * 10<sup>4</sup> s, 5.8 * 10<sup>3</sup> s, 14.3 ± 0.24 s, and 1.19 ± 0.08 s, 5000 cells: 6.53 * 10<sup>6</sup> s, 6.28 * 10<sup>5</sup> s, 901.15 s, and 11.99 s for simulating 100 ms activity by NEURON with initialization, NEURON without initialization, CNN-LSTM on CPU, and CNN-LSTM on GPU respectively, n = 5), resulting in a four to five orders of magnitude faster runtime (depending on initialization) for the CNN-LSTM in case of mid-sized simulations. These results demonstrate that our machine learning approach yields far superior runtimes compared to traditional simulating environments. Furthermore, this acceleration is comparable to that afforded by increased parallel CPU cores used for several network simulations (<xref ref-type="bibr" rid="bib115">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>), introducing the possibility of running large or full-scale network simulations on what are now widely available computational resources.</p></sec><sec id="s2-6"><title>Efficient parameter space mapping using ANNs</title><p>Due to slow simulation runtimes, network simulations are typically carried out only a few times (but see <xref ref-type="bibr" rid="bib14">Barros-Zulaica et al., 2019</xref>), hindering crucial network construction steps, such as parameter space optimization. Therefore, we sought to investigate whether our ANN approach was suitable for exploring parameter space in a pathophysiological system characterized by multidimensional circuit alterations, such as Rett syndrome. Rett syndrome is a neurodevelopmental disorder caused by loss-of-function mutations in the X-linked methyl-CpG binding protein (MeCP2) (<xref ref-type="bibr" rid="bib33">Chahrour and Zoghbi, 2007</xref>). Rett syndrome occurs in ~1:10,000 births worldwide, resulting in intellectual disability, dysmorphisms, declining cortical and motor function, stereotypies, and frequent myoclonic seizures, mostly in girls (<xref ref-type="bibr" rid="bib16">Belichenko et al., 1994</xref>; <xref ref-type="bibr" rid="bib8">Armstrong, 1997</xref>; <xref ref-type="bibr" rid="bib166">Steffenburg et al., 2001</xref>; <xref ref-type="bibr" rid="bib9">Armstrong, 2002</xref>; <xref ref-type="bibr" rid="bib96">Kishi and Macklis, 2004</xref>; <xref ref-type="bibr" rid="bib63">Fukuda et al., 2005</xref>; <xref ref-type="bibr" rid="bib17">Belichenko et al., 2009</xref>). Although the underlying cellular and network mechanisms are largely unknown, changes in synaptic transmission (<xref ref-type="bibr" rid="bib43">Dani et al., 2005</xref>; <xref ref-type="bibr" rid="bib118">Medrihan et al., 2008</xref>; <xref ref-type="bibr" rid="bib188">Zhang et al., 2010</xref>), morphological alterations in neurons (<xref ref-type="bibr" rid="bib2">Akbarian et al., 2001</xref>; <xref ref-type="bibr" rid="bib96">Kishi and Macklis, 2004</xref>), and altered network connectivity (<xref ref-type="bibr" rid="bib44">Dani and Nelson, 2009</xref>) have been reported in Rett syndrome.</p><p>We aimed to investigate the contribution of the distinct alterations on cortical circuit activity in Rett syndrome using a recurrent L5 PC network (<xref ref-type="bibr" rid="bib78">Hay and Segev, 2015</xref>) composed entirely of CNN-LSTM-L5-PCs (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). Simulations were run uninterrupted for 100 ms when a brief (1 ms) perisomatic excitation was delivered to mimic thalamocortical input onto thick tufted PCs (<xref ref-type="bibr" rid="bib49">de Kock et al., 2007</xref>; <xref ref-type="bibr" rid="bib123">Meyer et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Constantinople and Bruno, 2013</xref>). In control conditions, cells fired well-timed APs rapidly after the initial stimuli followed by an extended AP firing as a consequence of the circuit recurrent connectivity (<xref ref-type="fig" rid="fig8">Figure 8B</xref>; <xref ref-type="bibr" rid="bib105">Lien and Scanziani, 2013</xref>; <xref ref-type="bibr" rid="bib170">Sun et al., 2013</xref>). First, we compared the runtime of the simulated L5 microcircuit of CNN-LSTM models and the run time of 150 unconnected L5 PCs in NEURON. We found that for a single simulation, CNN-LSTM models were more than 9300 times faster compared to NEURON models (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, 21.153 ± 0.26 s vs. 54.69 hr for CNN-LSTM and NEURON models, respectively).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Efficient parameter-space mapping with convolutional neural network-long short-term memory (CNN-LSTM) reveals a joint effect of recurrent connectivity and E/I balance on network stability and efficacy in Rett syndrome.</title><p>(<bold>A</bold>) 150 CNN-LSTM models of level 5 (L5) pyramidal cells (PCs) were simulated in a recurrent microcircuit. (<bold>B</bold>) The experimental setup consisted of a stable baseline condition for 100 ms, a thalamocortical input at t = 100 ms, and network response, monitored for 150 ms. Example trace from the first simulated CNN-LSTM L5 PC on top, raster plot of 150 L5 PCs in the middle, number of firing cells with 5 ms binning for the same raster plot in the bottom. Time is aligned to the stimulus onset (t = 0, black arrowhead). (<bold>C</bold>) Simulation runtime for single simulation (left, network of 150 cells simulated for 250 ms) and parameter space mapping (right, 150 cells simulated for 250 ms, 2500 times, for generating <bold>B</bold>). Teal border represents data extrapolation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig8-v1.tif"/></fig></sec><sec id="s2-7"><title>Rett cortical network alterations counteract circuit hyperexcitability</title><p>Cortical networks endowed with frequent recurrent connections between excitatory principal cells are prone to exhibit oscillatory behavior, which is often the mechanistic basis of pathophysiological network activities (<xref ref-type="bibr" rid="bib116">McCormick and Contreras, 2001</xref>; <xref ref-type="fig" rid="fig9">Figure 9A</xref>). We quantified oscillatory activity (<xref ref-type="bibr" rid="bib48">D’Cruz et al., 2010</xref>; <xref ref-type="bibr" rid="bib117">McLeod et al., 2013</xref>; <xref ref-type="bibr" rid="bib143">Roche et al., 2019</xref>) and the immediate response to thalamocortical stimuli independently (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). By systematically changing excitatory quantal size (<xref ref-type="bibr" rid="bib43">Dani et al., 2005</xref>) and the ratio of recurrent L5 PC innervation to mimic reduced recurrent connectivity and synaptic drive in Rett syndrome, we found that both alterations had considerable influence over network instability (<xref ref-type="fig" rid="fig9">Figure 9B</xref>, left panel; excitatory drive: 17.85 ± 61.61 vs. 388.92 ± 170.03 pre-stimulus APs for excitatory drive scaled by 0.75 and 1.25, respectively, n = 100 each, p&lt;0.001; recurrent connectivity: 321.96 ± 200.42 vs. 157.66 ± 192.5 pre-stimulus APs for 10 and 5.2% recurrent connectivity, similar to reported values for adult wild-type and <italic>Mecp2</italic>-null mutant mice [<xref ref-type="bibr" rid="bib44">Dani and Nelson, 2009</xref>], n = 50 each, p&lt;0.001) and response to stimuli (excitatory drive: 147.58 ± 17.2 vs. 119.23 ± 18.1 APs upon stimulus for excitatory drive scaled by 0.75 and 1.25, respectively, n = 100 each, p=2.3 * 10<sup>–22</sup>, t(198) = 11.03, <italic>two-sample t-test</italic>; recurrent connectivity: 134.76 ± 21.37 vs. 112.74 ± 34.99 APs upon stimulus for 10 and 5.2% recurrent connectivity, n = 50 each, p=2.54 * 10<sup>–4</sup>, t(98) = 3.8, <italic>two-sample t-test</italic>). Contrary to disruption of the excitatory drive, when inhibitory quantal size (<xref ref-type="bibr" rid="bib35">Chao et al., 2010</xref>) was altered, we found that inhibition had a negligible effect on network instability, as connectivity below 9% never resulted in oscillatory activity (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>; inhibition corresponds to random inhibitory drive, as the network did not contain ANNs representing feed-forward inhibitory cells). Interestingly, we found no measurable relationship between the inhibitory quantal size and the network response to thalamocortical stimuli either. These results suggest that lowered recurrent connectivity reduces network instability. Specifically, recurrent connectivity observed in young <italic>Mecp2</italic>-null mice (7.8%; <xref ref-type="bibr" rid="bib44">Dani and Nelson, 2009</xref>) yielded more stable microcircuits (54% of networks were stable, n = 100) than wild-type conditions (34% of networks were stable, n = 50). Recurrent connection probability of older animals (5.3%) further stabilized this network (64% of networks were stable). Taken together, our model suggests that reduced recurrent connectivity between L5 PCs is not causal to seizure generation and abnormal network activity (<xref ref-type="bibr" rid="bib166">Steffenburg et al., 2001</xref>; <xref ref-type="bibr" rid="bib143">Roche et al., 2019</xref>), which are crucial symptoms of Rett syndrome at a young age, but instead normal PC activation is disrupted. This may correspond to the early stages of Rett syndrome where cortical dysfunction emerges before the appearance of seizures (<xref ref-type="bibr" rid="bib33">Chahrour and Zoghbi, 2007</xref>).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Recurrent connectivity and excitatory drive jointly define network stability in a reduced level 5 (L5) cortical network.</title><p>(<bold>A</bold>) Two independent parameters were quantified: network instability (number of cells firing before the stimulus) and immediate response (number of cells firing within 10 ms of the stimulus onset). The example simulation depicts highly unstable network conditions. (<bold>B</bold>) Network instability (left) and immediate response (right) as a function of altered L5 pyramidal cell (PC) connectivity and excitatory drive. *a indicates network parameters used for generating panel (<bold>A</bold>). The white arrow in the right panel denotes circuit alterations observed in Rett syndrome. Namely, 5% recurrent connectivity between L5 PCs instead of 10% in control conditions and reduced excitatory drive.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Microcircuit stability and efficacy is robust to changes in inhibitory drive.</title><p>Network parameters were quantified as shown in <xref ref-type="fig" rid="fig8">Figure 8A</xref>. Recurrent connectivity constrains network stability (9.14 ± 2.21 vs. 320.78 ± 237.66 action potentials [APs], n = 1740 vs. 760 for below 9% connectivity and connectivity between 9–15%, respectively, p=2.2 * 10<sup>–219</sup>, two-sample <italic>t</italic>-test), while inhibitory inputs have a negligible effect (133.62 ± 29.32 vs. 131.72 ± 32.32 APs upon thalamocortical stimulus for inhibitory input scaling of 1 and 0.5, respectively, n = 50 each, p=0.76, t(98) = 0.31, two-sample <italic>t</italic>-test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79535-fig9-figsupp1-v1.tif"/></fig></fig-group><p>Using the ANN approach, we successfully implemented multidimensional parameter space mapping in a cortical circuit exhibiting pathophysiological changes and could identify the isolated outcome of distinct circuit alterations. Furthermore, our accelerated multicompartmental neural circuit model demonstrated that parameter space mapping is not only attainable by CNN-LSTM models on commercially available computational resources, but it is almost fourfold faster than completing a single NEURON simulation.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we present an ANN architecture (CNN-LSTM) capable of accurately capturing neuronal membrane dynamics. Most of the investigated ANN architectures predicted subthreshold voltage fluctuations of point neurons; however, only the CNN-LSTM was able to generate APs. This model could generalize well to novel input and also predict various other features of neuronal cells, such as voltage-dependent ionic current dynamics. Furthermore, the CNN-LSTM accounted for the majority of the variance of subthreshold voltage fluctuations of biophysically realistic L5 PC models with excitatory and inhibitory synapses distributed along the entirety of the dendritic tree. The timing of the predicted APs closely matched the ground truth data. Importantly, we found that the CNN-LSTM has superior scaling for large network simulations. Specifically, in the case of mid-sized biophysically detailed networks (50 cells), ANNs were more than three orders of magnitude faster, while for large-scale networks (5000 cells) ANNs are predicted to be five orders of magnitude faster than traditional modeling systems. These accelerated simulation runtimes allowed us to quickly investigate an L5 PC network in distinct conditions, for example, to uncover network effects of altered connectivity and synaptic signaling observed in Rett syndrome. In our Rett cortical circuit model, recurrent connectivity and excitatory drive jointly shape network stability and responses to sensory stimuli, showing the power of this approach in generating testable hypotheses for further empirical work. Together, the described model architecture provides a suitable alternative to traditional modeling environments with superior simulation speed for biophysically detailed cellular network simulations.</p><sec id="s3-1"><title>Advantages and limitations of the CNN-LSTM architecture</title><p>As our familiarity with neuronal circuits grows, so does the complexity of models tasked with describing their activity. Consequently, supercomputers are a regular occurrence in research articles that describe large-scale network dynamics built upon morphologically and biophysically detailed neuronal models (<xref ref-type="bibr" rid="bib115">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>). Here, we developed an alternative to these traditional models, which can accurately represent the full dynamic range of neuronal membrane voltages in multicompartmental cells, but with substantially accelerated simulation runtimes.</p><p>ANNs are ideal substitutes for traditional model systems for several reasons. First, ANNs do not require hard-coding of the governing rules for neuronal signal processing. Upon creation, ANNs serve as a blank canvas that can derive the main principles of input–output processing and neglect otherwise unimpactful processes (<xref ref-type="bibr" rid="bib20">Benitez et al., 1997</xref>; <xref ref-type="bibr" rid="bib47">Dayhoff and DeLeo, 2001</xref>; <xref ref-type="bibr" rid="bib31">Castelvecchi, 2016</xref>). The degree of simplification depends only on the ANN itself, not the developer, thereby reducing human errors. However, architecture construction and training dataset availability represent limiting steps in ANN development (<xref ref-type="bibr" rid="bib3">Alwosheel et al., 2018</xref>). Fortunately, the latter issue is void as virtually infinite neuronal activity training datasets are now available for deep learning. Conversely, as we have demonstrated, the former concern can significantly impede ANN construction. Although we have shown that markedly divergent ANN architectures can accurately depict subthreshold signal processing, we found only one suitable for both subthreshold active membrane potential prediction. The presented architecture is unlikely to be the only suitable ANN model for neural simulations as machine learning is a rapidly progressing field that frequently generates highly divergent ANN constructs (<xref ref-type="bibr" rid="bib45">da Silva et al., 2017</xref>). The importance of the network architecture is further emphasized by our findings demonstrating that ANNs with comparable or even greater numbers of freely adjustable parameters could not handle suprathreshold information.</p><p>The prevailing CNN-LSTM architecture was proven suitable for depicting membrane potential and ionic current dynamics of both simplified and biophysically detailed neuronal models and generalized well for previously unobserved simulation conditions. These results indicate that ANNs are ideal substitutes for traditional model systems for representing various features of neuronal information processing with significantly accelerated simulations. Future architecture alterations should focus on the continued improvement of AP timing and prediction, as well as the integration of additional dendritic and axonal properties.</p><p>A recent publication presented an excellent implementation of an ANN architecture for predicting neuronal membrane potentials (<xref ref-type="bibr" rid="bib19">Beniaguev et al., 2021</xref>) of complex cortical neurons. The featured architecture was composed of nested convolutional layers, and membrane potential dynamics was represented with a combination of two output vectors (subthreshold membrane potential and a binarized vector for AP timing). Building on this idea, we aimed to design an architecture that could (1) produce sequential output with smaller temporal increments, (2) generalize to previously unobserved temporal patterns and discrepant synaptic weights as well, and lastly, (3) produce APs with plausible waveforms in addition to subthreshold signals. Fulfillment of these three criteria is imperative for modeling these cells in a network environment. Our ANN architecture fulfills these requirements, thus representing the first ANN implementation that can serve as a viable alternate for biophysically and morphologically realistic neurons in a network model environment.</p><p>The CNN-LSTM architecture has several advantages over traditional modeling environments beyond the runtime acceleration. For example, connectivity has no influence over simulation speed as connection implementation is a basic matrix transformation carried out on the entire population simultaneously. However, this approach is not without limitations. First, although ANN training can be carried out on affordable and widely available resources, training times can last up to 24 hr to achieve accurate fits (‘Methods’). Furthermore, judicious restrictions are needed in the amount of synaptic contact sites, to preserve realistic responses and at the same time mitigate computational requirements, as the number of contact sites directly correlates with simulation runtimes and memory consumption. Additionally, the 1 ms temporal discretization hinders the implementation of certain biological phenomena that operate on much faster timescales, such as gap junctions. Depending on the degree of justifiable simplification, several other modeling environments exist, which are faster and computationally less demanding than our ANN approach. These environments mostly rely on simplified point neurons, such as the Izhikevich formulation (<xref ref-type="fig" rid="fig6">Figure 6</xref>), often developed specifically to leverage accelerated GPU computations (<xref ref-type="bibr" rid="bib144">Ros et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Fidjeland et al., 2009</xref>; <xref ref-type="bibr" rid="bib131">Nageswaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib129">Mutch, 2010</xref>; <xref ref-type="bibr" rid="bib174">Thibeault, 2011</xref>; <xref ref-type="bibr" rid="bib137">Nowotny et al., 2014</xref>; <xref ref-type="bibr" rid="bib181">Vitay et al., 2015</xref>; <xref ref-type="bibr" rid="bib186">Yavuz et al., 2016</xref>; <xref ref-type="bibr" rid="bib97">Knight et al., 2021</xref>). Therefore, depending on the required biophysical resolution and the available computational resources, the ANN approach presented here has an advantage over other environments in certain situations, while traditional modeling environments such as NEURON and GPU accelerated network simulators have a distinct edge in other use cases.</p></sec><sec id="s3-2"><title>Simulation runtime acceleration</title><p>Accelerated simulation runtimes are particularly advantageous for large-scale biological network simulations, which have seen an unprecedented surge in recent years. These network simulations not only provide support for experimentally gathered information but also as testing benchmarks in the future for several network-related queries such as pharmaceutical target testing and for systemic interrogation of cellular-level abnormalities in pathophysiological conditions (<xref ref-type="bibr" rid="bib64">Gambazzi et al., 2010</xref>; <xref ref-type="bibr" rid="bib92">Kerr et al., 2013</xref>; <xref ref-type="bibr" rid="bib134">Neymotin et al., 2016a</xref>, <xref ref-type="bibr" rid="bib149">Sanjay, 2017</xref>; <xref ref-type="bibr" rid="bib55">Domanski et al., 2019</xref>; <xref ref-type="bibr" rid="bib190">Zhang and Santaniello, 2019</xref>; <xref ref-type="bibr" rid="bib106">Liou et al., 2020</xref>). However, widespread adaptation of large-scale network simulations is hindered by the computational demand of these models that can only be satisfied by the employment of supercomputer clusters (<xref ref-type="bibr" rid="bib115">Markram et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">Bezaire et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>). Because these resources are expensive, they do not constitute a justifiable option for general practice. Importantly, we have shown that ANNs can provide a suitable alternative to traditional modeling systems, and that their simulation runtimes are also superior due to the structure of the machine learning platform (i.e., Tensorflow).</p><p>Traditional model systems linearly increase the number of equations to be solved for parallelly simulated cells, while ANNs can handle cells belonging to the same cell type on the same ANN graph (<xref ref-type="bibr" rid="bib54">Dillon, 2017</xref>). In our network models (150 cells; <xref ref-type="fig" rid="fig8">Figure 8</xref>), NEURON simulations yield 150 times more linear equations for every time step, while ANNs used the same graph for all simulated cells. This property of ANNs in particular suits biological networks consisting of many cells. For example, the Allen Institute reported a computational model of the mouse V1 cortical area (<xref ref-type="bibr" rid="bib23">Billeh et al., 2020</xref>), consisting of 114 models corresponding to 17 different cell types (with the number of cells corresponding to these cell types ranging from hundreds to more than 10,000), which means that simulation of a complete cortical area is feasible using only 114 ANNs. We have demonstrated that even for small networks consisting of only 150 cells of the same type ANNs are more than four orders of magnitude faster compared to model environments used in the aforementioned V1 simulations. As large-scale network simulations are typically run using several thousand CPU cores in parallel, the predicted runtime acceleration suggests that network simulations relying on ANNs could negate the need for supercomputers. Instead, ANN-equivalent models could be run on commercially available computational resources such as personal computers with reasonable time frames.</p><p>Another advantage of our approach is the utilization of GPU processing, which provides a substantially larger number of processing cores (<xref ref-type="bibr" rid="bib12">Asano et al., 2009</xref>; <xref ref-type="bibr" rid="bib120">Memon et al., 2017</xref>). The runtime differences are observable by comparing CNN-LSTM simulations on CPU and GPU (<xref ref-type="fig" rid="fig7">Figure 7B and C</xref>), which yields more than an order of magnitude faster simulations on GPU in the case of small-size networks (50 cells) and approximately two orders of magnitude difference for mid-sized networks. Our results demonstrate that cortical PC network simulations are at least four orders of magnitude faster than traditional modeling environments, confirming that disparities in the number of cores can only partially account for the observed ANN runtime acceleration. Furthermore, the NEURON simulation environment does not benefit as much from GPU processing as for ANN simulations (<xref ref-type="bibr" rid="bib182">Vooturi et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Kumbhar et al., 2019</xref>). These results confirm that the drastic runtime acceleration is the direct consequence of the parallelized graph-based ANN approach.</p></sec><sec id="s3-3"><title>Efficient mapping of network parameter involvement in complex pathophysiological conditions</title><p>To demonstrate the superiority of ANNs in a biologically relevant network simulation, we mapped the effects of variable network parameters observed in Rett syndrome. Rett syndrome is a neurodevelopmental disorder leading to a loss of cognitive and motor functions, impaired social interactions, and seizures in young females due to loss-of-function mutations in the X-linked <italic>MeCP2</italic> gene (<xref ref-type="bibr" rid="bib33">Chahrour and Zoghbi, 2007</xref>). Like many brain diseases, these behavioral alterations are likely due to changes in several different synaptic and circuit parameters. MeCP2-deficient mice exhibit multiple changes in synaptic communication, affecting both excitatory and inhibitory neurotransmission and circuit-level connectivity. Excitatory transmission is bidirectionally modulated by <italic>MeCP2</italic> knockout (<xref ref-type="bibr" rid="bib133">Nelson et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Chao et al., 2007</xref>) and overexpression (<xref ref-type="bibr" rid="bib130">Na et al., 2012</xref>), and long-term synaptic plasticity is also impaired in MeCP2-deficient mice (<xref ref-type="bibr" rid="bib11">Asaka et al., 2006</xref>; <xref ref-type="bibr" rid="bib72">Guy et al., 2007</xref>). Inhibitory signaling is also altered in several different brain areas (<xref ref-type="bibr" rid="bib43">Dani et al., 2005</xref>; <xref ref-type="bibr" rid="bib118">Medrihan et al., 2008</xref>). Importantly, synaptic transmission is affected not only at the level of quantal parameters but also regarding synaptic connections as MeCP2 directly regulates the number of glutamatergic synapses (<xref ref-type="bibr" rid="bib34">Chao et al., 2007</xref>). This regulation amounts to a 39% reduction of putative excitatory synapses in the hippocampus (<xref ref-type="bibr" rid="bib34">Chao et al., 2007</xref>) and a 50% reduction in recurrent excitatory connections between L5 PCs (<xref ref-type="bibr" rid="bib44">Dani and Nelson, 2009</xref>). Here, we investigated how these diverse underlying mechanisms contribute to overall circuit pathology using our ANN network model approach.</p><p>We found that the ability of the network to respond to external stimuli is affected by both alterations in synaptic excitation and changes in the recurrent connectivity of L5 PCs. Our results suggest that disruption of inhibitory transmission is not necessary to elicit network instability in Rett as changes in synaptic excitation and recurrent connectivity alone were sufficient in destabilizing the network. These results are supported by previous findings showing that both constitutive (<xref ref-type="bibr" rid="bib28">Calfa et al., 2011</xref>) and excitatory-cell-targeted (<xref ref-type="bibr" rid="bib189">Zhang et al., 2014</xref>) <italic>MeCP2</italic> mutations lead to network seizure generation as opposed to inhibitory-cell-targeted <italic>MeCP2</italic> mutation, which causes frequent hyperexcitability discharges but never seizures (<xref ref-type="bibr" rid="bib35">Chao et al., 2010</xref>). Furthermore, our results suggest that excitatory synaptic alterations in Rett affect both general network responses and network stability, which may serve as substrates to cognitive dysfunction and seizures, respectively. Taken together, our results reveal how cellular-synaptic mechanisms may relate to symptoms at the behavioral level. Importantly, investigation of the multidimensional parameter space was made possible by the significantly reduced simulation times of our ANN as identical simulations with traditional modeling systems are proposed to be four orders of magnitude slower.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Single-compartmental NEURON simulation</title><p>Passive and active membrane responses to synaptic inputs were simulated in NEURON (<xref ref-type="bibr" rid="bib81">Hines and Carnevale, 1997</xref>, version 7.7, available at <ext-link ext-link-type="uri" xlink:href="http://www.neuron.yale.edu/neuron/">http://www.neuron.yale.edu/neuron/</ext-link>). Morphology (single compartment with length and diameter of 25 µm) and passive cellular parameters (<italic>R<sub>m</sub></italic>: 1 kΩ/cm<sup>2</sup>; <italic>C<sub>m</sub></italic>: 1 µF/cm<sup>2</sup>; <italic>R<sub>i</sub></italic>: 35.4 Ω/cm) were the same for both cases and resting membrane potential was set to –70 mV. Additionally, the built-in mixed sodium, potassium and leak channel (<xref ref-type="bibr" rid="bib89">Jaslove, 1992</xref>, based on the original Hodgkin–Huxley descriptions) was included in the active model (g<sub>Na</sub>: 0.12 pS/µm<sup>2</sup>; g<sub>K</sub>: 0.036 pS/µm<sup>2</sup>; g<sub>leak</sub>: 0.3 nS/µm<sup>2</sup>). Reversal potentials were set to 50 mV for sodium, –77 mV for potassium, and –54.3 mV for leak conductance. Simulations were run with a custom steady-state initialization procedure (<xref ref-type="bibr" rid="bib30">Carnevale and Hines, 2006</xref>) for 2 s, after which the temporal integration step size was set to 25 µs.</p><p>In order to simulate membrane responses to excitatory and inhibitory inputs, the built-in AlphaSynapse class of NEURON was used (excitatory synapse: <italic>τ</italic>: 2 ms; <italic>g<sub>pas</sub></italic>: 2.5 nS; <italic>E<sub>rev</sub></italic>: 0 mV; inhibitory synapse: <italic>τ</italic>: 1 ms; <italic>g<sub>pas</sub></italic>: 8 nS; <italic>E<sub>rev</sub></italic>: –90 mV). The number of synapses was determined by a pseudo-random uniform number generator (ratio of excitatory to inhibitory synapses: 8:3). Timing of individual synapses was also randomly picked from a uniform distribution. During the 10-s-long simulations, the membrane potential, I<sub>Na</sub>, and I<sub>K</sub> currents were recorded along with the input timings and weights and were subsequently saved to text files. Simulations were carried out in three different conditions. First, resting membrane potential was recorded without synaptic activity. Second, passive membrane potential was recorded. Third, active membrane potential responses were recorded with fixed synaptic weights.</p><p>The amount of training each ANN received varied widely, based on the complexity of the modeled system. We used model checkpoints to stop the training if the prediction error on the validation dataset did not improve within 20 training epochs. This checkpoint was reached between 12 and 24 hr, training on a single GPU.</p></sec><sec id="s4-2"><title>Multicompartmental NEURON simulation</title><p>Active multicompartmental simulations were carried out using an in vivo-labeled and fully reconstructed thick tufted cortical L5 PC (<xref ref-type="bibr" rid="bib74">Hallermann et al., 2012</xref>). The biophysical properties were unchanged, and a class representation was created for network simulations. Excitatory and inhibitory synapses were handled similarly to single-compartmental simulations. A total of 100 excitatory (<italic>τ</italic>: 1 ms; <italic>g<sub>pas</sub></italic>: 3.6 nS; <italic>E<sub>rev</sub></italic>: 0 mV) and 30 inhibitory synapses (<italic>τ</italic>: 1 ms; <italic>g<sub>pas</sub></italic>: 3 nS; <italic>E<sub>rev</sub></italic>: –90 mV) were placed on the apical, oblique, or tuft dendrites, and 50 excitatory and 20 inhibitory synapses were placed on basal dendrites. The placement of the synapses was governed by two uniform pseudo-random number generators, which selected dendritic segments weighed by their respective lengths and the location along the segment (ratio 2:1:1:1 for apical excitatory, apical inhibitory, basal excitatory, and basal inhibitory synapses). Simulations were carried out with varied synaptic weights and a wide range of synapse numbers.</p></sec><sec id="s4-3"><title>ANN benchmarking</title><p>MTSF models are ideal candidates for modeling neuronal behavior in a stepwise manner as they can be designed to receive information about past synaptic inputs and membrane potentials in order to predict subsequent voltage responses. These ANNs have recently been demonstrated to be superior to other algorithms in handling multivariate temporal data such as audio signals (<xref ref-type="bibr" rid="bib100">Kons and Toledo-Ronen, 2013</xref>), natural language (<xref ref-type="bibr" rid="bib39">Collobert and Weston, 2008</xref>), and various other types of fluctuating time-series datasets (<xref ref-type="bibr" rid="bib192">Zheng et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Che et al., 2018</xref>; <xref ref-type="bibr" rid="bib191">Zhang et al., 2019</xref>). To validate the overall suitability of different ANN architectures tested in this article for MTSF, we used a weather time-series dataset recorded by the Max Planck Institute for Biogeochemistry. The dataset contains 14 different features, including humidity, temperature, and atmospheric pressure collected every 10 min. The dataset was prepared by François Chollet for his book <italic>Deep Learning with Python</italic> (dataset preparation steps can be found on the Tensorflow website: <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/tutorials/structured_data/time_series">https://www.tensorflow.org/tutorials/structured_data/time_series</ext-link>). All ANN architectures were implemented using the Keras deep-learning API (<ext-link ext-link-type="uri" xlink:href="https://keras.io/">https://keras.io/</ext-link>) of the Tensorflow open-source library (version 2.3, <xref ref-type="bibr" rid="bib1">Abadi, 2015</xref>;, <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link>), with Python 3.7.</p><p>The first architecture we implemented was a simple linear model consisting of three layers without activation functions; a Flatten layer, a Dense (fully connected) layer with 64 units, and a Dense layer with 3 units. The second architecture was a linear model with added nonlinear processing. The model contained three layers identical to the linear model, but the second layer had a sigmoid activation function. The third model was a deep neural net with mixed linear and nonlinear layers. Similar to the first two models, this architecture had a Flatten layer and a Dense layer with 64 units as the first two layers, followed by nine Dense layers (units 128, 256, 512, 1024, 1024, 512, 256, 128, and 64 for the nine Dense layers) with hyperbolic tangent (tanh) activation function and Dropout layers with 0.15 dropout rate. The last layer was the same Dense layer with three units as in case of the linear and nonlinear models. The fourth model was a modified version of the WaveNet architecture introduced in 2016 (<xref ref-type="bibr" rid="bib141">Oord, 2016</xref>), implemented based on a previous publication (<xref ref-type="bibr" rid="bib19">Beniaguev et al., 2021</xref>). The fifth and final architecture was a convolutional LSTM model (<xref ref-type="bibr" rid="bib56">Donahue et al., 2015</xref>) that consists of three distinct functional layer segments. The lowest layers (close to the input layer) were three, one-dimensional convolutional layers (Conv1D) with 128, 100, and 50 units, and causal padding for temporal data processing. The first and third layers had a kernel size of 1, and the second layer had a kernel size of 5. The first two layers had ‘rectified linear unit’ (relu) activation functions, and the third layer had tanh activation; therefore, the first two layers were initialized by He-uniform variance scaling initializers (<xref ref-type="bibr" rid="bib79">He et al., 2015</xref>), while the third layer was initialized by Glorot-uniform initialization (also known as Xavier uniform initialization) (<xref ref-type="bibr" rid="bib67">Glorot, 2011</xref>). After flattening and repeating the output of this functional unit, a single LSTM layer (<xref ref-type="bibr" rid="bib84">Hochreiter and Schmidhuber, 1997</xref>) handled the arriving input, providing recurrent information processing. This layer had 128 units, tanh activation function, Glorot-uniform initialization, and was tasked to return sequences instead of the last output. The final functional unit was composed of four Dense layers with 100 units, scaled exponential linear unit (selu) activations, and accordingly, LeCun-uniform initializations (<xref ref-type="bibr" rid="bib127">Montavon et al., 2012</xref>). The dropout rate between Dense layers was set to 0.15.</p><p>All benchmarked architectures were compiled and fitted with the same protocol. During compiling, the loss function was set to calculate mean squared error and the Adam algorithm (<xref ref-type="bibr" rid="bib95">Kingma and Ba, 2014</xref>) was chosen as the optimizer. The maximum number of epochs was set to 20; however, an early stopping protocol was defined to have a patience of 10, which was reached in all cases.</p></sec><sec id="s4-4"><title>Single-compartmental simulation representation with ANNs</title><p>As neural nets favor processed data scaled between –1 and 1 or 0 and 1, we normalized the recorded membrane potentials and ionic currents. Due to the 1 Hz recording frequency, AP amplitudes were variable beyond physiologically plausible ranges; therefore, peak amplitudes were standardized. The trainable time-series data was consisting of 64-ms-long input matrices with three or five columns (corresponding to membrane potential, excitatory input, inhibitory input, and optionally I<sub>Na</sub> and I<sub>K</sub> current recordings) and target sequences were vectors with one or three elements (membrane potential and optional ionic currents). Training, testing, and validation datasets were created by splitting time-series samples 80-10–10%.</p><p>Benchmarking the five different ANN architectures proved that these models can handle time-series data predicting with similar accuracy; however, in order to obtain the best results, several optimization steps of the hyperparameter space were undertaken. Unless stated otherwise, layer and optimization parameters were unchanged compared to benchmarking procedures. First, linear models were created without a Flatten layer, instead of which a TimeDistributed wrapper was applied to the first Dense layer. The same changes were employed in case of the nonlinear model and the deep neural net. The fourth, convolutional model had 12 Conv1D layers with 128 filters, kernel size of 2, causal padding tanh activation function and dilatation rates constantly increasing by 2<sup>n</sup>. We found that the best optimization algorithm for passive and active membrane potential prediction is the Adam optimizer accelerated with Nesterov momentum (<xref ref-type="bibr" rid="bib57">Dozat, 2015</xref>), with gradient clipping set to 1. Although mean absolute error and mean absolute percentage error were sufficient for passive membrane potential prediction, the active version warranted the usage of mean squared error in order to put emphasis on APs. We found out that the mechanistic inference of the full dynamic range of simulated neurons was a hard task for ANNs; therefore, we sequentially trained these models in a specific order. First, we taught the resting membrane potential by supplying voltage recordings with only a few or no synaptic inputs. This step was also useful to learn the isolated shapes of certain inputs. Second, we supplied highly active subthreshold membrane traces to the models and finally inputted suprathreshold membrane potential recordings. During the subsequent training steps, previous learning phases were mixed into the new training dataset in order to avoid the catastrophic forgetting of gradient-based neural networks (<xref ref-type="bibr" rid="bib68">Goodfellow, 2015</xref>).</p><p>During altered excitation–inhibition ratios, the previously constructed single-compartmental model was used without modifications in layer weights and biases. Firing responses were fitted with different curves, a linear model,<disp-formula id="equ1"><mml:math id="m1"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:math></disp-formula></p><p>which could account for either subtractive or divisive inhibition (<xref ref-type="bibr" rid="bib22">Bhatia et al., 2019</xref>), and a logistic curve,<disp-formula id="equ2"><mml:math id="m2"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:math></disp-formula></p><p>representing divisive normalization. Although the latter arithmetic operation is often approximated by an exponential curve, we felt the necessity to account for datapoints without spiking.</p><p>In experiments aimed at quantifying the effect of biophysical modifications of delayed rectifier potassium conductances, left- and right-shifted models were compared to control conditions point-by-point upon identical synaptic input streams, and the deviation from control conditions was expressed as absolute difference, measured in millivolts.</p><p>NMDA point-process model was constructed as a compound model consisting of an AMPA and an NMDA segment, both of which were designed based on NEURON’s built-in AlphaSynapse class. The logic of the model was based on a previous publication (<xref ref-type="bibr" rid="bib94">Kim et al., 2013</xref>), where the AMPA model was only dependent on local membrane potential, while the NMDA model had an additional constraining Boltzmann function for gating voltage-dependent activation. The ANN was trained on several datasets having consistently higher randomly distributed synaptic inputs. The training dataset did not contain activity patterns tested in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The training dataset consisted of an nX4 matrix, where the columns were membrane voltage, AMPA conductance, NMDA conductance, and inhibitory conductance. In the training dataset, AMPA and NMDA synapses were applied independently, and the Boltzmann function of NMDA was omitted. After the model learned the correct representation of NDMA activations, a hand-crafted layer was inserted into the ANN, which recalculated the conductance maximum of NMDA in accordance with the instantaneous membrane potential. Specifically, the function was expressed as<disp-formula id="equ3"><mml:math id="m3"><mml:mi>g</mml:mi><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:math></disp-formula></p><p>where <italic>A</italic>1 is 1, <italic>A2</italic> is –1, <italic>v</italic> denotes membrane potential, <italic>x</italic>0 is set to –63.32 in NEURON and 1.44 in the ANN, while <italic>dx</italic> is 0.013 in NEURON and 0.12 in the ANN.</p></sec><sec id="s4-5"><title>CCN-LSTM for multicompartmental simulation representation</title><p>Data preprocessing was done as described for single-compartmental representations. Time-series data for CNN-LSTM input was prepared as matrices having 201 rows for membrane potential and 200 synapse vectors, and 64 rows (64-ms-long input). The CNN-LSTM architecture consisted of three Conv1d layers (512, 256, and 128 units), a Flatten layer, a RepeatVector, three LSTM layers (128 units each), and six Dense layers (128, 100, 100, 100, 100, 1 units). Activation functions and initializations were similar to the CNN-LSTM described above, with the exception of the first Dense layer, which included the relu activation function and He-uniform initialization. Additionally, Lasso regularization (<xref ref-type="bibr" rid="bib151">Santosa and Symes, 1986</xref>) was applied to the first Conv1D layer. We found that the best optimizer for our purposes was a variant of the Adam optimizer based on the infinity norm, called Adamax (<xref ref-type="bibr" rid="bib95">Kingma and Ba, 2014</xref>). Due to the non-normal distribution of the predicted membrane potentials, an inherent bias was present in our results, which was scaled by either an additional bias term, or a nonlinear function transformation.</p><p>Network construction was based on a previous publication (<xref ref-type="bibr" rid="bib78">Hay and Segev, 2015</xref>). Briefly, 150 L5 PC were simulated in a network with varying unidirectional connectivity, and bidirectional connectivity proportional to the unidirectional connectivity (P<sub>bidirectional</sub> = 0.5 * P<sub>unidirecional</sub>). Reciprocal connections were 1.5 times stronger than unidirectional connections. In order to implement connectivity, a connection matrix was created, where presynaptic cells corresponded to the rows, and postsynaptic cells corresponded to the columns of the matrix. If there was a connection between two cells, the appropriate element of the matrix was set to 1, otherwise the matrix contained zeros. Next, cells were initialized with random input matrices. After a prediction was made for the subsequent membrane potential values, every cell was tested for suprathreshold activity. Upon spiking, rows of the connectivity matrix corresponding to the firing cells were selected, and the input matrices of the postsynaptic cells were supplemented with <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> corresponds to the element of the connectivity matrix for presynaptic cell <italic>i</italic>, and postsynaptic cells <italic>j</italic>, and <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refers to the conductance of the synapses between two connected cells. As this step is carried out upon presynaptic spiking, regardless of whether two cells are connected or not (<inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be 0 or 1), the degree of connectivity does not influence simulation runtimes.</p><p>The delay between presynaptic AP at the soma and the onset of the postsynaptic response was 1 ms measured from the AP peak as the network simulations represent local circuit activity. If the simulated network is made to include spatially circuit components with more variability in their synaptic delays, to account for their spatial segregation, a buffer matrix must be created. The aim of this buffer matrix is to contain synaptic conductance values upon AP detection from the presynaptic cells, without immediately posting it on the input matrices of postsynaptic cells. Each connection consisted of five proximal contact sites. Compared to the original publication, we modified the parameters of the Tsodyks–Markram model (<xref ref-type="bibr" rid="bib176">Tsodyks and Markram, 1997</xref>) used to govern synaptic transmission and plasticity. Based on a recent publication (<xref ref-type="bibr" rid="bib14">Barros-Zulaica et al., 2019</xref>), we set U (fraction on synaptic resources used by a single spike) to 0.38, D (time constant for recovery from depression) to 365.6, and F (time constant for recovery from facilitation) to 25.71. The simulation was run for 250 or 300 ms, which consisted of a pre-stimuli period (to observe the occurrence of structured activity patterns) for 100 ms, and a post-stimuli period (to quantify network amplification). The stimulus itself consisted of a strong excitatory input (can be translated to 50 nS) delivered to a proximal dendritic segment, calibrated to elicit APs from all 150 cells in a 10-ms-long time window. Scaling of inhibitory inputs was carried out by changing inhibitory quantal size of background inputs, while scaling of excitatory drive affected quantal size of recurrent synaptic connections as well.</p></sec><sec id="s4-6"><title>Custom top layers</title><p>We created custom top layers operating on the output layer of the CNN-LSTM in two different configurations, First, the ‘custom Izhikevich layer’ was implemented using the ‘CustomLayer’ class of Tensorflow. The internal variables and governing functions were implemented based on the original description of this model (<xref ref-type="bibr" rid="bib87">Izhikevich, 2003</xref>). Briefly, the layer calculates the values of v and u dimensionless variables (v represents membrane potential, and u represents a membrane recovery variable), based on a, b, c, and d dimensionless parameters (a corresponds to the timescale of u, b sets the sensitivity of u, c describes the after-spike reset value of v, and d sets the after-spike reset value of u). Additionally, we set dt (time step) parameter free as it was necessary for accounting for the membrane time constant. Due to the low number of trainable parameters, this layer can be fitted with conventional fitting algorithms, such as the Nelder–Mead minimalization (<xref ref-type="bibr" rid="bib162">Singer and Nelder, 2009</xref>), available in the ‘scipy’ package of Python. As the Izhikevich equations require information about the state of both u and v variable, yet the CNN-LSTM only predicts v, this layer requires inputs from two sources, v coming from the CNN-LSTM and u coming from previous predictions of the custom layer, directly bypassing the CNN-LSTM. Therefore, the previously used Sequential Application Programming Interface (API) of Tensorflow was discarded in favor of the Functional API. As the equations governing v and u require current as input, not voltage, the CNN-LSTM in this case needs to be tasked with solving for synaptic (and subsequent membrane) current. Consequently, to gauge the upper limits of this method, we administered a synaptic current waveform as input during layer evaluation.</p><p>The second approach we took for custom top layer creation involved a more conventional route, where recurrent encoder (stacked LSTM layers having first decreasing and then increasing number of units) were constructed, operating on a longer batch of CNN-LSTM predictions. Specifically, the encoder responsible for fluorescent calcium signal generation took 3 s of voltage input, while the voltage reporter encoder and decoder operated on 1024 ms of signal input.</p></sec><sec id="s4-7"><title>Computational resources</title><p>We used several different commercially available and free-to-use computational resources to demonstrate the attainableness of large network simulations using neural networks. Single-compartmental NEURON simulations were carried out on a single CPU (Intel Core i7-5557U CPU @3.1 GHz), equipped with four logical processors and two cores. Python had access to the entirety of the CPU; however, no explicit attempts were made to enable code parallelization. To test runtimes on a CPU, only a single core was used. For multicompartmental NEURON simulations, we used the publicly available National Science Foundation-funded High Performance Computing resource via the Neuroscience Gateway (<xref ref-type="bibr" rid="bib163">Sivagnanam et al., 2013</xref>). This resource was only used to generate training datasets. Speed comparison using CPUs was always carried out on the aforementioned single CPU. In contrast to NEURON models, ANN calculations are designed to run on GPUs rather than CPUs. Therefore, ANN models were run on the freely accessible Google Collaboratory GPUs (NVIDIA Tesla K80), Google Collaboratory TPUs (designed for handling tensor calculations typically created by Tensorflow) or a single high-performance GPU (GeForce GTX 1080 Ti). For speed comparisons, we ran these models on a single Google Collaboratory CPU (Intel Xeon, not specified, @2.2 GHz) and the previously mentioned single CPU as well. During NEURON and ANN simulations, parallelization was only employed for Neuroscience Gateway simulations and ANN fitting.</p></sec><sec id="s4-8"><title>Statistics</title><p>Averages of multiple measurements are presented as mean ± SD. Data were statistically analyzed by ANOVA test using Origin software and custom-written Python scripts. Normality of the data was analyzed with Shapiro–Wilk test. Explained variance was quantified as 1 minus the fitting error normalized by the variance of the signal (<xref ref-type="bibr" rid="bib178">Ujfalussy et al., 2018</xref>). For accuracy measurements, APs were counted within a 10 ms time window as true-positive APs. Precision and recall were calculated based on the following equations:<disp-formula id="equ4"><mml:math id="m4"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where FP in the false-positive rate and FN is the false-negative rate.</p></sec><sec id="s4-9"><title>Data and software availability</title><p>All codes used for simulating single- and multicompartmental NEURON models for training dataset creation, ANN benchmarking, ANN representations, and the L5 microcircuit are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViktorJOlah/Neuro_ANN">https://github.com/ViktorJOlah/Neuro_ANN</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9a4bb33ea65af35384c87764f456648c12f08d78;origin=https://github.com/ViktorJOlah/Neuro_ANN;visit=swh:1:snp:dfa0fca6e35143608ab67fcea4ae9431cd7f7cf5;anchor=swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f">swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f</ext-link>; <xref ref-type="bibr" rid="bib140">Oláh, 2022</xref>) and Dryad.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Funding acquisition, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Formal analysis, Funding acquisition, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Software, Funding acquisition, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-79535-transrepform1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code used for simulating single and multicompartmental NEURON models, ANN benchmarking, ANN representations, and the layer 5 microcircuit are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViktorJOlah/Neuro_ANN">https://github.com/ViktorJOlah/Neuro_ANN</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9a4bb33ea65af35384c87764f456648c12f08d78;origin=https://github.com/ViktorJOlah/Neuro_ANN;visit=swh:1:snp:dfa0fca6e35143608ab67fcea4ae9431cd7f7cf5;anchor=swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f">swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f</ext-link>) and Dryad (doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.0cfxpnw60">https://doi.org/10.5061/dryad.0cfxpnw60</ext-link>). To adhere with eLife data availability policies, we also uploaded all data points displayed in the text and figures, on Dryad (doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.0cfxpnw60">https://doi.org/10.5061/dryad.0cfxpnw60</ext-link>) in compliance with FAIR (Findable, Accessible, Interoperable, Reusable) principles.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Oláh</surname><given-names>VJ</given-names></name><name><surname>Pedersen</surname><given-names>NP</given-names></name><name><surname>Rowan</surname><given-names>MJM</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Ultrafast simulation of large-scale neocortical microcircuitry with biophysically realistic neurons</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.0cfxpnw60</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH grants R56-AG072473 (MJMR) and the Emory Alzheimer’s Disease Research Center Grant 00100569 (MJMR) with partial support (NPP) provided by CURE Epilepsy and the National Institutes of Health K08NS105929.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akbarian</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>RZ</given-names></name><name><surname>Gribnau</surname><given-names>J</given-names></name><name><surname>Rasmussen</surname><given-names>TP</given-names></name><name><surname>Fong</surname><given-names>H</given-names></name><name><surname>Jaenisch</surname><given-names>R</given-names></name><name><surname>Jones</surname><given-names>EG</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Expression pattern of the rett syndrome gene mecp2 in primate prefrontal cortex</article-title><source>Neurobiology of Disease</source><volume>8</volume><fpage>784</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1006/nbdi.2001.0420</pub-id><pub-id pub-id-type="pmid">11592848</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alwosheel</surname><given-names>A</given-names></name><name><surname>van Cranenburgh</surname><given-names>S</given-names></name><name><surname>Chorus</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Is your dataset big enough? sample size requirements when using artificial neural networks for discrete choice analysis</article-title><source>Journal of Choice Modelling</source><volume>28</volume><fpage>167</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.jocm.2018.07.002</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amsalem</surname><given-names>O</given-names></name><name><surname>Eyal</surname><given-names>G</given-names></name><name><surname>Rogozinski</surname><given-names>N</given-names></name><name><surname>Gevaert</surname><given-names>M</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An efficient analytical reduction of detailed nonlinear neuron models</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-13932-6</pub-id><pub-id pub-id-type="pmid">31941884</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Antolík</surname><given-names>J</given-names></name><name><surname>Monier</surname><given-names>C</given-names></name><name><surname>Frégnac</surname><given-names>Y</given-names></name><name><surname>Davison</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A Comprehensive Data-Driven Model of Cat Primary Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/416156</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aradi</surname><given-names>I</given-names></name><name><surname>Holmes</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Role of multiple calcium and calcium-dependent conductances in regulation of hippocampal dentate granule cell excitability</article-title><source>Journal of Computational Neuroscience</source><volume>6</volume><fpage>215</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1023/a:1008801821784</pub-id><pub-id pub-id-type="pmid">10406134</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Gratiy</surname><given-names>S</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Buice</surname><given-names>M</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>da Costa</surname><given-names>N</given-names></name><name><surname>de Vries</surname><given-names>S</given-names></name><name><surname>Denman</surname><given-names>D</given-names></name><name><surname>Durand</surname><given-names>S</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Soler-Llavina</surname><given-names>G</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual physiology of the layer 4 cortical circuit in silico</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006535</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006535</pub-id><pub-id pub-id-type="pmid">30419013</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Review of rett syndrome</article-title><source>Journal of Neuropathology and Experimental Neurology</source><volume>56</volume><fpage>843</fpage><lpage>849</lpage><pub-id pub-id-type="doi">10.1097/00005072-199708000-00001</pub-id><pub-id pub-id-type="pmid">9258253</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neuropathology of Rett syndrome</article-title><source>Mental Retardation and Developmental Disabilities Research Reviews</source><volume>8</volume><fpage>72</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1002/mrdd.10027</pub-id><pub-id pub-id-type="pmid">12112730</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuropathology of Rett syndrome</article-title><source>Journal of Child Neurology</source><volume>20</volume><fpage>747</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1177/08830738050200090901</pub-id><pub-id pub-id-type="pmid">16225830</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaka</surname><given-names>Y</given-names></name><name><surname>Jugloff</surname><given-names>DGM</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Eubanks</surname><given-names>JH</given-names></name><name><surname>Fitzsimonds</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Hippocampal synaptic plasticity is impaired in the MECP2-null mouse model of Rett syndrome</article-title><source>Neurobiology of Disease</source><volume>21</volume><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.nbd.2005.07.005</pub-id><pub-id pub-id-type="pmid">16087343</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Asano</surname><given-names>S</given-names></name><name><surname>Maruyama</surname><given-names>T</given-names></name><name><surname>Yamaguchi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>2009 International Conference on Field Programmable Logic and Applications (FPL)</article-title><conf-name>Performance comparison of FPGA, GPU and CPU in image processing</conf-name><pub-id pub-id-type="doi">10.1109/FPL.2009.5272532</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ascoli</surname><given-names>GA</given-names></name><name><surname>L.</surname><given-names>AN</given-names></name><name><surname>Anderson</surname><given-names>SA</given-names></name><name><surname>Barrionuevo</surname><given-names>G</given-names></name><name><surname>Benavides-Piccione</surname><given-names>R</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Cauli</surname><given-names>B</given-names></name><name><surname>DeFelipe</surname><given-names>J</given-names></name><name><surname>Fairén</surname><given-names>A</given-names></name><name><surname>Feldmeyer</surname><given-names>D</given-names></name><name><surname>Fishell</surname><given-names>G</given-names></name><name><surname>Fregnac</surname><given-names>Y</given-names></name><name><surname>Freund</surname><given-names>TF</given-names></name><name><surname>Gardner</surname><given-names>D</given-names></name><name><surname>Gardner</surname><given-names>EP</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name><name><surname>Hestrin</surname><given-names>S</given-names></name><name><surname>Karube</surname><given-names>F</given-names></name><name><surname>Kisvárday</surname><given-names>ZF</given-names></name><name><surname>Lambolez</surname><given-names>B</given-names></name><name><surname>Lewis</surname><given-names>DA</given-names></name><name><surname>Marin</surname><given-names>O</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Muñoz</surname><given-names>A</given-names></name><name><surname>Packer</surname><given-names>A</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name><name><surname>Rockland</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Petilla terminology: nomenclature of features of gabaergic interneurons of the cerebral cortex</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>557</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1038/nrn2402</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barros-Zulaica</surname><given-names>N</given-names></name><name><surname>Rahmon</surname><given-names>J</given-names></name><name><surname>Chindemi</surname><given-names>G</given-names></name><name><surname>Perin</surname><given-names>R</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Ramaswamy</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Estimating the readily-releasable vesicle pool size at synaptic connections in the neocortex</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>11</volume><elocation-id>29</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2019.00029</pub-id><pub-id pub-id-type="pmid">31680928</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartos</surname><given-names>M</given-names></name><name><surname>Vida</surname><given-names>I</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Meyer</surname><given-names>A</given-names></name><name><surname>Monyer</surname><given-names>H</given-names></name><name><surname>Geiger</surname><given-names>JRP</given-names></name><name><surname>Jonas</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fast synaptic inhibition promotes synchronized gamma oscillations in hippocampal interneuron networks</article-title><source>PNAS</source><volume>99</volume><fpage>13222</fpage><lpage>13227</lpage><pub-id pub-id-type="doi">10.1073/pnas.192233099</pub-id><pub-id pub-id-type="pmid">12235359</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belichenko</surname><given-names>PV</given-names></name><name><surname>Oldfors</surname><given-names>A</given-names></name><name><surname>Hagberg</surname><given-names>B</given-names></name><name><surname>Dahlström</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Rett syndrome: 3-D confocal microscopy of cortical pyramidal dendrites and afferents</article-title><source>Neuroreport</source><volume>5</volume><fpage>1509</fpage><lpage>1513</lpage><pub-id pub-id-type="doi">10.1097/00001756-199407000-00025</pub-id><pub-id pub-id-type="pmid">7948850</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belichenko</surname><given-names>P</given-names></name><name><surname>Wright</surname><given-names>EE</given-names></name><name><surname>Belichenko</surname><given-names>NP</given-names></name><name><surname>Masliah</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>HH</given-names></name><name><surname>Mobley</surname><given-names>WC</given-names></name><name><surname>Francke</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Widespread changes in dendritic and axonal morphology in mecp2-mutant mouse models of rett syndrome: evidence for disruption of neuronal networks</article-title><source>The Journal of Comparative Neurology</source><volume>514</volume><fpage>240</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1002/cne.22009</pub-id><pub-id pub-id-type="pmid">19296534</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Shalom</surname><given-names>R</given-names></name><name><surname>Ladd</surname><given-names>A</given-names></name><name><surname>Artherya</surname><given-names>NS</given-names></name><name><surname>Cross</surname><given-names>C</given-names></name><name><surname>Kim</surname><given-names>KG</given-names></name><name><surname>Sanghevi</surname><given-names>H</given-names></name><name><surname>Korngreen</surname><given-names>A</given-names></name><name><surname>Bouchard</surname><given-names>KE</given-names></name><name><surname>Bender</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>NeuroGPU: accelerating multi-compartment, biophysically detailed neuron simulations on gpus</article-title><source>Journal of Neuroscience Methods</source><volume>366</volume><elocation-id>109400</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2021.109400</pub-id><pub-id pub-id-type="pmid">34728257</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beniaguev</surname><given-names>D</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>London</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Single cortical neurons as deep artificial neural networks</article-title><source>Neuron</source><volume>109</volume><fpage>2727</fpage><lpage>2739</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.002</pub-id><pub-id pub-id-type="pmid">34380016</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benitez</surname><given-names>JM</given-names></name><name><surname>Castro</surname><given-names>JL</given-names></name><name><surname>Requena</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Are artificial neural networks black boxes?</article-title><source>IEEE Transactions on Neural Networks</source><volume>8</volume><fpage>1156</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1109/72.623216</pub-id><pub-id pub-id-type="pmid">18255717</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bezaire</surname><given-names>MJ</given-names></name><name><surname>Raikov</surname><given-names>I</given-names></name><name><surname>Burk</surname><given-names>K</given-names></name><name><surname>Vyas</surname><given-names>D</given-names></name><name><surname>Soltesz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Interneuronal mechanisms of hippocampal theta oscillations in a full-scale model of the rodent CA1 circuit</article-title><source>eLife</source><volume>5</volume><elocation-id>e18566</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18566</pub-id><pub-id pub-id-type="pmid">28009257</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhatia</surname><given-names>A</given-names></name><name><surname>Moza</surname><given-names>S</given-names></name><name><surname>Bhalla</surname><given-names>US</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Precise excitation-inhibition balance controls gain and timing in the hippocampus</article-title><source>eLife</source><volume>8</volume><elocation-id>e43415</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43415</pub-id><pub-id pub-id-type="pmid">31021319</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Cai</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Systematic integration of structural and functional data into multi-scale models of mouse primary visual cortex</article-title><source>Neuron</source><volume>106</volume><fpage>388</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.040</pub-id><pub-id pub-id-type="pmid">32142648</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branco</surname><given-names>T</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Synaptic integration gradients in single cortical pyramidal cell dendrites</article-title><source>Neuron</source><volume>69</volume><fpage>885</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.006</pub-id><pub-id pub-id-type="pmid">21382549</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive exponential integrate-and-fire model as an effective description of neuronal activity</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>3637</fpage><lpage>3642</lpage><pub-id pub-id-type="doi">10.1152/jn.00686.2005</pub-id><pub-id pub-id-type="pmid">16014787</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunner</surname><given-names>J</given-names></name><name><surname>Szabadics</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Analogue modulation of back-propagating action potentials enables dendritic hybrid signalling</article-title><source>Nature Communications</source><volume>7</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/ncomms13033</pub-id><pub-id pub-id-type="pmid">27703164</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname><given-names>PC</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Reduced compartmental models of neocortical pyramidal cells</article-title><source>Journal of Neuroscience Methods</source><volume>46</volume><fpage>159</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/0165-0270(93)90151-g</pub-id><pub-id pub-id-type="pmid">8474259</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calfa</surname><given-names>G</given-names></name><name><surname>Hablitz</surname><given-names>JJ</given-names></name><name><surname>Pozzo-Miller</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Network hyperexcitability in hippocampal slices from MeCP2 mutant mice revealed by voltage-sensitive dye imaging</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>1768</fpage><lpage>1784</lpage><pub-id pub-id-type="doi">10.1152/jn.00800.2010</pub-id><pub-id pub-id-type="pmid">21307327</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carnevale</surname><given-names>NT</given-names></name><name><surname>Hines</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The NEURON Book</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511541612</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelvecchi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Can we open the black box of AI?</article-title><source>Nature</source><volume>538</volume><fpage>20</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1038/538020a</pub-id><pub-id pub-id-type="pmid">27708329</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chadderdon</surname><given-names>GL</given-names></name><name><surname>Mohan</surname><given-names>A</given-names></name><name><surname>Suter</surname><given-names>BA</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>Kerr</surname><given-names>CC</given-names></name><name><surname>Francis</surname><given-names>JT</given-names></name><name><surname>Shepherd</surname><given-names>GMG</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Motor cortex microcircuit simulation based on brain activity mapping</article-title><source>Neural Computation</source><volume>26</volume><fpage>1239</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00602</pub-id><pub-id pub-id-type="pmid">24708371</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chahrour</surname><given-names>M</given-names></name><name><surname>Zoghbi</surname><given-names>HY</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The story of Rett syndrome: from clinic to neurobiology</article-title><source>Neuron</source><volume>56</volume><fpage>422</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.001</pub-id><pub-id pub-id-type="pmid">17988628</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>H-T</given-names></name><name><surname>Zoghbi</surname><given-names>HY</given-names></name><name><surname>Rosenmund</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mecp2 controls excitatory synaptic strength by regulating glutamatergic synapse number</article-title><source>Neuron</source><volume>56</volume><fpage>58</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.018</pub-id><pub-id pub-id-type="pmid">17920015</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>H-T</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Samaco</surname><given-names>RC</given-names></name><name><surname>Xue</surname><given-names>M</given-names></name><name><surname>Chahrour</surname><given-names>M</given-names></name><name><surname>Yoo</surname><given-names>J</given-names></name><name><surname>Neul</surname><given-names>JL</given-names></name><name><surname>Gong</surname><given-names>S</given-names></name><name><surname>Lu</surname><given-names>H-C</given-names></name><name><surname>Heintz</surname><given-names>N</given-names></name><name><surname>Ekker</surname><given-names>M</given-names></name><name><surname>Rubenstein</surname><given-names>JLR</given-names></name><name><surname>Noebels</surname><given-names>JL</given-names></name><name><surname>Rosenmund</surname><given-names>C</given-names></name><name><surname>Zoghbi</surname><given-names>HY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dysfunction in GABA signalling mediates autism-like stereotypies and Rett syndrome phenotypes</article-title><source>Nature</source><volume>468</volume><fpage>263</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1038/nature09582</pub-id><pub-id pub-id-type="pmid">21068835</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Petrantonakis</surname><given-names>PC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dendrites of dentate gyrus granule cells contribute to pattern separation by controlling sparsity</article-title><source>Hippocampus</source><volume>27</volume><fpage>89</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1002/hipo.22675</pub-id><pub-id pub-id-type="pmid">27784124</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Che</surname><given-names>Z</given-names></name><name><surname>Purushotham</surname><given-names>S</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Sontag</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Recurrent neural networks for multivariate time series with missing values</article-title><source>Scientific Reports</source><volume>8</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-24271-9</pub-id><pub-id pub-id-type="pmid">29666385</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T-W</given-names></name><name><surname>Wardill</surname><given-names>TJ</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Pulver</surname><given-names>SR</given-names></name><name><surname>Renninger</surname><given-names>SL</given-names></name><name><surname>Baohan</surname><given-names>A</given-names></name><name><surname>Schreiter</surname><given-names>ER</given-names></name><name><surname>Kerr</surname><given-names>RA</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Kim</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Collobert</surname><given-names>R</given-names></name><name><surname>Weston</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A unified architecture for natural language processing: Deep neural networks with multitask learning</article-title><conf-name>Proceedings of the 25th international conference on Machine learning</conf-name><pub-id pub-id-type="doi">10.1145/1390156.1390177</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinople</surname><given-names>CM</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deep cortical layers are activated directly by thalamus</article-title><source>Science</source><volume>340</volume><fpage>1591</fpage><lpage>1594</lpage><pub-id pub-id-type="doi">10.1126/science.1236425</pub-id><pub-id pub-id-type="pmid">23812718</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutsuridis</surname><given-names>V.</given-names></name><name><surname>Wennekers</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hippocampus, microcircuits and associative memory</article-title><source>Neural Networks</source><volume>22</volume><fpage>1120</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2009.07.009</pub-id><pub-id pub-id-type="pmid">19647982</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutsuridis</surname><given-names>V</given-names></name><name><surname>Cobb</surname><given-names>S</given-names></name><name><surname>Graham</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Encoding and retrieval in a model of the hippocampal CA1 microcircuit</article-title><source>Hippocampus</source><volume>20</volume><fpage>423</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1002/hipo.20661</pub-id><pub-id pub-id-type="pmid">19489002</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dani</surname><given-names>V</given-names></name><name><surname>Chang</surname><given-names>Q</given-names></name><name><surname>Maffei</surname><given-names>A</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Jaenisch</surname><given-names>R</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reduced cortical activity due to a shift in the balance between excitation and inhibition in a mouse model of rett syndrome</article-title><source>PNAS</source><volume>102</volume><fpage>12560</fpage><lpage>12565</lpage><pub-id pub-id-type="doi">10.1073/pnas.0506071102</pub-id><pub-id pub-id-type="pmid">16116096</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dani</surname><given-names>VS</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Intact long-term potentiation but reduced connectivity between neocortical layer 5 pyramidal neurons in a mouse model of rett syndrome</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>11263</fpage><lpage>11270</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1019-09.2009</pub-id><pub-id pub-id-type="pmid">19741133</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>da Silva</surname><given-names>IN</given-names></name><name><surname>Hernane Spatti</surname><given-names>D</given-names></name><name><surname>Andrade Flauzino</surname><given-names>R</given-names></name><name><surname>Liboni</surname><given-names>LHB</given-names></name><name><surname>dos Reis Alves</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Artificial Neural Network Architectures and Training Processes</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-43162-8</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems-Computational Neuroscience Series</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayhoff</surname><given-names>JE</given-names></name><name><surname>DeLeo</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Artificial neural networks: opening the black box</article-title><source>Cancer</source><volume>91</volume><fpage>1615</fpage><lpage>1635</lpage><pub-id pub-id-type="doi">10.1002/1097-0142(20010415)91:8+3.0.co;2-l</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Cruz</surname><given-names>JA</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Zahid</surname><given-names>T</given-names></name><name><surname>El-Hayek</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Eubanks</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Alterations of cortical and hippocampal EEG activity in mecp2-deficient mice</article-title><source>Neurobiology of Disease</source><volume>38</volume><fpage>8</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/j.nbd.2009.12.018</pub-id><pub-id pub-id-type="pmid">20045053</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Kock</surname><given-names>CPJ</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name><name><surname>Spors</surname><given-names>H</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Layer- and cell-type-specific suprathreshold stimulus representation in rat primary somatosensory cortex</article-title><source>The Journal of Physiology</source><volume>581</volume><fpage>139</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2006.124321</pub-id><pub-id pub-id-type="pmid">17317752</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Schutter</surname><given-names>E</given-names></name><name><surname>Bower</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>An active membrane model of the cerebellar Purkinje cell. I. simulation of current clamps in slice</article-title><source>Journal of Neurophysiology</source><volume>71</volume><fpage>375</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.71.1.375</pub-id><pub-id pub-id-type="pmid">7512629</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destexhe</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Conductance-based integrate-and-fire models</article-title><source>Neural Computation</source><volume>9</volume><fpage>503</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.3.503</pub-id><pub-id pub-id-type="pmid">9097470</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destexhe</surname><given-names>A</given-names></name><name><surname>Neubig</surname><given-names>M</given-names></name><name><surname>Ulrich</surname><given-names>D</given-names></name><name><surname>Huguenard</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Dendritic low-threshold calcium currents in thalamic relay cells</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>3574</fpage><lpage>3588</lpage><pub-id pub-id-type="pmid">9570789</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Destexhe</surname><given-names>A</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Thalamocortical Assemblies: How Ion Channels, Single Neurons and Large-Scale Networks Organize Sleep Oscillations</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dillon</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tensorflow Distributions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1711.10604">https://arxiv.org/abs/1711.10604</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domanski</surname><given-names>APF</given-names></name><name><surname>Booker</surname><given-names>SA</given-names></name><name><surname>Wyllie</surname><given-names>DJA</given-names></name><name><surname>Isaac</surname><given-names>JTR</given-names></name><name><surname>Kind</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cellular and synaptic phenotypes lead to disrupted information processing in fmr1-KO mouse layer 4 barrel cortex</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-12736-y</pub-id><pub-id pub-id-type="pmid">31645553</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Hendricks</surname><given-names>LA</given-names></name><name><surname>Guadarrama</surname><given-names>S</given-names></name><name><surname>Rohrbach</surname><given-names>M</given-names></name><name><surname>Venugopalan</surname><given-names>S</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Saenko</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Long-term recurrent convolutional networks for visual recognition and description</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298878</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Dozat</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Incorporating Nesterov Momentum into Adam Technical Report</source><publisher-name>Stanford University</publisher-name></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Berger</surname><given-names>TK</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name><name><surname>Hill</surname><given-names>S</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Effective stimuli for constructing reliable neuron models</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002133</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002133</pub-id><pub-id pub-id-type="pmid">21876663</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egmont-Petersen</surname><given-names>M</given-names></name><name><surname>de Ridder</surname><given-names>D</given-names></name><name><surname>Handels</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Image processing with neural networks—a review</article-title><source>Pattern Recognition</source><volume>35</volume><fpage>2279</fpage><lpage>2301</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(01)00178-9</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enoki</surname><given-names>R</given-names></name><name><surname>Inoue</surname><given-names>M</given-names></name><name><surname>Hashimoto</surname><given-names>Y</given-names></name><name><surname>Kudo</surname><given-names>Y</given-names></name><name><surname>Miyakawa</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Gabaergic control of synaptic summation in hippocampal CA1 pyramidal neurons</article-title><source>Hippocampus</source><volume>11</volume><fpage>683</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.1002/hipo.1083</pub-id><pub-id pub-id-type="pmid">11811662</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eppler</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>PyNEST: a convenient interface to the nest simulator</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>2008</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.012.2008</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Roesch</surname><given-names>EB</given-names></name><name><surname>Shanahan</surname><given-names>MP</given-names></name><name><surname>Luk</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>NeMo: a platform for neural modelling of spiking neurons using GPUs</article-title><conf-name>2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors (ASAP)</conf-name><pub-id pub-id-type="doi">10.1109/ASAP.2009.24</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukuda</surname><given-names>T</given-names></name><name><surname>Yamashita</surname><given-names>Y</given-names></name><name><surname>Nagamitsu</surname><given-names>S</given-names></name><name><surname>Miyamoto</surname><given-names>K</given-names></name><name><surname>Jin</surname><given-names>J-J</given-names></name><name><surname>Ohmori</surname><given-names>I</given-names></name><name><surname>Ohtsuka</surname><given-names>Y</given-names></name><name><surname>Kuwajima</surname><given-names>K</given-names></name><name><surname>Endo</surname><given-names>S</given-names></name><name><surname>Iwai</surname><given-names>T</given-names></name><name><surname>Yamagata</surname><given-names>H</given-names></name><name><surname>Tabara</surname><given-names>Y</given-names></name><name><surname>Miki</surname><given-names>T</given-names></name><name><surname>Matsuishi</surname><given-names>T</given-names></name><name><surname>Kondo</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Methyl-Cpg binding protein 2 gene (MeCP2) variations in Japanese patients with Rett syndrome: pathological mutations and polymorphisms</article-title><source>Brain &amp; Development</source><volume>27</volume><fpage>211</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1016/j.braindev.2004.06.003</pub-id><pub-id pub-id-type="pmid">15737703</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gambazzi</surname><given-names>L</given-names></name><name><surname>Gokce</surname><given-names>O</given-names></name><name><surname>Seredenina</surname><given-names>T</given-names></name><name><surname>Katsyuba</surname><given-names>E</given-names></name><name><surname>Runne</surname><given-names>H</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Giugliano</surname><given-names>M</given-names></name><name><surname>Luthi-Carter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Diminished activity-dependent brain-derived neurotrophic factor expression underlies cortical neuron microcircuit hypoconnectivity resulting from exposure to mutant huntingtin fragments</article-title><source>The Journal of Pharmacology and Experimental Therapeutics</source><volume>335</volume><fpage>13</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1124/jpet.110.167551</pub-id><pub-id pub-id-type="pmid">20624994</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghoddusi</surname><given-names>H</given-names></name><name><surname>Creamer</surname><given-names>GG</given-names></name><name><surname>Rafizadeh</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine learning in energy economics and finance: A review</article-title><source>Energy Economics</source><volume>81</volume><fpage>709</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1016/j.eneco.2019.05.006</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaze</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neurophysiology of rett syndrome</article-title><source>Journal of Child Neurology</source><volume>20</volume><fpage>740</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1177/08830738050200090801</pub-id><pub-id pub-id-type="pmid">16225829</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Deep sparse rectifier neural networks</article-title><conf-name>Proceedings of the fourteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings</conf-name></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>IJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6211">https://arxiv.org/abs/1312.6211</ext-link></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Systematic generation of biophysically detailed models for diverse cortical neuron types</article-title><source>Nature Communications</source><volume>9</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-017-02718-3</pub-id><pub-id pub-id-type="pmid">29459718</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Baftizadeh</surname><given-names>F</given-names></name><name><surname>Budzillo</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>BR</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name><name><surname>Alfiler</surname><given-names>L</given-names></name><name><surname>Baker</surname><given-names>K</given-names></name><name><surname>Barkan</surname><given-names>E</given-names></name><name><surname>Berry</surname><given-names>K</given-names></name><name><surname>Bertagnolli</surname><given-names>D</given-names></name><name><surname>Bickley</surname><given-names>K</given-names></name><name><surname>Bomben</surname><given-names>J</given-names></name><name><surname>Braun</surname><given-names>T</given-names></name><name><surname>Brouner</surname><given-names>K</given-names></name><name><surname>Casper</surname><given-names>T</given-names></name><name><surname>Crichton</surname><given-names>K</given-names></name><name><surname>Daigle</surname><given-names>TL</given-names></name><name><surname>Dalley</surname><given-names>R</given-names></name><name><surname>de Frates</surname><given-names>RA</given-names></name><name><surname>Dee</surname><given-names>N</given-names></name><name><surname>Desta</surname><given-names>T</given-names></name><name><surname>Lee</surname><given-names>SD</given-names></name><name><surname>Dotson</surname><given-names>N</given-names></name><name><surname>Egdorf</surname><given-names>T</given-names></name><name><surname>Ellingwood</surname><given-names>L</given-names></name><name><surname>Enstrom</surname><given-names>R</given-names></name><name><surname>Esposito</surname><given-names>L</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Fong</surname><given-names>O</given-names></name><name><surname>Gala</surname><given-names>R</given-names></name><name><surname>Gamlin</surname><given-names>C</given-names></name><name><surname>Gary</surname><given-names>A</given-names></name><name><surname>Glandon</surname><given-names>A</given-names></name><name><surname>Goldy</surname><given-names>J</given-names></name><name><surname>Gorham</surname><given-names>M</given-names></name><name><surname>Graybuck</surname><given-names>L</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Hadley</surname><given-names>K</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Henry</surname><given-names>AM</given-names></name><name><surname>Hill</surname><given-names>D</given-names></name><name><surname>Hupp</surname><given-names>M</given-names></name><name><surname>Kebede</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>TK</given-names></name><name><surname>Kim</surname><given-names>L</given-names></name><name><surname>Kroll</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Link</surname><given-names>KE</given-names></name><name><surname>Mallory</surname><given-names>M</given-names></name><name><surname>Mann</surname><given-names>R</given-names></name><name><surname>Maxwell</surname><given-names>M</given-names></name><name><surname>McGraw</surname><given-names>M</given-names></name><name><surname>McMillen</surname><given-names>D</given-names></name><name><surname>Mukora</surname><given-names>A</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Ngo</surname><given-names>K</given-names></name><name><surname>Nicovich</surname><given-names>PR</given-names></name><name><surname>Oldre</surname><given-names>A</given-names></name><name><surname>Park</surname><given-names>D</given-names></name><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Penn</surname><given-names>O</given-names></name><name><surname>Pham</surname><given-names>T</given-names></name><name><surname>Pom</surname><given-names>A</given-names></name><name><surname>Popović</surname><given-names>Z</given-names></name><name><surname>Potekhina</surname><given-names>L</given-names></name><name><surname>Rajanbabu</surname><given-names>R</given-names></name><name><surname>Ransford</surname><given-names>S</given-names></name><name><surname>Reid</surname><given-names>D</given-names></name><name><surname>Rimorin</surname><given-names>C</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Ronellenfitch</surname><given-names>K</given-names></name><name><surname>Ruiz</surname><given-names>A</given-names></name><name><surname>Sandman</surname><given-names>D</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Sulc</surname><given-names>J</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Tieu</surname><given-names>M</given-names></name><name><surname>Torkelson</surname><given-names>A</given-names></name><name><surname>Trinh</surname><given-names>J</given-names></name><name><surname>Tung</surname><given-names>H</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Ward</surname><given-names>K</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Sümbül</surname><given-names>U</given-names></name><name><surname>Lein</surname><given-names>ES</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Integrated morphoelectric and transcriptomic classification of cortical gabaergic cells</article-title><source>Cell</source><volume>183</volume><fpage>935</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.057</pub-id><pub-id pub-id-type="pmid">33186530</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Graupe</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Principles of Artificial Neural Networks</source><publisher-name>World Scientific</publisher-name><pub-id pub-id-type="doi">10.1142/8868</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guy</surname><given-names>J</given-names></name><name><surname>Gan</surname><given-names>J</given-names></name><name><surname>Selfridge</surname><given-names>J</given-names></name><name><surname>Cobb</surname><given-names>S</given-names></name><name><surname>Bird</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reversal of neurological defects in a mouse model of Rett syndrome</article-title><source>Science</source><volume>315</volume><fpage>1143</fpage><lpage>1147</lpage><pub-id pub-id-type="doi">10.1126/science.1138389</pub-id><pub-id pub-id-type="pmid">17289941</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagberg</surname><given-names>B</given-names></name><name><surname>Goutières</surname><given-names>F</given-names></name><name><surname>Hanefeld</surname><given-names>F</given-names></name><name><surname>Rett</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Rett syndrome: criteria for inclusion and exclusion</article-title><source>Brain &amp; Development</source><volume>7</volume><fpage>372</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1016/s0387-7604(85)80048-6</pub-id><pub-id pub-id-type="pmid">4061772</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallermann</surname><given-names>S</given-names></name><name><surname>de Kock</surname><given-names>CPJ</given-names></name><name><surname>Stuart</surname><given-names>GJ</given-names></name><name><surname>Kole</surname><given-names>MHP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>State and location dependence of action potential metabolic cost in cortical pyramidal neurons</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1007</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1038/nn.3132</pub-id><pub-id pub-id-type="pmid">22660478</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harnett</surname><given-names>MT</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Williams</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distribution and function of HCN channels in the apical dendritic tuft of neocortical pyramidal neurons</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>1024</fpage><lpage>1037</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2813-14.2015</pub-id><pub-id pub-id-type="pmid">25609619</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hassoun</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Fundamentals of Artificial Neural Networks</source><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.1109/JPROC.1996.503146</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Mel</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dendrites: bug or feature?</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>372</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(03)00075-8</pub-id><pub-id pub-id-type="pmid">12850223</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hay</surname><given-names>E</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dendritic excitability and gain control in recurrent cortical microcircuits</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3561</fpage><lpage>3571</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu200</pub-id><pub-id pub-id-type="pmid">25205662</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</article-title><conf-name>2015 IEEE International Conference on Computer Vision (ICCV)</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hendrickson</surname><given-names>EB</given-names></name><name><surname>Edgerton</surname><given-names>JR</given-names></name><name><surname>Jaeger</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The capabilities and limitations of conductance-based compartmental neuron models with reduced branched or unbranched morphologies and active dendrites</article-title><source>Journal of Computational Neuroscience</source><volume>30</volume><fpage>301</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1007/s10827-010-0258-z</pub-id><pub-id pub-id-type="pmid">20623167</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Carnevale</surname><given-names>NT</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The neuron simulation environment</article-title><source>Neural Computation</source><volume>9</volume><fpage>1179</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.6.1179</pub-id><pub-id pub-id-type="pmid">9248061</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Carnevale</surname><given-names>NT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Translating network models to parallel hardware in neuron</article-title><source>Journal of Neuroscience Methods</source><volume>169</volume><fpage>425</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.09.010</pub-id><pub-id pub-id-type="pmid">17997162</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Fully implicit parallel simulation of single neurons</article-title><source>Journal of Computational Neuroscience</source><volume>25</volume><fpage>439</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1007/s10827-008-0087-5</pub-id><pub-id pub-id-type="pmid">18379867</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodgkin</surname><given-names>AL</given-names></name><name><surname>Huxley</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title><source>The Journal of Physiology</source><volume>117</volume><fpage>500</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1952.sp004764</pub-id><pub-id pub-id-type="pmid">12991237</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holmstrom</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Machine Learning Applied to Weather Forecasting</source><publisher-name>Stanford University</publisher-name></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Simple model of spiking neurons</article-title><source>IEEE Transactions on Neural Networks</source><volume>14</volume><fpage>1569</fpage><lpage>1572</lpage><pub-id pub-id-type="doi">10.1109/TNN.2003.820440</pub-id><pub-id pub-id-type="pmid">18244602</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarsky</surname><given-names>T</given-names></name><name><surname>Roxin</surname><given-names>A</given-names></name><name><surname>Kath</surname><given-names>WL</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Conditional dendritic spike propagation following distal synaptic activation of hippocampal CA1 pyramidal neurons</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1667</fpage><lpage>1676</lpage><pub-id pub-id-type="doi">10.1038/nn1599</pub-id><pub-id pub-id-type="pmid">16299501</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaslove</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The integrative properties of spiny distal dendrites</article-title><source>Neuroscience</source><volume>47</volume><fpage>495</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(92)90161-t</pub-id><pub-id pub-id-type="pmid">1584406</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Mejias</surname><given-names>JF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inter-areal balanced amplification enhances signal propagation in a large-scale circuit model of the primate cortex</article-title><source>Neuron</source><volume>98</volume><fpage>222</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.02.031</pub-id><pub-id pub-id-type="pmid">29576389</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kawaguchi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Learning without Poor Local Minima</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1605.07110">https://arxiv.org/abs/1605.07110</ext-link></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerr</surname><given-names>CC</given-names></name><name><surname>Van Albada</surname><given-names>SJ</given-names></name><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>Chadderdon</surname><given-names>GL</given-names></name><name><surname>Robinson</surname><given-names>PA</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical information flow in Parkinson’s disease: a composite network/field model</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00039</pub-id><pub-id pub-id-type="pmid">23630492</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Guzman</surname><given-names>SJ</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Jonas</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Active dendrites support efficient initiation of dendritic spikes in hippocampal CA3 pyramidal neurons</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>600</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1038/nn.3060</pub-id><pub-id pub-id-type="pmid">22388958</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Paré</surname><given-names>D</given-names></name><name><surname>Nair</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mechanisms contributing to the induction and storage of pavlovian fear memories in the lateral amygdala</article-title><source>Learning &amp; Memory</source><volume>20</volume><fpage>421</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1101/lm.030262.113</pub-id><pub-id pub-id-type="pmid">23864645</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kishi</surname><given-names>N</given-names></name><name><surname>Macklis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Mecp2 is progressively expressed in post-migratory neurons and is involved in neuronal maturation rather than cell fate decisions</article-title><source>Molecular and Cellular Neurosciences</source><volume>27</volume><fpage>306</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1016/j.mcn.2004.07.006</pub-id><pub-id pub-id-type="pmid">15519245</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knight</surname><given-names>J</given-names></name><name><surname>Komissarov</surname><given-names>A</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>PyGeNN: a python library for GPU-enhanced neural networks</article-title><source>Frontiers in Neuroinformatics</source><volume>15</volume><elocation-id>659005</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2021.659005</pub-id><pub-id pub-id-type="pmid">33967731</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knight</surname><given-names>J</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Larger GPU-accelerated brain simulations with procedural connectivity</article-title><source>Nature Computational Science</source><volume>1</volume><fpage>136</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/s43588-020-00022-7</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kole</surname><given-names>MHP</given-names></name><name><surname>Ilschner</surname><given-names>SU</given-names></name><name><surname>Kampa</surname><given-names>BM</given-names></name><name><surname>Williams</surname><given-names>SR</given-names></name><name><surname>Ruben</surname><given-names>PC</given-names></name><name><surname>Stuart</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Action potential generation requires a high sodium channel density in the axon initial segment</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>178</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1038/nn2040</pub-id><pub-id pub-id-type="pmid">18204443</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kons</surname><given-names>Z</given-names></name><name><surname>Toledo-Ronen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Audio event classification using deep neural networks</article-title><conf-name>Interspeech 2013</conf-name><pub-id pub-id-type="doi">10.21437/Interspeech.2013-384</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Douglas</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cognitive computational neuroscience</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1148</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0210-5</pub-id><pub-id pub-id-type="pmid">30127428</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Schiff</surname><given-names>O</given-names></name><name><surname>Barkai</surname><given-names>E</given-names></name><name><surname>Mel</surname><given-names>BW</given-names></name><name><surname>Poleg-Polsky</surname><given-names>A</given-names></name><name><surname>Schiller</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nmda spikes mediate amplification of inputs in the rat piriform cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e38446</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38446</pub-id><pub-id pub-id-type="pmid">30575520</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Fouriaux</surname><given-names>J</given-names></name><name><surname>Ovcharenko</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Delalondre</surname><given-names>F</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>CoreNEURON: an optimized compute engine for the neuron simulator</article-title><source>Frontiers in Neuroinformatics</source><volume>13</volume><elocation-id>63</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2019.00063</pub-id><pub-id pub-id-type="pmid">31616273</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname><given-names>ME</given-names></name><name><surname>Nevian</surname><given-names>T</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Polsky</surname><given-names>A</given-names></name><name><surname>Schiller</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Synaptic integration in tuft dendrites of layer 5 pyramidal neurons: a new unifying principle</article-title><source>Science</source><volume>325</volume><fpage>756</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1126/science.1171958</pub-id><pub-id pub-id-type="pmid">19661433</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lien</surname><given-names>AD</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tuned thalamic excitation is amplified by visual cortical circuits</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1315</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1038/nn.3488</pub-id><pub-id pub-id-type="pmid">23933748</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liou</surname><given-names>J-Y</given-names></name><name><surname>Smith</surname><given-names>EH</given-names></name><name><surname>Bateman</surname><given-names>LM</given-names></name><name><surname>Bruce</surname><given-names>SL</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Goodman</surname><given-names>RR</given-names></name><name><surname>Emerson</surname><given-names>RG</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A model for focal seizure onset, propagation, evolution, and progression</article-title><source>eLife</source><volume>9</volume><elocation-id>e50927</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50927</pub-id><pub-id pub-id-type="pmid">32202494</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Makara</surname><given-names>JK</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Compartmentalized dendritic plasticity and input feature storage in neurons</article-title><source>Nature</source><volume>452</volume><fpage>436</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1038/nature06725</pub-id><pub-id pub-id-type="pmid">18368112</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lytton</surname><given-names>WW</given-names></name><name><surname>Seidenstein</surname><given-names>AH</given-names></name><name><surname>Dura-Bernal</surname><given-names>S</given-names></name><name><surname>McDougal</surname><given-names>RA</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name><name><surname>Hines</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Simulation neurotechnologies for advancing brain research: parallelizing large networks in neuron</article-title><source>Neural Computation</source><volume>28</volume><fpage>2063</fpage><lpage>2090</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00876</pub-id><pub-id pub-id-type="pmid">27557104</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Cook</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Somatic EPSP amplitude is independent of synapse location in hippocampal pyramidal neurons</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>895</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1038/78800</pub-id><pub-id pub-id-type="pmid">10966620</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Joerges</surname><given-names>J</given-names></name><name><surname>Huguenard</surname><given-names>JR</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>A model of spike initiation in neocortical pyramidal neurons</article-title><source>Neuron</source><volume>15</volume><fpage>1427</fpage><lpage>1439</lpage><pub-id pub-id-type="doi">10.1016/0896-6273(95)90020-9</pub-id><pub-id pub-id-type="pmid">8845165</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Influence of dendritic structure on firing pattern in model neocortical neurons</article-title><source>Nature</source><volume>382</volume><fpage>363</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/382363a0</pub-id><pub-id pub-id-type="pmid">8684467</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Major</surname><given-names>G</given-names></name><name><surname>Larkman</surname><given-names>AU</given-names></name><name><surname>Jonas</surname><given-names>P</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Jack</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Detailed passive cable models of whole-cell recorded CA3 pyramidal neurons in rat hippocampal slices</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>4613</fpage><lpage>4638</lpage><pub-id pub-id-type="pmid">8046439</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Major</surname><given-names>Guy</given-names></name><name><surname>Polsky</surname><given-names>A</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name><name><surname>Schiller</surname><given-names>J</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatiotemporally graded NMDA spike/plateau potentials in basal dendrites of neocortical pyramidal neurons</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>2584</fpage><lpage>2601</lpage><pub-id pub-id-type="doi">10.1152/jn.00011.2008</pub-id><pub-id pub-id-type="pmid">18337370</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marasco</surname><given-names>A</given-names></name><name><surname>Limongiello</surname><given-names>A</given-names></name><name><surname>Migliore</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fast and accurate low-dimensional reduction of biophysically detailed neuron models</article-title><source>Scientific Reports</source><volume>2</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/srep00928</pub-id><pub-id pub-id-type="pmid">23226594</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Ramaswamy</surname><given-names>S</given-names></name><name><surname>Reimann</surname><given-names>MW</given-names></name><name><surname>Abdellah</surname><given-names>M</given-names></name><name><surname>Sanchez</surname><given-names>CA</given-names></name><name><surname>Ailamaki</surname><given-names>A</given-names></name><name><surname>Alonso-Nanclares</surname><given-names>L</given-names></name><name><surname>Antille</surname><given-names>N</given-names></name><name><surname>Arsever</surname><given-names>S</given-names></name><name><surname>Kahou</surname><given-names>GAA</given-names></name><name><surname>Berger</surname><given-names>TK</given-names></name><name><surname>Bilgili</surname><given-names>A</given-names></name><name><surname>Buncic</surname><given-names>N</given-names></name><name><surname>Chalimourda</surname><given-names>A</given-names></name><name><surname>Chindemi</surname><given-names>G</given-names></name><name><surname>Courcol</surname><given-names>J-D</given-names></name><name><surname>Delalondre</surname><given-names>F</given-names></name><name><surname>Delattre</surname><given-names>V</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Dumusc</surname><given-names>R</given-names></name><name><surname>Dynes</surname><given-names>J</given-names></name><name><surname>Eilemann</surname><given-names>S</given-names></name><name><surname>Gal</surname><given-names>E</given-names></name><name><surname>Gevaert</surname><given-names>ME</given-names></name><name><surname>Ghobril</surname><given-names>J-P</given-names></name><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Graham</surname><given-names>JW</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Haenel</surname><given-names>V</given-names></name><name><surname>Hay</surname><given-names>E</given-names></name><name><surname>Heinis</surname><given-names>T</given-names></name><name><surname>Hernando</surname><given-names>JB</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Kanari</surname><given-names>L</given-names></name><name><surname>Keller</surname><given-names>D</given-names></name><name><surname>Kenyon</surname><given-names>J</given-names></name><name><surname>Khazen</surname><given-names>G</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>King</surname><given-names>JG</given-names></name><name><surname>Kisvarday</surname><given-names>Z</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Lasserre</surname><given-names>S</given-names></name><name><surname>Le Bé</surname><given-names>J-V</given-names></name><name><surname>Magalhães</surname><given-names>BRC</given-names></name><name><surname>Merchán-Pérez</surname><given-names>A</given-names></name><name><surname>Meystre</surname><given-names>J</given-names></name><name><surname>Morrice</surname><given-names>BR</given-names></name><name><surname>Muller</surname><given-names>J</given-names></name><name><surname>Muñoz-Céspedes</surname><given-names>A</given-names></name><name><surname>Muralidhar</surname><given-names>S</given-names></name><name><surname>Muthurasa</surname><given-names>K</given-names></name><name><surname>Nachbaur</surname><given-names>D</given-names></name><name><surname>Newton</surname><given-names>TH</given-names></name><name><surname>Nolte</surname><given-names>M</given-names></name><name><surname>Ovcharenko</surname><given-names>A</given-names></name><name><surname>Palacios</surname><given-names>J</given-names></name><name><surname>Pastor</surname><given-names>L</given-names></name><name><surname>Perin</surname><given-names>R</given-names></name><name><surname>Ranjan</surname><given-names>R</given-names></name><name><surname>Riachi</surname><given-names>I</given-names></name><name><surname>Rodríguez</surname><given-names>J-R</given-names></name><name><surname>Riquelme</surname><given-names>JL</given-names></name><name><surname>Rössert</surname><given-names>C</given-names></name><name><surname>Sfyrakis</surname><given-names>K</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Shillcock</surname><given-names>JC</given-names></name><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>Silva</surname><given-names>R</given-names></name><name><surname>Tauheed</surname><given-names>F</given-names></name><name><surname>Telefont</surname><given-names>M</given-names></name><name><surname>Toledo-Rodriguez</surname><given-names>M</given-names></name><name><surname>Tränkler</surname><given-names>T</given-names></name><name><surname>Van Geit</surname><given-names>W</given-names></name><name><surname>Díaz</surname><given-names>JV</given-names></name><name><surname>Walker</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zaninetta</surname><given-names>SM</given-names></name><name><surname>DeFelipe</surname><given-names>J</given-names></name><name><surname>Hill</surname><given-names>SL</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reconstruction and simulation of neocortical microcircuitry</article-title><source>Cell</source><volume>163</volume><fpage>456</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.09.029</pub-id><pub-id pub-id-type="pmid">26451489</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>DA</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>On the cellular and network bases of epileptic seizures</article-title><source>Annual Review of Physiology</source><volume>63</volume><fpage>815</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1146/annurev.physiol.63.1.815</pub-id><pub-id pub-id-type="pmid">11181977</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLeod</surname><given-names>F</given-names></name><name><surname>Ganley</surname><given-names>R</given-names></name><name><surname>Williams</surname><given-names>L</given-names></name><name><surname>Selfridge</surname><given-names>J</given-names></name><name><surname>Bird</surname><given-names>A</given-names></name><name><surname>Cobb</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reduced seizure threshold and altered network oscillatory properties in a mouse model of Rett syndrome</article-title><source>Neuroscience</source><volume>231</volume><fpage>195</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2012.11.058</pub-id><pub-id pub-id-type="pmid">23238573</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medrihan</surname><given-names>L</given-names></name><name><surname>Tantalaki</surname><given-names>E</given-names></name><name><surname>Aramuni</surname><given-names>G</given-names></name><name><surname>Sargsyan</surname><given-names>V</given-names></name><name><surname>Dudanova</surname><given-names>I</given-names></name><name><surname>Missler</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Early defects of GABAergic synapses in the brain stem of a MeCP2 mouse model of Rett syndrome</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>112</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1152/jn.00826.2007</pub-id><pub-id pub-id-type="pmid">18032561</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Megías</surname><given-names>M</given-names></name><name><surname>Emri</surname><given-names>Z</given-names></name><name><surname>Freund</surname><given-names>TF</given-names></name><name><surname>Gulyás</surname><given-names>AI</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Total number and distribution of inhibitory and excitatory synapses on hippocampal CA1 pyramidal cells</article-title><source>Neuroscience</source><volume>102</volume><fpage>527</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/s0306-4522(00)00496-6</pub-id><pub-id pub-id-type="pmid">11226691</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Memon</surname><given-names>ZA</given-names></name><name><surname>Samad</surname><given-names>F</given-names></name><name><surname>Awan</surname><given-names>ZR</given-names></name><name><surname>Aziz</surname><given-names>A</given-names></name><name><surname>Siddiqi</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cpu-gpu processing</article-title><source>International Journal of Computer Science and Network Security</source><volume>17</volume><fpage>188</fpage><lpage>193</lpage></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meredig</surname><given-names>B</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Kirklin</surname><given-names>S</given-names></name><name><surname>Saal</surname><given-names>JE</given-names></name><name><surname>Doak</surname><given-names>JW</given-names></name><name><surname>Thompson</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name><name><surname>Wolverton</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Combinatorial screening for new materials in unconstrained composition space with machine learning</article-title><source>Physical Review B</source><volume>89</volume><elocation-id>094104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevB.89.094104</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Merembayev</surname><given-names>T</given-names></name><name><surname>Yunussov</surname><given-names>R</given-names></name><name><surname>Yedilkhan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Machine Learning Algorithms for Classification Geology Data from Well Logging</article-title><conf-name>2018 14th International Conference on Electronics Computer and Computation (ICECCO)</conf-name><pub-id pub-id-type="doi">10.1109/ICECCO.2018.8634775</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>HS</given-names></name><name><surname>Wimmer</surname><given-names>VC</given-names></name><name><surname>Hemberger</surname><given-names>M</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name><name><surname>de Kock</surname><given-names>CPJ</given-names></name><name><surname>Frick</surname><given-names>A</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cell type-specific thalamic innervation in a column of rat vibrissal cortex</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>2287</fpage><lpage>2303</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq069</pub-id><pub-id pub-id-type="pmid">20534783</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Migliore</surname><given-names>M</given-names></name><name><surname>Cook</surname><given-names>EP</given-names></name><name><surname>Jaffe</surname><given-names>DB</given-names></name><name><surname>Turner</surname><given-names>DA</given-names></name><name><surname>Johnston</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Computer simulations of morphologically reconstructed CA3 hippocampal neurons</article-title><source>Journal of Neurophysiology</source><volume>73</volume><fpage>1157</fpage><lpage>1168</lpage><pub-id pub-id-type="doi">10.1152/jn.1995.73.3.1157</pub-id><pub-id pub-id-type="pmid">7608762</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Migliore</surname><given-names>M</given-names></name><name><surname>Hoffman</surname><given-names>DA</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Johnston</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Role of an A-type K+ conductance in the back-propagation of action potentials in the dendrites of hippocampal pyramidal neurons</article-title><source>Journal of Computational Neuroscience</source><volume>7</volume><fpage>5</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1023/a:1008906225285</pub-id><pub-id pub-id-type="pmid">10481998</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Migliore</surname><given-names>M</given-names></name><name><surname>Shepherd</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dendritic action potentials connect distributed dendrodendritic microcircuits</article-title><source>Journal of Computational Neuroscience</source><volume>24</volume><fpage>207</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1007/s10827-007-0051-9</pub-id><pub-id pub-id-type="pmid">17674173</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Montavon</surname><given-names>G</given-names></name><name><surname>Orr</surname><given-names>GB</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Neural networks: tricks of the trade</chapter-title><person-group person-group-type="editor"><name><surname>Montavon</surname><given-names>G</given-names></name></person-group><source>Efficient Backprop</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>9</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-35289-8</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montavon</surname><given-names>G</given-names></name><name><surname>Rupp</surname><given-names>M</given-names></name><name><surname>Gobre</surname><given-names>V</given-names></name><name><surname>Vazquez-Mayagoitia</surname><given-names>A</given-names></name><name><surname>Hansen</surname><given-names>K</given-names></name><name><surname>Tkatchenko</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name><name><surname>Anatole von Lilienfeld</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Machine learning of molecular electronic properties in chemical compound space</article-title><source>New Journal of Physics</source><volume>15</volume><elocation-id>095003</elocation-id><pub-id pub-id-type="doi">10.1088/1367-2630/15/9/095003</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Mutch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>CNS: a GPU-based framework for simulating cortically-organized networks MIT CSAIL</article-title><ext-link ext-link-type="uri" xlink:href="http://128.30.100.62:8080/media/fb/ps/mit-csail-tr-2010-013.pdf">http://128.30.100.62:8080/media/fb/ps/mit-csail-tr-2010-013.pdf</ext-link><date-in-citation iso-8601-date="2013-05-17">May 17, 2013</date-in-citation></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Na</surname><given-names>ES</given-names></name><name><surname>Nelson</surname><given-names>ED</given-names></name><name><surname>Adachi</surname><given-names>M</given-names></name><name><surname>Autry</surname><given-names>AE</given-names></name><name><surname>Mahgoub</surname><given-names>MA</given-names></name><name><surname>Kavalali</surname><given-names>ET</given-names></name><name><surname>Monteggia</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A mouse model for MeCP2 duplication syndrome: MeCP2 overexpression impairs learning and memory and synaptic transmission</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3109</fpage><lpage>3117</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6000-11.2012</pub-id><pub-id pub-id-type="pmid">22378884</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nageswaran</surname><given-names>JM</given-names></name><name><surname>Dutt</surname><given-names>N</given-names></name><name><surname>Krichmar</surname><given-names>JL</given-names></name><name><surname>Nicolau</surname><given-names>A</given-names></name><name><surname>Veidenbaum</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A configurable simulation environment for the efficient simulation of large-scale spiking neural networks on graphics processors</article-title><source>Neural Networks</source><volume>22</volume><fpage>791</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2009.06.028</pub-id><pub-id pub-id-type="pmid">19615853</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Marcille</surname><given-names>N</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Firing patterns in the adaptive exponential integrate-and-fire model</article-title><source>Biol Cybern</source><volume>99</volume><fpage>335</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1007/s00422-008-0264-7</pub-id><pub-id pub-id-type="pmid">19011922</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>ED</given-names></name><name><surname>Kavalali</surname><given-names>ET</given-names></name><name><surname>Monteggia</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mecp2-Dependent transcriptional repression regulates excitatory neurotransmission</article-title><source>Current Biology</source><volume>16</volume><fpage>710</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.02.062</pub-id><pub-id pub-id-type="pmid">16581518</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>Dura-Bernal</surname><given-names>S</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Sanger</surname><given-names>TD</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Multitarget multiscale simulation for pharmacological treatment of dystonia in motor cortex</article-title><source>Frontiers in Pharmacology</source><volume>7</volume><elocation-id>157</elocation-id><pub-id pub-id-type="doi">10.3389/fphar.2016.00157</pub-id><pub-id pub-id-type="pmid">27378922</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neymotin</surname><given-names>SA</given-names></name><name><surname>McDougal</surname><given-names>RA</given-names></name><name><surname>Bulanova</surname><given-names>AS</given-names></name><name><surname>Zeki</surname><given-names>M</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Terman</surname><given-names>D</given-names></name><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Calcium regulation of HCN channels supports persistent activity in a multiscale model of neocortex</article-title><source>Neuroscience</source><volume>316</volume><fpage>344</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2015.12.043</pub-id><pub-id pub-id-type="pmid">26746357</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nikolic</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Temporal Dynamics of Information Content Carried by Neurons in the Primary Visual Cortex</source><publisher-name>NIPS</publisher-name></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowotny</surname><given-names>T</given-names></name><name><surname>Cope</surname><given-names>AJ</given-names></name><name><surname>Yavuz</surname><given-names>E</given-names></name><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Goodman</surname><given-names>DF</given-names></name><name><surname>Marshall</surname><given-names>J</given-names></name><name><surname>Gurney</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>SpineML and brian 2.0 interfaces for using GPU enhanced neuronal networks (genn)</article-title><source>BMC Neuroscience</source><volume>15</volume><fpage>1</fpage><lpage>2</lpage><pub-id pub-id-type="doi">10.1186/1471-2202-15-S1-P148</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oláh</surname><given-names>VJ</given-names></name><name><surname>Lukacsovich</surname><given-names>D</given-names></name><name><surname>Winterer</surname><given-names>J</given-names></name><name><surname>Arszovszki</surname><given-names>A</given-names></name><name><surname>Lőrincz</surname><given-names>A</given-names></name><name><surname>Nusser</surname><given-names>Z</given-names></name><name><surname>Földy</surname><given-names>C</given-names></name><name><surname>Szabadics</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Functional specification of CCK+ interneurons by alternative isoforms of kv4.3 auxiliary subunits</article-title><source>eLife</source><volume>9</volume><elocation-id>e58515</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58515</pub-id><pub-id pub-id-type="pmid">32490811</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oláh</surname><given-names>VJ</given-names></name><name><surname>Goettemoeller</surname><given-names>AM</given-names></name><name><surname>Dimidschstein</surname><given-names>J</given-names></name><name><surname>Rowan</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biophysical Kv Channel Alterations Dampen Excitability of Cortical PV Interneurons and Contribute to Network Hyperexcitability in Early Alzheimer’s</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.10.25.465789</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Oláh</surname><given-names>VJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Neuro_ANN</data-title><version designator="swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f">swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9a4bb33ea65af35384c87764f456648c12f08d78;origin=https://github.com/ViktorJOlah/Neuro_ANN;visit=swh:1:snp:dfa0fca6e35143608ab67fcea4ae9431cd7f7cf5;anchor=swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f">https://archive.softwareheritage.org/swh:1:dir:9a4bb33ea65af35384c87764f456648c12f08d78;origin=https://github.com/ViktorJOlah/Neuro_ANN;visit=swh:1:snp:dfa0fca6e35143608ab67fcea4ae9431cd7f7cf5;anchor=swh:1:rev:52616946edd6489a967a645bbab805577b15ad7f</ext-link></element-citation></ref><ref id="bib141"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oord</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Wavenet: A Generative Model for Raw Audio</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</ext-link></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Brannon</surname><given-names>T</given-names></name><name><surname>Mel</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pyramidal neuron as two-layer neural network</article-title><source>Neuron</source><volume>37</volume><fpage>989</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00149-1</pub-id><pub-id pub-id-type="pmid">12670427</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roche</surname><given-names>KJ</given-names></name><name><surname>LeBlanc</surname><given-names>JJ</given-names></name><name><surname>Levin</surname><given-names>AR</given-names></name><name><surname>O’Leary</surname><given-names>HM</given-names></name><name><surname>Baczewski</surname><given-names>LM</given-names></name><name><surname>Nelson</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Electroencephalographic spectral power as a marker of cortical function and disease severity in girls with Rett syndrome</article-title><source>Journal of Neurodevelopmental Disorders</source><volume>11</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1186/s11689-019-9275-z</pub-id><pub-id pub-id-type="pmid">31362710</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ros</surname><given-names>E</given-names></name><name><surname>Carrillo</surname><given-names>R</given-names></name><name><surname>Ortigosa</surname><given-names>EM</given-names></name><name><surname>Barbour</surname><given-names>B</given-names></name><name><surname>Agís</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Event-driven simulation scheme for spiking neural networks using lookup tables to characterize neuronal dynamics</article-title><source>Neural Computation</source><volume>18</volume><fpage>2959</fpage><lpage>2993</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.12.2959</pub-id><pub-id pub-id-type="pmid">17052155</pub-id></element-citation></ref><ref id="bib145"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rössert</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Automated Point-Neuron Simplification of Data-Driven Microcircuit Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1604.00087">https://arxiv.org/abs/1604.00087</ext-link></element-citation></ref><ref id="bib146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowan</surname><given-names>MJM</given-names></name><name><surname>Tranquil</surname><given-names>E</given-names></name><name><surname>Christie</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distinct Kv channel subtypes contribute to differences in spike signaling properties in the axon initial segment and presynaptic boutons of cerebellar interneurons</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>6611</fpage><lpage>6623</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4208-13.2014</pub-id><pub-id pub-id-type="pmid">24806686</pub-id></element-citation></ref><ref id="bib147"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib148"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacerdote</surname><given-names>L</given-names></name><name><surname>Giraudo</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stochastic integrate and fire models: a review on mathematical methods and their applications</article-title><source>Stochastic Biomathematical Models</source><volume>1</volume><fpage>99</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-32157-3_5</pub-id></element-citation></ref><ref id="bib149"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sanjay</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multiscale computer modeling of epilepsy</article-title><conf-name>Computational Models of Brain Behavior</conf-name></element-citation></ref><ref id="bib150"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santhakumar</surname><given-names>V</given-names></name><name><surname>Aradi</surname><given-names>I</given-names></name><name><surname>Soltesz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Role of mossy fiber sprouting and mossy cell loss in hyperexcitability: a network model of the dentate gyrus incorporating cell types and axonal topography</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>437</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1152/jn.00777.2004</pub-id><pub-id pub-id-type="pmid">15342722</pub-id></element-citation></ref><ref id="bib151"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santosa</surname><given-names>F</given-names></name><name><surname>Symes</surname><given-names>WW</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Linear inversion of band-limited reflection seismograms</article-title><source>SIAM Journal on Scientific and Statistical Computing</source><volume>7</volume><fpage>1307</fpage><lpage>1330</lpage><pub-id pub-id-type="doi">10.1137/0907087</pub-id></element-citation></ref><ref id="bib152"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>J</given-names></name><name><surname>Schiller</surname><given-names>Y</given-names></name><name><surname>Stuart</surname><given-names>G</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Calcium action potentials restricted to distal apical dendrites of rat neocortical pyramidal neurons</article-title><source>The Journal of Physiology</source><volume>505 (Pt 3)</volume><fpage>605</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7793.1997.605ba.x</pub-id><pub-id pub-id-type="pmid">9457639</pub-id></element-citation></ref><ref id="bib153"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>J</given-names></name><name><surname>Major</surname><given-names>G</given-names></name><name><surname>Koester</surname><given-names>HJ</given-names></name><name><surname>Schiller</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>NMDA spikes in basal dendrites of cortical pyramidal neurons</article-title><source>Nature</source><volume>404</volume><fpage>285</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1038/35005094</pub-id><pub-id pub-id-type="pmid">10749211</pub-id></element-citation></ref><ref id="bib154"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>M</given-names></name><name><surname>Bakker</surname><given-names>R</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Diesmann</surname><given-names>M</given-names></name><name><surname>van Albada</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multi-scale account of the network structure of macaque visual cortex</article-title><source>Brain Structure &amp; Function</source><volume>223</volume><fpage>1409</fpage><lpage>1435</lpage><pub-id pub-id-type="doi">10.1007/s00429-017-1554-4</pub-id><pub-id pub-id-type="pmid">29143946</pub-id></element-citation></ref><ref id="bib155"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schütt</surname><given-names>KT</given-names></name><name><surname>Chmiela</surname><given-names>S</given-names></name><name><surname>von Lilienfeld</surname><given-names>OA</given-names></name><name><surname>Tkatchenko</surname><given-names>A</given-names></name><name><surname>Tsuda</surname><given-names>K</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Machine Learning Meets Quantum Physics</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-030-40245-7</pub-id></element-citation></ref><ref id="bib156"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwalger</surname><given-names>T</given-names></name><name><surname>Chizhov</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mind the last spike - firing rate models for mesoscopic populations of spiking neurons</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>155</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.08.003</pub-id><pub-id pub-id-type="pmid">31590003</pub-id></element-citation></ref><ref id="bib157"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Churchland</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Computational neuroscience</article-title><source>Science</source><volume>241</volume><fpage>1299</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1126/science.3045969</pub-id><pub-id pub-id-type="pmid">3045969</pub-id></element-citation></ref><ref id="bib158"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shai</surname><given-names>AS</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Physiology of layer 5 pyramidal neurons in mouse primary visual cortex: coincidence detection through bursting</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004090</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004090</pub-id><pub-id pub-id-type="pmid">25768881</pub-id></element-citation></ref><ref id="bib159"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>N</given-names></name><name><surname>Sharma</surname><given-names>P</given-names></name><name><surname>Irwin</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predicting solar generation from weather forecasts using machine learning</article-title><conf-name>2011 IEEE international conference on smart grid communications (SmartGridComm</conf-name><pub-id pub-id-type="doi">10.1109/SmartGridComm.2011.6102379</pub-id></element-citation></ref><ref id="bib160"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/doi/10.5555/2969239.2969329">https://dl.acm.org/doi/10.5555/2969239.2969329</ext-link></element-citation></ref><ref id="bib161"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shimoura</surname><given-names>RO</given-names></name><name><surname>Kamiji</surname><given-names>NL</given-names></name><name><surname>de Oliveira Pena</surname><given-names>RF</given-names></name><name><surname>Cordeiro</surname><given-names>VL</given-names></name><name><surname>Ceballos</surname><given-names>CC</given-names></name><name><surname>Romaro</surname><given-names>C</given-names></name><name><surname>Roque</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reimplementation of the Potjans-Diesmann Cortical Microcircuit Model: From NEST to Brian</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/248401v1.full.pdf</pub-id></element-citation></ref><ref id="bib162"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>S</given-names></name><name><surname>Nelder</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Nelder-mead algorithm</article-title><source>Scholarpedia</source><volume>4</volume><elocation-id>2928</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.2928</pub-id></element-citation></ref><ref id="bib163"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sivagnanam</surname><given-names>S</given-names></name><name><surname>Astakhov</surname><given-names>V</given-names></name><name><surname>Yoshimoto</surname><given-names>K</given-names></name><name><surname>Carnevale</surname><given-names>T</given-names></name><name><surname>Martone</surname><given-names>M</given-names></name><name><surname>Majumdar</surname><given-names>A</given-names></name><name><surname>Bandrowski</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A Neuroscience Gateway XSEDE ’13</article-title><conf-name>XSEDE ’13</conf-name><pub-id pub-id-type="doi">10.1145/2484762.2484816</pub-id></element-citation></ref><ref id="bib164"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cooperative switch determines the sign of synaptic plasticity in distal dendrites of neocortical pyramidal neurons</article-title><source>Neuron</source><volume>51</volume><fpage>227</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.017</pub-id><pub-id pub-id-type="pmid">16846857</pub-id></element-citation></ref><ref id="bib165"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SL</given-names></name><name><surname>Smith</surname><given-names>IT</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dendritic spikes enhance stimulus selectivity in cortical neurons in vivo</article-title><source>Nature</source><volume>503</volume><fpage>115</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1038/nature12600</pub-id><pub-id pub-id-type="pmid">24162850</pub-id></element-citation></ref><ref id="bib166"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steffenburg</surname><given-names>U</given-names></name><name><surname>Hagberg</surname><given-names>G</given-names></name><name><surname>Hagberg</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Epilepsy in a representative series of rett syndrome</article-title><source>Acta Paediatrica</source><volume>90</volume><fpage>34</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1080/080352501750064842</pub-id><pub-id pub-id-type="pmid">11227330</pub-id></element-citation></ref><ref id="bib167"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stuart</surname><given-names>G.</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Determinants of voltage attenuation in neocortical pyramidal neuron dendrites</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>3501</fpage><lpage>3510</lpage><pub-id pub-id-type="pmid">9570781</pub-id></element-citation></ref><ref id="bib168"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stuart</surname><given-names>GJ</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dendritic integration: 60 years of progress</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1713</fpage><lpage>1721</lpage><pub-id pub-id-type="doi">10.1038/nn.4157</pub-id><pub-id pub-id-type="pmid">26605882</pub-id></element-citation></ref><ref id="bib169"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stuart</surname><given-names>G</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Dendrites</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198745273.001.0001</pub-id></element-citation></ref><ref id="bib170"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>YJ</given-names></name><name><surname>Kim</surname><given-names>Y-J</given-names></name><name><surname>Ibrahim</surname><given-names>LA</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Synaptic mechanisms underlying functional dichotomy between intrinsic-bursting and regular-spiking neurons in auditory cortical layer 5</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>5326</fpage><lpage>5339</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4810-12.2013</pub-id><pub-id pub-id-type="pmid">23516297</pub-id></element-citation></ref><ref id="bib171"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>H</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Pathway interactions and synaptic plasticity in the dendritic tuft regions of CA1 pyramidal neurons</article-title><source>Neuron</source><volume>62</volume><fpage>102</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.007</pub-id><pub-id pub-id-type="pmid">19376070</pub-id></element-citation></ref><ref id="bib172"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>N</given-names></name><name><surname>Oertner</surname><given-names>TG</given-names></name><name><surname>Hegemann</surname><given-names>P</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Active cortical dendrites modulate perception</article-title><source>Science</source><volume>354</volume><fpage>1587</fpage><lpage>1590</lpage><pub-id pub-id-type="doi">10.1126/science.aah6066</pub-id><pub-id pub-id-type="pmid">28008068</pub-id></element-citation></ref><ref id="bib173"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teeter</surname><given-names>C</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name><name><surname>Gouwens</surname><given-names>N</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Hawrylycz</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Generalized leaky integrate-and-fire models classify multiple neuron types</article-title><source>Nature Communications</source><volume>9</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41467-017-02717-4</pub-id><pub-id pub-id-type="pmid">29459723</pub-id></element-citation></ref><ref id="bib174"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Thibeault</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A novel multi-GPU neural simulator</article-title><conf-name>BICoB</conf-name></element-citation></ref><ref id="bib175"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Traub</surname><given-names>RD</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name><name><surname>Cunningham</surname><given-names>MO</given-names></name><name><surname>Murray</surname><given-names>H</given-names></name><name><surname>LeBeau</surname><given-names>FEN</given-names></name><name><surname>Roopun</surname><given-names>A</given-names></name><name><surname>Bibbig</surname><given-names>A</given-names></name><name><surname>Wilent</surname><given-names>WB</given-names></name><name><surname>Higley</surname><given-names>MJ</given-names></name><name><surname>Whittington</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Single-column thalamocortical network model exhibiting gamma oscillations, sleep spindles, and epileptogenic bursts</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>2194</fpage><lpage>2232</lpage><pub-id pub-id-type="doi">10.1152/jn.00983.2004</pub-id><pub-id pub-id-type="pmid">15525801</pub-id></element-citation></ref><ref id="bib176"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>MV</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability</article-title><source>PNAS</source><volume>94</volume><fpage>719</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.2.719</pub-id><pub-id pub-id-type="pmid">9012851</pub-id></element-citation></ref><ref id="bib177"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Li</surname><given-names>WK</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Pandi</surname><given-names>I</given-names></name><name><surname>O’Hare</surname><given-names>J</given-names></name><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>Ladow</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>JF</given-names></name><name><surname>Zemelman</surname><given-names>BV</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Vasoactive intestinal polypeptide-expressing interneurons in the hippocampus support goal-oriented spatial learning</article-title><source>Neuron</source><volume>101</volume><fpage>1150</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.009</pub-id><pub-id pub-id-type="pmid">30713030</pub-id></element-citation></ref><ref id="bib178"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ujfalussy</surname><given-names>BB</given-names></name><name><surname>Makara</surname><given-names>JK</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Global and multiplexed dendritic computations under in vivo-like conditions</article-title><source>Neuron</source><volume>100</volume><fpage>579</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.032</pub-id><pub-id pub-id-type="pmid">30408443</pub-id></element-citation></ref><ref id="bib179"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>P</given-names></name><name><surname>Roth</surname><given-names>A</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Propagation of action potentials in dendrites depends on dendritic morphology</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>926</fpage><lpage>937</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.85.2.926</pub-id><pub-id pub-id-type="pmid">11160523</pub-id></element-citation></ref><ref id="bib180"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villette</surname><given-names>V</given-names></name><name><surname>Chavarha</surname><given-names>M</given-names></name><name><surname>Dimov</surname><given-names>IK</given-names></name><name><surname>Bradley</surname><given-names>J</given-names></name><name><surname>Pradhan</surname><given-names>L</given-names></name><name><surname>Mathieu</surname><given-names>B</given-names></name><name><surname>Evans</surname><given-names>SW</given-names></name><name><surname>Chamberland</surname><given-names>S</given-names></name><name><surname>Shi</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>R</given-names></name><name><surname>Kim</surname><given-names>BB</given-names></name><name><surname>Ayon</surname><given-names>A</given-names></name><name><surname>Jalil</surname><given-names>A</given-names></name><name><surname>St-Pierre</surname><given-names>F</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Bi</surname><given-names>G</given-names></name><name><surname>Toth</surname><given-names>K</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Dieudonné</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>MZ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ultrafast two-photon imaging of a high-gain voltage indicator in awake behaving mice</article-title><source>Cell</source><volume>179</volume><fpage>1590</fpage><lpage>1608</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.11.004</pub-id><pub-id pub-id-type="pmid">31835034</pub-id></element-citation></ref><ref id="bib181"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vitay</surname><given-names>J</given-names></name><name><surname>Dinkelbach</surname><given-names>HÜ</given-names></name><name><surname>Hamker</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ANNarchy: a code generation approach to neural simulations on parallel hardware</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00019</pub-id><pub-id pub-id-type="pmid">26283957</pub-id></element-citation></ref><ref id="bib182"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vooturi</surname><given-names>DT</given-names></name><name><surname>Kothapalli</surname><given-names>K</given-names></name><name><surname>Bhalla</surname><given-names>US</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallelizing Hines Matrix Solver in Neuron Simulations on GPU</article-title><conf-name>2017 IEEE 24th International Conference on High Performance Computing (HiPC</conf-name><pub-id pub-id-type="doi">10.1109/HiPC.2017.00051</pub-id></element-citation></ref><ref id="bib183"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Gamma oscillation by synaptic inhibition in a hippocampal interneuronal network model</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>6402</fpage><lpage>6413</lpage><pub-id pub-id-type="pmid">8815919</pub-id></element-citation></ref><ref id="bib184"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Computational principles of movement neuroscience</article-title><source>Nature Neuroscience</source><volume>3 Suppl</volume><fpage>1212</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1038/81497</pub-id><pub-id pub-id-type="pmid">11127840</pub-id></element-citation></ref><ref id="bib185"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wybo</surname><given-names>WA</given-names></name><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Ellenberger</surname><given-names>B</given-names></name><name><surname>Marti Mengual</surname><given-names>U</given-names></name><name><surname>Nevian</surname><given-names>T</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Data-driven reduction of dendritic morphologies with preserved dendro-somatic responses</article-title><source>eLife</source><volume>10</volume><elocation-id>e60936</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.60936</pub-id><pub-id pub-id-type="pmid">33494860</pub-id></element-citation></ref><ref id="bib186"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yavuz</surname><given-names>E</given-names></name><name><surname>Turner</surname><given-names>J</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GeNN: a code generation framework for accelerated brain simulations</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/srep18854</pub-id><pub-id pub-id-type="pmid">26740369</pub-id></element-citation></ref><ref id="bib187"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>McTavish</surname><given-names>TS</given-names></name><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Shepherd</surname><given-names>GM</given-names></name><name><surname>Valenti</surname><given-names>C</given-names></name><name><surname>Migliore</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sparse distributed representation of odors in a large-scale olfactory bulb circuit</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003014</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003014</pub-id><pub-id pub-id-type="pmid">23555237</pub-id></element-citation></ref><ref id="bib188"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z-W</given-names></name><name><surname>Zak</surname><given-names>JD</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>MeCP2 is required for normal development of gabaergic circuits in the thalamus</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>2470</fpage><lpage>2481</lpage><pub-id pub-id-type="doi">10.1152/jn.00601.2009</pub-id><pub-id pub-id-type="pmid">20200124</pub-id></element-citation></ref><ref id="bib189"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Peterson</surname><given-names>M</given-names></name><name><surname>Beyer</surname><given-names>B</given-names></name><name><surname>Frankel</surname><given-names>WN</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Loss of mecp2 from forebrain excitatory neurons leads to cortical hyperexcitation and seizures</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>2754</fpage><lpage>2763</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4900-12.2014</pub-id><pub-id pub-id-type="pmid">24523563</pub-id></element-citation></ref><ref id="bib190"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Santaniello</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Role of cerebellar gabaergic dysfunctions in the origins of essential tremor</article-title><source>PNAS</source><volume>116</volume><fpage>13592</fpage><lpage>13601</lpage><pub-id pub-id-type="doi">10.1073/pnas.1817689116</pub-id><pub-id pub-id-type="pmid">31209041</pub-id></element-citation></ref><ref id="bib191"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Song</surname><given-names>D</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>X</given-names></name><name><surname>Lumezanu</surname><given-names>C</given-names></name><name><surname>Cheng</surname><given-names>W</given-names></name><name><surname>Ni</surname><given-names>J</given-names></name><name><surname>Zong</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Chawla</surname><given-names>NV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><volume>33</volume><fpage>1409</fpage><lpage>1416</lpage><pub-id pub-id-type="doi">10.1609/aaai.v33i01.33011409</pub-id></element-citation></ref><ref id="bib192"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>E</given-names></name><name><surname>Ge</surname><given-names>Y</given-names></name><name><surname>Zhao</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Time series classification using multi-channels deep convolutional neural networks</article-title><conf-name>International conference on web-age information management</conf-name><pub-id pub-id-type="doi">10.1007/978-3-319-08010-9</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79535.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bhalla</surname><given-names>Upinder Singh</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ht1xw27</institution-id><institution>Tata Institute of Fundamental Research</institution></institution-wrap><country>India</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.02.22.432356" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.02.22.432356"/></front-stub><body><p>This study describes the use of artificial neural network (ANN) methods to accurately replicate the biophysical behavior of detailed single-neuron models. The method has the potential to greatly increase the speed of neuronal modeling compared to conventional differential equation-based modeling, and scales particularly well for large network models. The authors demonstrate the fidelity of their ANN model cells over a wide range of stimulus and recording conditions including electrical and optical readouts.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79535.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bhalla</surname><given-names>Upinder Singh</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ht1xw27</institution-id><institution>Tata Institute of Fundamental Research</institution></institution-wrap><country>India</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Davison</surname><given-names>Andrew P</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002v40q27</institution-id><institution>Paris-Saclay Institute of Neuroscience</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.02.22.432356">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.02.22.432356v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Ultrafast Simulation of Large-Scale Neocortical Microcircuitry with Biophysically Realistic Neurons&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board if Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Andrew P. Davison (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1. The reviewers all felt this was a potentially exciting advance for speeding up neuronal simulations.</p><p>2. Can the authors more clearly compare the accuracy of NEURON and ANN network simulations, especially as a function of the duration of simulation? Current injection comparisons would also be useful.</p><p>3. It would help to have more detail on how the approach would scale in network simulations, especially as the synaptic connectivity is increased, and more cell types are introduced.</p><p>4. The reviewers all would like to see better software documentation and tutorials for the various steps in model implementation.</p><p>5. The reviewers would like to see a clearer comparison between NEURON and the ANN on different architectures.</p><p>6. Can the authors include details on the process and computational resources required to train the ANN? One expects that this is extensively documented in the code repository, but there should be a good starting account of this in the body of the paper.</p><p>7. Can the authors place their work in a somewhat better context? The reviewers pointed out some prior work by Beniaguev et al., and would like to see more detail on how the method might handle some existing complex simulations.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. The authors state that the code will be available upon publication. This precludes the ability of the reviewers to test the code, comment on its usability, and see how well it is documented. For a methods paper, this is a surprising omission and I cannot complete the review without full code availability. Ideally, this should be in an anonymous form such as uploaded to the Journal website or provided as a package for pip install.</p><p>2. Can the authors more completely document the (a) process and (b) the computational resources required for training the ANN? Ideally (a) should be packaged in a manner where the user gives the system a model specified in NEUROML or Neuron code, and it generates the ANN a few minutes later. Maybe the authors could even provide a web resource to do this. (b) is also important to know – do we need a supercomputer to train the ANN, even if it subsequently runs on a laptop? Can the authors properly benchmark this, just as they have benchmarked runtime resource requirements? For example, what does it take to train a multicompartmental model? How does it scale with the number of compartments and variety of ion channels?</p><p>Specific points</p><p>3. Figure 5 seems to show that the ANN does indeed have an internal representation of the input placement and its effect on somatic potential. It would be very useful to see if additional readouts could report dendritic potential and Ca levels. Is there a way to read out a couple of things that would be of great interest to people studying dendritic computation?</p><p>– The membrane potential at different points on the dendrites.</p><p>– The calcium levels at different points on the dendrites.</p><p>4. Can the authors provide a readout in terms of Ca fluorescent signals?</p><p>This is now one of the major ways of monitoring large numbers of neurons in vivo in networks.</p><p>5. Can the authors explain what changes in NEURON with initialization? This seems to be used as an optional step in the comparisons with the ANN neuronal mode.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>My main comments are mostly driven by practical considerations. If one wants to use the method and the code, one would like to know the following.</p><p>– What happens if more synapses are added? For example, the L5 PC case is presented with 200 synapses. What if one needs to use 2,000 or 20,000 synapses, which is a more realistic scenario – will one need to re-train the ANN, or will it work out of the box?</p><p>– How does the model performance change with time beyond the NEURON-simulated period that ANN is trained on? I assume that after some time the voltage trace generated by the ANN will diverge from the NEURON-simulated one, especially with respect to the timing of APs. Can the authors show a figure where such divergence is characterized as a function of time? For example, if one trains the ANN for 1 second of a NEURON simulation, how well does the ANN simulation compare to the NEURON simulation at 5 seconds? How about 10 or 100 seconds?</p><p>– How well can the trained ANN mimic responses of the neuron to current injections? Current injections (e.g., with synaptic inputs blocked) are often used to probe intrinsic properties of neurons, and there's much data available from such experiments. These data provide a natural way for model builders to test how well their neuron models are working. Furthermore, realistic perturbations that one may want to model – such as optogenetic perturbations – can often be represented rather well as an injection of positive or negative current to a cell. Can the authors demonstrate that their ANN correctly reproduces a voltage response of a NEURON-simulated cell, for example, to a step current injection?</p><p>Additional comments:</p><p>– Figure 1 (and the rest of the manuscript): the variance explained for the &quot;winning&quot; ANN is ~50%, which doesn't sound high. However, the ANN trace looks very close to the NEURON trace. The authors may want to elaborate on the way the agreement is quantified as the variance is explained. Maybe it will help if they compute the variance explained for the voltage traces with APs clipped. Will the variance explained be much higher in that case? It might be worth reporting that along with the variance explained for the traces that include APs (as shown currently in Figure 1).</p><p>– Figure 5 – the variance explained, precision, and recall are only shown for L5 PC, but not for L2/3, L4, and L6 PC. The precision and recall for these cells are summarized in the text, combined for the 3 neurons. It would be important to show all 3 quantities individually for each neuron, just like they are shown here for the L5 PC.</p><p>– Figure 6 – As far as I can tell, these are not connected networks. Simulating 5,000 disconnected cells is very different from 5,000 highly interconnected cells, and the speed-ups can be drastically different. This is OK for the purposes of this manuscript, but the description should be clear about what's being done. The text mentions &quot;network&quot; everywhere in this section, including its title. The authors should change it and make it clear that simulations involve 50 or 5,000 disconnected cells. Or, if I got this wrong, and these are indeed simulations of connected networks with 50 or 5,000 cells, then please provide the description of the network connectivity, synaptic weights, etc. (In Methods, I only see the description of a 150-neuron network for Figures 7 and 8.)</p><p>– Figure 6 – also, the authors may want to say something here about the comparison of an ANN on GPU vs. NEURON on 1 CPU is not perfect. Ideally, one would run the ANN and NEURON simulations on the same parallel hardware and compare the performance as a function of the number of parallel cores used. I understand that is hard to achieve, so it is fair that the authors do not show such a comparison. However, it is instructive to consider the following thought experiment. Even if one ran the NEURON simulation of 5,000 cells on 5,000 CPUs, the performance would likely be about the same as that for one cell on one CPU. But even then, the time of the NEURON simulation would be ~185 s (for the L5 PC), whereas the time of the CNN simulation on a SINGLE GPU is ~12 s. So, the CNN is over 10 times more efficient on a single GPU than one expects NEURON to be on 5,000 CPUs.</p><p>– Simulations of the Rett syndrome model – it might be useful to give a little more detail about the network used for these simulations in the Results (otherwise one has to check Methods for all the details). The important piece to mention is that the network does not have any inhibitory cells, and instead, inhibition is provided as external inputs together with excitation. In other words, it is a feedforward inhibition model (if I understood it correctly).</p><p>– Figure 7c, parameter mapping – I assume the bar for NEURON is interpolation?</p><p>– Page 22, &quot;which means that a complete cortical area can be simulated using only 17 ANNs&quot; – I am not sure this is correct. The Billeh et al., model used about 100 distinct neuronal models belonging to 17 cell types. So, simulation of this model would require about 100 ANNs, rather than 17. Of course, this is still a huge improvement relative to the hundreds of thousands of neurons in the original NEURON model.</p><p>– Discussion – the authors almost do not mention the closely related work by Beniaguev et al., (Neuron, 2022), though they do cite that paper. I believe the work by Olah et al., is sufficiently different and novel, and it offers many interesting new insights as well as opportunities for computational neuroscientists who might want to use this method and code. I would suggest that the authors add a paragraph to the Discussion and describe how their work differs from Beniaguev et al., and what their unique contributions are.</p><p>– Data and software availability – the GitHub link doesn't work. I assume the authors plan to make it public upon paper publication. But it would be nice to provide the code to the reviewers, to get some idea about the completeness of the code, since it represents one of the main results of this paper. It is also important to mention that the code shared with the community should include the functions and procedures for training the ANNs. That is one of the most valuable contributions, which will be of great interest to many neuroscientists.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I think this study is very nice. As noted above in the Public Review, however, I think the manuscript would be greatly improved and its impact increased by (i) showing an accuracy comparison of the results obtained with NEURON and those obtained with the ANN network for the Rett syndrome circuit model, (ii) adding performance measures for the GeNN simulator, or some other simulator that is designed to run on GPUs, at least for the point neuron model.</p><p>The availability of the source code is very welcome. However, it is not well documented. The impact of this study would be increased by providing at least a README explaining the structure of the repository, and ideally by providing instructions for reproducing at least some of your results (e.g. generating the training data, training the ANNs, using the trained networks to generate predictions, etc.)</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Ultrafast Simulation of Large-Scale Neocortical Microcircuitry with Biophysically Realistic Neurons&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Joshua Gold (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The authors have substantially addressed most issues raised by the reviewers.</p><p>I would like to come back to several points in the revised version where more details in the text would greatly improve the accessibility of the study.</p><p>1. One of the key earlier reviewer points has to do with scaling with connected network size, especially with very large numbers of synapses. While the authors have responded, I was not able to understand this, and hence ask for a more complete explanation in the text so that it becomes more accessible to the readers.</p><p>The authors say:</p><p>&quot;We thank the reviewers for pointing this out. This issue is now added to the discussion. Briefly, synaptic connectivity has no impact on simulation runtimes as the matrix transformations necessary for implementing connections take place regardless of whether two given cells are connected or not. On the other hand, inclusion of additional cell types linearly increases simulation times (assuming comparable cell numbers per cell type), as every cell type warrants the execution of additional artificial neural nets every time step.&quot;</p><p>Can the authors explain this matrix transformation step and its mapping to synaptic connectivity? I did not find an explanation in either the text or the responses to the reviewers. Possibly it may help if I were to reiterate the synaptic connectivity bottleneck in conventional simulations.</p><p>2. Each individual synaptic projection introduces a distinct delay in how long it takes for the source action potential to reach the postsynaptic synapse. This delay can be up to 10ms or sometimes longer depending on axon fiber type and length. 2. Each postsynaptic synapse is usually implemented as a conductance change obeying a single or dual α function of time. such as gSyn = gPeak * 1/tp * exp(1 -t/tp) where t is time since spike arrived at synapse and tp is time of peak of synaptic conductance.</p><p>The common observation in large spiking network models is that the combination of these calculations can lead to quite large demands, including in managing the event queues to implement the synaptic delays, since the delays may be long enough to permit multiple action potentials. The synaptic dynamics of the α functions also introduce a computational cost. Since the number of synaptic connections is very large, in some large simulations the computation time is dominated by synaptic transmission.</p><p>It would be helpful if the authors can respond by addressing a few specific points, and include the information in the text.</p><p>a. Confirm and elaborate on how their method indeed accomplishes the same computations as this, both the distinct synaptic delay for each synapse, and the equivalent of α function synapses.</p><p>b. Explain how their matrix transform addresses the two computational bottlenecks that occur with the conventional simulation approach,</p><p>c. The authors on the one hand state (line 594) &quot;the number of contact sites directly correlates with simulation runtimes and memory consumption.&quot;, and on the other they say that synaptic connectivity has no impact on simulation runtimes. Please clarify what is different here.</p><p>3. Could the authors move some further details of the ANN training into the paper? For example, I did not see the time taken to train the ANN (~24 hours from the response to reviewers) stated in the paper. It would be very helpful for people trying to implement such networks to know what to expect in terms of training resources and time, not to mention the learning curve for the researchers themselves to figure out how to do the training.</p><p>A related point: the data availability statement explains how to access the generated models. I did not see a clear mention of the code and resources used to build the ANNs from the training set.</p><p>I understand we are still in the early days of the use of this method. It took several years after the development of the underlying matrix calculation code for neuronal calculations before there were a couple of standard simulators that helped with many things from standard libraries to graphical interfaces. Nevertheless, it would be very helpful if the authors could provide a more complete indication in the paper of what it would take for users to do such model building for themselves.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79535.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The reviewers all felt this was a potentially exciting advance for speeding up neuronal simulations.</p></disp-quote><p>We thank the reviewers for their enthusiasm.</p><disp-quote content-type="editor-comment"><p>2. Can the authors more clearly compare the accuracy of NEURON and ANN network simulations, especially as a function of the duration of simulation? Current injection comparisons would also be useful.</p></disp-quote><p>We thank the reviewers for these helpful points. To address these questions, we have made several changes and have now added new additional data. First, we included an additional panel detailing the absence of accumulating errors during simulations. Our results show a lack of compounding deviation from NEURON simulations both in terms of raw difference and in explained variance as well. Second, we extended the ANN model with the ability to generate firing patterns upon current injections in a separate figure. To this end, we decided to showcase a promising feature of ANNs, where preselected neuronal dynamics can be hard-coded in custom layers to aid both learning and precision. We selected the ‘Izhikevich’ formulation (Izhikevich 2003) as the basis of action potential generation and created a custom ANN layer that can handle both previously shown synaptic dynamics and current injection as well, to produce a variety of activity patterns.</p><disp-quote content-type="editor-comment"><p>3. It would help to have more detail on how the approach would scale in network simulations, especially as the synaptic connectivity is increased, and more cell types are introduced.</p></disp-quote><p>We thank the reviewers for pointing this out. This issue is now added to the discussion. Briefly, synaptic connectivity has no impact on simulation runtimes as the matrix transformations necessary for implementing connections take place regardless of whether two given cells are connected or not. On the other hand, inclusion of additional cell types linearly increases simulation times (assuming comparable cell numbers per cell type), as every cell type warrants the execution of additional artificial neural nets every time step.</p><disp-quote content-type="editor-comment"><p>4. The reviewers all would like to see better software documentation and tutorials for the various steps in model implementation.</p></disp-quote><p>We agree and have now made the code publicly available, with documentation. The code is currently available on a public Github repository (https://github.com/ViktorJOlah/Neuro_ANN), and we have uploaded it to Dryad (https://doi.org/10.5061/dryad.0cfxpnw60 , temporary link: https://datadryad.org/stash/share/keJYkykT0Vv0h6YdA1wFNMyfEBk8TUYgRk79szUJKHM).</p><disp-quote content-type="editor-comment"><p>5. The reviewers would like to see a clearer comparison between NEURON and the ANN on different architectures.</p></disp-quote><p>We thank the reviewers for pointing this out and made further elaborations on the subject matter. The NEURON simulation environment is designed to run on CPUs, while ANNs created in Tensorflow are most suitable for GPUs. The inherent disparity between the two computational resources raises the possibility that the main advantage of our ANN approach arises from the computational units employed. Indeed, one of the key advancement of our manuscript is the implementation of neuronal simulations on a computational resource much faster, than what has been traditionally used. However, as Tensorflow models can be run on CPUs, we purposefully compared the two simulation environments on the same resource (a single core CPU). We have shown, that even when ANN simulations were intentionally run on a slower processing units, NEURON was only faster, when single point neurons were simulated (Figure 7). This proves, that ANN simulations not only benefit from their suitability for GPUs, but are much faster on the same resource as well. The manuscript is now extended with these clarifications (page 13).</p><disp-quote content-type="editor-comment"><p>6. Can the authors include details on the process and computational resources required to train the ANN? One expects that this is extensively documented in the code repository, but there should be a good starting account of this in the body of the paper.</p></disp-quote><p>We agree with the reviewers, and further elaborated on the computational resources used throughout the Results section where appropriate.</p><disp-quote content-type="editor-comment"><p>7. Can the authors place their work in a somewhat better context? The reviewers pointed out some prior work by Beniaguev et al., and would like to see more detail on how the method might handle some existing complex simulations.</p></disp-quote><p>We agree with this point and have now included a detailed comparison of our work and the mentioned prior publication. As we mentioned in the Introduction section, it was postulated that ANNs might be used to represent neuronal activity in single cell models by Poirazi et al., in 2003. The recent publication suggested by reviewers (Beniaguev et al.,) did implement this approach in practice.</p><p>Our aim was to create a more versatile tool to serve as a viable substituent for traditional modeling systems, with substantial innovation compared with previous work. For example, our ANN model has the ability for generalization, can produce sequential output, and generates temporally accurate action potentials without external thresholding. We demonstrated that the CNN-LSTM architecture is well suited for this task. To our knowledge, this study is the first use of ANNs that can not only serve as an alternative for single cell neuronal modeling of single, biophysically and anatomically realistic neurons, but can also produce unprecedented acceleration in network simulation runtimes.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. The authors state that the code will be available upon publication. This precludes the ability of the reviewers to test the code, comment on its usability, and see how well it is documented. For a methods paper, this is a surprising omission and I cannot complete the review without full code availability. Ideally, this should be in an anonymous form such as uploaded to the Journal website or provided as a package for pip install.</p></disp-quote><p>We agree and have now made the code publicly available, with documentation. The code is currently available on a public Github repository (https://github.com/ViktorJOlah/Neuro_ANN), and we have uploaded it to Dryad (https://doi.org/10.5061/dryad.0cfxpnw60 , temporary link: https://datadryad.org/stash/share/keJYkykT0Vv0h6YdA1wFNMyfEBk8TUYgRk79szUJKHM) as per instructions from the <italic>eLife</italic> editors.</p><disp-quote content-type="editor-comment"><p>2. Can the authors more completely document the (a) process and (b) the computational resources required for training the ANN? Ideally (a) should be packaged in a manner where the user gives the system a model specified in NEUROML or Neuron code, and it generates the ANN a few minutes later. Maybe the authors could even provide a web resource to do this. (b) is also important to know – do we need a supercomputer to train the ANN, even if it subsequently runs on a laptop? Can the authors properly benchmark this, just as they have benchmarked runtime resource requirements? For example, what does it take to train a multicompartmental model? How does it scale with the number of compartments and variety of ion channels?</p></disp-quote><p>We thank the reviewer for pointing this out. Besides making the code publicly available, we now further elaborated on ANN training in the Results section (page 9) by including a flowchart of the training process (Figure 5 —figure supplement 2.). Although the minutes time scale is currently unfeasible for complete ANN training, as our naïve architectures take roughly one day (24 hours) to train with current methods, there are several ways to expedite this process. Notably, transfer learning allows the reuse of previously trained ANN architectures for fitting on similar cell types. In order to properly take advantage of this method, several criteria have to be fulfilled. First, the number of synaptic contacts has to be the same, as input matrices with different dimensions require different ANN architectures. Second, the spatial relationship of the contact sites must be preserved, because a key task of the ANN is to learn the rules of synaptic interplay between selected contact sites. If this relationship is disrupted, the internal logic of the ANN is useless.</p><p>Our central aim was to lower the barrier to entry for large scale simulations of realistic neurons. This barrier is often the lack of computational resources needed for executing these experiments. Therefore, we carefully considered several publicly available options, and decided that Google Colaboratory, which is a free and public resource, was ideally suited for this purpose. The most daunting task, ANN training, was carried out in Google Colaboratory. Therefore no supercomputers are needed to replicate our findings. These details are now detailed in the methods (page 27).</p><p>The field of machine learning is rapidly evolving, producing new ANN architectures and inspired solutions constantly. Therefore, the presented ANN architecture only serves to illustrate one possibility of substituting NEURON simulations with an ANN based approached. Our architecture is not yet optimized, which is illustrated by the fact that single compartmental neurons and reconstructed L5 PC ANNs have similar number of trainable parameters despite representing cells with vastly different complexities. Architecture optimization even for simple models require immense computational resources. Therefore, we did not carry out training benchmarking, as it would be performed on unoptimized architectures.</p><p>Training of biophysicaly and morphologically realistic multicompartmental was carried out in Google Collaboratory, which is a free resource. Although this process was the most computationally exhausting task included in the paper, this step didn’t require supercomputers either. However, some simplifications had to be made for this to be feasibly carried out on the mentioned resource. The main simplification is the reduction in the number of synaptic contact sites (which doesn’t limit the number of presynaptic cells; this issue is addressed in the Discussion section). The input of the ANN contained somatic membrane potentials and synaptic input vectors. Besides the first one (membrane voltage), every column in the input matrix corresponds to a synaptic contact site. Adding more synaptic contact sites would place more memory requirements on ANN training, thereby limiting the number of trainable instances, which is the most crucial aspect of ANN fitting. On the other hand, as we mentioned previously, our current ANN architectures are unoptimized, meaning that similar number of trainable parameters were fitted on passive single compartmental cells and fully reconstructed multicompartmental active cells as well, therefore the number of ion channels in the original NEURON models had no effect of ANN fitting or query.</p><disp-quote content-type="editor-comment"><p>Specific points</p><p>3. Figure 5 seems to show that the ANN does indeed have an internal representation of the input placement and its effect on somatic potential. It would be very useful to see if additional readouts could report dendritic potential and Ca levels. Is there a way to read out a couple of things that would be of great interest to people studying dendritic computation?</p><p>– The membrane potential at different points on the dendrites.</p><p>– The calcium levels at different points on the dendrites.</p></disp-quote><p>We thank the reviewer for this question. As we have shown on a single compartmental cell, it is possible to read out additional features of neuronal activity from the ANN, such as sodium and potassium current fluctuations (page 8, Figure 3). However, we recognize the importance of demonstrating the ability of voltage readout from multiple subcellular segments, therefore we now have included an additional supplementary figure (Figure 5 —figure supplement 3.), demonstrating the voltage and calcium level readout from multiple neuronal compartments using an ANN representing a fully reconstructed morphologically realistic cortical neuron.</p><disp-quote content-type="editor-comment"><p>4. Can the authors provide a readout in terms of Ca fluorescent signals?</p><p>This is now one of the major ways of monitoring large numbers of neurons in vivo in networks.</p></disp-quote><p>We thank the reviewer for this suggestion. We agree that calcium fluorescence readouts provide crucial information about network activity, and methods built around recording these signals are now cornerstones of modern neuroscience. We recognize that demonstrating calcium fluorescence output from ANNs can entice a large community of researchers to utilize ANN based network modeling. Therefore we have extended our manuscript with a supplementary figure (Figure 6 —figure supplement 1.) showcasing accurate fluorescence readout from ANNs. Briefly, fluorescent indicators give rise to a compound signal depending on internal calcium (or voltage, in case of voltage reporters) fluctuations and the dynamics of the reporter. We demonstrated previously, that CNN-LSTMs can predict ion channel dynamics and membrane voltage simultaneously, therefore the first part of the compound signal can be properly addressed. The second part, corresponding to the dynamics of the reporter, is fixed, therefore it would theoretically require no refitting in between different architectures and cell types. Therefore, we created a custom ANN encoder, which can be trained on reporter dynamics, and can subsequently be added to the trained CNN-LSTMs. This encoder can be added to every CNN-LSTM detailed in our manuscript, and requires no further training.</p><disp-quote content-type="editor-comment"><p>5. Can the authors explain what changes in NEURON with initialization? This seems to be used as an optional step in the comparisons with the ANN neuronal mode.</p></disp-quote><p>Initialization is an optional step in NEURON simulation that ensures that all conductances are in steady state conditions at the start of the simulation, and therefore the beginning phase of the simulation is not contaminated by initialized membrane potential dependent currents. We now have included additional clarification regarding initialization in the main text (page 13).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>My main comments are mostly driven by practical considerations. If one wants to use the method and the code, one would like to know the following.</p><p>– What happens if more synapses are added? For example, the L5 PC case is presented with 200 synapses. What if one needs to use 2,000 or 20,000 synapses, which is a more realistic scenario – will one need to re-train the ANN, or will it work out of the box?</p></disp-quote><p>We thank the reviewer for the question. We agree that the 200 synapses used for L5 PC simulations do not represent all potentially active synapses, however, this number aimed to represent 200 synaptic contact sites, which can be occupied by multiple synapses at the same time. We realize that there are certain model instances necessitating a higher number of synaptic contact sites. This is feasible, but naturally results in lower simulation speeds and much higher memory consumption. To explore the details this of this issue, we now included an additional supplementary figure (Figure 5 —figure supplement 1.) demonstrating that increasing the number of synaptic contact sites yields greater accuracy compared to spatially non-discretized simulations- but only up to a certain point. Simulation accuracy was quantified by comparing voltage traces gathered from simulations with truly randomly placed synapses to simulations with the synapses placed evenly on a restricted number of synaptic contact sites. Unsurprisingly, this also causes a higher computational load and increased simulation runtimes.</p><p>The present CNN-LSTM architecture therefore allows the addition of unlimited numbers of synapses to the same synaptic contact sites, and in this case the model will work out of the box, however, if the number of synaptic contact sites is to be modified, the ANN needs to be retrained, due to the differences in graph structures.</p><disp-quote content-type="editor-comment"><p>– How does the model performance change with time beyond the NEURON-simulated period that ANN is trained on? I assume that after some time the voltage trace generated by the ANN will diverge from the NEURON-simulated one, especially with respect to the timing of APs. Can the authors show a figure where such divergence is characterized as a function of time? For example, if one trains the ANN for 1 second of a NEURON simulation, how well does the ANN simulation compare to the NEURON simulation at 5 seconds? How about 10 or 100 seconds?</p></disp-quote><p>We thank the reviewer for raising this question. To address the possibility of ANN divergence, we now demonstrate that prediction error is stationary and does not accumulate with time. Our simulation (25 seconds) surprisingly shows reduced prediction errors and higher explained variance over time, even when synaptic activity was increased. We attribute the lack of compounding error to the multistep training approach we took, where high emphasis was put on learning resting membrane potential as the first training step (Figure 5 —figure supplement 2.). The stimulation pattern used for predictions was completely new to the ANN, which was previously trained using different synaptic weights and input frequencies. We have extended our manuscript with these explanations and new panels (Figure 1, panel I,J,K) highlighting these findings.</p><disp-quote content-type="editor-comment"><p>– How well can the trained ANN mimic responses of the neuron to current injections? Current injections (e.g., with synaptic inputs blocked) are often used to probe intrinsic properties of neurons, and there's much data available from such experiments. These data provide a natural way for model builders to test how well their neuron models are working. Furthermore, realistic perturbations that one may want to model – such as optogenetic perturbations – can often be represented rather well as an injection of positive or negative current to a cell. Can the authors demonstrate that their ANN correctly reproduces a voltage response of a NEURON-simulated cell, for example, to a step current injection?</p></disp-quote><p>We thank the reviewer for raising this issue. We recognize that one of the most common ways to characterize neuronal cells is to identify its firing behavior upon current injection. This technique is widely recognized as one of the most thorough classifiers of neuronal classes besides their anatomical features, as firing properties not only hint at the repertoire of conductances shaping neuronal excitability, but also provides information about the cell’s potential in vivo functions. Therefore, we addressed this important question by creating a new, custom ANN layer capable of generating a diverse array of firing patterns (Figure 6.). This layer is a based on the ‘Izhikevich’ equations (Izhikevich 2003), extended with the option for membrane time constant modulation.</p><p>Although ANNs are considered “black boxes” which only require minimal information about the internal encoding logic, our approach here demonstrates that hard-coding different aspects of neuronal dynamics within the ANN is also possible. Furthermore, the variables of this layer are freely adjustable, therefore the experimenter gains the ability to modify the cell’s natural activity pattern instantaneously, without altering synaptic rules or having to retrain the ANN. As the Izhikevich formulation has only four variables, neuronal firing patterns can first be fitted outside of Tensorflow, with conventional fitting algorithms. We have extended our manuscript with a new figure (Figure 6) and the corresponding explanations highlighting these findings. Additional details have also been added to the methods section and this code will be made publically available as described above.</p><disp-quote content-type="editor-comment"><p>Additional comments:</p><p>– Figure 1 (and the rest of the manuscript): the variance explained for the &quot;winning&quot; ANN is ~50%, which doesn't sound high. However, the ANN trace looks very close to the NEURON trace. The authors may want to elaborate on the way the agreement is quantified as the variance is explained. Maybe it will help if they compute the variance explained for the voltage traces with APs clipped. Will the variance explained be much higher in that case? It might be worth reporting that along with the variance explained for the traces that include APs (as shown currently in Figure 1).</p></disp-quote><p>We have made these changes as recommended.</p><disp-quote content-type="editor-comment"><p>– Figure 5 – the variance explained, precision, and recall are only shown for L5 PC, but not for L2/3, L4, and L6 PC. The precision and recall for these cells are summarized in the text, combined for the 3 neurons. It would be important to show all 3 quantities individually for each neuron, just like they are shown here for the L5 PC.</p></disp-quote><p>We have made these changes as recommended.</p><disp-quote content-type="editor-comment"><p>– Figure 6 – As far as I can tell, these are not connected networks. Simulating 5,000 disconnected cells is very different from 5,000 highly interconnected cells, and the speed-ups can be drastically different. This is OK for the purposes of this manuscript, but the description should be clear about what's being done. The text mentions &quot;network&quot; everywhere in this section, including its title. The authors should change it and make it clear that simulations involve 50 or 5,000 disconnected cells. Or, if I got this wrong, and these are indeed simulations of connected networks with 50 or 5,000 cells, then please provide the description of the network connectivity, synaptic weights, etc. (In Methods, I only see the description of a 150-neuron network for Figures 7 and 8.)</p></disp-quote><p>The cells in Figure 6. are indeed disconnected cells and we have made the recommended changes in the manuscript. We agree with the reviewer in that high levels of interconnection can cause drastically lower simulation runtimes, but importantly, only in case of NEURON simulations. Contrastingly, the speed of our ANN approach is not dependent of the level of connectivity, as connections are applied through a simple matrix transformation, which is carried out without regards to the number of connections. Consequently, the overwhelming majority of the simulation is spent on ANN evaluation, rather than implementing synaptic communication. Due to these disparate features of NEURON and ANN simulations we decided that it would be unfair to burden NEURON simulations by enforcing further runtime impeding synaptic connectivity, however these important details are now touched upon in the manuscript (page 13) to better inform the reader.</p><disp-quote content-type="editor-comment"><p>– Figure 6 – also, the authors may want to say something here about the comparison of an ANN on GPU vs. NEURON on 1 CPU is not perfect. Ideally, one would run the ANN and NEURON simulations on the same parallel hardware and compare the performance as a function of the number of parallel cores used. I understand that is hard to achieve, so it is fair that the authors do not show such a comparison. However, it is instructive to consider the following thought experiment. Even if one ran the NEURON simulation of 5,000 cells on 5,000 CPUs, the performance would likely be about the same as that for one cell on one CPU. But even then, the time of the NEURON simulation would be ~185 s (for the L5 PC), whereas the time of the CNN simulation on a SINGLE GPU is ~12 s. So, the CNN is over 10 times more efficient on a single GPU than one expects NEURON to be on 5,000 CPUs.</p></disp-quote><p>We thank the reviewers for pointing this out. We made additional clarifications about this issue on the main text (page 13). We agree that one of the main strengths of ANNs is their suitability for GPUs, however, we intentionally leveled the playing field by showing ANN simulations on the same single CPU as well. On Figure 6 panel b and c, the light magenta bars represent ANN simulations on the same hardware as the NEURON simulations were run on. It is important to note that the single point neuron simulation was the only case where NEURON was faster than ANNs on CPU. The advantage of our approach is twofold; first, ANNs are faster on the same hardware, and second, ANNs can take advantage of GPUs.</p><disp-quote content-type="editor-comment"><p>– Simulations of the Rett syndrome model – it might be useful to give a little more detail about the network used for these simulations in the Results (otherwise one has to check Methods for all the details). The important piece to mention is that the network does not have any inhibitory cells, and instead, inhibition is provided as external inputs together with excitation. In other words, it is a feedforward inhibition model (if I understood it correctly).</p></disp-quote><p>We thank the reviewers for this point, we made the proposed clarification in the main text (page 15).</p><disp-quote content-type="editor-comment"><p>– Figure 7c, parameter mapping – I assume the bar for NEURON is interpolation?</p></disp-quote><p>We thank the reviewers for pointing this out, we colored the bar plot similarly as in Figure 6 and added and additional label.</p><disp-quote content-type="editor-comment"><p>– Page 22, &quot;which means that a complete cortical area can be simulated using only 17 ANNs&quot; – I am not sure this is correct. The Billeh et al., model used about 100 distinct neuronal models belonging to 17 cell types. So, simulation of this model would require about 100 ANNs, rather than 17. Of course, this is still a huge improvement relative to the hundreds of thousands of neurons in the original NEURON model.</p></disp-quote><p>We agree, and we made a note of this in the main text.</p><disp-quote content-type="editor-comment"><p>– Discussion – the authors almost do not mention the closely related work by Beniaguev et al., (Neuron, 2022), though they do cite that paper. I believe the work by Olah et al., is sufficiently different and novel, and it offers many interesting new insights as well as opportunities for computational neuroscientists who might want to use this method and code. I would suggest that the authors add a paragraph to the Discussion and describe how their work differs from Beniaguev et al., and what their unique contributions are.</p></disp-quote><p>We thank the reviewer for pointing out the usefulness of a comparison. The work by Beniaguev et al., nicely puts the idea that single neurons can be represented by ANNs (proposed by Poirazi et al., Neuron 2003) into practice. Their ANNs are able to accurately represent membrane potential dynamics of both single compartmental and more complex, fully reconstructed cells. However, our aim was to find an ANN architecture which fulfills three main criteria. First, sequential output with small temporal increments is warranted for the dynamic implementation of synaptic connectivity. The model presented by Beniaguev et al., outputs longer predictions at once, thereby precluding the possibility of synaptic timing and weight updates. Second, a critical step in ANN performance evaluation is to demonstrate generalization, which we found to be lacking in ANNs composed of stacked convolutional layers (used by Beniaguev et al.,). These tests were carried out not only on randomly timed synaptic inputs but randomized synaptic weights as well. Third, action potential generation is not a static process in realistic neurons, as the dynamic interplay between conductance activation and inactivation can drastically alter action potential threshold. Therefore, we explored several different architectures to finally settle on CCN-LSTMs, which can learn firing, therefore making external thresholding obsolete. Due to these differences, our work does not rely on the findings presented at Benigauev et al., instead proposes an ANN architecture that can be considered a viable substitute of traditional modeling systems. We have now included a paragraph in the discussion with these details discussed in context (page 18).</p><disp-quote content-type="editor-comment"><p>– Data and software availability – the GitHub link doesn't work. I assume the authors plan to make it public upon paper publication. But it would be nice to provide the code to the reviewers, to get some idea about the completeness of the code, since it represents one of the main results of this paper. It is also important to mention that the code shared with the community should include the functions and procedures for training the ANNs. That is one of the most valuable contributions, which will be of great interest to many neuroscientists.</p></disp-quote><p>We agree and have now made the code publicly available, with documentation. The code is currently available on a public Github repository (https://github.com/ViktorJOlah/Neuro_ANN), and we have uploaded it to Dryad (https://doi.org/10.5061/dryad.0cfxpnw60 , temporary link: https://datadryad.org/stash/share/keJYkykT0Vv0h6YdA1wFNMyfEBk8TUYgRk79szUJKHM).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I think this study is very nice. As noted above in the Public Review, however, I think the manuscript would be greatly improved and its impact increased by (i) showing an accuracy comparison of the results obtained with NEURON and those obtained with the ANN network for the Rett syndrome circuit model, (ii) adding performance measures for the GeNN simulator, or some other simulator that is designed to run on GPUs, at least for the point neuron model.</p></disp-quote><p>We thank the reviewer for making this point. Our aim with the Rett syndrome circuit model was to showcase the aptness of the ANN network for parameter space mapping. To compare the speed of the simulations, we created an ANN network of L5 PCs, as described in the manuscript and compared the runtime to 150 unconnected L5 PCs in NEURON. We agree that this clarification is missing from the result section and made the necessary correction (page 15).</p><p>We simulated disconnected cells for two reasons. First, while implementing synaptic connectivity in the ANN network does not constrain simulation runtime due to simple matrix transformations, connections in NEURON simulations severely impede simulation runtimes. As our ANN simulations involve dynamic alterations in the rate of connectivity, NEURON simulation runtimes can vary. Therefore, we compared the ANN simulations to the fastest possible NEURON simulations, involving zero percent connectivity. Second, the proposed simulations have been largely inspired by a previous publication (Hay, Segev 2005 Cerebral Cortex), where the authors scrutinized response properties upon altered network size and connection probabilities, which results were taken into consideration during network construction.</p><p>We agree with the reviewer in that there are simulation environments, which are specifically designed for neuronal circuit simulations, yet discussion of these were missing from our manuscript. We have now addressed the advantages of these simulators in the manuscript with the necessary citations (page 2). As stated in the manuscript, ANN simulations on a single, one compartment neuron were significantly slower compared to NEURON simulations, and we further emphasized that for simplified neurons, NEURON is one of the slower simulators. We are aware that for the GeNN simulator and other environments, dealing with ODEs are much faster, and now discuss these points in the manuscript, with the appropriate citations (page 18). We again thank the reviewer for this question.</p><disp-quote content-type="editor-comment"><p>The availability of the source code is very welcome. However, it is not well documented. The impact of this study would be increased by providing at least a README explaining the structure of the repository, and ideally by providing instructions for reproducing at least some of your results (e.g. generating the training data, training the ANNs, using the trained networks to generate predictions, etc.)</p></disp-quote><p>We agree and have now made the code publicly available, with documentation. The code is currently available on a public Github repository (https://github.com/ViktorJOlah/Neuro_ANN), and we have uploaded it to Dryad (https://doi.org/10.5061/dryad.0cfxpnw60 , temporary link: https://datadryad.org/stash/share/keJYkykT0Vv0h6YdA1wFNMyfEBk8TUYgRk79szUJKHM).</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>1. One of the key earlier reviewer points has to do with scaling with connected network size, especially with very large numbers of synapses. While the authors have responded, I was not able to understand this, and hence ask for a more complete explanation in the text so that it becomes more accessible to the readers.</p><p>The authors say:</p><p>&quot;We thank the reviewers for pointing this out. This issue is now added to the discussion. Briefly, synaptic connectivity has no impact on simulation runtimes as the matrix transformations necessary for implementing connections take place regardless of whether two given cells are connected or not. On the other hand, inclusion of additional cell types linearly increases simulation times (assuming comparable cell numbers per cell type), as every cell type warrants the execution of additional artificial neural nets every time step.&quot;</p><p>Can the authors explain this matrix transformation step and its mapping to synaptic connectivity? I did not find an explanation in either the text or the responses to the reviewers. Possibly it may help if I were to reiterate the synaptic connectivity bottleneck in conventional simulations.</p></disp-quote><p>We thank the editors for pointing this out. We have made clarifications in the main text (page 26) regarding the implementation of synaptic connectivity. Briefly, during the initialization of the circuit, a connectivity matrix is defined, where columns correspond to presynaptic cells, and rows correspond to postsynaptic cells. If two cells are connected, the value in the matrix is 1, otherwise it is zero. During circuit simulations, after membrane predictions took place at every time step, the program creates the next input for the next prediction cycle. This involves checking every presynaptic cell for suprathreshold activity and setting the value of the appropriate column in the input matrix to (synaptic conductance) * (synaptic connection, zero or one). Therefore, whether any two given cells are connected or not, the program checks for connections and sets the synaptic input accordingly. This means that simulation runtime doesn’t depend on synaptic connectivity. Although this is not the most efficient method for implementing connections, our aim was to increase the transparency of our code and in addition, we found that this portion of our program account for only a negligible portion of simulation runtimes.</p><disp-quote content-type="editor-comment"><p>2. Each individual synaptic projection introduces a distinct delay in how long it takes for the source action potential to reach the postsynaptic synapse. This delay can be up to 10ms or sometimes longer depending on axon fiber type and length. 2. Each postsynaptic synapse is usually implemented as a conductance change obeying a single or dual α function of time. such as gSyn = gPeak * 1/tp * exp(1 -t/tp) where t is time since spike arrived at synapse and tp is time of peak of synaptic conductance.</p><p>The common observation in large spiking network models is that the combination of these calculations can lead to quite large demands, including in managing the event queues to implement the synaptic delays, since the delays may be long enough to permit multiple action potentials. The synaptic dynamics of the α functions also introduce a computational cost. Since the number of synaptic connections is very large, in some large simulations the computation time is dominated by synaptic transmission.</p><p>It would be helpful if the authors can respond by addressing a few specific points, and include the information in the text.</p><p>a. Confirm and elaborate on how their method indeed accomplishes the same computations as this, both the distinct synaptic delay for each synapse, and the equivalent of α function synapses.</p><p>b. Explain how their matrix transform addresses the two computational bottlenecks that occur with the conventional simulation approach,</p><p>c. The authors on the one hand state (line 594) &quot;the number of contact sites directly correlates with simulation runtimes and memory consumption.&quot;, and on the other they say that synaptic connectivity has no impact on simulation runtimes. Please clarify what is different here.</p></disp-quote><p>We agree with the editors in that in large scale circuit simulations, handling the α function and synaptic delays comes with an immense computational burden. We have elaborated on these points in the main text (page 26). To address the specific points:</p><p>a. We did not implement synaptic delays in our code. However, as synaptic delay is assumed to be constant, at least for the purposes of large-scale simulations, implementation of this feature is straight forward. In our case, synaptic delays must be determined during circuit initialization, and appropriate buffer matrices have to be created. During the exertion of synaptic inputs upon presynaptic discharge, the input would not be immediately written to the input matrix of the subsequent prediction round, instead it would be written to the buffer matrix, which is also advanced by one time step (advancement of the matrix refers to deletion of the first row, shifting every row up, and defining the last row, based on whether new inputs arrived or not, page 26).</p><p>Our ANN approach does not use α functions. We have implemented an ANN architecture that is used for various time series forecasting problems, such as stock market predictions and weather forecasts. The CNN-LSTM architecture consists of two main functional components; convolutional layers (CNN) and long short-term memory layers (LSTM). The CNN part is responsible for pattern recognition, and the LSTM layers handle temporal aspects. If the ANN is supplemented with an appropriate amount of training data, the CNN layers can learn the distinct shapes of postsynaptic responses belonging to different synaptic contacts. Although the α functions haven’t been explicitly defined in our architecture, the ANN could accurately predict (and not calculate) the postsynaptic responses, as demonstrated by the high amount of explained variance and correlation between ground truth data and prediction in our manuscript (Figure 1,2,3,4,5,6).</p><p>b. In particular, matrix transformation only addresses issues arising from abundant connectivity. As we explained for (a.), synaptic delays were not implemented in our code, but can be circumvented by buffer matrices, and α functions are not computed explicitly, rather predicted based on phenomenological observations (page 26).</p><p>c. We thank the editors for pointing out the necessity for clarification, which we made in the main text (page 9). Throughout the manuscript, we used the number of contact sites and synaptic connectivity as not interchangeable phrases, as the two refer to differently implemented quantities. First, synaptic contact sites refer to distinguished points in the dendritic tree which onto which synaptic connections arrived in NEURON simulations. These contact sites further correspond to columns in the input matrix of ANNs, where the ANN receives an n*m matrix, and every column (except for the first one contains membrane voltages) corresponds to one of these distinguished points. These columns can be written by several presynaptic cells if they establish connection on the same synaptic contact site. Therefore, the number of synaptic contact sites is a fixed constant at ANN creation, while synaptic connectivity can be dynamically altered between simulations. As previously described, synaptic connectivity does not alter simulation runtimes, while increasing the size of the input matrix places an increased computational burden on the system.</p><disp-quote content-type="editor-comment"><p>3. Could the authors move some further details of the ANN training into the paper? For example, I did not see the time taken to train the ANN (~24 hours from the response to reviewers) stated in the paper. It would be very helpful for people trying to implement such networks to know what to expect in terms of training resources and time, not to mention the learning curve for the researchers themselves to figure out how to do the training.</p><p>A related point: the data availability statement explains how to access the generated models. I did not see a clear mention of the code and resources used to build the ANNs from the training set.</p><p>I understand we are still in the early days of the use of this method. It took several years after the development of the underlying matrix calculation code for neuronal calculations before there were a couple of standard simulators that helped with many things from standard libraries to graphical interfaces. Nevertheless, it would be very helpful if the authors could provide a more complete indication in the paper of what it would take for users to do such model building for themselves.</p></disp-quote><p>We thank the for making this point. We now included additional details in the manuscript regarding ANN training (page 22).</p><p>The code availability statement has been updated to reflect that not only the code for ANN generation, but the NEURON code used for dataset building is available as well, in the same folder.</p></body></sub-article></article>