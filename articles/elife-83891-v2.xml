<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83891</article-id><article-id pub-id-type="doi">10.7554/eLife.83891</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>The functional form of value normalization in human reinforcement learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-281803"><name><surname>Bavard</surname><given-names>Sophie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9283-2976</contrib-id><email>sophie.bavard@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-151403"><name><surname>Palminteri</surname><given-names>Stefano</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5768-6646</contrib-id><email>stefano.palminteri@ens.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>Laboratoire de Neurosciences Cognitives et Computationnelles, Institut National de la Santé et Recherche Médicale</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>Département d’Etudes Cognitives, Ecole Normale Supérieure, PSL University</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00g30e956</institution-id><institution>Department of Psychology, University of Hamburg</institution></institution-wrap><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse Intramural Research Program</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>07</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e83891</elocation-id><history><date date-type="received" iso-8601-date="2022-10-02"><day>02</day><month>10</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-07-09"><day>09</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-07-16"><day>16</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.14.500032"/></event></pub-history><permissions><copyright-statement>© 2023, Bavard and Palminteri</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Bavard and Palminteri</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83891-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83891-figures-v2.pdf"/><abstract><p>Reinforcement learning research in humans and other species indicates that rewards are represented in a context-dependent manner. More specifically, reward representations seem to be normalized as a function of the value of the alternative options. The dominant view postulates that value context-dependence is achieved via a divisive normalization rule, inspired by perceptual decision-making research. However, behavioral and neural evidence points to another plausible mechanism: range normalization. Critically, previous experimental designs were ill-suited to disentangle the divisive and the range normalization accounts, which generate similar behavioral predictions in many circumstances. To address this question, we designed a new learning task where we manipulated, across learning contexts, the number of options and the value ranges. Behavioral and computational analyses falsify the divisive normalization account and rather provide support for the range normalization rule. Together, these results shed new light on the computational mechanisms underlying context-dependence in learning and decision-making.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>decision-making</kwd><kwd>computational model</kwd><kwd>cognitive science</kwd><kwd>efficient coding</kwd><kwd>relative value</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>101043804</award-id><principal-award-recipient><name><surname>Palminteri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-21-CE23-0002-02</award-id><principal-award-recipient><name><surname>Palminteri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-21-CE37-0008-01</award-id><principal-award-recipient><name><surname>Palminteri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-21-CE28-0024-01</award-id><principal-award-recipient><name><surname>Palminteri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Challenging a popular theory in neuroeconomics, a computational cognitive study provides evidence against divisive normalization, a supposedly canonical neural computation, in favor of an alternative account, range normalization, in the context of value learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The process of attributing economic values to behavioral options is highly context-dependent: the representation of an option’s utility does not solely depend on its objective value, but is strongly influenced by its surrounding (i.e., other options simultaneously or recently presented). This is true in an extremely wide range of experimental paradigms, ranging from decision among lotteries to reinforcement learning problems (<xref ref-type="bibr" rid="bib29">Kahneman and Tversky, 1984</xref>; <xref ref-type="bibr" rid="bib27">Huber et al., 1982</xref>; <xref ref-type="bibr" rid="bib30">Klein et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>). This is also true for a wide variety of species, including mammals (<xref ref-type="bibr" rid="bib72">Yamada et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Conen and Padoa-Schioppa, 2019</xref>), birds (<xref ref-type="bibr" rid="bib56">Pompilio and Kacelnik, 2010</xref>) and insects (<xref ref-type="bibr" rid="bib55">Pompilio et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Solvi et al., 2022</xref>). The pervasiveness of this effect across tasks and species suggests that context-dependence may reflect the way neuron-based decision systems address a fundamental computational trade-off between behavioral performance and neural constraints (<xref ref-type="bibr" rid="bib16">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Padoa-Schioppa, 2009</xref>; <xref ref-type="bibr" rid="bib31">Kobayashi et al., 2010</xref>; <xref ref-type="bibr" rid="bib38">Louie and Glimcher, 2012</xref>).</p><p>Indeed, it has been showed that context-dependence often takes the form of a normalization process where option values are rescaled as a function of the other available options, which has the beneficial consequence of adapting the response to the distribution of the outcomes (<xref ref-type="bibr" rid="bib38">Louie and Glimcher, 2012</xref>). The idea that neural codes and internal representations are structured to carry as much as information per action is the cornerstone of the efficient coding hypothesis, demonstrated both at the behavioral and neural levels, in perceptual decision-making (<xref ref-type="bibr" rid="bib57">Reynolds and Heeger, 2009</xref>).</p><p>Probably due to its popularity in perception neuroscience, the dominant view regarding the computational implementation of value normalization in economic decisions postulates that it follows a divisive rule, according to which the subjective value of an option is rescaled as a function of the sum of the value of all available options (<xref ref-type="bibr" rid="bib37">Louie et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Louie et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Webb et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Pirrone and Tsetsos, 2022</xref>). In addition, to be validated in the perceptual domain, the divisive normalization rule also presents the appeal of being reminiscent of Herrnstein’s matching law for behavioral allocation (<xref ref-type="bibr" rid="bib25">Herrnstein, 1961</xref>).</p><p>Even though, to date, most of the empirical studies proposing divisive normalization as a valid model of economic value encoding proposed that option values are vehiculated by explicit features of the stimulus (such as food snacks or lotteries: so-called described options; <xref ref-type="bibr" rid="bib26">Hertwig and Erev, 2009</xref>; <xref ref-type="bibr" rid="bib39">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Garcia et al., 2021</xref>; <xref ref-type="bibr" rid="bib14">Daviet and Webb, 2023</xref>), few recent studies have extended the framework to account for subjective valuation in the reinforcement learning (or experience-based) context (<xref ref-type="bibr" rid="bib28">Juechems et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Louie, 2022</xref>). Adjusting the divisive normalization model to a reinforcement learning scenario is easily achieved by assuming that the normalization step occurs at the outcome stage, that is, when the participant is presented with the obtained (and forgone) outcomes.</p><p>While predominant in the current neuroeconomic debate about value encoding and adaptive coding (<xref ref-type="bibr" rid="bib8">Bucher and Brandenburger, 2022</xref>), the divisive normalization account of value normalization is not consensual (<xref ref-type="bibr" rid="bib45">Padoa-Schioppa, 2009</xref>; <xref ref-type="bibr" rid="bib31">Kobayashi et al., 2010</xref>; <xref ref-type="bibr" rid="bib46">Padoa-Schioppa and Rustichini, 2014</xref>; <xref ref-type="bibr" rid="bib9">Burke et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Gluth et al., 2020</xref>). Indeed, range normalization represents a possible alternative account of value normalization and is made plausible by both behavioral and neural observations (<xref ref-type="bibr" rid="bib50">Parducci, 1963</xref>; <xref ref-type="bibr" rid="bib60">Rustichini et al., 2017</xref>). According to the range normalization rule, option values are rescaled as a function of the maximum and the minimum values presented in a context, irrespective of the number of options or outcomes (or set size; <xref ref-type="bibr" rid="bib12">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib50">Parducci, 1963</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>). Answering this question bears important consequences for neuroscience because understanding the scaling between objective and subjective outcomes is paramount to investigate the neural codes of economic values and understand the neural mechanisms of decision-making (<xref ref-type="bibr" rid="bib13">Cox and Kable, 2014</xref>; <xref ref-type="bibr" rid="bib34">Lebreton et al., 2019</xref>). Yet, the experimental paradigms used so far in reinforcement learning research were ill-suited to distinguish between two accounts of value normalization in the context of reinforcement learning (<xref ref-type="bibr" rid="bib30">Klein et al., 2017</xref>). To address this issue, we designed a new reinforcement learning protocol where, by simultaneously manipulating outcome ranges and choice set sizes, we made the divisive and the range normalization predictions qualitatively diverge in many respects (<xref ref-type="bibr" rid="bib58">Roberts and Pashler, 2000</xref>; <xref ref-type="bibr" rid="bib48">Palminteri et al., 2017</xref>.) We opted for a reinforcement learning paradigm because it has a greater potential for translational and cross-species research (<xref ref-type="bibr" rid="bib18">Garcia et al., 2021</xref>). In a total of eight experiments (N = 500 in total), we deployed several variations of this new behavioral protocol where we controlled for several factors. The behavioral, model fitting and simulation results convergently rejected divisive normalization as a satisfactory explanation of the results in favor of the range normalization account. Results also suggested that the range normalization account should be further improved by a nonlinear weighting process. To check the robustness of our results across different elicitation methods and representational systems, we also assessed option values using explicit ratings. Values inferred from explicit, declarative, ratings were remarkably consistent with those inferred from more traditional, choice-based, methods.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Computational hypotheses and ex ante model simulations</title><p>The goal of this study was to characterize the functional form of outcome (or reward) normalization in human reinforcement learning. More specifically, we aimed at arbitrating between two equally plausible hypotheses: range normalization and divisive normalization. Both hypotheses assume that after reception of a given objective reward <inline-formula><mml:math id="inf1"><mml:mi>R</mml:mi></mml:math></inline-formula>, the learner forms an internal, subjective, representation of it, <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></inline-formula> which is influenced by other contextually relevant rewards. Crucially, the two models differ in how <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is calculated. According to the range normalization hypothesis, the subjective normalized reward <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as the position of the objective reward <inline-formula><mml:math id="inf5"><mml:mi>R</mml:mi></mml:math></inline-formula> within its contextual range:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the endpoints of the contextually relevant distribution and together form the range (<inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). On the other side, the divisive normalization hypothesis, in its simplest form, postulates that the subjective normalized reward <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is calculated by dividing the objective reward by the sum of all the other contextually rewards (<xref ref-type="bibr" rid="bib41">Louie, 2022</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of contextually relevant stimuli. These hypotheses concerning value normalization are then easily plugged into the reinforcement learning framework, simply by assuming that the value of an option is updated by minimizing a prediction error, calculated on the basis of the subjective reward. Although these normalization functions are mathematically distinct, they make identical (or very similar) behavioral predictions in many of the experimental protocols designed to investigate context-dependent reinforcement learning so far (<xref ref-type="bibr" rid="bib30">Klein et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Palminteri et al., 2015</xref>). It should be noted here that, although divisive normalization has been more frequently applied to the prospective evaluation of described outcomes (e.g., lotteries; snack-food items), rather than retrospective evaluation of obtained outcomes (e.g., bandits), it has both historical (<xref ref-type="bibr" rid="bib25">Herrnstein, 1961</xref>) and recent (<xref ref-type="bibr" rid="bib41">Louie, 2022</xref>) antecedents in the context of reinforcement learning. For the present study, we designed reinforcement learning tasks designed to adjudicate these computational hypotheses. The key idea behind our behavioral protocol is to orthogonally manipulate, across different learning contexts, the amplitude of the range of the possible outcomes and the number of options (often referred to as choice size set; <xref ref-type="fig" rid="fig1">Figure 1A</xref>; see also <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref> for an alternative version). The first factor (the amplitude of the range of the possible outcomes) is key to differentiate an unbiased model (where <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi></mml:math></inline-formula>) from both our normalization models. The second factor (number of options) is key to differentiate the range normalization from divisive normalization. The reason for this can easily be inferred from <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> because adding more outcomes has a significant impact on the subjective reward <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> only following the divisive normalization rule (<xref ref-type="bibr" rid="bib14">Daviet and Webb, 2023</xref>; as clearly put by the main advocate of the divisive normalization rule for value-based decision-making: “[…] a system would be highly sensitive to the number of options under consideration. As the number of elements in the denominator grows, so does the aggregate value of the denominator, shifting the overall firing rates lower and lower” (<xref ref-type="bibr" rid="bib21">Glimcher, 2022</xref>, page 14). To quantitatively substantiate these predictions, we ran model simulations using three models. We compared a standard model with unbiased subjective values (UNBIASED), and two normalization models using either the divisive or the range normalization rules (referred to as DIVISIVE and RANGE, respectively). First, we simulated a learning phase, where each learning context in our factorial design was presented 45 times. After each trial, the simulated agent was informed about the outcomes that were drawn from normal distributions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To avoid sampling issues and ambiguity concerning the definition of the relevant normalization variables, the simulated agents were provided information about the outcomes of all options (‘complete’ feedback; <xref ref-type="bibr" rid="bib26">Hertwig and Erev, 2009</xref>; <xref ref-type="bibr" rid="bib35">Li and Daw, 2011</xref>). After the <italic>learning</italic> phase, the simulated agents went through a <italic>transfer</italic> phase, where they made decisions among all possible binary combinations of the options (without additional feedback being provided). Similarly constructed experiments, coupling a learning to a transfer phase, have been proven key to demonstrate contextual effects in previous studies (<xref ref-type="bibr" rid="bib30">Klein et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib56">Pompilio and Kacelnik, 2010</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Palminteri et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Hayes and Wedell, 2022</xref>; <xref ref-type="bibr" rid="bib28">Juechems et al., 2022</xref>). When analyzing model simulations, we focused on choice patterns in the transfer phase (of note, accuracy during the learning phase is weakly diagnostic because all models predict above chance accuracy and, to some extent, a choice size set effect, whose level depends on the choice stochasticity parameter of the softmax decision rule). <xref ref-type="fig" rid="fig1">Figure 1C</xref> plots the average simulated choice rate in the transfer phase. For a given option, the transfer phase choice rate was calculated by dividing the number of times an option is chosen by the number of times the option is presented. In the transfer phase, the 10 cues from the learning phase were presented in all possible binary combinations (45, not including pairs formed by the same cue). Each pair of cues was presented four times, leading to a total of 180 trials. Since a given comparison counts for the calculation of the transfer phase choice rate of both involved options, this implies that this variable will not sum to one. Nonetheless, the relative ranking between transfer choice rate can be taken as a behavioral proxy of their subjective values.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and model predictions of Experiment 1.</title><p>(<bold>A</bold>) Choice contexts in the learning phase. Participants were presented with four choice contexts varying in the amplitude of the outcomes’ range (narrow or wide) and the number of options (binary or trinary decisions). (<bold>B</bold>) Means of each reward distribution. After each decision, the outcome of each option was displayed on the screen. Each outcome was drawn from a normal distribution with variance <inline-formula><mml:math id="inf13"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula>. NB: narrow binary, NT: narrow trinary, WB: wide binary, WT: wide trinary. (<bold>C</bold>) Model predictions of the transfer phase choice rates for the UNBIASED (left), DIVISIVE (middle), and RANGE (right) models. Note that choice rate in the transfer phase is calculated across all possible binary combinations involving a given option. While score is proportional to the agent’s preference for a given option, it does not sum to one because any given choice counts for the final score of two options. Dashed lines represent the key prediction for each model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig1-v2.tif"/></fig><p>Crucially, even if the transfer phase involves only binary choices, it can still tease apart the normalization rules affecting outcome valuation during the learning phase. This is because transfer choices are made based on the memory of values acquired during the learning phase, where we purposely manipulated the number of options and their ranges of values, in order to create learning contexts that allow to confidently discriminate between the two normalization accounts, in the reinforcement learning context.</p><p>Unsurprisingly, within each learning context, in all models the choice rates are higher for high-value options compared to lower value options. However, model simulations show that the models produce choice patterns that differ in many key aspects. Let’s start considering the UNBIASED model as a benchmark (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, left). Since it encodes outcomes in an unbiased manner, it predicts higher choice rates for the high-value option in the ‘wide’ contexts (WB<sub>86</sub> and WT<sub>86</sub>) compared to high-value options in the ‘narrow’ contexts (NB<sub>50</sub> and NT<sub>50</sub>). On the other side, the UNBIASED model predicts that choice rate in the transfer phase is not affected by whether or not the option belonged to a binary or a trinary learning context. Moving to the DIVISIVE model, we note that the difference between the choice rates of high-value options of the ‘wide’ contexts (WB<sub>86</sub> and WT<sub>86</sub>) compared those of the ‘narrow’ contexts (NB<sub>50</sub> and NT<sub>50</sub>) is now much smaller due to the normalization process (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, middle). However, the DIVISIVE model also predicts that the choice rate is strongly affected by whether or not the option belonged to a binary or a trinary learning context. For instance, WB<sub>86</sub> and NB<sub>50</sub> present a much higher choice rate compared to WT<sub>86</sub> and NT<sub>50</sub>, respectively, despite their objective expected value being the same. This is an easily identifiable and direct consequence of the denominator of the divisive formulation rule increasing as a function of the number of options (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Concerning the RANGE model (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, right), it predicts choice rates being similar across all high-value options, regardless of their objective values (because of the normalization) and whether or not the option belonged to a trinary or binary context (because of the range normalization rule; <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Finally, the choice rates of the low-value (14) options also discriminate the DIVISIVE model, where it is strongly modulated by the task factors, from the other two models, where all low-value options present the same choice rate. To conclude, model simulations confirm that our design, involving a factorial learning phase and a transfer phase, is well suited to disentangle our three a priori models because they predict qualitatively differentiable patterns of choices (see also <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref> for similar conclusions based on an alternative task design; <xref ref-type="bibr" rid="bib48">Palminteri et al., 2017</xref>; <xref ref-type="bibr" rid="bib64">Teodorescu and Usher, 2013</xref>).</p></sec><sec id="s2-2"><title>Behavioral results</title><p>The above-described behavioral protocol was administered to N = 50 participants recruited online, who played for real monetary incentives as previously described (<xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>). We first tested whether the correct choice rate (i.e., the probability of choosing the option with the highest expected value) was overall above chance level during the learning phase to ensure that the participants engaged in the task. Indeed, correct response rate was significantly higher than chance level (0.5 and 0.33 in the binary and trinary learning contexts, respectively) in all conditions (least significant comparison: <italic>t</italic>(49) = 18.93, p&lt;0.0001, <italic>d</italic> = 2.68; on average: t(49) = 24.01, p&lt;0.0001, d = 3.96; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). We further checked whether the task factors affected performance in the learning phase and found a significant effect of the decision problem (the correct choice rate being higher in the binary compared to the trinary contexts: <italic>F</italic>(1,49) = 9.26, p=0.0038, η² = 0.16), but no effect of range amplitude (wide versus narrow; <italic>F</italic>(1,49) = 0.52, p=0.48, η² = 0.01) nor interaction (<italic>F</italic>(1,49) = 2.23, p=0.14, η² = 0.04).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral results of Experiment 1.</title><p>Top: successive screens of a typical trial for the three versions of the main experiment: without forced trials (<bold>A</bold>), with forced trials and complete feedback information (<bold>B</bold>) and with forced trials and partial feedback information (<bold>C</bold>). Bottom: correct choice rate in the learning phase as a function of the choice context (left panels), and choice rate per option in the transfer phase (right panels) for the three versions of the main experiment: without forced trials (<bold>A</bold>), with forced trials and complete feedback information (<bold>B</bold>) and with forced trials and partial feedback information (<bold>C</bold>). In all panels, points indicate individual average, shaded areas indicate probability density function, 95% confidence interval, and SEM (n=50). NB: narrow binary, NT: narrow trinary, WB: wide binary, WT: wide trinary.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Behavioral results of the pilot Experiment similar to Experiments 1 and 2.</title><p>Correct choice rate in the learning phase as a function of the choice context (left panels), and choice rate per option in the transfer phase (right panels) for pilot Experiment 1 (<bold>A</bold>) and pilot Experiment 2 (<bold>B</bold>). In all panels, points indicate individual average, shaded areas indicate probability density function, 95% confidence interval, and SEM. NB: narrow binary, NT: narrow trinary, WB: wide binary, WT: wide trinary. To ascertain that our task design would be feasible and that participants would be able to learn the values of 10 options by trial-and-error, a pilot online-based experiment was originally performed. We recruited 40 participants (23 females, 16 males, 1 N/A, aged 30.35 ± 9.73 y) via Prolific (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.co">https://www.prolific.co</ext-link>). In the pilot experiments, the outcome variance was set to <inline-formula><mml:math id="inf14"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, that is, the rewards were displayed without any noise (in the main tasks, the variance was set to <inline-formula><mml:math id="inf15"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula>). In order to characterize learning behavior of participants, we analyzed the correct response rate in the learning and the transfer phases, that is, choices directed toward the most favorable option at each trial. To assess successful learning, we first tested participants’ correct response rate against chance level. We found it to be above chance level in both the learning phase (0.5 and 0.33 in the binary and trinary learning contexts, respectively; pilot experiment 1: <italic>t</italic>(19) = 11.83, p&lt;0.0001, <italic>d</italic> = 2.65; pilot experiment 2: <italic>t</italic>(19) = 7.39, p&lt;0.0001, <italic>d</italic> = 1.65) and the transfer phase (0.5; pilot experiment 1: <italic>t</italic>(19) = 5.87, p&lt;0.0001, <italic>d</italic> = 1.31; pilot experiment 2: <italic>t</italic>(19) = 3.01, p=0.0072, <italic>d</italic> = 0.67).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Experimental design, model predictions, and behavioral results concerning Experiment 2.</title><p>(<bold>A</bold>) Choice contexts in the learning phase. Participants were presented with four choice contexts varying in the amplitude of the outcomes’ range (narrow or wide) and the number of options (binary or trinary decisions). (<bold>B</bold>) Means of each reward distribution. After each decision, the outcome of each option was displayed on the screen. Each outcome was drawn from a normal distribution with variance <inline-formula><mml:math id="inf16"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula>. NB: narrow binary, NT: narrow trinary, WB: wide binary, WT: wide trinary. (<bold>C</bold>) Model predictions of the transfer phase choice rates for the UNBIASED (left), DIVISIVE (middle), and RANGE (right) models. Dashed lines represent the key prediction for each model. (<bold>D–F</bold>) Correct choice rate in the learning phase as a function of the choice context (left panels), and choice rate per option in the transfer phase (right panels) for the three versions of the main experiment: without forced trials (<bold>D</bold>), with forced trials and complete feedback information (<bold>E</bold>) and with forced trials and partial feedback information (<bold>F</bold>). In all panels, points indicate individual average, shaded areas indicate probability density function, 95% confidence interval, and SEM. NB: narrow binary, NT: narrow trinary, WB: wide binary, WT: wide trinary. In addition to Experiment 1, whose results are presented in the main text, we recruited 150 participants to perform a modified version of Experiment 1. The only difference between Experiment 1 and Experiment 2 is the value of the options in the narrow contexts: in Experiment 1, they went from 14 to 50; in Experiment 2, they went from 50 to 86. Similar to Experiment 1, participants were given a bonus depending on the number of points won in the experiment (average money won in pounds: 6.38 ± 0.58, average performance against chance during the learning phase and transfer phase: M = 0.78 ± 0.099, <italic>t</italic>(149) = 34.65, p&lt;0.0001, <italic>d</italic> = 2.83). No data had to be excluded for technical issues. In the learning phase, the correct response rate was significantly higher than chance level (0.5 and 0.33 in the binary and trinary learning contexts, respectively) in all conditions (least significant: <italic>t</italic>(49) = 13.71, p&lt;0.0001, <italic>d</italic> = 1.94; on average: <italic>t</italic>(49) = 15.75, p&lt;0.0001, <italic>d</italic> = 2.23). We further checked whether the task factors affected performance in the learning phase and found a small significant effect of the decision problem (the correct choice rate being higher in the binary compared to the trinary contexts: <italic>F</italic>(1,49) = 4.11, p=0.048, η<sup>2</sup> = 0.08), but no effect of range amplitude (wide versus narrow; <italic>F</italic>(1,49) = 0.027, p=0.87, η<sup>2</sup> = 0.00) or interaction (<italic>F</italic>(1,49) = 0.92, p=0.34, η<sup>2</sup> = 0.02). In the transfer phase, the correct choice rate in the transfer was significantly higher than chance (<italic>t</italic>(49) = 9.56, p&lt;0.0001, <italic>d</italic> = 1.35), thus providing positive evidence of value retrieval and generalization (<xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>; <xref ref-type="bibr" rid="bib24">Hayes and Wedell, 2022</xref>). Contrary to what was predicted by the UNBIASED or the DIVISIVE models, the choice rate for the lowest value options (NB<sub>50</sub>, NT<sub>50</sub>, WB<sub>14,</sub> and WT<sub>14</sub>) did not follow the patterns depicted in panel (<bold>C</bold>). In fact, all lowest value options displayed a similar choice rate (<italic>F</italic>(3,49) = 1.85, p=0.14, η<sup>2</sup> = 0.04), which is only consistent with the predictions of the RANGE model. Concerning other features of the transfer phase performance, the mid-value options valuation is also consistent with the RANGE model, which predict that these options will be valued equally (NT<sub>68</sub> and WT<sub>50</sub>; <italic>t</italic>(49) = –1.34, p=0.19, <italic>d</italic> = −0.19), contrary to the UNIBIASED and DIVISIVE models that both predict NT<sub>68</sub> to be greater than WT<sub>50</sub>. Similar to Experiment 1, these mid-value options displayed a choice rate very close to that of the corresponding lowest value options (NT<sub>50</sub> and WT<sub>14</sub>): this feature is still not perfectly captured by the RANGE model (which predicts their choice rate perfectly in between those of high- and low-value options). To rule out that this effect was not due to a lack of attention for the low- and mid-value options, we also designed two additional experiments where we added forced-choice trials to focus the participants’ attention on all possible options (<xref ref-type="table" rid="table1">Table 1</xref>). As in Experiment 1, focusing participants’ attention to all possible outcomes by forcing their choice did not significantly affect the behavioral performance neither in the learning phase (<italic>F</italic>(2,147) = 0.78, p=0.46, η<sup>2</sup> = 0.01, Levene’s test <italic>F</italic>(2,147) = 0.38, p=0.69) nor in the transfer phase (<italic>F</italic>(2,147) = 0.81, p=0.45, η<sup>2</sup> = 0.01, Levene’s test <italic>F</italic>(2,147) = 0.81, p=0.45). Given the absence of detectable differences across experiments, in the model-based analyses that follow, we pooled the three experiments together. To sum up, the behavioral results are consistent with those of Experiment 1, are in contrast with the predictions of both the UNBIASED and the DIVISIVE models, and are rather consistent with the range normalization process proposed by the RANGE model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig2-figsupp2-v2.tif"/></fig></fig-group><p>We next turned to the results of the transfer phase. Following the analytical strategy used in previous studies, we first checked that the correct choice rate in the transfer was significantly higher than chance (<italic>t</italic>(49) = 9.10, p&lt;0.0001, <italic>d</italic> = 1.29), thus providing positive evidence of value retrieval and generalization (<xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>; <xref ref-type="bibr" rid="bib24">Hayes and Wedell, 2022</xref>). We analyzed the choice rate per symbol, which is the average frequency with which a given symbol is chosen in the transfer phase, and can therefore be taken as a measure of the subjective preference for a given option (<xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Palminteri et al., 2015</xref>). We focus on key comparisons that crucially discriminate between competing models of normalization. First, and contrary to what was predicted by the DIVISIVE model, the choice rate for the high-value options in the trinary contexts (NT<sub>50</sub> and WT<sub>86</sub>) was not lower compared to that of the binary ones (NB<sub>50</sub> and WB<sub>86</sub>). Indeed, if anything, their choice rate was higher (NT<sub>50</sub> vs. NB<sub>50</sub>: <italic>t</italic>(49) = 1.66, p=0.10, <italic>d</italic> = 0.29; WT<sub>86</sub> vs. WB<sub>86</sub>: <italic>t</italic>(49) = 2.80, p=0.0072, <italic>d</italic> = 0.53). Similarly, the choice rate of the low-value options was not affected by their belonging to a binary or trinary context in the direction predicted by the DIVISIVE model. Concerning other features of the transfer phase performance, some comparisons were consistent with the UNBIASED model and not with the RANGE model, such as the fact that high-value options in the narrow contexts (NB<sub>50</sub> and NT<sub>50</sub>) displayed a lower choice rate compared to the high-value options of the wide contexts (WB<sub>86</sub> and WT<sub>86</sub>; <italic>t</italic>(49) = −4.19, p=0.00011, <italic>d</italic> = −0.72), even if the size of the difference appeared to be much smaller to that expected from ex ante model simulations (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, right). Other features were clearly more consistent with the RANGE model. For instance, the fact that the mid-value option in the wide trinary context WT<sub>50</sub> displayed a significantly lower choice rate compared to the high-value options in the narrow contexts (NT<sub>50</sub> and NB<sub>50</sub>) was not predicted by the UNBIASED model. One feature was not explained by any of the models, such as the higher choice rate for the high-value options in the trinary contexts (NT<sub>50</sub> and WT<sub>86</sub>) compared to the binary contexts (NB<sub>50</sub> and WB<sub>86</sub>; <italic>t</italic>(49) = 3.53, p=0.00090, <italic>d</italic> = 0.50; please note that the statistical test stays significant when taking into account all experiments: <italic>t</italic>(149) = 4.11, p&lt;0.0001, <italic>d</italic> = 0.34). Of note, the direction of the effect for this comparison is in stronger contrast with the DIVISIVE (which predicts a difference in the opposite direction) compared the RANGE and UNBIASED models (which predict no difference).</p><p>Finally, the mid-value options (NT<sub>32</sub> and WT<sub>50</sub>) displayed a choice rate very close to that of the corresponding low-value options (NT<sub>14</sub> and WT<sub>14</sub>): this feature is clearly in contrast with both the DIVISIVE and UNBIASED models (which predict their choice rate closer to that of the corresponding high-value options: NT<sub>50</sub> and WT<sub>86</sub>), but not perfectly captured either by the RANGE model (which predicts their choice rate exactly in between those of high- and low-value options). To rule out that this effect was not due to a lack of attention for the low- and mid-value options, we designed two additional experiments where we added forced-choice trials to focus the participants’ attention on all possible options (<xref ref-type="table" rid="table1">Table 1</xref>; <xref ref-type="bibr" rid="bib11">Chambon et al., 2020</xref>). In one experiment (N = 50), forced-choice trials were followed by complete feedback (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), in another experiment (N = 50) forced-choice trials were followed by partial feedback (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Focusing participants’ attention to all possible outcomes by forcing their choice did not significantly affect the behavioral performance neither in the learning phase (<italic>F</italic>(2,147) = 2.75, p=0.067, η² = 0.04, Levene’s test <italic>F</italic>(2,147) = 2.43, p=0.092) nor in the transfer phase (<italic>F</italic>(2,147) = 0.64, p=0.53, η² = 0.00, Levene’s test <italic>F</italic>(2,147) = 0.64, p=0.53). This suggests that the choice rates of the mid options reflect their underlying valuation (rather than lack of information). Given the absence of detectable differences across experiments, in the model-based analyses that follow, we pooled the three experiments together. To sum up, behavioral results, specifically in the transfer phase, are in contrast with the predictions of the DIVISIVE model and are rather consistent with the range normalization process proposed by the RANGE model. Behavioral results in three experiments (N = 50 each) featuring a slightly different design, where we added a mid-value option (NT<sub>68</sub>) between NT<sub>50</sub> and NT<sub>87,</sub> converge to the same broad conclusion: the behavioral pattern in the transfer phase is largely incompatible with that predicted by outcome divisive normalization during the learning phase (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). In the following section, we substantiate these claims by formal model comparison and ex post model simulations analysis.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Experimental design.</title><p>Each version of each experiment was composed of four different learning contexts. Results of Experiments 1 and 3 are presented in the main text; results of Experiment 2 are presented in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. Entries inside square brackets represent the mean outcomes for the lowest, mid (when applicable), and highest value option in a given context. Concerning ‘forced choices,’ ‘unary’ refers to situations where only one option is available and the participants cannot make a choice; ‘binary’ refers to situations where the participant can choose between two out of three options (the high-value option cannot be chosen).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" rowspan="2"/><th align="left" valign="middle" rowspan="2">N</th><th align="left" valign="middle" colspan="6">Learning contexts</th><th align="left" valign="middle" rowspan="2">N forced choices(type / feedback)</th></tr><tr><th align="left" valign="middle">[14,50]</th><th align="left" valign="middle">[14,32,50]</th><th align="left" valign="middle">[50,86]</th><th align="left" valign="middle">[50,68,86]</th><th align="left" valign="middle">[14,86]</th><th align="left" valign="middle">[14,50,86]</th></tr></thead><tbody><tr><td align="left" valign="middle">Experiment 1a</td><td align="left" valign="middle">50</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">0</td></tr><tr><td align="left" valign="middle">Experiment 1b</td><td align="left" valign="middle">50</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">50 (unary / complete)</td></tr><tr><td align="left" valign="middle">Experiment 1</td><td align="left" valign="middle">50</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">50 (unary / partial)</td></tr><tr><td align="left" valign="middle">Experiment 2a</td><td align="left" valign="middle">50</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">0</td></tr><tr><td align="left" valign="middle">Experiment 2b</td><td align="left" valign="middle">50</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">50 (unary / complete)</td></tr><tr><td align="left" valign="middle">Experiment 2c</td><td align="left" valign="middle">50</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">X</td><td align="left" valign="middle">50 (unary / partial)</td></tr><tr><td align="left" valign="middle">Experiment 3a</td><td align="left" valign="middle">100</td><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">90 (binary / complete)</td></tr><tr><td align="left" valign="middle">Experiment 3b</td><td align="left" valign="middle">100</td><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle"/><td align="left" valign="middle">X</td><td align="left" valign="middle">135 (binary / complete)</td></tr></tbody></table></table-wrap></sec><sec id="s2-3"><title>Model comparison and ex post model simulations</title><p>Behavioral analyses of transfer phase choices suggest that learning and valuation are more consistent with the predictions of the RANGE model compared to those of the UNBIASED or the DIVISIVE model. To quantitatively substantiate this claim, we formally compared the quality of fit of the three models using an out-of-sample log-likelihood (<xref ref-type="bibr" rid="bib71">Wilson and Collins, 2019</xref>). Specifically, we first optimized the models’ free parameters (learning rates and choice inverse temperature) in order to maximize the log-likelihood of observing the learning phase choices, given the model and the parameters. We then used these parameters to generate the log-likelihood of observing the choices in the transfer phase, which were not included in the original model fitting. The RANGE model displayed a much higher mean and median out-of-sample log-likelihood (which indicated better fit) compared to both the DIVISIVE and the UNBIASED models (oosLL<sub>RAN</sub> vs. oosLL<sub>DIV</sub>: <italic>t</italic>(149) = 10.10, p&lt;0.0001, <italic>d</italic> = 0.41; oosLL<sub>RAN</sub> vs. oosLL<sub>UNB</sub>: <italic>t</italic>(149) = 8.34, p&lt;0.0001, <italic>d</italic> = 0.82; <xref ref-type="table" rid="table2">Table 2</xref>). Subsequently, we simulated transfer choice phase using the best fitting, that is, empirical, parameter values (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The results of this ex post simulations confirmed what was inferred from the ex ante simulations and indicated that the RANGE model predicted results much closer to the observed ones, in respect of many key comparisons. All these results were replicated in three additional experiments feature with slightly different design, where the DIVISIVE model displayed a higher mean log-likelihood compared to the UNBIASED model, indicating no robust improvement in the quality of fit (see <xref ref-type="table" rid="table2">Table 2</xref>). Despite the superiority of the RANGE model in terms of both predictive (out-of-sample log-likelihood) and generative (simulation) performance (<xref ref-type="bibr" rid="bib71">Wilson and Collins, 2019</xref>) compared to the UNBIASED and DIVISIVE one, it still failed to perfectly capture transfer phase preference, specifically concerning the mid-value options. In the subsequent section, we propose how the RANGE model could be further improved to obviate this issue.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Quantitative model comparison in Experiments 1 and 2.</title><p>Values reported here represent mean ± SD and median of out-of-sample log-likelihood for each model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model</th><th align="left" valign="bottom" colspan="2">Experiment 1 (N = 150)Out-of-sample log-likelihood</th><th align="left" valign="bottom" colspan="2">Experiment 2 (N = 150s)Out-of-sample log-likelihood</th></tr><tr><th align="left" valign="bottom">Mean ± SD</th><th align="left" valign="bottom">Median</th><th align="left" valign="bottom">Mean ± SD</th><th align="left" valign="bottom">Median</th></tr></thead><tbody><tr><td align="left" valign="bottom">UNBIASED</td><td align="char" char="plusmn" valign="bottom">–275.31 ± 268.75</td><td align="char" char="." valign="bottom">–162.53</td><td align="char" char="plusmn" valign="bottom">–227.24 ± 269.72</td><td align="char" char="." valign="bottom">–125.40</td></tr><tr><td align="left" valign="bottom">DIVISIVE</td><td align="char" char="plusmn" valign="bottom">–143.38 ± 70.40</td><td align="char" char="." valign="bottom">–124.91</td><td align="char" char="plusmn" valign="bottom">–159.89 ± 65.20</td><td align="char" char="." valign="bottom">–141.07</td></tr><tr><td align="left" valign="bottom">RANGE</td><td align="char" char="plusmn" valign="bottom">–116.72 ± 57.91</td><td align="char" char="." valign="bottom">–109.23</td><td align="char" char="plusmn" valign="bottom">–109.71 ± 43.91</td><td align="char" char="." valign="bottom">–106.83</td></tr><tr><td align="left" valign="bottom">RANGE (ω)</td><td align="char" char="plusmn" valign="bottom">–97.70 ± 55.52</td><td align="char" char="." valign="bottom">–78.73</td><td align="char" char="plusmn" valign="bottom">–91.99 ± 37.79</td><td align="char" char="." valign="bottom">–79.57</td></tr></tbody></table></table-wrap><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Qualitative model comparison.</title><p>Behavioral data (black dots, n=150) superimposed on simulated data (colored bars) for the UNBIASED (<bold>A</bold>), DIVISIVE (<bold>B</bold>), and RANGE (<bold>C</bold>) models. Simulated data in the transfer phase were obtained with the best-fitting parameters, optimized on all four contexts of the learning phase. Dashed lines represent the key prediction for each model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Ruling out more complex forms of divisive normalization.</title><p>Behavioral data (black dots, n=150) superimposed on simulated data (colored bars) for Experiment 1 (top row) and Experiment 2 (bottom row), for the DIVISIVE (<bold>A, D</bold>), DIVISIVE<sup>ω</sup> (<bold>B, E</bold>), and DIVISIVE<sup>full</sup> (<bold>C, F</bold>) models. Simulated data in the transfer phase were obtained with the best-fitting parameters, optimized on all four contexts of the learning phase. Dashed lines represent the key features that allow discriminating divisive normalization models from range normalization and unbiased value representations. To confirm that our power manipulation in the RANGE<sup>ω</sup> model would not affect our predictions in the DIVISIVE model (especially, the difference in value between options from binary and trinary contexts, e.g., WB<sub>86</sub> and WT<sub>86</sub>), we implemented the same manipulation in the DIVISIVE model. In the DIVISIVE<sup>ω</sup> model, the normalized outcome is power-transformed by the <inline-formula><mml:math id="inf17"><mml:mi>ω</mml:mi></mml:math></inline-formula> parameter (<inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>ω</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), as follows: <inline-formula><mml:math id="inf19"><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>⁡</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> where <inline-formula><mml:math id="inf20"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of contextually relevant stimuli. Crucially, for <inline-formula><mml:math id="inf21"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, the DIVISIVE<sup>ω</sup> model reduces to the DIVISIVE model. As expected, the DIVISIVE<sup>ω</sup> model was unable to match participants’ behavior despite a small improvement in the prediction of the choice rates for mid and lowest value options, in both experiments. Moreover, over both experiments, quantitative model comparison favored the DIVISIVE<sup>ω</sup> model over the DIVISIVE model (oosLL<sub>DIV(ω)</sub> = −129.25 ± 64.94, median = −116.98; oosLL<sub>DIV(ω)</sub> vs. oosLL<sub>DIV</sub>: <italic>t</italic>(299) = 11.50, p&lt;0.0001, <italic>d</italic> = 0.66) but not over range-adaptation model (oosLL<sub>DIV(ω)</sub> vs. oosLL<sub>RAN(ω)</sub>: <italic>t</italic>(299) = −11.71, p&lt;0.0001, <italic>d</italic> = −0.68). In conclusion, the addition of a power transformation was insufficient to correct the behavioral predictions of the DIVISIVE model. Finally, we acknowledge that the normalization rule we implemented is a simpler implementation of classical divisive normalization (<xref ref-type="bibr" rid="bib39">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Louie et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Webb et al., 2021</xref>). To make sure that this over-simplification did not affect the main results of this study, we implemented a more complex version of divisive normalization, including a semi-saturation parameter, a normalization weight parameter, and a <inline-formula><mml:math id="inf22"><mml:mi>p</mml:mi></mml:math></inline-formula>-norm parameter (<xref ref-type="bibr" rid="bib70">Webb et al., 2021</xref>): <inline-formula><mml:math id="inf23"><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>η</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></inline-formula> where <inline-formula><mml:math id="inf24"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of contextually relevant stimuli; the parameter <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>σ</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> determines, in a neural system, how neural activity saturates with increased input and can be interpreted as the baseline activity level in the normalization; the parameter <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>η</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> determines the contribution to the normalization from other alternatives; each alternative is scaled by the magnitude and number of its elements by a norm of degree <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Crucially, the DIVISIVE<sup>full</sup> model is nested within the DIVISIVE and UNBIASED model: when <inline-formula><mml:math id="inf28"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf29"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, the DIVISIVE<sup>full</sup> model reduces to the DIVISIVE model; when <inline-formula><mml:math id="inf31"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, the DIVISIVE<sup>full</sup> model reduces to the UNBIASED model (no normalization). Quantitative model comparison showed a substantial improvement in the ability of the DIVISIVE<sup>full</sup> model to fit participants’ data over the simple DIVISIVE model (oosLL<sub>DIV(full)</sub> = −136.19 ± 88.33, median = −120.63; oosLL<sub>DIV(full)</sub> vs. oosLL<sub>DIV</sub>: <italic>t</italic>(299) = 4.69, p&lt;0.0001, <italic>d</italic> = 0.27), but not over range adaptation models (oosLL<sub>DIV(full)</sub> vs. oosLL<sub>RAN(ω)</sub>: <italic>t</italic>(299) = −10.36, p&lt;0.0001, <italic>d</italic> = −0.60). These comparisons were consistent with model simulations, which clearly show that the key features of the model fail to predict transfer performance. In conclusion, and unsurprisingly given the structure of the model, the more complex version of the divisive normalization rule was unable to match participants’ behavior in our task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Choice rates per option in the transfer phase and model simulations.</title><p>Colored maps of pairwise choice rates during the transfer phase of Experiment 1 (top row) and experiment 2 (bottom row), for each option when compared to each of the nine other options, noted here generically as Option 1 and Option 2 in increasing order. Model simulations (colored circles) are superimposed over behavioral data (colored squares). Comparisons between the same symbols are undefined (black squares).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Model attributions across participants.</title><p>Model attributions per participant and percentage of participants (n=150) explained by the RANGE, DIVISIVE, UNBIASED (left), and RANGE<sup>ω</sup> (right) models in Experiment 1 (<bold>A</bold>) and Experiment 2 (<bold>B</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig3-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Improving the RANGE model</title><p>Although model comparison and model simulation both unambiguously favored the RANGE model over the UNBIASED and DIVISIVE models, the RANGE model is not perfect at predicting participants’ choices in the transfer phase (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). As mentioned previously, the mid-value options in trinary contexts (NT<sub>32</sub> and WT<sub>50</sub>) displayed a choice rate closer to that of the corresponding low-value options (NT<sub>14</sub> and WT<sub>14</sub>): a feature that was not captured by the RANGE model, which predicts their choice rate to be exactly halfway of those of low-value (NT<sub>14</sub> and WT<sub>14</sub>) and high-value (NT<sub>50</sub> and WT<sub>86</sub>) options. In addition, the choice rate of low-value options of all contexts (NB<sub>14</sub>, NT<sub>14</sub>, WB<sub>14</sub>, and WT<sub>14</sub>) was underestimated by the RANGE model. These observations are <italic>prima facie</italic> compatible with the idea that outcomes are not processed linearly (<xref ref-type="bibr" rid="bib6">Bernoulli, 1738</xref>; <xref ref-type="bibr" rid="bib42">Ludvig et al., 2018</xref>). To formally test this hypothesis with the goal of improving the RANGE model, we augmented it with a free parameter <inline-formula><mml:math id="inf33"><mml:mi>ω</mml:mi></mml:math></inline-formula> that applies a nonlinear transformation to the normalized outcome. More specifically, in this modified RANGE model (RANGE<sup>ω</sup>), the normalized outcome is power-transformed by the <inline-formula><mml:math id="inf34"><mml:mi>ω</mml:mi></mml:math></inline-formula> parameter (<inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>ω</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) as follows:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Crucially, for <inline-formula><mml:math id="inf36"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, the RANGE<sup>ω</sup> reduced to the RANGE model; for <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the RANGE<sup>ω</sup> model induces a concave deformation of the normalized outcome; for <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ω</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the RANGE<sup>ω</sup> model induces a convex deformation of the normalized outcome. Quantitative model comparison favored the RANGE<sup>ω</sup> model over all other models, including the RANGE model (<xref ref-type="table" rid="table2">Table 2</xref>) (oosLL<sub>RAN</sub> vs. oosLL<sub>RAN(ω)</sub>: <italic>t</italic>(149) = −6.98, p&lt;0.0001, <italic>d</italic> = −0.57; <xref ref-type="table" rid="table2">Table 2</xref>). The inspection of model simulations confirmed that the RANGE<sup>ω</sup> model closely captures participants’ behavior in the transfer phase. More specifically, the mid-value options (NT<sub>32</sub> and WT<sub>50</sub>) and the low-value options (NB<sub>14</sub>, NT<sub>14</sub>, WB<sub>14</sub>, and WT<sub>14</sub>) were better estimated in all contexts (<xref ref-type="fig" rid="fig4">Figure 4A</xref>; this was also true for Experiment 2; see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). On average, the power parameter <inline-formula><mml:math id="inf39"><mml:mi>ω</mml:mi></mml:math></inline-formula> was &gt;1 (mean ± SD: 2.97 ± 1.36, <italic>t</italic>(149) = 17.81, p&lt;0.0001, <italic>d</italic> = 1.45), suggesting that participants value the mid outcome less than the midpoint between the lowest and highest outcomes (i.e., closer to the low-value option, <xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Predictions of the nonlinear RANGE model.</title><p>(<bold>A</bold>) Curvature function of the normalized reward per participant. Each gray line was simulated with the best-fitting power parameter <inline-formula><mml:math id="inf40"><mml:mi>ω</mml:mi></mml:math></inline-formula> for each participant. Dashed line represents the identity function (<inline-formula><mml:math id="inf41"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), purple line represents the average curvature over participant, and shaded area represents SEM. (<bold>B</bold>) Behavioral data (black dots, n=150) superimposed on simulated data (colored bars) for the RANGE<sup>ω</sup> model. Simulated data in the transfer phase were obtained with the best-fitting parameters, optimized on all four contexts of the learning phase. Dashed lines represent the key prediction for the model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig4-v2.tif"/></fig></sec><sec id="s2-5"><title>Investigating the attentional mechanisms underlying weighted normalization</title><p>However, our current design does not allow to tease apart two possible mechanisms underlying subjective weighting of outcome captured by power transformation. One possibility (implicit in the formulation we used) is that participants ‘perceive’ mid outcomes as being closer to the low one because the high outcome ‘stands out’ due to its value. Another possibility is that participants give a higher subjective weighting to chosen outcomes because of the very fact that they were chosen and obtained. The current design and results do not allow to tease apart these interpretations because during the learning phase the mid-value options were chosen as much as the low-value options (7.2% and 6.8%, <italic>t</italic>(149) = 0.97, p=0.33, <italic>d</italic> = 0.04) and therefore mid outcomes were almost systematically unchosen outcomes.</p><p>To address this issue, we ran two additional experiments (Experiments 3a and 3b), featuring, as before, wide and narrow learning contexts (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The key manipulation in this new experiment consisted of learning contexts where we interleaved trinary choices with binary choices, where the high-value option was presented but not available to the participant (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). We reasoned that by doing so we would be able to increase the number of times the mid-value options were chosen. The manipulation was successful in doing so: in the learning contexts featuring binary choices, the mid-value options were chosen on 48% of the trials (Experiment 3a) and 67% (Experiment 3b); significantly more than the corresponding high-value option in the same learning context (Experiment 3a, wide: <italic>t</italic>(99) = 6.03, p&lt;0.0001, <italic>d</italic> = 0.95; narrow <italic>t</italic>(99) = 5.43, p&lt;0.0001, <italic>d</italic> = 0.80; Experiment 3b, wide: <italic>t</italic>(99) = 33.27, p&lt;0.0001, <italic>d</italic> = 4.47; narrow <italic>t</italic>(99) = 34.06, p&lt;0.0001, <italic>d</italic> = 4.33; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Experimental design and main results of Experiment 3.</title><p>(<bold>A</bold>) Choice contexts in the learning phase. Participants were presented with four choice contexts varying in the amplitude of the outcomes’ range (narrow or wide) and the number of available options (trinary or binary decisions). (<bold>B</bold>) Trial sequence for a binary trial (50 or 75% of the total number of learning trials), where the high-value option was presented but not available to the participant, and a standard trinary trial (50 or 25% of the total number of learning trials). (<bold>C</bold>) Behavioral data (black dots, n=200) superimposed on simulated data (colored bars) for the RANGE<sup>ω</sup> and RANGE<sup>ω+</sup> models. Simulated data in the transfer phase were obtained with the best-fitting parameters, optimized on all four contexts of the learning phase. Dashed lines represent the key prediction for the model. (<bold>D</bold>) Curvature functions of the normalized reward per participant. Each gray line was simulated with the best-fitting power parameters <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each participant. Dashed line represents the identity function (<inline-formula><mml:math id="inf44"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), purple line represents the average curvature over participant, and shaded area represents SEM. Results in (<bold>C</bold>) and (<bold>D</bold>) are pooled data for Experiments 3a and 3b.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Design and behavioral results of Experiment 3.</title><p>(<bold>A</bold>) Experiment design of Experiments 3a (n=100) and 3b (n=100). All contexts included three options, but one option was not selectable on either 50 or 75% of the learning trials. (<bold>B–, C</bold>) Behavior results of Experiment 3a (<bold>B</bold>) and Experiment 3b (<bold>C</bold>) in each learning context. Left: choice rate in the trinary trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig5-figsupp1-v2.tif"/></fig></fig-group><p>We then turned to the analysis of the transfer choices and found that the manipulation was also effective in manipulating the mid-value option, so that in the contexts featuring binary choices (i.e., impossibility of choosing the high-value options), the mid options were valued more compared to the full trinary contexts (i.e., when they were almost never chosen) (Experiment 3a, wide: <italic>t</italic>(99) = 22.80, p&lt;0.0001, <italic>d</italic> = 3.46; narrow: <italic>t</italic>(99) = 20.10, <italic>d</italic> = 3.06, p&lt;0.0001; Experiment 3b, wide: <italic>t</italic>(99) = 21.96, p&lt;0.0001, <italic>d</italic> = 3.88; narrow <italic>t</italic>(99) = 20.46, p&lt;0.0001, <italic>d</italic> = 3.76; <xref ref-type="fig" rid="fig5">Figure 5C</xref>). Interestingly, the results were virtually identical in the experiment with 50% and that with 25% trinary trials despite the choosiness of the high-value options being very different in the two experiments. In addition, the signatures of range adaptation (narrow vs. wide) being replicated, we pooled the experiments in the main figure.</p><p>The behavioral results thus suggest that mid outcomes, although range normalized, <italic>can</italic> be valued correctly in between the lowest and the highest outcome if we force choices toward the mid-value option. These results are therefore consistent with the hypothesis that outcome weighting is contingent with option choosiness rather than a bias in outcome evaluation per se. To objectify this conclusion, we compared the RANGE<sup>ω</sup> previously described, with a more complex one (RANGE<sup>ω+</sup>) where two different power <inline-formula><mml:math id="inf45"><mml:mi>ω</mml:mi></mml:math></inline-formula> parameters apply to the obtained (chosen: <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and forgone (unchosen: <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) outcomes. This augmented model displayed better higher quality of fit in both experiments (as proxied by the out-of-sample log-likelihood of the transfer phase; oosLL<sub>RAN(ω)</sub> vs. oosLL<sub>RAN(ω+)</sub>: <italic>t</italic>(199) = −7.73, p&lt;0.0001, <italic>d</italic> = −0.30). This quantitative result was backed up by model simulations analysis showing that only the RANGE<sup>ω+</sup> was able to capture the change in valuation in the mid-value options (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Finally, we compared the weighting parameters and found <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> significantly lower than <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<italic>t</italic>(199) = −17.28, p&lt;0.0001, <italic>d</italic> = −1.92; <xref ref-type="fig" rid="fig5">Figure 5D</xref>). To conclude, these additional experiments further clarify the cognitive mechanisms (and specifically the role of attention) underlying outcome encoding.</p></sec><sec id="s2-6"><title>Explicit assessment of option values</title><p>In addition to the transfer phase, participants performed another value elicitation assessment, where they were asked to explicitly rate the average value of each option using a slider ranging from 0 to 100. This explicit elicitation phase allowed us to have a complementary estimation of participants’ subjective valuations of each option to compare them with the choice-based transfer phase. The subjective values elicited through explicit ratings were consistent with those elicited in the transfer phase hrough binary choices in many key aspects (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Indeed, against what was predicted by the DIVISIVE principle, option subjective values did not depend on the number of options in each context, but rather on their ordinal value within the context (minimum, mid, maximum). This pattern is even clearer when looking at the difference between reported subjective values and the objective values of each option (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Crucially, the subjective value of the options with an objective value of 50 (NB<sub>50</sub>, NT<sub>50</sub>, and WT<sub>50</sub>) is completely determined by its position in the range of its context, and not by the total sum of the options in this context. Finally, to compare elicitation methods, we simulated transfer phase choices based on the explicit elicitation ratings. More specifically, for each participant and comparison, we generated choices using an argmax selection rule on the subjective values they explicitly reported for each option (see <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>). We found the pattern simulated using explicit ratings to closely match the actual choice rates of the transfer phase (simulated data vs. behavioral data per option: Spearman’s <italic>ρ</italic>(8) = 0.99, p&lt;0.0001, <xref ref-type="fig" rid="fig6">Figure 6C</xref>), suggesting that both elicitation methods tap into the same valuation system. Similar results and conclusions could be drawn from Experiment 2 (Spearman’s <italic>ρ</italic>(8) = 0.99, p&lt;0.0001, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) and Experiment 3, where we confirmed that the explicit ratings of a given option were highly dependent on its position within the range (see <xref ref-type="fig" rid="fig6">Figure 6D and E</xref>). Furthermore, we also confirmed that the pattern simulated using explicit ratings closely matched the actual choice rates of Experiment 3 transfer phase (simulated data vs. behavioral data per option: Spearman’s <italic>ρ</italic>(10) = 1.00, p&lt;0.0001, <xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Results from the explicit elicitation phase of Experiments 1 and 3.</title><p>(<bold>A, D</bold>) Reported subjective values in the elicitation phase for each option, in Experiment 1 (<bold>A</bold>, n=150) and Experiment 3 (<bold>D</bold>, n=200). Points indicate individual average, and shaded areas indicate probability density function, 95% confidence interval, and SEM. Purple areas indicate the actual objective value for each option. (<bold>B, E</bold>) Difference between reported subjective value and actual objective value for each option, arranged in ascending order, in Experiment 1 (<bold>B</bold>) and Experiment 3. The legend of the x-axis represents the values of the options of the context in which each option was learned (actual option value shown in bold). Points indicate individual average, and shaded areas indicate probability density function, 95% confidence interval, and SEM. (<bold>C, F</bold>) Behavioral choice-based data (black dots) superimposed on simulated choice-based data (colored bars), in Experiment 1 (<bold>C</bold>) and Experiment 3 (<bold>F</bold>). Simulated data were obtained with an argmax rule assuming that participants were making transfer phase decision based on the explicit subjective ratings of each option.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Qualitative model comparison and explicit ratings of Experiment 2.</title><p>Behavioral data (black dots, n=150) superimposed on simulated data (colored bars) for the UNBIASED (<bold>A</bold>), DIVISIVE (<bold>B</bold>), RANGE (<bold>C</bold>), and RANGE<sup>ω</sup> (<bold>D</bold>) models. Simulated data in the transfer phase were obtained with the best-fitting parameters, optimized on all four contexts of the learning phase. Dashed lines represent the key prediction for each model. (<bold>E</bold>) Curvature function of the normalized reward per participant. Each gray line was simulated with the best-fitting power parameter <inline-formula><mml:math id="inf50"><mml:mi>ω</mml:mi></mml:math></inline-formula> for each participant. Dashed line represents the identity function (<inline-formula><mml:math id="inf51"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), purple line represents the average curvature over participant, and shaded area represents SEM. (<bold>F</bold>) Reported subjective values in the elicitation phase for each option. Points indicate individual average, and shaded areas indicate probability density function, 95% confidence interval, and SEM. Purple areas indicate the actual objective value for each option. (<bold>G</bold>) Difference between reported subjective value and actual objective value for each option, arrange in ascending order. The legend of the x-axis represents the values of the options of the context in which each option was learned (actual option value shown in bold). Points indicate individual average, and shaded areas indicate probability density function, 95% confidence interval, and SEM. (<bold>H</bold>) Behavioral choice-based data (black dots) superimposed on simulated choice-based data (colored bars). Simulated data were obtained with an argmax rule assuming that subjects were making transfer phase decisions based on the explicit subjective ratings of each option. Similar to Experiment 1, model comparison favored the RANGE model over to both the DIVISIVE and the UNBIASED models (oosLL<sub>RAN</sub> vs. oosLL<sub>DIV</sub>: <italic>t</italic>(149) = 10.61, p&lt;0.0001, <italic>d</italic> = 0.87; oosLL<sub>RAN</sub> vs. oosLL<sub>UNB</sub>: <italic>t</italic>(149) = 5.71, p&lt;0.0001, <italic>d</italic> = 0.47; Table 2). Model simulations also confirmed what inferred from the ex ante simulations and indicated that the RANGE model predicts results much closer to the observed ones in respect of many key comparisons. Model comparison and model simulations of the RANGE<sup>ω</sup> model supported the conclusions from Experiment 1. On average, the power parameter <inline-formula><mml:math id="inf52"><mml:mi>ω</mml:mi></mml:math></inline-formula> was &gt;1 (mean ± SD: 2.83 ± 1.47, <italic>t</italic>(149) = 15.25, p&lt;0.0001, <italic>d</italic> = 1.25), suggesting that participants value the mid-value options less than the midpoint between the lowest and highest options (i.e., closer to the lowest option). Quantitative model comparison favored the RANGE<sup>ω</sup> model over all other models, including the RANGE model (Table 2) (oosLL<sub>RAN</sub> vs. oosLL<sub>RAN(ω)</sub>: <italic>t</italic>(149) = −8.63, p&lt;0.0001, <italic>d</italic> = −0.70; Table 2). Moreover, the inspection of model simulations confirmed that the RANGE<sup>ω</sup> model perfectly captures participants’ behavior in the transfer phase. More specifically, the mid-value options (NT<sub>68</sub> and WT<sub>50</sub>) and the lowest value options (NB<sub>14</sub>, NT<sub>14</sub>, WB<sub>14</sub>, and WT<sub>14</sub>) were better estimated in all contexts. To conclude, the addition of a power parameter allowed our model to match participants’ behavior almost perfectly. Consistently with the results of Experiment 1, the subjective values elicited through explicit ratings were consistent with those elicited through binary choices in many key aspects. Again, to compare elicitation methods, we simulated transfer phase choices based on the explicit elicitation ratings. We found the pattern simulated using explicit ratings to perfectly match the actual choice rates of the transfer phase (<italic>t</italic>(149) = 1.18, p=0.24, <italic>d</italic> = 0.10), suggesting that both elicitation methods tap into the same valuation system.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-fig6-figsupp1-v2.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this article, we sought to challenge the current dominant view of how value-related signals are encoded for economic decision-making. More precisely we designed behavioral paradigms perfectly tailored to discriminate between unbiased (or ‘absolute’) representations from context-dependent or normalized representations following different, antagonist, views. To do so, we deployed a series of six behavioral reinforcement learning experiments consisting of an initial learning phase (where participants learned to associate options to rewards) and a transfer – or generalization – phase allowing us to infer the subjective learned value of each option (<xref ref-type="bibr" rid="bib49">Palminteri and Lebreton, 2021</xref>).</p><p>By systematically manipulating outcome ranges, we were able to confirm that behavioral data was inconsistent with the idea that humans learn and represent values in an unbiased manner. Indeed, subjective values were similar for the options presented in the decision contexts with narrow or wide decision ranges despite their objective values being very different. This result was quantitatively backed up by both model simulations and out-of-sample likelihood analyses that suggested the unbiased model being worst on average than any other normalization model. Thus, our findings significantly add up to the now overwhelming body of evidence indicating that value representations are highly context-dependent even in the reinforcement learning scenario, both in human (<xref ref-type="bibr" rid="bib30">Klein et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Juechems et al., 2022</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Palminteri et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Hayes and Wedell, 2022</xref>) and nonhuman animals (<xref ref-type="bibr" rid="bib72">Yamada et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib56">Pompilio and Kacelnik, 2010</xref>; <xref ref-type="bibr" rid="bib55">Pompilio et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Solvi et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Matsumoto and Mizunami, 2004</xref>; <xref ref-type="bibr" rid="bib17">Ferrucci et al., 2019</xref>).</p><p>On the other side, by contrasting binary to trinary options decision problems, we were able to provide robust evidence against the idea that value context-dependence follows the functional form of divisive normalization in the reinforcement learning scenario (<xref ref-type="bibr" rid="bib38">Louie and Glimcher, 2012</xref>; <xref ref-type="bibr" rid="bib40">Louie et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Louie, 2022</xref>; <xref ref-type="bibr" rid="bib21">Glimcher, 2022</xref>; <xref ref-type="bibr" rid="bib14">Daviet and Webb, 2023</xref>). Our demonstration relied on the straightforward and well-accepted idea that virtually any instantiation of the divisive normalization model would predict a strong (we specify ‘strong’ because a minimal non-null choice set size effect is also predicted by simply assuming choices deriving from a softmax decision rule) choice (description-based task) or outcome (reinforcement-based task) set size effect: all things being equal, the subjective value of an option or an outcome in a trinary decision context should be lower compared to that of a similar value belonging to a binary context. We find no evidence for such an effect. In fact, if anything, the subjective values of options belonging to trinary decision contexts were numerically higher compared to that of the binary decision contexts. Beyond the quantitative analysis of behavioral performance, model fitting and simulations analysis also revealed that the divisive normalization model dramatically failed to correctly reproduce the behavioral pattern (<xref ref-type="bibr" rid="bib48">Palminteri et al., 2017</xref>). Crucially, this was also true for fully parameterized versions of the divisive normalization model (<xref ref-type="bibr" rid="bib70">Webb et al., 2021</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>The behavioral results were rather consistent with an alternative rule, range normalization, according to which subjective value signals are normalized as a function of the maximum and minimum values (regardless of the number of options). This normalization rule is reminiscent of the range principle proposed by Parducci to explain perceptual (and later also affective) judgments (<xref ref-type="bibr" rid="bib51">Parducci, 1995</xref>). In contrast to divisive normalization, range normalization predicts that contextually high- and low-value options are assigned the same subjective value regardless of their absolute value and the number of options in a given choice set. This behavior hallmark of divisive normalization was falsified in all experiments. Of note, although the quality of the fit of the range normalization model was significantly better compared to that of the divisive normalization model, in terms of both predictive accuracy (out-of-sample likelihood) and behavioral signatures, it manifestly failed to properly capture the subjective values attributed to the mid-value options in trinary choice contexts. This pattern was not affected by introducing forced-choice trials to focus the participants’ attention on mid-option outcomes (for at least five trials) (<xref ref-type="bibr" rid="bib11">Chambon et al., 2020</xref>). To further improve the range normalization model fit, we endowed it with a nonlinear weighting of normalized subjective values. Nonlinear weighting parameters, although they do not provide mechanistic accounts, are often introduced in models of decision-making to satisfactorily account for utility and probability distortions (<xref ref-type="bibr" rid="bib26">Hertwig and Erev, 2009</xref>; <xref ref-type="bibr" rid="bib68">Wakker, 2010</xref>). Introducing this weighting parameter improved the model fit qualitatively and quantitatively in a substantial manner. Importantly, introducing the same parameter into the divisive model did not improve its performance significantly (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The weighted range normalization model improved the fit assuming that the mid- and low-value options are subjectively perceived as much closer than they are in reality. We believe that this may derive from attentional mechanisms that bias evidence accumulation as a function of outcomes and option expected values (<xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>; <xref ref-type="bibr" rid="bib33">Krajbich et al., 2010</xref>; <xref ref-type="bibr" rid="bib73">Zilker and Pachur, 2022</xref>). To further probe this hypothesis, we designed and ran an additional experiment (Experiment 3) where we manipulate the possibility of choosing the high-value option in trinary learning contexts. This manipulation successfully managed to ‘correct’ the subjective valuation of mid-value options (while leaving unaffected the valuation of the other options). The behavior in this experiment was successfully captured by further tweaking the weighted range normalization model by assuming that different weighting parameters apply to the chosen and unchosen outcomes. By finding concave and convex weighting functions for the chosen and unchosen outcomes, respectively, the model managed to explain why forcing the participant to choose the mid-value option increases its subjective valuation. We believe that these results further reinforce the hypothesis that outcome valuation interacts with attentional deployment. In fact, it is reasonable to assume that the obtained outcome is attended more than the forgone ones (after all it is the more behaviorally relevant outcome) and that increased attention devoted to the obtained outcomes “boosts” its value (<xref ref-type="bibr" rid="bib33">Krajbich et al., 2010</xref>). This effect can also be conceptually linked to a form of choice-confirmation bias, where the mid-value outcome is perceived as better than it actually is (<xref ref-type="bibr" rid="bib11">Chambon et al., 2020</xref>).</p><p>In addition to assessing subjective values using choices as standardly done in behavioral economics and nonhuman animal-based neuroscience, we also assessed subjective option values using explicit ratings (<xref ref-type="bibr" rid="bib18">Garcia et al., 2021</xref>). Despite the fact that a wealth of evidence in decision-making research suggests that subjective values are highly dependent on whether they are inferred from choices or ratings (also referred to as the <italic>revealed</italic> versus <italic>stated</italic> preferences dichotomy; <xref ref-type="bibr" rid="bib36">Lichtenstein and Slovic, 2006</xref>), post-learning explicit ratings delivered results remarkably consistent with choice-revealed preferences. Indeed, transfer phase choices could almost perfectly be reproduced from explicit ratings, which were, in turn, more consistent with range, rather than divisive normalization process. In addition to provide a welcome test of robustness of our results, the similarity between choice-based and rating-based subjective values also demonstrates the context-dependent valuation span across procedural as well as declarative representational systems (<xref ref-type="bibr" rid="bib19">Gershman and Daw, 2017</xref>; <xref ref-type="bibr" rid="bib7">Biderman and Shohamy, 2021</xref>).</p><p>Beyond the specific question of its functional form, one could ask why option values would be learned and represented in a relative or normalized manner? In other terms, what is the functional reason for context-dependent representations? One partial answer to this question can be tracked to studies showing that context-dependent preferences are ecologically rational (in other words, they convey a statistical or strategical advantage over unbiased representations; <xref ref-type="bibr" rid="bib44">McNamara et al., 2012</xref>). In a similar vein, it could be proposed that unbiased value representations are computationally more costly, making relative or context-dependent encoding an efficient solution. Consistent with this idea, a recent study indicates that human participants are capable of adaptatively modulating their value representations from relative to absolute as they learn that the latter scheme is more advantageous (<xref ref-type="bibr" rid="bib28">Juechems et al., 2022</xref>). Another study confirmed that value representations are somehow flexible by showing that shifting the attentional focus from subject emotions to perceive outcome shifts the balance from relative to unbiased value encoding (<xref ref-type="bibr" rid="bib24">Hayes and Wedell, 2022</xref>). However, neither of these studies manage to report situations in which the representational code was fully context-independent.</p><p>Taking into account that relative value encoding has been shown in a plethora of species and situations (<xref ref-type="bibr" rid="bib72">Yamada et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib56">Pompilio and Kacelnik, 2010</xref>; <xref ref-type="bibr" rid="bib55">Pompilio et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Solvi et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Matsumoto and Mizunami, 2004</xref>; <xref ref-type="bibr" rid="bib17">Ferrucci et al., 2019</xref>), it seems also reasonable to suppose that these findings stem from some deep preserved principles of how (value-based) information is encoded and represented in the brain. Accordingly, normalization naturally emerges as a solution to maximize the gain function between underlying stimuli (whose range may vary greatly) and neural response (<xref ref-type="bibr" rid="bib1">Barlow, 1961</xref>). Crucially, and consistently with our results, while there is ample evidence that the divisive normalization rule provides a good account of information encoding in the perceptual system (<xref ref-type="bibr" rid="bib10">Carandini and Heeger, 2011</xref>), several primate neurophysiological studies indicate the value-related signals in dopaminergic neurons (<xref ref-type="bibr" rid="bib65">Tobler et al., 2005</xref>) and the orbitofrontal cortex (<xref ref-type="bibr" rid="bib45">Padoa-Schioppa, 2009</xref>; <xref ref-type="bibr" rid="bib31">Kobayashi et al., 2010</xref>): hubs of the brain valuation system (<xref ref-type="bibr" rid="bib2">Bartra et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Pessiglione and Delgado, 2015</xref>). Similar findings have been replicated in human fMRI (<xref ref-type="bibr" rid="bib9">Burke et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Cox and Kable, 2014</xref>; <xref ref-type="bibr" rid="bib54">Pischedda et al., 2020</xref>). On the other side, neural evidence of divisive normalization in value-based decision-making is scant in the brain valuation system (but see <xref ref-type="bibr" rid="bib72">Yamada et al., 2018</xref>), although it remains possible that activity in the perceptual and attentional systems (such as the parietal cortex) displays signs of divisive normalization (<xref ref-type="bibr" rid="bib37">Louie et al., 2011</xref>).</p><p>Multiple elements of our results concordantly show that divisive normalization does not provide a good account of subjective value representation in human reinforcement learning. More precisely, we were concerned about whether at the outcome stages the subjective values of rewards were normalized according to a divisive (or range) normalization rule (<xref ref-type="bibr" rid="bib28">Juechems et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Louie, 2022</xref>). It is nonetheless still possible that this rule provides a good description of human behavior in other value-based decision-making domains. In fact, most of the previous studies claiming evidence for divisive normalization used other tasks involving items whose values are described (such as snacks or lotteries food) and have not to be extracted from experience (<xref ref-type="bibr" rid="bib39">Louie et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Webb et al., 2021</xref>; <xref ref-type="bibr" rid="bib8">Bucher and Brandenburger, 2022</xref>; <xref ref-type="bibr" rid="bib59">Robinson and Tymula, 2019</xref>). In addition, our study only addressed value normalization of the outcome magnitude space and did not address whether the same rule would apply to other outcome attributes, such as probability (<xref ref-type="bibr" rid="bib14">Daviet and Webb, 2023</xref>).</p><p>However, it is worth noting that evidence concerning previous reports of divisive normalization in humans has been recently challenged and alternative accounts, such as range normalization, have not been systematically tested in these datasets (<xref ref-type="bibr" rid="bib22">Gluth et al., 2020</xref>; <xref ref-type="bibr" rid="bib69">Webb et al., 2020</xref>, although see <xref ref-type="bibr" rid="bib70">Webb et al., 2021</xref> for an exception, where nonetheless the range and the divisive models were not given equal chances due to different parametrization). It is worth mentioning that the range normalization principle has been recently successfully adapted to account for decision-making under risk (<xref ref-type="bibr" rid="bib32">Kontek and Lewandowski, 2018</xref>). On the other side, another recent study compared range and divisive normalization in multi-attribute, description-based decision problems and found evidence for divisive normalization (<xref ref-type="bibr" rid="bib14">Daviet and Webb, 2023</xref>). Taken together, we believe that our and other recent findings call for a critical appraisal of normalization in value-based decision-making comparing with alternative models and using highly diagnostic experimental designs, as the one used here.</p><p>On the other side, by using a reinforcement learning framework, our design has the advantage that it can be readily translated into animal research to further extend and characterize the neural mechanisms underlying these findings. Further research will also determine whether the range normalization rule also applies to primary (positive and negative) rewards even if previous evidence (in humans and animals) suggests that context-dependent principles apply to food and electric shocks (<xref ref-type="bibr" rid="bib72">Yamada et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Conen and Padoa-Schioppa, 2019</xref>; <xref ref-type="bibr" rid="bib56">Pompilio and Kacelnik, 2010</xref>; <xref ref-type="bibr" rid="bib55">Pompilio et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Solvi et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Matsumoto and Mizunami, 2004</xref>; <xref ref-type="bibr" rid="bib17">Ferrucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Vlaev et al., 2009</xref>).</p><p>Despite the fact that an even a ‘naïve’ range normalization presents several computational advantages compared to divisive normalization (such as the fact of being easily translatable to partial feedback and punishment avoidance tasks; <xref ref-type="bibr" rid="bib3">Bavard et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bavard et al., 2021</xref>), we also showed that its fit was far from perfect and there was still room for improvement. For instance, the weighted range normalization rule, although descriptively accurate, is silent concerning its cognitive origin mechanism. Future research, for example, featuring eye-tracking, will be necessary to shed light on these aspects. Future research will also be needed to assess the extent to which the same rule applies to vast decision spaces involving more than three options. Finally, further experiments will be needed to generalize the current models to partial feedback situations where the contextual variables have to be inferred and stored in memory.</p><p>Despite the fact that including the (attentional) weighting parameter improved the quality of fit (both in terms of out-of-sample log-likelihood and model simulations) of range normalization process, we acknowledge that some features of the data were still not perfectly accounted. For instance, even if the effect was small in size, from averaging across several experiments it appeared that the choice rate for the high-value options in the trinary contexts was higher compared to those in the binary ones. Although this feature provides strong evidence against the divisive normalization framework (which predicts the opposite effect), it is also not coherent with the range normalization process. It could be hypothesized that other cognitive and valuation mechanisms concur to generate this effect, such as instance-based or comparison-based decision valuation processes where the options in the trinary contexts would benefit from an additional (positive) comparison (<xref ref-type="bibr" rid="bib23">Gonzalez and Lebiere, 2005</xref>; <xref ref-type="bibr" rid="bib67">Vlaev et al., 2011</xref>).</p><p>To conclude, while our results cast serious doubt about the relevance of the divisive normalization principle in value-based decision-making (<xref ref-type="bibr" rid="bib21">Glimcher, 2022</xref>), they also establish once again that context-dependence represents one of the most pervasive features of human cognition and provides further insights into its algorithmic instantiation.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Across three experiments, we recruited 500 participants (227 females, 243 males, 30 N/A, aged 26.44 ± 8.31 years old) via the Prolific platform (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.co">https://www.prolific.co</ext-link>). The research was carried out following the principles and guidelines for experiments including human participants provided in the Declaration of Helsinki (1964, revised in 2013). The INSERM Ethical Review Committee/IRB00003888 approved the study, and participants were provided written informed consent prior to their inclusion. The results presented in the main text are those of Experiment 1 (N = 150) and Experiment 3 (N = 200). The results of an alternative design (Experiment 2) are presented in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. To sustain motivation throughout the experiment, participants were given a bonus depending on the number of points won in the experiment (average money won in pounds: 5.05 ± 0.50, average performance against chance during the learning phase and transfer phase: M = 0.74 ± 0.087, <italic>t</italic>(149) = 34.04, p&lt;0.0001, <italic>d</italic> = 2.78). The data of one participant for the explicit phase was not included due to technical issues. A pilot online-based experiment was originally performed (N = 40, the results are also presented in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></sec><sec id="s4-2"><title>Behavioral tasks</title><p>Participants performed an online version of a probabilistic instrumental learning task, instantiated as a multiarmed bandit task. After checking the consent form, participants received written instructions explaining how the task worked and that their final payoff would be affected by their choices in the task. During the instructions, the possible outcomes in points (from 0 to 100 points) were explicitly showed as well as their conversion rate (1 point = 0.02 pence). The instructions were concluded with a short three-item quiz to make sure participants’ understanding of the task was sufficient. The instructions were then followed by a short training session of 12 trials aiming at familiarizing the participants with the response modalities. If participants’ performance during the training session did not reach 60% of correct answers (i.e., choices toward the option with the highest expected value), they had to repeat the training session. Participants could also voluntarily repeat the training session up to two times and then started the actual experiment.</p><p>In our main task, options were materialized by abstract stimuli (cues) taken from randomly generated identicons, colored such that the subjective hue and saturation were very similar according to the HSLUV color scheme (<ext-link ext-link-type="uri" xlink:href="https://www.hsluv.org">https://www.hsluv.org</ext-link>). On each trial, two or three cues were presented on different positions (left/middle/right) on the screen. The position of a given cue was randomized, such that a given cue was presented an equal number of times on the left, the middle, and the right. Participants were required to select between the cues by clicking on one cue. The choice window was self-paced. A brief delay after the choice was recorded (500 ms); the outcome was displayed for 1000 ms. There was no fixation screen between trials.</p><sec id="s4-2-1"><title>Experimental design (version a)</title><p>The full task consisted of three phases: one learning phase and two elicitation phases. During the learning phase, cues appeared in fixed pairs/triplets. Each pair/triplet was presented 45 times, leading to a total of 180 trials. Within each pair/triplet, the cues were associated to a deterministic outcome drawn from a normal distribution with variable means <inline-formula><mml:math id="inf53"><mml:mi>μ</mml:mi><mml:mi>ϵ</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,100</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> and fixed variance <inline-formula><mml:math id="inf54"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula> (<xref ref-type="table" rid="table1">Table 1</xref>). At the end of the trial, the cues disappeared and were replaced by the outcome. Once they had completed the learning phase, participants were displayed with the total points earned and their monetary equivalent.</p><p>After the learning phase, participants performed two elicitation phases: a transfer phase and an explicit rating phase. The order of the elicitation phases was counterbalanced across participants. In the transfer phase, the 10 cues from the learning phase were presented in all possible binary combinations (45, not including pairs formed by the same cue). Each pair of cues was presented four times, leading to a total of 180 trials. Participants were explained that they would be presented with pairs of cues taken from the learning phase, and that all pairs would not have been necessarily displayed together before. On each trial, they had to indicate which of the cues was the one with the highest value. In the explicit rating phase, each cue from the learning phase was presented alone. Participants were asked what was the average value of the cue and had to move a cursor ranging from 0 to 100. Each cue was presented four times, leading to a total of 40 trials. In both elicitation phases, the outcome was not provided in order not to modify the subjective option values learned during the learning phase, but participants were informed that their choices would count for the final payoff.</p></sec><sec id="s4-2-2"><title>Experimental design (versions b and c)</title><p>In the learning phase, we added forced-choice trials to the 180 free-choice trials (<xref ref-type="bibr" rid="bib11">Chambon et al., 2020</xref>). In these forced trials, only one option was selectable and the other cue(s) were shaded. We added five forced-choice trials per option, leading to a total of 230 trials in the learning phase. In version b, even in the forced-choice trial, the participants could only see the outcomes of all options. In version c, participants could only see the outcome of the chosen option. The elicitation phases (transfer and explicit rating) remained unchanged.</p></sec><sec id="s4-2-3"><title>Experiment 3</title><p>In the learning phase, cues appeared in fixed triplets only. Each triplet was presented 45 times, leading to a total of 180 trials. We used a 2 × 2 design manipulating the range spread (as in Experiment 1a) and the option availability: in half of the contexts, for some proportion of trials (Experiment 3a: 50%; Experiment 3b: 75%), the most favorable option was unavailable (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). It was displayed on the screen with a shaded mask and was not clickable. At the end of each trial, all cues disappeared and were replaced by the outcome (shaded outcome for the nonclickable option). In the transfer phase, the 12 cues from the learning phase were presented in all possible binary combinations (66, not including pairs formed by the same cue). Each pair of cues was presented two times, leading to a total of 132 trials. In the explicit phase, each cue was presented two times, leading to a total of 24 trials.</p></sec></sec><sec id="s4-3"><title>Behavioral analyses</title><p>For all experiments, we were interested in three different variables reflecting participants’ learning: (1) correct choice rate in the learning phase, that is, choices directed toward the option with the highest objective value; (2) choice rate in the transfer phase, that is, the number of times an option is chosen, divided by the number of times the option is presented; and (3) subjective valuation in the explicit phase, that is, average reported value per option. Statistical effects were assessed using multiple-way repeated-measures ANOVAs with range amplitude (narrow or wide) and number of presented options (binary or trinary decision problem, <xref ref-type="fig" rid="fig1">Figure 1</xref>) as within-participant factor and experiment version (a,b,c) as between-participant factors. Post hoc tests were performed using one-sample and two-sample <italic>t</italic>-tests for respectively within- and between-experiment comparisons. To assess overall performance, additional one-sample <italic>t</italic>-tests were performed against chance level (0.5 – two-option contexts – and 0.33 – three-option contexts). We report the <italic>t</italic>-statistic, p-value, and Cohen’s <italic>d</italic> to estimate effect size (two-sample <italic>t</italic>-test only). Given the large sample size (n = 500), central limit theorem allows us to assume normal distribution of our overall performance data and apply properties of normal distribution in our statistical analyses, as well as sphericity hypotheses. Concerning ANOVA, we report Levene’s test for homogeneity of variance, the uncorrected statistical, as well as Huynh–Feldt correction for repeated-measures ANOVA when applicable (<xref ref-type="bibr" rid="bib20">Girden, 1992</xref>), <italic>F-</italic>statistic, p-value, partial eta-squared (η<sub>p</sub>²), and generalized eta-squared (η²) (when Huynh–Feldt correction is applied) to estimate effect size. All statistical analyses were performed using MATLAB (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link>) and R (<ext-link ext-link-type="uri" xlink:href="https://www.r-project.org">https://www.r-project.org</ext-link>).</p></sec><sec id="s4-4"><title>Computational models</title><p>The goal of our models is to estimate the subjective value of each option and choose the option that maximizes the expected reward (in our case, with the highest expected value). At each trial <inline-formula><mml:math id="inf55"><mml:mi>t</mml:mi></mml:math></inline-formula>, in each context <inline-formula><mml:math id="inf56"><mml:mi>s</mml:mi></mml:math></inline-formula>, the expected value <inline-formula><mml:math id="inf57"><mml:mi>Q</mml:mi></mml:math></inline-formula> of each option <inline-formula><mml:math id="inf58"><mml:mi>i</mml:mi></mml:math></inline-formula> is updated with a delta rule:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a prediction error term. For all our models, at each trial, the chosen and unchosen options are updated with two distinct learning rates for chosen (<inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and unchosen (<inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) options, and separate, outcome-specific, prediction error terms <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , calculated as the difference between the subjective outcome <inline-formula><mml:math id="inf64"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and the expected one:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>We modeled participants’ choice behavior using a softmax decision rule representing the probability for a participant to choose one option <inline-formula><mml:math id="inf65"><mml:mi>a</mml:mi></mml:math></inline-formula> over the other options – one alternative in binary contexts (<inline-formula><mml:math id="inf66"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>), two in trinary contexts (<inline-formula><mml:math id="inf67"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>) in each context <inline-formula><mml:math id="inf68"><mml:mi>s</mml:mi></mml:math></inline-formula>:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mi>*</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mi>*</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf69"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of outcomes presented in a given trial (<inline-formula><mml:math id="inf70"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>) and <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the inverse temperature parameter. High temperatures (<inline-formula><mml:math id="inf72"><mml:mi>β</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) cause the action to be all (nearly) equiprobable. Low temperatures (<inline-formula><mml:math id="inf73"><mml:mi>β</mml:mi><mml:mo>→</mml:mo><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:math></inline-formula>) cause a greater difference in selection probability for actions that differ in their value estimates (<xref ref-type="bibr" rid="bib63">Sutton and Barto, 1998</xref>).</p><p>We compared four alternative computational models: the unbiased (UNBIASED) model, which encodes outcomes on an absolute scale independently of the choice context in which they are presented; the range normalization (RANGE) model, where the reward is normalized as a function of the range of the outcomes, the divisive normalization (DIVISIVE) model, where the reward is normalized as a function of the sum of all the outcomes; and the nonlinear range normalization (RANGE<sup>ω</sup>) model, where the normalized outcome is power-transformed with an additional free parameter.</p><sec id="s4-4-1"><title>Unbiased model</title><p>At trial <inline-formula><mml:math id="inf74"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, for all contexts <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mo>.</mml:mo></mml:math></inline-formula> For each option <inline-formula><mml:math id="inf76"><mml:mi>i</mml:mi></mml:math></inline-formula>, the subjective values <inline-formula><mml:math id="inf77"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> are encoded as the participants see the outcomes, that is, their objective value in points.<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,100</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula></p></sec><sec id="s4-4-2"><title>Range normalization model</title><p>At trial <inline-formula><mml:math id="inf78"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, for all contexts <inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>.</mml:mo></mml:math></inline-formula> The subjective values <inline-formula><mml:math id="inf80"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> are encoded depending on the value of the other options, specifically the maximum and the minimum available rewards.<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are, respectively, the maximum and minimum outcomes presented in a given trial. In version c, where only the reward of the chosen option is displayed, the outcomes of unchosen options are replaced with the last seen outcomes for these options (<xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>).</p></sec><sec id="s4-4-3"><title>Divisive normalization model</title><p>At trial <inline-formula><mml:math id="inf83"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, for all options <inline-formula><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>.</mml:mo></mml:math></inline-formula> The outcomes are encoded depending on the value of all the other options, specifically the sum of all available rewards.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf85"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of outcomes presented in a given trial (<inline-formula><mml:math id="inf86"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>). In version c, where only the reward of the chosen option is displayed, the outcomes of unchosen options are replaced with the last seen outcomes for these options (<xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>).</p></sec><sec id="s4-4-4"><title>Nonlinear range normalization model</title><p>At trial <inline-formula><mml:math id="inf87"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, for all contexts <inline-formula><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>.</mml:mo></mml:math></inline-formula> The subjective values <inline-formula><mml:math id="inf89"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are encoded depending on the value of the other options, specifically the maximum and the minimum available rewards. This normalized outcome is then set to the power of <inline-formula><mml:math id="inf90"><mml:mi>ω</mml:mi></mml:math></inline-formula>, with <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>ω</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are, respectively, the maximum and minimum outcomes presented in a given trial. In version c, where only the reward of the chosen option is displayed, the outcomes of unchosen options are replaced with the last seen outcomes for these options (<xref ref-type="bibr" rid="bib62">Spektor et al., 2019</xref>).</p></sec><sec id="s4-4-5"><title>Conditional, nonlinear range normalization model</title><p>Finally, in Experiments 3a and 3b only, we tested a more complex version of the model, which allowed for different weighting parameters for obtained (<inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and forgone (<inline-formula><mml:math id="inf95"><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) outcomes. The weighting parameters were allowed same range as before.</p></sec></sec><sec id="s4-5"><title>Ex ante simulations</title><p>The model predictions displayed in <xref ref-type="fig" rid="fig1">Figure 1C</xref> were obtained by simulating choices of artificial agents. The simulated choices were equivalent to those later performed by the participants, that is, 180 trials (45 per learning contexts) in the learning phase (where the deterministic outcomes were drawn from a normal distribution with variable means <inline-formula><mml:math id="inf96"><mml:mi>μ</mml:mi><mml:mi>ϵ</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,100</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> and fixed variance <inline-formula><mml:math id="inf97"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula>) and 180 trials (4 per comparison) in the transfer phase. The update rule for the option values is described in <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>. Predictions for each experiment were simulated for a set of 50 agents to match our number of participants per version. Each agent was associated with a set of parameters <inline-formula><mml:math id="inf98"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> for the inverse temperature, the learning rate of the chosen option, and the learning rate for unchosen options, respectively. The parameters were independently drawn from prior distributions, which we took to be Beta(1.1,1.1) for the learning rates and Gamma(1.2,5) for the inverse temperature (<xref ref-type="bibr" rid="bib15">Daw et al., 2011</xref>). The value of the inverse temperature is irrelevant in the learning phase because the feedback is always complete, which means that the options should converge, in average, to their subjective average value independently of the choice, provided that the learning rates are different from 0. Moreover, in the transfer phase, to obtain the agents’ preferences based on the learned option values, we chose to use an argmax decision rule instead of a softmax decision rule. At each trial <inline-formula><mml:math id="inf99"><mml:mi>t</mml:mi></mml:math></inline-formula> in the transfer phase, comparing option <inline-formula><mml:math id="inf100"><mml:mi>a</mml:mi></mml:math></inline-formula> and option <inline-formula><mml:math id="inf101"><mml:mi>b</mml:mi></mml:math></inline-formula>, the probability of choosing option <inline-formula><mml:math id="inf102"><mml:mi>a</mml:mi></mml:math></inline-formula> is calculated as follows:<disp-formula id="equ11"> <label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.5</mml:mn></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a vector of the final <inline-formula><mml:math id="inf104"><mml:mi>Q</mml:mi></mml:math></inline-formula>-values at the end of the learning phase.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The research was carried out following the principles and guidelines for experiments including human participants provided in the declaration of Helsinki (1964, revised in 2013). The INSERM Ethical Review Committee / IRB00003888 approved and participants were provided written informed consent prior to their inclusion.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83891-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and codes are available here <ext-link ext-link-type="uri" xlink:href="https://github.com/hrl-team/3options">https://github.com/hrl-team/3options</ext-link> (copy archived at <xref ref-type="bibr" rid="bib5">Bavard and Palminteri, 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>SP is supported by the European Research Council under the European Union’s Horizon 2020 research and innovation program (ERC) (RaReMem: 101043804), and the Agence National de la Recherche (CogFinAgent: ANR-21-CE23-0002-02; RELATIVE: ANR-21-CE37-0008-01; RANGE: ANR-21-CE28-0024-01). The Departement d’études cognitives is supported by the Agence National de la Recherche (ANR; FrontCog ANR-17-EURE-0017). SB acknowledges support from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement no. 948545).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Possible principles underlying the transformations of sensory messages</article-title><source>Sens Commun</source><volume>1</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartra</surname><given-names>O</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The valuation system: a coordinate-based meta-analysis of BOLD fMRI experiments examining neural correlates of subjective value</article-title><source>NeuroImage</source><volume>76</volume><fpage>412</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.02.063</pub-id><pub-id pub-id-type="pmid">23507394</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bavard</surname><given-names>S</given-names></name><name><surname>Lebreton</surname><given-names>M</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Coricelli</surname><given-names>G</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reference-point centering and range-adaptation enhance human reinforcement learning at the cost of irrational preferences</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4503</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06781-2</pub-id><pub-id pub-id-type="pmid">30374019</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bavard</surname><given-names>S</given-names></name><name><surname>Rustichini</surname><given-names>A</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Two sides of the same coin: Beneficial and detrimental consequences of range adaptation in human reinforcement learning</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabe0340</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abe0340</pub-id><pub-id pub-id-type="pmid">33811071</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bavard</surname><given-names>S</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>3Options</data-title><version designator="swh:1:rev:6a269e4733e5b068dc6ce2f70ffcbaaf0df8d9df">swh:1:rev:6a269e4733e5b068dc6ce2f70ffcbaaf0df8d9df</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3bf96fd88a4ab96fe0ab1d03284f683a54b90222;origin=https://github.com/hrl-team/3options;visit=swh:1:snp:27bed8f502e1160c7950bf5cc3e359073c5434c1;anchor=swh:1:rev:6a269e4733e5b068dc6ce2f70ffcbaaf0df8d9df">https://archive.softwareheritage.org/swh:1:dir:3bf96fd88a4ab96fe0ab1d03284f683a54b90222;origin=https://github.com/hrl-team/3options;visit=swh:1:snp:27bed8f502e1160c7950bf5cc3e359073c5434c1;anchor=swh:1:rev:6a269e4733e5b068dc6ce2f70ffcbaaf0df8d9df</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernoulli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1738">1738</year><article-title>Specimen theoriae novae de mensura sortis</article-title><source>Commentarii Academiae Scientiarum Imperialis Petropolitanae</source><volume>5</volume><fpage>175</fpage><lpage>192</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biderman</surname><given-names>N</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Memory and decision making interact to shape the value of unchosen options</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>4648</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-24907-x</pub-id><pub-id pub-id-type="pmid">34330909</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bucher</surname><given-names>SF</given-names></name><name><surname>Brandenburger</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Divisive normalization is an efficient code for multivariate Pareto-distributed environments</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2120581119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2120581119</pub-id><pub-id pub-id-type="pmid">36161961</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Baddeley</surname><given-names>M</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Partial adaptation of obtained and observed value signals preserves information about gains and losses</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>10016</fpage><lpage>10025</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0487-16.2016</pub-id><pub-id pub-id-type="pmid">27683899</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambon</surname><given-names>V</given-names></name><name><surname>Théro</surname><given-names>H</given-names></name><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Vandendriessche</surname><given-names>H</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Information about action outcomes differentially affects learning from self-determined versus imposed choices</article-title><source>Nature Human Behaviour</source><volume>4</volume><fpage>1067</fpage><lpage>1079</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-0919-5</pub-id><pub-id pub-id-type="pmid">32747804</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conen</surname><given-names>KE</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Partial adaptation to the value range in the macaque orbitofrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3498</fpage><lpage>3513</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2279-18.2019</pub-id><pub-id pub-id-type="pmid">30833513</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>KM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>BOLD subjective value signals exhibit robust range adaptation</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>16533</fpage><lpage>16543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3927-14.2014</pub-id><pub-id pub-id-type="pmid">25471589</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daviet</surname><given-names>R</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A test of attribute normalization via A double decoy effect</article-title><source>Journal of Mathematical Psychology</source><volume>113</volume><elocation-id>102741</elocation-id><pub-id pub-id-type="doi">10.1016/j.jmp.2022.102741</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title><source>Neuron</source><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1038/35090500</pub-id><pub-id pub-id-type="pmid">11518957</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrucci</surname><given-names>L</given-names></name><name><surname>Nougaret</surname><given-names>S</given-names></name><name><surname>Brunamonti</surname><given-names>E</given-names></name><name><surname>Genovesio</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effects of reward size and context on learning in macaque monkeys</article-title><source>Behavioural Brain Research</source><volume>372</volume><elocation-id>111983</elocation-id><pub-id pub-id-type="doi">10.1016/j.bbr.2019.111983</pub-id><pub-id pub-id-type="pmid">31141723</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>B</given-names></name><name><surname>Cerrotti</surname><given-names>F</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The description-experience gap: a challenge for the neuroeconomics of decision-making under uncertainty</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>376</volume><elocation-id>20190665</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0665</pub-id><pub-id pub-id-type="pmid">33423626</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reinforcement learning and episodic memory in humans and animals: an integrative framework</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>101</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033625</pub-id><pub-id pub-id-type="pmid">27618944</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Girden</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>ANOVA: Repeated Measures</source><publisher-loc>Thousand Oaks, CA</publisher-loc><publisher-name>SAGE</publisher-name><pub-id pub-id-type="doi">10.4135/9781412983419</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Efficiently irrational: deciphering the riddle of human choice</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>669</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.04.007</pub-id><pub-id pub-id-type="pmid">35643845</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname><given-names>S</given-names></name><name><surname>Kern</surname><given-names>N</given-names></name><name><surname>Kortmann</surname><given-names>M</given-names></name><name><surname>Vitali</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Value-based attention but not divisive normalization influences decisions with multiple alternatives</article-title><source>Nature Human Behaviour</source><volume>4</volume><fpage>634</fpage><lpage>645</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-0822-0</pub-id><pub-id pub-id-type="pmid">32015490</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>C</given-names></name><name><surname>Lebiere</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Instance-Based Cognitive Models of Decision-Making</source><publisher-name>Carnegie Mellon University</publisher-name><pub-id pub-id-type="doi">10.1184/R1/6571187.v1</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>WM</given-names></name><name><surname>Wedell</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reinforcement learning in and out of context: The effects of attentional focus</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>1</volume><elocation-id>0001145</elocation-id><pub-id pub-id-type="doi">10.1037/xlm0001145</pub-id><pub-id pub-id-type="pmid">35787139</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrnstein</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Relative and absolute strength of response as a function of frequency of reinforcement</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>4</volume><fpage>267</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1901/jeab.1961.4-267</pub-id><pub-id pub-id-type="pmid">13713775</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertwig</surname><given-names>R</given-names></name><name><surname>Erev</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The description-experience gap in risky choice</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>517</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.09.004</pub-id><pub-id pub-id-type="pmid">19836292</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Payne</surname><given-names>JW</given-names></name><name><surname>Puto</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Adding asymmetrically dominated alternatives: violations of regularity and the similarity hypothesis</article-title><source>Journal of Consumer Research</source><volume>9</volume><elocation-id>90</elocation-id><pub-id pub-id-type="doi">10.1086/208899</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Altun</surname><given-names>T</given-names></name><name><surname>Hira</surname><given-names>R</given-names></name><name><surname>Jarvstad</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Human value learning and representation reflect rational adaptation to task demands</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>1268</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01360-4</pub-id><pub-id pub-id-type="pmid">35637297</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Choices, values, and frames</article-title><source>American Psychologist</source><volume>39</volume><fpage>341</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1037/0003-066X.39.4.341</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>TA</given-names></name><name><surname>Ullsperger</surname><given-names>M</given-names></name><name><surname>Jocham</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning relative values in the striatum induces violations of normative decision making</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>16033</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms16033</pub-id><pub-id pub-id-type="pmid">28631734</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Pinto de Carvalho</surname><given-names>O</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Adaptation of reward sensitivity in orbitofrontal neurons</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>534</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4009-09.2010</pub-id><pub-id pub-id-type="pmid">20071516</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kontek</surname><given-names>K</given-names></name><name><surname>Lewandowski</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Range-dependent utility</article-title><source>Management Science</source><volume>64</volume><fpage>2812</fpage><lpage>2832</lpage><pub-id pub-id-type="doi">10.1287/mnsc.2017.2744</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Armel</surname><given-names>C</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual fixations and the computation and comparison of value in simple choice</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1292</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1038/nn.2635</pub-id><pub-id pub-id-type="pmid">20835253</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname><given-names>M</given-names></name><name><surname>Bavard</surname><given-names>S</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Assessing inter-individual differences with task-related functional neuroimaging</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>897</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0681-8</pub-id><pub-id pub-id-type="pmid">31451737</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Signals in human striatum are appropriate for policy update rather than value prediction</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>5504</fpage><lpage>5511</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6316-10.2011</pub-id><pub-id pub-id-type="pmid">21471387</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lichtenstein</surname><given-names>S</given-names></name><name><surname>Slovic</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The Construction of Preference</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511618031</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Grattan</surname><given-names>LE</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reward value-based gain control: divisive normalization in parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10627</fpage><lpage>10639</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1237-11.2011</pub-id><pub-id pub-id-type="pmid">21775606</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient coding and the neural representation of value</article-title><source>Annals of the New York Academy of Sciences</source><volume>1251</volume><fpage>13</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2012.06496.x</pub-id><pub-id pub-id-type="pmid">22694213</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Khaw</surname><given-names>MW</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Normalization is a general neural mechanism for context-dependent decision making</article-title><source>PNAS</source><volume>110</volume><fpage>6139</fpage><lpage>6144</lpage><pub-id pub-id-type="doi">10.1073/pnas.1217854110</pub-id><pub-id pub-id-type="pmid">23530203</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adaptive neural coding: from biological to behavioral decision-making</article-title><source>Current Opinion in Behavioral Sciences</source><volume>5</volume><fpage>91</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2015.08.008</pub-id><pub-id pub-id-type="pmid">26722666</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Asymmetric and adaptive reward coding via normalized reinforcement learning</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1010350</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010350</pub-id><pub-id pub-id-type="pmid">35862443</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Madan</surname><given-names>CR</given-names></name><name><surname>McMillan</surname><given-names>N</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Spetch</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Living near the edge: How extreme outcomes and their neighbors drive risky choice</article-title><source>Journal of Experimental Psychology. General</source><volume>147</volume><fpage>1905</fpage><lpage>1918</lpage><pub-id pub-id-type="doi">10.1037/xge0000414</pub-id><pub-id pub-id-type="pmid">29565605</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>Y</given-names></name><name><surname>Mizunami</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Context-dependent olfactory learning in an insect</article-title><source>Learning &amp; Memory</source><volume>11</volume><fpage>288</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1101/lm.72504</pub-id><pub-id pub-id-type="pmid">15169858</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>JM</given-names></name><name><surname>Trimmer</surname><given-names>PC</given-names></name><name><surname>Houston</surname><given-names>AI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The ecological rationality of state-dependent valuation</article-title><source>Psychological Review</source><volume>119</volume><fpage>114</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1037/a0025958</pub-id><pub-id pub-id-type="pmid">22022832</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Range-adapting representation of economic value in the orbitofrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>14004</fpage><lpage>14014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3751-09.2009</pub-id><pub-id pub-id-type="pmid">19890010</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Rustichini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rational attention and adaptive coding: a puzzle and a solution</article-title><source>The American Economic Review</source><volume>104</volume><fpage>507</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1257/aer.104.5.507</pub-id><pub-id pub-id-type="pmid">25484369</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Joffily</surname><given-names>M</given-names></name><name><surname>Coricelli</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Contextual modulation of value signals in reward and punishment learning</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8096</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9096</pub-id><pub-id pub-id-type="pmid">26302782</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The importance of falsification in computational cognitive modeling</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id><pub-id pub-id-type="pmid">28476348</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Lebreton</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Context-dependent outcome encoding in human reinforcement learning</article-title><source>Current Opinion in Behavioral Sciences</source><volume>41</volume><fpage>144</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.06.006</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parducci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Range-frequency compromise in judgment</article-title><source>Psychological Monographs</source><volume>77</volume><fpage>1</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1037/h0093829</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parducci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Happiness, Pleasure, and Judgment: The Contextual Theory and Its Applications</source><publisher-name>Lawrence Erlbaum Associates</publisher-name></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname><given-names>M</given-names></name><name><surname>Delgado</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The good, the bad and the brain: neural correlates of appetitive and aversive values underlying decision making</article-title><source>Current Opinion in Behavioral Sciences</source><volume>5</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2015.08.006</pub-id><pub-id pub-id-type="pmid">31179377</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pirrone</surname><given-names>A</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>On multiple sources of value sensitivity</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2207053119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2207053119</pub-id><pub-id pub-id-type="pmid">35969802</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pischedda</surname><given-names>D</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Coricelli</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The effect of counterfactual information on outcome value coding in medial prefrontal and cingulate cortex: from an absolute to a relative neural code</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3268</fpage><lpage>3277</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1712-19.2020</pub-id><pub-id pub-id-type="pmid">32156831</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pompilio</surname><given-names>L</given-names></name><name><surname>Kacelnik</surname><given-names>A</given-names></name><name><surname>Behmer</surname><given-names>ST</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>State-dependent learned valuation drives choice in an invertebrate</article-title><source>Science</source><volume>311</volume><fpage>1613</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1126/science.1123924</pub-id><pub-id pub-id-type="pmid">16543461</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pompilio</surname><given-names>L</given-names></name><name><surname>Kacelnik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Context-dependent utility overrides absolute memory as a determinant of choice</article-title><source>PNAS</source><volume>107</volume><fpage>508</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907250107</pub-id><pub-id pub-id-type="pmid">19966285</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id><pub-id pub-id-type="pmid">19186161</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>S</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>How persuasive is a good fit? A comment on theory testing</article-title><source>Psychological Review</source><volume>107</volume><fpage>358</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.107.2.358</pub-id><pub-id pub-id-type="pmid">10789200</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>T</given-names></name><name><surname>Tymula</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Divisive normalisation of value explains choice-reversals in decision-making under risk</article-title><source>SSRN Electronic Journal</source><volume>1</volume><elocation-id>3492823</elocation-id><pub-id pub-id-type="doi">10.2139/ssrn.3492823</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rustichini</surname><given-names>A</given-names></name><name><surname>Conen</surname><given-names>KE</given-names></name><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal coding and neuronal adaptation in economic decisions</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1208</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01373-y</pub-id><pub-id pub-id-type="pmid">29084949</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solvi</surname><given-names>C</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name><name><surname>Reid</surname><given-names>RJ</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Bumblebees retrieve only the ordinal ranking of foraging options when comparing memories obtained in distinct settings</article-title><source>eLife</source><volume>11</volume><elocation-id>e78525</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.78525</pub-id><pub-id pub-id-type="pmid">36164830</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spektor</surname><given-names>MS</given-names></name><name><surname>Gluth</surname><given-names>S</given-names></name><name><surname>Fontanesi</surname><given-names>L</given-names></name><name><surname>Rieskamp</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How similarity between choice options affects decisions from experience: the accentuation-of-differences model</article-title><source>Psychological Review</source><volume>126</volume><fpage>52</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1037/rev0000122</pub-id><pub-id pub-id-type="pmid">30604988</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Reinforcement learning: an introduction</article-title><source>IEEE Transactions on Neural Networks</source><volume>9</volume><elocation-id>1054</elocation-id><pub-id pub-id-type="doi">10.1109/TNN.1998.712192</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teodorescu</surname><given-names>AR</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Disentangling decision models: from independence to competition</article-title><source>Psychological Review</source><volume>120</volume><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1037/a0030776</pub-id><pub-id pub-id-type="pmid">23356779</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive coding of reward value by dopamine neurons</article-title><source>Science</source><volume>307</volume><fpage>1642</fpage><lpage>1645</lpage><pub-id pub-id-type="doi">10.1126/science.1105370</pub-id><pub-id pub-id-type="pmid">15761155</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlaev</surname><given-names>I</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The price of pain and the value of suffering</article-title><source>Psychological Science</source><volume>20</volume><fpage>309</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02304.x</pub-id><pub-id pub-id-type="pmid">19254237</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlaev</surname><given-names>I</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Stewart</surname><given-names>N</given-names></name><name><surname>Brown</surname><given-names>GDA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Does the brain calculate value?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>546</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.09.008</pub-id><pub-id pub-id-type="pmid">21983149</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wakker</surname><given-names>PP</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Prospect Theory: For Risk and Ambiguity</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511779329</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>R</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Divisive normalization does influence decisions with multiple alternatives</article-title><source>Nature Human Behaviour</source><volume>4</volume><fpage>1118</fpage><lpage>1120</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00941-5</pub-id><pub-id pub-id-type="pmid">32929203</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>R</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The normalization of consumer valuations: context-dependent preferences from neurobiological constraints</article-title><source>Management Science</source><volume>67</volume><fpage>93</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1287/mnsc.2019.3536</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id><pub-id pub-id-type="pmid">31769410</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamada</surname><given-names>H</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Tymula</surname><given-names>A</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Free choice shapes normalized value signals in medial orbitofrontal cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>162</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02614-w</pub-id><pub-id pub-id-type="pmid">29323110</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zilker</surname><given-names>V</given-names></name><name><surname>Pachur</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Nonlinear probability weighting can reflect attentional biases in sequential sampling</article-title><source>Psychological Review</source><volume>129</volume><fpage>949</fpage><lpage>975</lpage><pub-id pub-id-type="doi">10.1037/rev0000304</pub-id><pub-id pub-id-type="pmid">34370495</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83891.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>National Institute on Drug Abuse Intramural Research Program</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.07.14.500032" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.14.500032"/></front-stub><body><p>This important study presents a series of behavioral experiments that test whether value normalization during reinforcement learning follows divisive or range normalization. Behavioral data from probe tests with two alternatives demonstrate convincingly that range normalization provides a better account for choice behavior and value ratings in this setting. These findings will be of interest for readers interested in neuroeconomics and cognitive neuroscience.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83891.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse Intramural Research Program</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Garrett</surname><given-names>Neil</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026k5mg93</institution-id><institution>University of East Anglia</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.14.500032">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.07.14.500032v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;The functional form of value normalization in human reinforcement learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Neil Garrett (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>As you can see in the individual comments there was some confusion about key aspects of your experimental approach and measures. This confusion was resolved during a discussion among reviewers. It would be important that your revised manuscript is more clear about these key aspects. In addition, it would be important to clearly discuss the differences between the predictions made by the different models in choice and learning contexts.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. It would be important to more clearly (and early on) state how 'choice rate' was computed for the transfer phase. This may require describing the transfer phase in more detail.</p><p>2. In lines 272 and 299, please don't use 'perfectly' to qualify the match between model predictions and behavior. I agree that the match is pretty close, but perfectly implies an errorless match, which is not the case here. The authors could consider using 'closely captures' and 'closely match' instead.</p><p>3. There are several typographical errors throughout the manuscript. I am listing a few below but I'd encourage the authors to proofread the manuscript.</p><p>– Line 190: &quot;WB_86 and WT_86&quot;, not &quot;NB_86 and NT_86&quot;.</p><p>– Line 367-368: parenthesis is not closed.</p><p>– Line 377-378: parenthesis is not closed.</p><p>– Line 382: &quot;that value representations are&quot; not &quot;that values representations are&quot;.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The main issues with the paper are expositional. Both a clarification of the simulations used to generate the predictions (Figure 1C) and a clarification of the predictions and appropriate setting of each model in previous literature is needed. In particular, DN makes no relevant predictions in a binary choice setting, so in my initial ignorance, I simply assumed the transfer phase was a trinary choice (because otherwise, DM has no bite). I was so confused by these issues that I needed a discussion with the editor after I submitted my initial report. I have left these comments in my report below (points 1-3 below) so the authors are aware of this miscommunication and that it might help them improve Figure 1 and lines 110 – 130. Finally, the statement of the results (and discussion) is rather opinionated. The writing could do with more nuance regarding the predictions of each model, the appropriate setting for those predictions, and the patterns in the data. In particular, there appear to be patterns that also reject a range normalization account, suggesting that we still have much to learn on this topic.</p><p>Before discussion with the editor:</p><p>1) The choice probabilities do not sum to 1 within condition (for example, in treatment NB, the &quot;50&quot; option seems to be chosen with probability.66 and the &quot;14&quot; with probability &lt;.2. Similar issues arise for DIVISIVE and RANGE. Either I deeply misunderstand the experimental setting, or there is something wrong with the simulation underlying the figure (the simulation uses an argmax, so I do not see how the probabilities can not sum to 1).</p><p>2) Why are the unbiased choice probabilities for &quot;50&quot; equal across NB and NT in UNBIASED? Loosely speaking, the Q-learning rule used by the authors will generate a Q_45 that is normally distributed around means [14,50,32,86] (see SuttonandBarto Equation 2.6 and impose a CentralLimitTheorem). Taking the arg-max of these normal random variables will require that P(50) (weakly) decreases as the set size increases. The authors seem to be claiming that it is constant.</p><p>3) In an effort to find the causes of these differences, I attempted to simulate the model. Apart from a number of issues with the description of the computational model (see below), the reported simulation size leads to substantial variation between simulation runs (with different RNG seeds).</p><p>– Note that I did not include heterogeneity in the learning rate in these simulations, since the theoretical setting does not require it (at least the authors don't claim that it does). So α is set to 0.5, which is the mean of the Β(1.1,1.1) distribution the authors use.</p><p>After discussion with the editor:</p><p>5) Line 123: &quot;It is calculated across all the comparisons involving a given option.&quot; This seemingly innocuous sentence needs much more explanation. Does the transfer phase include ALL binary choices between ALL stimuli, including those across conditions? So is the &quot;86&quot; from the WB paired with the &quot;86&quot; from &quot;WT&quot;? If so, presumably this is what is generating the identical choice rates in UNBIASED since they are essentially the same Q_45(86). It is still deeply unsettling that nothing adds up to 1 in Figure 1C, but at least now I understand the source of the differing predictions. However, there still are some issues:</p><p>– Why is there a slight increase in the choice proportion for 86 between WT and WB? Shouldn't Q_45 be the same for them because the range doesn't change?</p><p>– The number of simulations should be increased (i.e. is it just &quot;10 choose 2&quot; x 50 choices). Given comment 3 above, are we sure there is no noise in the predictions (see points above)? Also, the issue of introducing α heterogeneity into the simulation needs to be addressed, as it seems pointless.</p><p>6) The framing in the introduction of the paper needs to be clear about the use of a binary choice transfer phase. DN, as conceptualized by previous work the authors cite, is a model for how valuations are normalized in the choice process (e.g. Webb et al., Mng Sci, 2020, Figure A.4). As such, it is acknowledged that it has limited prediction in binary choice (see Webb et al., Mng Sci, 2020 Proposition 1). Nearly all of its testable implications arise in trinary (or larger) choice sets. For example, the quote in Footnote 1 is the first sentence of the first paragraph of a section titled &quot;Choice Set Size Effects&quot; which describes studies in which there are more than two alternatives in the choice set. There is no choice set size effects in this experiment because it only examines binary choice in the transfer phase, therefore it excludes the types of choice sets in which DN has been previously observed.</p><p>As such, the authors write that they opt for a reinforcement learning paradigm &quot;because it has greater potential for translational and cross-species research&quot;. It may indeed have translational benefits for cross-species research, and learning settings are important in their own right, but the use of an RL paradigm is not innocuous. There may be critical differences in the form of value coding computations between conditioned learning settings and &quot;descriptive&quot; settings in which valuations are already known (as acknowledged by the authors in an earlier ScienceAdvances paper), not the least of which might be due to the differences between cortical and sub-cortical neural computations (the neural evidence for DN is primarily in cortical regions). There is only one paper I am aware of that applies DN in an RL setting (Louie, PLOSCompBio2022) to account for gain/loss asymmetry in behaviour without the need for different learning rates (though not cited by the authors). This paper should make clear that the application of DN to a conditioned learning phase is a hypothesis raised in this paper, and clarify the distinction between settings in which DN has been previously proposed and examined in other work.</p><p>7) Previous studies add alternatives to a choice set and study value normalization. They are important distinctions from this work because they test for choice set size effects explicitly. Exp 2 in Webb et al. 2020 does such an experiment (albeit not in a learning setting) and finds support for DN rather than RN. The key difference is that the additional alternatives are added to the choice set, rather than only in a learning phase. Daviet and Webb also report a double decoy experiment of description-based choice, which follows a similar intuition to this experiment by placing an alternative &quot;inside&quot; two existing alternatives (though again, directly in the choice set rather than a learning phase). The conclusions from that paper are also in favour of DN rather than RN since this interior alternative appears to alter choice behaviour. This latter paper is particularly interesting because it relies on the same intuition for experimental design as in the experiment reported here. The difference is just the learning vs. choice setting, suggesting this distinction is critical for observing predictions consistent with RN vs DN.</p><p>8) The experiment tests RN vs DN by expanding the range upward with the &quot;86&quot; option. However, RN would make an equivalent set of predictions if the range were equivalently adjusted downward instead (for example by adding a &quot;68&quot; option to &quot;50&quot; and &quot;86&quot;, and then comparing to WB and WT, because it effectively sets the value to &quot;0&quot; or &quot;1&quot; depending only on the range). The predictions of DN would differ however because adding a low-value alternative to the normalization would not change it much. Given the design of the experiment, this would seem like a fairly straightforward prediction to test for RN. Would the behaviour of subjects be symmetric for equivalent ranges, as RN predicts? If so this would be a compelling result, because symmetry is a strong theoretical assumption.</p><p>9) There were issues with the reporting of the results that need to be addressed:</p><p>line 183, typo NB_50?</p><p>lines 182-188 need copy-editing.</p><p>line 186; &quot;numerically lower&quot; has no meaning in a statistical comparison. The standard error (t-test) is telling us that there is too much uncertainty to determine if there is a difference between treatments. No more, no less. Arguing for &quot;numerical&quot; significance is equivalent to arguing that there is signal in noise. Statements like this should be removed from the paper.</p><p>line 188 – &quot;superficially&quot;? a test either rejects a prediction or doesn't. More specifically, this observation also rejects a prediction of RN and should be stated as such for clarity.</p><p>line 183-184 – given that experiments a, b, and c, seem similar and are pooled in later analyses, it would be useful to also pool them to test whether there is an increase in the NT_86 vs NB_86 and WT_86 vs WB_86 choice rates (perhaps using a pooled paired t-test). A tripling of the number of observations might be helpful here for testing whether NB_50&lt;nt_50&lt;wb_86</p><p>line 272 – &quot;perfectly&quot; is an overstatement, given the log-likelihood is not zero. Also see the NB_50&lt;nt_50&lt;wb_86</p><p>line 299 – perhaps also not &quot;perfect&quot; What is the t-test reported here? And how to interpret this p-value of.13? Is this an argument for accepting the null hypothesis?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I have a few comments for the authors to consider:</p><p>1) Training performance (Figure 2) seems very close to the ceiling (i.e. accuracy of 100%) for many of the participants. I would be slightly worried that there isn't enough stochasticity for some participants to derive meaningful parameters from the model – did the authors worry about this or run any checks to guard against this possibility at all? Perhaps more interestingly, this pattern (which suggests participants quickly identify the best option and just keep selecting this) also made me wonder whether the unbiased model with a persistence parameter (i.e. which bonuses the most recent choice) would fare as well in terms of log-likelihood scores (Table 2) and/or capture the same pattern of probe (transfer phase) choices as either the divisive or range models. Could the authors refute this possibility?</p><p>2) Previous models from this group (e.g., Palminteri 2015 Nature Comms) have conceptualised &quot;contextual value&quot; as an iteratively learned quantity (like the Q values here). But in the normalisation models presented, this aspect is absent. For example, min(R) and max(R) in equation 8 used for the range normalisation model are simply the min/max rewards on that trial (or on the previous trial when full feedback is absent). Similarly, the denominator in equation 9 for the divisive normalisation model. In practice, these could each be updated. For example, instead of min(Ri) in eq. 9, they could use something along the lines of Vmin(i) which updates as:</p><p>Vmin(i)t+1 = Vmin(i)t + α*δ t</p><p>δt = min(Ri) – Vmin(i)t</p><p>Did the authors consider this? Or, another way of asking this, is to ask why the authors assume these key components of the model are not learned quantities that get updated over trial-by-trial experience.</p><p>3) Transfer phase – the transfer phase only contains binary combinations. However, during training participants were presented with two binary combination choices and two trinary combination choices (i.e. choices were 50% binary choices and 50% trinary). Could exclusively using binary combinations in the probe phase bias the results at all towards finding stronger evidence for the range model? I'm thinking (and I could be wrong!) that the divisive approach might be better suited to a world where choices are over &gt;2 options. So perhaps participants make choices more consistent with the range model when the probe phase consists of binary choices but might switch to making choices more consistent with the divisive model if this consisted of trinary choices instead. The explicit rating (Figure 5) results do go some way to showing that the results are not just the result of the binary nature of the probe phase (as here ratings are provided one by one I believe). Nonetheless, I'd be interested if the authors had considered this at all or considered this a limitation of the design used. &lt;/nt_50&lt;wb_86&lt;/nt_50&lt;wb_86</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;The functional form of value normalization in human reinforcement learning&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>As you can see, two of the original reviewers were satisfied with your revisions. However, Reviewer #2 does not feel some of their points were fully addressed. Most importantly, there are open issues regarding the contextualization of the current study as well as the important fact that DN was designed to account for choice set effects, rather than normalization during learning, as tested here. Specifically, the binary choice test is only sensitive to value normalization during learning but not whether the form or normalization of these learned values during choice. In other words, while the current experiments test the interesting and worthwhile question of how values are normalized during learning, they do not test whether DN is still applied to RN-learned values at the time of choice. In summary, there is an important distinction between the design of the current study and what has been done to study DN in the past as well as how DN was originally conceptualized. We agree with Reviewer #2 and feel that this distinction should be acknowledged and clarified throughout the paper, most importantly early on in the introduction. Also, the discussion would benefit from revisions that restrict the conclusions of the findings to the appropriate (i.e., learning) context. We are looking forward to receiving a revised version of your interesting and important manuscript.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors have addressed some of my comments from the previous round, though a few remain. In particular, there still seems to be some disagreement over how to describe the study and place it in the context of previous literature. I realize that the authors are addressing (what they interpret) to be overly-strong general statements about the form of normalization in decision-making. This is a useful undertaking. But repeating the same mistake of over-generalizing and not placing the results in context will only serve to continue the confusion.</p><p>1) In their reply, the authors write:</p><p>&quot;The outcome presentation stage is precisely where (range or divisive) normalization is supposed to happen and affect the values of the options (see also Louie et al., 2022). Once values have been acquired during the learning phase (and affected by a normalization process that may or may not depend on the number of outcomes), it does not matter if they are tested in a binary way.&quot;</p><p>and:</p><p>&quot;First of all, we note that nowhere in the papers proposing and/or defending the DN hypothesis of value-based choices, we could find an explicit disclaimer and/or limitation indicating the authors believe that it should only apply to prospective valuation of described prospects and not to retrospective valuation of obtained outcomes.&quot;</p><p>I fundamentally disagree on this point. There are various opinions on where normalization is &quot;supposed to happen.&quot; Multiple papers have conceptualized DN as a process that occurs during decision and not learning (the citations were provided in the previous round, including some that I feel qualified to interpret). Louie et al., 2022 is the first paper that I am aware of to apply it in learning settings (though an argument can be made that Khaw, Louie, Glimcher (2017, PNAS) has elements of a learning task). That it took 15 years of empirical research to explicitly approach the learning setting should be evidence enough of how its proponents were conceptualizing it. The 2011 review paper that the authors interpret as making this claim simply notes that the matching law implies that &quot;the primary determinant of behaviour is the relative value of rewards.&quot; (pg 17). In the immediate sentence following they pose the question: &quot;Does the brain represent action values in absolute terms independent of the other available options, or in relative terms?&quot; to set up the remainder of the paper. The use of the term &quot;action values&quot; by Louie and Glimcher is critical. This is neuro-physiological terminology used to describe the motor/decision stage, where the values that have been learned (by the then-standard RL and RPE circuitry covered in pg 14-16) are used to make decisions (these are the Vs in the 2011 paper, or the Qs in the author's model). It is these Vs that are transformed at the time of decision. See also Webb, Louie, Glimcher (pg 4) for a clear statement of this. Nowhere in these articles is it claimed that the action values in learning settings (again, the Vs!) are formed in a relative manner (like in the authors' Equation 4, 5, and 9 for their Qs).</p><p>Of course, this point does not diminish the fact that the authors are testing for DN during the updating step in learning, with some compelling evidence. But we need to be clear on where the DN theory has been largely proposed by previous literature, and where the DN theory has been previously shown to capture behaviour (choice sets with more than two options). This is what I mean when I say it is a hypothesis the authors (or their previous reviewers) are proposing and evaluating in their papers. There are perfectly good reasons to examine DN in this setting (see the normative discussion below) and that should be sufficient to frame the question. I believe these issues need to be framed clearly by the introduction of the paper, rather than as an aside at the end of the Discussion.</p><p>line 61: This seems like an ideal spot to clearly state that nearly all empirical applications of DN apply it at the decision stage and address choices with more than two options. In particular, the study by Daviet and Webb implements a protocol similar to the design used here but in a &quot;descriptive&quot; rather than a learning setting (see comment below). The text could then note that &quot;Few recent studies have extended….&quot;</p><p>As a final point, if the design included three or more alternatives in their transfer phase, there is some reason to believe that behaviour would depart from the model the authors propose (i.e. we might start to see behaviour influenced by more options at the decision stage). Therefore, contrary to the author's claim, it would &quot;matter if they are tested in a binary way&quot;. Therefore the results should only be interpreted as applying to the learning of the valuations, not their use during decision. I agree with the changes to lines 141-145, however, the paper up to this point has still not stated that DN at the decision stage has no strong predictions in binary choice. The reader thus has no context on why that statement of line 141 is true. This needs to be clarified.</p><p>2) I am re-stating here my comment from Round 1: Previous studies add alternatives to a choice set and study value normalization. They are important distinctions from this work because they test for choice set size effects explicitly. Exp 2 in Webb et al. 2020 does such an experiment (albeit not in a learning setting) and finds support for DN rather than RN. The key difference is that the additional alternatives are added to the choice set, rather than only in a learning phase. Daviet and Webb also report a double decoy experiment of description-based choice, which follows a similar intuition to this experiment by placing an alternative &quot;inside&quot; two existing alternatives (though again, directly in the choice set rather than a learning phase). The conclusions from that paper are also in favour of DN rather than RN since this interior alternative appears to alter choice behaviour. This latter paper is particularly interesting because it relies on the same intuition for experimental design as in the experiment reported here. The difference is just the learning vs. choice setting, suggesting this distinction is critical for observing predictions consistent with RN vs DN.</p><p>R2.5 We thank the Reviewer for pointing another important reference that we originally missed. The suggested paper is now cited in the relevant part of the discussion (which is copied in R2.4 above).</p><p>I'm not sure that the authors interpreted my comment correctly. I don't understand citation #64 in the Discussion or what that paper has to do with risk. I raise the Daviet and Webb study because it implemented a similar design intuition as this study (inserting an option between two options so as not to alter the range), except with &quot;described&quot; alternatives and not learned outcomes. The conclusions in this paper differed from those reported. The manuscript should clearly state this similarity (and important difference in conclusion) between the two as context for the paradigm the authors are using.</p><p>3) In Discussion:</p><p>a) lines 361-371: This paragraph argues that &quot;We find no evidence for such a [choice set size] effect&quot;. As noted in my previous report (and again above), this study is not examining choice set size effects. The size of the choice set in the transfer phase is always binary. Instead, the study is examining the relative coding of learning. This paragraph needs to be re-written so that this is clearly stated. Referring to the results from the transfer task as a &quot;trinary decision context&quot; is confusing.</p><p>b) &quot;However, it is worth noting that evidence concerning previous reports of divisive normalization in humans has been recently challenged and alternatively accounts, such as range normalization, have not been properly tested in these datasets 29,65.&quot;</p><p>The test for Range normalization in the original Louie et al. dataset is in Webb, Louie andGlimcher 2020, Mng Sci and is rejected in favour of DN. It is not clear what the authors mean by &quot;properly.&quot;</p><p>4) R2.3b In the ex-ante simulations, we randomly sampled both of these parameters from Β(1.1,1.1) distributions, which inherently leads to different predictions in the transfer phase for the binary and trinary contexts, because the choice rate impacts the update of each option. The fact that small differences derive from allowing different learning rates for obtained and forgone outcomes, is confirmed by simulations. We nonetheless kept the original simulations, because they correspond better to the models that we simulated (with two learning rates). The authors are arguing that there is a systematic difference in the predictions for Range WT_86 WB 86 when using two learning rates. This is not an obvious result, but the increase to 500 simulations seems to confirm that it is systematic. This should be clarified in the text. Otherwise, it will be confusing to readers why the predictions change.</p><p>5) From Round 1: line 415 – what is meant by &quot;computational advantage&quot; here? The rational inattention literature, which provides the normative foundation of efficient encoding, places emphasis on the frequency of stimuli (which is completely ignored by range normalization) (e.g. Heng, Woodford, Polania, <italic>eLife</italic>).</p><p>Reply: &quot;R2.18 Range and Divisive normalization both provide, to some extent, the computational advantage of being a form of adaptive coding. If the topic interests the Reviewer, we could point to a recent study where their normative status is discussed: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4320894&quot;</p><p>The normative argument for relative coding (including both DN and RN) that the authors point to is specific to a learning context (via the exploration/exploitation tradeoff). And even then, it is not an argument that an efficient code should use only the range. Thus it is important to keep in mind that the results reported in the current manuscript may be somewhat specific to the learning setting implemented in the experimental design. Multiple theoretical and empirical papers from across neuroscience and economics have examined the form of normative computation in general settings (Heng et al., cited above; Steverson et al. cited in previous report; Netzer 2009 AER; Robson and Whitehead, Schaffner et al., 2023, NHB) and all have shown that the efficient code adapts to the full distribution of stimuli. It is reasonable to imagine that some physical systems might approximate this with a piecewise linear algorithm based on the range, but this has not been observed in the nervous system (V1, A1, LIP etc.). Of course this can't rule out the possibility that V1, A1 and LIP get it right with a standard cortical computation, but for some reason other areas have to resort to a cruder approximation. But that isn't the obvious hypothesis. Tracking the minimum and maximum – including identifying which is which – seems both unnecessary and complicated in real-world scenarios. Whether (and how much) the results reported in the paper generalize outside this experimental design is an open question.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83891.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>As you can see in the individual comments there was some confusion about key aspects of your experimental approach and measures. This confusion was resolved during a discussion among reviewers. It would be important that your revised manuscript is more clear about these key aspects. In addition, it would be important to clearly discuss the differences between the predictions made by the different models in choice and learning contexts.</p></disp-quote><p>We thank the Editor and the Reviewers for the opportunity to revise and re-submit our manuscript, which we believe has been substantially improved with the help of the Reviewers’ feedback. We were delighted to see that all the Reviewers were positive about our work and we were equally happy to integrate their insightful and constructive comments. In the revised manuscript, we addressed the concerns about the readability of the paper, more specifically the detailed task description in the introduction and Results sections. Moreover, we now include results from two additional experiments designed to better pinpoint the attentional mechanisms behind the weighted normalization process (and exclude choice repetition as a possible interpretation of our results, see Reviewer 3, point 1). All of the other comments have been addressed point-by-point below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. It would be important to more clearly (and early on) state how 'choice rate' was computed for the transfer phase. This may require describing the transfer phase in more detail.</p></disp-quote><p>We thank the Reviewer for the helpful suggestion. In line with Reviewer 2’s comments, we improved the Results section by rewriting the transfer explanation in more details:</p><p>“For a given option, the transfer phase choice rate was calculated by dividing the number of times an option is chosen by the number of times the option is presented. In the transfer phase, the 10 cues from the learning phase were presented in all possible binary combinations (45, not including pairs formed by the same cue). Each pair of cues was presented four times, leading to a total of 180 trials. Since a given comparison counts for the calculation of the transfer phase choice rate of both involved options, this implies that this variable will not sum to one. Nonetheless, the relative ranking between transfer choice rate can be taken as a behavioral proxy of subjective values.”</p><disp-quote content-type="editor-comment"><p>2. In lines 272 and 299, please don't use 'perfectly' to qualify the match between model predictions and behavior. I agree that the match is pretty close, but perfectly implies an errorless match, which is not the case here. The authors could consider using 'closely captures' and 'closely match' instead.</p></disp-quote><p>This is a fair point, we replaced “perfectly” with “closely” in both sentences, also according to Reviewer 2’s suggestions (Reviewer 2, point 1).</p><disp-quote content-type="editor-comment"><p>3. There are several typographical errors throughout the manuscript. I am listing a few below but I'd encourage the authors to proofread the manuscript.</p></disp-quote><p>Thanks for all, we corrected all these (and other) typos found in the submitted manuscript.</p><disp-quote content-type="editor-comment"><p>– Line 190: &quot;WB_86 and WT_86&quot;, not &quot;NB_86 and NT_86&quot;.</p></disp-quote><p>Thank you, we’ve made the correction.</p><disp-quote content-type="editor-comment"><p>– Line 367-368: parenthesis is not closed.</p></disp-quote><p>Thank you, we’ve made the correction.</p><disp-quote content-type="editor-comment"><p>– Line 377-378: parenthesis is not closed.</p></disp-quote><p>Thank you, we’ve made the correction.</p><disp-quote content-type="editor-comment"><p>– Line 382: &quot;that value representations are&quot; not &quot;that values representations are&quot;.</p></disp-quote><p>Thank you, we’ve made the correction.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1) The main issues with the paper are expositional. Both a clarification of the simulations used to generate the predictions (Figure 1C) and a clarification of the predictions and appropriate setting of each model in previous literature is needed. In particular, DN makes no relevant predictions in a binary choice setting, so in my initial ignorance, I simply assumed the transfer phase was a trinary choice (because otherwise, DM has no bite). I was so confused by these issues that I needed a discussion with the editor after I submitted my initial report. I have left these comments in my report below (points 1-3 below) so the authors are aware of this miscommunication and that it might help them improve Figure 1 and lines 110 – 130. Finally, the statement of the results (and discussion) is rather opinionated. The writing could do with more nuance regarding the predictions of each model, the appropriate setting for those predictions, and the patterns in the data. In particular, there appear to be patterns that also reject a range normalization account, suggesting that we still have much to learn on this topic.</p></disp-quote><p>Concerning the first point, we thank the Reviewer for pointing out the lack of clarity on a crucial aspect of the design (specifically, the relationship between the learning phase and the transfer phase). In the revised manuscript, we better explain the rational between the learning phase (binary vs. trinary choice with feedback) versus the transfer phase (binary choice without feedback).</p><p>In our task, because feedback is absent in the transfer phase, values are only formed (or acquired) during the learning phase. The outcome presentation stage is precisely where (range or divisive) normalization is supposed to happen and affect the values of the options (see also Louie et al., 2022). Once values have been acquired during the learning phase (and affected by a normalization process that may or may not depend on the number of outcomes), it does not matter if they are tested in a binary way. We better explained this rationale in the current version of the manuscript:</p><p>“Crucially, even if the transfer phase involves only binary choices, it can still tease apart the normalization rules affecting outcome valuation during the learning phase. This is because transfer choices are made based on the memory of values acquired during the learning phase, where we purposely manipulated the number of options and their ranges of values, in order to create learning contexts that allow to confidently discriminate between the two normalization accounts.”</p><p>Concerning the second point, we acknowledge that the use of the word “perfectly” may not have be warranted in our case (or probably is not in general in empirical sciences) and, accordingly, we removed its occurrences in favor or more modest nuanced terms (such as “closely”). We also better and sooner highlight the fact that, even though divisive normalization has been proposed in the context of reinforcement learning in the way we implement it, it has been mainly used to explain description-based choices do represent a difference compared to our design:</p><p>“It should be noted here that, although divisive normalization has been more frequently applied to the prospective evaluation of described outcomes (e.g., lotteries; snack-food items), rather than retrospective evaluation of obtained outcomes (e.g., bandits), it has both historical<sup>21</sup> and recent<sup>25</sup> antecedents in the context of reinforcement learning.”</p><disp-quote content-type="editor-comment"><p>Before discussion with the editor:</p><p>2) The choice probabilities do not sum to 1 within condition (for example, in treatment NB, the &quot;50&quot; option seems to be chosen with probability.66 and the &quot;14&quot; with probability &lt;.2. Similar issues arise for DIVISIVE and RANGE. Either I deeply misunderstand the experimental setting, or there is something wrong with the simulation underlying the figure (the simulation uses an argmax, so I do not see how the probabilities can not sum to 1).</p><p>3) Why are the unbiased choice probabilities for &quot;50&quot; equal across NB and NT in UNBIASED? Loosely speaking, the Q-learning rule used by the authors will generate a Q_45 that is normally distributed around means [14,50,32,86] (see SuttonandBarto Equation 2.6 and impose a CentralLimitTheorem). Taking the arg-max of these normal random variables will require that P(50) (weakly) decreases as the set size increases. The authors seem to be claiming that it is constant.</p><p>4) In an effort to find the causes of these differences, I attempted to simulate the model. Apart from a number of issues with the description of the computational model (see below), the reported simulation size leads to substantial variation between simulation runs (with different RNG seeds).</p><p>– Note that I did not include heterogeneity in the learning rate in these simulations, since the theoretical setting does not require it (at least the authors don't claim that it does). So α is set to 0.5, which is the mean of the Β(1.1,1.1) distribution the authors use.</p></disp-quote><p>We thank the Reviewer for raising these points, and for even taking the time to make simulations. Since these points have been reformulated after discussion with the editor, we refer to the points below for detailed responses, that, we hope address the questions raised by the Reviewer (while contributing improving the manuscript).</p><disp-quote content-type="editor-comment"><p>After discussion with the editor:</p><p>5) Line 123: &quot;It is calculated across all the comparisons involving a given option.&quot; This seemingly innocuous sentence needs much more explanation. Does the transfer phase include ALL binary choices between ALL stimuli, including those across conditions? So is the &quot;86&quot; from the WB paired with the &quot;86&quot; from &quot;WT&quot;? If so, presumably this is what is generating the identical choice rates in UNBIASED since they are essentially the same Q_45(86). It is still deeply unsettling that nothing adds up to 1 in Figure 1C, but at least now I understand the source of the differing predictions. However, there still are some issues:</p></disp-quote><p>We thank the Reviewer for the opportunity to clarify. Indeed, the transfer phase involves binary choices between all possible combinations (among 10 stimuli, hence 45 combinations) that were repeated 4 times in order to increase the reliability. For a given option, we indeed plot the number of times it has been chosen, considering all comparisons involving the given option (and this is why the rates do not sum up to 1). We understand that this may be confusing if one is not familiar with this paradigm and this is why we revised the manuscript (Results and Figure 1s’ legend) in order to make it clearer:</p><p>Results section</p><p>“For a given option, the transfer phase choice rate was calculated by dividing the number of times an option is chosen by the number of times the option is presented. In the transfer phase, the 10 cues from the learning phase were presented in all possible binary combinations (45, not including pairs formed by the same cue). Each pair of cues was presented four times, leading to a total of 180 trials. Since a given comparison counts for the calculation of the transfer phase choice rate of both involved options, this implies that this variable will not sum to one. Nonetheless, the relative ranking between transfer choice rate can be taken as a behavioral proxy of subjective values.”</p><p>Legend of Figure 1</p><p>“Note that choice rate in the transfer phase is calculated across all possible binary combinations involving a given option. While score is proportional to the agent’s preference for a given option, it does not sum to one, because any given choice counts for the final score of two options.”</p><p>The reviewer may also find useful Figure 3 —figure supplement 2 that has been used by our and other groups (Bavard et al., 2018; Hayes et al., 2022), where each individual transfer phase choice is presented:</p><disp-quote content-type="editor-comment"><p>– Why is there a slight increase in the choice proportion for 86 between WT and WB? Shouldn't Q_45 be the same for them because the range doesn't change?</p></disp-quote><p>This is an interesting point. In our simulations, as in our optimization procedures, we allowed the models to learn differently from the chosen and the unchosen option(s). In other words, as in previous studies (e.g., Palminteri et al., 2015), all models include two different learning rates: factual (chosen option) and counterfactual (unchosen option(s)). This is implemented to account for the (reasonable) possibility that less attention is given to the unchosen option(s).</p><p>In the ex-ante simulations, we randomly sampled both of these parameters from Β(1.1,1.1) distributions, which inherently leads to different predictions in the transfer phase for the binary and trinary contexts, because the choice rate impacts the update of each option. The fact that small differences derive from allowing different learning rates for obtained and forgone outcomes, is confirmed by simulations. When using only one learning rate for all option updates, the slight increase in the choice proportion disappears:</p><p>We nonetheless kept the original simulations, because they correspond better to the models that we simulated (with two learning rates).</p><disp-quote content-type="editor-comment"><p>– The number of simulations should be increased (i.e. is it just &quot;10 choose 2&quot; x 50 choices). Given comment 3 above, are we sure there is no noise in the predictions (see points above)? Also, the issue of introducing α heterogeneity into the simulation needs to be addressed, as it seems pointless.</p></disp-quote><p>Since the original version of the experiment included a sample size of 50 participants, we chose to run model predictions with the same number of simulations. However, we agree that we need to make sure that increasing the number of simulations will not change the predictions: we therefore simulated the same models but increased the number of simulations from 50 to 500. The results suggest that increasing the number of simulations does not change the model predictions:</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Model predictions generated from 500 simulations.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig1-v2.tif"/></fig><p>Since the simulation results are identical, we prefer to keep the 50 run simulations in order to keep coherent with our actual sample size (and to minimize changes in an otherwise quite abundantly revised manuscripts).</p><disp-quote content-type="editor-comment"><p>6) The framing in the introduction of the paper needs to be clear about the use of a binary choice transfer phase. DN, as conceptualized by previous work the authors cite, is a model for how valuations are normalized in the choice process (e.g. Webb et al., Mng Sci, 2020, Figure A.4). As such, it is acknowledged that it has limited prediction in binary choice (see Webb et al., Mng Sci, 2020 Proposition 1). Nearly all of its testable implications arise in trinary (or larger) choice sets. For example, the quote in Footnote 1 is the first sentence of the first paragraph of a section titled &quot;Choice Set Size Effects&quot; which describes studies in which there are more than two alternatives in the choice set. There is no choice set size effects in this experiment because it only examines binary choice in the transfer phase, therefore it excludes the types of choice sets in which DN has been previously observed.</p><p>As such, the authors write that they opt for a reinforcement learning paradigm &quot;because it has greater potential for translational and cross-species research&quot;. It may indeed have translational benefits for cross-species research, and learning settings are important in their own right, but the use of an RL paradigm is not innocuous. There may be critical differences in the form of value coding computations between conditioned learning settings and &quot;descriptive&quot; settings in which valuations are already known (as acknowledged by the authors in an earlier ScienceAdvances paper), not the least of which might be due to the differences between cortical and sub-cortical neural computations (the neural evidence for DN is primarily in cortical regions). There is only one paper I am aware of that applies DN in an RL setting (Louie, PLOSCompBio2022) to account for gain/loss asymmetry in behaviour without the need for different learning rates (though not cited by the authors). This paper should make clear that the application of DN to a conditioned learning phase is a hypothesis raised in this paper, and clarify the distinction between settings in which DN has been previously proposed and examined in other work.</p></disp-quote><p>The Reviewer here raises a good point (we note that a similar point was also raised and, we hope, addressed in Reviewer2, poinry1, even if we happy to provide additional, related, arguments below). In short, we agree with the Reviewers that our introduction needed to better articulated, including the transition between the current literature on divisive normalization and our study. And we also agree that Louie (2022)’s paper should be mentioned in the introduction, since it nicely set the stage concerning how divisive normalization should be logically implemented in the reinforcement learning framework. This is how we modified the introduction:</p><p>“Even though, to date, most of the empirical studies proposing divisive normalization as a valid model of economic value encoding proposed that option values are vehiculated by explicit features of the stimulus (such as food snacks or lotteries: so-called described options<sup>22,17,23</sup>), few recent studies have extended the framework to account for subjective valuation in the reinforcement learning (or experience-based) context<sup>24,25</sup>. Adjusting the divisive normalization model to a reinforcement learning scenario is easily achieved by assuming that the normalization step occurs at the outcome stage, i.e., when the participant is presented with the obtained (and forgone) outcomes.”</p><p>Concerning the discussion, we expanded the paragraph where we discuss the fact that things may be different in description-based choices (and included more citations):</p><p>“Multiple elements of our results concordantly show that divisive normalization does not provide a good account of subjective value representation in human reinforcement learning. More precisely, we were concerned about whether at the outcome stages, the subjective values of rewards were normalized according to a divisive (or range) normalization rule<sup>24,25</sup>. It is nonetheless still possible that this rule provides a good description of human behavior in other value-based decision-making domains. In fact, most of the previous studies claiming evidence for divisive normalization used other tasks involving items whose values are described (such as snacks or lotteries food) and have not to be extracted from experience<sup>17,19,26,63</sup>. In addition, our study only addressed value normalization only the outcome magnitude space and we did not address whether the same rule would apply to other outcome attributes, such as probability<sup>64</sup>.”</p><disp-quote content-type="editor-comment"><p>7) Previous studies add alternatives to a choice set and study value normalization. They are important distinctions from this work because they test for choice set size effects explicitly. Exp 2 in Webb et al. 2020 does such an experiment (albeit not in a learning setting) and finds support for DN rather than RN. The key difference is that the additional alternatives are added to the choice set, rather than only in a learning phase. Daviet and Webb also report a double decoy experiment of description-based choice, which follows a similar intuition to this experiment by placing an alternative &quot;inside&quot; two existing alternatives (though again, directly in the choice set rather than a learning phase). The conclusions from that paper are also in favour of DN rather than RN since this interior alternative appears to alter choice behaviour. This latter paper is particularly interesting because it relies on the same intuition for experimental design as in the experiment reported here. The difference is just the learning vs. choice setting, suggesting this distinction is critical for observing predictions consistent with RN vs DN.</p></disp-quote><p>We thank the Reviewer for pointing out another important reference that we originally missed. The suggested paper is now cited in the relevant part of the discussion (which is copied in Reviewer 2, point 4 above).</p><disp-quote content-type="editor-comment"><p>8) The experiment tests RN vs DN by expanding the range upward with the &quot;86&quot; option. However, RN would make an equivalent set of predictions if the range were equivalently adjusted downward instead (for example by adding a &quot;68&quot; option to &quot;50&quot; and &quot;86&quot;, and then comparing to WB and WT, because it effectively sets the value to &quot;0&quot; or &quot;1&quot; depending only on the range). The predictions of DN would differ however because adding a low-value alternative to the normalization would not change it much. Given the design of the experiment, this would seem like a fairly straightforward prediction to test for RN. Would the behaviour of subjects be symmetric for equivalent ranges, as RN predicts? If so this would be a compelling result, because symmetry is a strong theoretical assumption.</p></disp-quote><p>We addressed this point above (see Reviewer 2, point 1), but, in short, we ran three versions of the suggested experiment (N=50 each) and the results confirm our conclusions.</p><disp-quote content-type="editor-comment"><p>9) There were issues with the reporting of the results that need to be addressed:</p><p>line 183, typo NB_50?</p></disp-quote><p>Thank you, we’ve made the correction.</p><disp-quote content-type="editor-comment"><p>lines 182-188 need copy-editing.</p></disp-quote><p>Thank you, we’ve made the corrections.</p><disp-quote content-type="editor-comment"><p>line 186; &quot;numerically lower&quot; has no meaning in a statistical comparison. The standard error (t-test) is telling us that there is too much uncertainty to determine if there is a difference between treatments. No more, no less. Arguing for &quot;numerical&quot; significance is equivalent to arguing that there is signal in noise. Statements like this should be removed from the paper.</p></disp-quote><p>Thank you, we agree with the Reviewer and deleted this sentence from the paper.</p><disp-quote content-type="editor-comment"><p>line 188 – &quot;superficially&quot;? a test either rejects a prediction or doesn't. More specifically, this observation also rejects a prediction of RN and should be stated as such for clarity.</p></disp-quote><p>Correct. We modified the sentence accordingly:</p><p>“Concerning other features of the transfer phase performance, some comparisons were consistent with the UNBIASED model and not with the RANGE model, such as the fact that high value options in the narrow contexts (NB<sub>50</sub> and NT<sub>50</sub>) displayed a lower choice rate compared to the high value options of the wide contexts (WB<sub>86</sub> and WT<sub>86</sub>; t(49)=-4.19, p=.00011, d=-0.72), even if the size of the difference appeared to be much smaller to that expected from ex ante model simulations (Figure 1C, right).”</p><disp-quote content-type="editor-comment"><p>line 183-184 – given that experiments a, b, and c, seem similar and are pooled in later analyses, it would be useful to also pool them to test whether there is an increase in the NT_86 vs NB_86 and WT_86 vs WB_86 choice rates (perhaps using a pooled paired t-test). A tripling of the number of observations might be helpful here for testing whether NB_50&lt;nt_50&lt;wb_86.</p></disp-quote><p>This is an interesting point that we indeed did not address in the previous version of the manuscript. To our understanding, this can be addressed by making two comparisons:</p><p>1) We pooled most favorable options from the narrow (NB50 + NT50) contexts on one side, and the wide (WB86 + WT86) contexts on the other side, and we compared them: t(149)=-7.42, p&lt;.0001, d=-0.61. This effect, already mentioned in the Results section (see previous point) can be explained by the fact that some participants are best explained by the UNBIASED or the DIVISIVE models, which both account for this effect. Another explanation, in line with our previous results suggesting that context-dependence in reinforcement learning arises progressively over the task time (Bavard et al., 2018, 2021), could be that values are encoded on an objective (absolute) scale at the beginning of the learning experiment, and are progressively rescaled into context-dependent (relative) values. As we chose not to include such parameters in our models, none of them could capture these potential dynamic effects.</p><p>2) We pooled the most favorable options from the binary (NB50 + WB86) contexts on one side, and the trinary (NT50 + WT86) contexts on the other side, and we compared them: t(149)=-4.11, p&lt;.0001, d=-0.34. This result actually provides <italic>strong</italic> evidence against the DN account, which would predict a significant effect in the <italic>opposite direction</italic>. However, it is true that this effect is not predicted by the RN account either. We believe that this effect (i.e., over-evaluating the most favorable options in trinary contexts compared to the binary ones) may arise from additional (not modelled) valuation processes. A plausible process could be to suppose that the most favorable options in trinary contexts receive a bonus because they have been previously identified as being better than two options (rather than just one as in the binary contexts). This interpretation is consistent with instance-based (or more indirectly with decision-sampling) accounts of decision-making. This observation is interesting and will motivate further research in our (and hopefully also other) teams, involving decision contexts with more than 4 options, but it does not affect our main claim (strong falsification of the DN account). We mentioned this result in the revised manuscript:</p><p>Results:</p><p>“One feature was not explained by any of the models, such as the higher choice rate for the high value options in the trinary contexts (NT<sub>50</sub> and WT<sub>86</sub>) compared to the binary contexts (NB<sub>50</sub> and WB<sub>86</sub>; t(49)=3.53, p=.00090, d=0.50) Please note that, while the statistical test is significant in Experiment 1a, it stays so when taking into account all experiments (t(149)=4.11, p&lt;.0001, d=0.34). Of note, the direction of the effect for this comparison is in stronger contrast with the DIVISIVE (which predicts a difference in the opposite direction) rather than the RANGE and UNBIASED models (which predict no difference).”</p><p>Discussion:</p><p>“Despite the fact that including the (attentional) weighting parameter improved the quality of fit (both in terms of out-of-sample log-likelihood and model simulations) of range normalization process, we acknowledge that some features of the data were still not perfectly accounted. For instance, even if the effect was small in size, from averaging across several experiments it appeared that choice rate for the high value options in the trinary contexts were higher compared to those in the binary ones. Although this feature provides strong evidence against the divisive normalization framework (which predicts the opposite effect), it is also not coherent with the range normalization process. It could be hypothesized that other cognitive and valuation mechanisms concur to generate this effect, such as instance-based or comparison-based decision valuation processes where the options in the trinary contexts would benefit from an additional (positive) comparison<sup>68,69</sup>.”</p><disp-quote content-type="editor-comment"><p>line 272 – &quot;perfectly&quot; is an overstatement, given the log-likelihood is not zero. Also see the NB_50&lt;nt_50&lt;wb_86</p></disp-quote><p>We agree with the Reviewer, and replaced “perfectly” with “closely”, also following Reviewer 1’s suggestion.</p><disp-quote content-type="editor-comment"><p>line 299 – perhaps also not &quot;perfect&quot; What is the t-test reported here? And how to interpret this p-value of.13? Is this an argument for accepting the null hypothesis?</p></disp-quote><p>We agree with the Reviewer, and replaced “perfectly” with “closely”, also following Reviewer 1’s suggestion. We thank the Reviewer for pointing out that a t-test might not be the most appropriate way of comparing simulations (using the explicit values) and behavioral data in this case. To better account for the comparison, we reported in the revised text the correlation between simulated and behavioral data per option (Experiment 1: Spearman’s <italic>ρ</italic>(8)=0.99, <italic>p</italic>&lt;.0001; Experiment 2: Spearman’s <italic>ρ</italic>(8)=0.99, <italic>p</italic>&lt;.0001; Experiment 3: Spearman’s <italic>ρ</italic>(10)=1.00, <italic>p</italic>&lt;.0001):</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Simulated and behavioral data for the Explicit phase.</title><p>Top: behavioral choice-based data (black dots) superimposed on simulated choice-based data (colored bars). Simulated data were obtained using the explicit ratings as values and a argmax decision rule (see Methods). Bottom: average simulated data per option (horizontal error bars) as a function of average behavioral data per option (vertical error bars). Error bars represent s.e.m., plain diagonal line represents idendity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I have a few comments for the authors to consider:</p><p>1) Training performance (Figure 2) seems very close to the ceiling (i.e. accuracy of 100%) for many of the participants. I would be slightly worried that there isn't enough stochasticity for some participants to derive meaningful parameters from the model – did the authors worry about this or run any checks to guard against this possibility at all? Perhaps more interestingly, this pattern (which suggests participants quickly identify the best option and just keep selecting this) also made me wonder whether the unbiased model with a persistence parameter (i.e. which bonuses the most recent choice) would fare as well in terms of log-likelihood scores (Table 2) and/or capture the same pattern of probe (transfer phase) choices as either the divisive or range models. Could the authors refute this possibility?</p></disp-quote><p>We thank Prof. Garrett for this interesting point that we separate in two parts. The first one, concerning whether or not the models are recoverable in our task (even assuming the high level of performance observed). We checked and this is the case (see <xref ref-type="fig" rid="sa2fig3">Author response image 3</xref>). This being said, we note that the specific pattern we are interested in (namely post-learning transfer phase choices) it is quite robust to the choice of the parameter (we ensured that by running ex ante model simulations using specifically distributed parameters (Daw et al., Neuron 2011)) and our claims are based on transfer phase choice.</p><fig id="sa2fig3" position="float"><label>Author response image 3.</label><caption><title>Confusion matrix for model recovery.</title><p>For each model, we performed ex-ante simulations following the procedure described in the Methods section. We then fitted the simulated data with each model and computed the out-of-sample log-likelihood on the transfer phase data. Each cell depicts the frequency with which each model (columns) is best predictive for the simulated data for each model (rows).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig3-v2.tif"/></fig><p>The second point concerns possible confounds that could arise, assuming that the participants might present a form of choice repetition which is not included in the model. Indeed, we were well aware of this possibility and this is why in our first publication about the topic we included (and rejected) a model (that we called HABIT) that precisely instantiated this idea. We refer to Bavard et al. (2021) for more information about the results (see also Palminteri, Behavioral Neuroscience 2023, for a, perhaps even deeper, discussion of the perseveration model in the context of optimistic update).</p><p>Of course, this does not automatically guarantee that in our current set-up, choice repetition or perseveration do not play a role. This is why, following Prof. Garrett’s suggestion, we tested the habit model and show that, while it quantitatively (yet not so much qualitatively) outperforms the UNBIASED model, it does not work as well as the RANGE<sup>w</sup> model (oosLL<sub>HAB</sub> vs oosLL<sub>UNB</sub> : <italic>t</italic>(149)=9.55, <italic>p</italic>&lt;.0001, <italic>d</italic>=0.26; oosLL<sub>HAB</sub> vs oosLL<sub>RAN</sub><sup>w</sup> : <italic>t</italic>(149)=-7.16, <italic>p</italic>&lt;.0001, <italic>d</italic>=-0.72; see <xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>).</p><fig id="sa2fig4" position="float"><label>Author response image 4.</label><caption><title>Model predictions of the UNBIASED (left), RANGE<sup>w</sup> (middle) and HABIT (right).</title><p>Simulated data in the transfer phase were obtained with the best-fitting parameters, optimized on all four contexts of the learning phase.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig4-v2.tif"/></fig><p>Finally, in addition to running the supplementary analyses reported here (and because we thought that these experiments could help better address the cognitive/attentional mechanisms value normalization), we ran two additional experiments in some specific choice contexts, where we prevented the participant to choose the maximum value option. We therefore managed to orthogonalize <italic>option values</italic> from <italic>option choosiness</italic>, because in the conditions where the maximum option could not be chosen, “option choosiness” is higher for the mid-value option. The key idea behind this manipulation is that, in this design, if preferences in the transfer phase derive from a choice repetition mechanism, the mid options should be preferred to the max options (of the forced choice contexts) and the max option of the free choice contexts should be preferred to the max option of the free choice contexts. In short, neither of these two predictions of the choice repetition model are fulfilled by the experiment (accordingly the out-of-sample log-likelihood of the choice repetition model, which is much lower compared to that of our winning model). These experiments, whose main results, methods and figures are reported below for convenience, do inform us however about the attentional mechanisms underlying value normalization.</p><p>Results:</p><p>“Investigating the attentional mechanisms underlying weighted normalization</p><p>However, our design, as implemented so far, does not allow to tease apart two possible mechanisms underlying subjective weighting of outcome captured by power transformation. One possibility (implicit in the formulation we used) is that participants “perceive” mid outcomes as being closer to the low one, because the high outcome “stands out” due to its value. Another possibility is that participants give a higher subjective weighting to chosen outcomes, because of the very fact that they were chosen and obtained. The current design and results do not allow to tease apart these interpretations, because during the learning phase the mid values options were chosen as much as the low value options (7.2% and 6.8%, t(149)=0.97, p=.33, d=0.04) and therefore mid outcomes were almost systematically unchosen outcomes.</p><p>To address this issue, we ran two additional experiments (Experiment 3a and 3b), featuring, as before, wide and narrow learning contexts (Figure 5A). The key manipulation in this new experiment consisted in leaning contexts where we interleaved trinary choices with binary choices, where the high value option was presented, but not available to the participant (Figure 5B). We reasoned that by doing so, we would be able to increase the number of times the mid value options were chosen. The manipulation was successful in doing so: in the learning contexts featuring binary choices, the mid value options were chosen on 48% of the trials (Experiment 3a) and 67% (Experiment 3b); significantly more than the corresponding high value option in the same learning context (Experiment 3a, wide: t(99)=6.03, p&lt;.0001, d=0.95; narrow t(99)=5.43, p&lt;.0001, d=0.80; Experiment 3b, wide: t(99)=33.27, p&lt;.0001, d=4.47; narrow t(99)=34.06, p&lt;.0001, d=4.33; Supplementary Figure S7).</p><p>We then turned to the analysis of the transfer choices and found that the manipulation was also effective in manipulating the mid option value, so that in the contexts featuring binary choices (i.e., impossibility of choosing the high value options), the mid options were valued more compared to the full trinary contexts (i.e., when they were almost never chosen) (Experiment 3a, wide: t(99)=22.80, p&lt;.0001, d=3.46; narrow: t(99)=20.10, d=3.06, p&lt;.0001; Experiment 3b, wide: t(99)=21.96, p&lt;.0001, d=3.88; narrow t(99)=20.46, p&lt;.0001, d=3.76; Figure 5C). Interestingly, the results were virtually identical in the experiment with 50% and that with 25% trinary trials, despite the choosiness of the high value options being very different in the two experiments and the signatures of range adaptation (narrow vs. wide) were replicated (and we therefore pooled the experiment in the main figure).</p><p>The behavioral results thus suggest that mid outcomes, although range normalized, can be valued correctly in between the lowest and the highest outcome, if we force choices toward the mid value option. These results are therefore consistent with the hypothesis that outcome weighting is contingent with option choosiness and a bias in outcome evaluation per so. To objectify this conclusion, we compared the RANGE<sup>w</sup> previously described, with a more complex one (RANGE<sup>w+</sup>) where two different power <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameters apply to the obtained (chosen: <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and forgone (unchosen: <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) outcomes. This augmented model displayed better higher quality of fit in both experiments (as proxied by the out-of-sample log-likelihood of the transfer phase; oosLL<sub>RAN(ω)</sub> vs oosLL<sub>RAN(ω+)</sub> : t(199)=-7.73, p&lt;.0001, d=-0.30). This quantitative result was backed up by model simulations analysis showing that only the RANGE<sup>w+</sup> was able to capture the change in valuation in the mid value options (Figure 5C). Finally, we compared the weighting parameters and found <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> significantly lower then <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (t(199)=-17.28, p&lt;.0001, d=-1.92; Figure 5D). To conclude, these additional experiments further clarify the cognitive mechanisms (and specifically the role of attention) underlying outcome encoding.”</p><p>Discussion:</p><p>“We believe that this may derive from attentional mechanisms that bias evidence accumulation as a function of outcomes and option expected values<sup>5,51,52</sup>. To further probe this hypothesis, we designed and ran an additional experiment (Experiment 3) where we manipulate the possibility of choosing the high value option in trinary learning contexts. This manipulation successfully managed to “correct” the subjective valuation of mid-value options (while leaving unaffected the valuation of the other options). The behavior in this experiment was successfully captured by further tweaking the weighted range normalization model by assuming that different weighting parameters applies to chosen and unchosen outcomes. By finding concave and convex weighting functions for chosen and unchosen outcomes, respectively, the model managed to explain why forcing the participant to choose the mid value option increases its subjective valuation. We believe that these results further reinforce the hypothesis that outcome valuation interacts with attentional deployment. In fact, it is reasonable to assume that the obtained outcome is attended more than the forgone ones (after all it is the more behaviorally relevant outcome) and that increased attention devoted to the obtained outcomes “boosts” its value<sup>51</sup>. This effect can also be conceptually linked to a form of choice-confirmation bias, where the mid value outcome is perceived as better than it actually is<sup>42</sup>.”</p><disp-quote content-type="editor-comment"><p>2) Previous models from this group (e.g., Palminteri 2015 Nature Comms) have conceptualised &quot;contextual value&quot; as an iteratively learned quantity (like the Q values here). But in the normalisation models presented, this aspect is absent. For example, min(R) and max(R) in equation 8 used for the range normalisation model are simply the min/max rewards on that trial (or on the previous trial when full feedback is absent). Similarly, the denominator in equation 9 for the divisive normalisation model. In practice, these could each be updated. For example, instead of min(Ri) in eq. 9, they could use something along the lines of Vmin(i) which updates as:</p><p>Vmin(i)t+1 = Vmin(i)t + α*δ t</p><p>δt = min(Ri) – Vmin(i)t</p><p>Did the authors consider this? Or, another way of asking this, is to ask why the authors assume these key components of the model are not learned quantities that get updated over trial-by-trial experience.</p></disp-quote><p>Prof. Garrett correctly identifies the fact that our normalization variable in the current study are not latent or inferred, but rather extracted from outcome information on a trial-by-trial basis. This is indeed different from most of our previous models (see Palminteri and Lebreton, COBS, 2021). One reason we opted for this formulation is that in the current design, it is quite <italic>plausible</italic>. First, feedback is always complete, so the participant gets at every trial a fairly good picture of the range. This is not the case of course in partial feedback scenarios (half the experiments of Bavard et al., 2018; 2021). In addition to being complete, the feedback was – although stochastic with Gaussian noise – continuous and not binary (or Bernoullian). For instance, even in the complete feedback experiments of Bavard et al. (2021), it was quite possible that in a given trial the subjects show something like [Rc=0; Ru=0] or [Rc=10; Ru=10] and one case see why in this scenario it may be sound to not rely on min(R) or max(R), but rather keep more stable (latent) variable. The second reason is that the current version is more <italic>parsimonious</italic>. In fact, once showed that it works in our set-up, why bother with an additional free parameter? But we of course recognize that shall we move to a partial feedback scenario, latent estimates of Rmax(s) and Rmin(s) will become necessary.</p><p>We acknowledge this fact in the revised discussion:</p><p>“Finally, further experiments will be needed to generalize the current models to partial feedback situations where the contextual variables have to be inferred and stored in memory.”</p><disp-quote content-type="editor-comment"><p>3) Transfer phase – the transfer phase only contains binary combinations. However, during training participants were presented with two binary combination choices and two trinary combination choices (i.e. choices were 50% binary choices and 50% trinary). Could exclusively using binary combinations in the probe phase bias the results at all towards finding stronger evidence for the range model? I'm thinking (and I could be wrong!) that the divisive approach might be better suited to a world where choices are over &gt;2 options. So perhaps participants make choices more consistent with the range model when the probe phase consists of binary choices but might switch to making choices more consistent with the divisive model if this consisted of trinary choices instead. The explicit rating (Figure 5) results do go some way to showing that the results are not just the result of the binary nature of the probe phase (as here ratings are provided one by one I believe). Nonetheless, I'd be interested if the authors had considered this at all or considered this a limitation of the design used. &lt;/nt_50&lt;wb_86&lt;/nt_50&lt;wb_86</p></disp-quote><p>Prof. Garrett here asks whether “<italic>using</italic> binary combinations in the probe phase bias the results at all towards finding stronger evidence for the range model?” The short answer of this question is “no”. To understand why, one has to first consider that in our task, outcomes are received only during the learning phase. During the learning phase, outcomes could be ternary of binary and here is the key step/phase where the two models diverge in how value is processed. The second point to consider is that the simulations, ex ante and ex post alike show that the three models are perfectly discriminable, so as far as one accepts that we are correctly instantiating divisive and range normalization in the context of RL (and we believe Prof. Garrett does so), one should also accept that our set-up allows to discriminate among them without providing unfair advantage to any of them (Please also see some related responses to Reviewer 2).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>As you can see, two of the original reviewers were satisfied with your revisions. However, Reviewer #2 does not feel some of their points were fully addressed. Most importantly, there are open issues regarding the contextualization of the current study as well as the important fact that DN was designed to account for choice set effects, rather than normalization during learning, as tested here. Specifically, the binary choice test is only sensitive to value normalization during learning but not whether the form or normalization of these learned values during choice. In other words, while the current experiments test the interesting and worthwhile question of how values are normalized during learning, they do not test whether DN is still applied to RN-learned values at the time of choice. In summary, there is an important distinction between the design of the current study and what has been done to study DN in the past as well as how DN was originally conceptualized. We agree with Reviewer #2 and feel that this distinction should be acknowledged and clarified throughout the paper, most importantly early on in the introduction. Also, the discussion would benefit from revisions that restrict the conclusions of the findings to the appropriate (i.e., learning) context. We are looking forward to receiving a revised version of your interesting and important manuscript.</p></disp-quote><p>We thank the Editors for taking care of our submission, their thoughtful and careful comments, as well as the overall positive appreciation of our paper.</p><p>We are happy to submit a further revised version of the manuscript that further stress the difference between previous and current literature, together with a point-by-point response to the interesting points raised by Reviewer 2.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The authors have addressed some of my comments from the previous round, though a few remain. In particular, there still seems to be some disagreement over how to describe the study and place it in the context of previous literature. I realize that the authors are addressing (what they interpret) to be overly-strong general statements about the form of normalization in decision-making. This is a useful undertaking. But repeating the same mistake of over-generalizing and not placing the results in context will only serve to continue the confusion.</p></disp-quote><p>We thank the Reviewer for their time and unquestionable dedication in providing a very thorough review. We are also glad to see that, on the important matters (design, analyses and results), we seem to be on the same page. We appreciate that there remains some degree of disagreement between the perception of the claims that have been made in literature concerning the domain of application of DN, even though we believe that (especially in the Revised manuscript) we made some substantial steps to make our literature more accurate (thanks to the thoughtful suggestions of the Reviewer).</p><p>We believe that <xref ref-type="fig" rid="sa2fig5">Author response image 5</xref> correctly exemplifies the current state of affair and how we presented things in our revised manuscript. Based on the previous and the current exchanges, it is also clear that the Reviewer agrees with us. Of note, even though Author response image5 would probably not be appropriated for the main text of a published paper, we are glad that <italic>eLife</italic> policy is to publish the Rebuttal letter and that it could be used to further clarify the evolution of the literature concerning the applications of DN to the value-based framework.</p><p>Finally, before diving into the specific responses, we hope that the Reviewer did not miss the point that the very reason we got interested in this research question in the first place, is that we found the DN framework extremely convincing and stimulating. We praise all the authors previously involved this line of research to deliver an impressive set of (testable) theories and findings.</p><fig id="sa2fig5" position="float"><label>Author response image 5.</label><caption><title>Value-based decision-making framework (as depicted by Rangel et al.) and where DN normalization has been traditionally (“Valuation”) and recently (“Outcome evaluation”) applied.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig5-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>1) In their reply, the authors write:</p><p>&quot;The outcome presentation stage is precisely where (range or divisive) normalization is supposed to happen and affect the values of the options (see also Louie et al., 2022). Once values have been acquired during the learning phase (and affected by a normalization process that may or may not depend on the number of outcomes), it does not matter if they are tested in a binary way.&quot;</p><p>and:</p><p>&quot;First of all, we note that nowhere in the papers proposing and/or defending the DN hypothesis of value-based choices, we could find an explicit disclaimer and/or limitation indicating the authors believe that it should only apply to prospective valuation of described prospects and not to retrospective valuation of obtained outcomes.&quot;</p><p>I fundamentally disagree on this point. There are various opinions on where normalization is &quot;supposed to happen.&quot; Multiple papers have conceptualized DN as a process that occurs during decision and not learning (the citations were provided in the previous round, including some that I feel qualified to interpret). Louie et al., 2022 is the first paper that I am aware of to apply it in learning settings (though an argument can be made that Khaw, Louie, Glimcher (2017, PNAS) has elements of a learning task). That it took 15 years of empirical research to explicitly approach the learning setting should be evidence enough of how its proponents were conceptualizing it. The 2011 review paper that the authors interpret as making this claim simply notes that the matching law implies that &quot;the primary determinant of behaviour is the relative value of rewards.&quot; (pg 17). In the immediate sentence following they pose the question: &quot;Does the brain represent action values in absolute terms independent of the other available options, or in relative terms?&quot; to set up the remainder of the paper. The use of the term &quot;action values&quot; by Louie and Glimcher is critical. This is neuro-physiological terminology used to describe the motor/decision stage, where the values that have been learned (by the then-standard RL and RPE circuitry covered in pg 14-16) are used to make decisions (these are the Vs in the 2011 paper, or the Qs in the author's model). It is these Vs that are transformed at the time of decision. See also Webb, Louie, Glimcher (pg 4) for a clear statement of this. Nowhere in these articles is it claimed that the action values in learning settings (again, the Vs!) are formed in a relative manner (like in the authors' Equation 4, 5, and 9 for their Qs).</p><p>Of course, this point does not diminish the fact that the authors are testing for DN during the updating step in learning, with some compelling evidence. But we need to be clear on where the DN theory has been largely proposed by previous literature, and where the DN theory has been previously shown to capture behaviour (choice sets with more than two options). This is what I mean when I say it is a hypothesis the authors (or their previous reviewers) are proposing and evaluating in their papers. There are perfectly good reasons to examine DN in this setting (see the normative discussion below) and that should be sufficient to frame the question. I believe these issues need to be framed clearly by the introduction of the paper, rather than as an aside at the end of the Discussion.</p><p>line 61: This seems like an ideal spot to clearly state that nearly all empirical applications of DN apply it at the decision stage and address choices with more than two options. In particular, the study by Daviet and Webb implements a protocol similar to the design used here but in a &quot;descriptive&quot; rather than a learning setting (see comment below). The text could then note that &quot;Few recent studies have extended….&quot;</p><p>As a final point, if the design included three or more alternatives in their transfer phase, there is some reason to believe that behaviour would depart from the model the authors propose (i.e. we might start to see behaviour influenced by more options at the decision stage). Therefore, contrary to the author's claim, it would &quot;matter if they are tested in a binary way&quot;. Therefore the results should only be interpreted as applying to the learning of the valuations, not their use during decision. I agree with the changes to lines 141-145, however, the paper up to this point has still not stated that DN at the decision stage has no strong predictions in binary choice. The reader thus has no context on why that statement of line 141 is true. This needs to be clarified.</p></disp-quote><p>We thank the Reviewer for this interesting and scholarly arguments. In link with our previous comment, we want to emphasize that the 15 years necessary to conceptualize and operationalize DN in the context of value-based decision-making have been well-spent. The measure of this success is given by the fact that many other groups (including our own) picked up the same or similar research question, thus provoking a dialectical and constructive debate that will push us a step closer to having a satisfactory descriptive (and yet normatively ground) model of human decision-making.</p><p>Nonetheless, concerning the specific points, we are a bit puzzled because we agree with the Reviewer, buy we fail to see how the incriminated paragraph does not vehiculate these ideas. Around line 61ish, we believe it is clearly stated that</p><p>(1) most of the existing account of DN are framed within descriptive framework:</p><p>“Even though, to date, most of the empirical studies proposing divisive normalization as a valid model of economic value encoding proposed that option values are vehiculated by explicit features of the stimulus (such as food snacks or lotteries: so-called described options<sup>22,17,23</sup>),…”</p><p>(2) that only few studies applied it to the experience-based domain:</p><disp-quote content-type="editor-comment"><p>“…few recent studies have extended the framework to account for subjective valuation in the reinforcement learning (or experience-based) context<sup>24,25</sup>.”</p></disp-quote><p>(3) and, finally, what is the main computational modification entailed by the description experience translation.</p><p>“Adjusting the divisive normalization model to a reinforcement learning scenario is easily achieved by assuming that the normalization step occurs at the outcome stage, i.e., when the participant is presented with the obtained (and forgone) outcomes.”</p><p>Similarly, around line 140ish is also clear that normalization is applied to outcome evaluation:</p><p>“Crucially, even if the transfer phase involves only binary choices, it can still tease apart the normalization rules affecting outcome valuation during the learning phase. This is because transfer choices are made based on the memory of values acquired during the learning phase, where we purposely manipulated the number of options and their ranges of values, in order to create learning contexts that allow to confidently discriminate between the two normalization accounts, in the reinforcement learning context.”</p><p>(of note, we just added “in the reinforcement learning context” to make our domain of application even clearer).</p><p>We defer to the Editors’ guidance to find better formulation, if needed, but we maintain that our chain of reasoning is not misleading (and actually quite on line with the views proposed by the Reviewer).</p><disp-quote content-type="editor-comment"><p>2) I am re-stating here my comment from Round 1: Previous studies add alternatives to a choice set and study value normalization. They are important distinctions from this work because they test for choice set size effects explicitly. Exp 2 in Webb et al. 2020 does such an experiment (albeit not in a learning setting) and finds support for DN rather than RN. The key difference is that the additional alternatives are added to the choice set, rather than only in a learning phase. Daviet and Webb also report a double decoy experiment of description-based choice, which follows a similar intuition to this experiment by placing an alternative &quot;inside&quot; two existing alternatives (though again, directly in the choice set rather than a learning phase). The conclusions from that paper are also in favour of DN rather than RN since this interior alternative appears to alter choice behaviour. This latter paper is particularly interesting because it relies on the same intuition for experimental design as in the experiment reported here. The difference is just the learning vs. choice setting, suggesting this distinction is critical for observing predictions consistent with RN vs DN.</p><p>R2.5 We thank the Reviewer for pointing another important reference that we originally missed. The suggested paper is now cited in the relevant part of the discussion (which is copied in R2.4 above).</p><p>I'm not sure that the authors interpreted my comment correctly. I don't understand citation #64 in the Discussion or what that paper has to do with risk. I raise the Daviet and Webb study because it implemented a similar design intuition as this study (inserting an option between two options so as not to alter the range), except with &quot;described&quot; alternatives and not learned outcomes. The conclusions in this paper differed from those reported. The manuscript should clearly state this similarity (and important difference in conclusion) between the two as context for the paradigm the authors are using.</p></disp-quote><p>We thank the Reviewer for clarifying this point. For the sake of completeness (even if it is outside our original scope), we did test different forms of normalization during the learning phase where a contrast between two and three options is possible as in Daviet and Webb. In these simulations the Q-values are learned in an absolute (or unbiased) scale but they are normalized before being fed to the softmax decision function (NB: previous studies we called a similar model the “POLICY model” because the normalization occurs at the decision, not the update; see Bavard et al. 2018; 2021).</p><fig id="sa2fig6" position="float"><label>Author response image 6.</label><caption><title>Participants’ accuracy (colored rectangles) and model simulations (black dots) in each condition of the learning phase.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig6-v2.tif"/></fig><p>First, we note that all models predict (to some extent) lower accuracy in the 3 versus 2 options decision contexts: the effect is inherent to the softmax decision rule (and indicate that this effect, in absence of the appropriate controls, is only weakly diagnostic of DN). Second, and more relevant for our discussion here, while no model is perfect, it looks like that the DN model produces the simulations that diverge the most from the actual data. This indicates that participants’ behavior (during the learning phase) was not significantly affected by the number of options, beyond what is “mechanically” predicted by the softmax rule. Therefore, it seems that, even in this phase, their behavior was not consistent with a DN process (contrary to what was observed by Daviet and Webb with similar manipulation). These analyses were not planned (and we know in advance that these models are not fully plausible: they could not account for transfer phase preferences!), yet, especially since <italic>eLife</italic>’s rebuttal letters are published, we are happy to leave this information here for future readers questioning the effect of the number of options during the learning phase in our task.</p><disp-quote content-type="editor-comment"><p>3) In Discussion:</p><p>a) lines 361-371: This paragraph argues that &quot;We find no evidence for such a [choice set size] effect&quot;. As noted in my previous report (and again above), this study is not examining choice set size effects. The size of the choice set in the transfer phase is always binary. Instead, the study is examining the relative coding of learning. This paragraph needs to be re-written so that this is clearly stated. Referring to the results from the transfer task as a &quot;trinary decision context&quot; is confusing.</p></disp-quote><p>We rephrased this paragraph to make the description-experience distinction (i.e., the different domain of application of the two models) even clearer:</p><p>“Our demonstration relied on the straightforward and well accepted idea that virtually any instantiation of the divisive normalization model would predict a strong choice (description-based task) or outcome (reinforcement-based task) set size effect: all things being equal, the subjective value of an option or an outcome in a trinary decision context should be lower compared that of a similar value belonging to a binary context. We find no evidence for such an effect. In fact, if anything, the subjective values of options belonging to trinary decision contexts were numerically higher compared to that of the binary decision contexts.”</p><disp-quote content-type="editor-comment"><p>b) &quot;However, it is worth noting that evidence concerning previous reports of divisive normalization in humans has been recently challenged and alternatively accounts, such as range normalization, have not been properly tested in these datasets 29,65.&quot;</p><p>The test for Range normalization in the original Louie et al. dataset is in Webb, Louie andGlimcher 2020, Mng Sci and is rejected in favour of DN. It is not clear what the authors mean by &quot;properly.&quot;</p></disp-quote><p>Good point, we replace “properly” by “systematically” and rephrased the sentence:</p><p>“However, it is worth noting that evidence concerning previous reports of divisive normalization in humans have been recently challenged and alternative accounts, such as range normalization, have not been systematically tested in these datasets<sup>30,65</sup>, although see <sup>19</sup> for an exception. »</p><disp-quote content-type="editor-comment"><p>4) R2.3b In the ex-ante simulations, we randomly sampled both of these parameters from Β(1.1,1.1) distributions, which inherently leads to different predictions in the transfer phase for the binary and trinary contexts, because the choice rate impacts the update of each option. The fact that small differences derive from allowing different learning rates for obtained and forgone outcomes, is confirmed by simulations. We nonetheless kept the original simulations, because they correspond better to the models that we simulated (with two learning rates). The authors are arguing that there is a systematic difference in the predictions for Range WT_86 WB 86 when using two learning rates. This is not an obvious result, but the increase to 500 simulations seems to confirm that it is systematic. This should be clarified in the text. Otherwise, it will be confusing to readers why the predictions change.</p></disp-quote><p>We have a different perspective than the Reviewer on this point. The difference is barely noticeable (0.035 relative increase) and we are afraid that adding a lengthily and technical explanation of this effect at this stage of the paper in this journal might distract the reader from the main points and results rather than be useful. Nonetheless, the nerdier readers will have a chance to refer to the rebuttal letters (as well as all our codes and data, openly accessible online) for clarification.</p><fig id="sa2fig7" position="float"><label>Author response image 7.</label><caption><title>For the sake of completeness we show again here the two-learning rates-induced discrepancy between the preferences in N86/N50 and W86/W50.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83891-sa2-fig7-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>5) From Round 1: line 415 – what is meant by &quot;computational advantage&quot; here? The rational inattention literature, which provides the normative foundation of efficient encoding, places emphasis on the frequency of stimuli (which is completely ignored by range normalization) (e.g. Heng, Woodford, Polania, eLife).</p><p>Reply: &quot;R2.18 Range and Divisive normalization both provide, to some extent, the computational advantage of being a form of adaptive coding. If the topic interests the Reviewer, we could point to a recent study where their normative status is discussed: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4320894&quot;</p><p>The normative argument for relative coding (including both DN and RN) that the authors point to is specific to a learning context (via the exploration/exploitation tradeoff). And even then, it is not an argument that an efficient code should use only the range. Thus it is important to keep in mind that the results reported in the current manuscript may be somewhat specific to the learning setting implemented in the experimental design. Multiple theoretical and empirical papers from across neuroscience and economics have examined the form of normative computation in general settings (Heng et al., cited above; Steverson et al. cited in previous report; Netzer 2009 AER; Robson and Whitehead, Schaffner et al., 2023, NHB) and all have shown that the efficient code adapts to the full distribution of stimuli. It is reasonable to imagine that some physical systems might approximate this with a piecewise linear algorithm based on the range, but this has not been observed in the nervous system (V1, A1, LIP etc.). Of course this can't rule out the possibility that V1, A1 and LIP get it right with a standard cortical computation, but for some reason other areas have to resort to a cruder approximation. But that isn't the obvious hypothesis. Tracking the minimum and maximum – including identifying which is which – seems both unnecessary and complicated in real-world scenarios. Whether (and how much) the results reported in the paper generalize outside this experimental design is an open question.</p></disp-quote><p>Across two rounds of reviewing, the Reviewer has spent a lot of justified effort to push us to better clarify that we are working in reinforcement learning framework. As a consequence, we believe it should seem rather appropriate that, when referring to normative advantage, we focus on applications in reinforcement learning.</p><p>Concerning the very last point:</p><p>“Tracking the minimum and maximum – including identifying which is which – seems both unnecessary and complicated in real-world scenarios. Whether (and how much) the results reported in the paper generalize outside this experimental design is an open question.”</p><p>While our position differs from the first part (our intuition is that there is nothing easier and more natural than tracking the max and the min of any contextual variable), we do agree with the second part (generalization in the field is an open question).</p></body></sub-article></article>