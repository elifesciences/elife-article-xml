<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93242</article-id><article-id pub-id-type="doi">10.7554/eLife.93242</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93242.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Microbiology and Infectious Disease</subject></subj-group></article-categories><title-group><article-title>An antimicrobial drug recommender system using MALDI-TOF MS and dual-branch neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>De Waele</surname><given-names>Gaetan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0367-9699</contrib-id><email>gaetan.dewaele@ugent.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Waegeman</surname><given-names>Willem</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Menschaert</surname><given-names>Gerben</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cv9y106</institution-id><institution>Department of Data Analysis and Mathematical Modelling, Ghent University</institution></institution-wrap><addr-line><named-content content-type="city">Ghent</named-content></addr-line><country>Belgium</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Folkman</surname><given-names>Lukas</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02sc3r913</institution-id><institution>Griffith University</institution></institution-wrap><country>Australia</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Garrett</surname><given-names>Wendy S</given-names></name><role>Senior Editor</role><aff><institution>Harvard T.H. Chan School of Public Health</institution><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP93242</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-12-14"><day>14</day><month>12</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-10-10"><day>10</day><month>10</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.09.28.559916"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-01"><day>01</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93242.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-03"><day>03</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93242.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-03"><day>03</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93242.3"/></event></pub-history><permissions><copyright-statement>© 2024, De Waele et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>De Waele et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93242-v1.pdf"/><abstract><p>Timely and effective use of antimicrobial drugs can improve patient outcomes, as well as help safeguard against resistance development. Matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) is currently routinely used in clinical diagnostics for rapid species identification. Mining additional data from said spectra in the form of antimicrobial resistance (AMR) profiles is, therefore, highly promising. Such AMR profiles could serve as a drop-in solution for drastically improving treatment efficiency, effectiveness, and costs. This study endeavors to develop the first machine learning models capable of predicting AMR profiles for the whole repertoire of species and drugs encountered in clinical microbiology. The resulting models can be interpreted as drug recommender systems for infectious diseases. We find that our dual-branch method delivers considerably higher performance compared to previous approaches. In addition, experiments show that the models can be efficiently fine-tuned to data from other clinical laboratories. MALDI-TOF-based AMR recommender systems can, hence, greatly extend the value of MALDI-TOF MS for clinical diagnostics. All code supporting this study is distributed on PyPI and is packaged at https://github.com/gdewael/maldi-nn.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>recommender systems</kwd><kwd>MALDI-TOF MS</kwd><kwd>neural networks</kwd><kwd>antimicrobial resistance</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003130</institution-id><institution>Fonds Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>1153024N</award-id><principal-award-recipient><name><surname>De Waele</surname><given-names>Gaetan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011878</institution-id><institution>Vlaamse regering</institution></institution-wrap></funding-source><award-id>Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen</award-id><principal-award-recipient><name><surname>Waegeman</surname><given-names>Willem</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>It is possible to accurately recommend antimicrobial drugs from mass spectra using neural networks and large datasets, paving the way for more efficient clinical diagnostics.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In diagnostic laboratories, matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) is routinely used for microbial species identification (<xref ref-type="bibr" rid="bib19">Hou et al., 2019</xref>). Usually, microbial samples only require an overnight culturing step before being analyzed with mass spectrometry (<xref ref-type="bibr" rid="bib44">van Veen et al., 2010</xref>; <xref ref-type="bibr" rid="bib10">Cuénod et al., 2021</xref>). Consequently, the technology provides a time- and cost-efficient way to accurately identify the pathogen underlying an infection.</p><p>Due to the rapid evolution of antibiotic-resistant strains, it is increasingly difficult to determine a treatment based on only species identity. It has been estimated that infections caused by antibiotic-resistant bacteria have caused the deaths of 1.27 million people in 2019, making antimicrobial resistance (AMR) one of the leading causes of death on earth (<xref ref-type="bibr" rid="bib34">Murray et al., 2022</xref>). Projections have estimated that this annual number could rise to 10 million by 2050 (<xref ref-type="bibr" rid="bib36">O’Neill, 2016</xref>), highlighting the need for responsible antimicrobial drug use. In light of this, diagnostic laboratories will often perform various tests, such as dilution arrays or disc diffusion tests, to probe which drug will be effective (<xref ref-type="bibr" rid="bib24">Khan et al., 2019</xref>). Such experiments typically require further culturing and are either costly, labor-intensive, time-intensive, or a mixture of the above (<xref ref-type="bibr" rid="bib21">Humphries, 2022</xref>).</p><p>Given that MALDI-TOF spectra are already routinely used for identification, it is worth investigating to which extent they can contain further information regarding the resistance status of strains (<xref ref-type="bibr" rid="bib53">Weis et al., 2020a</xref>). Mining this information from the spectra could help inform healthcare workers of candidate drugs. This may nullify the need for phenotypical experiments, or (at least) direct the tests by narrowing down the choices. Furthermore, possessing a detailed resistance profile allows us to treat with more specifically working drugs (instead of broad-spectrum antibiotics) (<xref ref-type="bibr" rid="bib55">Weis et al., 2022</xref>). Consequently, predicting resistance status from MALDI-TOF spectra could help toward the goals of antibiotic stewardship (<xref ref-type="bibr" rid="bib41">Shlaes et al., 1997</xref>).</p><p>It has been described that some known resistance mechanisms are outside of the m/z range that MALDI-TOF spectrometers can accurately measure (<xref ref-type="bibr" rid="bib21">Humphries, 2022</xref>). Still, it remains largely unknown to which extent co-evolved traits, such as subtle changes in metabolism caused by the resistance mechanism, can be detected by MALDI-TOF spectra. A number of studies have shown that some resistant strains can reliably be predicted from MALDI-TOF MS, either by identifying and detecting specific markers (e.g., peaks) or by learning patterns from data (see ‘Related work’). To our knowledge, all of these studies have modeled AMR prediction for specific species–drug combinations. For this reason, they learn very specific markers of resistance, not guaranteed to extrapolate well to other drugs and species. As susceptibility rapidly evolves, it is practically impossible to perform such studies for all clinically relevant species–drug combinations. As such, the value of aforementioned studies remains of exploratory nature with limited practical value. In addition, their performance remains limited owing to small sample sizes and, likely, the inability of MALDI-TOF spectra to fully discriminate between the characteristics of interest (<xref ref-type="bibr" rid="bib2">Bai et al., 2017</xref>). The recently published DRIAMS dataset (<xref ref-type="bibr" rid="bib55">Weis et al., 2022</xref>) contains phenotypic AMR data covering a wide range of species and drugs, allowing to study MALDI-TOF-based AMR prediction on an unprecedented scale.</p><p>We posit that the most pertinent challenge healthcare workers face regarding AMR is to choose between all possible drugs given an infection, not whether one specific drug will be effective or not. For this reason, we argue that our models and evaluation metrics should be designed to optimally answer that question. In this study, a recommender model is proposed that can predict AMR for the whole range of pathogens and drugs encountered in clinical microbiology. In addition, species-specific recommender models for a range of common species are also trained. Our method jointly learns representations for antibiotic drugs and bacterial MALDI-TOF spectra. It can be used to recommend the most likely drug to work for any drug–spectrum combination. Consequently, the model is broadly applicable and practical to use. To summarize, our contributions are as follows:</p><list list-type="order"><list-item><p>We formulate a dual-branch neural network recommender system for the prediction of AMR profiles. The model operates on MALDI-TOF spectra, as well as a representation of the candidate drug.</p></list-item><list-item><p>We evaluate multiple state-of-the-art techniques for representing drug identity in the model.</p></list-item><list-item><p>We compare ‘general’ recommenders (trained on all spectra from all species) against species-specific recommender models</p></list-item><list-item><p>We perform evaluations by comparing our methods to non-recommender system baselines.</p></list-item><list-item><p>We show that the model efficiently transfers to data from diagnostic laboratories it was not trained on. Making the model easy to adopt for hospitals lacking the means and/or volume to collect large data.</p></list-item></list><sec id="s1-1"><title>Related work</title><sec id="s1-1-1"><title>MALDI-TOF-based machine learning</title><p>The most canonical task for MALDI-TOF-based methods is species identification. Identification solutions are usually provided by the MS manufacturers and are built on large, proprietary, in-house databases (<xref ref-type="bibr" rid="bib43">van Belkum et al., 2012</xref>). It is unclear how these closed-source identification pipelines work, but it is likely that query spectra are directly compared to the in-house database in an approach akin to nearest neighbors (<xref ref-type="bibr" rid="bib11">Dauwalder et al., 2023</xref>). While this approach works excellently for identification of most species, some strains remain problematic (<xref ref-type="bibr" rid="bib4">Cao et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Vrioni et al., 2018</xref>). Furthermore, by presumably focusing on the presence or absence of specific peaks, a lot of spectral information stands unused (<xref ref-type="bibr" rid="bib15">Florio et al., 2018</xref>).</p><p>For various difficult prediction cases, such as strain typing, researchers often resort to machine learning (<xref ref-type="bibr" rid="bib18">Hettick et al., 2006</xref>; <xref ref-type="bibr" rid="bib51">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">De Bruyne et al., 2011</xref>). Stifled by a historical lack of large open data, machine learning research on MALDI-TOF data remains in its infancy. Most studies have narrow scopes and simple datasets (e.g., binary classification), only warranting standard preprocessing and off-the-shelf learning techniques (<xref ref-type="bibr" rid="bib58">Yu et al., 2022</xref>; <xref ref-type="bibr" rid="bib59">Zhang et al., 2023</xref>; <xref ref-type="bibr" rid="bib9">Chung et al., 2023</xref>). Only a handful of examples exist of more advanced learning techniques specifically adapted to a MALDI-TOF-based task (<xref ref-type="bibr" rid="bib33">Mortier et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Weis et al., 2020a</xref>; <xref ref-type="bibr" rid="bib46">Vervier et al., 2015</xref>). For a more thorough overview of MALDI-TOF-based machine learning, readers are referred to the review of <xref ref-type="bibr" rid="bib54">Weis et al., 2020b</xref>.</p><p>During peer review, our attention was brought to a similar concurrent study by <xref ref-type="bibr" rid="bib48">Visonà et al., 2023</xref>. Their study similarly shows that recommender systems-like models outperform more narrowly trained single-species and single-drug models. Their analysis, however, remains limited to fingerprint-based molecular representations. In addition, in this work, we demonstrate transfer learning between hospitals.</p></sec><sec id="s1-1-2"><title>Dual-branch neural networks</title><p>The idea of processing and combining two separate streams of information with two neural networks is applied in many fields of machine learning, collectively referred to as deep multitarget prediction (<xref ref-type="bibr" rid="bib50">Waegeman et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Iliadis et al., 2022</xref>).</p><p>In collaborative filtering, the goal is to predict the preference of a user to items (<xref ref-type="bibr" rid="bib16">He et al., 2017</xref>). In its most elementary neural form, both users and items are represented by one-hot encodings, generating a model unable to make salient predictions for new users or items without having seen them during training. To solve this, a body of works exists on trying to communicate user- and item identity to the model via side information encoded in features (<xref ref-type="bibr" rid="bib60">Zheng et al., 2017</xref>).</p><p>Dual-branch neural networks are also prevalent in language and vision. Recent advances in (multi-modal) contrastive learning of image (and text) representations often rely on two neural encoders to learn a matching score between two views of the same or discordant objects (<xref ref-type="bibr" rid="bib38">Radford et al., 2021</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2020</xref>). Language retrieval systems typically compare input vectors with a database of key vectors, each derived from a neural network, using approximate nearest-neighbor search techniques (<xref ref-type="bibr" rid="bib23">Karpukhin et al., 2020</xref>). In biology, fields of research employing dual-branch neural networks include (1) drug–target interaction (<xref ref-type="bibr" rid="bib30">Lee et al., 2019</xref>), (2) single-cell multi-omics analysis (<xref ref-type="bibr" rid="bib28">Lance et al., 2022</xref>), and (3) transcription factor binding prediction (<xref ref-type="bibr" rid="bib56">Yang et al., 2020</xref>), among countless others.</p><p>Most of these applications can, to varying extents, be interpreted as (collaborative filtering) recommender systems. For example, contrastive language-image models have been used to retrieve the most semantically similar images to a piece of text (<xref ref-type="bibr" rid="bib3">Beaumont, 2022</xref>).</p></sec></sec></sec><sec id="s2" sec-type="methods"><title>Methods</title><sec id="s2-1"><title>Data</title><p>To train models, we use the recently published DRIAMS database, consisting of 765,048 AMR measurements derived from 55,773 spectra across four different hospitals, spanning in total 74 different drugs (<xref ref-type="bibr" rid="bib55">Weis et al., 2022</xref>). (These figures reflect the size of the dataset as downloaded from the original Dryad repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.bzkh1899q">https://doi.org/10.5061/dryad.bzkh1899q</ext-link>, and after processing. For example, the number of spectra listed here corresponds to all spectra in DRIAMS for which there exists at least one AMR measurement. The total number of spectra in DRIAMS counts 250,070, but no labels are associated with these extra spectra. Further, the naming of drugs was further preprocessed such that every drug can be linked to a single chemical identifier. For more information on which drugs were merged and how this was performed, see Appendix 1.) Every drug is characterized by a canonical SMILES string obtained from PubChem (<xref ref-type="bibr" rid="bib25">Kim et al., 2023</xref>). As in the original DRIAMS publication, AMR measurements are binarized according to the EUCAST norms per drug. Specifically, intermediate or resistant values are assigned a positive label, and susceptible samples a negative one. Furthermore, spectra are identically processed as in the original publication. Briefly, the following steps are performed: (1) square-root transformation of the intensities, (2) smoothing using a Savitzky–Golay filter with half-window size of 10, (3) baseline correction using 20 iterations of the SNIP algorithm, (4) trimming to the 2000–20,000 Da range, (5) intensity calibration so that the total intensity sums to 1, and (6) binning the intensities by summing all values in intervals of 3 Da. After preprocessing, every spectrum is represented as a 6000-dimensional vector.</p><p>The main experiments concern models that are trained on data from one hospital only (DRIAMS-A, University Hospital Basel). All spectra and measurements derived from the other three hospitals in DRIAMS are left out for transfer learning experiments (see Results). Within DRIAMS-A, all spectra from before 2018 are allocated to the training set, and all spectra measured during 2018 are evenly split between validation and test set. This split in time reflects a realistic evaluation scenario, as models trained on historical data need to generalize to new patients possibly infected by newly evolved strains. The final sizes of all splits are as follows: 409,395 labels across 28,331 spectra for the training set, 76,431 labels across 4994 spectra for the validation set, and 76,133 labels across 4999 spectra for the test set.</p></sec><sec id="s2-2"><title>Metrics</title><p>The main objective of this study is to train models to effectively recommend treatments for patients. Hence, unless otherwise noted, metrics are computed on a per-patient basis, and then averaged. This is equivalent to macro-averaged metrics, but then computed per instance (spectrum), instead of per class (drug) (<xref ref-type="bibr" rid="bib50">Waegeman et al., 2019</xref>). For simplicity, we omit the ‘macro’ prefix from metrics, and – unless otherwise indicated – always use spectrum-macro metrics.</p><p>The area under the receiver operating characteristic curve (ROC-AUC) measures the probability that any positive (resistant or intermediate) sample is assigned a higher predicted probability of being positive as compared to any negative (susceptible) sample. It is a measure of the average quality of the ranking of suggested drugs to a patient. To compute the (per-patient average) ROC-AUC, for any spectrum/patient, all observed drug resistance labels and their corresponding predictions are gathered. Then, the patient-specific ROC-AUC is computed on that subset of labels and predictions. Finally, all ROC-AUCs per patient are averaged to a ‘spectrum-macro’ ROC-AUC.</p><p>The Precision at 1 of the negative class (Prec@1(-)) evaluates how often the top-ranked prediction is correct. Hence, in this case, it reports the proportion of cases for which the ‘most likely susceptible drug’ prediction is actually an effective one. In a scenario where the top recommended drug is always administered, it corresponds to the percentage of correctly suggested treatments.</p></sec><sec id="s2-3"><title>Model architecture</title><p>We formulate AMR prediction as a multitarget classification problem with side information for both instances and targets, also referred to as dyadic prediction (<xref ref-type="bibr" rid="bib50">Waegeman et al., 2019</xref>). In this context, let us denote a sample in the dataset <inline-formula><mml:math id="inf1"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:math></inline-formula> by a triplet <inline-formula><mml:math id="inf2"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒔</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>𝒅</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the resistance label of a microbial spectrum <inline-formula><mml:math id="inf4"><mml:msub><mml:mi>𝒔</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>n</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">}</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> w.r.t. a drug <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>𝒅</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>m</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">}</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>. This dataset can be arranged in an incomplete score matrix <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>𝒀</mml:mi><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>0,1</mml:mn><mml:msup><mml:mo form="postfix" stretchy="false">}</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. In what follows, the final architectural set-ups used to present the results are described. For details on hyperparameter tuning, readers are referred to Appendix 2.</p><p>The model consists of two separate neural network embedders <inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">⋅</mml:mo><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">⋅</mml:mo><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for processing the spectra and drugs, respectively. The resulting instance and target embeddings <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> are then combined into a single score by their scaled dot product <inline-formula><mml:math id="inf11"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mi>h</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib39">Rendle et al., 2020</xref>). The scaling factor <inline-formula><mml:math id="inf12"><mml:msqrt><mml:mi>h</mml:mi></mml:msqrt></mml:math></inline-formula>, with <inline-formula><mml:math id="inf13"><mml:mi>h</mml:mi></mml:math></inline-formula> the dimensionality of both embeddings, is inspired by the formulation of self-attention (<xref ref-type="bibr" rid="bib45">Vaswani et al., 2017</xref>). It ensures the dot products to be of manageable magnitudes, even for large values of <inline-formula><mml:math id="inf14"><mml:mi>h</mml:mi></mml:math></inline-formula>. This score can be used together with the sigmoid function and the cross-entropy loss to optimize the two-branch neural network to map a spectrum–drug pair to a resistance label (<xref ref-type="bibr" rid="bib22">Iliadis et al., 2022</xref>). An overview of the model is visualized in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Architectural overview of the proposed model.</title><p>Antimicrobial resistance (AMR) labels of spectrum–drug pairs can be represented in an incomplete matrix. A microbial sample that is susceptible to a drug is denoted by a negative label (orange), whereas positive labels (blue) signify an intermediate or resistant combination. Instance (spectrum) and target (drug) embeddings <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> are obtained from their respective input representations passed through their respective neural network branch. The two resulting embeddings are aggregated to a single score by their (scaled) dot product. The cross-entropy loss optimizes this score to be maximal or minimal for positive or negative combinations of microbial spectra and drugs, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-fig1-v1.tif"/></fig><p>The representations of the instance vectors <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are extracted from a neural network <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> operating on the processed and binned MALDI-TOF spectra <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>𝒔</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is parameterized by a multi-layer perceptron (MLP), consisting of a series of fully connected layers. Between every two such layers, a series of operations consisting of (1) a GeLU activation (<xref ref-type="bibr" rid="bib17">Hendrycks and Gimpel, 2016</xref>), (2) a dropout rate of 0.2 (<xref ref-type="bibr" rid="bib42">Srivastava et al., 2014</xref>), and (3) layer normalization (<xref ref-type="bibr" rid="bib1">Ba et al., 2016</xref>) is applied. We include multiple model sizes in our final results (<xref ref-type="table" rid="table1">Table 1</xref>). To make comparisons easier, all models output the same number of hidden dimensions that are used in the dot product, <inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>All tested model sizes for the (instance) spectrum branch.</title><p>Hidden sizes represent the evolution of the hidden state dimensionality as it goes through the model, with every hyphen defining one fully connected layer. The listed number of parameters only includes those of the instance (spectrum) branch.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Size</th><th align="left" valign="bottom"># weights</th><th align="left" valign="bottom">Hidden sizes</th></tr></thead><tbody><tr><td align="left" valign="bottom">S</td><td align="left" valign="bottom">1,578,176</td><td align="char" char="hyphen" valign="bottom">6000-256-128-64</td></tr><tr><td align="left" valign="bottom">M</td><td align="left" valign="bottom">3,246,784</td><td align="char" char="hyphen" valign="bottom">6000-512-256-128-64</td></tr><tr><td align="left" valign="bottom">L</td><td align="left" valign="bottom">6,846,144</td><td align="char" char="hyphen" valign="bottom">6000-1024-512-256-128-64</td></tr><tr><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">15,093,440</td><td align="char" char="hyphen" valign="bottom">6000-2048-1024-512-256-128-64</td></tr></tbody></table></table-wrap><p>Drug identity can be communicated to the model in a number of ways. In this work, we study the following different input representations <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>𝒅</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and embedder <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">⋅</mml:mo><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> combinations:</p><list list-type="order"><list-item><p>As indices in a one-hot encoding paired with a single linear layer.</p></list-item><list-item><p>As Extended Connectivity Fingerprints paired with a single linear layer.</p></list-item><list-item><p>As DeepSMILES strings (<xref ref-type="bibr" rid="bib35">O’Boyle and Dalke, 2018</xref>) paired with a 1D convolutional neural network (CNN).</p></list-item><list-item><p>As DeepSMILES strings paired with a gated recurrent unit neural network (GRU).</p></list-item><list-item><p>As DeepSMILES strings paired with a transformer neural network.</p></list-item><list-item><p>As images paired with a 2D CNN.</p></list-item><list-item><p>As rows of a pre-computed string kernel on the SMILES strings (LINGO <xref ref-type="bibr" rid="bib47">Vidal et al., 2005</xref>), paired with a single linear layer.</p></list-item></list><p>For all these combinations, the embedder outputs target embeddings <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. For more details on the different drug embedders and their hyperparameters (as well as their tuning), see Appendix 2. For every combination of spectrum embedder (four sizes: S, M, L, and XL) and drug embedder (seven types), six different learning rates (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>3</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) are tested. For all these different combinations, five models are trained (using different random seeds for model initialization and batching of data). For every spectrum and drug embedder combination, only results from the best learning rate are presented; that is, the learning rate resulting in the best average validation ROC-AUC for that combination.</p><p>All models are trained with the Adam optimizer (<xref ref-type="bibr" rid="bib26">Kingma and Ba, 2014</xref>) for a maximum of 50 epochs with a batch size of 128. A linear learning rate warm-up over the first 250 steps is applied, after which the rate is kept constant. As every epoch constitutes one pass over every label and, hence, multiple passes over every individual drug and spectrum, a branch can technically already be overfitting before the end of the first epoch. Because of this, performance on the validation set is checked every tenth of an epoch. Training is halted early when validation ROC-AUC has not improved for 10 validation set checks. The checkpoint of the best performing model (in terms of validation ROC-AUC) is used as the final model.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>The following section will first relay the results of the different dual-branch model configurations. Afterward, the ‘general’ AMR recommender is matched up against ‘species-specific’ and ‘species–drug-specific’ models. Finally, the models’ capabilities and representations are examined through transfer learning and embeddings.</p><sec id="s3-1"><title>Encoding species and drugs effectively</title><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows the performance of all trained models in terms of their average ROC-AUC and Prec@1(-). It can be seen that, in general, performance differences between model configurations occupy a small margin. However, trends can still be found. Models using Morgan fingerprints typically outperform other drug embedding strategies. Morgan fingerprints provide a compressed and preprocessed input format, the nature of which provides an apparent advantage over input representations that require more pattern extraction. The small number of different antimicrobial drugs may not be conducive to learning complex representations. Indeed, embedding drugs without a compound information (i.e., one-hot embedding) is a competitive approach for this problem, resulting in the – on average – second best models in terms of ROC-AUC. On the spectrum embedder side, it is observed that the medium or large variants typically perform best. The full ROC curve (showing sensitivity and specificity) for the best-performing model is shown in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Barplots showing test performance results for all trained models.</title><p>Area under the receiver operating characteristic curve (ROC-AUC) evaluates overall ranking of predictions. Prec@1(-) evaluates how often the top suggested treatment would be effective. Both metrics are calculated per spectrum/patient and then averaged. Errorbars represent the standard deviation over five random model seeds. The x-axis and colors show the different drug and spectrum embedders, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-fig2-v1.tif"/></fig><p>Performance in terms of Macro ROC-AUC can be found in <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2</xref>. The Macro ROC-AUC averages the ROC-AUC for every individual drug. Here, Morgan fingerprints similarly reach the best performances. The full list of performances can be found in <xref ref-type="table" rid="app3table1">Appendix 3—table 1</xref>.</p><p>In <xref ref-type="fig" rid="app3fig3">Appendix 3—figure 3</xref>, the performance of the spectrum embedder sizes is compared against a linear baseline. The linear baseline uses the same preprocessed input spectrum representation, but only uses a single linear combination to produce an embedding. For this comparison, only the Morgan fingerprint drug embedders are used as they produce the best-performing models overall. Models using nonlinear multi-layer spectrum embedders obtain considerably better performance over linear embedders.</p></sec><sec id="s3-2"><title>Species-specific models improve recommendation</title><p>The recommender systems presented in the previous section provide an incredibly general tool. Trained as single models for all species and drugs, their versatility is unparalleled compared to previous studies that create classifiers for specific drug–species combinations (<xref ref-type="bibr" rid="bib54">Weis et al., 2020b</xref>). In between the extremes of ‘one model for everything’ and ‘a model per species and per drug’, there lies a compromising approach: a species-specific recommender system for all drugs. Such recommender systems would be more specialized in nature, but their usefulness hinges upon having done prior species identification. As these are typically included in the MS’ manufacturer’s software, a more specialized species-specific recommender may provide better performance without incurring extra cost. The disadvantage of such models is that (1) they cannot be used for species for which there is not enough data to train a separate model (i.e., rarely occurring species), and (2) they rely on the prior identification step to be correct.</p><p>Here, we create species-specific recommender models for the 25 most occurring species in DRIAMS-A. The training setup for these models is kept the same as in the previous section. The difference between ‘general’ recommenders and ‘species-specific recommenders’ is that each species-specific recommender model is only trained on the subset of data covering their respective species (as these models use a smaller training set, validation is checked every fourth of an epoch instead of every tenth). Together, the test predictions of the 25 species-specific recommenders cover 4229 spectra, 56 drugs, and 69,827 AMR labels (covering 91.27% of the original test set). <xref ref-type="table" rid="table2">Table 2</xref> compares the two best ‘general’ recommenders from the previous section to their species-specific recommender counterparts. It is observed that species-specific recommenders deliver better predictions across all evaluated metrics.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Test performance of selected general and species-specific dual branch recommender models.</title><p>The listed averages and standard deviations are calculated over five independent runs of the same model. Performance is computed on the subset of labels spanning the 25 most common species in DRIAMS-A.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model</th><th align="left" valign="top">ROC-AUC</th><th align="left" valign="top">Prec@1(-)</th><th align="left" valign="top">Macro ROC-AUC</th></tr></thead><tbody><tr><td align="left" valign="top">General recommender (Morgan fingerprints – M)</td><td align="char" char="plusmn" valign="top">0.9411 ± 0.0007</td><td align="char" char="plusmn" valign="top">0.9967 ± 0.0011</td><td align="char" char="plusmn" valign="top">0.7684 ± 0.0050</td></tr><tr><td align="left" valign="top">General recommender (one-hot – L)</td><td align="char" char="plusmn" valign="top">0.9408 ± 0.0011</td><td align="char" char="plusmn" valign="top">0.9940 ± 0.0009</td><td align="char" char="plusmn" valign="top">0.7746 ± 0.0316</td></tr><tr><td align="left" valign="top">Species-specific recommenders (Morgan fingerprints – M)</td><td align="char" char="plusmn" valign="top">0.9461 ± 0.0010</td><td align="left" valign="top"><bold>0.9973 ± 0.0004</bold></td><td align="left" valign="top"><bold>0.7905 ± 0.0151</bold></td></tr><tr><td align="left" valign="top">Species-specific recommenders (one-hot – L)</td><td align="left" valign="top"><bold>0.9468 ± 0.0012</bold></td><td align="char" char="plusmn" valign="top">0.9950 ± 0.0011</td><td align="char" char="plusmn" valign="top">0.7686 ± 0.0155</td></tr></tbody></table><table-wrap-foot><fn><p>ROC-AUC, area under the receiver operating characteristic curve.</p></fn></table-wrap-foot></table-wrap><p>As opposed to the species-specific models, the ‘general’ recommender can use learned representations from one species to enhance predictions for other species, benefitting from multitask learning. The fact that this latter mode of learning performs worse on this problem, however, indicates that such transfer of learned knowledge is of limited usefulness for AMR prediction. Still, the ‘general’ recommender model remains useful in instances where the species could not be identified, or is rare. In <xref ref-type="table" rid="app3table2">Appendix 3—table 2</xref>, the 25 species for which specific recommenders were trained are listed, along with their performances.</p></sec><sec id="s3-3"><title>Dual-branch recommenders improve over baselines</title><p>In order to gain better insight into the performance of our models, in this section, both our ‘general’ and ‘species-specific’ recommenders are squared up against extensive baselines.</p><p>Previous studies have studied AMR prediction in specific species–drug combinations. For this reason, it is useful to compare how the dual-branch setup weighs up against training separate models for separate species and drugs. In <xref ref-type="bibr" rid="bib54">Weis et al., 2020b</xref>, for example, binary AMR classifiers are trained for the following three combinations: (1) <italic>Escherichia coli</italic> with ceftriaxone, (2) <italic>Klebsiella pneumoniae</italic> with ceftriaxone, and (3) <italic>Staphylococcus aureus</italic> with oxacillin. Here, such ‘species–drug-specific classifiers’ are trained for the 200 most common combinations of species and drugs in the training dataset. For these combinations, binary logistic regression, XGBoost (<xref ref-type="bibr" rid="bib6">Chen and Guestrin, 2016</xref>), and MLPs are tested. The tested MLPs come in the same four sizes as the spectrum branches of the dual-branch models. Other than having an output node of size 1 for binary classification, they share all hyperparameters with the tested spectrum branches. For details on the training and tuning procedure of all baselines, see Appendix 2.</p><p>There exist many species–drug combinations for which there are either only positive or only negative labels. As it is impossible to train and evaluate models for these cases, models are trained only for the 200 most occurring combinations for which both labels are present in the training, validation, and test set. We refer to these models as ‘species–drug classifiers’.</p><p>In addition, it is useful to probe model performance against what experts may be able to guess. Given knowledge of the species identity in question, an expert will – in many cases – already be able to make a good guess toward what drugs will be effective or not. Hence, baseline ‘best guess’ performance would not result in a ROC-AUC of 0.5. A way to simulate such ‘expert’s best guess’ baseline predictions is through counting label frequencies in the training set. More specifically, for a test label belonging to a certain species and drug, the labels in the training set corresponding to that drug and species can be gathered. The frequency by which that training set is positive or negative can be used to infer a test predicted probability. We refer to this baseline as ‘simulated expert’s best guess’. More formally, considering all training spectra as <inline-formula><mml:math id="inf26"><mml:msub><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi><mml:mtext>train</mml:mtext></mml:msub></mml:math></inline-formula>, all training labels corresponding to one drug <inline-formula><mml:math id="inf27"><mml:mi>j</mml:mi></mml:math></inline-formula> and species <inline-formula><mml:math id="inf28"><mml:mi>t</mml:mi></mml:math></inline-formula> are gathered: <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mtext>subset</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub><mml:mo>∧</mml:mo><mml:mtext>species</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The ‘simulated expert’s best guess’ predicted probability for any spectrum <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and drug <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, then, corresponds to the fraction of positive labels in their corresponding training label set <inline-formula><mml:math id="inf32"><mml:msubsup><mml:mi class="MJX-tex-caligraphic" mathvariant="script">Y</mml:mi><mml:mtext>subset</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>:<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mtext>species</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">Y</mml:mi></mml:mrow><mml:mtext>subset</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mtext>subset</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><xref ref-type="table" rid="table3">Table 3</xref> compares the recommenders from the previous section to non-recommender baselines. As the baselines are only trained on the 200 most common species–drug combinations, performance is computed on that subset of test labels. This reduced test set spans 4017 spectra, 35 drugs, and 53,503 labels (covering 70.28% of the original test set). Dual-branch recommenders outperform baselines on all but one metric. Logistic regression baselines result in the best average ROC-AUC for individual species–drug combinations. By all other metrics, dual-branch recommenders outshine a collection of species–drug-specific classifiers. It is illustrated that, when the question is to choose between drugs for a patient (evaluated by the patient-averaged ROC-AUC or Prec@1(-)), a model designed as a recommender will outperform binary classification models trained to predict AMR for specific drugs. On the other hand, species-specific binary classifiers are optimal for distinguishing spectra for a specific drug. The crux of our case in favor of recommender models relies, hence, on the fact that patient-averaged metrics are more representative of AMR models’ utility in clinical diagnostics.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Test performance of selected recommender models, compared to the performance of a collection of models – each trained on only one species–drug combination – coined ‘species–drug classifiers’.</title><p>‘Species–drug classifiers’ refer to a collection of binary classifiers, each trained to predict antimicrobial resistance (AMR) status for a subset of data comprising a single species–drug combination. ‘Simulated expert’s best guess’ refers to counting AMR label frequencies in single species–drug combinations and taking those as predictions. The listed averages and standard deviations are calculated over five independent runs of the same model. Given the non-stochastic nature of the logistic regression and XGBoost implementations, only one set of models is trained and, hence, no standard deviations are reported. Performance is computed on the subset of labels spanning the 200 most common species–drug combinations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model</th><th align="left" valign="top">ROC-AUC</th><th align="left" valign="top">Prec@1(-)</th><th align="left" valign="top">Macro ROC-AUC</th><th align="left" valign="top">Species–drug macro ROC-AUC</th></tr></thead><tbody><tr><td align="left" valign="top">Species-specific recommenders (Morgan fingerprints – M)</td><td align="char" char="plusmn" valign="top">0.9009 ± 0.0018</td><td align="left" valign="top"><bold>0.9830 ± 0.0015</bold></td><td align="left" valign="top"><bold>0.8283 ± 0.0059</bold></td><td align="char" char="plusmn" valign="top">0.6381 ± 0.0121</td></tr><tr><td align="left" valign="top">Species-specific recommenders (one-hot – L)</td><td align="left" valign="top"><bold>0.9030 ± 0.0018</bold></td><td align="char" char="plusmn" valign="top">0.9814 ± 0.0020</td><td align="char" char="plusmn" valign="top">0.8129 ± 0.0079</td><td align="char" char="plusmn" valign="top">0.6511 ± 0.0290</td></tr><tr><td align="left" valign="top">General recommender (Morgan fingerprints – M)</td><td align="char" char="plusmn" valign="top">0.8939 ± 0.0016</td><td align="char" char="plusmn" valign="top">0.9746 ± 0.0006</td><td align="char" char="plusmn" valign="top">0.8114 ± 0.0064</td><td align="char" char="plusmn" valign="top">0.6517 ± 0.0076</td></tr><tr><td align="left" valign="top">General recommender (one-hot – L)</td><td align="char" char="plusmn" valign="top">0.8933 ± 0.0020</td><td align="char" char="plusmn" valign="top">0.9778 ± 0.0023</td><td align="char" char="plusmn" valign="top">0.8124 ± 0.0033</td><td align="char" char="plusmn" valign="top">0.6521 ± 0.0078</td></tr><tr><td align="left" valign="top">Species–drug classifiers (MLP – S)</td><td align="char" char="plusmn" valign="top">0.8341 ± 0.0135</td><td align="char" char="plusmn" valign="top">0.9420 ± 0.0123</td><td align="char" char="plusmn" valign="top">0.8005 ± 0.0032</td><td align="char" char="plusmn" valign="top">0.6745 ± 0.0218</td></tr><tr><td align="left" valign="top">Species–drug classifiers (MLP – M)</td><td align="char" char="plusmn" valign="top">0.8382 ± 0.0077</td><td align="char" char="plusmn" valign="top">0.9421 ± 0.0196</td><td align="char" char="plusmn" valign="top">0.8075 ± 0.0049</td><td align="char" char="plusmn" valign="top">0.6797 ± 0.0097</td></tr><tr><td align="left" valign="top">Species–drug classifiers (MLP – L)</td><td align="char" char="plusmn" valign="top">0.8457 ± 0.0088</td><td align="char" char="plusmn" valign="top">0.9505 ± 0.0100</td><td align="char" char="plusmn" valign="top">0.8037 ± 0.0079</td><td align="char" char="plusmn" valign="top">0.6648 ± 0.0149</td></tr><tr><td align="left" valign="top">Species–drug classifiers (MLP – XL)</td><td align="char" char="plusmn" valign="top">0.8611 ± 0.0049</td><td align="char" char="plusmn" valign="top">0.9722 ± 0.0041</td><td align="char" char="plusmn" valign="top">0.8106 ± 0.0069</td><td align="char" char="plusmn" valign="top">0.6801 ± 0.0101</td></tr><tr><td align="left" valign="top">Species–drug classifiers (logistic regression)</td><td align="char" char="." valign="top">0.8684</td><td align="char" char="." valign="top">0.9432</td><td align="char" char="." valign="top">0.7989</td><td align="left" valign="top"><bold>0.7200</bold></td></tr><tr><td align="left" valign="top">Species–drug classifiers (XGBoost)</td><td align="char" char="." valign="top">0.8346</td><td align="char" char="." valign="top">0.9196</td><td align="char" char="." valign="top">0.7763</td><td align="char" char="." valign="top">0.6236</td></tr><tr><td align="left" valign="top">Simulated expert’s best guess</td><td align="char" char="." valign="top">0.8681</td><td align="char" char="." valign="top">0.9743</td><td align="char" char="." valign="top">0.7159</td><td align="char" char="." valign="top">0.5000</td></tr></tbody></table><table-wrap-foot><fn><p>ROC-AUC, area under the receiver operating characteristic curve.</p></fn></table-wrap-foot></table-wrap><p>It is useful to note that <italic>any</italic> gain in performance over the ‘simulated expert’ means that AMR signal could be mined from the spectra. Hence, any performance above this level results in a real-world information gain for clinical diagnostic laboratories.</p></sec><sec id="s3-4"><title>Efficient transfer learning to new hospitals</title><p>An AMR prediction model trained using data from one hospital may not be suitable for use in other hospitals for several reasons. First, protocols such as sample preparation and culturing media differ from hospital to hospital, resulting in systematic differences in MALDI-TOF spectra (<xref ref-type="bibr" rid="bib55">Weis et al., 2022</xref>). Second, epidemiology is spatially varied. Drug-resistant clades may be prevalent in one region or country, but absent in another (<xref ref-type="bibr" rid="bib21">Humphries, 2022</xref>). Finally, the MALDI-TOF instruments themselves may also be specific to the hospital and influence the readout. This influences prediction models, as a hospital-specific effect is reported by the study introducing the DRIAMS dataset (<xref ref-type="bibr" rid="bib55">Weis et al., 2022</xref>). They find that models typically perform best when trained with data from the same hospital. Here, hospital transferability is studied in the context of transfer learning.</p><p>Data from DRIAMS-B, -C, and -D are split into training, validation, and test set. The train set for these hospitals consists of 1000 randomly drawn spectra, simulating a small data scenario where the hospital has not spent considerable efforts in data collection. The remaining spectra for all three hospitals are evenly split among validation and test set.</p><p>For all three hospitals, we train models in the same way as previously (see Methods). A comparison is made between fine-tuning starting from models trained on DRIAMS-A (i.e., models from previous sections) and dual-branch models trained from scratch (<xref ref-type="fig" rid="fig3">Figure 3</xref>). For simplicity, we transfer the non-species-specific, ‘general’ recommenders as we feel this reflects a more realistic use case for labs that cannot afford to gather spectra for all possible species and additionally fine-tune them. Over all three hospitals, models fine-tuned from a DRIAMS-A checkpoint generally outperform models trained from scratch. This trend holds true over different numbers of spectra available in the fine-tuning set. In general, it can be seen that pre-trained models require very little fine-tuning spectra to obtain performances in the same order of magnitude as with DRIAMS-A (see previous Results sections). Performance comparisons of the same models in terms of other metrics are shown in <xref ref-type="fig" rid="app3fig4">Appendix 3—figure 4</xref>.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Transfer learning of DRIAMS-A models to other hospitals.</title><p>Errorbands show the standard deviation over five runs. Results in terms of other evaluation metrics are shown in <xref ref-type="fig" rid="app3fig4">Appendix 3—figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-fig3-v1.tif"/></fig><p>Lowering the amount of data required is paramount to expedite the uptake of AMR models in clinical diagnostics. The transfer learning qualities of dual-branch models may be ascribed to multiple properties. First of all, since different hospitals use much of the same drugs, transferred drug embedders allow for expressively representing drugs out of the box. Secondly, owing to multitask learning, even with a limited number of spectra, a considerable fine-tuning dataset may be obtained, as all available data is ‘thrown on one pile’.</p></sec><sec id="s3-5"><title>MALDI-TOF spectra embeddings</title><p>To investigate what the dual-branch models have learned to represent, MALDI-TOF spectra embeddings are examined. For this purpose, both the best-performing ‘general’ recommender and ‘species-specific’ recommender are used. Here, we visualize the embeddings <inline-formula><mml:math id="inf34"><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> of all test set spectra from the 25 most occurring pathogens. To visualize in a two-dimensional space, UMAP is applied (using default parameters apart from min_dist = 0.5) (Increasing this parameter helps reduce UMAP packing points too tightly together, hence, making for a more-legible plot.). <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the resulting embeddings, colored by species identity, as well as by their AMR status to a selection of drugs.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>UMAP scatterplots of test set matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) spectra embeddings <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</title><p>Top: embeddings from a ‘general’ (trained on all species) recommender. Only embeddings belonging to the 25 most occurring species in the test set are shown. The panels on the right show the same embeddings as on the left, but colored according to its antimicrobial resistance (AMR) status to a certain drug. The four displayed drugs are selected based on a ranking of the product of the number of positive and negative labels <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. In this way, the drugs that have a lot of observed labels, both positives and negatives, are displayed. Bottom: highlighted embeddings from a <italic>S. epidermidis</italic>-specific recommender model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-fig4-v1.tif"/></fig><p>The MALDI-TOF embeddings from the ‘general’ recommender model are grouped primarily per species. This shows that, without being instructed to discriminate between species, the model has learned to group spectra of the same species together. Furthermore, species under the same genus are typically grouped close together, illustrating that the model can pick up hierarchical relations in the tree of life from the data. Within species clusters, the AMR status subplots show that samples are often grouped according to their resistance. For example, for <italic>Staphylococcus epidermidis</italic> and <italic>S. aureus</italic>, multidrug-resistant variants clearly form subclusters. In addition, the cluster of <italic>E. coli</italic> spectra shows a clear tail with samples resistant to ciprofloxacin. Embeddings from the species-specific recommender models show this phenomenon more clearly. UMAP embedding plots from the ‘general recommender’ colored by other drugs are shown in <xref ref-type="fig" rid="app3fig5">Appendix 3—figure 5</xref>. In addition, species-specific recommender system embeddings for some prominent species are shown in <xref ref-type="fig" rid="app3fig6">Appendix 3—figure 6</xref>.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>Prior work on AMR prediction has always modeled within the boundaries of one clade and drug (class), using standard machine learning practices. This work differentiates itself from others by constructing one model for the whole range of drugs encountered in clinical diagnostics. We propose to model AMR prediction via dual-branch neural networks, producing a novel MALDI-TOF-based AMR recommender system. The proposed models come with improved performance over the approaches taken in previous works.</p><p>In clinical diagnostics, AMR predictions could be used to decide which drug to administer on a per-patient basis. For this reason, we argue that evaluation metrics should probe the average quality of predictions per patient (i.e., spectrum-macro metrics). We show that, for these metrics, recommender systems consistently outperform baselines.</p><p>We postulate that the performance of the proposed models is still limited due to (1) lacking a MALDI-TOF-specific learning architecture, (2) collection of more data, especially on rarely encountered species and drugs, and (3) inherent technological limitations of MALDI-TOF MS. Whilst the former is the subject of further machine learning research, the latter two can be considered by equipping the model with some notion of uncertainty, epistemic and aleatoric, respectively (<xref ref-type="bibr" rid="bib20">Hüllermeier and Waegeman, 2021</xref>). In medical decision-making applications, effective uncertainty estimates would be an invaluable tool to aid understanding the models’ predictions. A fourth factor to consider is that perfect test set performance may also be unattainable due to labeling errors. This comes as a consequence of (1) error-prone laboratory measurements of minimum inhibitory concentration (MIC) values, and (2) the fact that EUCAST norms change over time, resulting in outdated label thresholds for historical data.</p><p>As bacterial strains readily adapt resistance to new and frequently used antibiotics, it is impossible for an AMR model to maintain its performance over time. Consequently, an obvious need for continual data collection and online machine learning approaches presents itself. It is for this reason that ML for AMR prediction will prove most valuable when integrated tightly in the inner workings of healthcare (<xref ref-type="bibr" rid="bib31">Lee and Lee, 2020</xref>).</p><p>It stands to reason that blindly following the recommender system’s predictions spells misery. For example, healthcare practitioners should additionally take into account host-specific factors such as patient age, medical history, and concurrent medication. Additionally, as the model is trained on the whole repertoire of antimicrobial drugs, it will have learnt that broad-spectrum antibiotics are typically effective. Hence, it may overrecommend their use. As a consequence, the model’s proposed treatment strategies may not be aligned with antibiotic stewardship, instead exacerbating the very issue it is designed to mitigate. To tackle this problem, one could downweigh the prediction probabilities of undesirable drugs, or, alternatively, train a dual-branch model on only more specifically working drugs.</p><p>In summary, this study serves as the first proof of concept for large MALDI-TOF-based antimicrobial drug recommenders. In this context, we highlight the need for appropriate metrics, proposing that per-patient metrics are most suitable. Extensive experiments on our proposed dual-branch model allow us to assemble some conclusions w.r.t. its use. Firstly, we find that medium-sized MLP spectrum embedders (counting 3.2 M weights) generally perform best. Second, incorporating chemical information works best using Morgan fingerprints. Third, while more data may skew the favor toward the other side, given the current available data, species-specific models outperform recommenders trained for all species. For the smaller datasets used in the transfer learning experiments, the structural inductive bias lent to the model via Morgan fingerprints delivers best results. Our experiments demonstrate that dual-branch recommenders outperform non-recommender baselines on relevant metrics. In the above discussion, some considerations are listed w.r.t. its practical implementation in healthcare. Taken together, this work demonstrates the potential of AMR recommenders to greatly extend the value of MALDI-TOF MS for clinical diagnostics.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93242-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. All experiments build upon publicly available data and code, all instructions to reproduce our study are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/gdewael/maldi-nn">https://github.com/gdewael/maldi-nn</ext-link> (copy archived at <xref ref-type="bibr" rid="bib14">De Waele, 2024</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Weis</surname><given-names>C</given-names></name><name><surname>Cuenod</surname><given-names>A</given-names></name><name><surname>Rieck</surname><given-names>B</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name><name><surname>Egli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>DRIAMS: Database of Resistance Information on Antimicrobials and MALDI-TOF Mass Spectra</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.bzkh1899q</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Research Foundation – Flanders (FWO) (PhD fellowship fundamental research grant 1153024N to GDW). WW also received funding from the Flemish Government under the 'Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen' Programme.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ba</surname><given-names>JL</given-names></name><name><surname>Kiros</surname><given-names>JR</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Layer Normalization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Fan</surname><given-names>ZC</given-names></name><name><surname>Zhang</surname><given-names>LP</given-names></name><name><surname>Xu</surname><given-names>XY</given-names></name><name><surname>Zhang</surname><given-names>ZL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Classification of Methicillin-Resistant and Methicillin-Susceptible <italic>Staphylococcus aureus</italic> Using an Improved Genetic Algorithm for Feature Selection Based on Mass Spectra</article-title><conf-name>Proceedings of the 9th International Conference on Bioinformatics and Biomedical Technology</conf-name><fpage>57</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1145/3093293.3093299</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Beaumont</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Clip retrieval: easily compute clip embeddings and build a clip retrieval system with them</data-title><version designator="ee0931f">ee0931f</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/rom1504/clip-retrieval">https://github.com/rom1504/clip-retrieval</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>P</given-names></name><name><surname>Fan</surname><given-names>W</given-names></name><name><surname>Gu</surname><given-names>B</given-names></name><name><surname>Ju</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Accuracy of matrix-assisted laser desorption ionization–time of flight mass spectrometry for identification of mycobacteria: A systematic review and meta-analysis</article-title><source>Scientific Reports</source><volume>8</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-22642-w</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Capecchi</surname><given-names>A</given-names></name><name><surname>Probst</surname><given-names>D</given-names></name><name><surname>Reymond</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>One molecular fingerprint to rule them all: drugs, biomolecules, and the metabolome</article-title><source>Journal of Cheminformatics</source><volume>12</volume><elocation-id>43</elocation-id><pub-id pub-id-type="doi">10.1186/s13321-020-00445-4</pub-id><pub-id pub-id-type="pmid">33431010</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Xgboost: a scalable tree boosting system</article-title><conf-name>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</conf-name><fpage>785</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1145/2939672.2939785</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A simple framework for contrastive learning of visual representations</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>1597</fpage><lpage>1607</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>van Merrienboer</surname><given-names>B</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Bougares</surname><given-names>F</given-names></name><name><surname>Schwenk</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.1078">https://arxiv.org/abs/1406.1078</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>CR</given-names></name><name><surname>Wang</surname><given-names>HY</given-names></name><name><surname>Yao</surname><given-names>CH</given-names></name><name><surname>Wu</surname><given-names>LC</given-names></name><name><surname>Lu</surname><given-names>JJ</given-names></name><name><surname>Horng</surname><given-names>JT</given-names></name><name><surname>Lee</surname><given-names>TY</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Data-driven two-stage framework for identification and characterization of different antibiotic-resistant <italic>Escherichia coli</italic> isolates based on mass spectrometry data</article-title><source>Microbiology Spectrum</source><volume>11</volume><elocation-id>e0347922</elocation-id><pub-id pub-id-type="doi">10.1128/spectrum.03479-22</pub-id><pub-id pub-id-type="pmid">37042778</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuénod</surname><given-names>A</given-names></name><name><surname>Foucault</surname><given-names>F</given-names></name><name><surname>Pflüger</surname><given-names>V</given-names></name><name><surname>Egli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Factors associated with MALDI-TOF mass spectral quality of species identification in clinical routine diagnostics</article-title><source>Frontiers in Cellular and Infection Microbiology</source><volume>11</volume><elocation-id>646648</elocation-id><pub-id pub-id-type="doi">10.3389/fcimb.2021.646648</pub-id><pub-id pub-id-type="pmid">33796488</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dauwalder</surname><given-names>O</given-names></name><name><surname>Cecchini</surname><given-names>T</given-names></name><name><surname>Rasigade</surname><given-names>JP</given-names></name><name><surname>Vandenesch</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Matrix Assisted Laser Desorption Ionisation/Time Of Flight (MALDI/TOF) mass spectrometry is not done revolutionizing clinical microbiology diagnostic</article-title><source>Clinical Microbiology and Infection</source><volume>29</volume><fpage>127</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/j.cmi.2022.10.005</pub-id><pub-id pub-id-type="pmid">36216238</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Bruyne</surname><given-names>K</given-names></name><name><surname>Slabbinck</surname><given-names>B</given-names></name><name><surname>Waegeman</surname><given-names>W</given-names></name><name><surname>Vauterin</surname><given-names>P</given-names></name><name><surname>De Baets</surname><given-names>B</given-names></name><name><surname>Vandamme</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Bacterial species identification from MALDI-TOF mass spectra through data analysis and machine learning</article-title><source>Systematic and Applied Microbiology</source><volume>34</volume><fpage>20</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.syapm.2010.11.003</pub-id><pub-id pub-id-type="pmid">21295428</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>De Waele</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Maldi-nn</data-title><version designator="swh:1:rev:92464a1325b273efa639fbf31de5372a5d672a72">swh:1:rev:92464a1325b273efa639fbf31de5372a5d672a72</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:5a486c82151caed3cae8f8047608704758053bbf;origin=https://github.com/gdewael/maldi-nn;visit=swh:1:snp:83eba9f3bacef3f8a984211f55823bdfdbe72e79;anchor=swh:1:rev:92464a1325b273efa639fbf31de5372a5d672a72">https://archive.softwareheritage.org/swh:1:dir:5a486c82151caed3cae8f8047608704758053bbf;origin=https://github.com/gdewael/maldi-nn;visit=swh:1:snp:83eba9f3bacef3f8a984211f55823bdfdbe72e79;anchor=swh:1:rev:92464a1325b273efa639fbf31de5372a5d672a72</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Florio</surname><given-names>W</given-names></name><name><surname>Tavanti</surname><given-names>A</given-names></name><name><surname>Barnini</surname><given-names>S</given-names></name><name><surname>Ghelardi</surname><given-names>E</given-names></name><name><surname>Lupetti</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Recent advances and ongoing challenges in the diagnosis of microbial infections by MALDI-TOF mass spectrometry</article-title><source>Frontiers in Microbiology</source><volume>9</volume><elocation-id>1097</elocation-id><pub-id pub-id-type="doi">10.3389/fmicb.2018.01097</pub-id><pub-id pub-id-type="pmid">29896172</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>X</given-names></name><name><surname>Liao</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Nie</surname><given-names>L</given-names></name><name><surname>Hu</surname><given-names>X</given-names></name><name><surname>Chua</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural Collaborative Filtering</article-title><conf-name>Proceedings of the 26th international conference on world wide web</conf-name><fpage>173</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1145/3038912.3052569</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hendrycks</surname><given-names>D</given-names></name><name><surname>Gimpel</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Gaussian Error Linear Units (Gelus)</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hettick</surname><given-names>JM</given-names></name><name><surname>Kashon</surname><given-names>ML</given-names></name><name><surname>Slaven</surname><given-names>JE</given-names></name><name><surname>Ma</surname><given-names>Y</given-names></name><name><surname>Simpson</surname><given-names>JP</given-names></name><name><surname>Siegel</surname><given-names>PD</given-names></name><name><surname>Mazurek</surname><given-names>GN</given-names></name><name><surname>Weissman</surname><given-names>DN</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Discrimination of intact mycobacteria at the strain level: A combined MALDI-TOF MS and biostatistical analysis</article-title><source>Proteomics</source><volume>6</volume><fpage>6416</fpage><lpage>6425</lpage><pub-id pub-id-type="doi">10.1002/pmic.200600335</pub-id><pub-id pub-id-type="pmid">17109381</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>TY</given-names></name><name><surname>Chiang-Ni</surname><given-names>C</given-names></name><name><surname>Teng</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Current status of MALDI-TOF mass spectrometry in clinical microbiology</article-title><source>Journal of Food and Drug Analysis</source><volume>27</volume><fpage>404</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.jfda.2019.01.001</pub-id><pub-id pub-id-type="pmid">30987712</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hüllermeier</surname><given-names>E</given-names></name><name><surname>Waegeman</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods</article-title><source>Machine Learning</source><volume>110</volume><fpage>457</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1007/s10994-021-05946-3</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Ad hoc antimicrobial susceptibility testing from MALDI-TOF MS spectra in the clinical microbiology laboratory</article-title><source>Clinical Chemistry</source><volume>68</volume><fpage>1118</fpage><lpage>1120</lpage><pub-id pub-id-type="doi">10.1093/clinchem/hvac044</pub-id><pub-id pub-id-type="pmid">35352088</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iliadis</surname><given-names>D</given-names></name><name><surname>De Baets</surname><given-names>B</given-names></name><name><surname>Waegeman</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multi-target prediction for dummies using two-branch neural networks</article-title><source>Machine Learning</source><volume>111</volume><fpage>651</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1007/s10994-021-06104-5</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Karpukhin</surname><given-names>V</given-names></name><name><surname>Oguz</surname><given-names>B</given-names></name><name><surname>Min</surname><given-names>S</given-names></name><name><surname>Lewis</surname><given-names>P</given-names></name><name><surname>Wu</surname><given-names>L</given-names></name><name><surname>Edunov</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Yih</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dense Passage Retrieval for Open-Domain Question Answering</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2004.04906">https://arxiv.org/abs/2004.04906</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>ZA</given-names></name><name><surname>Siddiqui</surname><given-names>MF</given-names></name><name><surname>Park</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Current and emerging methods of antibiotic susceptibility testing</article-title><source>Diagnostics</source><volume>9</volume><elocation-id>49</elocation-id><pub-id pub-id-type="doi">10.3390/diagnostics9020049</pub-id><pub-id pub-id-type="pmid">31058811</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>T</given-names></name><name><surname>Gindulyte</surname><given-names>A</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Shoemaker</surname><given-names>BA</given-names></name><name><surname>Thiessen</surname><given-names>PA</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Zaslavsky</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Bolton</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>PubChem 2023 update</article-title><source>Nucleic Acids Research</source><volume>51</volume><fpage>D1373</fpage><lpage>D1380</lpage><pub-id pub-id-type="doi">10.1093/nar/gkac956</pub-id><pub-id pub-id-type="pmid">36305812</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krenn</surname><given-names>M</given-names></name><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Nigam</surname><given-names>A</given-names></name><name><surname>Friederich</surname><given-names>P</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation</article-title><source>Machine Learning: Science and Technology</source><volume>1</volume><elocation-id>045024</elocation-id><pub-id pub-id-type="doi">10.1088/2632-2153/aba947</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lance</surname><given-names>C</given-names></name><name><surname>Luecken</surname><given-names>MD</given-names></name><name><surname>Burkhardt</surname><given-names>DB</given-names></name><name><surname>Cannoodt</surname><given-names>R</given-names></name><name><surname>Rautenstrauch</surname><given-names>P</given-names></name><name><surname>Laddach</surname><given-names>A</given-names></name><name><surname>Ubingazhibov</surname><given-names>A</given-names></name><name><surname>Cao</surname><given-names>ZJ</given-names></name><name><surname>Deng</surname><given-names>K</given-names></name><name><surname>Khan</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Russkikh</surname><given-names>N</given-names></name><name><surname>Ryazantsev</surname><given-names>G</given-names></name><name><surname>Ohler</surname><given-names>U</given-names></name><name><surname>Pisco</surname><given-names>AO</given-names></name><name><surname>Bloom</surname><given-names>J</given-names></name><name><surname>Krishnaswamy</surname><given-names>S</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name><collab>NeurIPS 2021 Multimodal data integration competition participants</collab></person-group><year iso-8601-date="2022">2022</year><article-title>Multimodal Single Cell Data Integration Challenge: Results and Lessons Learned</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.04.11.487796</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Landrum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>Rdkit documentation</data-title><publisher-name>RDKit: Open-Source Cheminformatics Software</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.rdkit.org/docs/index.html">https://www.rdkit.org/docs/index.html</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>I</given-names></name><name><surname>Keum</surname><given-names>J</given-names></name><name><surname>Nam</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepConv-DTI: Prediction of drug-target interactions via deep learning with convolution on protein sequences</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007129</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007129</pub-id><pub-id pub-id-type="pmid">31199797</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>CS</given-names></name><name><surname>Lee</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Clinical applications of continual learning machine learning</article-title><source>The Lancet. Digital Health</source><volume>2</volume><fpage>e279</fpage><lpage>e281</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(20)30102-3</pub-id><pub-id pub-id-type="pmid">33328120</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Mao</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>CY</given-names></name><name><surname>Feichtenhofer</surname><given-names>C</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A ConvNet for the 2020s</article-title><conf-name>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</conf-name><fpage>11976</fpage><lpage>11986</lpage><pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01167</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mortier</surname><given-names>T</given-names></name><name><surname>Wieme</surname><given-names>AD</given-names></name><name><surname>Vandamme</surname><given-names>P</given-names></name><name><surname>Waegeman</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bacterial species identification using MALDI-TOF mass spectrometry and machine learning techniques: A large-scale benchmarking study</article-title><source>Computational and Structural Biotechnology Journal</source><volume>19</volume><fpage>6157</fpage><lpage>6168</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2021.11.004</pub-id><pub-id pub-id-type="pmid">34938408</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>CJL</given-names></name><name><surname>Ikuta</surname><given-names>KS</given-names></name><name><surname>Sharara</surname><given-names>F</given-names></name><name><surname>Swetschinski</surname><given-names>L</given-names></name><name><surname>Robles Aguilar</surname><given-names>G</given-names></name><name><surname>Gray</surname><given-names>A</given-names></name><name><surname>Han</surname><given-names>C</given-names></name><name><surname>Bisignano</surname><given-names>C</given-names></name><name><surname>Rao</surname><given-names>P</given-names></name><name><surname>Wool</surname><given-names>E</given-names></name><name><surname>Johnson</surname><given-names>SC</given-names></name><name><surname>Browne</surname><given-names>AJ</given-names></name><name><surname>Chipeta</surname><given-names>MG</given-names></name><name><surname>Fell</surname><given-names>F</given-names></name><name><surname>Hackett</surname><given-names>S</given-names></name><name><surname>Haines-Woodhouse</surname><given-names>G</given-names></name><name><surname>Kashef Hamadani</surname><given-names>BH</given-names></name><name><surname>Kumaran</surname><given-names>EAP</given-names></name><name><surname>McManigal</surname><given-names>B</given-names></name><name><surname>Achalapong</surname><given-names>S</given-names></name><name><surname>Agarwal</surname><given-names>R</given-names></name><name><surname>Akech</surname><given-names>S</given-names></name><name><surname>Albertson</surname><given-names>S</given-names></name><name><surname>Amuasi</surname><given-names>J</given-names></name><name><surname>Andrews</surname><given-names>J</given-names></name><name><surname>Aravkin</surname><given-names>A</given-names></name><name><surname>Ashley</surname><given-names>E</given-names></name><name><surname>Babin</surname><given-names>F-X</given-names></name><name><surname>Bailey</surname><given-names>F</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name><name><surname>Basnyat</surname><given-names>B</given-names></name><name><surname>Bekker</surname><given-names>A</given-names></name><name><surname>Bender</surname><given-names>R</given-names></name><name><surname>Berkley</surname><given-names>JA</given-names></name><name><surname>Bethou</surname><given-names>A</given-names></name><name><surname>Bielicki</surname><given-names>J</given-names></name><name><surname>Boonkasidecha</surname><given-names>S</given-names></name><name><surname>Bukosia</surname><given-names>J</given-names></name><name><surname>Carvalheiro</surname><given-names>C</given-names></name><name><surname>Castañeda-Orjuela</surname><given-names>C</given-names></name><name><surname>Chansamouth</surname><given-names>V</given-names></name><name><surname>Chaurasia</surname><given-names>S</given-names></name><name><surname>Chiurchiù</surname><given-names>S</given-names></name><name><surname>Chowdhury</surname><given-names>F</given-names></name><name><surname>Clotaire Donatien</surname><given-names>R</given-names></name><name><surname>Cook</surname><given-names>AJ</given-names></name><name><surname>Cooper</surname><given-names>B</given-names></name><name><surname>Cressey</surname><given-names>TR</given-names></name><name><surname>Criollo-Mora</surname><given-names>E</given-names></name><name><surname>Cunningham</surname><given-names>M</given-names></name><name><surname>Darboe</surname><given-names>S</given-names></name><name><surname>Day</surname><given-names>NPJ</given-names></name><name><surname>De Luca</surname><given-names>M</given-names></name><name><surname>Dokova</surname><given-names>K</given-names></name><name><surname>Dramowski</surname><given-names>A</given-names></name><name><surname>Dunachie</surname><given-names>SJ</given-names></name><name><surname>Duong Bich</surname><given-names>T</given-names></name><name><surname>Eckmanns</surname><given-names>T</given-names></name><name><surname>Eibach</surname><given-names>D</given-names></name><name><surname>Emami</surname><given-names>A</given-names></name><name><surname>Feasey</surname><given-names>N</given-names></name><name><surname>Fisher-Pearson</surname><given-names>N</given-names></name><name><surname>Forrest</surname><given-names>K</given-names></name><name><surname>Garcia</surname><given-names>C</given-names></name><name><surname>Garrett</surname><given-names>D</given-names></name><name><surname>Gastmeier</surname><given-names>P</given-names></name><name><surname>Giref</surname><given-names>AZ</given-names></name><name><surname>Greer</surname><given-names>RC</given-names></name><name><surname>Gupta</surname><given-names>V</given-names></name><name><surname>Haller</surname><given-names>S</given-names></name><name><surname>Haselbeck</surname><given-names>A</given-names></name><name><surname>Hay</surname><given-names>SI</given-names></name><name><surname>Holm</surname><given-names>M</given-names></name><name><surname>Hopkins</surname><given-names>S</given-names></name><name><surname>Hsia</surname><given-names>Y</given-names></name><name><surname>Iregbu</surname><given-names>KC</given-names></name><name><surname>Jacobs</surname><given-names>J</given-names></name><name><surname>Jarovsky</surname><given-names>D</given-names></name><name><surname>Javanmardi</surname><given-names>F</given-names></name><name><surname>Jenney</surname><given-names>AWJ</given-names></name><name><surname>Khorana</surname><given-names>M</given-names></name><name><surname>Khusuwan</surname><given-names>S</given-names></name><name><surname>Kissoon</surname><given-names>N</given-names></name><name><surname>Kobeissi</surname><given-names>E</given-names></name><name><surname>Kostyanev</surname><given-names>T</given-names></name><name><surname>Krapp</surname><given-names>F</given-names></name><name><surname>Krumkamp</surname><given-names>R</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Kyu</surname><given-names>HH</given-names></name><name><surname>Lim</surname><given-names>C</given-names></name><name><surname>Lim</surname><given-names>K</given-names></name><name><surname>Limmathurotsakul</surname><given-names>D</given-names></name><name><surname>Loftus</surname><given-names>MJ</given-names></name><name><surname>Lunn</surname><given-names>M</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Manoharan</surname><given-names>A</given-names></name><name><surname>Marks</surname><given-names>F</given-names></name><name><surname>May</surname><given-names>J</given-names></name><name><surname>Mayxay</surname><given-names>M</given-names></name><name><surname>Mturi</surname><given-names>N</given-names></name><name><surname>Munera-Huertas</surname><given-names>T</given-names></name><name><surname>Musicha</surname><given-names>P</given-names></name><name><surname>Musila</surname><given-names>LA</given-names></name><name><surname>Mussi-Pinhata</surname><given-names>MM</given-names></name><name><surname>Naidu</surname><given-names>RN</given-names></name><name><surname>Nakamura</surname><given-names>T</given-names></name><name><surname>Nanavati</surname><given-names>R</given-names></name><name><surname>Nangia</surname><given-names>S</given-names></name><name><surname>Newton</surname><given-names>P</given-names></name><name><surname>Ngoun</surname><given-names>C</given-names></name><name><surname>Novotney</surname><given-names>A</given-names></name><name><surname>Nwakanma</surname><given-names>D</given-names></name><name><surname>Obiero</surname><given-names>CW</given-names></name><name><surname>Ochoa</surname><given-names>TJ</given-names></name><name><surname>Olivas-Martinez</surname><given-names>A</given-names></name><name><surname>Olliaro</surname><given-names>P</given-names></name><name><surname>Ooko</surname><given-names>E</given-names></name><name><surname>Ortiz-Brizuela</surname><given-names>E</given-names></name><name><surname>Ounchanum</surname><given-names>P</given-names></name><name><surname>Pak</surname><given-names>GD</given-names></name><name><surname>Paredes</surname><given-names>JL</given-names></name><name><surname>Peleg</surname><given-names>AY</given-names></name><name><surname>Perrone</surname><given-names>C</given-names></name><name><surname>Phe</surname><given-names>T</given-names></name><name><surname>Phommasone</surname><given-names>K</given-names></name><name><surname>Plakkal</surname><given-names>N</given-names></name><name><surname>Ponce-de-Leon</surname><given-names>A</given-names></name><name><surname>Raad</surname><given-names>M</given-names></name><name><surname>Ramdin</surname><given-names>T</given-names></name><name><surname>Rattanavong</surname><given-names>S</given-names></name><name><surname>Riddell</surname><given-names>A</given-names></name><name><surname>Roberts</surname><given-names>T</given-names></name><name><surname>Robotham</surname><given-names>JV</given-names></name><name><surname>Roca</surname><given-names>A</given-names></name><name><surname>Rosenthal</surname><given-names>VD</given-names></name><name><surname>Rudd</surname><given-names>KE</given-names></name><name><surname>Russell</surname><given-names>N</given-names></name><name><surname>Sader</surname><given-names>HS</given-names></name><name><surname>Saengchan</surname><given-names>W</given-names></name><name><surname>Schnall</surname><given-names>J</given-names></name><name><surname>Scott</surname><given-names>JAG</given-names></name><name><surname>Seekaew</surname><given-names>S</given-names></name><name><surname>Sharland</surname><given-names>M</given-names></name><name><surname>Shivamallappa</surname><given-names>M</given-names></name><name><surname>Sifuentes-Osornio</surname><given-names>J</given-names></name><name><surname>Simpson</surname><given-names>AJ</given-names></name><name><surname>Steenkeste</surname><given-names>N</given-names></name><name><surname>Stewardson</surname><given-names>AJ</given-names></name><name><surname>Stoeva</surname><given-names>T</given-names></name><name><surname>Tasak</surname><given-names>N</given-names></name><name><surname>Thaiprakong</surname><given-names>A</given-names></name><name><surname>Thwaites</surname><given-names>G</given-names></name><name><surname>Tigoi</surname><given-names>C</given-names></name><name><surname>Turner</surname><given-names>C</given-names></name><name><surname>Turner</surname><given-names>P</given-names></name><name><surname>van Doorn</surname><given-names>HR</given-names></name><name><surname>Velaphi</surname><given-names>S</given-names></name><name><surname>Vongpradith</surname><given-names>A</given-names></name><name><surname>Vongsouvath</surname><given-names>M</given-names></name><name><surname>Vu</surname><given-names>H</given-names></name><name><surname>Walsh</surname><given-names>T</given-names></name><name><surname>Walson</surname><given-names>JL</given-names></name><name><surname>Waner</surname><given-names>S</given-names></name><name><surname>Wangrangsimakul</surname><given-names>T</given-names></name><name><surname>Wannapinij</surname><given-names>P</given-names></name><name><surname>Wozniak</surname><given-names>T</given-names></name><name><surname>Young Sharma</surname><given-names>TEMW</given-names></name><name><surname>Yu</surname><given-names>KC</given-names></name><name><surname>Zheng</surname><given-names>P</given-names></name><name><surname>Sartorius</surname><given-names>B</given-names></name><name><surname>Lopez</surname><given-names>AD</given-names></name><name><surname>Stergachis</surname><given-names>A</given-names></name><name><surname>Moore</surname><given-names>C</given-names></name><name><surname>Dolecek</surname><given-names>C</given-names></name><name><surname>Naghavi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Global burden of bacterial antimicrobial resistance in 2019: A systematic analysis</article-title><source>The Lancet</source><volume>399</volume><fpage>629</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(21)02724-0</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>O’Boyle</surname><given-names>N</given-names></name><name><surname>Dalke</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepSMILES: An Adaptation of SMILES for Use in Machine-Learning of Chemical Structures</article-title><source>ChemRxiv</source><pub-id pub-id-type="doi">10.26434/chemrxiv.7097960</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>O’Neill</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Tackling Drug-Resistant Infections Globally: Final Report and Recommendations</source><publisher-name>Government of the United Kingdom</publisher-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Öztürk</surname><given-names>H</given-names></name><name><surname>Ozkirimli</surname><given-names>E</given-names></name><name><surname>Özgür</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A comparative study of SMILES-based compound similarity functions for drug-target interaction prediction</article-title><source>BMC Bioinformatics</source><volume>17</volume><elocation-id>128</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-016-0977-x</pub-id><pub-id pub-id-type="pmid">26987649</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>JW</given-names></name><name><surname>Hallacy</surname><given-names>C</given-names></name><name><surname>Ramesh</surname><given-names>A</given-names></name><name><surname>Goh</surname><given-names>G</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning transferable visual models from natural language supervision</article-title><conf-name>International Conference on Machine Learning PMLR</conf-name><fpage>8748</fpage><lpage>8763</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rendle</surname><given-names>S</given-names></name><name><surname>Krichene</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Anderson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural Collaborative Filtering vs. Matrix Factorization Revisited</article-title><conf-name>Proceedings of the 14th ACM Conference on Recommender Systems</conf-name><fpage>240</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1145/3383313.3412488</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shazeer</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Glu Variants Improve Transformer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.05202">https://arxiv.org/abs/2002.05202</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shlaes</surname><given-names>DM</given-names></name><name><surname>Gerding</surname><given-names>DN</given-names></name><name><surname>John</surname><given-names>JF</given-names></name><name><surname>Craig</surname><given-names>WA</given-names></name><name><surname>Bornstein</surname><given-names>DL</given-names></name><name><surname>Duncan</surname><given-names>RA</given-names></name><name><surname>Eckman</surname><given-names>MR</given-names></name><name><surname>Farrer</surname><given-names>WE</given-names></name><name><surname>Greene</surname><given-names>WH</given-names></name><name><surname>Lorian</surname><given-names>V</given-names></name><name><surname>Levy</surname><given-names>S</given-names></name><name><surname>McGowan</surname><given-names>JE</given-names></name><name><surname>Paul</surname><given-names>SM</given-names></name><name><surname>Ruskin</surname><given-names>J</given-names></name><name><surname>Tenover</surname><given-names>FC</given-names></name><name><surname>Watanakunakorn</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Society for healthcare epidemiology of america and infectious diseases society of america joint committee on the prevention of antimicrobial resistance: Guidelines for the prevention of antimicrobial resistance in hospitals</article-title><source>Infection Control and Hospital Epidemiology</source><volume>18</volume><fpage>275</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.2307/30141215</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title><source>The Journal of Machine Learning Research</source><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Belkum</surname><given-names>A</given-names></name><name><surname>Welker</surname><given-names>M</given-names></name><name><surname>Erhard</surname><given-names>M</given-names></name><name><surname>Chatellier</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Biomedical mass spectrometry in today’s and tomorrow’s clinical microbiology laboratories</article-title><source>Journal of Clinical Microbiology</source><volume>50</volume><fpage>1513</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1128/JCM.00420-12</pub-id><pub-id pub-id-type="pmid">22357505</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Veen</surname><given-names>SQ</given-names></name><name><surname>Claas</surname><given-names>ECJ</given-names></name><name><surname>Kuijper</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>High-throughput identification of bacteria and yeast by matrix-assisted laser desorption ionization-time of flight mass spectrometry in conventional medical microbiology laboratories</article-title><source>Journal of Clinical Microbiology</source><volume>48</volume><fpage>900</fpage><lpage>907</lpage><pub-id pub-id-type="doi">10.1128/JCM.02071-09</pub-id><pub-id pub-id-type="pmid">20053859</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vervier</surname><given-names>K</given-names></name><name><surname>Mahé</surname><given-names>P</given-names></name><name><surname>Veyrieras</surname><given-names>JB</given-names></name><name><surname>Vert</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Benchmark of Structured Machine Learning Methods for Microbial Identification from Mass-Spectrometry Data</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.07251">https://arxiv.org/abs/1506.07251</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname><given-names>D</given-names></name><name><surname>Thormann</surname><given-names>M</given-names></name><name><surname>Pons</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>LINGO, an efficient holographic text based method to calculate biophysical properties and intermolecular similarities</article-title><source>Journal of Chemical Information and Modeling</source><volume>45</volume><fpage>386</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1021/ci0496797</pub-id><pub-id pub-id-type="pmid">15807504</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visonà</surname><given-names>G</given-names></name><name><surname>Duroux</surname><given-names>D</given-names></name><name><surname>Miranda</surname><given-names>L</given-names></name><name><surname>Sükei</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name><name><surname>Oliver</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Multimodal learning in clinical proteomics: enhancing antimicrobial resistance prediction models with chemical information</article-title><source>Bioinformatics</source><volume>39</volume><elocation-id>btad717</elocation-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btad717</pub-id><pub-id pub-id-type="pmid">38001023</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vrioni</surname><given-names>G</given-names></name><name><surname>Tsiamis</surname><given-names>C</given-names></name><name><surname>Oikonomidis</surname><given-names>G</given-names></name><name><surname>Theodoridou</surname><given-names>K</given-names></name><name><surname>Kapsimali</surname><given-names>V</given-names></name><name><surname>Tsakris</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>MALDI-TOF mass spectrometry technology for detecting biomarkers of antimicrobial resistance: current achievements and future perspectives</article-title><source>Annals of Translational Medicine</source><volume>6</volume><elocation-id>240</elocation-id><pub-id pub-id-type="doi">10.21037/atm.2018.06.28</pub-id><pub-id pub-id-type="pmid">30069442</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waegeman</surname><given-names>W</given-names></name><name><surname>Dembczyński</surname><given-names>K</given-names></name><name><surname>Hüllermeier</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multi-target prediction: A unifying view on problems and methods</article-title><source>Data Mining and Knowledge Discovery</source><volume>33</volume><fpage>293</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1007/s10618-018-0595-5</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>HY</given-names></name><name><surname>Lee</surname><given-names>TY</given-names></name><name><surname>Tseng</surname><given-names>YJ</given-names></name><name><surname>Liu</surname><given-names>TP</given-names></name><name><surname>Huang</surname><given-names>KY</given-names></name><name><surname>Chang</surname><given-names>YT</given-names></name><name><surname>Chen</surname><given-names>CH</given-names></name><name><surname>Lu</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A new scheme for strain typing of methicillin-resistant <italic>Staphylococcus aureus</italic> on the basis of matrix-assisted laser desorption ionization time-of-flight mass spectrometry by using machine learning approach</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0194289</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0194289</pub-id><pub-id pub-id-type="pmid">29534106</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weininger</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</article-title><source>Journal of Chemical Information and Computer Sciences</source><volume>28</volume><fpage>31</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weis</surname><given-names>C</given-names></name><name><surname>Horn</surname><given-names>M</given-names></name><name><surname>Rieck</surname><given-names>B</given-names></name><name><surname>Cuénod</surname><given-names>A</given-names></name><name><surname>Egli</surname><given-names>A</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Topological and kernel-based microbial phenotype prediction from MALDI-TOF mass spectra</article-title><source>Bioinformatics</source><volume>36</volume><fpage>i30</fpage><lpage>i38</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa429</pub-id><pub-id pub-id-type="pmid">32657381</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weis</surname><given-names>CV</given-names></name><name><surname>Jutzeler</surname><given-names>CR</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Machine learning for microbial identification and antimicrobial susceptibility testing on MALDI-TOF mass spectra: A systematic review</article-title><source>Clinical Microbiology and Infection</source><volume>26</volume><fpage>1310</fpage><lpage>1317</lpage><pub-id pub-id-type="doi">10.1016/j.cmi.2020.03.014</pub-id><pub-id pub-id-type="pmid">32217160</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weis</surname><given-names>C</given-names></name><name><surname>Cuénod</surname><given-names>A</given-names></name><name><surname>Rieck</surname><given-names>B</given-names></name><name><surname>Dubuis</surname><given-names>O</given-names></name><name><surname>Graf</surname><given-names>S</given-names></name><name><surname>Lang</surname><given-names>C</given-names></name><name><surname>Oberle</surname><given-names>M</given-names></name><name><surname>Brackmann</surname><given-names>M</given-names></name><name><surname>Søgaard</surname><given-names>KK</given-names></name><name><surname>Osthoff</surname><given-names>M</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name><name><surname>Egli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Direct antimicrobial resistance prediction from clinical MALDI-TOF mass spectra using machine learning</article-title><source>Nature Medicine</source><volume>28</volume><fpage>164</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1038/s41591-021-01619-9</pub-id><pub-id pub-id-type="pmid">35013613</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Ng</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ProbeRating: a recommender system to infer binding profiles for nucleic acid-binding proteins</article-title><source>Bioinformatics</source><volume>36</volume><fpage>4797</fpage><lpage>4804</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa580</pub-id><pub-id pub-id-type="pmid">32573679</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Youden</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="1950">1950</year><article-title>Index for rating diagnostic tests</article-title><source>Cancer</source><volume>3</volume><fpage>32</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1002/1097-0142(1950)3:1&lt;32::aid-cncr2820030106&gt;3.0.co;2-3</pub-id><pub-id pub-id-type="pmid">15405679</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Tien</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>YC</given-names></name><name><surname>Cho</surname><given-names>DY</given-names></name><name><surname>Chen</surname><given-names>JW</given-names></name><name><surname>Tsai</surname><given-names>YT</given-names></name><name><surname>Huang</surname><given-names>YC</given-names></name><name><surname>Chao</surname><given-names>HJ</given-names></name><name><surname>Chen</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Rapid identification of methicillin-resistant <italic>Staphylococcus aureus</italic> using MALDI-TOF MS and machine learning from over 20,000 clinical isolates</article-title><source>Microbiology Spectrum</source><volume>10</volume><elocation-id>e0048322</elocation-id><pub-id pub-id-type="doi">10.1128/spectrum.00483-22</pub-id><pub-id pub-id-type="pmid">35293803</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>YM</given-names></name><name><surname>Tsao</surname><given-names>MF</given-names></name><name><surname>Chang</surname><given-names>CY</given-names></name><name><surname>Lin</surname><given-names>KT</given-names></name><name><surname>Keller</surname><given-names>JJ</given-names></name><name><surname>Lin</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Rapid identification of carbapenem-resistant Klebsiella pneumoniae based on matrix-assisted laser desorption ionization time-of-flight mass spectrometry and an artificial neural network model</article-title><source>Journal of Biomedical Science</source><volume>30</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.1186/s12929-023-00918-2</pub-id><pub-id pub-id-type="pmid">37069555</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>Noroozi</surname><given-names>V</given-names></name><name><surname>Yu</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Joint Deep Modeling of Users and Items Using Reviews for Recommendation</article-title><conf-name>Proceedings of the tenth ACM international conference on web search and data mining</conf-name><fpage>425</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1145/3018661.3018665</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>DRIAMS processing</title><p>As our models require every target to correspond to one specific drug (for which a SMILES string can be obtained), data provided by <xref ref-type="bibr" rid="bib55">Weis et al., 2022</xref> is further cleaned up. First, as ‘quinolones’ and ‘aminoglycosides’ constitute classes of drugs rather than single ones, these drugs and their corresponding measurements are removed from the dataset. Second, some drug names in DRIAMS that refer to the same chemical structure are merged to a single drug. As this merging of drugs also combines their labels, care is taken so that no conflicting labels are combined. If, for a single spectrum, labels exist for both of the merging drugs in question, the label is only kept if both measurements are congruent (either both resistant, intermediate, or susceptible). Otherwise, the merged label is discarded. Finally, some drugs are renamed such that there is less ambiguity as to exactly which compound is referred to by their name. The full list of modifications to drug names is provided in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Full list of modifications made to drug names in DRIAMS.</title><p>Modifications consist of (1) removal of drugs, (2) merging of drugs, and (3) renaming drugs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Original drug name</th><th align="left" valign="bottom">Step undertaken</th></tr></thead><tbody><tr><td align="left" valign="bottom">Quinolones</td><td align="left" valign="bottom">Removed</td></tr><tr><td align="left" valign="bottom">Aminoglycosides</td><td align="left" valign="bottom">Removed</td></tr><tr><td align="left" valign="bottom">Ofloxacin</td><td align="left" valign="bottom">Merged with levofloxacin</td></tr><tr><td align="left" valign="bottom">Benzylpenicillin</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Benzylpenicillin_others</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Benzylpenicillin_with_meningitis</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Benzylpenicillin_with_pneumonia</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Penicillin_with_endokarditis</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Penicillin_without_endokarditis</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Penicillin_without_meningitis</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Penicillin_with_meningitis</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Penicillin_with_pneumonia</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Penicillin_with_other_infections</td><td align="left" valign="bottom">Merged with penicillin</td></tr><tr><td align="left" valign="bottom">Cefuroxime.1</td><td align="left" valign="bottom">Merged with cefuroxime</td></tr><tr><td align="left" valign="bottom">Cotrimoxazol</td><td align="left" valign="bottom">Merged with cotrimoxazole</td></tr><tr><td align="left" valign="bottom">Gentamicin_high_level</td><td align="left" valign="bottom">Merged with gentamicin</td></tr><tr><td align="left" valign="bottom">Cefoxitin_screen</td><td align="left" valign="bottom">Merged with cefoxitin</td></tr><tr><td align="left" valign="bottom">Teicoplanin_GRD</td><td align="left" valign="bottom">Merged with teicoplanin</td></tr><tr><td align="left" valign="bottom">Vancomycin_GRD</td><td align="left" valign="bottom">Merged with vancomycin</td></tr><tr><td align="left" valign="bottom">Rifampicin_1mg-l</td><td align="left" valign="bottom">Merged with rifampicin</td></tr><tr><td align="left" valign="bottom">Meropenem_with_meningitis</td><td align="left" valign="bottom">Merged with meropenem</td></tr><tr><td align="left" valign="bottom">Meropenem_without_meningitis</td><td align="left" valign="bottom">Merged with meropenem</td></tr><tr><td align="left" valign="bottom">Meropenem_with_pneumonia</td><td align="left" valign="bottom">Merged with meropenem</td></tr><tr><td align="left" valign="bottom">Amoxicillin-Clavulanic acid_uncomplicated_HWI</td><td align="left" valign="bottom">Merged with amoxicillin-clavulanic acid</td></tr><tr><td align="left" valign="bottom">Strepomycin_high_level</td><td align="left" valign="bottom">Renamed to streptomycin</td></tr><tr><td align="left" valign="bottom">Bacitracin</td><td align="left" valign="bottom">Renamed to bacitracin A</td></tr><tr><td align="left" valign="bottom">Ceftarolin</td><td align="left" valign="bottom">Renamed to ceftaroline fosamil</td></tr><tr><td align="left" valign="bottom">Fosfomycin-Trometamol</td><td align="left" valign="bottom">Renamed to fosfomycin tromethamine</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><p>To present drugs to the model, all names of drugs are converted to SMILES strings. In this work, PubChem’s canonical SMILES strings of every compound are used. In PubChem, canonical SMILES are not isomeric, which means that stereochemistry is ignored. As such, two drugs that are stereoisomers are treated as a single drug, this is the case for ofloxacin and levofloxacin. Furthermore, many drugs in the dataset refer to the co-administration of two compounds (such as, for example, ampicillin-sulbactam or amoxicillin-clavulanic acid). These cases are treated as a single drug with a SMILES string consisting of the strings of both constituent compounds separated by a ‘.’ character, as is common practice with SMILES strings.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Modeling set-up</title><sec sec-type="appendix" id="s9-1"><title>Drug embedders</title><p>In this article, seven ways to encode drugs in a model are tested out. In this section, those seven drug embedders are described in detail. All descriptions correspond to the final set-up used to present results, hyperparameter tuning results are presented in Appendix 2.</p><p>All drug embedders encode drugs to a vector <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∈</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The most simple way to obtain a dense vector of that size for every drug is via a <bold>one-hot embedding</bold>. Every drug gets assigned an index in a vector, and the resulting vectors are embedded to a dense representation via a single linear layer. Encoding drugs in this way is the most straightforward, but no structural information of the underlying active compound is included. No inductive bias is presented to the model that will give structurally similar drugs comparable embeddings. As such, all this information must be learnt from data. Similarly, such drug embedders cannot be generalized out-of-the-box to drugs it has not seen in the training data, as there are no indices – and learnt embeddings – for them.</p><p>The (local) structure of drugs can be encoded via fingerprints. A molecular fingerprint corresponds to a bit-vector in which every bit corresponds to the presence or absence of a substructure (<xref ref-type="bibr" rid="bib5">Capecchi et al., 2020</xref>). In this article, <bold>Morgan fingerprints</bold> with a diameter of 4 and consisting of 512 bits are derived from RDKit (<xref ref-type="bibr" rid="bib29">Landrum, 2013</xref>). The resulting vector is embedded with a linear layer to get a dense drug representation. Embedding drugs using such structural features overcomes the aforementioned drawbacks with one-hot embeddings.</p><p>Similarly, the identity of a drug can be communicated via the textual representation known as SMILES strings (<xref ref-type="bibr" rid="bib52">Weininger, 1988</xref>). Here, an adaptation of SMILES for machine and deep learning applications is used, called DeepSMILES (<xref ref-type="bibr" rid="bib35">O’Boyle and Dalke, 2018</xref>). All the different letters in the alphabet are assigned an index in a one-hot vector. Hence, every molecule can be encoded to a matrix <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∈</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf39"><mml:mi>v</mml:mi></mml:math></inline-formula> the vocabulary size of the DeepSMILES alphabet and <inline-formula><mml:math id="inf40"><mml:mi>l</mml:mi></mml:math></inline-formula> the string length of the molecule. This representation can be processed to a vector embedding using any neural network type that is appropriate for variable-length sequences.</p><p>A <bold>1D CNN</bold> detects and composes local patterns in the DeepSMILES string to a final drug embedding. Every input channel corresponds to a specific letter in the SMILES alphabet. The convolutional network used here consists of a position-wise linear layer to embed the channels to 64 dimensions, four convolutional blocks placed in sequence, followed by a global max-pooling operation across the length axis and a final linear layer to return a vector <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. The global max-pooling layer allows the same network to be used for variable-length inputs. Each convolutional block consists of a structure similar to the one found in transformers (<xref ref-type="bibr" rid="bib45">Vaswani et al., 2017</xref>). A first layer normalization is followed by a (padded) convolutional layer with a kernel size of 5, a first residual connection is wrapped around these two operations. After this, a position-wise feedforward makes up the second half of the convolutional block. The position-wise feedforward consists of a layer normalization, after which a GeLU-based gated linear unit identical to the one introduced by <xref ref-type="bibr" rid="bib40">Shazeer, 2020</xref> is employed: <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>0.2</mml:mn></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝒙</mml:mi><mml:mi>𝑾</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>⊙</mml:mo><mml:mi>𝒙</mml:mi><mml:mi>𝑽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:msub><mml:mi>𝑾</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. First, the input is sent to two position-wise linear layers via <inline-formula><mml:math id="inf43"><mml:mi>𝑾</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:mi>𝑽</mml:mi></mml:math></inline-formula>, each of them exploding the hidden dimensions of the input by a factor of 4. By sending the result of the first linear layer to a GeLU activation and multiplying element-wise with the result of the second linear layer, a gated linear unit structure is obtained. The output of this gated linear unit is sent to a dropout layer with rate 0.2 and then returned to the original dimension size via a final linear layer <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>𝑾</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. Around this second LayerNorm and feedforward structure, a residual connection is again wrapped. All residual blocks have an input and output hidden dimension of 64. <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> shows the structure of this convolutional block. Its design adopts the current state-of-the-art practices in transformers, which are increasingly being used in convolutional networks (<xref ref-type="bibr" rid="bib32">Liu et al., 2022</xref>).</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Structure used for the residual blocks, used in the 1D CNN, 2D CNN, and transformer.</title><p>In the case of convolutions, the output is zero padded so as to produce the same output dimensions as in the input.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app2-fig1-v1.tif"/></fig><p>A <bold>transformer</bold> can be used to learn and compose signals in the DeepSMILES strings that occur sequence-wide, as opposed to the local pattern detection with a 1D CNN. The DeepSMILES strings are similarly embedded to 64 dimensions per character. After this, sinusoidal positional encodings (<xref ref-type="bibr" rid="bib45">Vaswani et al., 2017</xref>) are added, and a CLS token embedding is prepended to the sequence. Four transformer blocks are employed, each with 64 as hidden dimension. The structure of the blocks are identical as with the 1D CNN (<xref ref-type="fig" rid="fig1">Figure 1</xref>), but using scaled dot-product self-attention instead of 1D convolutions. The self-attention operation uses eight heads. The output at the CLS token is used as a ‘summary’ of the content in the sequence (as opposed to the global max-pooling with the CNN). A final linear layer on the output of the CLS token returns the drug embedding <inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>A <bold>recurrent neural network</bold> (RNN) is used to process the DeepSMILES strings sequentially. The RNN used here consists of a bidirectional GRU with 64 hidden dimensions (<xref ref-type="bibr" rid="bib8">Cho et al., 2014</xref>). The two final hidden states of the GRU are used as ‘summaries’ of the content in the sequence. These two final states are averaged (element-wise) and sent to a final linear layer returning <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>All three aforementioned neural network structures work on variable-length (Deep)SMILES strings. With mini-batches, input drugs are (zero) padded so that everything fits into a tensor <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>𝐒</mml:mi><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf49"><mml:mi>b</mml:mi></mml:math></inline-formula> the batch size, <inline-formula><mml:math id="inf50"><mml:mi>v</mml:mi></mml:math></inline-formula> the vocabulary size, and <inline-formula><mml:math id="inf51"><mml:mi>l</mml:mi></mml:math></inline-formula> the longest length of a drug in the batch. The three aforementioned neural nets are adapted so that no information can flow from masked tokens to actual drug tokens (through masking after convolutions or in the attention matrices).</p><fig id="app2fig2" position="float"><label>Appendix 2—figure 2.</label><caption><title>Overview of all different drug embedders tested in this work.</title><p>One-hot embeddings are the only technique not incorporating prior knowledge of the structure of the compound. Hence, they are the only technique incapable of directly transferring to new compounds. Morgan fingerprints produce a bit-vector containing information on the presence of certain substructures. DeepSMILES strings are encoded and processed with a 1D CNN, GRU, or transformer. Drawings of molecules are processed with a 2D CNN. A string kernel on SMILES strings produces a numerical vector for every drug (taken as the row in the resulting Gram matrix).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app2-fig2-v1.tif"/></fig><p>As (Deep)SMILES are a 1D representation of a 3D molecular structure, a more detailed view of the drug may be obtained by permitting an extra dimension into its input representation. Drawings of drugs achieve this 2D view of the molecule. Here, 128 × 128 drawings of drugs are obtained through RDKit. The RGB values are inverted so as to make the parts of the image containing molecule ‘activated’. Also, the RGB values are scaled to the range of 0–1 by dividing by 255.</p><p>A <bold>2D CNN</bold> processes the images to a drug embedding. The CNN consists of an input convolutional layer with kernel size and stride of 2. The input layer takes the three input channels and returns 32 hidden dimensions. Afterward, two convolutional blocks of the same structure as with the transformer and 1D CNN are placed in tandem (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>). The 2D convolutional operation used in the convolutional operation has a kernel size of 5. Hereafter, a global max-pooling operation across the height and width of the image is performed, followed by a final linear layer producing the drug embedding <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>A final way to obtain a numerical representation of drugs tested here is through similarity matrices. A <bold>string kernel</bold> is used to create a Gram matrix of all drugs in the training set. The input representation of a drug is then simply a row in said Gram matrix. This approach is generalizable to unseen drugs at inference time as obtaining a representation for them involves running the kernel function of the new compound to all training drugs. In this work, the LINGO string kernel (using 4-mers) is used (<xref ref-type="bibr" rid="bib47">Vidal et al., 2005</xref>) as this kernel performed well in a recent benchmark (<xref ref-type="bibr" rid="bib37">Öztürk et al., 2016</xref>). Note that here SMILES strings are used instead of DeepSMILES (as with the 1D CNN, RNN, and transformer). A linear layer produces the final drug embedding <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>𝒕</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mtext/><mml:mo>∈</mml:mo><mml:mtext/><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>64</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> from a row in the Gram matrix.</p><p>A visual overview of all seven drug embedders in shown in <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref>.</p></sec></sec><sec sec-type="appendix" id="s10"><title>Hyperparameter tuning</title><sec sec-type="appendix" id="s10-1"><title>Dual-branch models</title><p>Due to the complexity of tuning two branches and the size of the dataset, tuning is mostly done in an ad hoc fashion, relying on knowledge of current best practices in deep learning. Only some hyperparameters of interest are tuned on the validation set. Here, we present validation model results of those experiments. All results presented here concern models that are trained with a medium-sized spectrum embedder, with hyperparameters otherwise as described in Appendix 2. All numbers indicate an average over five runs, similarly choosing the best average out of four tested learning rates (1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3).</p><p><xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3A</xref> shows validation set performances for a grid of different kernel sizes and hidden dimensionalities for the SMILES 1-D CNN. The best-performing hidden dimensionality (64) is copied to the (Deep)SMILES Transformer and GRU without further tuning. In <xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3B</xref>, a similar grid is shown for the Image 2-D CNN, where it is found that a smaller hidden size is favored. <xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3C</xref> shows the performance for using different molecular string representations as input to the 1-D CNN model: SMILES, DeepSMILES (<xref ref-type="bibr" rid="bib35">O’Boyle and Dalke, 2018</xref>), and SELFIES (<xref ref-type="bibr" rid="bib27">Krenn et al., 2020</xref>). While all techniques perform competitively, DeepSMILES strings outperform the other two by a small margin. Similarly, DeepSMILES are thus selected as input representations for the Transformer and GRU, without further tuning. <xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3D</xref> shows how sinusoidal positional encodings outperform learned positional encodings (as in <xref ref-type="bibr" rid="bib13">Devlin et al., 2018</xref>). It is found that a bidirectional GRU considerably outperforms a unidirectional one (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3E</xref>). Finally, the number of bits in the Morgan fingerprint encoding is also tuned (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3F</xref>). It is seen that including lower than 512 bits degenerates performance, but including more than 512 introduces instabilities in model training, as the model becomes prone to overfitting the drug branch.</p><fig id="app2fig3" position="float"><label>Appendix 2—figure 3.</label><caption><title>All hyperparameter tuning experiments.</title><p>All evaluations are listed in terms of validation area under the receiver operating characteristic curve (ROC-AUC). All numbers are averages of five model runs, with errorbars showing standard deviations. In every experiment, the highest average is chosen to use in the final models. (<bold>A</bold>) Tuning of kernel and hidden size in a DeepSMILES CNN. (<bold>B</bold>) Tuning of kernel and hidden size in an Image CNN. (<bold>C</bold>) Tuning of alphabet in a DeepSMILES CNN. (<bold>D</bold>) Tuning of positional encodings in a DeepSMILES Transformer. (<bold>E</bold>) Tuning of directionality in a DeepSMILES GRU. (<bold>F</bold>) Tuning of number of bits in a Morgen Fingerprint-based drug embedder.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app2-fig3-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10-2"><title>Specialist baselines</title><p>All baselines are trained using the same data splits as used with the dual-branch model. In essence: all DRIAMS-A spectra before the year 2018 are in the training set. The remaining spectra from 2018 are evenly divided among validation and test (with which belonging to which corresponding with the splits used for the dual-branch experiments). The same preprocessed 6000-dimensional spectrum representations are used as input.</p><p>Logistic regression baselines are trained with the L-BFGS solver for a maximum of 500 training iterations. For every species–drug combination, a grid search is performed on various hyperparameters, selecting the best based on validation ROC-AUC. The hyperparameters that are tuned are the scaling method on the features (either none, or standard scaling), and the L2 regularization strength (<inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo separator="true">,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo separator="true">,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo separator="true">,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo separator="true">,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>).</p><p>For XGBoost, default parameters are used apart from those tuned. For every species–drug combination, a grid is run, testing different numbers of trees (<monospace>n_estimators</monospace> <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>200</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) and learning rate (<monospace>learning_rate</monospace> <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>For the MLP baselines, the same hyperparameters are used as for the spectrum branch. Briefly recapitulated: between every two fully connected layers, a series of operations consisting of (1) a GeLU activation, (2) a dropout rate of 0.2, and (3) layer normalization is applied. The sizes of the models are as in <xref ref-type="table" rid="table1">Table 1</xref>, but then ending in 1 node instead of 64. For every species–drug combination, models are trained using the cross-entropy loss and the Adam optimizer for a maximum of 250 epochs. A batch size of 128 is employed. A linear learning rate warm-up is applied over the first 250 steps. Early stopping based on validation ROC-AUC is applied with a patience of 10 epochs. The model with the best validation ROC-AUC during training is kept as final model. the best model out of four different learning rates (learning_rate <inline-formula><mml:math id="inf57"><mml:mrow><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>5</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>5</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>5</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>4</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>5</mml:mn><mml:mtext>e-</mml:mtext><mml:mn>4</mml:mn><mml:mo form="postfix" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>) is chosen based on their validation ROC-AUC.</p></sec></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s11"><title>Tables and figures supporting the results section</title><table-wrap id="app3table1" position="float"><label>Appendix 3—table 1.</label><caption><title>Full table of test results.</title><p>The listed averages and standard deviations are calculated over five independent runs of the same model. The best models for every metric per drug embedder are underlined. The overall best model for every metric is in bold face.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Drug embedder</th><th align="left" valign="bottom">Spectrum embedder</th><th align="left" valign="bottom">ROC-AUC</th><th align="left" valign="bottom">Prec@1(-)</th><th align="left" valign="bottom">Macro ROC-AUC</th></tr></thead><tbody><tr><td align="left" valign="top">Morgan fingerprints</td><td align="left" valign="top">S</td><td align="left" valign="top">0.9341 ± 0.0014</td><td align="left" valign="top">0.9917 ± 0.0009</td><td align="left" valign="top"><bold><underline>0.8158</underline></bold> <underline><bold>± 0.0070</bold></underline></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top">M</td><td align="left" valign="top"><underline><bold>0.9345 ± 0.0014</bold></underline></td><td align="left" valign="top"><underline><bold>0.9922 ± 0.0009</bold></underline></td><td align="left" valign="top">0.8078 ± 0.0081</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom">0.9341 ± 0.0007</td><td align="left" valign="bottom">0.9920 ± 0.0010</td><td align="left" valign="bottom">0.8070 ± 0.0128</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9322 ± 0.0017</td><td align="left" valign="bottom">0.9920 ± 0.0012</td><td align="left" valign="bottom">0.7904 ± 0.0155</td></tr><tr><td align="left" valign="bottom">One-hot embedding</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">0.9326 ± 0.0017</td><td align="left" valign="bottom">0.9899 ± 0.0010</td><td align="left" valign="bottom">0.7984 ± 0.0086</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">M</td><td align="left" valign="bottom">0.9337 ± 0.0014</td><td align="left" valign="bottom"><underline>0.9910 ± 0.0016</underline></td><td align="left" valign="bottom">0.7920 ± 0.0175</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom"><underline>0.9338 ± 0.0018</underline></td><td align="left" valign="bottom">0.9882 ± 0.0010</td><td align="left" valign="bottom"><underline>0.8011 ± 0.0116</underline></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9327 ± 0.0011</td><td align="left" valign="bottom">0.9890 ± 0.0026</td><td align="left" valign="bottom">0.7932 ± 0.0201</td></tr><tr><td align="left" valign="bottom">DeepSMILES 1-D CNN</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">0.9303 ± 0.0012</td><td align="left" valign="bottom">0.9864 ± 0.0016</td><td align="left" valign="bottom">0.7949 ± 0.0185</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">M</td><td align="left" valign="bottom">0.9336 ± 0.0011</td><td align="left" valign="bottom"><underline>0.9903 ± 0.0008</underline></td><td align="left" valign="bottom"><underline>0.8009 ± 0.0044</underline></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom"><underline>0.9337 ± 0.0015</underline></td><td align="left" valign="bottom">0.9890 ± 0.0014</td><td align="left" valign="bottom">0.7940 ± 0.0052</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9317 ± 0.0012</td><td align="left" valign="bottom">0.9898 ± 0.0020</td><td align="left" valign="bottom">0.7960 ± 0.0155</td></tr><tr><td align="left" valign="bottom">String Kernel (LINGO)</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">0.9327 ± 0.0022</td><td align="left" valign="bottom">0.9913 ± 0.0012</td><td align="left" valign="bottom"><underline>0.7972 ± 0.0087</underline></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">M</td><td align="left" valign="bottom"><underline>0.9332 ± 0.0017</underline></td><td align="left" valign="bottom"><underline>0.9916 ± 0.0008</underline></td><td align="left" valign="bottom">0.7919 ± 0.0051</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom">0.9317 ± 0.0017</td><td align="left" valign="bottom">0.9909 ± 0.0013</td><td align="left" valign="bottom">0.7859 ± 0.0136</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9303 ± 0.0021</td><td align="left" valign="bottom">0.9893 ± 0.0025</td><td align="left" valign="bottom">0.7935 ± 0.0135</td></tr><tr><td align="left" valign="bottom">Image – 2-D CNN</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">0.9310 ± 0.0016</td><td align="left" valign="bottom">0.9888 ± 0.0025</td><td align="left" valign="bottom">0.7820 ± 0.0101</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">M</td><td align="left" valign="bottom">0.9317 ± 0.0008</td><td align="left" valign="bottom">0.9885 ± 0.0019</td><td align="left" valign="bottom"><underline>0.7866 ± 0.0084</underline></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom"><underline>0.9332 ± 0.0010</underline></td><td align="left" valign="bottom"><underline>0.9901 ± 0.0016</underline></td><td align="left" valign="bottom">0.7758 ± 0.0070</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9309 ± 0.0012</td><td align="left" valign="bottom">0.9900 ± 0.0013</td><td align="left" valign="bottom">0.7711 ± 0.0109</td></tr><tr><td align="left" valign="bottom">DeepSMILES Transformer</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">0.9306 ± 0.0021</td><td align="left" valign="bottom"><underline>0.9900 ± 0.0022</underline></td><td align="left" valign="bottom">0.7862 ± 0.0124</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">M</td><td align="left" valign="bottom"><underline>0.9325 ± 0.0012</underline></td><td align="left" valign="bottom">0.9891 ± 0.0014</td><td align="left" valign="bottom"><underline>0.7925 ± 0.0075</underline></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom">0.9308 ± 0.0014</td><td align="left" valign="bottom">0.9885 ± 0.0027</td><td align="left" valign="bottom">0.7902 ± 0.0072</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9311 ± 0.0014</td><td align="left" valign="bottom">0.9895 ± 0.0009</td><td align="left" valign="bottom">0.7791 ± 0.0075</td></tr><tr><td align="left" valign="bottom">DeepSMILES RNN</td><td align="left" valign="bottom">S</td><td align="left" valign="bottom">0.9291 ± 0.0015</td><td align="left" valign="bottom">0.9872 ± 0.0032</td><td align="left" valign="bottom"><underline>0.7881 ± 0.0053</underline></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">M</td><td align="left" valign="bottom"><underline>0.9293 ± 0.0028</underline></td><td align="left" valign="bottom">0.9863 ± 0.0008</td><td align="left" valign="bottom">0.7793 ± 0.0116</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">L</td><td align="left" valign="bottom">0.9266 ± 0.0012</td><td align="left" valign="bottom">0.9868 ± 0.0019</td><td align="left" valign="bottom">0.7684 ± 0.0058</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">XL</td><td align="left" valign="bottom">0.9278 ± 0.0029</td><td align="left" valign="bottom"><underline>0.9879 ± 0.0027</underline></td><td align="left" valign="bottom">0.7689 ± 0.0113</td></tr></tbody></table><table-wrap-foot><fn><p>ROC-AUC area under the receiver operating characteristic curve.</p></fn></table-wrap-foot></table-wrap><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Spectrum-macro receiver operating characteristic (ROC) curve for best-performing model (Morgan fingerprints drug embedder, medium-sized spectrum embedder).</title><p>The y-axis shows the average sensitivity (across patients), while the x-axis shows one minus the average specificity. Note that this ROC curve is not a traditional ROC curve constructed from one single label set and one corresponding prediction set. Rather, it is constructed from spectrum-macro metrics as follows: for any possible threshold value, binarize all predictions. Then, for every spectrum/patient independently, compute the sensitivity and specificity for the subset of labels corresponding to that spectrum/patient. Finally, those sensitivities and specificities are averaged across patients to obtain one point on above ROC curve. In blue, the optimal sensitivity and specificity (according to the Youden index) is indicated (<xref ref-type="bibr" rid="bib57">Youden, 1950</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app3-fig1-v1.tif"/></fig><fig id="app3fig2" position="float"><label>Appendix 3—figure 2.</label><caption><title>Barplots showing test performance results for all trained models.</title><p>Colors represent the different spectrum embedder model sizes. Performance is shown in terms of macro area under the receiver operating characteristic curve (ROC-AUC) (computed per drug and averaged). Errorbars represent the standard deviation over five random seeds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app3-fig2-v1.tif"/></fig><table-wrap id="app3table2" position="float"><label>Appendix 3—table 2.</label><caption><title>Test area under the receiver operating characteristic curve (ROC-AUC) performance per species.</title><p>Reported figures are averages across the five different medium-sized Morgan fingerprint-based recommenders.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Species</th><th align="left" valign="bottom">ROC-AUC</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Staphylococcus aureus</italic></td><td align="char" char="." valign="bottom">0.9578</td></tr><tr><td align="left" valign="bottom"><italic>Staphylococcus epidermidis</italic></td><td align="char" char="." valign="bottom">0.9478</td></tr><tr><td align="left" valign="bottom"><italic>Escherichia coli</italic></td><td align="char" char="." valign="bottom">0.9184</td></tr><tr><td align="left" valign="bottom"><italic>Klebsiella pneumoniae</italic></td><td align="char" char="." valign="bottom">0.9643</td></tr><tr><td align="left" valign="bottom"><italic>Pseudomonas aeruginosa</italic></td><td align="char" char="." valign="bottom">0.7614</td></tr><tr><td align="left" valign="bottom"><italic>Enterobacter cloacae</italic></td><td align="char" char="." valign="bottom">0.9831</td></tr><tr><td align="left" valign="bottom"><italic>Proteus mirabilis</italic></td><td align="char" char="." valign="bottom">0.9727</td></tr><tr><td align="left" valign="bottom"><italic>Staphylococcus hominis</italic></td><td align="char" char="." valign="bottom">0.9594</td></tr><tr><td align="left" valign="bottom"><italic>Serratia marcescens</italic></td><td align="char" char="." valign="bottom">0.9848</td></tr><tr><td align="left" valign="bottom"><italic>Staphylococcus capitis</italic></td><td align="char" char="." valign="bottom">0.9425</td></tr><tr><td align="left" valign="bottom"><italic>Enterococcus faecium</italic></td><td align="char" char="." valign="bottom">0.9914</td></tr><tr><td align="left" valign="bottom"><italic>Klebsiella oxytoca</italic></td><td align="char" char="." valign="bottom">0.9861</td></tr><tr><td align="left" valign="bottom"><italic>Klebsiella variicola</italic></td><td align="char" char="." valign="bottom">0.9824</td></tr><tr><td align="left" valign="bottom"><italic>Citrobacter koseri</italic></td><td align="char" char="." valign="bottom">0.9970</td></tr><tr><td align="left" valign="bottom"><italic>Enterococcus faecalis</italic></td><td align="char" char="." valign="bottom">0.9594</td></tr><tr><td align="left" valign="bottom"><italic>Staphylococcus lugdunensis</italic></td><td align="char" char="." valign="bottom">0.9705</td></tr><tr><td align="left" valign="bottom"><italic>Citrobacter freundii</italic></td><td align="char" char="." valign="bottom">0.9622</td></tr><tr><td align="left" valign="bottom"><italic>Morganella morganii</italic></td><td align="char" char="." valign="bottom">0.9931</td></tr><tr><td align="left" valign="bottom"><italic>Proteus vulgaris</italic></td><td align="char" char="." valign="bottom">0.9828</td></tr><tr><td align="left" valign="bottom"><italic>Staphylococcus haemolyticus</italic></td><td align="char" char="." valign="bottom">0.9751</td></tr><tr><td align="left" valign="bottom"><italic>Candida albicans</italic></td><td align="char" char="." valign="bottom">0.7446</td></tr><tr><td align="left" valign="bottom"><italic>Streptococcus pneumoniae</italic></td><td align="char" char="." valign="bottom">0.9059</td></tr><tr><td align="left" valign="bottom"><italic>Stenotrophomonas maltophilia</italic></td><td align="char" char="." valign="bottom">1.0000</td></tr><tr><td align="left" valign="bottom"><italic>Campylobacter jejuni</italic></td><td align="char" char="." valign="bottom">1.0000</td></tr><tr><td align="left" valign="bottom"><italic>Haemophilus influenzae</italic></td><td align="char" char="." valign="bottom">1.0000</td></tr></tbody></table></table-wrap><fig id="app3fig3" position="float"><label>Appendix 3—figure 3.</label><caption><title>Performance of models compared against a linear spectrum embedder baseline.</title><p>The comparison is only shown for the best-performing drug embedder (Morgan fingerprints). Errorbars represent the standard deviation over five random seeds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app3-fig3-v1.tif"/></fig><fig id="app3fig4" position="float"><label>Appendix 3—figure 4.</label><caption><title>Transfer learning of DRIAMS-A models to other hospitals.</title><p>Errorbands show the standard deviation over five runs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app3-fig4-v1.tif"/></fig><fig id="app3fig5" position="float"><label>Appendix 3—figure 5.</label><caption><title>UMAP scatterplots of test set matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) spectra embeddings <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</title><p>Embeddings from a ‘general’ (trained on all spectra across species) recommender are shown. Only embeddings belonging to the 25 most occurring species in the test set are shown. Spectra are colored according to its antimicrobial resistance (AMR) status to a certain drug. The 20 displayed drugs were selected based on a ranking of the product of the number of positive and negative labels <inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. In this way, the drugs that have a lot of observed labels, both positives and negatives, are displayed. The drugs here are ranked 5–24 (the first four are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>). In order to map the clusters back to species, readers are referred back to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app3-fig5-v1.tif"/></fig><fig id="app3fig6" position="float"><label>Appendix 3—figure 6.</label><caption><title>UMAP scatterplots of test set matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) spectra embeddings <inline-formula><mml:math id="inf60"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</title><p>Embeddings from two ‘species-specific’ recommenders are shown. Spectra are colored according to its antimicrobial resistance (AMR) status to a certain drug.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93242-app3-fig6-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93242.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Folkman</surname><given-names>Lukas</given-names></name><role specific-use="editor">Reviewing Editor</role></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study presents a machine learning model to recommend effective antimicrobial drugs from patients' samples analyzed with mass spectrometry. The evidence supporting the claims of the authors is <bold>convincing</bold>. This work will be of interest to computational biologists, microbiologists, and clinicians.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93242.4.sa1</article-id><title-group><article-title>Joint public reviews:</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>De Waele et al. framed the mass-spectrum-based prediction of antimicrobial resistance (AMR) prediction as a drug recommendation task. Neural networks were trained on the recently available DRIAMS database of MALDI-TOF (matrix-assisted laser desorption/ionization time-of-flight) mass spectrometry data and their associated antibiotic susceptibility profiles (Weis et al. 2022). Weis et al. (2022) also introduced the benchmark models which take as the input a single species and are trained to predict resistance to a single drug. Instead here, a pair of drugs and spectrum are fed to two neural network models to predict a resistance probability. In this manner, knowledge from different drugs and species can be shared through the model parameters. Questions asked: What is the best way to encode the drugs? Does the dual neural network outperform the single spectrum-drug network?</p><p>The authors showed consistent performance of their strategy to predict antibiotic susceptibility for different spectrum and antibiotic representations (i.e., embedders). Remarkably, the authors showed how small datasets collected at one location can improve the performance of a model trained with limited data collected at a second location. The authors also showed that species-specific models (trained in multiple antibiotic resistance profiles) outperformed both the single recommender model and the individual species-antibiotic combination models.</p><p>Strengths:</p><p>• A single antimicrobial resistance recommender system could potentially facilitate the adoption of MALDI-TOF based antibiotic susceptibility profiling into clinical practices by reducing the number of models to be considered, and the efforts that may be required to periodically update them.</p><p>• The authors tested multiple combinations of embedders for the mass spectra and antibiotics while using different metrics to evaluate the performance of the resulting models. Models trained using different spectrum embedder-antibiotic embedder combinations had remarkably good performance for all tested metrics. The average ROC AUC scores for global and species-specific evaluations were above 0.8.</p><p>• Authors developed species-specific recommenders as an intermediate layer between the single recommender system and single species-antibiotic models. This intermediate approach achieved maximum performance (with one type of the species-specific recommender achieving a 0.9 ROC AUC), outlining the potential of this type of recommenders for frequent pathogens.</p><p>• Authors showed that data collected in one location can be leveraged to improve the performance of models generated using a smaller number of samples collected at a different location. This result may encourage researchers to optimize data integration to reduce the burden of data generation for institutions interested in testing this method.</p><p>Weaknesses:</p><p>• Authors do not offer information about the model features associated with resistance. While reviewers understand that it is difficult to map mass spectra to specific pathways or metabolites, mechanistic insights are much more important in the context of AMR than in the context of bacterial identification. For example, this information may offer additional antimicrobial targets. Thus, authors should at least identify mass spectra peaks highly associated with resistance profiles. Are those peaks consistent across species? This would be a key step towards a proteomic survey of mechanisms of AMR. See previous work on this topic (Hrabak et al. 2013, Torres-Sangiao et al. 2022).</p><p>References:</p><p>Hrabak et al. (2013). Clin Microbiol Rev 26. doi: 10.1128/CMR.00058-12.</p><p>Torres-Sangiao et al. (2022). Front Med 9. doi: 10.3389/fmed.2022.850374.</p><p>Weis et al. (2022). Nat Med 28. doi: 10.1038/s41591-021-01619-9.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93242.4.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>De Waele</surname><given-names>Gaetan</given-names></name><role specific-use="author">Author</role><aff><institution>Ghent University</institution><addr-line><named-content content-type="city">Ghent</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Waegeman</surname><given-names>Willem</given-names></name><role specific-use="author">Author</role><aff><institution>Ghent University</institution><addr-line><named-content content-type="city">Ghent</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Menschaert</surname><given-names>Gerben</given-names></name><role specific-use="author">Author</role><aff><institution>Ghent University</institution><addr-line><named-content content-type="city">Ghent</named-content></addr-line><country>Belgium</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>Section 4.3 (&quot;expert baseline model&quot;): the authors need to explain how the probabilities defined as baselines were exactly used to predict individual patient susceptible profiles.</p></disp-quote><p>We have added a more detailed and mathematically formal explanation of the “simulated expert’s best guess” in Section 4.3.</p><p>This section now reads:</p><p>“More formally, considering all training spectra as <italic>Strain</italic>, all training labels corresponding to one drug <italic>j</italic> and species <italic>t</italic> are gathered: <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">y</mml:mi></mml:mrow><mml:mrow><mml:mtext>subset </mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mtext>train </mml:mtext></mml:mrow></mml:msub><mml:mo>∧</mml:mo><mml:mi>species</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The &quot;simulated expert's best guess&quot; predicted probability for any spectrum <italic>si</italic> and drug <italic>dj</italic>, then, corresponds to, the fraction of positive labels in their corresponding training label set <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mtext>subset </mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>: <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">y</mml:mi></mml:mrow><mml:mrow><mml:mtext>subset </mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mtext>train </mml:mtext></mml:mrow></mml:msub><mml:mo>∧</mml:mo><mml:mi>species</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>”</p><disp-quote content-type="editor-comment"><p>Authors should explain in more detail how a ROC curve is generated from a single spectrum (i.e., per patient) and then average across spectra. I have an idea of how it's done but I am not completely sure.</p></disp-quote><p>We have added a more detailed explanation in Section 3.2. It reads:</p><p>To compute the (per-patient average) ROC-AUC, for any spectrum/patient, all observed drug resistance labels and their corresponding predictions are gathered. Then, the patient-specific ROC-AUC is computed on that subset of labels and predictions. Finally, all ROC-AUCs per patient are averaged to a &quot;spectrum-macro&quot; ROC-AUC.</p><p>In addition, our description under Supplementary Figure 8 (showing the ROC curve) provides additional clarification:</p><p>Note that this ROC curve is not a traditional ROC curve constructed from one single label set and one corresponding prediction set. Rather, it is constructed from spectrum-macro metrics as follows: for any possible threshold value, binarize all predictions. Then, for every spectrum/patient independently, compute the sensitivity and specificity for the subset of labels corresponding to that spectrum/patient. Finally, those sensititivies and specificities are averaged across patients to obtain one point on above ROC curve.</p><disp-quote content-type="editor-comment"><p>Section 3.2 &amp; reply # 1: can the authors compute and apply the Youden cutoff that gives max precision-sensitivity for each ROC curve? In that way the authors could report those values.</p></disp-quote><p>We have computed this cut-off on the curve shown in Supplementary Figure 8. The Figure now shows the sensitivity and specificity at the Youden cutoff in addition to the ROC. We have chosen only to report these values for this model as we did not want to inflate our manuscript with additional metrics (especially since the ROC-AUC already captures sensitivities and specificities). We do, however, see the value of adding this once, so that biologists have an indication of what kind of values to expect for these metrics.</p><disp-quote content-type="editor-comment"><p>Related to reply #5: assuming that different classifiers are trained in the same data, with the same number of replicates, could authors use the DeLong test compare ROC curves? If not, please explain why.</p></disp-quote><p>We thank the reviewer for bringing our attention to the DeLong’s test. It does indeed seem true that this test is appropriate for comparing two ROC-AUCs using the same ground truth values.</p><p>We have chosen not to use this test for one conceptual and one practical reason:</p><p>(1) Our point still stands that in machine learning one chooses the test set, and hence one can artificially increase statistical power by simply allocating a larger fraction of the data to test.</p><p>(2) DeLong’s test is defined for single AUCs (i.e. to compare two lists of predictions against one list of ground truths), but here we report the spectrum/patient-macro ROC-AUC. It is not clear how to adjust the test to macro-evaluated AUCs. One option may be to apply the test per patient ROC curve, and perform multiple testing correction, but then we are not comparing models, but models per patient. In addition, the number of labels/predictions per patient is prohibitively small for statistical power.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p></disp-quote><p>After revision, all issues were been resolved.</p></body></sub-article></article>