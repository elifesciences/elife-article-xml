<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="article-commentary" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">84463</article-id><article-id pub-id-type="doi">10.7554/eLife.84463</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Insight</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Computational Neuroscience</subject></subj-group></article-categories><title-group><article-title>A faster way to model neuronal circuitry</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-128994"><name><surname>Davison</surname><given-names>Andrew P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4793-7541</contrib-id><email>andrew.davison@cnrs.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><bio><p><bold>Andrew P Davison</bold> is in the Institut des Neurosciences Paris-Saclay, Université Paris-Saclay, CNRS, Saclay, France</p></bio></contrib><contrib contrib-type="author" id="author-299230"><name><surname>Appukuttan</surname><given-names>Shailesh</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0148-8023</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><bio><p><bold>Shailesh Appukuttan</bold> is in the Institut des Neurosciences Paris-Saclay, Université Paris-Saclay, CNRS, Saclay, France</p></bio></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002v40q27</institution-id><institution>Institut des Neurosciences Paris-Saclay, Université Paris-Saclay, CNRS</institution></institution-wrap><addr-line><named-content content-type="city">Saclay</named-content></addr-line><country>France</country></aff></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e84463</elocation-id><permissions><copyright-statement>© 2022, Davison and Appukuttan</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Davison and Appukuttan</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-84463-v1.pdf"/><related-article related-article-type="commentary-article" ext-link-type="doi" xlink:href="10.7554/eLife.79535" id="ra1"/><abstract><p>Artificial neural networks could pave the way for efficiently simulating large-scale models of neuronal networks in the nervous system.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational model</kwd><kwd>artificial neural net</kwd><kwd>NMDA</kwd><kwd>cortex</kwd><kwd>deep learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Artificial neural networks could pave the way for efficiently simulating large-scale models of neuronal networks in the nervous system.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></article-meta></front><body><boxed-text><p><bold>Related research article</bold> Oláh VJ, Pedersen NP, Rowan MJM. 2022. Ultrafast simulation of large-scale neocortical microcircuitry with biophysically realistic neurons. <italic>eLife</italic> <bold>11</bold>:e79535. doi: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.7554/eLife.79535">10.7554/eLife.79535</ext-link>.</p></boxed-text><p>Computational modelling and simulation are widely used to help understand the brain. To represent the billions of neurons and trillions of synapses that make up our nervous system, models express electrical and chemical activity mathematically, using equations that they solve with computational methods.</p><p>Coarse-grained models of the brain – where each equation represents the collective activity of hundreds of thousands or millions of neurons – have been valuable in helping us understand the coordination of activity across the whole brain (<xref ref-type="bibr" rid="bib9">Sanz Leon et al., 2013</xref>). The equations from these models can be solved using a normal computer that any researcher might have on their desk. But if we start to investigate how individual neurons and synapses interact to give rise to the collective activity of the brain, the number of equations to be solved becomes enormous. In this case, even powerful supercomputers running flat out for many hours can only simulate the activity of a few cubic millimeters of brain for a few seconds (<xref ref-type="bibr" rid="bib2">Billeh et al., 2020</xref>; <xref ref-type="bibr" rid="bib5">Markram et al., 2015</xref>).</p><p>Now, in eLife, Viktor Oláh, Nigel Pedersen and Matthew Rowan from the Emory University School of Medicine report on a promising new technique that relies on machine learning tools to greatly accelerate simulations of networks of biologically realistic neurons, without the need for supercomputers (<xref ref-type="bibr" rid="bib7">Oláh et al., 2022</xref>).</p><p>Machine learning approaches have become ubiquitous in recent years, whether it be in self-driving cars, computer-generated art or in the computers that have beat grandmasters in chess and Go. One of the most widely-used tools for machine learning is the artificial neural network, or ANN.</p><p>First developed around the middle of the 20th century, ANNs are based on a highly simplified model of how real neurons work (<xref ref-type="bibr" rid="bib6">McCulloch and Pitts, 1943</xref>; <xref ref-type="bibr" rid="bib8">Rosenblatt, 1958</xref>). However, it was only in the early 2000s that their use really took off, due to a combination of increased computing power and theoretical advances that allowed ‘deep learning’ (which involves training ANNs with many layers of artificial neurons; reviewed in <xref ref-type="bibr" rid="bib10">Schmidhuber, 2015</xref>). Each layer in an ANN takes the data from the previous layer as an input, transforms it and feeds it into the next layer, allowing the ANN to perform complex computations (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Illustration of various types of artificial neural networks (ANN) and their associated components.</title><p>(<bold>A</bold>) A basic ANN consists of an input layer (red circles), one or more hidden layers (peach circles), and an output layer (blue circle). In the case of neuronal modelling, the input could be features such as the membrane potential (V<sub>m</sub>), and the excitatory (exc) and inhibitory (inh) synaptic inputs. The hidden layers perform computations on the inputs, with the actual operations depending on the type of ANN. Their objective is to identify features in the inputs and use these to correlate a given input and the correct output. An ANN can have multiple outputs: in this example, the output is a prediction of the membrane potential. (<bold>B</bold>) A deep neural network (DNN) is an ANN with multiple hidden layers. (<bold>C</bold>) A convolutional neural network (CNN) is a type of DNN that can be trained to extract important features contained in the input data, which can then be used as inputs to the other hidden layers, significantly improving the performance of the overall network. (<bold>D</bold>) Some details of the feature extraction process of a CNN, which consists of several hidden layers. First, it has multiple filters (F1, F2, F3), each configured to capture specific features. This process can greatly increase the size of the data, so a pooling layer (P1, P2, P3) is then used to reduce this size. The pooling process does not lead to the loss of valuable data; instead, it helps remove noise and consolidate meaningful data. The flattening layer converts the pooled data into a 1-dimensional stream. This serves as an input for the subsequent fully connected layer, which does the final evaluation to produce the output based on the features extracted by the convolution layers. (<bold>E</bold>) A CNN with a long short-term memory (LSTM) layer. The additional LSTM layer enables the network to benefit from long-term memory, in addition to the existent short-term working memory. (<bold>F</bold>) The LSTM layer achieves this long-term memory through its ability to relay both the cell state (dashed green arrows) and the output generated by each module (solid maroon arrows) across its several modules, allowing the flow of useful information. This enables the network to better identify context in the input data over longer time periods. CNN-LSTMs have been found useful for predicting time series data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84463-fig1-v1.tif"/></fig><p>A type of ANN known as a recurrent network has proven to be highly effective at learning to predict changes over time (<xref ref-type="bibr" rid="bib4">Hewamalage et al., 2021</xref>). In these networks, the activity of a layer of neurons is fed back into itself or into earlier layers, allowing the network to integrate new inputs with its own previous activity. Such ANNs have been used for stock market predictions, machine translation, to accelerate weather and climate change simulations (review in <xref ref-type="bibr" rid="bib3">Chantry et al., 2021</xref>), and to predict the electrical activity of individual biological neurons (<xref ref-type="bibr" rid="bib1">Beniaguev et al., 2021</xref>; <xref ref-type="bibr" rid="bib11">Wang et al., 2022</xref>). Oláh et al. have now developed ANNs that can predict the activity of entire networks of biologically realistic neurons with good levels of accuracy.</p><p>First, the team tested several different ANN architectures, and found that a particular type of recurrent neural network – which they call a convolutional neural network with long short-term memory (CNN-LSTM) – was able to accurately predict not only the sub-threshold activity but also the shape and timing of action potentials of neurons. For single neurons, their approach was comparable in speed to traditional simulators. However, when they simulated networks made up of many similar neurons, the performance of the CNN-LSTM was much better, becoming over 10,000 times faster than traditional simulators in certain cases.</p><p>In summary, the work of Oláh et al. shows that ANNs are a promising tool for greatly increasing the scope of what can be modelled with generally available computing hardware, reducing the bottleneck of supercomputer availability. Further studies will be needed to better understand the tradeoffs between performance and accuracy for this approach. By clearly describing the successful CNN-LSTM model and providing their source code in a public repository, Oláh et al. have laid a strong foundation for such future exploration.</p></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beniaguev</surname><given-names>D</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>London</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Single cortical neurons as deep artificial neural networks</article-title><source>Neuron</source><volume>109</volume><fpage>2727</fpage><lpage>2739</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.002</pub-id><pub-id pub-id-type="pmid">34380016</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Cai</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Systematic integration of structural and functional data into multi-scale models of mouse primary visual cortex</article-title><source>Neuron</source><volume>106</volume><fpage>388</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.040</pub-id><pub-id pub-id-type="pmid">32142648</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chantry</surname><given-names>M</given-names></name><name><surname>Christensen</surname><given-names>H</given-names></name><name><surname>Dueben</surname><given-names>P</given-names></name><name><surname>Palmer</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Opportunities and challenges for machine learning in weather and climate modelling: hard, medium and soft AI</article-title><source>Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences</source><volume>379</volume><elocation-id>20200083</elocation-id><pub-id pub-id-type="doi">10.1098/rsta.2020.0083</pub-id><pub-id pub-id-type="pmid">33583261</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hewamalage</surname><given-names>H</given-names></name><name><surname>Bergmeir</surname><given-names>C</given-names></name><name><surname>Bandara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Recurrent neural networks for time series forecasting: current status and future directions</article-title><source>International Journal of Forecasting</source><volume>37</volume><fpage>388</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/j.ijforecast.2020.06.008</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Ramaswamy</surname><given-names>S</given-names></name><name><surname>Reimann</surname><given-names>MW</given-names></name><name><surname>Abdellah</surname><given-names>M</given-names></name><name><surname>Sanchez</surname><given-names>CA</given-names></name><name><surname>Ailamaki</surname><given-names>A</given-names></name><name><surname>Alonso-Nanclares</surname><given-names>L</given-names></name><name><surname>Antille</surname><given-names>N</given-names></name><name><surname>Arsever</surname><given-names>S</given-names></name><name><surname>Kahou</surname><given-names>GAA</given-names></name><name><surname>Berger</surname><given-names>TK</given-names></name><name><surname>Bilgili</surname><given-names>A</given-names></name><name><surname>Buncic</surname><given-names>N</given-names></name><name><surname>Chalimourda</surname><given-names>A</given-names></name><name><surname>Chindemi</surname><given-names>G</given-names></name><name><surname>Courcol</surname><given-names>JD</given-names></name><name><surname>Delalondre</surname><given-names>F</given-names></name><name><surname>Delattre</surname><given-names>V</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Dumusc</surname><given-names>R</given-names></name><name><surname>Dynes</surname><given-names>J</given-names></name><name><surname>Eilemann</surname><given-names>S</given-names></name><name><surname>Gal</surname><given-names>E</given-names></name><name><surname>Gevaert</surname><given-names>ME</given-names></name><name><surname>Ghobril</surname><given-names>JP</given-names></name><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Graham</surname><given-names>JW</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Haenel</surname><given-names>V</given-names></name><name><surname>Hay</surname><given-names>E</given-names></name><name><surname>Heinis</surname><given-names>T</given-names></name><name><surname>Hernando</surname><given-names>JB</given-names></name><name><surname>Hines</surname><given-names>M</given-names></name><name><surname>Kanari</surname><given-names>L</given-names></name><name><surname>Keller</surname><given-names>D</given-names></name><name><surname>Kenyon</surname><given-names>J</given-names></name><name><surname>Khazen</surname><given-names>G</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>King</surname><given-names>JG</given-names></name><name><surname>Kisvarday</surname><given-names>Z</given-names></name><name><surname>Kumbhar</surname><given-names>P</given-names></name><name><surname>Lasserre</surname><given-names>S</given-names></name><name><surname>Le Bé</surname><given-names>JV</given-names></name><name><surname>Magalhães</surname><given-names>BRC</given-names></name><name><surname>Merchán-Pérez</surname><given-names>A</given-names></name><name><surname>Meystre</surname><given-names>J</given-names></name><name><surname>Morrice</surname><given-names>BR</given-names></name><name><surname>Muller</surname><given-names>J</given-names></name><name><surname>Muñoz-Céspedes</surname><given-names>A</given-names></name><name><surname>Muralidhar</surname><given-names>S</given-names></name><name><surname>Muthurasa</surname><given-names>K</given-names></name><name><surname>Nachbaur</surname><given-names>D</given-names></name><name><surname>Newton</surname><given-names>TH</given-names></name><name><surname>Nolte</surname><given-names>M</given-names></name><name><surname>Ovcharenko</surname><given-names>A</given-names></name><name><surname>Palacios</surname><given-names>J</given-names></name><name><surname>Pastor</surname><given-names>L</given-names></name><name><surname>Perin</surname><given-names>R</given-names></name><name><surname>Ranjan</surname><given-names>R</given-names></name><name><surname>Riachi</surname><given-names>I</given-names></name><name><surname>Rodríguez</surname><given-names>JR</given-names></name><name><surname>Riquelme</surname><given-names>JL</given-names></name><name><surname>Rössert</surname><given-names>C</given-names></name><name><surname>Sfyrakis</surname><given-names>K</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Shillcock</surname><given-names>JC</given-names></name><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>Silva</surname><given-names>R</given-names></name><name><surname>Tauheed</surname><given-names>F</given-names></name><name><surname>Telefont</surname><given-names>M</given-names></name><name><surname>Toledo-Rodriguez</surname><given-names>M</given-names></name><name><surname>Tränkler</surname><given-names>T</given-names></name><name><surname>Van Geit</surname><given-names>W</given-names></name><name><surname>Díaz</surname><given-names>JV</given-names></name><name><surname>Walker</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zaninetta</surname><given-names>SM</given-names></name><name><surname>DeFelipe</surname><given-names>J</given-names></name><name><surname>Hill</surname><given-names>SL</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>Schürmann</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reconstruction and simulation of neocortical microcircuitry</article-title><source>Cell</source><volume>163</volume><fpage>456</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.09.029</pub-id><pub-id pub-id-type="pmid">26451489</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1943">1943</year><article-title>A logical calculus of the ideas immanent in nervous activity</article-title><source>The Bulletin of Mathematical Biophysics</source><volume>5</volume><fpage>115</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1007/BF02478259</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oláh</surname><given-names>VJ</given-names></name><name><surname>Pedersen</surname><given-names>NP</given-names></name><name><surname>Rowan</surname><given-names>MJM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Ultrafast simulation of large-scale neocortical microcircuitry with biophysically realistic neurons</article-title><source>eLife</source><volume>11</volume><elocation-id>e79535</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.79535</pub-id><pub-id pub-id-type="pmid">36341568</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title><source>Psychological Review</source><volume>65</volume><fpage>386</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1037/h0042519</pub-id><pub-id pub-id-type="pmid">13602029</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanz Leon</surname><given-names>P</given-names></name><name><surname>Knock</surname><given-names>SA</given-names></name><name><surname>Woodman</surname><given-names>MM</given-names></name><name><surname>Domide</surname><given-names>L</given-names></name><name><surname>Mersmann</surname><given-names>J</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The virtual brain: a simulator of primate brain network dynamics</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00010</pub-id><pub-id pub-id-type="pmid">23781198</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Scholarpedia</source><volume>10</volume><elocation-id>32832</elocation-id><pub-id pub-id-type="doi">10.4249/scholarpedia.32832</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Cao</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Predicting spike features of hodgkin-huxley-type neurons with simple artificial neural network</article-title><source>Frontiers in Computational Neuroscience</source><volume>15</volume><elocation-id>800875</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2021.800875</pub-id><pub-id pub-id-type="pmid">35197835</pub-id></element-citation></ref></ref-list></back></article>