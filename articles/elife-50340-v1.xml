<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">50340</article-id><article-id pub-id-type="doi">10.7554/eLife.50340</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Experience shapes activity dynamics and stimulus coding of VIP inhibitory cells</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-60002"><name><surname>Garrett</surname><given-names>Marina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5271-2291</contrib-id><email>marinag@alleninstitute.org</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152183"><name><surname>Manavi</surname><given-names>Sahar</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-121422"><name><surname>Roll</surname><given-names>Kate</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-95964"><name><surname>Ollerenshaw</surname><given-names>Douglas R</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152184"><name><surname>Groblewski</surname><given-names>Peter A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8415-1118</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-172399"><name><surname>Ponvert</surname><given-names>Nicholas D</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152185"><name><surname>Kiggins</surname><given-names>Justin T</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152187"><name><surname>Casal</surname><given-names>Linzy</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152188"><name><surname>Mace</surname><given-names>Kyla</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152189"><name><surname>Williford</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152190"><name><surname>Leon</surname><given-names>Arielle</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-116806"><name><surname>Jia</surname><given-names>Xiaoxuan</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-159460"><name><surname>Ledochowitsch</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-95966"><name><surname>Buice</surname><given-names>Michael A</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-172400"><name><surname>Wakeman</surname><given-names>Wayne</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-94457"><name><surname>Mihalas</surname><given-names>Stefan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2629-7100</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-73112"><name><surname>Olsen</surname><given-names>Shawn R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9568-7057</contrib-id><email>shawno@alleninstitute.org</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con17"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution>Allen Institute for Brain Science</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution>CNRS</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>02</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e50340</elocation-id><history><date date-type="received" iso-8601-date="2019-07-19"><day>19</day><month>07</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-02-05"><day>05</day><month>02</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Garrett et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Garrett et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-50340-v1.pdf"/><abstract><p>Cortical circuits can flexibly change with experience and learning, but the effects on specific cell types, including distinct inhibitory types, are not well understood. Here we investigated how excitatory and VIP inhibitory cells in layer 2/3 of mouse visual cortex were impacted by visual experience in the context of a behavioral task. Mice learned a visual change detection task with a set of eight natural scene images. Subsequently, during 2-photon imaging experiments, mice performed the task with these familiar images and three sets of novel images. Strikingly, the temporal dynamics of VIP activity differed markedly between novel and familiar images: VIP cells were stimulus-driven by novel images but were suppressed by familiar stimuli and showed ramping activity when expected stimuli were omitted from a temporally predictable sequence. This prominent change in VIP activity suggests that these cells may adopt different modes of processing under novel versus familiar conditions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual system</kwd><kwd>behavior</kwd><kwd>learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Allen Institute for Brain Science</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Garrett</surname><given-names>Marina</given-names></name><name><surname>Manavi</surname><given-names>Sahar</given-names></name><name><surname>Roll</surname><given-names>Kate</given-names></name><name><surname>Ollerenshaw</surname><given-names>Douglas R</given-names></name><name><surname>Groblewski</surname><given-names>Peter A</given-names></name><name><surname>Kiggins</surname><given-names>Justin T</given-names></name><name><surname>Casal</surname><given-names>Linzy</given-names></name><name><surname>Mace</surname><given-names>Kyla</given-names></name><name><surname>Williford</surname><given-names>Ali</given-names></name><name><surname>Leon</surname><given-names>Arielle</given-names></name><name><surname>Jia</surname><given-names>Xiaoxuan</given-names></name><name><surname>Mihalas</surname><given-names>Stefan</given-names></name><name><surname>Ponvert</surname><given-names>Nicholas D</given-names></name><name><surname>Ledochowitsch</surname><given-names>Peter</given-names></name><name><surname>Buice</surname><given-names>Michael A</given-names></name><name><surname>Wakeman</surname><given-names>Wayne</given-names></name><name><surname>Olsen</surname><given-names>Shawn R</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>VIP inhibitory neurons are active in response to novel images but are suppressed by familiar stimuli and show ramping activity when expected stimuli are omitted.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neural circuits are dynamically shaped by experience and learned expectations (<xref ref-type="bibr" rid="bib7">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">LeMessurier and Feldman, 2018</xref>; <xref ref-type="bibr" rid="bib49">Pakan et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Ranganath and Rainer, 2003</xref>). Visual experience can modify cortical representations, including changes in gain, selectivity, correlations, and population dynamics (<xref ref-type="bibr" rid="bib25">Jurjut et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Khan et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Makino and Komiyama, 2015</xref>; <xref ref-type="bibr" rid="bib56">Poort et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Weskelblatt and Niell, 2019</xref>; <xref ref-type="bibr" rid="bib70">Woloszyn and Sheinberg, 2012</xref>). Moreover, sensory and behavioral experience can lead to the emergence of predictive activity in the visual cortex including reward anticipation (<xref ref-type="bibr" rid="bib56">Poort et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Shuler and Bear, 2006</xref>), spatial expectation (<xref ref-type="bibr" rid="bib10">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Saleem et al., 2018</xref>), anticipatory recall (<xref ref-type="bibr" rid="bib15">Gavornik and Bear, 2014</xref>; <xref ref-type="bibr" rid="bib71">Xu et al., 2012</xref>) and prediction error signals (<xref ref-type="bibr" rid="bib10">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Hamm and Yuste, 2016</xref>; <xref ref-type="bibr" rid="bib21">Homann et al., 2017</xref>). These learning-related changes in sensory cortex can involve top-down feedback (<xref ref-type="bibr" rid="bib10">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Makino and Komiyama, 2015</xref>; <xref ref-type="bibr" rid="bib52">Petro et al., 2014</xref>; <xref ref-type="bibr" rid="bib73">Zhang et al., 2014</xref>) and neuromodulatory inputs (<xref ref-type="bibr" rid="bib5">Chubykin et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Kuchibhotla et al., 2017</xref>; <xref ref-type="bibr" rid="bib55">Pinto et al., 2013</xref>), and may be associated with a shift in the balance of bottom-up sensory and top-down contextual signals (<xref ref-type="bibr" rid="bib3">Batista-Brito et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Khan and Hofer, 2018</xref>). Inhibitory interneurons likely play a key role in this process by dynamically regulating the flow of information (<xref ref-type="bibr" rid="bib20">Hangya et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Kepecs and Fishell, 2014</xref>; <xref ref-type="bibr" rid="bib66">Wang and Yang, 2018</xref>). Elucidating how different cell populations, particularly inhibitory cells, contribute to experience-dependent changes in sensory coding is critical to understand the dynamic nature of cortical circuits.</p><p>Vasoactive intestinal peptide (VIP) expressing cells comprise a major class of inhibitory neurons and are well-positioned to mediate top-down and neuromodulatory influences on local circuits in sensory cortex. VIP cells receive long-range projections from frontal areas (<xref ref-type="bibr" rid="bib33">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib65">Wall et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Zhang et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Zhang et al., 2014</xref>) as well as cholinergic and noradrenergic inputs (<xref ref-type="bibr" rid="bib1">Alitto and Dan, 2013</xref>; <xref ref-type="bibr" rid="bib11">Fu et al., 2014</xref>). VIP cells are highly active during states of arousal (<xref ref-type="bibr" rid="bib11">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Reimer et al., 2014</xref>), are modulated by task engagement (<xref ref-type="bibr" rid="bib31">Kuchibhotla et al., 2017</xref>), and are responsive to behavioral reinforcement (<xref ref-type="bibr" rid="bib30">Krabbe et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Letzkus et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Pi et al., 2013</xref>). In the local cortical circuitry, VIP cells primarily inhibit another major class of inhibitory interneuron, somatostatin (SST) cells (<xref ref-type="bibr" rid="bib33">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Munoz et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Pfeffer et al., 2013</xref>; <xref ref-type="bibr" rid="bib54">Pi et al., 2013</xref>), which can result in disinhibition of excitatory neurons (<xref ref-type="bibr" rid="bib12">Fu et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Letzkus et al., 2011</xref>). SST cells target the apical dendrites of pyramidal neurons (<xref ref-type="bibr" rid="bib27">Kepecs and Fishell, 2014</xref>) and removal of this inhibition may facilitate the association of top-down and bottom-up input by pyramidal cells (<xref ref-type="bibr" rid="bib4">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Larkum, 2013</xref>; <xref ref-type="bibr" rid="bib38">Makino and Komiyama, 2015</xref>). However, little is known about how VIP cell activity is modified by visual experience.</p><p>Here we investigated how long-term behavioral experience with natural scene images alters activity of cortical VIP inhibitory and excitatory pyramidal cells in layers 2/3 of mouse visual cortex. Mice were trained to perform a change detection task in which images were presented in a periodic manner and mice were rewarded for detecting changes in image identity. Mice learned the task with one set of eight natural images, which were viewed thousands of times and were thus highly familiar. During subsequent 2-photon imaging, these familiar images as well as three novel image sets were tested. Familiar images were associated with lower overall population activity in both excitatory and VIP cells. Notably, VIP inhibitory cells had distinct activity dynamics during sessions with familiar versus novel images. VIP cells were stimulus-driven by novel images but displayed ramping activity between presentations of familiar images and were suppressed by stimulus onset. These cells showed even greater ramping activity when an expected stimulus was omitted from the regular image sequence. Overall, these results show distinct experience-dependent changes in two cortical cell classes and suggest that VIP cells may adopt different modes of processing during familiar versus novel conditions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Visual change detection task with familiar and novel images</title><p>We trained mice on a go/no-go visual change detection task with natural scene stimuli. In this task, mice see a continuous stream of repeatedly presented images (250 ms stimulus presentation followed by 500 ms gray screen; <xref ref-type="fig" rid="fig1">Figure 1A,B</xref>). On ‘go’ trials, the image identity changes and mice report the change by licking a reward spout within 750 ms (<xref ref-type="fig" rid="fig1">Figure 1B,C</xref>). False alarms are quantified during ‘catch’ trials when the image does not change. To test whether expectation signals exist in the visual cortex due to the temporal regularity of this task, we randomly omitted ~5% of all image presentations (not including image changes to avoid interfering with behavior performance). These omissions appeared as an extended gray period to the mouse and corresponded to a gap in the periodic timing of stimuli (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Natural image change detection task with familiar and novel images.</title><p>(<bold>A</bold>) Schematic of stimulus presentation during the task. Images are presented for 250 ms followed by 500 ms of gray screen. (<bold>B</bold>) Trial structure. Colors represent different images. On go trials, the image identity changes and mice must lick within the 750 ms response window to receive a water reward. On catch trials no image change occurs and the behavioral response is measured to quantify guessing behavior. (<bold>C</bold>) Example lick raster, aligned to the image change time. Purple dots indicate rewards and green ticks are reward consumption licks. Red ticks indicate incorrect licking responses outside the response window. (<bold>D</bold>) Example behavior performance over four minutes of one session, separated into one-minute epochs. Colored vertical bars indicate stimulus presentations (different colors are different images). Green tick marks indicate licks, purple triangles indicate rewards. 5% of all non-change image flashes are omitted, visible as a gap in the otherwise regular stimulus sequence. (<bold>E</bold>) Training stages. Mice are initially trained with gratings of 2 orientations, first with no intervening gray screen (stage 1), then with a 500 ms inter-stimulus delay (stage 2). Next, mice perform change detection with eight natural scene images (stage 3, image set A). During the 2-photon imaging portion of the experiment, mice are tested with image set A as well as three novel image sets (B, C, D) on subsequent days. (<bold>F</bold>) The four sets of 8 natural images. Image set A is the familiar training set, and image sets B, C and D were the novel sets shown for the first time during 2-photon imaging. (<bold>G</bold>) Example training time course of one mouse. (<bold>H</bold>) Number of sessions spent in each stage across mice. Mean ± 95% confidence intervals in color, individual mice in gray. (<bold>I</bold>) Response rates for go and catch trials are similar across image sets. Individual behavior sessions are shown in gray and average ± 95% confidence intervals across sessions for each image set are shown in color. (J) Reaction times, measured as latency to first lick, are not significantly different across image sets. Mean ± 95% confidence intervals in color, individual sessions in gray.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Behavior is similar across image sets.</title><p>(<bold>A</bold>) Number of sessions in each training stage for all mice in the study. Slc17a7+ mice are shown in green and VIP+ mice are shown in purple. Inset shows color legend for training stages. (<bold>B</bold>) Average running speed is similar across image sets (p&gt;0.05 for all image set comparisons, Welch’s t-test used for all statistical comparisons, see Materials and methods for additional details). Mean running speed is computed for each stimulus presentation within a session then averaged across the session. Gray lines indicate session average running speed for each mouse, colors indicate average across mice with 95% confidence intervals. (<bold>C</bold>) Average change triggered running speed is similar across image sets. Right panel shows slowing of running behavior following a stimulus change, with median lick latency (values from <xref ref-type="fig" rid="fig1">Figure 1J</xref>) for each image set indicated by arrows and dotted lines. (<bold>D</bold>) Mean d-prime is similar across image sets (p&gt;0.05 for all image set pairs except A-B, where p=0.002). (<bold>E</bold>) Average licking response rate (± SEM) measured for image change trials, catch trials, all non-change stimulus presentations, stimulus omissions, and for the stimulus presentation immediately following an omitted stimulus. (<bold>F</bold>) Pattern of mouse licking behavior around the time of stimulus omission is similar across image sets. Gray bars show histogram of lick counts in each time bin pooled over all mice (left y-axis). Colored lines show kernel density estimate of lick distribution over time for each image set (right y-axis).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig1-figsupp1-v1.tif"/></fig></fig-group><p>Mice learned the task through a series training stages, starting with oriented gratings and then progressing to natural images (<xref ref-type="fig" rid="fig1">Figure 1E–G</xref>; see Materials and methods for additional details about training procedure). During the natural image stage, mice were trained with one set of eight images (image set A) for an extended number of sessions (range = 6–46 sessions with image set A, median = 17 sessions; <xref ref-type="fig" rid="fig1">Figure 1H</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). On average, mice viewed each of the eight images from the familiar set 10,350 times prior to the 2-photon imaging stage (range: 944–26,784 individual stimulus presentations per image).</p><p>During the 2-photon imaging portion of the experiment, mice performed the task with either the familiar image set or one of three additional novel image sets (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Hit rates, false alarm rates, and reaction times (lick latency) were similar across image sets (<xref ref-type="fig" rid="fig1">Figure 1I,J</xref>). During the task, mice are free to run on a circular disk and typically stop running to lick. There was no difference in running behavior between novel and familiar image sessions (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B,C</xref>). Licking behavior around the time of omission was also similar across image sets (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E,F</xref>). Together these results show that mouse behavior was similar for novel and familiar images.</p></sec><sec id="s2-2"><title>Imaging excitatory and VIP inhibitory cell populations during task performance</title><p>We imaged activity in transgenic mice expressing the calcium indicator GCaMP6f in excitatory pyramidal cells (Slc17a7-IRES2-Cre; CaMKII-tTA; Ai93-GCaMP6f) or VIP inhibitory cells (VIP-IRES-Cre; Ai148-GCaMP6f) (see <xref ref-type="table" rid="table1">Table 1</xref> for numbers of mice, sessions, and cells in the dataset). On average we imaged 181 ± 77 (mean ± SD) Slc17a7+ cells or 15 ± 10 VIP+ cells per session. Measurements were made in primary visual cortex (VISp) and one higher visual area (VISal) but we did not observe major differences between these two areas, so the datasets were combined for the analyses reported here. Because calcium signals have a slow decay time that could lead to an artificial enhancement of the response to stimuli shown close together in time, we performed event detection to identify the onset timing of ‘spike’ events underlying the GCaMP signal (<xref ref-type="bibr" rid="bib8">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Jewell and Witten, 2017</xref>; <xref ref-type="bibr" rid="bib22">Jewell et al., 2019</xref>). This method produces a timeseries of detected events which have a magnitude proportional to the change in calcium activity. Event magnitude (arbitrary units) was used for all subsequent analysis.</p><p>Excitatory cells typically responded to only one or a few images in each set, showing fluorescence increases after stimulus onset or sometimes after stimulus offset (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>). VIP cells were less image selective and showed correlated fluctuations in activity (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>). Interestingly, VIP cells had distinct activity dynamics relative to stimulus onset in novel versus familiar image sessions. Novel images generated stimulus-locked activity in VIP cells (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), but this was reduced or absent with familiar images. Instead, during familiar image sessions many VIP neurons had ramping activity that preceded stimulus presentation and decayed after stimulus onset (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). These ramping responses were even more pronounced when an image presentation was omitted (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, right panel). In contrast, VIP cells showed little activity during the omission of novel image presentations (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, right panel).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Activity in layer 2/3 excitatory and VIP inhibitory cells during change detection task.</title><p>(<bold>A</bold>) Example cell responses to the familiar image set A, from excitatory cells in layer 2/3 of an Slc17a7-IRES2-Cre;CaMkII-tTa;Ai93 mouse expressing GCaMP6f. Left panel: dF/F traces (gray) from four excitatory cells over a 90 second epoch of a behavior session (scale bars on left indicate 75% dF/F). Deconvolved events are shown in blue below the dF/F trace. Colored vertical bars indicate image presentation times; timing of licks and reward delivery are shown at bottom. Right panel: response of the same 4 cells to each image, as well activity during stimulus omission (right column, gray shading indicates the time where a stimulus would have been displayed). Scale bars indicate 0.05 event magnitude in arbitrary units. (<bold>B</bold>) Example excitatory cells from a session with novel image set C. Left panel scale bars indicate 100% dF/F, right panel scale bars indicate event magnitude of 0.05. (<bold>C</bold>) Example VIP inhibitory cells from layer 2/3 of a VIP-IRES-Cre;Ai148 mouse expressing GCaMP6f, from a session with familiar image set A. Left panel scale bars indicate 225% dF/F, right panel scale bars indicate event magnitude of 0.05. (<bold>D</bold>) Example VIP inhibitory cells for a session with novel image set C. Left panel scale bars indicate 200% dF/F, right panel scale bars indicate event magnitude of 0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig2-v1.tif"/></fig><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Number of mice, sessions, and cells in dataset.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">cell class</th><th align="center" valign="bottom">image set</th><th align="center" valign="bottom">mice</th><th align="center" valign="bottom">sessions</th><th align="center" valign="bottom">total cells</th></tr></thead><tbody><tr><td valign="bottom">Excitatory</td><td align="center" valign="bottom">A</td><td align="center" valign="bottom">11</td><td align="center" valign="bottom">13</td><td align="center" valign="bottom">2046</td></tr><tr><td valign="bottom">Excitatory</td><td align="center" valign="bottom">B</td><td align="center" valign="bottom">11</td><td align="center" valign="bottom">13</td><td align="center" valign="bottom">2594</td></tr><tr><td valign="bottom">Excitatory</td><td align="center" valign="bottom">C</td><td align="center" valign="bottom">11</td><td align="center" valign="bottom">13</td><td align="center" valign="bottom">2172</td></tr><tr><td valign="bottom">Excitatory</td><td align="center" valign="bottom">D</td><td align="center" valign="bottom">11</td><td align="center" valign="bottom">12</td><td align="center" valign="bottom">2232</td></tr><tr><td valign="bottom">VIP Inhibitory</td><td align="center" valign="bottom">A</td><td align="center" valign="bottom">10</td><td align="center" valign="bottom">13</td><td align="center" valign="bottom">183</td></tr><tr><td valign="bottom">VIP Inhibitory</td><td align="center" valign="bottom">B</td><td align="center" valign="bottom">10</td><td align="center" valign="bottom">13</td><td align="center" valign="bottom">209</td></tr><tr><td valign="bottom">VIP Inhibitory</td><td align="center" valign="bottom">C</td><td align="center" valign="bottom">10</td><td align="center" valign="bottom">12</td><td align="center" valign="bottom">186</td></tr><tr><td valign="bottom">VIP Inhibitory</td><td align="center" valign="bottom">D</td><td align="center" valign="bottom">10</td><td align="center" valign="bottom">12</td><td align="center" valign="bottom">175</td></tr></tbody></table></table-wrap><p>These differences in image responsiveness and temporal dynamics, already evident in single cell activity, are further quantified across the population of recorded neurons in the subsequent sections.</p></sec><sec id="s2-3"><title>Reduced image-evoked activity with familiar image sets</title><p>We used a heatmap to visualize activity of the full population of recorded neurons to the eight stimuli for each image set, as well as for omitted stimuli (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Most excitatory neurons responded to one of the eight images from a given set and showed little activity when stimuli were omitted. VIP neurons could also show robust image responses, particularly for the novel image sets. In contrast, during sessions with familiar image sets, VIP neurons were most strongly active during the extended gray screen period when stimuli were omitted (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Reduced image-evoked activity for familiar stimuli.</title><p>(<bold>A</bold>) Heatmap showing the mean response of excitatory (left panels) and VIP inhibitory (right panels) cells to images and stimulus omission for familiar and novel image sets. Response is computed using detected events in a 500 ms window after stimulus onset and averaged over all presentations of a given image or image omission. (<bold>B</bold>) Cumulative distribution of response magnitude for each cell’s preferred image (excluding omissions) demonstrating reduced image-evoked activity for familiar compared to novel image sets. Insets show comparisons with p&lt;0.008 (Welch’s t-test with Bonferroni correction was used for all statistical comparisons, see Materials and methods for additional details). (<bold>C</bold>) Cumulative distribution of omission response magnitude across cells, demonstrating increased activity during stimulus omission for the familiar image set A. Insets are as described in panel <bold>B</bold>. (<bold>D</bold>) Fraction of image responsive cells is higher for novel image sets compared to the familiar image set. Image responsiveness is defined for each cell as having &gt;25% of preferred image stimulus presentations with a significant response compared to a shuffled distribution of values taken from omission periods with extended gray screen. The fraction of image responsive cells is the number of cells within each session that meet the criterion for image responsiveness. Individual sessions are shown in gray, with mean across sessions ± 95% confidence intervals in color. p&lt;0.008 for all comparisons with image set A. (<bold>E</bold>) Fraction of omission responsive cells is higher for the familiar image set in VIP inhibitory cells. Omission responsiveness is defined for each cell as having &gt;10% of stimulus omissions with a significant response compared to a shuffled distribution of values taken from image presentations. The fraction of omission responsive cells is the number of cells within each session that meet the criterion for omission responsiveness. Individual sessions are shown in gray, with mean across sessions ± 95% confidence intervals in color. p&lt;0.008 for A-B and A-C in VIP cells.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Response sparseness for familiar and novel images.</title><p>(<bold>A</bold>) Cumulative distribution of lifetime sparseness values for cells meeting the criteria for image responsiveness described in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. Equation for lifetime sparseness is described in Materials and methods. Briefly, this value indicates whether a cell is highly selective in its response across images (value = 1), or whether it responds equally across images (value = 0). For excitatory cells, inset shows comparisons where p&lt;0.008 (statistics described in Materials and methods). For VIP cells, no cells met the criteria for image responsiveness to the familiar image set A, and there were no significant differences across novel image sets. (<bold>B</bold>) Population tuning curves averaged across all cells meeting the criteria for image responsiveness. Each cell’s response across images within an image set was rank sorted, then averaged. The excitatory population of image responsive cells shows an enhanced response to the preferred image. No image responsive cells for the familiar image set were present in the VIP inhibitory population, so no tuning curve for image set A is shown.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig3-figsupp1-v1.tif"/></fig></fig-group><p>Quantifying each cell’s mean response to its preferred image in each set revealed that both excitatory and VIP inhibitory populations had reduced activity levels with familiar images (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, p&lt;0.008 for all comparisons with image set A for both excitatory and VIP inhibitory populations, except comparison between A-C for excitatory neurons where p=0.05, Welch’s t-test with Bonferroni correction, see Materials and methods for detailed description of statistics used throughout). Both VIP and excitatory cells showed increased stimulus omission activity in familiar image sessions (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, p&lt;0.008 for all comparisons with image set A). The fraction of cells that were significantly more responsive during image presentations compared to stimulus omission was higher for novel image sets for both cell classes (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, p&lt;0.008 for all comparisons with image set A). While excitatory cells had a small but significant increase in the fraction of omission responsive cells (2.3% for image set A versus &lt;1% for image sets B-D), a large fraction of VIP cells were omission responsive with the familiar image set compared to novel image sets (55% for image set A, 6–20% for image sets B-D) (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, for VIP: p&lt;0.008 for A-B and A-C, p=0.03 for A-D; for excitatory: p=0.01 for A-B and A-C, p=0.05 for A-D). This indicates a trade-off between image responsiveness and omission activity in VIP cells.</p><p>We also observed that the responses of individual excitatory neurons to natural images were more selective for familiar compared to novel stimuli. To evaluate single cell image selectivity, we quantified lifetime sparseness (<xref ref-type="bibr" rid="bib64">Vinje and Gallant, 2000</xref>) for image responsive cells. Excitatory populations had higher lifetime sparseness values for the familiar image set compared to the novel image sets (p&lt;0.008 for all comparisons with image set A and for B-C for excitatory cells), and excitatory cells were typically sparser than VIP cells (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). Plotting the population tuning curve for each image set revealed sharper tuning in image responsive excitatory cells for familiar images due to a selective increase in the preferred image response (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>), consistent with previous literature (<xref ref-type="bibr" rid="bib70">Woloszyn and Sheinberg, 2012</xref>).Together, these results demonstrate that while overall population activity levels were reduced for familiar images, single cell selectivity was sharpened.</p></sec><sec id="s2-4"><title>Inter-stimulus activity dynamics of VIP cells are altered by training history</title><p>Next we investigated the temporal dynamics of activity during stimulus presentation and the preceding inter-stimulus interval by examining the average population activity of VIP and excitatory cells for each each image set (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The excitatory population showed a sharp increase in activity following stimulus onset, and while familiar images evoked a smaller population response magnitude, the timing of activity was similar across novel and familiar image sets (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, left panel). In contrast, VIP population dynamics were very different between familiar and novel images sets. With novel images, the VIP population response increased following stimulus onset, but with familiar images, VIP activity ramped up during the inter-stimulus interval and peaked at the time of stimulus onset (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This pre-stimulus ramping activity was readily apparent in individual VIP cells but was rare in excitatory cells (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Consistent with this effect, the distribution of peak response times across VIP cells was shifted earlier in time for familiar versus novel images sets (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, p&lt;0.008 for all image set comparisons except B-C for VIP cells). Excitatory cells also showed a small but significant difference in the peak time distribution across image sets (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, p&lt;0.008 for all comparisons with image set A, as well as B-C for excitatory cells).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Experience-dependent shift in the dynamics of VIP inhibitory cells.</title><p>(<bold>A</bold>) Population activity averaged over all image presentations for excitatory (left panel) and VIP inhibitory cells (right panel). Traces show mean ± SEM across cells. Note distinct dynamics in VIP population for novel versus familiar images sets. (<bold>B</bold>) Example single cell traces showing average image-evoked response for excitatory (left panel) and VIP inhibitory cells (right panel). Background shading denotes stimulus presentation, with color indicating the image set shown during the session for that cell. Each trace represents a unique cell recorded in a single session. Scale bar on left of each trace indicates a response magnitude of 0.005. (<bold>C</bold>) Histogram of time to peak response after stimulus onset for excitatory (left panel) and VIP inhibitory cells (right panel). Inset shows comparisons across image sets where p&lt;0.008 (Welch’s t-test with Bonferroni correction was used for all statistical comparisons, see Materials and methods for additional details). (<bold>D</bold>) Stimulus ramping and pre-stimulus ramping are negatively correlated. Each point is one neuron. The stimulus ramp index was computed over a 125 ms window after stimulus onset. The pre-stimulus ramp index was computed over a 400 ms window prior to stimulus onset. Data points across all image sets for each panel were fit with linear least-squares regression. Correlation and significance values for the fit are shown in lower left of each panel. (<bold>E</bold>) Novel image sets have an increased fraction of cells with stimulus-driven activity, whereas a larger fraction of cells was stimulus-suppressed for familiar images. Cells with a positive stimulus ramp index are considered stimulus-driven and those with a negative ramp index are stimulus-suppressed. p&lt;0.008 for all comparisons with image set A. (<bold>F</bold>) Population average image evoked response for cells that met the criteria for stimulus driven or stimulus-suppressed, as described in panel <bold>E</bold>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Changes in image-evoked VIP dynamics are not explained by running behavior or pupil diameter.</title><p>(<bold>A</bold>) Average image-evoked response for excitatory (left panel) and VIP inhibitory cells (right panel) for image presentations where mice were running (mean running speed during the [−0.5, 0.75] second window around stimulus onset was greater than 5 cm/s). Mean image response during running was computed across all running image presentations for each cell, then averaged across cells. Traces show mean ± SEM across cells. (<bold>B</bold>) Averaged image-locked running speed for image presentations where mice were stationary (mean running speed during the [−0.5, 0.75] second window around stimulus onset was less than 5 cm/s). (<bold>C</bold>) Average stimulus-triggered running speed across image sets during image presentations where mice were running. Mean running speed was first calculated across all running image presentations within one session, then averaged across sessions. Traces show mean ± SEM across sessions. (<bold>D</bold>) Average stimulus-triggered running speed across image sets during image presentations where mice were stationary. (<bold>E</bold>) Average image-locked pupil area during image presentations where mice were running. Mean pupil area was first calculated across all running image presentations within a session, then averaged across sessions. Traces show mean ± SEM across sessions. (<bold>F</bold>) Average image-locked pupil area during image presentations where mice were stationary.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig4-figsupp1-v1.tif"/></fig></fig-group><p>To characterize these dynamics across the population, we made use of a ramping index to quantify activity increases or decreases within the pre-stimulus and stimulus epochs. This index compares activity between early and late portions of a defined temporal window and is positive for activity increases and negative for activity decreases (<xref ref-type="bibr" rid="bib38">Makino and Komiyama, 2015</xref>). The distribution of the stimulus ramp index was shifted towards positive values for novel images, consistent with increased image responsiveness (see marginal distributions in <xref ref-type="fig" rid="fig4">Figure 4D</xref>, p&lt;0.008 for all comparisons with image set A, as well as for B-D and C-D for excitatory cells). The distribution of the pre-stimulus ramp index values was also shifted towards positive values for VIP cells with the familiar image set, indicating an increase in pre-stimulus ramping (see marginals distributions in <xref ref-type="fig" rid="fig4">Figure 4D</xref>, p&lt;0.008 for all comparisons with image set A, as well as for B-C and C-D for excitatory cells and B-D for VIP cells). The values of the ramp index for the pre-stimulus and stimulus periods were inversely correlated (<xref ref-type="fig" rid="fig4">Figure 4D</xref>; VIP: r = −0.69, p&lt;0.005; excitatory: r = −0.36, p&lt;0.005; linear least-squares regression fit across all cells). This indicates a tradeoff between stimulus-driven activity and inter-stimulus ramping, particularly in VIP cells.</p><p>To further quantify how these response profiles relate to experience, we determined the fraction of cells that were stimulus responsive (positive stimulus ramp index) or stimulus suppressed (negative stimulus ramp index) across image sets (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). The majority of excitatory neurons were stimulus responsive (&lt;85%), although there was a slight increase in the fraction of stimulus suppressed cells for the familiar image set (1.4% for image set A versus ~0.4% for image sets B-D) (<xref ref-type="fig" rid="fig4">Figure 4E,F</xref>, left panels; p&lt;0.008 for all comparisons with image set A). VIP inhibitory cells showed a larger difference across image sets: during novel image sessions, most cells were stimulus-driven (&lt;80%), whereas during familiar image sessions, the majority of VIP cells (60%) were stimulus-suppressed and showed pre-stimulus ramping (<xref ref-type="fig" rid="fig4">Figure 4E,F</xref> right panels; p&lt;0.008 for all comparisons with image set A).</p><p>As VIP cell activity has been associated with locomotion and arousal (<xref ref-type="bibr" rid="bib9">Dipoppa et al., 2018</xref>; <xref ref-type="bibr" rid="bib11">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Pakan et al., 2016</xref>), we performed several control analyses to determine whether these factors could account for VIP cell pre-stimulus ramping activity. First, we sorted the data according to whether the mouse was running or stationary (see Materials and methods for description of running classification). We found that VIP cells exhibited clear pre-stimulus ramping even when the mouse stationary (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A–D</xref>). Moreover, we examined stimulus-triggered changes in running speed and found that the pattern of running behavior was similar for novel and familiar sessions, despite a clear difference in VIP cell dynamics (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C,D</xref>). We also analyzed differences in pupil size across image sets to evaluate whether changes in pupil size around the time of stimulus onset could explain VIP activity dynamics. We found that in novel sessions pupil area was slightly larger on average, but the stimulus-triggered dynamics of pupil area changes were relatively flat and did not match the VIP cell dynamics (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E,F</xref>). Together these analyses demonstrate that VIP ramping activity does not trivially reflect locomotor or pupil dynamics.</p></sec><sec id="s2-5"><title>VIP cells have strong ramping activity during omission of an expected stimulus</title><p>Would cells with pre-stimulus ramping activity continue to ramp if an image was omitted? To assess this, we analyzed activity during periods in which stimulus presentations were randomly omitted from the regular sequence. Such trials made up 5% of all non-change stimuli during 2-photon imaging sessions (stimuli were never omitted during behavioral training).</p><p>Strikingly, VIP population activity continued to ramp up during stimulus omission, until the subsequent stimulus presentation (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Activity following stimulus omission was much stronger during familiar compared to novel image sessions (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; p&lt;0.008 for all comparisons except A-C for VIP). Omission ramping was not prominent in the excitatory population (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) but there was a small yet significant increase in the strength of omission activity for the familiar image set (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, p&lt;0.008 for all comparisons with image set A for excitatory). Visualizing mean omission related activity for all cells as a heatmap for each image set confirmed that very few excitatory cells showed increases in activity following stimulus omission and were primarily active during the stimulus presentations before and after omission (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). In contrast, most VIP cells showed a dramatic increase in activity following stimulus omission for familiar stimuli (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). In novel image sessions, VIP cell activity was primarily concentrated outside the omission period, with visible stimulus-locked activity in the surrounding timepoints (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>VIP cells show strong ramping activity during stimulus omission.</title><p>(<bold>A</bold>) Average population activity around the time of stimulus omission. On average, excitatory neurons have little change in activity following stimulus omission (left panel). In contrast, activity of the VIP population for the familiar image set A continues to ramp up until the time of the next stimulus presentation. In sessions with novel images (image sets B, C, D), the VIP population also shows some change in activity following stimulus omission, but to a lesser degree than with familiar images. (<bold>B</bold>) Mean activity following stimulus omission is higher during sessions with the familiar image set A. The mean response in a 750 ms window following the time of stimulus omission was first computed for each cell in a given session, then averaged across cells in that session. Connected gray points indicate sessions recorded in a given mouse. Colored points represent the average across sessions for each image set ± 95% confidence intervals. p&lt;0.008 for all comparisons with image set A, except A-C in VIP inhibitory cells (Welch’s t-test used for all statistical comparisons, see Materials and methods for additional details). (<bold>C</bold>) Heatmap of activity around the time of stimulus omission across all excitatory (left panels) and VIP inhibitory cells (right panels), sorted by magnitude of activity in the omission window. Start of omission period is shown by white vertical line at time = 0 and extends to 750 ms thereafter when the next stimulus is presented. (<bold>D</bold>) The strength of the omission ramp index (y-axis) and pre-stimulus ramp index (x-axis) are positively correlated across VIP cells, but not excitatory cells, indicating that VIP cells with pre-stimulus activity typically also show ramping during stimulus omission. Data points across all image sets for each panel were fit with linear least-squares regression. Correlation and significance values for the fit are shown in lower left of each panel. (<bold>E</bold>) Example cells showing different response dynamics during stimulus (colored bars) and omission (time of expected stimulus indicated as gray dashed line) for excitatory and VIP inhibitory cells. Color of shaded bars indicates image set (familiar images in red, novel images in blue). Cells typically show either stimulus-evoked activity and no omission response, or pre-stimulus ramping and strong omission responses. Some cells (examples in right column of right panel) show a combination of stimulus-evoked and omission activity. Scale bar indicates a response of 0.01.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>VIP omission ramping is not explained by running behavior or pupil diameter.</title><p>(<bold>A</bold>) Average omission-triggered activity for excitatory (left panel) and VIP inhibitory cells (right panel) for omission trials where mice were running (mean running speed during the ± 2 s window around the omission time was greater than 5 cm/s). Traces show mean ± SEM across cells. (<bold>B</bold>) Averaged omission-triggered running speed for stimulus omissions where mice were stationary (mean running speed during the omission window was less than 5 cm/s). (<bold>C</bold>) Average omission triggered running speed across stimulus omissions where mice were running. Mean running speed was calculated for each session, then averaged across sessions. Traces show mean ± SEM across sessions. (<bold>D</bold>) Average omission-triggered running speed during stimulus omissions where mice were stationary. (<bold>E</bold>) Average omission-locked pupil area during stimulus omissions where mice were running. Mean pupil area was first calculated for each session, then averaged across sessions. Traces show mean ± SEM across sessions. (<bold>F</bold>) Average omission locked pupil area during stimulus omissions where mice were stationary.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50340-fig5-figsupp1-v1.tif"/></fig></fig-group><p>We again assessed whether the dynamics of neural activity were simply correlated with changes in locomotor behavior or pupil area. We found that omission ramping activity was present even when the mice were not running (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A–D</xref>). Moreover, omission-triggered pupil dynamics were similar during novel and familiar sessions (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1E–F</xref>).</p><p>To examine the relationship between omission ramping and pre-stimulus ramping in single cells, we computed the ramp index during the omission window and compared it to the pre-stimulus ramp index. We found a positive correlation between the strength of pre-stimulus and omission ramping for VIP cells (<xref ref-type="fig" rid="fig5">Figure 5D</xref>; r = 0.49, p&lt;0.005, linear least-squares regression across all VIP cells). While strong omission ramping typically occurred in neurons that had pre-stimulus ramping, it was also possible for cells without pre-stimulus ramping to show increases in activity following stimulus omission (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). In contrast, pre-stimulus and omission dynamics were not correlated in excitatory cells.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We imaged activity in L2/3 excitatory and VIP inhibitory neurons in response to highly familiar and novel images during a visual task with a predictable temporal structure. This revealed several changes associated with training history. Extended experience with a set of images resulted in reduced overall activity levels in both excitatory and VIP cells. Strikingly, VIP cells exhibited distinct activity dynamics when tested with familiar versus novel images. Novel images drove stimulus-locked activity in VIP cells, whereas with familiar images VIP cells had ramping activity during the inter-stimulus interval and were suppressed by stimulus onset. Moreover, these ramping responses continued increasing when stimuli were omitted from the expected stimulus sequence. The magnitude of VIP omission-related activity was several times larger than stimulus-driven activity, indicating these are meaningful signals that could strongly influence network activity. This represents a major experience-dependent cell type-specific change in response dynamics in sensory cortex.</p><sec id="s3-1"><title>Predictive processing and changes in activity with experience</title><p>Predictive processing has emerged as a powerful paradigm for understanding brain function and may help reconcile the traditional view of sensory processing with increasing evidence for experience and context-dependent modulation in early sensory areas. This family of theories posits that the brain constructs an internal model of the environment based on experience, and that incoming sensory information is compared with learned expectations to continually update the model (<xref ref-type="bibr" rid="bib7">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Keller and Mrsic-Flogel, 2018</xref>; <xref ref-type="bibr" rid="bib36">Lochmann and Deneve, 2011</xref>; <xref ref-type="bibr" rid="bib58">Rao and Ballard, 1999</xref>). This dynamic updating with experience is proposed to shift the balance of bottom-up sensory and top-down predictive pathways. As stimuli become familiar with learning, predictive signals may suppress bottom-up input, resulting in a sparser code. On the other hand, novel or surprising stimuli are expected to robustly drive neural activity, signaling deviations from learned predictions.</p><p>We observed reduced activity in both VIP and excitatory cells due to long-term experience with a set of images. The fraction of image responsive cells and mean response magnitude was lower with familiar versus novel images. Previous studies have shown reductions in activity with experience (<xref ref-type="bibr" rid="bib2">Anderson et al., 2008</xref>; <xref ref-type="bibr" rid="bib38">Makino and Komiyama, 2015</xref>; <xref ref-type="bibr" rid="bib43">Mruczek and Sheinberg, 2007</xref>; <xref ref-type="bibr" rid="bib70">Woloszyn and Sheinberg, 2012</xref>), and also novelty enhancement (<xref ref-type="bibr" rid="bib19">Hamm and Yuste, 2016</xref>; <xref ref-type="bibr" rid="bib21">Homann et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Ranganath and Rainer, 2003</xref>). Reduced activity for highly familiar stimuli can efficiently code predictable stimuli, utilizing a smaller population of cells to represent learned information (<xref ref-type="bibr" rid="bib34">LeMessurier and Feldman, 2018</xref>). On the other hand, enhanced activity for novel stimuli could aid detection of salient and behaviorally meaningful events by augmenting output to downstream targets and facilitating associative plasticity (<xref ref-type="bibr" rid="bib57">Ranganath and Rainer, 2003</xref>).</p><p>Our results demonstrating reduced activity with long-term experience and enhancement with novelty are consistent with the predictive coding framework. Increased VIP activity in response to novel images may serve to disinhibit excitatory cell responses to novel stimuli (via the VIP-SST disinhibitory circuit). When stimuli are familiar, VIP cells are suppressed by image presentation, which may permit increased inhibition of excitatory neurons by SST or PV cells, thus increasing the sparseness of stimulus representations. Our observation that VIP cells show ramping activity between familiar images, reminiscent of anticipatory signals, further implicates them in predictive processing.</p></sec><sec id="s3-2"><title>Ramping activity in VIP cells</title><p>What does pre-stimulus and stimulus-omission ramping activity in VIP cells represent? One possibility is that this activity reflects the temporal structure of the behavioral task such that these signals encode predictions about stimulus timing or reward expectation, or serve as a temporal attention signal (<xref ref-type="bibr" rid="bib45">Nobre and van Ede, 2018</xref>). Previous studies have described stimulus and reward expectation signals in the visual cortex of rodents. An early example showed that pairing visual stimulation with a temporally predictable reward produces reward timing signals in visual cortex (<xref ref-type="bibr" rid="bib61">Shuler and Bear, 2006</xref>). A more recent study using a visual orientation discrimination task in mice found pre-stimulus ramping activity specifically in the subpopulation of excitatory cells that encoded the rewarded stimulus, suggestive of reward anticipation (<xref ref-type="bibr" rid="bib56">Poort et al., 2015</xref>). In our task, each stimulus presentation is a potential opportunity to earn a reward if there is a stimulus change. VIP ramping during the inter-stimulus interval may be important for enhancing the responses of excitatory cells to an anticipated change stimulus and consequent reward.</p><p>Visual cortex has also been shown to learn experience-dependent stimulus predictions for repeated sequences of visual stimuli in the absence of association with reward. One study demonstrated anticipatory recall of an omitted stimulus in a learned spatiotemporal sequence (<xref ref-type="bibr" rid="bib15">Gavornik and Bear, 2014</xref>). In a virtual navigation paradigm in which mice locomote along a linear track, V1 neurons have been found to predict upcoming stimuli at specific locations, and to signal a deviation from expectation when stimuli are omitted (<xref ref-type="bibr" rid="bib10">Fiser et al., 2016</xref>). In these studies, the predictive signal peaked at the expected time of the predicted event, whereas our results show a continued ramping past the expected time of stimulus onset on omission trials. This suggests that the ramping signals we observe may represent something other than a pure prediction of stimulus timing. It is also important to note that most prior studies documenting predictive or ramping activity reflect measurements from excitatory neurons, and thus may not be directly comparable to our results in VIP cells. Nonetheless, VIP cells may serve to gate the predictive or anticipatory signals documented in excitatory cells.</p><p>VIP cells in visual cortex have been shown to be modulated by locomotion and arousal in a context-dependent manner (<xref ref-type="bibr" rid="bib9">Dipoppa et al., 2018</xref>; <xref ref-type="bibr" rid="bib11">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Pakan et al., 2016</xref>). The ramping activity in VIP cells that we observe, however, is not simply a reflection of locomotion or arousal (as indexed by pupil diameter), since the differences in the dynamics of VIP cells cannot be explained by differences in the pattern of animal running behavior or changes in pupil diameter relative to stimulus and omission onset.</p></sec><sec id="s3-3"><title>Role of VIP cells in learning and salience detection</title><p>Theoretical work suggests that VIP cells could mediate associative learning and experience-dependent signal routing via disinhibition of local excitatory populations (<xref ref-type="bibr" rid="bib66">Wang and Yang, 2018</xref>; <xref ref-type="bibr" rid="bib69">Wilmes and Clopath, 2018</xref>; <xref ref-type="bibr" rid="bib72">Yang et al., 2016</xref>). Several lines of experimental evidence support this hypothesis. For example, VIP cells in auditory cortex respond to salient reinforcement signals including reward and punishment, and activation of VIP cells enhances the gain of auditory responses (<xref ref-type="bibr" rid="bib35">Letzkus et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Pi et al., 2013</xref>). Inactivation of VIP cells in visual cortex impairs plasticity following monocular deprivation, while activating VIP cells enhances plasticity (<xref ref-type="bibr" rid="bib12">Fu et al., 2017</xref>). In the amygdala, VIP cells are necessary for associative memory formation through disinhibition of local pyramidal cells during fear learning (<xref ref-type="bibr" rid="bib30">Krabbe et al., 2019</xref>). Further, VIP cells are modulated by expectation (<xref ref-type="bibr" rid="bib30">Krabbe et al., 2019</xref>).</p><p>Our finding that VIP cells switch from a stimulus responsive to stimulus suppressed mode depending on past experience with an image set may be consistent with a role for VIP cells in enhancing the representation of salient events to facilitate learning and adaptive behavior. During sessions with novel images, VIP activity is strongly driven by stimulus presentation, potentially serving to increase the gain of stimulus evoked excitatory responses to salient, novel stimuli. When stimuli are familiar, other aspects of the task may become more relevant, such as the unexpected omission of stimuli from a predictable temporal sequence, and VIP cells switch from signaling the presence of a stimulus to signaling the absence of a stimulus.</p><p>VIP cells in the visual cortex of passively viewing mice are suppressed by high contrast stimuli and show direction selective responses at low contrast, leading to the proposal that VIP cells are involved in enhancing the gain of weak but salient stimuli (<xref ref-type="bibr" rid="bib8">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Millman et al., 2019</xref>). The inter-stimulus and omission ramping activity we observed, with a decay in the VIP response following onset of familiar images, is consistent with the phenomenon of contrast suppression. However our finding that VIP cells show robust stimulus-evoked responses to novel stimuli demonstrates that VIP responses are flexible and depend on experience. These seemingly divergent findings can be unified under the view that VIP cells represent salient stimuli in a context-dependent manner. Novelty may put cortical circuits in a regime requiring high sensitivity, involving VIP signaling to enhance the gain of salient stimuli. When stimuli are familiar and the environment is more predictable, VIP cells may be more sensitive to unexpected events, such as stimulus omission or violations of temporal expectation. It remains an open question whether VIP activity switches from stimulus driven to stimulus suppressed following passive exposure to a novel set of images in the absence of reward, or whether active task performance and reinforcement are necessary to observe this context-dependent switch in temporal dynamics.</p></sec><sec id="s3-4"><title>Future directions</title><p>Further modifications of our visual change detection task to include omitted rewards or variation in the predictability of the inter-stimulus interval could help to distinguish between coding of stimulus timing versus reward anticipation. Studies examining the evolution of activity in identified VIP cells across multiple behavior sessions as novel images become familiar are needed to determine the time course of the observed experience dependent effects. Concurrent recordings of VIP, excitatory, and other inhibitory cell classes including SST cells will be important to establish a direct link between VIP activity and disinhibition of local excitatory neurons during task learning. Finally, experiments examining the activity and impact of the diverse inputs to VIP cells, including neuromodulatory inputs (<xref ref-type="bibr" rid="bib33">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Letzkus et al., 2011</xref>), thalamic inputs (<xref ref-type="bibr" rid="bib68">Williams and Holtmaat, 2019</xref>), and feedback projections from other cortical regions (<xref ref-type="bibr" rid="bib65">Wall et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Zhang et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Zhang et al., 2014</xref>), will be critical to establish the function of and mechanism behind the shift in VIP dynamics with experience.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Reagent (species) <break/>or resource</th><th valign="bottom">Designation</th><th valign="bottom">Source or reference</th><th valign="bottom">Identifiers</th><th valign="bottom">Additional <break/>information</th></tr></thead><tbody><tr><td valign="bottom">Genetic reagent (<italic>M. musculus</italic>)</td><td valign="bottom">Slc17a7-IRES2-Cre; Slc17a7+; Excitatory</td><td valign="bottom">Jackson Laboratory</td><td valign="bottom">Stock #:023527; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/Addgene_61574">Addgene_61574</ext-link></td><td valign="bottom">PMID: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/25741722">25741722</ext-link></td></tr><tr><td valign="bottom">Genetic reagent (<italic>M. musculus</italic>)</td><td valign="bottom">VIP-IRES-Cre; VIP+; VIP Inhibitory</td><td valign="bottom">Jackson Laboratory</td><td valign="bottom">Stock #: 010908; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/MGI:4436915">MGI:4436915</ext-link></td><td valign="bottom">Dr. Z Josh Huang (Cold Spring Harbor Laboratory)</td></tr><tr><td valign="bottom">Genetic reagent (<italic>M. musculus</italic>)</td><td valign="bottom">CaMKII-tTA x Ai93-GCaMP6f</td><td valign="bottom">Jackson Laboratory</td><td valign="bottom">Stock #: 024108; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/IMSR_JAX:024108">IMSR_JAX:024108</ext-link></td><td valign="bottom">PMID: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/22855807">22855807</ext-link>; PMID: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/25741722">25741722</ext-link></td></tr><tr><td valign="bottom">Genetic reagent (<italic>M. musculus</italic>)</td><td valign="bottom">Ai148-GCaMP6f</td><td valign="bottom">Jackson Laboratory</td><td valign="bottom">Stock #: 030328; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/IMSR_JAX:030328">IMSR_JAX:030328</ext-link></td><td valign="bottom">PMID: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/30007418">30007418</ext-link></td></tr><tr><td valign="bottom">Software, algorithm</td><td valign="bottom">numpy</td><td valign="bottom">NumPy</td><td valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008633">SCR_008633</ext-link></td><td valign="bottom"/></tr><tr><td valign="bottom">Software, algorithm</td><td valign="bottom">scipy</td><td valign="bottom">SciPy</td><td valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008058">SCR_008058</ext-link></td><td valign="bottom"/></tr><tr><td valign="bottom">Software, algorithm</td><td valign="bottom">matplotlib</td><td valign="bottom">MatPlotLib</td><td valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008624">SCR_008624</ext-link></td><td valign="bottom"/></tr><tr><td valign="bottom">Software, algorithm</td><td valign="bottom">pandas</td><td valign="bottom">pandas</td><td valign="bottom">DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3509134">10.5281/zenodo.3509134</ext-link></td><td valign="bottom"/></tr><tr><td valign="bottom">Software, algorithm</td><td valign="bottom">seaborn</td><td valign="bottom">seaborn</td><td valign="bottom">DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1313201">10.5281/zenodo.1313201</ext-link></td><td valign="bottom"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Mice</title><p>All experiments and procedures were performed in accordance with protocols approved by the Allen Institute Animal Care and Use Committee. We used male and female transgenic mice expressing GCaMP6f in VIP inhibitory interneurons (double transgenic: VIP-IRES-Cre x Ai148 mice; <ext-link ext-link-type="uri" xlink:href="https://www.jax.org/strain/010908">https://www.jax.org/strain/010908</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://www.jax.org/strain/030328">https://www.jax.org/strain/030328</ext-link>) (<xref ref-type="bibr" rid="bib6">Daigle et al., 2018</xref>) or in excitatory glutamatergic neurons (triple transgenic: Slc17a7-IRES2-Cre x CaMKII-tTA x Ai93; <ext-link ext-link-type="uri" xlink:href="https://www.jax.org/strain/023527">https://www.jax.org/strain/023527</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.jax.org/strain/024108">https://www.jax.org/strain/024108</ext-link>) (<xref ref-type="bibr" rid="bib37">Madisen et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Mayford et al., 1996</xref>). Mice were single housed and maintained on a reverse 12 hr light cycle (off at 9am, on at 9pm) and all experiments were performed during the dark cycle.</p></sec><sec id="s4-2"><title>Surgery</title><p>Surgical procedures were performed as described in <xref ref-type="bibr" rid="bib8">de Vries et al. (2020)</xref> (see Supplementary Figure 14). Headpost and cranial window surgery was performed on healthy mice that ranged in age from 5 to 12 weeks. Pre-operative injections of dexamethasone (3.2 mg/kg, S.C.) were administered at 12 hr and 3 hr before surgery. Mice were initially anesthetized with 5% isoflurane (1–3 min) and placed in a stereotaxic frame (Model# 1900, Kopf, Tujunga, CA), and isoflurane levels were maintained at 1.5–2.5% for surgery. An incision was made to remove skin, and the exposed skull was levelled with respect to pitch (bregma-lambda level), roll and yaw. The stereotax was zeroed at lambda using a custom headframe holder equipped with stylus affixed to a clamp-plate. The stylus was then replaced with the headframe to center the headframe well at 2.8 mm lateral and 1.3 mm anterior to lambda. The headframe was affixed to the skull with white Metabond and once dried, the mouse was placed in a custom clamp to position the skull at a rotated angle of 23° such that visual cortex was horizontal to facilitate the craniotomy. A circular piece of skull 5 mm in diameter was removed, and a durotomy was performed. A coverslip stack (two 5 mm and one 7 mm glass coverslip adhered together) was cemented in place with Vetbond (<xref ref-type="bibr" rid="bib16">Goldey et al., 2014</xref>). Metabond cement was applied around the cranial window inside the well to secure the glass window. Post-surgical brain health was documented using a custom photo-documentation system and at one, two, and seven days following surgery, animals were assessed for overall health (bright, alert, and responsive), cranial window clarity, and brain health. After a 1–2 week recovery from surgery, animals underwent intrinsic signal imaging for retinotopic mapping, then entered into behavioral training.</p></sec><sec id="s4-3"><title>Intrinsic signal imaging</title><p>Intrinsic signal imaging (ISI) was performed as described in <xref ref-type="bibr" rid="bib8">de Vries et al. (2020)</xref> (see Supplementary Figure 15) to produce a retinotopic map to define visual area boundaries and target in vivo two-photon calcium imaging experiments to the center of visual space in each imaged area. Mice were lightly anesthetized with 1–1.4% isoflurane administered with a somnosuite (model #715; Kent Scientific, CON). Vital signs were monitored with a Physiosuite (model # PS-MSTAT-RT; Kent Scientific). Eye drops (Lacri-Lube Lubricant Eye Ointment; Refresh) were applied to maintain hydration and clarity of eye during anesthesia. Mice were headfixed for imaging.</p><p>The brain surface was illuminated with two independent LED lights: green (peak λ = 527 nm; FWHM = 50 nm; Cree Inc, C503B-GCN-CY0C0791) and red (peak λ = 635 nm and FWHM of 20 nm; Avago Technologies, HLMP-EG08-Y2000) mounted on the optical lens. A pair of Nikon lenses lens (Nikon Nikkor 105 mm f/2.8, Nikon Nikkor 35 mm f/1.4), provided 3.0x magnification (M = 105/35) onto an Andor Zyla 5.5 10tap sCMOS camera. A bandpass filter (Semrock; FF01-630/92 nm) was used to record reflected red light from the brain.</p><p>A 24’ monitor was positioned 10 cm from the right eye. The monitor was rotated 30° relative to the animal’s dorsoventral axis and tilted 70° off the horizon to ensure that the stimulus was perpendicular to the optic axis of the eye (<xref ref-type="bibr" rid="bib47">Oommen and Stahl, 2008</xref>). The visual stimulus for mapping retinotopy was a 20° x 155° drifting bar containing a checkerboard pattern, with individual square sizes measuring 25°, that alternated black and white as it moved across a mean-luminance gray background. The bar moved in each of the four cardinal directions 10 times. The stimulus was warped spatially so that a spherical representation could be displayed on a flat monitor (<xref ref-type="bibr" rid="bib39">Marshel et al., 2011</xref>).</p><p>After defocusing from the surface vasculature (between 500 μm and 1500 μm along the optical axis), up to 10 independent ISI timeseries were acquired and used to measure the hemodynamic response to the visual stimulus. Averaged sign maps were produced from a minimum of 3 timeseries images for a combined minimum of 30 stimulus sweeps in each direction (<xref ref-type="bibr" rid="bib13">Garrett et al., 2014</xref>).</p><p>The resulting ISI maps were automatically segmented by comparing the sign, location, size, and spatial relationships of the segmented areas against those compiled in an ISI-derived atlas of visual areas (<xref ref-type="bibr" rid="bib8">de Vries et al., 2020</xref>). A cost function, defined by the discrepancy between the properties of the matched areas, was minimized to identify the best match between visual areas in the experimental sign map and those in the atlas, resulting in an auto-segmented and annotated map for each experiment. Manual correction and editing of the results included merging and splitting of segmented and annotated areas to correct errors. Finally, target maps were created to guide in vivo two-photon imaging location using the retinotopic map. The center of retinotopic space was computed from azimuth and altitude maps and adjusted for variability in eye position relative to the monitor by zeroing to the anatomical center V1. The corresponding retinotopic location was identified for each visual area, and two-photon imaging was targeted to a region within 20° of the center of gaze.</p></sec><sec id="s4-4"><title>Behavior training</title><sec id="s4-4-1"><title>Water restriction and habituation</title><p>Throughout behavior training mice were water-restricted in order to maintain consistent motivation to learn and perform the behavioral task (<xref ref-type="bibr" rid="bib17">Guo et al., 2014</xref>). Prior to water restriction mice were weighed once daily for three days to obtain a stable, initial baseline weight. During the first week of water restriction mice were handled daily and habituated to increasing duration of head fixation in the behavior enclosure over a five-day period. Thus, the first day of behavior training occurred after 10 days of water restriction. Mice were trained 5 days per week and could earn as much water as possible during the daily one-hour sessions; supplemental water was provided if earned volume fell below 1.0 mL and/or body weight fell under 80–85% of their initial baseline weight. On non-training days mice were weighed and received enough water provision to reach their target weight of 80–85% (never less than 1.0 mL per day).</p></sec><sec id="s4-4-2"><title>Apparatus</title><p>Mice were trained in custom-designed, sound-attenuating behavior enclosures. Visual stimuli were displayed on a 24’ LCD monitor (ASUS, Model # PA248Q) placed at a ~ 15 cm distance from the mouse’s right eye. The monitor was rotated 30° relative to the animal’s dorsoventral axis and tilted 70° off the horizon to ensure that the stimulus was perpendicular to the optic axis of the eye (<xref ref-type="bibr" rid="bib47">Oommen and Stahl, 2008</xref>). A behavior stage was placed in a consistent location using a kinematic mount and consisted of a standardized headframe clamp to enable repeatable positioning of the mouse relative to the monitor, and a 6.5’ running wheel tilted upwards by 10–15 degrees (see Supplementary Figure 13 of <xref ref-type="bibr" rid="bib8">de Vries et al., 2020</xref>). Running behavior was measured by a rotational encoder. Water rewards were delivered using a solenoid (NResearch, Model #161K011) that allowed for a calibrated volume of fluid to pass through a blunted, 17 g hypodermic needle (Hamilton) positioned approximately 2–3 mm from the animal’s mouth. Licks were detected by a capacitive sensor coupled to the reward delivery spout. Running speed, lick times, and reward delivery times were recorded on a NI PCI-6612 digital IO board and sampled at the frequency of the visual display (60 Hz).</p></sec><sec id="s4-4-3"><title>Behavioral training procedure</title><p>Mice were trained for 1 hour/day, 5 days/week using an automated training algorithm. Briefly, mice were trained to lick when the identity of a flashed visual stimulus changed. If mice responded correctly within a short, post-change response window (750 ms) a water reward (5–10 uL) was delivered. On Day 1 of the automated training procedure mice received a short, 15 min ‘open loop’ conditioning session during which non-contingent water rewards were delivered coincident with 90 degree changes in orientation of a full-field, static square-wave grating. This session was intended to 1) introduce the mouse to the fluid delivery system, 2) provide the technician an opportunity to identify the optimal lick spout position for each mouse and 3) condition the association between stimulus changes and reward delivery. Each session thereafter was run in ‘closed loop’ mode, and progressed through 3 stages of the operant task (schematized in <xref ref-type="fig" rid="fig1">Figure 1E</xref>): 1) static, full-field square wave gratings (changes between 0 and 90 degrees, spatial frequency 0.04 cycles per degree), 2) full-field square-wave gratings (changes between 0 and 90 degrees, spatial frequency 0.04 cycles per degree) presented for 250 ms with an 500 ms inter stimulus gray period, and 3) full-field natural scenes (eight natural images from the Allen Brain Observatory) presented for 250 ms with a 500 ms inter stimulus gray period between stimuli. Progression through each stage required mice to achieve a session maximum d-prime of 1 on two of the last three sessions. The shortest amount of time to reach the final stage of training was five sessions. Once in stage 3, mice were considered ‘ready for imaging’ when 2 out of 3 sequential sessions had a d-prime &gt;1 and mice performed at least 100 trials. However, many mice remained in stage 3 of behavior training until the 2-photon microscope became available. This resulted in a variable training duration in stage three across mice (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>).</p></sec><sec id="s4-4-4"><title>Session and trial structure</title><p>Each behavior session lasted 60 min and consisted of a continuous series of image presentations with GO and CATCH trials interspersed, schematized in <xref ref-type="fig" rid="fig1">Figure 1A,B</xref>. Briefly, prior to the start of each trial a change-type and change-time were selected. Change-type was chosen based on predetermined frequencies such that GO and CATCH trials occurred with equal probabilities for sessions with two oriented gratings. For the natural image phase in which there were 64 change-pair possibilities, CATCH frequency was set to 12.5% (1/8 of the number of image transitions). To ensure even sampling of all stimulus transitions, a transition path is selected at random from a matrix of 1000 pre-generated paths. Each path takes a pre-determined route through each of the 64 possible transitions, including same-to-same, or catch, transitions. Once a transition path is completed, another path is chosen at random.</p><p>Change times were selected from an exponential distribution ranging from 2.25 to 8.25 s (mean of 4.25 s) following the start of a trial. Catch trial times were drawn from the same distribution such that false alarm rates were measured with the same temporal statistics as change trials, to account for any learning of the temporal distribution of change times. On trials when a mouse licked prior to the change or catch time, the trial was restarted with the same scheduled change or catch time. To prevent mice from getting stuck on a single trial, the number of times a trial could be repeated was limited to five. GO and CATCH trials, when combined with mouse’s licking response, yield HIT, MISS, FALSE ALARM, and CORRECT REJECTION trials. In addition to the four trial types described above, behavior sessions contained a subset of ‘free reward’ trials (GO trials followed immediately by delivery of a non-contingent reward). Behavior sessions across all phases began with five free-reward trials to help prime engagement with the task. Additionally, to promote continued task engagement, one of these free rewards was delivered after 10 consecutive MISS trials.</p><p>Each image was shown an average of 487 times during a given one-hour session. On average, there were 4699 stimulus presentations in each session.</p></sec></sec><sec id="s4-5"><title>Two-photon imaging during behavior</title><sec id="s4-5-1"><title>Visual stimulation</title><p>Visual stimuli were generated using custom Python scripts written in PsychoPy (<ext-link ext-link-type="uri" xlink:href="https://www.psychopy.org/">https://www.psychopy.org/</ext-link>; <xref ref-type="bibr" rid="bib51">Peirce, 2009</xref>; <xref ref-type="bibr" rid="bib50">Peirce, 2007</xref>) and were displayed using an ASUS PA248Q LCD monitor, with 1920 × 1200 pixels. Stimuli were presented monocularly, and the monitor was positioned 15 cm from the mouse’s eye and spanned 120° X 95° of visual space. The monitor was rotated 30° relative to the animal’s midline and tilted 70° off the horizon to ensure that the stimulus was perpendicular to the optic axis of the eye (<xref ref-type="bibr" rid="bib47">Oommen and Stahl, 2008</xref>).</p><p>The monitor was gamma corrected and had a mean luminance of 50 cd/m<sup>2</sup>. To account for the close viewing angle of the mouse, a spherical warping was applied to all stimuli to ensure that the apparent size, speed, and spatial frequency were constant across the monitor as seen from the mouse’s perspective (<xref ref-type="bibr" rid="bib39">Marshel et al., 2011</xref>). Visual stimuli were presented at 60 Hz frame rate.</p><p>Visual stimuli consisted of a subset of the natural scene images used in the publicly available Allen Brain Observatory dataset (<ext-link ext-link-type="uri" xlink:href="https://observatory.brain-map.org/visualcoding/">https://observatory.brain-map.org/visualcoding/</ext-link>; <xref ref-type="bibr" rid="bib8">de Vries et al., 2020</xref>). The 32 natural images that we used originated from three different databases of natural scene images: the Berkeley Segmentation Dataset (images 000, 005, 012, 013, 024, 031, 034, 035, 036, 044, 047, 045, 054, 057) (<xref ref-type="bibr" rid="bib62">Strasburger et al., 2011</xref>), the van Hateren Natural Image Dataset (images 061, 062, 063, 065, 066, 069, 072, 073, 075, 077, 078, 085, 087, 091) (<xref ref-type="bibr" rid="bib63">van Hateren and van der Schaaf, 1998</xref>), and the McGill Calibrated Colour Image Database (images 104, 106, 114, 115) (<xref ref-type="bibr" rid="bib46">Olmos and Kingdom, 2004</xref>). The images were presented in grayscale, contrast normalized, matched to have equal mean luminance, and resized to 1174 × 918 pixels.</p></sec><sec id="s4-5-2"><title>Behavior apparatus</title><p>Running speed measurement, lick detection, and reward delivery were performed as described above for behavioral training. The monitor was placed in a fixed location relative to the behavior stage to ensure a consistent relationship between the mouse’s eye and the screen. Running speed, lick times, and reward delivery times were recorded on a NI PCI-6612 digital IO board and sampled at the frequency of the visual display (60 Hz).</p></sec><sec id="s4-5-3"><title>Two-photon calcium imaging during behavior</title><p>Calcium imaging was performed using a Scientifica Vivoscope (<ext-link ext-link-type="uri" xlink:href="https://www.scientifica.uk.com/products/scientifica-vivoscope">https://www.scientifica.uk.com/products/scientifica-vivoscope</ext-link>). Laser excitation was provided by a Ti:Sapphire laser (Chameleon Vision – Coherent) at 910 nm. Pre-compensation was set at −10,000 fs2. Movies were recorded at 30 Hz using resonant scanners over a 400 μm field of view (512 × 512 pixels). Temporal synchronization of calcium imaging, visual stimulation, reward delivery and behavioral output (lick times and running speed) was achieved by recording all experimental clocks on a single NI PCI-6612 digital IO board at 100 kHz.</p><p>Behavior sessions under the two-photon microscope were 1 hr in duration, with task parameters identical to stage 3 of the behavior training procedure as described above. In addition, during most two-photon imaging sessions, 5% of stimulus presentations were randomly omitted, excluding the change presentation and the presentation immediately prior to the change. These omitted presentations were added to the experimental protocol partway into the experiment, resulting in 86/101 (85%) imaging sessions including omitted presentations. The 15 sessions without omitted presentations included data from one Slc17a7-IRES2;CaMKII-tTA;Ai93 mouse (four sessions in VISp), and two Vip-IRES-Cre;Ai148 mice (three sessions from VISal, and eight sessions from VISp). Sessions without omitted presentations were excluded from any analysis depending on stimulus omission.</p><p>Movies of fluorescence were acquired near the center of retinotopic space in VISp and VISal, using ISI target maps and vasculature images as a guide. Once a cortical region was selected, the objective was shielded from stray light coming from the stimulus monitor using opaque black tape. All recordings were made at a depth of ~175 um from the brain surface. Once a field of view was selected, a combination of PMT gain and laser power was selected to maximize laser power (based on a look-up table against depth) and dynamic range while avoiding pixel saturation (max number of saturated pixels &lt; 1000). Immersion water was occasionally supplemented while imaging using a micropipette taped to the objective (Microfil MF28G67-5 WPI) and connected to a 5 ml syringe via an extension tubing. At the end of each experimental session, a z-stack of images (± 30 μm around imaging site, 0.1 μm step) was collected to evaluate cortical anatomy and evaluate z-drift during experiment. Experiments with z-drift above 10 µm over the course of the entire session were excluded.</p><p>For each field of view, imaging and behavior sessions were conducted using each of the four image sets shown in <xref ref-type="fig" rid="fig1">Figure 1F</xref>, including the familiar image set A used during behavior training, and three novel image sets first experienced by the mouse during the imaging phase of the experiment. On subsequent imaging days for a given field of view, we returned to the same location by matching (1) the pattern of vessels in epi-fluorescence with (2) the pattern of vessels in two photon imaging and (3) the pattern of cellular labelling in two photon imaging at the previously recorded location. Typically, only one field of view was imaged per mouse, however in 3 out of the 21 mice, fields of view were recorded in both VISp and VISal. In cases where an imaging session failed our QC criteria (for example for z-drift &gt;10 um), the session was retaken. As a result, some sessions with ‘novel’ image sets B, C or D were the second or third exposure (67% were first exposure, 27% were the second exposure, 6% were the third or fourth exposure). In contrast, mice were exposed to familiar image set A for an average of 17 ± 14 sessions during training.</p></sec></sec><sec id="s4-6"><title>Pupillometry</title><p>Pupil tracking was performed under 850 nm infrared illumination (OSRAM SYLVANIA Inc, LZ1-10R602-0000 mounted to a Thorlabs LB1092-B-ML bi-convex lens) using a 30 Hz infrared sensitive camera (Allied Vision Technologies Mako G-032B) mated to a 0.73x, 130 mm WD lens (Infinity InfiniStix, part #213073) and a 845–855 nm bandpass filter (Semrock FF01-850/10-25). The camera and IR LED were mounted to the left of the animal and focused on a short-pass dichroic mirror (Semrock FF750-SDi02−25 × 66, cutoff frequency = 750 nm) positioned between the animal and the monitor, thus allowing tracking of the right (monitor facing) eye and pupil. Pupil diameter was extracted from raw video frames using a processing pipeline based on the DeepLabCut tracking algorithm (<xref ref-type="bibr" rid="bib40">Mathis et al., 2018</xref>). Briefly, a model was fit using hand-annotated sample frames (12 points each from the perimeter of the pupil and eyelid) from multiple imaging rigs, subjects, and lighting conditions. The model was then applied to each frame of the 101 eye tracking videos acquired during imaging sessions, excluding 38 sessions for which frame timing information was incomplete. Points with a minimum confidence value of 0.8 (output by the DeepLabCut model) were used to fit separate ellipses to the eyelid and pupil (<xref ref-type="bibr" rid="bib18">Halir and Flusser, 1998</xref>). Any frames with fewer than six high-confidence eye or pupil tracking points, which generally occurred during blinking/squinting, were not fit (replaced with NaN). Pupil area was then calculated as the area of a circle with a radius equal to the major axis of the ellipse fit. Frames with calculated areas greater than three standard deviations from the mean were excluded, as were the two frames immediately before and after any missing fits. Pupil area was interpolated across periods with missing fits.</p></sec><sec id="s4-7"><title>Quality control</title><p>All data streams were required to pass strict quality control criteria (<xref ref-type="bibr" rid="bib8">de Vries et al., 2020</xref>; see Supplementary Figure 16). For example, Z-drift of the 2-photon imaging plane over the 1 hr imaging session was quantified by performing phase correlation between the frames of a 100 um z-stack taken after the imaging session and a 500 frame average from the beginning of the 2-photon movie and a 500 frame average at the end of the movie. If the distance between the z-stack frames found to be most correlated with the beginning and end of the movie is greater than 10 um, the session failed QC and was retaken. Only imaging sessions passing all QC criteria were included in this study.</p></sec><sec id="s4-8"><title>Data processing</title><p>Post-processing of 2-photon imaging data was performed as described in <xref ref-type="bibr" rid="bib8">de Vries et al. (2020)</xref> (see Supplementary Figures 19, 20, 22, 23). For each two-photon imaging session, the image processing pipeline included the following steps: (1) motion correction, (2) image normalization to minimize confounding random variations between sessions, (3) segmentation of ROIs, and (4) ROI filtering. Motion correction was performed using phase correlation and rigid translation. Segmentation was performed by morphological filtering on normalized periodic average images constructed from 400 frame blocks, followed by unification of masks across all blocks. ROI filtering was performed to remove segmented regions that were unlikely to correspond to cell somas, based on attributes including size and shape (for example, small ROIs likely corresponding to apical dendrites were removed).</p><p>Following identification of cell ROIs, the following steps were performed to obtain <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> traces and deconvolved event traces: (1) neuropil subtraction, (2) trace demixing, (3) <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> computation, (4) L0-regularized event detection. For each ROI, a neuropil mask was created, consisting of a 13 pixel ring around the cell soma, excluding any other ROIs. The raw fluorescence trace was generated by averaging all pixels within each cell ROI and the neuropil mask. A neuropil contamination ratio was computed for each ROI and the calcium trace was modeled as <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the measured fluorescence trace, <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the unknown true ROI fluorescence trace, <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the fluorescence of the surrounding neuropil, and <inline-formula><mml:math id="inf7"><mml:mi>r</mml:mi></mml:math></inline-formula> is the contamination ratio. After determination of <inline-formula><mml:math id="inf8"><mml:mi>r</mml:mi></mml:math></inline-formula>, we computed the true trace as <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which is used in all subsequent analysis. To avoid artificially correlating neurons' activity by averaging fluorescence over two spatially overlapping ROIs, we demixed the activity of all recorded ROIs, as described in <xref ref-type="bibr" rid="bib8">de Vries et al. (2020)</xref>. A global dF/F trace for each cell was computed, with the baseline F<sub>0</sub> determined by a rolling mode of 180 seconds across the raw fluorescence trace. An L0-penalized event detection algorithm was applied to the dF/F trace to obtain a timeseries of calcium events with a magnitude proportional to the increase in calcium activity (<ext-link ext-link-type="uri" xlink:href="https://github.com/jewellsean/FastLZeroSpikeInference">https://github.com/jewellsean/FastLZeroSpikeInference</ext-link>; <xref ref-type="bibr" rid="bib24">Jewell and Witten, 2018</xref>; <xref ref-type="bibr" rid="bib23">Jewell and Witten, 2017</xref>). Parameters used for event detection can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/matchings/visual_coding_2p_analysis/blob/master/visual_coding_2p_analysis/l0_analysis.py">https://github.com/matchings/visual_coding_2p_analysis/blob/master/visual_coding_2p_analysis/l0_analysis.py</ext-link> (<xref ref-type="bibr" rid="bib14">Garrett, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/visual_coding_2p_analysis">https://github.com/elifesciences-publications/visual_coding_2p_analysis</ext-link>). Event timeseries were smoothed with a casual half-Gaussian filter with a standard deviation of 0.065 sec. Temporal alignment was performed to link two-photon acquisition frames (30Hz frame rate) with visual stimulation frames (60Hz frame rate) and associated behavioral signals (licking, running speed, reward delivery, sampled at 60Hz frame rate of visual stimulus). The visual stimulus time nearest to each 2-photon frame time was computed, with the condition that the visual stimulus time must be before the 2-photon acquisition time, to ensure that dF/F responses were not attributed to stimulus or behavior events occurring after the change in the calcium signal.</p></sec><sec id="s4-9"><title>Data analysis</title><p>All data analysis was performed using custom scripts in Python and relied heavily on pandas (<ext-link ext-link-type="uri" xlink:href="https://pandas.pydata.org/">https://pandas.pydata.org/</ext-link>), numpy (<ext-link ext-link-type="uri" xlink:href="https://numpy.org/">https://numpy.org/</ext-link>), and scipy (<ext-link ext-link-type="uri" xlink:href="https://www.scipy.org/">https://www.scipy.org/</ext-link>). Data visualization was performed using matplotlib (<ext-link ext-link-type="uri" xlink:href="https://matplotlib.org/">https://matplotlib.org/</ext-link>) and seaborn (<ext-link ext-link-type="uri" xlink:href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</ext-link>).</p><sec id="s4-9-1"><title>Behavior</title><p>Response rates for GO and CATCH trials were calculated by evaluating the fraction of trials of each type where a lick was registered within the 750 ms response window following the change or sham change time (<xref ref-type="fig" rid="fig1">Figure 1I</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>). The fraction of GO trials with a response is the hit rate and the fraction of CATCH trials with a lick response is the false alarm rate. Response rate was similarly computed for all non-change stimulus presentations, as well as following stimulus omission and for the stimulus presentation directly following stimulus omission (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>). The d-prime value for each session (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>) was computed as:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Z</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where Z is the inverse of the cumulative distribution function (using scipy.stats.norm.ppf).</p><p>Reaction time was calculated as the time to first lick after the start of the change time on GO trials (<xref ref-type="fig" rid="fig1">Figure 1J</xref>) and displayed using seaborn pointplot. Mean run speed was calculated by taking the average of the running speed trace in a ± 2 s window around the image change time for each GO trial, then averaging across all GO trials in each session (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>).The average running speed trace across sessions (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>) was computed by averaging the running speed trace across all GO trials in a [−2,6] second window around the change time for GO trials. A histogram of lick times relative to stimulus omission was generated across all 2-photon sessions for all mice. The kernel density estimate of lick times was produced for each image set individually (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>) using seaborn kdeplot.</p><p>Calculation of all behavior metrics was limited to the portion of the session where the mouse was actively engaged in the behavioral task, where engagement was defined those periods during which the mouse earned at least two rewards per minute. Mice performed 248 engaged GO trials per session on average (range = 83–335).</p></sec><sec id="s4-9-2"><title>Physiology</title><p>All analysis was performed on detected events, with arbitrary units designated as ‘response’ throughout the text and figures.</p><p>Neural responses were analyzed with respect to stimulus onset and the time of stimulus omissions. Average stimulus evoked traces were generated by averaging all stimulus presentations for each image (<xref ref-type="fig" rid="fig2">Figure 2</xref>, right panels) or across all stimulus presentations of all images (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Average traces around the time of stimulus omission were generated relative to the time when a stimulus would have been presented (<xref ref-type="fig" rid="fig2">Figure 2</xref>, right columns of right panels, <xref ref-type="fig" rid="fig5">Figure 5A,C,D</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). For each individual image presentation (or stimulus omission), the mean response in a 500 ms window after stimulus (or omission) onset was computed (including the 250 ms stimulus duration and 250 ms after to include cells with delayed responses or off responses after stimulus offset). Then the average across all stimulus presentations for each image (or omission) was determined (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The cumulative distribution of mean image response magnitude across cells was generated using seaborn distplot, for the preferred image for each cell (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The preferred image was identified as the image evoking the largest mean response for each cell. The cumulative distribution of mean omission response magnitude across cells was similarly generated using seaborn distplot (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>Image responsiveness (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) was calculated by first comparing the mean response to each individual stimulus presentation for a cell’s preferred image to a shuffled distribution of omission responses from that same cell. Specifically, we resampled the responses to omitted stimuli by drawing randomly with replacement 10,000 times, and then assigned a p-value to each individual stimulus presentation equal to the proportion of resampled omitted-stimulus responses that were larger than the mean response to that stimulus presentation. Neurons were classified as ‘image responsive’ if at least 25% of presentations of the preferred stimulus had a p-value (with respect to omission responses) less than 0.05. Similarly, omission responsiveness (<xref ref-type="fig" rid="fig3">Figure 3F</xref>) was calculated by comparing each omission response to a shuffled distribution of image responses from the same cell to get a p-value, then classifying cells as ‘omission responsive’ if at least 10% of omitted-stimulus responses had a p-value less than 0.05.</p><p>To quantify selectivity for individual cells, we used a lifetime sparseness metric, computed using the definition in <xref ref-type="bibr" rid="bib64">Vinje and Gallant (2000)</xref>:<disp-formula id="equ2"><mml:math id="m2"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:math></disp-formula>where <italic>N</italic> is the number of images and <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the response of the neuron to image i averaged across trials. Lifetime sparseness was only computed for cells that met the image responsiveness criteria described above. The cumulative distribution of lifetime sparseness values for image responsive cells was generated using seaborn distplot (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). Since there were an insufficient number of image responsive cells for image set A (&lt;10), the distribution of lifetime sparseness values for this condition was not shown. We created population tuning curves across image responsive cells by rank sorting the mean response to the 8 images shown in each session for each cell, then averaging across cells (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). An insufficient number of VIP cells (&lt;10) were image responsive for image set A, thus a tuning curve was not included for this condition in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>.</p><p>The population average response was computed by first taking the average stimulus triggered response across all images for each cell (examples in <xref ref-type="fig" rid="fig4">Figure 4B</xref>), then averaging across cells (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The time to peak response was identified for each cell as the time where the average stimulus triggered response in a 500 ms window after stimulus onset reached its maximum value (<xref ref-type="fig" rid="fig4">Figure 4C</xref>).</p><p>The dynamics of cell responses were evaluated by computing a ramp index over different time windows of interest, similar to <xref ref-type="bibr" rid="bib38">Makino and Komiyama (2015)</xref>:<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is the mean response in the first half of a defined window of time, and <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> is the second half of the window. This index provides a measure of the magnitude and direction of a change in a signal within the window. For <xref ref-type="fig" rid="fig4">Figure 4D and E</xref>, the ramp index was computed for two windows: the pre-stimulus window (400 ms prior to stimulus onset, comparing the first 120 ms with the last 120 ms) and the stimulus window (125 ms after stimulus offset, comparing the first 65 ms with the last 65 ms in the window) for the mean events trace for each cell across all stimulus presentations of all images. If the cell trace is increasing during the window, the ramp index is positive. If the cell trace decreasing during the window, the ramp index is negative.</p><p>The pre-stimulus and stimulus ramp indices were plotted against each other on a cell by cell basis (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) and found to be correlated by least squares linear regression between the two measures (using scipy.stats.linregress). Cells with positive values of the stimulus ramp index were considered to be ‘stimulus driven’ and cells with negative values of the stimulus ramp index were considered to be ‘stimulus suppressed’ (<xref ref-type="fig" rid="fig4">Figure 4E,F</xref>). The fraction of cells that fell in each of these categories was calculated for each session, then averaged across sessions for each image set (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). The population average image response was created by averaging across all cells in each category, regardless of image set (<xref ref-type="fig" rid="fig4">Figure 4F</xref>).</p><p>The population average image response was also computed separately for image presentations where mice were running versus stationary (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A,B</xref>). Image presentations were classified as running if the mean running speed during the [−0.5, 0.75] second window around stimulus onset was &gt;5 cm/s and as stationary if the mean running speed was &lt;5 cm/s. To confirm this classification, and to evaluate any differences in locomotion and arousal across image sets, we also generated plots of average image triggered running speed and pupil area for stimulus presentations classified as running and stationary (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C–F</xref>). For both running speed and pupil area, traces aligned to the onset of stimulus presentations were first averaged within each session, then averaged across all sessions for each image set.</p><p>For <xref ref-type="fig" rid="fig4">Figure 4D and E</xref>, the ramp index was computed for two windows: the pre-stimulus window (400 ms prior to stimulus onset, comparing the first 120 ms with the last 120 ms) and the stimulus window (125 ms after stimulus offset, comparing the first 65 ms with the last 65 ms in the window) for the mean events trace for each cell across all stimulus presentations of all images. If the cell trace is increasing during the window, the ramp index is positive. If the cell trace decreasing during the window, the ramp index is negative.</p><p>The population average omission response was also computed separately for omissions where mice were running versus stationary (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A,B</xref>). Image presentations were classified as running if the mean running speed in a ± 2 s window around the time of omission &gt;5 cm/s, and as stationary if the mean running speed in the same window was &lt;5 cm/s. To confirm this classification, and to evaluate any differences in locomotion and arousal across image sets, we also generated plots of average omission triggered running speed and pupil area for omissions classified as running and stationary (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C–F</xref>). For both running speed and pupil area, traces aligned to the time where a stimulus would have been presented, first averaged within each session, then averaged across all sessions for each image set.</p></sec></sec><sec id="s4-10"><title>Statistics</title><p>For all statistical comparisons, ANOVA (scipy.stats.f_oneway) was used to test for an overall effect of image set within the excitatory or VIP inhibitory groups, followed by Welch’s two-sample t-test (scipy.stats.ttest_ind) for each image set pair, using Bonferroni correction for multiple comparisons to set significance level. p-values are reported throughout the text and figure legends, and significance of comparisons where p&lt;0.0083 (an alpha value of 0.05 divided by the number of pairwise comparisons) are indicated by asterisks in figure insets.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Jerome Lecoq and Kevin Takasaki for technical help with the 2-photon microscope, Derric Williams for help with behavior and stimulus control software, Douglas Kim, Janelia Research Campus, Howard Hughes Medical Institute, for providing GCaMP6f, and Saskia de Vries, Brian Hu, and Christof Koch for comments on the manuscript. The authors thank the Allen Institute founder, Paul G Allen, for his vision, encouragement, and support</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Data curation, Validation, Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Software, Formal analysis, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Software, Methodology</p></fn><fn fn-type="con" id="con8"><p>Project administration</p></fn><fn fn-type="con" id="con9"><p>Investigation</p></fn><fn fn-type="con" id="con10"><p>Project administration</p></fn><fn fn-type="con" id="con11"><p>Methodology</p></fn><fn fn-type="con" id="con12"><p>Software</p></fn><fn fn-type="con" id="con13"><p>Software, Validation</p></fn><fn fn-type="con" id="con14"><p>Software, Validation</p></fn><fn fn-type="con" id="con15"><p>Software, Project administration</p></fn><fn fn-type="con" id="con16"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con17"><p>Conceptualization, Supervision, Methodology, Writing - original draft, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All experiments and procedures were performed in accordance with protocols (#1801) approved by the Allen Institute Animal Care and Use Committee (IACUC).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-50340-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Figshare DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.c.4858779.v1">https://doi.org/10.6084/m9.figshare.c.4858779.v1</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Manavi</surname><given-names>S</given-names></name><name><surname>Roll</surname><given-names>KR</given-names></name><name><surname>Ollerenshaw</surname><given-names>D</given-names></name><name><surname>Groblewski</surname><given-names>P</given-names></name><name><surname>Ponvert</surname><given-names>ND</given-names></name><name><surname>Kiggins</surname><given-names>JT</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Experience shapes activity dynamics and stimulus coding of VIP inhibitory cells</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.c.4858779.v1</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alitto</surname> <given-names>HJ</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cell-type-specific modulation of neocortical activity by basal forebrain input</article-title><source>Frontiers in Systems Neuroscience</source><volume>6</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fnsys.2012.00079</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>B</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Kawasaki</surname> <given-names>K</given-names></name><name><surname>Sheinberg</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of familiarity on neural activity in monkey inferior temporal lobe</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>2540</fpage><lpage>2552</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn015</pub-id><pub-id pub-id-type="pmid">18296433</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batista-Brito</surname> <given-names>R</given-names></name><name><surname>Zagha</surname> <given-names>E</given-names></name><name><surname>Ratliff</surname> <given-names>JM</given-names></name><name><surname>Vinck</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Modulation of cortical circuits by top-down processing and arousal state in health and disease</article-title><source>Current Opinion in Neurobiology</source><volume>52</volume><fpage>172</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.06.008</pub-id><pub-id pub-id-type="pmid">30064117</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>SX</given-names></name><name><surname>Kim</surname> <given-names>AN</given-names></name><name><surname>Peters</surname> <given-names>AJ</given-names></name><name><surname>Komiyama</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Subtype-specific plasticity of inhibitory circuits in motor cortex during motor learning</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1109</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1038/nn.4049</pub-id><pub-id pub-id-type="pmid">26098758</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chubykin</surname> <given-names>AA</given-names></name><name><surname>Roach</surname> <given-names>EB</given-names></name><name><surname>Bear</surname> <given-names>MF</given-names></name><name><surname>Shuler</surname> <given-names>MGH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A Cholinergic Mechanism for Reward Timing within Primary Visual Cortex</article-title><source>Neuron</source><volume>77</volume><fpage>723</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.039</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daigle</surname> <given-names>TL</given-names></name><name><surname>Madisen</surname> <given-names>L</given-names></name><name><surname>Hage</surname> <given-names>TA</given-names></name><name><surname>Valley</surname> <given-names>MT</given-names></name><name><surname>Knoblich</surname> <given-names>U</given-names></name><name><surname>Larsen</surname> <given-names>RS</given-names></name><name><surname>Takeno</surname> <given-names>MM</given-names></name><name><surname>Huang</surname> <given-names>L</given-names></name><name><surname>Gu</surname> <given-names>H</given-names></name><name><surname>Larsen</surname> <given-names>R</given-names></name><name><surname>Mills</surname> <given-names>M</given-names></name><name><surname>Bosma-Moody</surname> <given-names>A</given-names></name><name><surname>Siverts</surname> <given-names>LA</given-names></name><name><surname>Walker</surname> <given-names>M</given-names></name><name><surname>Graybuck</surname> <given-names>LT</given-names></name><name><surname>Yao</surname> <given-names>Z</given-names></name><name><surname>Fong</surname> <given-names>O</given-names></name><name><surname>Nguyen</surname> <given-names>TN</given-names></name><name><surname>Garren</surname> <given-names>E</given-names></name><name><surname>Lenz</surname> <given-names>GH</given-names></name><name><surname>Chavarha</surname> <given-names>M</given-names></name><name><surname>Pendergraft</surname> <given-names>J</given-names></name><name><surname>Harrington</surname> <given-names>J</given-names></name><name><surname>Hirokawa</surname> <given-names>KE</given-names></name><name><surname>Harris</surname> <given-names>JA</given-names></name><name><surname>Nicovich</surname> <given-names>PR</given-names></name><name><surname>McGraw</surname> <given-names>MJ</given-names></name><name><surname>Ollerenshaw</surname> <given-names>DR</given-names></name><name><surname>Smith</surname> <given-names>KA</given-names></name><name><surname>Baker</surname> <given-names>CA</given-names></name><name><surname>Ting</surname> <given-names>JT</given-names></name><name><surname>Sunkin</surname> <given-names>SM</given-names></name><name><surname>Lecoq</surname> <given-names>J</given-names></name><name><surname>Lin</surname> <given-names>MZ</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Murphy</surname> <given-names>GJ</given-names></name><name><surname>da Costa</surname> <given-names>NM</given-names></name><name><surname>Waters</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Tasic</surname> <given-names>B</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A suite of transgenic driver and reporter mouse lines with enhanced Brain-Cell-Type targeting and functionality</article-title><source>Cell</source><volume>174</volume><fpage>465</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.035</pub-id><pub-id pub-id-type="pmid">30007418</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Heilbron</surname> <given-names>M</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname> <given-names>SEJ</given-names></name><name><surname>Lecoq</surname> <given-names>JA</given-names></name><name><surname>Buice</surname> <given-names>MA</given-names></name><name><surname>Groblewski</surname> <given-names>PA</given-names></name><name><surname>Ocker</surname> <given-names>GK</given-names></name><name><surname>Oliver</surname> <given-names>M</given-names></name><name><surname>Feng</surname> <given-names>D</given-names></name><name><surname>Cain</surname> <given-names>N</given-names></name><name><surname>Ledochowitsch</surname> <given-names>P</given-names></name><name><surname>Millman</surname> <given-names>D</given-names></name><name><surname>Roll</surname> <given-names>K</given-names></name><name><surname>Garrett</surname> <given-names>M</given-names></name><name><surname>Keenan</surname> <given-names>T</given-names></name><name><surname>Kuan</surname> <given-names>L</given-names></name><name><surname>Mihalas</surname> <given-names>S</given-names></name><name><surname>Olsen</surname> <given-names>S</given-names></name><name><surname>Thompson</surname> <given-names>C</given-names></name><name><surname>Wakeman</surname> <given-names>W</given-names></name><name><surname>Waters</surname> <given-names>J</given-names></name><name><surname>Williams</surname> <given-names>D</given-names></name><name><surname>Barber</surname> <given-names>C</given-names></name><name><surname>Berbesque</surname> <given-names>N</given-names></name><name><surname>Blanchard</surname> <given-names>B</given-names></name><name><surname>Bowles</surname> <given-names>N</given-names></name><name><surname>Caldejon</surname> <given-names>SD</given-names></name><name><surname>Casal</surname> <given-names>L</given-names></name><name><surname>Cho</surname> <given-names>A</given-names></name><name><surname>Cross</surname> <given-names>S</given-names></name><name><surname>Dang</surname> <given-names>C</given-names></name><name><surname>Dolbeare</surname> <given-names>T</given-names></name><name><surname>Edwards</surname> <given-names>M</given-names></name><name><surname>Galbraith</surname> <given-names>J</given-names></name><name><surname>Gaudreault</surname> <given-names>N</given-names></name><name><surname>Gilbert</surname> <given-names>TL</given-names></name><name><surname>Griffin</surname> <given-names>F</given-names></name><name><surname>Hargrave</surname> <given-names>P</given-names></name><name><surname>Howard</surname> <given-names>R</given-names></name><name><surname>Huang</surname> <given-names>L</given-names></name><name><surname>Jewell</surname> <given-names>S</given-names></name><name><surname>Keller</surname> <given-names>N</given-names></name><name><surname>Knoblich</surname> <given-names>U</given-names></name><name><surname>Larkin</surname> <given-names>JD</given-names></name><name><surname>Larsen</surname> <given-names>R</given-names></name><name><surname>Lau</surname> <given-names>C</given-names></name><name><surname>Lee</surname> <given-names>E</given-names></name><name><surname>Lee</surname> <given-names>F</given-names></name><name><surname>Leon</surname> <given-names>A</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Long</surname> <given-names>F</given-names></name><name><surname>Luviano</surname> <given-names>J</given-names></name><name><surname>Mace</surname> <given-names>K</given-names></name><name><surname>Nguyen</surname> <given-names>T</given-names></name><name><surname>Perkins</surname> <given-names>J</given-names></name><name><surname>Robertson</surname> <given-names>M</given-names></name><name><surname>Seid</surname> <given-names>S</given-names></name><name><surname>Shea-Brown</surname> <given-names>E</given-names></name><name><surname>Shi</surname> <given-names>J</given-names></name><name><surname>Sjoquist</surname> <given-names>N</given-names></name><name><surname>Slaughterbeck</surname> <given-names>C</given-names></name><name><surname>Sullivan</surname> <given-names>D</given-names></name><name><surname>Valenza</surname> <given-names>R</given-names></name><name><surname>White</surname> <given-names>C</given-names></name><name><surname>Williford</surname> <given-names>A</given-names></name><name><surname>Witten</surname> <given-names>DM</given-names></name><name><surname>Zhuang</surname> <given-names>J</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Farrell</surname> <given-names>C</given-names></name><name><surname>Ng</surname> <given-names>L</given-names></name><name><surname>Bernard</surname> <given-names>A</given-names></name><name><surname>Phillips</surname> <given-names>JW</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id><pub-id pub-id-type="pmid">31844315</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dipoppa</surname> <given-names>M</given-names></name><name><surname>Ranson</surname> <given-names>A</given-names></name><name><surname>Krumin</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vision and locomotion shape the interactions between neuron types in mouse visual cortex</article-title><source>Neuron</source><volume>98</volume><fpage>602</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id><pub-id pub-id-type="pmid">29656873</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname> <given-names>A</given-names></name><name><surname>Mahringer</surname> <given-names>D</given-names></name><name><surname>Oyibo</surname> <given-names>HK</given-names></name><name><surname>Petersen</surname> <given-names>AV</given-names></name><name><surname>Leinweber</surname> <given-names>M</given-names></name><name><surname>Keller</surname> <given-names>GB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1658</fpage><lpage>1664</lpage><pub-id pub-id-type="doi">10.1038/nn.4385</pub-id><pub-id pub-id-type="pmid">27618309</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname> <given-names>Y</given-names></name><name><surname>Tucciarone</surname> <given-names>JM</given-names></name><name><surname>Espinosa</surname> <given-names>JS</given-names></name><name><surname>Sheng</surname> <given-names>N</given-names></name><name><surname>Darcy</surname> <given-names>DP</given-names></name><name><surname>Nicoll</surname> <given-names>RA</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A cortical circuit for gain control by behavioral state</article-title><source>Cell</source><volume>156</volume><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.050</pub-id><pub-id pub-id-type="pmid">24630718</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname> <given-names>Y</given-names></name><name><surname>Kaneko</surname> <given-names>M</given-names></name><name><surname>Tang</surname> <given-names>Y</given-names></name><name><surname>Alvarez-Buylla</surname> <given-names>A</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A cortical disinhibitory circuit for enhancing adult plasticity</article-title><source>eLife</source><volume>4</volume><elocation-id>e05558</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05558</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname> <given-names>ME</given-names></name><name><surname>Nauhaus</surname> <given-names>I</given-names></name><name><surname>Marshel</surname> <given-names>JH</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Topography and areal organization of mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>12587</fpage><lpage>12600</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1124-14.2014</pub-id><pub-id pub-id-type="pmid">25209296</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Garrett</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Visual Coding 2P analysis code</data-title><source>GitHub</source><version designator="21c71f0">21c71f0</version><ext-link ext-link-type="uri" xlink:href="https://github.com/matchings/visual_coding_2p_analysis">https://github.com/matchings/visual_coding_2p_analysis</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavornik</surname> <given-names>JP</given-names></name><name><surname>Bear</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learned spatiotemporal sequence recognition and prediction in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>732</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/nn.3683</pub-id><pub-id pub-id-type="pmid">24657967</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldey</surname> <given-names>GJ</given-names></name><name><surname>Roumis</surname> <given-names>DK</given-names></name><name><surname>Glickfeld</surname> <given-names>LL</given-names></name><name><surname>Kerlin</surname> <given-names>AM</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Bonin</surname> <given-names>V</given-names></name><name><surname>Andermann</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Versatile cranial window strategies for long-term two-photon imaging in awake mice</article-title><source>Nature Protocols</source><volume>9</volume><fpage>2515</fpage><lpage>2538</lpage><pub-id pub-id-type="doi">10.1038/nprot.2014.165</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname> <given-names>ZV</given-names></name><name><surname>Hires</surname> <given-names>SA</given-names></name><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>O'Connor</surname> <given-names>DH</given-names></name><name><surname>Komiyama</surname> <given-names>T</given-names></name><name><surname>Ophir</surname> <given-names>E</given-names></name><name><surname>Huber</surname> <given-names>D</given-names></name><name><surname>Bonardi</surname> <given-names>C</given-names></name><name><surname>Morandell</surname> <given-names>K</given-names></name><name><surname>Gutnisky</surname> <given-names>D</given-names></name><name><surname>Peron</surname> <given-names>S</given-names></name><name><surname>Xu</surname> <given-names>NL</given-names></name><name><surname>Cox</surname> <given-names>J</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Procedures for behavioral experiments in head-fixed mice</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e88678</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0088678</pub-id><pub-id pub-id-type="pmid">24520413</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Halir</surname> <given-names>R</given-names></name><name><surname>Flusser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Numerically Stable Direct Least Squares Fitting of Ellipses</source><publisher-name>citeseerx</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamm</surname> <given-names>JP</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Somatostatin interneurons control a key component of mismatch negativity in mouse visual cortex</article-title><source>Cell Reports</source><volume>16</volume><fpage>597</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.06.037</pub-id><pub-id pub-id-type="pmid">27396334</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hangya</surname> <given-names>B</given-names></name><name><surname>Pi</surname> <given-names>HJ</given-names></name><name><surname>Kvitsiani</surname> <given-names>D</given-names></name><name><surname>Ranade</surname> <given-names>SP</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>From circuit motifs to computations: mapping the behavioral repertoire of cortical interneurons</article-title><source>Current Opinion in Neurobiology</source><volume>26</volume><fpage>117</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.007</pub-id><pub-id pub-id-type="pmid">24508565</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Homann</surname> <given-names>J</given-names></name><name><surname>Koay</surname> <given-names>SA</given-names></name><name><surname>Glidden</surname> <given-names>AM</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Berry Ii</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Predictive coding of novel versus familiar stimuli in the primary visual cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/197608</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jewell</surname> <given-names>SW</given-names></name><name><surname>Hocking</surname> <given-names>TD</given-names></name><name><surname>Fearnhead</surname> <given-names>P</given-names></name><name><surname>Witten</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast nonconvex deconvolution of calcium imaging data</article-title><source>Biostatistics</source><volume>10</volume><elocation-id>kxy083</elocation-id><pub-id pub-id-type="doi">10.1093/biostatistics/kxy083</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jewell</surname> <given-names>S</given-names></name><name><surname>Witten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Exact spike train inference via ℓ0 Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.08644">https://arxiv.org/abs/1703.08644</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jewell</surname> <given-names>S</given-names></name><name><surname>Witten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Exact spike train inference via ℓ<sub>0</sub> OPTIMIZATION</article-title><source>The Annals of Applied Statistics</source><volume>12</volume><fpage>2457</fpage><lpage>2482</lpage><pub-id pub-id-type="doi">10.1214/18-AOAS1162</pub-id><pub-id pub-id-type="pmid">30627301</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jurjut</surname> <given-names>O</given-names></name><name><surname>Georgieva</surname> <given-names>P</given-names></name><name><surname>Busse</surname> <given-names>L</given-names></name><name><surname>Katzner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning enhances sensory processing in mouse V1 before improving behavior</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6460</fpage><lpage>6474</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3485-16.2017</pub-id><pub-id pub-id-type="pmid">28559381</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname> <given-names>GB</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predictive processing: a canonical cortical computation</article-title><source>Neuron</source><volume>100</volume><fpage>424</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id><pub-id pub-id-type="pmid">30359606</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname> <given-names>A</given-names></name><name><surname>Fishell</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interneuron cell types are fit to function</article-title><source>Nature</source><volume>505</volume><fpage>318</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1038/nature12983</pub-id><pub-id pub-id-type="pmid">24429630</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname> <given-names>AG</given-names></name><name><surname>Poort</surname> <given-names>J</given-names></name><name><surname>Chadwick</surname> <given-names>A</given-names></name><name><surname>Blot</surname> <given-names>A</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct learning-induced changes in stimulus selectivity and interactions of GABAergic interneuron classes in visual cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>851</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0143-z</pub-id><pub-id pub-id-type="pmid">29786081</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname> <given-names>AG</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Contextual signals in visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>52</volume><fpage>131</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.05.003</pub-id><pub-id pub-id-type="pmid">29883940</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krabbe</surname> <given-names>S</given-names></name><name><surname>Paradiso</surname> <given-names>E</given-names></name><name><surname>d'Aquin</surname> <given-names>S</given-names></name><name><surname>Bitterman</surname> <given-names>Y</given-names></name><name><surname>Courtin</surname> <given-names>J</given-names></name><name><surname>Xu</surname> <given-names>C</given-names></name><name><surname>Yonehara</surname> <given-names>K</given-names></name><name><surname>Markovic</surname> <given-names>M</given-names></name><name><surname>Müller</surname> <given-names>C</given-names></name><name><surname>Eichlisberger</surname> <given-names>T</given-names></name><name><surname>Gründemann</surname> <given-names>J</given-names></name><name><surname>Ferraguti</surname> <given-names>F</given-names></name><name><surname>Lüthi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptive disinhibitory gating by VIP interneurons permits associative learning</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1834</fpage><lpage>1843</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0508-y</pub-id><pub-id pub-id-type="pmid">31636447</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchibhotla</surname> <given-names>KV</given-names></name><name><surname>Gill</surname> <given-names>JV</given-names></name><name><surname>Lindsay</surname> <given-names>GW</given-names></name><name><surname>Papadoyannis</surname> <given-names>ES</given-names></name><name><surname>Field</surname> <given-names>RE</given-names></name><name><surname>Sten</surname> <given-names>TAH</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallel processing by cortical inhibition enables context-dependent behavior</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>62</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/nn.4436</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex</article-title><source>Trends in Neurosciences</source><volume>36</volume><fpage>141</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2012.11.006</pub-id><pub-id pub-id-type="pmid">23273272</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Kruglikov</surname> <given-names>I</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Fishell</surname> <given-names>G</given-names></name><name><surname>Rudy</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A disinhibitory circuit mediates motor integration in the somatosensory cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1662</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1038/nn.3544</pub-id><pub-id pub-id-type="pmid">24097044</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeMessurier</surname> <given-names>AM</given-names></name><name><surname>Feldman</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Plasticity of population coding in primary sensory cortex</article-title><source>Current Opinion in Neurobiology</source><volume>53</volume><fpage>50</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.04.029</pub-id><pub-id pub-id-type="pmid">29775823</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letzkus</surname> <given-names>JJ</given-names></name><name><surname>Wolff</surname> <given-names>SB</given-names></name><name><surname>Meyer</surname> <given-names>EM</given-names></name><name><surname>Tovote</surname> <given-names>P</given-names></name><name><surname>Courtin</surname> <given-names>J</given-names></name><name><surname>Herry</surname> <given-names>C</given-names></name><name><surname>Lüthi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A disinhibitory microcircuit for associative fear learning in the auditory cortex</article-title><source>Nature</source><volume>480</volume><fpage>331</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1038/nature10674</pub-id><pub-id pub-id-type="pmid">22158104</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochmann</surname> <given-names>T</given-names></name><name><surname>Deneve</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural processing as causal inference</article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.05.018</pub-id><pub-id pub-id-type="pmid">21742484</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madisen</surname> <given-names>L</given-names></name><name><surname>Garner</surname> <given-names>AR</given-names></name><name><surname>Shimaoka</surname> <given-names>D</given-names></name><name><surname>Chuong</surname> <given-names>AS</given-names></name><name><surname>Klapoetke</surname> <given-names>NC</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>van der Bourg</surname> <given-names>A</given-names></name><name><surname>Niino</surname> <given-names>Y</given-names></name><name><surname>Egolf</surname> <given-names>L</given-names></name><name><surname>Monetti</surname> <given-names>C</given-names></name><name><surname>Gu</surname> <given-names>H</given-names></name><name><surname>Mills</surname> <given-names>M</given-names></name><name><surname>Cheng</surname> <given-names>A</given-names></name><name><surname>Tasic</surname> <given-names>B</given-names></name><name><surname>Nguyen</surname> <given-names>TN</given-names></name><name><surname>Sunkin</surname> <given-names>SM</given-names></name><name><surname>Benucci</surname> <given-names>A</given-names></name><name><surname>Nagy</surname> <given-names>A</given-names></name><name><surname>Miyawaki</surname> <given-names>A</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name><name><surname>Empson</surname> <given-names>RM</given-names></name><name><surname>Knöpfel</surname> <given-names>T</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transgenic mice for intersectional targeting of neural sensors and effectors with high specificity and performance</article-title><source>Neuron</source><volume>85</volume><fpage>942</fpage><lpage>958</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.02.022</pub-id><pub-id pub-id-type="pmid">25741722</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makino</surname> <given-names>H</given-names></name><name><surname>Komiyama</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning enhances the relative impact of top-down processing in the visual cortex</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1116</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1038/nn.4061</pub-id><pub-id pub-id-type="pmid">26167904</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname> <given-names>JH</given-names></name><name><surname>Garrett</surname> <given-names>ME</given-names></name><name><surname>Nauhaus</surname> <given-names>I</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization of seven mouse visual cortical Areas</article-title><source>Neuron</source><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id><pub-id pub-id-type="pmid">22196338</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayford</surname> <given-names>M</given-names></name><name><surname>Bach</surname> <given-names>ME</given-names></name><name><surname>Huang</surname> <given-names>YY</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Hawkins</surname> <given-names>RD</given-names></name><name><surname>Kandel</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Control of memory formation through regulated expression of a CaMKII transgene</article-title><source>Science</source><volume>274</volume><fpage>1678</fpage><lpage>1683</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1678</pub-id><pub-id pub-id-type="pmid">8939850</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Millman</surname> <given-names>DJ</given-names></name><name><surname>Ocker</surname> <given-names>GK</given-names></name><name><surname>Caldejon</surname> <given-names>S</given-names></name><name><surname>Kato</surname> <given-names>I</given-names></name><name><surname>Larkin</surname> <given-names>JD</given-names></name><name><surname>Lee</surname> <given-names>EK</given-names></name><name><surname>Luviano</surname> <given-names>J</given-names></name><name><surname>Nayan</surname> <given-names>C</given-names></name><name><surname>Nguyen</surname> <given-names>T</given-names></name> <name><surname>North</surname> <given-names>K</given-names></name><name><surname>Seid</surname> <given-names>S</given-names></name><name><surname>White</surname> <given-names>C</given-names></name><name><surname>Lecoq</surname> <given-names>JA</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Buice</surname> <given-names>MA</given-names></name><name><surname>de</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>VIP interneurons selectively enhance weak but behaviorally-relevant stimuli</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/858001</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Sheinberg</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Context familiarity enhances target processing by inferior temporal cortex neurons</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>8533</fpage><lpage>8545</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2106-07.2007</pub-id><pub-id pub-id-type="pmid">17687031</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munoz</surname> <given-names>W</given-names></name><name><surname>Tremblay</surname> <given-names>R</given-names></name><name><surname>Levenstein</surname> <given-names>D</given-names></name><name><surname>Rudy</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Layer-specific modulation of neocortical dendritic inhibition during active wakefulness</article-title><source>Science</source><volume>959</volume><fpage>954</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1126/science.aag2599</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nobre</surname> <given-names>AC</given-names></name><name><surname>van Ede</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Anticipated moments: temporal structure in attention</article-title><source>Nature Reviews Neuroscience</source><volume>19</volume><fpage>34</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.141</pub-id><pub-id pub-id-type="pmid">29213134</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olmos</surname> <given-names>A</given-names></name><name><surname>Kingdom</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A biologically inspired algorithm for the recovery of shading and reflectance images</article-title><source>Perception</source><volume>33</volume><fpage>1463</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1068/p5321</pub-id><pub-id pub-id-type="pmid">15729913</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oommen</surname> <given-names>BS</given-names></name><name><surname>Stahl</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Eye orientation during static tilts and its relationship to spontaneous head pitch in the laboratory mouse</article-title><source>Brain Research</source><volume>1193</volume><fpage>57</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.11.053</pub-id><pub-id pub-id-type="pmid">18178173</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pakan</surname> <given-names>JM</given-names></name><name><surname>Lowe</surname> <given-names>SC</given-names></name><name><surname>Dylda</surname> <given-names>E</given-names></name><name><surname>Keemink</surname> <given-names>SW</given-names></name><name><surname>Currie</surname> <given-names>SP</given-names></name><name><surname>Coutts</surname> <given-names>CA</given-names></name><name><surname>Rochefort</surname> <given-names>NL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behavioral-state modulation of inhibition is context-dependent and cell type specific in mouse visual cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e14985</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14985</pub-id><pub-id pub-id-type="pmid">27552056</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pakan</surname> <given-names>JM</given-names></name><name><surname>Francioni</surname> <given-names>V</given-names></name><name><surname>Rochefort</surname> <given-names>NL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Action and learning shape the activity of neuronal circuits in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>52</volume><fpage>88</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.04.020</pub-id><pub-id pub-id-type="pmid">29727859</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>PsychoPy--psychophysics software in python</article-title><source>Journal of Neuroscience Methods</source><volume>162</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id><pub-id pub-id-type="pmid">17254636</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating stimuli for neuroscience using PsychoPy</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.010.2008</pub-id><pub-id pub-id-type="pmid">19198666</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petro</surname> <given-names>LS</given-names></name><name><surname>Vizioli</surname> <given-names>L</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Contributions of cortical feedback to sensory processing in primary visual cortex</article-title><source>Frontiers in Psychology</source><volume>5</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01223</pub-id><pub-id pub-id-type="pmid">25414677</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname> <given-names>CK</given-names></name><name><surname>Xue</surname> <given-names>M</given-names></name><name><surname>He</surname> <given-names>M</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Scanziani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1068</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1038/nn.3446</pub-id><pub-id pub-id-type="pmid">23817549</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pi</surname> <given-names>HJ</given-names></name><name><surname>Hangya</surname> <given-names>B</given-names></name><name><surname>Kvitsiani</surname> <given-names>D</given-names></name><name><surname>Sanders</surname> <given-names>JI</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical interneurons that specialize in disinhibitory control</article-title><source>Nature</source><volume>503</volume><fpage>521</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1038/nature12676</pub-id><pub-id pub-id-type="pmid">24097352</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname> <given-names>L</given-names></name><name><surname>Goard</surname> <given-names>MJ</given-names></name><name><surname>Estandian</surname> <given-names>D</given-names></name><name><surname>Xu</surname> <given-names>M</given-names></name><name><surname>Kwan</surname> <given-names>AC</given-names></name><name><surname>Lee</surname> <given-names>SH</given-names></name><name><surname>Harrison</surname> <given-names>TC</given-names></name><name><surname>Feng</surname> <given-names>G</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast modulation of visual perception by basal forebrain cholinergic neurons</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1857</fpage><lpage>1863</lpage><pub-id pub-id-type="doi">10.1038/nn.3552</pub-id><pub-id pub-id-type="pmid">24162654</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poort</surname> <given-names>J</given-names></name><name><surname>Khan</surname> <given-names>AG</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Nemri</surname> <given-names>A</given-names></name><name><surname>Orsolic</surname> <given-names>I</given-names></name><name><surname>Krupic</surname> <given-names>J</given-names></name><name><surname>Bauza</surname> <given-names>M</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Keller</surname> <given-names>GB</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning enhances sensory and multiple Non-sensory representations in primary visual cortex</article-title><source>Neuron</source><volume>86</volume><fpage>1478</fpage><lpage>1490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.037</pub-id><pub-id pub-id-type="pmid">26051421</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganath</surname> <given-names>C</given-names></name><name><surname>Rainer</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cognitive neuroscience: neural mechanisms for detecting and remembering novel events</article-title><source>Nature Reviews Neurosci</source><volume>4</volume><fpage>193</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1038/nrn1052</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimer</surname> <given-names>J</given-names></name><name><surname>Froudarakis</surname> <given-names>E</given-names></name><name><surname>Cadwell</surname> <given-names>CR</given-names></name><name><surname>Yatsenko</surname> <given-names>D</given-names></name><name><surname>Denfield</surname> <given-names>GH</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil fluctuations track fast switching of cortical states during quiet wakefulness</article-title><source>Neuron</source><volume>84</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.033</pub-id><pub-id pub-id-type="pmid">25374359</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Diamanti</surname> <given-names>EM</given-names></name><name><surname>Fournier</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Coherent encoding of subjective spatial position in visual cortex and hippocampus</article-title><source>Nature</source><volume>562</volume><fpage>124</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0516-1</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuler</surname> <given-names>MG</given-names></name><name><surname>Bear</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reward timing in the primary visual cortex</article-title><source>Science</source><volume>311</volume><fpage>1606</fpage><lpage>1609</lpage><pub-id pub-id-type="doi">10.1126/science.1123513</pub-id><pub-id pub-id-type="pmid">16543459</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strasburger</surname> <given-names>H</given-names></name><name><surname>Rentschler</surname> <given-names>I</given-names></name><name><surname>Jüttner</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Peripheral vision and pattern recognition: a review</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/11.5.13</pub-id><pub-id pub-id-type="pmid">22207654</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname> <given-names>JH</given-names></name><name><surname>van der Schaaf</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B: Biological Sciences</source><volume>265</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0303</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinje</surname> <given-names>WE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Natural vision sparse coding and decorrelation in primary visual cortex during sparse coding and decorrelation in primary visual cortex during natural vision</article-title><source>Science</source><volume>287</volume><fpage>1273</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1126/science.287.5456.1273</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wall</surname> <given-names>NR</given-names></name><name><surname>De La Parra</surname> <given-names>M</given-names></name><name><surname>Sorokin</surname> <given-names>JM</given-names></name><name><surname>Taniguchi</surname> <given-names>H</given-names></name><name><surname>Huang</surname> <given-names>ZJ</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain-Wide maps of synaptic input to cortical interneurons</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>4000</fpage><lpage>4009</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3967-15.2016</pub-id><pub-id pub-id-type="pmid">27053207</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Yang</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A disinhibitory circuit motif and flexible information routing in the brain</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>75</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.01.002</pub-id><pub-id pub-id-type="pmid">29414069</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Weskelblatt</surname> <given-names>J</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distinct functional classes of excitatory neurons in mouse V1 are differentially modulated by learning and task engagement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/533463</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>LE</given-names></name><name><surname>Holtmaat</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Higher-Order thalamocortical inputs gate synaptic Long-Term potentiation via disinhibition</article-title><source>Neuron</source><volume>101</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.049</pub-id><pub-id pub-id-type="pmid">30472077</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wilmes</surname> <given-names>KA</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inhibitory microcircuits for top-down plasticity of sensory representations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/494989</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woloszyn</surname> <given-names>L</given-names></name><name><surname>Sheinberg</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effects of long-term visual experience on responses of distinct classes of single units in inferior temporal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.032</pub-id><pub-id pub-id-type="pmid">22500640</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>S</given-names></name><name><surname>Jiang</surname> <given-names>W</given-names></name><name><surname>Poo</surname> <given-names>M-ming</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Activity recall in a visual cortical ensemble</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>449</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1038/nn.3036</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>GR</given-names></name><name><surname>Murray</surname> <given-names>JD</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A dendritic disinhibitory circuit mechanism for pathway-specific gating</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12815</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12815</pub-id><pub-id pub-id-type="pmid">27649374</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>S</given-names></name><name><surname>Xu</surname> <given-names>M</given-names></name><name><surname>Kamigaki</surname> <given-names>T</given-names></name><name><surname>Hoang Do</surname> <given-names>JP</given-names></name><name><surname>Chang</surname> <given-names>W-C</given-names></name><name><surname>Jenvay</surname> <given-names>S</given-names></name><name><surname>Miyamichi</surname> <given-names>K</given-names></name><name><surname>Luo</surname> <given-names>L</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Long-range and local circuits for top-down modulation of visual cortex processing</article-title><source>Science</source><volume>345</volume><fpage>660</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1126/science.1254126</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>S</given-names></name><name><surname>Xu</surname> <given-names>M</given-names></name><name><surname>Chang</surname> <given-names>W-C</given-names></name><name><surname>Ma</surname> <given-names>C</given-names></name><name><surname>Hoang Do</surname> <given-names>JP</given-names></name><name><surname>Jeong</surname> <given-names>D</given-names></name><name><surname>Lei</surname> <given-names>T</given-names></name><name><surname>Fan</surname> <given-names>JL</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organization of long-range inputs and outputs of frontal cortex for top-down control</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1733</fpage><lpage>1742</lpage><pub-id pub-id-type="doi">10.1038/nn.4417</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50340.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution>CNRS</institution><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bathellier</surname><given-names>Brice</given-names> </name><role>Reviewer</role><aff><institution>CNRS</institution><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Keller</surname><given-names>Georg B</given-names></name><role>Reviewer</role><aff><institution>Friedrich Miescher Institute for Biomedical Research</institution><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This study demonstrates for the first time that VIP interneurons in the mouse visual cortex display a learnt response to an expected stimulus (overstrained image repetition), which takes the form of a temporal ramping of activity before the expected stimulus. This ramping is prolonged when the expected stimulus is omitted. These results indicate that VIP cells mediate some kind of predictive information within the circuit of visual cortex.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Experience shapes activity dynamics and stimulus coding of VIP inhibitory and excitatory cells in visual cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Brice Bathellier as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Georg B Keller (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this work Garrett and colleagues investigate the effect of experience on the response of excitatory and VIP positive inhibitory neurons to repeated stimulus presentations. The key finding is that VIP interneurons exhibit anticipatory responses when the mouse has extensive experience with an image set. These anticipatory responses appear to be suppressed by the presentation of the expected image as they continue increasing when the image is omitted. In addition, the study shows that V1 neurons tend to decrease responsiveness to frequent stimuli as compared to more novel stimuli, a result that is expected and not novel.</p><p>The study by Garret et al., is elegantly designed, rigorous and compelling. The key observations are interesting and topical as not much is known about how experience (in this case visual experience) influences VIP activity and as they provide further evidence for the implementation of predictive-like codes in visual cortex. The paper is overall well written and clear, but would benefit from streamlining, to read less as a collection of results.</p><p>In addition, a number of points need to be addressed to strengthen the observations and their interpretation.</p><p>Essential revisions:</p><p>1) The paper would profit from streamlining and condensing. Currently, it reads a bit like a random collection of findings of unclear relation. The authors should focus the paper on their main finding of ramping (and omission) responses.</p><p>2) &quot;change modulation index&quot; – Figure 4D. The authors' analysis would suggest that in VIP neurons the response to the 10th presentation is larger than the response to the 1st presentation. Given Figure 4C (right panel), potentially indicating that the stimulus driven response in VIP is also larger for the 1st presentation. The 10th presentation is simply riding on top of a slow decay (and hence appears larger when not baseline subtracted – the measure to quantify response to a stimulus using calcium imaging is stimulus-induced change in dF/F). This slow decay could be the result of sustained spiking activity, or a slower calcium decay kinetics. Unless the authors want to calibrate calcium decay differences between excitatory and VIP neurons, or redo all the experiments with inter-stimulus intervals that would allow for a calcium decay to baseline, they should rework or actually remove this analysis – the most likely explanation for this is a slower calcium decay.</p><p>3) The ramping effect in VIP neurons is observed in via calcium signals, which have a relatively slow time constant Tau (even if GCAMP6f is used) and which are not temporally deconvolved. Within a timescale of a few Tau's, constant firing will lead to a ramping of the calcium profile. It would be interesting that the authors either provide deconvolved versions of the observed ramping profile or at least simulations of the calcium signals for constant firing, given the range of time constants observed for GCAMP6f in pyramidal cell and VIP neurons. Clearly, the time constants are dependent on cell types, so it would be good to discuss to which extent ramping response could be confused with (nonetheless quite interesting) steady responses.</p><p>4) Although the behavior in which the animal is involved is well controlled, the authors do not provide data on saccades or pupil dilation. Could adaptation to known stimuli be related to pupil size / attentional processes? Is there a change of pupil size with novel stimuli? Or are there saccades related to stimulus change that could contribute to the larger responses? Ideally this should be checked with pupil tracking or at least discussed.</p><p>5) It is also important to sort the analysis based on locomotion state. It has been widely studied that locomotion is highly correlated with the activity of VIP neurons. Furthermore, in this manuscript (as in other studies) VIP cell activity is correlated with each other, and possibly triggered by locomotor activity. The authors mentioned that there is no overall changes in the locomotor activity between familiar, novel or omission trials, however it is possible that locomotor changes at specific periods during the trial impacts the activity of VIP cells and influence the interpretation of the results.</p><p>6) The authors provide a measure of reaction time as the time of the first lick after stimulus change. But in a supplementary figure they show that reaction times can also be measured based on running speed. It is important to show how similar are the values and if they both do not depend on stimulus novelty.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50340.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The paper would profit from streamlining and condensing. Currently, it reads a bit like a random collection of findings of unclear relation. The authors should focus the paper on their main finding of ramping (and omission) responses.</p></disp-quote><p>We have substantially revised and streamlined the manuscript to focus on our major novel finding of ramping (and omission) activity of VIP cells. We have removed analysis and figure panels that are ancillary to this main focus. Specifically, we removed the previous Figure 4 and associated text that described short-term adaptation of responses to repeated stimulation. We have also removed analysis of “change responses” previously included in Figure 3 and Figure 4 and Supplementary figure 3 and Supplementary figure 4.</p><p>By focusing our manuscript, we have condensed the main text from 6558 words to 4603 words. This should make the manuscript and our key results more digestible. Where we added new figures (Figure 4—figure supplement 1 and Figure 5—figure supplement 1), these that provide important control analyses of running and pupil size that strengthen our conclusions as suggested by the reviewers below.</p><disp-quote content-type="editor-comment"><p>2) &quot;change modulation index&quot; – Figure 4D. The authors' analysis would suggest that in VIP neurons the response to the 10th presentation is larger than the response to the 1st presentation. Given Figure 4C (right panel), potentially indicating that the stimulus driven response in VIP is also larger for the 1st presentation. The 10th presentation is simply riding on top of a slow decay (and hence appears larger when not baseline subtracted – the measure to quantify response to a stimulus using calcium imaging is stimulus-induced change in dF/F). This slow decay could be the result of sustained spiking activity, or a slower calcium decay kinetics. Unless the authors want to calibrate calcium decay differences between excitatory and VIP neurons, or redo all the experiments with inter-stimulus intervals that would allow for a calcium decay to baseline, they should rework or actually remove this analysis – the most likely explanation for this is a slower calcium decay.</p></disp-quote><p>Following the reviewers’ suggestion to focus and streamline the manuscript we have removed this analysis of “change modulation” from Figure 4 and the text.</p><disp-quote content-type="editor-comment"><p>3) The ramping effect in VIP neurons is observed in via calcium signals which have a relatively slow time constant Tau (even if GCAMP6f is used) and which are not temporally deconvolved. Within a timescale of a few Tau's, constant firing will lead to a ramping of the calcium profile. It would be interesting that the authors either provide deconvolved versions of the observed ramping profile or at least simulations of the calcium signals for constant firing, given the range of time constants observed for GCAMP6f in pyramidal cell and VIP neurons. Clearly, the time constants are dependent on cell types, so it would be good to discuss to which extent ramping response could be confused with (nonetheless quite interesting) steady responses.</p></disp-quote><p>We have now re-analyzed all of the data in our study using temporally deconvolved calcium signals. We used a published method for L0-regularized event detection to identify ‘events’ from the dF/F traces (Jewell and Witten, 2017). All the major results of our study are unchanged in this analysis. Importantly, the ramping activity of VIP cells during inter-stimulus intervals and during stimulus omission is clearly present in these deconvolved event traces. Since this event-based analysis provides a better estimate of the underlying spike rates compared to dF/F, we have replaced all figures with analysis performed on events.</p><disp-quote content-type="editor-comment"><p>4) Although the behavior in which the animal is involved is well controlled, the authors do not provide data on saccades or pupil dilation. Could adaptation to known stimuli be related to pupil size / attentional processes? Is there a change of pupil size with novel stimuli? Or are there saccades related to stimulus change that could contribute to the larger responses? Ideally this should be checked with pupil tracking or at least discussed.</p></disp-quote><p>We collected pupil tracking data for a subset of our experiments (88 out of 101) and now provide an analysis of these data in the manuscript. We show in Figure 4—figure supplement 1 that pupil size is slightly increased in novel stimulus sessions relative to familiar sessions. In the Results section and Discussion section we describe how this could reflect a difference in arousal/attention and might contribute to the higher stimulus response magnitudes of excitatory and VIP cells in the novel image session (Figure 3). Importantly, however, a simple gain change cannot account for our main finding of a shift in VIP cell temporal dynamics between novel and familiar sessions (Figure 4—figure supplement 1 and Figure 5—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>5) It is also important to sort the analysis based on locomotion state. It has been widely studied that locomotion is highly correlated with the activity of VIP neurons. Furthermore, in this manuscript (as in other studies) VIP cell activity is correlated with each other, and possibly triggered by locomotor activity. The authors mentioned that there is no overall changes in the locomotor activity between familiar, novel or omission trials, however it is possible that locomotor changes at specific periods during the trial impacts the activity of VIP cells and influence the interpretation of the results.</p></disp-quote><p>We have now performed an analysis in which we sort our dataset according to running state (running or stationary). We show in Figure 4—figure supplement 1 and Figure 5—figure supplement 1 that VIP cells exhibit inter-stimulus and omission ramping activity both during running and stationary periods. Thus, this phenomenon of ramping activity in VIP cells does not simply reflect locomotor behavior. Moreover, we examined stimulus-triggered (and omission-triggered) locomotor changes and found these are similar for novel and familiar sessions but VIP cell dynamics are different (Figure 4—figure supplement 1 and Figure 5—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>6) The authors provide a measure of reaction time as the time of the first lick after stimulus change. But in a supplementary figure they show that reaction times can also be measured based on running speed. It is important to show how similar are the values and if they both do not depend on stimulus novelty.</p></disp-quote><p>The reviewer is correct that running speed also provides a measure of reaction time. In Figure 1—figure supplement 1 we now show how changes in running speed correspond to lick reaction time. We find that reaction times based on running are quicker (that is, mice begin to slow before they lick). However, running reaction times are very similar for both novel and familiar images and so this cannot account for differences in VIP cell dynamics.</p></body></sub-article></article>