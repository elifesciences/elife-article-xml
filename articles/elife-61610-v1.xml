<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">61610</article-id><article-id pub-id-type="doi">10.7554/eLife.61610</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Songbirds can learn flexible contextual control over syllable sequencing</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-200176"><name><surname>Veit</surname><given-names>Lena</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9566-5253</contrib-id><email>lena.veit@uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" id="author-201034"><name><surname>Tian</surname><given-names>Lucas Y</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7346-7360</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa2">‡</xref></contrib><contrib contrib-type="author" id="author-201035"><name><surname>Monroy Hernandez</surname><given-names>Christian J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3796-989X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-9068"><name><surname>Brainard</surname><given-names>Michael S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9425-9907</contrib-id><email>msb@phy.ucsf.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Center for Integrative Neuroscience and Howard Hughes Medical Institute, University of California, San Francisco</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewing Editor</role><aff><institution>Cornell University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Institute for Neurobiology, University of Tübingen, Tübingen, Germany</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>The Rockefeller University, New York, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>06</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e61610</elocation-id><history><date date-type="received" iso-8601-date="2020-07-30"><day>30</day><month>07</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-04-25"><day>25</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Veit et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Veit et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-61610-v1.pdf"/><abstract><p>The flexible control of sequential behavior is a fundamental aspect of speech, enabling endless reordering of a limited set of learned vocal elements (syllables or words). Songbirds are phylogenetically distant from humans but share both the capacity for vocal learning and neural circuitry for vocal control that includes direct pallial-brainstem projections. Based on these similarities, we hypothesized that songbirds might likewise be able to learn flexible, moment-by-moment control over vocalizations. Here, we demonstrate that Bengalese finches (<italic>Lonchura striata domestica</italic>), which sing variable syllable sequences, can learn to rapidly modify the probability of specific sequences (e.g. ‘ab-c’ versus ‘ab-d’) in response to arbitrary visual cues. Moreover, once learned, this modulation of sequencing occurs immediately following changes in contextual cues and persists without external reinforcement. Our findings reveal a capacity in songbirds for learned contextual control over syllable sequencing that parallels human cognitive control over syllable sequencing in speech.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Human speech and birdsong share numerous parallels. Both humans and birds learn their vocalizations during critical phases early in life, and both learn by imitating adults. Moreover, both humans and songbirds possess specific circuits in the brain that connect the forebrain to midbrain vocal centers.</p><p>Humans can flexibly control what they say and how by reordering a fixed set of syllables into endless combinations, an ability critical to human speech and language. Birdsongs also vary depending on their context, and melodies to seduce a mate will be different from aggressive songs to warn other males to stay away. However, so far it was unclear whether songbirds are also capable of modifying songs independent of social or other naturally relevant contexts.</p><p>To test whether birds can control their songs in a purposeful way, Veit et al. trained adult male Bengalese finches to change the sequence of their songs in response to random colored lights that had no natural meaning to the birds. A specific computer program was used to detect different variations on a theme that the bird naturally produced (for example, “ab-c” versus “ab-d”), and rewarded birds for singing one sequence when the light was yellow, and the other when it was green. Gradually, the finches learned to modify their songs and were able to switch between the appropriate sequences as soon as the light cues changed. This ability persisted for days, even without any further training.</p><p>This suggests that songbirds can learn to flexibly and purposefully modify the way in which they sequence the notes in their songs, in a manner that parallels how humans control syllable sequencing in speech. Moreover, birds can learn to do this ‘on command’ in response to an arbitrarily chosen signal, even if it is not something that would impact their song in nature.</p><p>Songbirds are an important model to study brain circuits involved in vocal learning. They are one of the few animals that, like humans, learn their vocalizations by imitating conspecifics. The finding that they can also flexibly control vocalizations may help shed light on the interactions between cognitive processing and sophisticated vocal learning abilities.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>songbird</kwd><kwd>cognitive control</kwd><kwd>motor sequencing</kwd><kwd>Bengalese finch</kwd><kwd>syntax</kwd><kwd>vocal communication</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Leopoldina German National Academy of Sciences</institution></institution-wrap></funding-source><award-id>Postdoc Fellowship</award-id><principal-award-recipient><name><surname>Veit</surname><given-names>Lena</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009559</institution-id><institution>Life Sciences Research Foundation</institution></institution-wrap></funding-source><award-id>Howard Hughes Medical Institute Fellowship</award-id><principal-award-recipient><name><surname>Veit</surname><given-names>Lena</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Brainard</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><award-id>EXROP summer fellowship</award-id><principal-award-recipient><name><surname>Monroy Hernandez</surname><given-names>Christian J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Songbirds can use arbitrary visual cues to immediately, flexibly and adaptively control syntax of learned song vocalizations in a manner that parallels human cognitive control over syllable sequencing in speech.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A crucial aspect of the evolution of human speech is the development of flexible control over learned vocalizations (<xref ref-type="bibr" rid="bib2">Ackermann et al., 2014</xref>; <xref ref-type="bibr" rid="bib9">Belyk and Brown, 2017</xref>). Humans have unparalleled control over their vocal output, with a capacity to reorder a limited number of learned elements to produce an endless combination of vocal sequences that are appropriate for current contextual demands (<xref ref-type="bibr" rid="bib30">Hauser et al., 2002</xref>). This cognitive control over vocal production is thought to rely on the direct innervation of brainstem and midbrain vocal networks by executive control structures in the frontal cortex, which have become more elaborate over the course of primate evolution (<xref ref-type="bibr" rid="bib28">Hage and Nieder, 2016</xref>; <xref ref-type="bibr" rid="bib61">Simonyan and Horwitz, 2011</xref>). However, because of the comparatively limited flexibility of vocal production in nonhuman primates (<xref ref-type="bibr" rid="bib54">Nieder and Mooney, 2020</xref>), the evolutionary and neural circuit mechanisms that have enabled the development of this flexibility remain poorly understood.</p><p>Songbirds are phylogenetically distant from humans, but they have proven a powerful model for investigating neural mechanisms underlying learned vocal behavior. Song learning exhibits many parallels to human speech learning (<xref ref-type="bibr" rid="bib23">Doupe and Kuhl, 1999</xref>); in particular, juveniles need to hear an adult tutor during a sensitive period, followed by a period of highly variable sensory-motor exploration and practice, during which auditory feedback is used to arrive at a precise imitation of the tutor song (<xref ref-type="bibr" rid="bib15">Brainard and Doupe, 2002</xref>). This capacity for vocal learning is subserved by a well-understood network of telencephalic song control nuclei. Moreover, as in humans, this vocal control network includes strong projections directly from cortical (pallial) to brainstem vocal control centers (<xref ref-type="bibr" rid="bib23">Doupe and Kuhl, 1999</xref>; <xref ref-type="bibr" rid="bib61">Simonyan and Horwitz, 2011</xref>). These shared behavioral features and neural specializations raise the question of whether songbirds might also share the capacity to learn flexible control over syllable sequencing.</p><p>Contextual variation of song in natural settings, such as territorial counter-singing or female-directed courtship song, indicate that songbirds can rapidly alter aspects of their song, including syllable sequencing and selection of song types (<xref ref-type="bibr" rid="bib18">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Heinig et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">King and McGregor, 2016</xref>; <xref ref-type="bibr" rid="bib58">Sakata et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Searcy and Beecher, 2009</xref>; <xref ref-type="bibr" rid="bib66">Trillo and Vehrencamp, 2005</xref>). However, such modulation of song structure is often described as affectively controlled (<xref ref-type="bibr" rid="bib11">Berwick et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Nieder and Mooney, 2020</xref>). For example, the presence of potential mates or rivals elicits a global and unlearned modulation of song intensity (<xref ref-type="bibr" rid="bib41">James et al., 2018</xref>) related to the singer’s level of arousal or aggression (<xref ref-type="bibr" rid="bib3">Alcami et al., 2021</xref>; <xref ref-type="bibr" rid="bib32">Heinig et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Jaffe and Brainard, 2020</xref>). Hence, while prior observations suggest that a variety of ethologically relevant factors can be integrated to influence song production in natural settings, it remains unclear whether song can be modified more flexibly by learned or cognitive factors.</p><p>Here, we tested whether Bengalese finches can learn to alter specifically targeted vocal sequences within their songs in response to arbitrarily chosen visual cues, independent of social or other natural contexts. Each Bengalese finch song repertoire includes ~5–12 acoustically distinct elements (‘syllables’) that are strung together into sequences in variable but non-random order. For a given bird, the relative probabilities of specific transitions between syllables normally remain constant over time (<xref ref-type="bibr" rid="bib55">Okanoya, 2004</xref>; <xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>), but previous work has shown that birds can gradually adjust the probabilities of alternative sequences in response to training that reinforces the production of some sequences over others. In this case, changes to syllable sequencing develop over a period of hours to days (<xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>). In contrast, we investigate here whether birds can learn to change syllable sequencing on a moment-by-moment basis in response to arbitrary visual cues that signal which sequences are adaptive at any given time. Our findings reveal that songbirds can learn to immediately, flexibly, and adaptively adjust the sequencing of selected vocal elements in response to learned contextual cues.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Bengalese finches can learn context-dependent syllable sequencing</title><p>For each bird in the study, we first identified variably produced syllable sequences that could be gradually modified using a previously described aversive reinforcement protocol (‘single context training’; <xref ref-type="bibr" rid="bib68">Tumer and Brainard, 2007</xref>; <xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>). For example, a bird that normally transitioned from the fixed syllable sequence ‘ab’ to either ‘c’ or ‘d’ (<xref ref-type="fig" rid="fig1">Figure 1A,B</xref>, sequence probability of ~36% for ‘ab-c’ and ~64% for ‘ab-d’) was exposed to an aversive burst of white noise (WN) feedback immediately after the ‘target sequence’ ‘ab-d’ was sung. In response, the bird learned over a period of days to gradually decrease the relative probability of that sequence in favor of the alternative sequence ‘ab-c’ (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This change in sequence probabilities was adaptive in that it enabled the bird to escape from WN feedback. Likewise, when the sequence, ‘ab-c’ was targeted, the probability of ‘ab-d’ increased gradually over several days of training (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). These examples are consistent with prior work that showed such sequence modifications develop over a period of several days, with the slow time course suggesting a gradual updating of synaptic connections within syllable control networks in response to performance-related feedback (<xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>). In contrast, the ability to immediately and flexibly reorder vocal elements in speech must reflect mechanisms that enable contextual factors to exert moment-by-moment control over selection and sequencing of alternative vocal motor programs. Having identified sequences for each bird for which the probability of production could be gradually modified in this manner, we then tested whether birds could be trained to rapidly switch between those same sequences in a context-dependent manner.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Bengalese finches can learn context-dependent sequencing.</title><p>(<bold>A</bold>) Example spectrogram highlighting points in song with variable sequencing. Syllables are labeled based on their spectral structure, target sequences for the different experiments (ab-c and ab-d) are marked with colored bars. Y-axis shows frequency in Hz. (<bold>B</bold>) Transition diagram with probabilities for sequences ab-c and ab-d. The sequence probability of ab-d (and complementary probability ab-c) stayed relatively constant over five days. Shaded area shows 95% confidence interval for sequence probability. Source data in <xref ref-type="supplementary-material" rid="fig1sdata3">Figure 1—source data 3</xref>. (<bold>C</bold>) Aversive reinforcement training. Schematic showing aversive WN after target sequence ab-d; spectrogram shows WN stimulus, covering part of syllable d. WN targeted to sequence ab-d led to a gradual decrease in the probability of that sequence over several days, and a complementary increase in the probability of ab-c. (<bold>D</bold>) WN targeted to ab-c led to a gradual increase in the sequence probability of ab-d. Source data in <xref ref-type="supplementary-material" rid="fig1sdata2">Figure 1—source data 2</xref>. (<bold>E</bold>) Schematic of the contextual learning protocol, with target for WN signaled by colored lights. (<bold>F</bold>) Left: Two example days of baseline without WN but with alternating blocks of green and yellow context. Colors indicate light context (black indicates periods of lights off during the night), error bars indicate SEM across song bouts in each block. Right: Average sequence probability in yellow and green blocks during baseline. Open circles show individual blocks, error bars show SEM across blocks. (<bold>G</bold>) Left: Two example days after training (WN on). Right: Average sequence probability in yellow and green blocks after training. (<bold>H</bold>) Contextual difference in sequence probability for eight trained birds before and after training (**p&lt;0.01 signed rank test). Source data in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Switch magnitude during baseline and after training for all birds, to generate <xref ref-type="fig" rid="fig1">Figure 1H</xref>, and plots like <xref ref-type="fig" rid="fig1">Figure 1F,G</xref> for all birds.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig1-data1-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig1sdata2"><label>Figure 1—source data 2.</label><caption><title>Sequence data for the example bird during single-context training, to generate <xref ref-type="fig" rid="fig1">Figure 1C,D</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig1-data2-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig1sdata3"><label>Figure 1—source data 3.</label><caption><title>Sequence data for the example bird during baseline, to generate <xref ref-type="fig" rid="fig1">Figure 1B</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig1-data3-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig1-v1.tif"/></fig><p>To determine whether Bengalese finches can learn to flexibly select syllable sequences on a moment-by-moment basis, we paired WN targeting of specific sequences with distinct contextual cues. In this context-dependent training protocol, WN was targeted to defined sequences in the bird’s song as before, but the specific target sequence varied across alternating blocks, signaled by different colored lights in the home cage (see Materials and methods). <xref ref-type="fig" rid="fig1">Figure 1E</xref> shows an example experiment, with ‘ab-d’ targeted in yellow light, and ‘ab-c’ in green light. At baseline, without WN, switches between yellow and green contexts (at random intervals of 0.5–1.5 hr) did not lead to significant changes in the relative proportion of the target sequences, indicating that there was no inherent influence of the light cues on sequence probabilities (<xref ref-type="fig" rid="fig1">Figure 1F</xref>, p(ab-d) in yellow vs. green context was 67 ± 1.6% vs. 64 ± 1.5%, p=0.17, rank-sum test, n = 53 context blocks from baseline period). Training was then initiated in which WN was alternately targeted to each sequence, over blocks that were signaled by light cues. After 2 weeks of such context-specific training, significant sequencing differences developed between light contexts that were appropriate to reduce aversive feedback in each context (<xref ref-type="fig" rid="fig1">Figure 1G</xref>, p(ab-d) in yellow vs. green context shifted to 36.5 ± 4.8% vs. 83.1 ± 3.5%, p&lt;0.01, rank-sum test, n = 22 context blocks, block duration between 1 and 2.5 hr). Likewise, for all birds trained on this protocol (n = 8), context-dependent sequencing differences developed in the appropriate direction over a period of weeks (27 ± 6% difference in probabilities between contexts after a mean of 33 days training, versus 1% ± 2% average difference in probabilities at baseline; p&lt;0.01, n = 8, signed rank test, <xref ref-type="fig" rid="fig1">Figure 1H</xref>). Thus, Bengalese finches are able to learn context-specific modifications to syllable sequencing.</p></sec><sec id="s2-2"><title>Syllable sequencing shifts immediately following switches in context</title><p>Contextual differences between different blocks could arise through an immediate shift in sequence probabilities upon entry into a new context and/or by rapid learning within each block. We examined whether trained birds exhibited any immediate shifts in their syllable sequencing when entering a new light context by computing the average probability of target sequences across songs aligned with the switch between contexts (<xref ref-type="fig" rid="fig2">Figure 2A,B</xref>, example experiment). This ‘switch-triggered average’ revealed that across all birds, switches to the yellow context were accompanied by an immediate decrease in the probability of the yellow target sequence, whereas switches out of the yellow context (and into the green context) led to an immediate increase in the yellow target sequence (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>, p&lt;0.05, signed rank test comparing first and last song, n = 8). To quantify the size of these immediate shifts, we calculated the difference in sequence probability from the last five songs in the previous context to the first five songs in the current context; this difference averaged 0.24 ± 0.06 for switches to green light and −0.22 ± 0.06 for switches to yellow light (<xref ref-type="fig" rid="fig2">Figure 2E,F</xref>). These results indicate that birds could learn to immediately recall an acquired memory of context-appropriate sequencing upon entry into each context, even before having the chance to learn from reinforcing feedback within that context.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Sequence probabilities shift immediately following a switch in context.</title><p>(<bold>A, B</bold>) Average sequence probability per song for example Bird 1 aligned to switches from green to yellow context (<bold>A</bold>) and from yellow to green context (<bold>B</bold>). Error bars indicate SEM across song bouts (n = 35 switches (<bold>A</bold>), n = 33 switches (<bold>B</bold>)). (<bold>C</bold>) Changes in sequence probability from the last song in green context to the first song in yellow context for all eight birds. Example bird in (<bold>A</bold>, <bold>B</bold>) highlighted in bold. **p&lt;0.01 signed rank test. (<bold>D</bold>) Changes in sequence probability from the last song in yellow context to the first song in green context. *p&lt;0.05 signed rank test. (<bold>E</bold>) Shift magnitudes for all birds, defined as the changes in sequence probability from the last five songs in the green context to the first five songs in the yellow context. Open circles show individual birds, error bars indicate SEM across birds. (<bold>F</bold>) Same as (<bold>E</bold>) for switches from yellow to green. Source data in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>. (<bold>G</bold>) Shift magnitudes over training time for the example bird (11 days and 49 context switches; seven of the original 56 context switches are excluded from calculations of shift magnitudes because at least one of the involved blocks contained only one or two song bouts.). (<bold>H</bold>) Trajectory of switch-aligned sequence probabilities for the example bird early in training (red) and late in training (blue). Probabilities are normalized by the sequence probability in preceding block, and plotted so that the adaptive direction is positive for both switch directions (i.e. inverting the probabilities for switches to yellow.) (<bold>I</bold>) Slopes of fits to the sequence probability trajectories over song bouts within block. Units in change of relative sequence probability per song bout. (<bold>K</bold>) Intercepts of fits to sequence probability trajectories over song bouts within block. Units in relative sequence probability. (<bold>L</bold>) Changes in slopes and changes in intercepts for five birds over the training process, determined as the slopes of linear fits to curves as in (<bold>I</bold> and <bold>K</bold>) for each bird. Source data in <xref ref-type="supplementary-material" rid="fig2sdata2">Figure 2—source data 2</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Switch magnitude between all contexts after training, to generate <xref ref-type="fig" rid="fig2">Figures 2C–F</xref> and <xref ref-type="fig" rid="fig3">3E–H</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig2-data1-v1.mat"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Summary of training data, to generate <xref ref-type="fig" rid="fig2">Figure 2L</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig2-data2-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig2-v1.tif"/></fig><p>We next asked whether training additionally led to an increased rate of learning within each context, which also might contribute to increased contextual differences over time. Indeed, such faster re-learning for consecutive encounters of the same training context, or ‘savings’, is sometimes observed in contextual motor adaptation experiments (<xref ref-type="bibr" rid="bib50">Lee and Schweighofer, 2009</xref>). To compare the magnitude of the immediate shift and the magnitude of within-block learning over the course of training, we plotted the switch-aligned sequence probabilities at different points in the training process. <xref ref-type="fig" rid="fig2">Figure 2G</xref> shows for the example bird that the magnitude of the shift (computed between the first and last five songs across context switches) gradually increased over 11 days of training. <xref ref-type="fig" rid="fig2">Figure 2H</xref> shows the switch-aligned sequence probability trajectories (as in <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>) for this bird early in training (red) and late in training (blue), binned into groups of seven context switches. Qualitatively, there was both an abrupt change in sequence probability at the onset of each block (immediate shift at time point 0) and a gradual adjustment of sequence probability within each block (within-block learning over the first 80 songs following light switch). Over the course of training, the immediate shift at the onset of each block got larger, while the gradual change within blocks stayed approximately the same (learning trajectories remained parallel over training, <xref ref-type="fig" rid="fig2">Figure 2H</xref>). Linear fits to the sequence probabilities for each learning trajectory (i.e. the right side of <xref ref-type="fig" rid="fig2">Figure 2H</xref>) reveal that, indeed, the change in sequence probability at the onset of blocks (i.e. intercepts) increased over the training process (<xref ref-type="fig" rid="fig2">Figure 2K</xref>), while the rate of change within blocks (i.e. slopes) stayed constant (<xref ref-type="fig" rid="fig2">Figure 2I</xref>). To quantify this across birds, we measured the change over the course of learning in both the magnitude of immediate shifts (estimated as the intercepts from linear fits) and the rate of within-block learning (estimated as the slopes from linear fits). As for the example bird, we found that the rate of learning within each block stayed constant over time for all five birds (<xref ref-type="fig" rid="fig2">Figure 2L</xref>). In contrast, the magnitude of immediate shifts increased over time for all birds (<xref ref-type="fig" rid="fig2">Figure 2L</xref>). These analyses indicate that adjustments to sequence probability reflect two dissociable processes, an immediate cue-dependent shift in sequence probability at the beginning of blocks, that increases with contextual training, and a gradual adaptation of sequence probability within blocks, that does not increase with contextual training.</p></sec><sec id="s2-3"><title>Visual cues in the absence of reinforcement are sufficient to evoke sequencing changes</title><p>The ability of Bengalese finches to implement an immediate shift in sequencing on the first rendition in a block – and thus before they have a chance to learn from reinforcing feedback – argues that they can maintain context-specific motor memories and use contextual visual cues to anticipate correct sequencing in each context. To explicitly test whether birds can flexibly switch between sequencing appropriate for distinct contexts using only visual cues, we included short probe blocks which presented the same light cues without WN stimulation. Probe blocks were interspersed in the sequence of training blocks so that each switch between types of blocks was possible and, on average, every third switch was into a probe block (see Materials and methods). Light switches into probe blocks were associated with similar magnitude shifts in sequence probability as switches into WN blocks of the corresponding color (−0.22 ± 0.06 to both yellow WN and yellow probe blocks from green WN blocks, p=0.94, signed rank test; 0.24 ± 0.06 to green WN and 0.23 ± 0.07 to green probe blocks from yellow WN blocks, p=0.64, signed rank test). As the most direct test of whether light cues alone evoke adaptive sequencing changes, we compared songs immediately before and after switches between probe blocks without intervening WN training blocks (probe-probe switches). <xref ref-type="fig" rid="fig3">Figure 3A,B</xref> shows song bouts for one example bird (Bird 2) which were sung consecutively across a switch from yellow probe to green probe blocks. In the first song following the probe-probe switch, the yellow target sequence (‘f-ab’) was more prevalent, and the green target sequence (‘n-ab’) was less prevalent, and such an immediate effect was also apparent in the average sequence probabilities for this bird aligned to probe–probe switches (<xref ref-type="fig" rid="fig3">Figure 3C,D</xref>). Similar immediate and appropriately directed shifts in sequencing at switches between probe blocks were observed for all eight birds (<xref ref-type="fig" rid="fig3">Figure 3E,F</xref>, p&lt;0.05 signed rank test, n = 8), with average shifts in sequence probabilities of −0.21 ± 0.09 and 0.17 ± 0.08 (<xref ref-type="fig" rid="fig3">Figure 3G,H</xref>). The presence of such changes in the first songs sung after probe–probe switches indicates that visual cues alone are sufficient to cause anticipatory, learned shifts between syllable sequences.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Contextual cues alone are sufficient to enable immediate shifts in syllable sequencing.</title><p>(<bold>A,B</bold>) Examples of songs sung by Bird 2 immediately before (<bold>A</bold>) and after (<bold>B</bold>) a switch from a yellow probe block to a green probe block (full song bouts in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Scale for x-axis is 500 ms; y-axis shows frequency in Hz. (<bold>C, D</bold>) Average sequence probability per song for Bird 2 aligned to switches from green probe to yellow probe blocks (<bold>C</bold>) and from yellow probe to green probe blocks (<bold>D</bold>). Error bars indicate SEM across song bouts (n = 14 switches (<bold>C</bold>), 11 switches (<bold>D</bold>)). (<bold>E, F</bold>) Average sequence probabilities for all eight birds at the switch from the last song in green probe context and the first song in yellow probe context, and vice versa. Example Bird 2 is shown in bold. *p&lt;0.05 signed rank test. (<bold>G, H</bold>) Shift magnitudes for probe–probe switches for all birds. Open circles show individual birds; error bars indicate SEM across birds. Source data in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Example song bouts surrounding a probe–probe context switch.</title><p>Full song bouts (same as in <xref ref-type="fig" rid="fig3">Figure 3A,B</xref>) sung by Bird 2 immediately before and after a switch from a yellow probe block to a green probe block. Scale for x-axis is 1 s; y-axis shows frequency in Hz. The recording program was set to never switch lights during an ongoing song recording, so the time of light switch in between these two recordings cannot be shown.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig3-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Contextual changes are specific to target sequences</title><p>A decrease in the probability of a target sequence in response to contextual cues must reflect changes in the probabilities of transitions leading up to the target sequence. However, such changes could be restricted to the transitions that immediately precede the target sequence, or alternatively could affect other transitions throughout the song. For example, for the experiment illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>, the prevalence of the target sequence ‘ab-d’ was appropriately decreased in the yellow context, in which it was targeted. The complete transition diagram and corresponding transition matrix for this bird (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>) reveal that there were four distinct branch points at which syllables were variably sequenced (after ‘cr’, ‘wr’, ‘i’, and ‘aab’). Therefore, the decrease in the target sequence ‘ab-d’ could have resulted exclusively from an increase in the probability of the alternative transition ‘ab-c’ at the branch point following ‘aab’. However, a reduction in the prevalence of the target sequence could also have been achieved by changes in the probability of transitions earlier in song such that the sequence ‘aab’ was sung less frequently. To investigate the extent to which contextual changes in probability were specific to transitions immediately preceding target sequences, we calculated the difference between transition matrices in the yellow and green probe contexts (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). This difference matrix indicates that changes to transition probabilities were highly specific to the branch point immediately preceding the target sequences (specificity was defined as the proportion of total changes which could be attributed to the branch points immediately preceding target sequences; specificity for branch point ‘aab’ was 83.2%). Such specificity to branch points that immediately precede target sequences was typical across experiments, including cases in which different branch points preceded each target sequence (<xref ref-type="fig" rid="fig4">Figure 4D–F</xref>, specificity 96.9%). Across all eight experiments, the median specificity of changes to the most proximal branch points was 84.95%, and only one bird, which was also the worst learner in the contextual training paradigm, had a specificity of less than 50% (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). Hence, contextual changes were specific to target sequences and did not reflect the kind of global sequencing changes that characterize innate social modulation of song structure (<xref ref-type="bibr" rid="bib58">Sakata et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Sossinka and Böhner, 1980</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Contextual changes are local to the target sequences.</title><p>(<bold>A</bold>) Transition diagram for the song of Bird 6 (spectrogram in <xref ref-type="fig" rid="fig1">Figure 1</xref>) in yellow probe context. Sequences of syllables with fixed transition patterns (e.g. ‘aab’) as well as repeat phrases and introductory notes have been summarized as single states to simplify the diagram. (<bold>B</bold>) Transition matrix for the same bird, showing same data as in (<bold>A</bold>). (<bold>C</bold>) Differences between the two contexts are illustrated by subtracting the transition matrix in the yellow context from the one in the green context, so that sequence transitions which are more frequent in green context are positive (colored green) and sequence transitions which are more frequent in yellow are negative (colored yellow). For this bird, the majority of contextual differences occurred at the branch point (‘aab’) which most closely preceded the target sequences (‘ab-c’ and ‘ab-d’), while very little contextual difference occurred at the other three branch points (‘i’, ‘wr’, ‘cr’). (<bold>D–F</bold>) Same for Bird 2 for which two different branch points (‘f’ and ‘n’) preceded the target sequences (‘f-abcd’ and ‘n-abcd’) (spectrogram in <xref ref-type="fig" rid="fig3">Figure 3</xref>). (<bold>G</bold>) Proportion of changes at the branch point(s) most closely preceding the target sequences, relative to the total magnitude of context differences for each bird (see Materials and methods). Most birds exhibited high specificity of contextual changes to the relevant branch points. Source data in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Overview of different experimental parameters and song features for each bird, to generate (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig4-data1-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Possible explanations for differences in contextual learning.</title><p>Correlations of the magnitude of contextual differences with the birds’ age (<bold>A</bold>), transition entropy of the entire song (<bold>B</bold>), transition entropy at the branch points preceding the target sequences (<bold>C</bold>), and distance of the target sequence from the immediately preceding branch points (<bold>D</bold>). None of these variables were significantly correlated with the degree of contextual learning that birds expressed, p=0.81 (<bold>A</bold>), p=0.24 (<bold>B</bold>), p=0.21 (<bold>C</bold>), p=0.28 (<bold>D</bold>). Red numbers indicate correlation coefficients. Source data in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig4-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Distinct sequence probabilities are specifically associated with different visual cues</title><p>Our experiments establish that birds can shift between two distinct sequencing states in response to contextual cues. In order to test whether birds were capable of learning to shift to these two states from a third neutral context, we trained a subset of three birds with three different color-cued contexts. For these birds, after completion of training with WN targeted to distinct sequences in yellow and green contexts (as described above), we introduced interleaved blocks cued by white light in which there was no reinforcement. After this additional training, switches from the unreinforced context elicited changes in opposite directions for the green and yellow contexts (example bird <xref ref-type="fig" rid="fig5">Figure 5A</xref>). All birds (n = 3) showed adaptive sequencing changes for the first song bout in probe blocks (<xref ref-type="fig" rid="fig5">Figure 5B,C</xref>) as well as immediate shifts in the adaptive directions for all color contexts (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, 0.11 ± 0.04 and 0.19 ± 0.05 for switches to green WN and green probe blocks, respectively; −0.15 ± 0.06 and −0.09 ± 0.02 for switches to yellow WN and yellow probe blocks, respectively). While additional data would be required to establish the number of distinct associations between contexts and sequencing states that can be learned, these findings suggest that birds can maintain at least two distinct sequencing states separate from a ‘neutral’ state and use specific associations between cue colors and sequencing states to rapidly shift sequencing in distinct directions for each context.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Contextual cues allow shifts in both directions.</title><p>(<bold>A</bold>) Sequence probability for Bird 2 at the switch from neutral context to yellow and green WN contexts, as well as yellow and green probe contexts (no WN). Error bars indicate SEM across song bouts (n = 68 switches [green WN], 78 switches [yellow WN], 27 switches [green probe], 24 switches [yellow probe]). (<bold>B, C</bold>) Sequence probabilities for three birds for the last song in neutral context and the first song in the following probe context. Example bird in (<bold>A</bold>) highlighted in bold. (<bold>D</bold>) Shift magnitude for three birds at the switch from neutral context to all other contexts. Open circles show individual birds; error bars indicate SEM across birds. Source data in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Switch magnitude during third context experiment, to generate <xref ref-type="fig" rid="fig5">Figure 5B–D</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-61610-fig5-data1-v1.mat"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-fig5-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Speech, thought, and many other behaviors are composed of ordered sequences of simpler elements. The flexible control of sequencing is thus a fundamental aspect of cognition and motor function (<xref ref-type="bibr" rid="bib4">Aldridge and Berridge, 2002</xref>; <xref ref-type="bibr" rid="bib44">Jin and Costa, 2015</xref>; <xref ref-type="bibr" rid="bib65">Tanji, 2001</xref>). While the flexibility of human speech is unrivaled, our contextual training paradigm revealed a simpler, parallel capacity in birds to produce distinct vocal sequences in response to arbitrary contextual cues. The colors of the cues had no prior relevance to the birds, so that their meaning had to be learned as a new association between cues and the specific vocal sequences that were contextually appropriate (i.e. that escaped WN, given the current cues). Learned modulation of sequencing was immediately expressed in response to changes in cues, persisted following termination of training, and was largely restricted to the targeted sequences, without gross modifications of global song structure. Hence, for song, like speech, the ordering of vocal elements can be rapidly and specifically reconfigured to achieve learned, contextually appropriate goals. This shared capacity for moment-by-moment control of vocal sequencing in humans and songbirds suggests that the avian song system could be an excellent model for investigating how neural circuits enable flexible and adaptive reconfiguration of motor output in response to different cognitive demands.</p><sec id="s3-1"><title>Flexible control of vocalizations</title><p>Our demonstration of contextual control over the ordering of vocal elements in the songbird builds on previous work showing that a variety of animals can learn to emit or withhold innate vocalizations in response to environmental or experimentally imposed cues. For example, nonhuman primates and other animals can produce alarm calls that are innate in their acoustic structure, but that are deployed in a contextually appropriate fashion (<xref ref-type="bibr" rid="bib54">Nieder and Mooney, 2020</xref>; <xref ref-type="bibr" rid="bib64">Suzuki and Zuberbühler, 2019</xref>; <xref ref-type="bibr" rid="bib75">Wheeler and Fischer, 2012</xref>). Similarly, animals, including birds, can be trained to control their vocalizations in an experimental setting, by reinforcing the production of innate vocalizations in response to arbitrary cues to obtain food or water rewards (<xref ref-type="bibr" rid="bib16">Brecht et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Hage and Nieder, 2013</xref>; <xref ref-type="bibr" rid="bib54">Nieder and Mooney, 2020</xref>; <xref ref-type="bibr" rid="bib56">Reichmuth and Casey, 2014</xref>). In relation to these prior findings, our results demonstrate a capacity to flexibly reorganize the sequencing of learned vocal elements, rather than select from a fixed set of innate vocalizations, in response to arbitrary cues. This ability to contextually control the ordering, or syntax, of specifically targeted syllable transitions within the overall structure of learned song parallels the human capacity to differentially sequence a fixed set of syllables in speech.</p><p>The ability to alter syllable sequencing in a flexible fashion also contrasts with prior studies that have demonstrated modulation of vocalizations in more naturalistic settings. For example, songs produced in the context of courtship and territorial or aggressive encounters (‘directed song’) differ in acoustic structure from songs produced in isolation (‘undirected song’) (<xref ref-type="bibr" rid="bib58">Sakata et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Searcy and Beecher, 2009</xref>). This modulation of song structure by social context is characterized by global changes to the intensity of song production, with directed songs exhibiting faster tempo, and greater stereotypy of both syllable structure and syllable sequencing, than undirected songs (<xref ref-type="bibr" rid="bib58">Sakata et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Searcy and Beecher, 2009</xref>; <xref ref-type="bibr" rid="bib62">Sossinka and Böhner, 1980</xref>). This and other ethologically relevant modulation of song intensity may serve to communicate the singer’s affective state, such as level of arousal or aggression (<xref ref-type="bibr" rid="bib3">Alcami et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Hedley et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Heinig et al., 2014</xref>), and may largely reflect innate mechanisms (<xref ref-type="bibr" rid="bib41">James et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Kojima and Doupe, 2011</xref>) mediated by hypothalamic and neuromodulatory inputs to premotor regions (<xref ref-type="bibr" rid="bib11">Berwick et al., 2011</xref>; <xref ref-type="bibr" rid="bib24">Gadagkar et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">James et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Nieder and Mooney, 2020</xref>). In contrast, here we show that birds can learn to locally modulate specific features of their songs (i.e. individually targeted syllable transitions) in response to arbitrarily assigned contextual cues that have no prior ethological relevance.</p></sec><sec id="s3-2"><title>Evolution of control over vocal sequencing</title><p>The capacity for moment-by-moment adjustment of vocalizations in response to arbitrary learned cues may depend on similar capacities that evolved to enable appropriate modulation of vocalizations in ethologically relevant natural contexts. For example, some species of songbirds preferentially sing different song types depending on factors such as time of day, location of the singer, or the presence of an audience (<xref ref-type="bibr" rid="bib3">Alcami et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Hedley et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">King and McGregor, 2016</xref>; <xref ref-type="bibr" rid="bib59">Searcy and Beecher, 2009</xref>; <xref ref-type="bibr" rid="bib66">Trillo and Vehrencamp, 2005</xref>). Even birds with only a single song type, such as Bengalese finches, vary parameters of their song depending on social context, including the specific identity of the listener (<xref ref-type="bibr" rid="bib18">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Heinig et al., 2014</xref>; <xref ref-type="bibr" rid="bib58">Sakata et al., 2008</xref>). The ability to contextually control vocalizations is also relevant for the customization of vocal signatures for purposes of individual and group recognition (<xref ref-type="bibr" rid="bib72">Vignal et al., 2004</xref>) and to avoid overlap and enhance communication during vocal turn-taking and in response to environmental noises (<xref ref-type="bibr" rid="bib10">Benichov and Vallentin, 2020</xref>; <xref ref-type="bibr" rid="bib17">Brumm and Zollinger, 2013</xref>). Such capacities for vocal control likely reflect evolutionary advantages of incorporating sensory and contextual information about conspecifics and the environment in generating increasingly sophisticated vocal signaling. Our results indicate a latent capacity to integrate arbitrary sensory signals into the adaptive deployment of vocalizations in songbirds and suggest that some of the contextual control observed in natural settings may likewise rely on learned associations and other cognitive factors. Perhaps evolutionary pressures to develop nuanced social communication led to the elaboration of cortical (pallial) control over brainstem vocal circuitry (<xref ref-type="bibr" rid="bib28">Hage and Nieder, 2016</xref>), and thereby established a conduit that facilitated the integration of progressively more abstract cues and internal states in that control.</p></sec><sec id="s3-3"><title>Neural implementation of context-dependent vocal motor sequencing</title><p>The ability of birds to switch between distinct motor programs using visual cues is reminiscent of contextual speech and motor control studies in humans. For example, human subjects in both laboratory studies and natural settings can learn multiple ‘states’ of vocal motor adaptation and rapidly switch between them using contextual information (<xref ref-type="bibr" rid="bib35">Houde and Jordan, 2002</xref>; <xref ref-type="bibr" rid="bib47">Keough and Jones, 2011</xref>; <xref ref-type="bibr" rid="bib57">Rochet-Capellan and Ostry, 2011</xref>). Similarly, subjects can learn two separate states of motor adaptation for other motor skills, such as reaching, and switch between them using cues or other cognitive strategies (<xref ref-type="bibr" rid="bib20">Cunningham and Welch, 1994</xref>). Models of such context-dependent motor adaptation frequently assume at least two parallel processes (<xref ref-type="bibr" rid="bib1">Abrahamse et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Ashe et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Green and Abutalebi, 2013</xref>; <xref ref-type="bibr" rid="bib33">Hikosaka et al., 1999</xref>; <xref ref-type="bibr" rid="bib50">Lee and Schweighofer, 2009</xref>; <xref ref-type="bibr" rid="bib52">McDougle et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Rochet-Capellan and Ostry, 2011</xref>; <xref ref-type="bibr" rid="bib77">Wolpert et al., 2011</xref>), one that is more flexible, and sensitive to contextual information (<xref ref-type="bibr" rid="bib52">McDougle et al., 2016</xref>), and a second that cannot readily be associated with contextual cues and is only gradually updated during motor adaptation (<xref ref-type="bibr" rid="bib36">Howard et al., 2013</xref>). Specifically, in support of such a two-process model, <xref ref-type="bibr" rid="bib38">Imamizu and Kawato, 2009</xref> and <xref ref-type="bibr" rid="bib37">Imamizu et al., 2007</xref> found that contextual information can drive rapid shifts in adaptation at the beginning of new blocks, without affecting the rate of adaptation within blocks. The similar separation in our study between rapid context-dependent shifts in sequence probability at the onset of blocks, and gradual adaptation within blocks that does not improve with training (<xref ref-type="fig" rid="fig2">Figure 2G–L</xref>), suggests that such contextual sequence learning in the Bengalese finch may also be enabled by two distinct processes.</p><p>Humans studies of two-process models suggest that slow adaptation occurs primarily within primary motor structures, while fast context-dependent state switches, including for cued switching between languages in bilinguals, engage more frontal areas involved in executive control (<xref ref-type="bibr" rid="bib12">Bialystok, 2017</xref>; <xref ref-type="bibr" rid="bib14">Blanco-Elorrieta and Pylkkänen, 2016</xref>; <xref ref-type="bibr" rid="bib22">De Baene et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Imamizu and Kawato, 2009</xref>). In songbirds, the gradual adaptation of sequence probabilities within blocks might likewise be controlled by motor and premotor song control structures, while visual contextual cues could be processed in avian structures analogous to mammalian prefrontal cortex, outside the song system. For example, the association area nidopallium caudolaterale (<xref ref-type="bibr" rid="bib26">Güntürkün, 2005</xref>), is activated by arbitrary visual cues that encode learned rules (<xref ref-type="bibr" rid="bib71">Veit and Nieder, 2013</xref>; <xref ref-type="bibr" rid="bib70">Veit et al., 2015</xref>), and this or other avian association areas (<xref ref-type="bibr" rid="bib42">Jarvis et al., 2013</xref>) may serve as an intermediate representation of the arbitrary contextual cues that can drive rapid learned shifts in syllable sequencing.</p><p>At the level of song motor control, our results indicate a greater capacity for rapid and flexible adjustment of syllable transition probabilities than previously appreciated. Current models of song production include networks of neurons in the vocal premotor nucleus HVC responsible for the temporal control of individual syllables, which are linked together by activity in a recurrent loop through brainstem vocal centers (<xref ref-type="bibr" rid="bib5">Andalman et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Ashmore et al., 2005</xref>; <xref ref-type="bibr" rid="bib19">Cohen et al., 2020</xref>; <xref ref-type="bibr" rid="bib29">Hamaguchi et al., 2016</xref>). At branch points in songs with variable syllable sequencing, one influential model posits that which syllable follows a branch point is determined by stochastic processes that depend on the strength of the connections between alternative syllable production networks, and thus dynamics local to HVC (<xref ref-type="bibr" rid="bib43">Jin, 2009</xref>; <xref ref-type="bibr" rid="bib45">Jin and Kozhevnikov, 2011</xref>; <xref ref-type="bibr" rid="bib67">Troyer et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Zhang et al., 2017</xref>). Such models could account for a gradual adjustment of sequence probabilities over a period of hours or days (<xref ref-type="bibr" rid="bib51">Lipkind et al., 2013</xref>; <xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>) through plasticity of motor control parameters, such as the strength of synaptic connections within HVC. However, our results demonstrate that there is not a single set of relatively fixed transition probabilities that undergo gradual adjustments, as could be captured in synaptic connectivity of branched syllable control networks. Rather, the song system has the capacity to maintain distinct representations of transition probabilities and can immediately switch between those in response to visual cues. HVC receives a variety of inputs that potentially could convey such visual or cognitive influences on sequencing (<xref ref-type="bibr" rid="bib13">Bischof and Engelage, 1985</xref>; <xref ref-type="bibr" rid="bib21">Cynx, 1990</xref>; <xref ref-type="bibr" rid="bib60">Seki et al., 2008</xref>; <xref ref-type="bibr" rid="bib69">Ullrich et al., 2016</xref>; <xref ref-type="bibr" rid="bib76">Wild, 1994</xref>), and one of these inputs, Nif, has previously been shown to be relevant for sequencing (<xref ref-type="bibr" rid="bib34">Hosino and Okanoya, 2000</xref>; <xref ref-type="bibr" rid="bib73">Vyssotski et al., 2016</xref>). It therefore is likely that the control of syllable sequence in Bengalese finches involves a mix of processes local to nuclei of the song motor pathway (<xref ref-type="bibr" rid="bib8">Basista et al., 2014</xref>; <xref ref-type="bibr" rid="bib78">Zhang et al., 2017</xref>) as well as inputs that convey a variety of sensory feedback and contextual information. The well-understood circuitry of the avian song system makes this an attractive model to investigate how such top-down pathways orchestrate the kind of contextual control of vocalizations demonstrated in this study, and more broadly to uncover how differing cognitive demands can flexibly and adaptively reconfigure motor output.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects and sound recordings</title><p>The experiments were carried out on eight adult male Bengalese finches (<italic>Lonchura striata</italic>) obtained from the lab’s breeding colony (age range 128–320 days post-hatch, median 178 days, at start of experiment). Birds were placed in individual sound-attenuating boxes with continuous monitoring and auditory recording of song. Song was recorded using an omnidirectional microphone above the cage. We used custom software for the online recognition of target syllables and real-time delivery of short 40 ms bursts of WN depending on the syllable sequence (<xref ref-type="bibr" rid="bib68">Tumer and Brainard, 2007</xref>; <xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>). This LabView program, EvTAF, is included as an executable file with this submission, and further support is available from the corresponding authors upon request. All procedures were performed in accordance with animal care protocols approved by the University of California, San Francisco Institutional Animal Care and Use Committee (IACUC).</p></sec><sec id="s4-2"><title>Training procedure and blocks</title><p>Bengalese finch song consists of a discrete number of vocal elements, called syllables, that are separated by periods of silence. At the start of each experiment, a template was generated to recognize a specific sequence of syllables (the target sequence) for each bird based on their unique spectral structure. In the context-dependent auditory feedback protocol, the target sequence that received aversive WN feedback switched between blocks of different light contexts. Colored LEDs (<ext-link ext-link-type="uri" xlink:href="https://www.superbrightleds.com/">superbrightleds.com</ext-link>, St. Louis, MO; green 520 nm, amber 600 nm) produced two visually distinct environments (green and yellow) to serve as contextual cues to indicate which sequences would elicit WN and which would ‘escape’ (i.e. not trigger WN). We wanted to test whether the birds would be able to associate song changes with any arbitrary visual stimulus; therefore, there was no reason to choose these specific colors, and the birds’ color perception in this range should not matter, as long as they were able to discriminate the colors. The entire day was used for data acquisition by alternating the two possible light contexts. We determined sensitivity and specificity of the template to the target sequence on a randomly selected set of 20 song bouts on which labels and delivery of WN was hand-checked. Template sensitivity was defined as follows: sensitivity = (number of correct hits)/(total number of target sequences). The average template sensitivity across experiments was 91.3% (range 75.2–100%). Template specificity was defined as: specificity = (number of correct escapes)/(number of correct escapes plus number of false alarms), where correct escapes were defined as the number of target sequences of the currently inactive context that were not hit by WN, and false alarms were defined as any WN that was delivered either on the target sequence of the currently inactive context, or anywhere else in song. The average template specificity was 96.7% (range 90.6–100%).</p><p>At the start of each experiment, before WN training, songs were recorded during a baseline period in which cage illumination was switched between colors at random intervals. Songs from this baseline period were separately analyzed for each light color to confirm that there was no systematic, unlearned effect of light cues on sequencing before training. During initial training, cage illumination was alternatingly switched between colors at random intervals. Intervals were drawn from uniform distributions which differed between birds (60–150 min [four birds], 10–30 min [two birds], 60–240 min [one bird], 30–150 min [one bird]). Different training schedules were assigned to birds arbitrarily and were not related to a bird’s performance. After an extended period of training (average 33 days, range 12–79 days), probe blocks without WN were included, to test whether sequencing changes could be elicited by visual cues alone. During this period, probe blocks were interspersed with WN training blocks. Probe blocks made up approximately one third of total blocks (10 of 34 blocks in the sequence) and 7–35% of total time, depending on the bird. The duration of probe blocks was typically shorter or equal to the duration of WN blocks (10–30 min for six birds, 30–120 min for one bird, 18–46 min for one bird). The total duration of the experiment, consisting of baseline, training, and probe periods, was on average 52 days. During this period, birds sang 226 (range 66–356) bouts per day during baseline days and 258 (range 171–368) bouts per day during the period of probe collection at the end of training (14% increase). The average duration of song bouts also changed little, with both the average number of target sequences per bout (8.7 during baseline, 7.7 during probes, 7% decrease) and the average number of syllables per bout (74 during baseline, 71 during probes, 2% decrease) decreasing slightly. In addition to the eight birds that completed this training paradigm, three birds were started on contextual training but never progressed to testing with probe blocks, because they did not exhibit single-context learning (n = 1); because of technical issues with consistent targeting at branch points, (n = 1); or because they lost sequence variability during initial stages of training (n = 1); these birds are excluded from the results. Of the eight birds that completed training, three birds exhibited relatively small context-dependent changes in sequencing (<xref ref-type="fig" rid="fig1">Figure 1H</xref>). We examined several variables to assess whether they could account for differences in the magnitude of learning across birds, including the bird’s age, overall transition entropy of the song (<xref ref-type="bibr" rid="bib46">Katahira et al., 2013</xref>), transition entropy at the targeted branch points (<xref ref-type="bibr" rid="bib74">Warren et al., 2012</xref>), as well as the distance between the WN target and the closest preceding branch point in the sequence. None of these variables were significantly correlated with the degree of contextual learning that birds expressed (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), and consequently, all birds were treated as a single group in analysis and reporting of results. In a subset of experiments (n = 3), after completing measurements with probe blocks, we added a third, neutral context (<xref ref-type="fig" rid="fig5">Figure 5</xref>), signaled by white light, in which there was no WN reinforcement.</p></sec><sec id="s4-3"><title>Syllable sequence annotation</title><p>Syllable annotation for data analysis was performed offline. Each continuous period of singing that was separated from others by at least 2 s of silence was treated as an individual ‘song’ or ‘song bout’. Song was bandpass filtered between 500 Hz and 10,000 Hz and segmented into syllables and gaps based on amplitude threshold and timing parameters determined manually for each bird. A small sample of songs (approximately 20 song bouts) was then annotated manually based on visual inspection of spectrograms. These data were used to train an offline autolabeler (‘hybrid-vocal-classifier’, <xref ref-type="bibr" rid="bib53">Nicholson, 2021</xref>), which was then used to label the remaining song bouts. Autolabeled songs were processed further in a semi-automated way depending on each bird’s unique song, for example to separate or merge syllables that were not segmented correctly (detected by their duration distributions), to deal with WN covering syllables (detected by its amplitude), and to correct autolabeling errors detected based on the syllable sequence. A subset of songs was inspected manually for each bird to confirm correct labeling.</p></sec><sec id="s4-4"><title>Sequence probability analyses</title><p>Sequence probability was first calculated within each song bout as the frequency of the yellow target sequence relative to the total number of yellow and green target sequences: <inline-formula><mml:math id="inf1"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula>. Note that this differs from transition probabilities at branch points in song in that it ignores possible additional syllable transitions at the branch point, and does not require the targeted sequences to be directly following the same branch point. For example for the experiment in <xref ref-type="fig" rid="fig3">Figure 3</xref>, the target sequences were ‘n-ab’ and ‘f-ab’, so the syllable covered by WN (‘b’ in both contexts) was two to three syllables removed from the respective branch point in the syllable sequence (‘n-f’ vs. ‘n-a’ or ‘f-n’ vs. ‘f-a’). Note also that units of sequence probability are in percent; therefore, reported changes in percentages (e.g. <xref ref-type="fig" rid="fig1">Figures 1H</xref> and <xref ref-type="fig" rid="fig2">2E,F</xref>) describe absolute changes in sequence probability, which reflect the proportion of each target sequence, not percent changes. Song bouts that did not contain either of the two target sequences were discarded. In the plots of sequence probability over several days in <xref ref-type="fig" rid="fig1">Figure 1A–C</xref>, we calculated sequence probability for all bouts on a given day (average n = 1854 renditions of both target sequences per day). We estimated 95% confidence intervals by approximation with a normal distribution as <inline-formula><mml:math id="inf2"><mml:mi>p</mml:mi><mml:mo>±</mml:mo><mml:mi>z</mml:mi><mml:mi>*</mml:mi><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>*</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:msqrt></mml:math></inline-formula> with <inline-formula><mml:math id="inf3"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and z = 1.96. Context switches were processed to include only switches between adjacent blocks during the same day, that is excluding overnight switches and treating blocks as separate contexts if one day started with the same color that had been the last color on the previous day. If a bird did not produce any song during one block, this block was merged with any neighboring block of the same color (e.g. green probe without songs before green WN, where the context switch would not be noticeable for the bird). If the light color switched twice (or more) without any song bouts, those context switches were discarded.</p><p>In order to reduce variability associated with changes across individual song bouts, shift magnitude was calculated as the difference between the first five song bouts in the new context and the last five song bouts in the old context. Only context switches with at least three song bouts in each adjacent block were included in analyses of shift magnitude. In plots showing songs aligned to context switches, the x-axis is limited to show only points for which at least half of the blocks contributed data (i.e. in <xref ref-type="fig" rid="fig2">Figure 2D</xref>, half of the green probe blocks contained at least six songs). All statistical tests were performed with MATLAB. We used non-parametric tests to compare changes across birds (Wilcoxon rank-sum test for unpaired data, Wilcoxon signed-rank test for paired data), because with only eight birds/data points, it is more conservative to assume that data are not Gaussian distributed.</p></sec><sec id="s4-5"><title>Analysis of acquisition</title><p>In order to investigate how context-dependent performance developed over training (<xref ref-type="fig" rid="fig2">Figure 2G–L</xref>), we quantified changes to sequence probabilities across block switches for five birds for which we had a continuous record from the onset of training. Sequence probability curves (e.g. <xref ref-type="fig" rid="fig2">Figure 2H</xref>) for yellow switches were inverted so that both yellow and green switches were plotted in the same direction, aligned by the time of context switches, and were cut off at a time point relative to context switches where fewer than five switches contributed data. We then subtracted the mean pre-switch value from each sequence probability curve. For visual display of the example bird, sequence probability curves were smoothed with a nine bout boxcar window and displayed in bins of seven context switches. To calculate the slope of slopes and slope of intercepts (<xref ref-type="fig" rid="fig2">Figure 2L</xref>), we calculated a linear fit to the post-switch parts of the unsmoothed sequence probability curve for each individual context switch.</p></sec><sec id="s4-6"><title>Specificity to relevant branch points</title><p>To calculate the specificity of the context difference to the targeted branch points in song, we generated transition diagrams for each bird. To simplify the diagrams, introductory notes were summarized into a single introductory state. Introductory notes were defined for each bird as up to three syllables occurring at the start of song bouts before the main motif, which tended to be quieter, more variable, with high probabilities to repeat and to transition to other introductory notes. Repeat phrases were also summarized into a single state. Motifs, or chunks, in the song with fixed order of syllables were identified by the stereotyped transitions and short gap durations between syllables in the motif (<xref ref-type="bibr" rid="bib39">Isola et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Suge and Okanoya, 2010</xref>) and were also summarized as a single state in the diagram. Sometimes, the same syllable can be part of several fixed chunks (<xref ref-type="bibr" rid="bib46">Katahira et al., 2013</xref>), in which case it may appear several times in the transition diagram. We then calculated the difference between the transition matrices for the two probe contexts at each transition that was a branch point (defined as more than 3% and less than 97% transition probability). These context differences were split into ‘targeted branch points’, i.e., the branch point or branch points most closely preceding the target sequences in the two contexts, and ‘non-targeted branch points’, i.e., all other branch points in the song. We calculated the proportion of absolute contextual difference in the transition matrix that fell to the targeted branch points, for example for the matrix in <xref ref-type="fig" rid="fig4">Figure 4C</xref> (44 + 45)/(44 + 45 + 6+6 + 1+1 + 2+2)=83.2%. Typically, birds with clear contextual differences at the target sequence also had high specificity of sequence changes to the targeted branch points.</p><p>To calculate the transition entropy of baseline song, we again summarized introductory notes into a single introductory state. In addition, the same syllables as part of multiple fixed motifs, or in multiple positions within the same fixed motif, were renamed as different syllables, so as not to count as sequence variability what was really a stereotyped sequence (i.e. a-b 50% and b-c 50% in the fixed sequence ‘abbc’). Transition entropy was then calculated as in <xref ref-type="bibr" rid="bib46">Katahira et al., 2013</xref>: with x denoting the preceding syllable and y denoting the current syllable, over all syllables in the song.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Alla Karpova, Jon Sakata, Dave Mets, William Mehaffey, Assaf Breska, and Guy Avraham for helpful discussions and comments on earlier versions of this manuscript. This work was supported by the Howard Hughes Medical Institute. Lena Veit was supported as a Howard Hughes Medical Institute Fellow of the Life Sciences Research Foundation and by a postdoctoral fellowship from Leopoldina German National Academy of Sciences. Christian J Monroy Hernandez was supported by an HHMI EXROP summer fellowship.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Visualization</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were performed in accordance with protocols (#AN170723- 02) approved by the University of California, San Francisco Institutional Animal Care Use Committee (IACUC).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Matlab code to generate <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-61610-code1-v1.m.zip"/></supplementary-material><supplementary-material id="scode2"><label>Source code 2.</label><caption><title>Matlab code to generate <xref ref-type="fig" rid="fig2">Figures 2C–F</xref> and <xref ref-type="fig" rid="fig3">3E–H</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-61610-code2-v1.m.zip"/></supplementary-material><supplementary-material id="scode3"><label>Source code 3.</label><caption><title>Matlab code to generate <xref ref-type="fig" rid="fig2">Figure 2L</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-61610-code3-v1.m.zip"/></supplementary-material><supplementary-material id="scode4"><label>Source code 4.</label><caption><title>Matlab code to generate <xref ref-type="fig" rid="fig4">Figure 4G</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-61610-code4-v1.m.zip"/></supplementary-material><supplementary-material id="scode5"><label>Source code 5.</label><caption><title>Matlab code to generate <xref ref-type="fig" rid="fig5">Figure 5B–D</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-61610-code5-v1.m.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-61610-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Raw data are included in the manuscript and supporting files. Source data have been provided for all summary analyses, along with code to reproduce the figures.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrahamse</surname> <given-names>EL</given-names></name><name><surname>Ruitenberg</surname> <given-names>MF</given-names></name><name><surname>de Kleine</surname> <given-names>E</given-names></name><name><surname>Verwey</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Control of automated behavior: insights from the discrete sequence production task</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>82</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00082</pub-id><pub-id pub-id-type="pmid">23515430</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackermann</surname> <given-names>H</given-names></name><name><surname>Hage</surname> <given-names>SR</given-names></name><name><surname>Ziegler</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Brain mechanisms of acoustic communication in humans and nonhuman primates: an evolutionary perspective</article-title><source>Behavioral and Brain Sciences</source><volume>37</volume><fpage>529</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1017/S0140525X13003099</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Alcami</surname> <given-names>P</given-names></name><name><surname>Ma</surname> <given-names>S</given-names></name><name><surname>Gahr</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Telemetry reveals rapid duel-driven song plasticity in a naturalistic social environment</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/803411</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aldridge</surname> <given-names>JW</given-names></name><name><surname>Berridge</surname> <given-names>KC</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Coding of Behavioral Sequences in the Basal Ganglia</chapter-title><person-group person-group-type="editor"><name><surname>Nicholson</surname> <given-names>L. F. B</given-names></name><name><surname>Faull</surname> <given-names>R. L. M</given-names></name></person-group><source>The Basal Ganglia VII</source><publisher-loc>Boston</publisher-loc><publisher-name>Springer</publisher-name><fpage>53</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1007/978-1-4615-0715-4</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andalman</surname> <given-names>AS</given-names></name><name><surname>Foerster</surname> <given-names>JN</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Control of vocal and respiratory patterns in birdsong: dissection of forebrain and brainstem mechanisms using temperature</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e25461</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0025461</pub-id><pub-id pub-id-type="pmid">21980466</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashe</surname> <given-names>J</given-names></name><name><surname>Lungu</surname> <given-names>OV</given-names></name><name><surname>Basford</surname> <given-names>AT</given-names></name><name><surname>Lu</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical control of motor sequences</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>213</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.03.008</pub-id><pub-id pub-id-type="pmid">16563734</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashmore</surname> <given-names>RC</given-names></name><name><surname>Wild</surname> <given-names>JM</given-names></name><name><surname>Schmidt</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Brainstem and forebrain contributions to the generation of learned motor behaviors for song</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>8543</fpage><lpage>8554</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1668-05.2005</pub-id><pub-id pub-id-type="pmid">16162936</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basista</surname> <given-names>MJ</given-names></name><name><surname>Elliott</surname> <given-names>KC</given-names></name><name><surname>Wu</surname> <given-names>W</given-names></name><name><surname>Hyson</surname> <given-names>RL</given-names></name><name><surname>Bertram</surname> <given-names>R</given-names></name><name><surname>Johnson</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Independent premotor encoding of the sequence and structure of birdsong in avian cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16821</fpage><lpage>16834</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1940-14.2014</pub-id><pub-id pub-id-type="pmid">25505334</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belyk</surname> <given-names>M</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The origins of the vocal brain in humans</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>77</volume><fpage>177</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.03.014</pub-id><pub-id pub-id-type="pmid">28351755</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benichov</surname> <given-names>JI</given-names></name><name><surname>Vallentin</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inhibition within a premotor circuit controls the timing of vocal turn-taking in zebra finches</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-13938-0</pub-id><pub-id pub-id-type="pmid">31924758</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berwick</surname> <given-names>RC</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name><name><surname>Beckers</surname> <given-names>GJ</given-names></name><name><surname>Bolhuis</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Songs to syntax: the linguistics of birdsong</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>113</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.002</pub-id><pub-id pub-id-type="pmid">21296608</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialystok</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The bilingual adaptation: how minds accommodate experience</article-title><source>Psychological Bulletin</source><volume>143</volume><fpage>233</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1037/bul0000099</pub-id><pub-id pub-id-type="pmid">28230411</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bischof</surname> <given-names>HJ</given-names></name><name><surname>Engelage</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Flash evoked responses in a song control nucleus of the zebra finch (Taeniopygia guttata castanotis)</article-title><source>Brain Research</source><volume>326</volume><fpage>370</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(85)90048-4</pub-id><pub-id pub-id-type="pmid">3971162</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanco-Elorrieta</surname> <given-names>E</given-names></name><name><surname>Pylkkänen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bilingual language control in perception versus action: meg reveals comprehension control mechanisms in anterior cingulate cortex and Domain-General control of production in dorsolateral prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>290</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2597-15.2016</pub-id><pub-id pub-id-type="pmid">26758823</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>MS</given-names></name><name><surname>Doupe</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>What songbirds teach us about learning</article-title><source>Nature</source><volume>417</volume><fpage>351</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1038/417351a</pub-id><pub-id pub-id-type="pmid">12015616</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brecht</surname> <given-names>KF</given-names></name><name><surname>Hage</surname> <given-names>SR</given-names></name><name><surname>Gavrilov</surname> <given-names>N</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Volitional control of vocalizations in corvid songbirds</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000375</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000375</pub-id><pub-id pub-id-type="pmid">31454343</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brumm</surname> <given-names>H</given-names></name><name><surname>Zollinger</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>Avian Vocal Production in Noise</chapter-title><person-group person-group-type="editor"><name><surname>Brumm</surname> <given-names>H</given-names></name></person-group><source>Animal Communication and Noise</source><publisher-loc>Heidelberg, Berlin</publisher-loc><publisher-name>Springer</publisher-name><fpage>187</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-41494-7_7</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Matheson</surname> <given-names>LE</given-names></name><name><surname>Sakata</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mechanisms underlying the social enhancement of vocal learning in songbirds</article-title><source>PNAS</source><volume>113</volume><fpage>6641</fpage><lpage>6646</lpage><pub-id pub-id-type="doi">10.1073/pnas.1522306113</pub-id><pub-id pub-id-type="pmid">27247385</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>Y</given-names></name><name><surname>Shen</surname> <given-names>J</given-names></name><name><surname>Semu</surname> <given-names>D</given-names></name><name><surname>Leman</surname> <given-names>DP</given-names></name><name><surname>Liberti</surname> <given-names>WA</given-names></name><name><surname>Perkins</surname> <given-names>LN</given-names></name><name><surname>Liberti</surname> <given-names>DC</given-names></name><name><surname>Kotton</surname> <given-names>DN</given-names></name><name><surname>Gardner</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hidden neural states underlie canary song syntax</article-title><source>Nature</source><volume>582</volume><fpage>539</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2397-3</pub-id><pub-id pub-id-type="pmid">32555461</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname> <given-names>HA</given-names></name><name><surname>Welch</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Multiple concurrent visual-motor mappings: implications for models of adaptation</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>20</volume><fpage>987</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.20.5.987</pub-id><pub-id pub-id-type="pmid">7964533</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cynx</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Experimental determination of a unit of song production in the zebra finch (Taeniopygia guttata)</article-title><source>Journal of Comparative Psychology</source><volume>104</volume><fpage>3</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1037/0735-7036.104.1.3</pub-id><pub-id pub-id-type="pmid">2354628</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Baene</surname> <given-names>W</given-names></name><name><surname>Duyck</surname> <given-names>W</given-names></name><name><surname>Brass</surname> <given-names>M</given-names></name><name><surname>Carreiras</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Brain circuit for cognitive control is shared by task and language switching</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>1752</fpage><lpage>1765</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00817</pub-id><pub-id pub-id-type="pmid">25901448</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doupe</surname> <given-names>AJ</given-names></name><name><surname>Kuhl</surname> <given-names>PK</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Birdsong and human speech: common themes and mechanisms</article-title><source>Annual Review of Neuroscience</source><volume>22</volume><fpage>567</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.22.1.567</pub-id><pub-id pub-id-type="pmid">10202549</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gadagkar</surname> <given-names>V</given-names></name><name><surname>Puzerey</surname> <given-names>PA</given-names></name><name><surname>Goldberg</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dopamine neurons change their tuning according to courtship context in singing birds</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/822817</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname> <given-names>DW</given-names></name><name><surname>Abutalebi</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Language control in bilinguals: the adaptive control hypothesis</article-title><source>Journal of Cognitive Psychology</source><volume>25</volume><fpage>515</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1080/20445911.2013.796377</pub-id><pub-id pub-id-type="pmid">25077013</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güntürkün</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The avian 'prefrontal cortex' and cognition</article-title><source>Current Opinion in Neurobiology</source><volume>15</volume><fpage>686</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2005.10.003</pub-id><pub-id pub-id-type="pmid">16263260</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hage</surname> <given-names>SR</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Single neurons in monkey prefrontal cortex encode volitional initiation of vocalizations</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>2409</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms3409</pub-id><pub-id pub-id-type="pmid">24008252</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hage</surname> <given-names>SR</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dual neural network model for the evolution of speech and language</article-title><source>Trends in Neurosciences</source><volume>39</volume><fpage>813</fpage><lpage>829</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2016.10.006</pub-id><pub-id pub-id-type="pmid">27884462</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamaguchi</surname> <given-names>K</given-names></name><name><surname>Tanaka</surname> <given-names>M</given-names></name><name><surname>Mooney</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A distributed recurrent network contributes to temporally precise vocalizations</article-title><source>Neuron</source><volume>91</volume><fpage>680</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.019</pub-id><pub-id pub-id-type="pmid">27397518</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname> <given-names>MD</given-names></name><name><surname>Chomsky</surname> <given-names>N</given-names></name><name><surname>Fitch</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The faculty of language: what is it, who has it, and how did it evolve?</article-title><source>Science</source><volume>298</volume><fpage>1569</fpage><lpage>1579</lpage><pub-id pub-id-type="doi">10.1126/science.298.5598.1569</pub-id><pub-id pub-id-type="pmid">12446899</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedley</surname> <given-names>RW</given-names></name><name><surname>Denton</surname> <given-names>KK</given-names></name><name><surname>Weiss</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accounting for syntax in analyses of countersinging reveals hidden vocal dynamics in a songbird with a large repertoire</article-title><source>Animal Behaviour</source><volume>131</volume><fpage>23</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2017.06.021</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinig</surname> <given-names>A</given-names></name><name><surname>Pant</surname> <given-names>S</given-names></name><name><surname>Dunning</surname> <given-names>J</given-names></name><name><surname>Bass</surname> <given-names>A</given-names></name><name><surname>Coburn</surname> <given-names>Z</given-names></name><name><surname>Prather</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Male mate preferences in mutual mate choice: finches modulate their songs across and within male-female interactions</article-title><source>Animal Behaviour</source><volume>97</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2014.08.016</pub-id><pub-id pub-id-type="pmid">25242817</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hikosaka</surname> <given-names>O</given-names></name><name><surname>Nakahara</surname> <given-names>H</given-names></name><name><surname>Rand</surname> <given-names>MK</given-names></name><name><surname>Sakai</surname> <given-names>K</given-names></name><name><surname>Lu</surname> <given-names>X</given-names></name><name><surname>Nakamura</surname> <given-names>K</given-names></name><name><surname>Miyachi</surname> <given-names>S</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Parallel neural networks for learning sequential procedures</article-title><source>Trends in Neurosciences</source><volume>22</volume><fpage>464</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(99)01439-3</pub-id><pub-id pub-id-type="pmid">10481194</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosino</surname> <given-names>T</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Lesion of a higher-order song nucleus disrupts phrase level complexity in bengalese finches</article-title><source>NeuroReport</source><volume>11</volume><fpage>2091</fpage><lpage>2095</lpage><pub-id pub-id-type="doi">10.1097/00001756-200007140-00007</pub-id><pub-id pub-id-type="pmid">10923650</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houde</surname> <given-names>JF</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Sensorimotor adaptation of speech I: compensation and adaptation</article-title><source>Journal of Speech, Language, and Hearing Research: JSLHR</source><volume>45</volume><fpage>295</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2002/023)</pub-id><pub-id pub-id-type="pmid">12003512</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>IS</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Franklin</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of contextual cues on the encoding of motor memories</article-title><source>Journal of Neurophysiology</source><volume>109</volume><fpage>2632</fpage><lpage>2644</lpage><pub-id pub-id-type="doi">10.1152/jn.00773.2012</pub-id><pub-id pub-id-type="pmid">23446696</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imamizu</surname> <given-names>H</given-names></name><name><surname>Sugimoto</surname> <given-names>N</given-names></name><name><surname>Osu</surname> <given-names>R</given-names></name><name><surname>Tsutsui</surname> <given-names>K</given-names></name><name><surname>Sugiyama</surname> <given-names>K</given-names></name><name><surname>Wada</surname> <given-names>Y</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Explicit contextual information selectively contributes to predictive switching of internal models</article-title><source>Experimental Brain Research</source><volume>181</volume><fpage>395</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1007/s00221-007-0940-1</pub-id><pub-id pub-id-type="pmid">17437093</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imamizu</surname> <given-names>H</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Brain mechanisms for predictive control by switching internal models: implications for higher-order cognitive functions</article-title><source>Psychological Research Psychologische Forschung</source><volume>73</volume><fpage>527</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1007/s00426-009-0235-1</pub-id><pub-id pub-id-type="pmid">19347360</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isola</surname> <given-names>GR</given-names></name><name><surname>Vochin</surname> <given-names>A</given-names></name><name><surname>Sakata</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Manipulations of inhibition in cortical circuitry differentially affect spectral and temporal features of bengalese finch song</article-title><source>Journal of Neurophysiology</source><volume>123</volume><fpage>815</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1152/jn.00142.2019</pub-id><pub-id pub-id-type="pmid">31967928</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaffe</surname> <given-names>PI</given-names></name><name><surname>Brainard</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Acetylcholine acts on songbird premotor circuitry to invigorate vocal output</article-title><source>eLife</source><volume>9</volume><elocation-id>e53288</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53288</pub-id><pub-id pub-id-type="pmid">32425158</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>James</surname> <given-names>LS</given-names></name><name><surname>Dai</surname> <given-names>JB</given-names></name><name><surname>Sakata</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ability to modulate birdsong across social contexts develops without imitative social learning</article-title><source>Biology Letters</source><volume>14</volume><elocation-id>20170777</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2017.0777</pub-id><pub-id pub-id-type="pmid">29540565</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarvis</surname> <given-names>ED</given-names></name><name><surname>Yu</surname> <given-names>J</given-names></name><name><surname>Rivas</surname> <given-names>MV</given-names></name><name><surname>Horita</surname> <given-names>H</given-names></name><name><surname>Feenders</surname> <given-names>G</given-names></name><name><surname>Whitney</surname> <given-names>O</given-names></name><name><surname>Jarvis</surname> <given-names>SC</given-names></name><name><surname>Jarvis</surname> <given-names>ER</given-names></name><name><surname>Kubikova</surname> <given-names>L</given-names></name><name><surname>Puck</surname> <given-names>AE</given-names></name><name><surname>Siang-Bakshi</surname> <given-names>C</given-names></name><name><surname>Martin</surname> <given-names>S</given-names></name><name><surname>McElroy</surname> <given-names>M</given-names></name><name><surname>Hara</surname> <given-names>E</given-names></name><name><surname>Howard</surname> <given-names>J</given-names></name><name><surname>Pfenning</surname> <given-names>A</given-names></name><name><surname>Mouritsen</surname> <given-names>H</given-names></name><name><surname>Chen</surname> <given-names>CC</given-names></name><name><surname>Wada</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Global view of the functional molecular organization of the avian cerebrum: mirror images and functional columns</article-title><source>Journal of Comparative Neurology</source><volume>521</volume><fpage>3614</fpage><lpage>3665</lpage><pub-id pub-id-type="doi">10.1002/cne.23404</pub-id><pub-id pub-id-type="pmid">23818122</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>DZ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating variable birdsong syllable sequences with branching chain networks in avian premotor nucleus HVC</article-title><source>Physical Review E</source><volume>80</volume><elocation-id>051902</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.80.051902</pub-id><pub-id pub-id-type="pmid">20365001</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>X</given-names></name><name><surname>Costa</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shaping action sequences in basal ganglia circuits</article-title><source>Current Opinion in Neurobiology</source><volume>33</volume><fpage>188</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.06.011</pub-id><pub-id pub-id-type="pmid">26189204</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>DZ</given-names></name><name><surname>Kozhevnikov</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A compact statistical model of the song syntax in bengalese finch</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1001108</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001108</pub-id><pub-id pub-id-type="pmid">21445230</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katahira</surname> <given-names>K</given-names></name><name><surname>Suzuki</surname> <given-names>K</given-names></name><name><surname>Kagawa</surname> <given-names>H</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A simple explanation for the evolution of complex song syntax in bengalese finches</article-title><source>Biology Letters</source><volume>9</volume><elocation-id>20130842</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2013.0842</pub-id><pub-id pub-id-type="pmid">24284561</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keough</surname> <given-names>D</given-names></name><name><surname>Jones</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Contextual cuing contributes to the independent modification of multiple internal models for vocal control</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2448</fpage><lpage>2456</lpage><pub-id pub-id-type="doi">10.1152/jn.00291.2010</pub-id><pub-id pub-id-type="pmid">21346208</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>SL</given-names></name><name><surname>McGregor</surname> <given-names>PK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vocal matching: the what, the why and the how</article-title><source>Biology Letters</source><volume>12</volume><elocation-id>20160666</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2016.0666</pub-id><pub-id pub-id-type="pmid">28120803</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kojima</surname> <given-names>S</given-names></name><name><surname>Doupe</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Social performance reveals unexpected vocal competency in young songbirds</article-title><source>PNAS</source><volume>108</volume><fpage>1687</fpage><lpage>1692</lpage><pub-id pub-id-type="doi">10.1073/pnas.1010502108</pub-id><pub-id pub-id-type="pmid">21220335</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>JY</given-names></name><name><surname>Schweighofer</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dual adaptation supports a parallel architecture of motor memory</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>10396</fpage><lpage>10404</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1294-09.2009</pub-id><pub-id pub-id-type="pmid">19692614</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipkind</surname> <given-names>D</given-names></name><name><surname>Marcus</surname> <given-names>GF</given-names></name><name><surname>Bemis</surname> <given-names>DK</given-names></name><name><surname>Sasahara</surname> <given-names>K</given-names></name><name><surname>Jacoby</surname> <given-names>N</given-names></name><name><surname>Takahasi</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>K</given-names></name><name><surname>Feher</surname> <given-names>O</given-names></name><name><surname>Ravbar</surname> <given-names>P</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name><name><surname>Tchernichovski</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stepwise acquisition of vocal combinatorial capacity in songbirds and human infants</article-title><source>Nature</source><volume>498</volume><fpage>104</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/nature12173</pub-id><pub-id pub-id-type="pmid">23719373</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDougle</surname> <given-names>SD</given-names></name><name><surname>Ivry</surname> <given-names>RB</given-names></name><name><surname>Taylor</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Taking aim at the cognitive side of learning in sensorimotor adaptation tasks</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>535</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.05.002</pub-id><pub-id pub-id-type="pmid">27261056</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nicholson</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>NickleDave/hybrid-vocal-classifier</data-title><source>Github</source><version designator="0.3.0">0.3.0</version><ext-link ext-link-type="uri" xlink:href="https://github.com/NickleDave/hybrid-vocal-classifier.git">https://github.com/NickleDave/hybrid-vocal-classifier.git</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname> <given-names>A</given-names></name><name><surname>Mooney</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neurobiology of innate, volitional and learned vocalizations in mammals and birds</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>375</volume><elocation-id>20190054</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0054</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The bengalese finch: a window on the behavioral neurobiology of birdsong syntax</article-title><source>Annals of the New York Academy of Sciences</source><volume>1016</volume><fpage>724</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1196/annals.1298.026</pub-id><pub-id pub-id-type="pmid">15313802</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichmuth</surname> <given-names>C</given-names></name><name><surname>Casey</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Vocal learning in seals, sea lions, and walruses</article-title><source>Current Opinion in Neurobiology</source><volume>28</volume><fpage>66</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.06.011</pub-id><pub-id pub-id-type="pmid">25042930</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rochet-Capellan</surname> <given-names>A</given-names></name><name><surname>Ostry</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Simultaneous acquisition of multiple auditory-motor transformations in speech</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2657</fpage><lpage>2662</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6020-10.2011</pub-id><pub-id pub-id-type="pmid">21325534</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakata</surname> <given-names>JT</given-names></name><name><surname>Hampton</surname> <given-names>CM</given-names></name><name><surname>Brainard</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Social modulation of sequence and syllable variability in adult birdsong</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>1700</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1152/jn.01296.2007</pub-id><pub-id pub-id-type="pmid">18216221</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Searcy</surname> <given-names>WA</given-names></name><name><surname>Beecher</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Song as an aggressive signal in songbirds</article-title><source>Animal Behaviour</source><volume>78</volume><fpage>1281</fpage><lpage>1292</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2009.08.011</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seki</surname> <given-names>Y</given-names></name><name><surname>Suzuki</surname> <given-names>K</given-names></name><name><surname>Takahasi</surname> <given-names>M</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Song motor control organizes acoustic patterns on two levels in bengalese finches (Lonchura striata var Domestica)</article-title><source>Journal of Comparative Physiology A</source><volume>194</volume><fpage>533</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1007/s00359-008-0328-0</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Horwitz</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Laryngeal motor cortex and control of speech in humans</article-title><source>The Neuroscientist</source><volume>17</volume><fpage>197</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1177/1073858410386727</pub-id><pub-id pub-id-type="pmid">21362688</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sossinka</surname> <given-names>R</given-names></name><name><surname>Böhner</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Song types in the zebra finch Poephila guttata castanotis1</article-title><source>Zeitschrift Für Tierpsychologie</source><volume>53</volume><fpage>123</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0310.1980.tb01044.x</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suge</surname> <given-names>R</given-names></name><name><surname>Okanoya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptual chunking in the self-produced songs of bengalese finches (Lonchura striata var Domestica)</article-title><source>Animal Cognition</source><volume>13</volume><fpage>515</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1007/s10071-009-0302-4</pub-id><pub-id pub-id-type="pmid">20039089</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname> <given-names>TN</given-names></name><name><surname>Zuberbühler</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Animal syntax</article-title><source>Current Biology</source><volume>29</volume><fpage>R669</fpage><lpage>R671</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.05.045</pub-id><pub-id pub-id-type="pmid">31336078</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanji</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Sequential organization of multiple movements: involvement of cortical motor Areas</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>631</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.631</pub-id><pub-id pub-id-type="pmid">11520914</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trillo</surname> <given-names>PA</given-names></name><name><surname>Vehrencamp</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Song types and their structural features are associated with specific contexts in the banded wren</article-title><source>Animal Behaviour</source><volume>70</volume><fpage>921</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2005.02.004</pub-id><pub-id pub-id-type="pmid">17173097</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troyer</surname> <given-names>TW</given-names></name><name><surname>Brainard</surname> <given-names>MS</given-names></name><name><surname>Bouchard</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Timing during transitions in bengalese finch song: implications for motor sequencing</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>1556</fpage><lpage>1566</lpage><pub-id pub-id-type="doi">10.1152/jn.00296.2017</pub-id><pub-id pub-id-type="pmid">28637816</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tumer</surname> <given-names>EC</given-names></name><name><surname>Brainard</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Performance variability enables adaptive plasticity of 'crystallized' adult birdsong</article-title><source>Nature</source><volume>450</volume><fpage>1240</fpage><lpage>1244</lpage><pub-id pub-id-type="doi">10.1038/nature06390</pub-id><pub-id pub-id-type="pmid">18097411</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullrich</surname> <given-names>R</given-names></name><name><surname>Norton</surname> <given-names>P</given-names></name><name><surname>Scharff</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Waltzing Taeniopygia: integration of courtship song and dance in the domesticated australian zebra finch</article-title><source>Animal Behaviour</source><volume>112</volume><fpage>285</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2015.11.012</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veit</surname> <given-names>L</given-names></name><name><surname>Pidpruzhnykova</surname> <given-names>G</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Associative learning rapidly establishes neuronal representations of upcoming behavioral choices in crows</article-title><source>PNAS</source><volume>112</volume><fpage>15208</fpage><lpage>15213</lpage><pub-id pub-id-type="doi">10.1073/pnas.1509760112</pub-id><pub-id pub-id-type="pmid">26598669</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veit</surname> <given-names>L</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Abstract rule neurons in the endbrain support intelligent behaviour in corvid songbirds</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>2878</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms3878</pub-id><pub-id pub-id-type="pmid">24285080</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vignal</surname> <given-names>C</given-names></name><name><surname>Mathevon</surname> <given-names>N</given-names></name><name><surname>Mottin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Audience drives male songbird response to partner's voice</article-title><source>Nature</source><volume>430</volume><fpage>448</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1038/nature02645</pub-id><pub-id pub-id-type="pmid">15269767</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyssotski</surname> <given-names>AL</given-names></name><name><surname>Stepien</surname> <given-names>AE</given-names></name><name><surname>Keller</surname> <given-names>GB</given-names></name><name><surname>Hahnloser</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A neural code that is isometric to vocal output and correlates with its sensory consequences</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e2000317</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000317</pub-id><pub-id pub-id-type="pmid">27723764</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>TL</given-names></name><name><surname>Charlesworth</surname> <given-names>JD</given-names></name><name><surname>Tumer</surname> <given-names>EC</given-names></name><name><surname>Brainard</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Variable sequencing is actively maintained in a well learned motor skill</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>15414</fpage><lpage>15425</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1254-12.2012</pub-id><pub-id pub-id-type="pmid">23115179</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheeler</surname> <given-names>BC</given-names></name><name><surname>Fischer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functionally referential signals: a promising paradigm whose time has passed</article-title><source>Evolutionary Anthropology: Issues, News, and Reviews</source><volume>21</volume><fpage>195</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1002/evan.21319</pub-id><pub-id pub-id-type="pmid">23074065</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wild</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Visual and somatosensory inputs to the avian song system via nucleus uvaeformis (Uva) and a comparison with the projections of a similar thalamic nucleus in a nonsongbird, Columba livia</article-title><source>The Journal of Comparative Neurology</source><volume>349</volume><fpage>512</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1002/cne.903490403</pub-id><pub-id pub-id-type="pmid">7860787</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Flanagan</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Principles of sensorimotor learning</article-title><source>Nature Reviews Neuroscience</source><volume>12</volume><fpage>739</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1038/nrn3112</pub-id><pub-id pub-id-type="pmid">22033537</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>YS</given-names></name><name><surname>Wittenbach</surname> <given-names>JD</given-names></name><name><surname>Jin</surname> <given-names>DZ</given-names></name><name><surname>Kozhevnikov</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temperature manipulation in songbird brain implicates the premotor nucleus HVC in birdsong syntax</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>2600</fpage><lpage>2611</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1827-16.2017</pub-id><pub-id pub-id-type="pmid">28159910</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61610.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewing Editor</role><aff><institution>Cornell University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewer</role><aff><institution>Cornell University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Scharff</surname><given-names>Constance</given-names> </name><role>Reviewer</role><aff><institution>Freie Universitaet Berlin</institution><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Bengalese finches sing syntactically complex songs with flexible transitions between specific syllables. Here, the authors show that birds can modify syllable transitions depending on an arbitrary light cue. This surprising result shows that learning in the 'song system' – a neural circuit in songbirds known to drive vocal output – can be controlled by yet-to-be defined visual inputs, setting the stage for new directions in the songbird field that move beyond sequence production and into a more cognitive realm.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Songbirds can learn flexible contextual control over syllable sequencing&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Jesse H Goldberg as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Constance Scharff (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>Veit et al. test if arbitrary visual cues can influence syllable sequence 'choices' in adult Bengalese finches. Using light- and sequence- contingent distorted auditory feedback, they find that birds robustly learn to produce context dependent syllables. The learning slowly proceeds over weeks, but once trained birds can rapidly transition between two sequence probabilities. This is a really interesting finding because it shows that the HVC chains that drive syllable phonology and sequencing can 'learn' to be gated by yet-to-be defined visual inputs. The paper is only a behavioral study without neural correlates or a candidate neural architecture that could even solve the problem – but the reviewers did not see this as a major problem for the paper. By analogy, the Tumer et al., 2007 paper from the Brainard lab was also only a behavioral study yet it has launched dozens of follow up studies and a new branch of songbird neuroscience. This paper has the potential to do the same. Follow-up studies that figure out exactly how the visual system interfaces with the songs system to dictate syllable selection will be a really interesting direction for the field – moving birdsong beyond simple sequence production and into a more cognitive realm. Thus this paper is likely to be high impact, highly cited, and important for the field.</p><p>Essential revisions:</p><p>1. Please address the question, raised by Reviewer 2, about whether or not the production of the syllable initiating the target sequence was affected by the light context. i.e. if the training was abc vs abd, did the probability of producing a or ab change depending on context? Reviwere 2 wondered, if this is indeed the case, how this would affect the interpretations of the paper, especially &quot;postulated parallels to language?&quot;</p><p>2. Please also address the comments below, of which there are many. A point-by-point response will not be necessary, but it should be clear that all reviewers wanted some of the claims of novelty and connection to human cognition to be tempered, i.e. keep the conclusions closer to the data. This will mostly involve re-wording in the Introduction and Discussion. There are also several sources of confusion where the methods and analyses should be presented more clearly. Please see more details below.</p><p><italic>Reviewer 1:</italic></p><p>In Figure 4: n=3 birds on Figure 4 is a bit thin. New data is not absolutely necessary because the effect is robust and consistent across birds, but the authors may want to replicate this in 1-2 more birds to really nail the finding that specific light cues can drive shifts bidirectionally.</p><p>The discussion would be improved if it included examples of natural context-dependent changes in song syllable selection – for example song matching by buntings, great tits, and sparrows. In these cases, birds sequence and select syllables in a context dependent way, even depending on where they are in a territory. Thus there may be some precedent and evolutionarily context for the core finding here, i.e. that a 'place' representation can access the song system to influence syllable sequence and song selection. This consideration would fit in the &quot;Evolution of control over vocal sequencing&quot; section.</p><p><italic>Reviewer 2:</italic></p><p>Line 28: 'parallels aspects of human cognitive control over speech'. I find that an overstatement, unless I misunderstand the data. The authors condition birds to avoid a particular sequence by punishing ('aversively reinforcing') it with white noise and link this to a visual stimulus. How does that parallel human cognitive control over speech? Can the authors please provide more explanation?</p><p>Line 35. Please provide a reference with evidence for the part in italics (mine) in the following statement, or rephrase more evidence-based: 'This flexibility ingrained in human language stands in striking contrast to the largely innate and stereotypic vocalization patterns of most animal species, including our closest relatives, the non-human primates.' Most of the roughly 8.7 million animal species? How many have been analyzed? Of those, how many vocalization patterns are 'largely innate'? And stereotypic? At what level stereotypic?</p><p>Line 57: '…affective behavior, elicited instinctually by contact with potential mates, rivals, or performed..' What do the authors mean by 'affective' and 'instinctually'? Human speech also has affective components (prosody for instance) and we instinctually change aspects of our speech (and language) when we talk to children, partners, strangers. This is an unsophisticated dichotomy 'humans/birdsong', please consider rephrasing (rethinking).</p><p>Line 59: 'There are differences between songs produced in distinct social contexts…' Refs to this sentence should include Heinig et al. 2014 Male mate preferences in mutual mate choice: finches modulate their songs across and within male-female interactions. Anim Behav. 97:1-12.</p><p>Line 62. ‚However, these social influences likely reflect a general modulation of song structure related to the animal's affective state (Berwick, Okanoya, Beckers, and Bolhuis, 2011). What is the concrete evidence for the 'likely' in this sentence?</p><p>Line 64 'and do not reveal whether song can be modified more flexibly by different cognitive factors.' But the fact that Bengalese finches sing different song sequences to different females (Heinig et al. paper, above) raise the possibility that 'cognitive factors' could play a role, since it's all 'affective' courtship song but different depending on which female is being sung to.</p><p>Line 78 'immediately, flexibly, and adaptively adjust their sequencing of vocal elements in response to learned contextual cues, in a manner that parallels key aspects of human cognitive control over speech' Same comment as to line 28. I think the authors are not doing themselves a favor in phrasing this claim so broadly and non-specifically.</p><p>line 110: 'alternating blocks' first mentioned here. Please include a section in the methods about blocks. I found it hard to extract the information from various points in the text how long blocks were, how many blocks per day on average (from the figures it seems that the entire day was used for data acquisition?) and what 'short probe blocks' (line 227) meant in terms of timing. Also, why were there many more block switches (Figure 1F) during baseline than during training (Figure 1G)?</p><p>Line 125: Figure 1A: it would help to point out the individual 'songs' in that figure, since song is defined differently in different species. In zebra finches a song as defined by the authors would be called 'a motif': (line 69) a 'song consist of ca 5-12 acoustically distinct elements'. Where do songs start and end? How is that determined? This relates also to my question above, whether sequences are modified in probability of occurrence or songs (or song types).</p><p>Line 237: 'Figure 3A,B shows song bouts for one example bird'. Since song bouts are defined by authors as 'separated by at least 2 sec' I would like to know whether the shown spectrogram is the entire bout and the silence before and after are just not shown or whether A and B show part of a bout. If so, can you show the entire bout, including the time when the light changes?</p><p>Line 332: 'The ability to alter syllable sequencing in a flexible fashion also contrasts with prior studies that have demonstrated modulation of vocalizations in more naturalistic settings.(…). In contrast, here we show that birds can learn to locally modulate specific features of their songs (i.e. individually targeted syllable transitions) in response to arbitrarily assigned contextual stimuli that have no prior ethological relevance.' Could the authors please comment on the following conundrum: If flexible use of song sequences under natural conditions were 'hard-wired/innate/reflexive/affective' as the authors suggest, how would the ability to pair an arbitrary contextual cue with a particular song sequence have evolved in Bengalese finches? Why would neural connections exist that allow this pairing of visual input to motor output? Isn't it more parsimonious to postulate that under natural conditions, visual stimuli do lead to different vocal motor responses because in addition to the known 'affective' mediators (hormones, dopamine etc) there is some 'top down', 'cognitive' control? (Reviewer 1 agrees with this point).</p><p>Line 348: ' Evolution of control over vocal sequencing' section is in line with my above comment, e.g. suggests what some animals might use the ability to use contextual visual information for adaptive motor output but again negates that Bengalese finches actually use it in their current behavior, instead the authors call it 'latent capacity'. I do not follow their logic.</p><p>Line 367: Neural implementation. Does the two process model relate to human speech and language? Please explain.</p><p>Line 428: add 'male'. The manuscript does not mention anywhere whether males and females in Bengalese finches sing….Or add it to line 23 in the Abstract.</p><p>Line 429: 'age range 128-320 days' was there any age-related difference in learning? Looking at the figures some birds seemed to have performed quite a bit better than others. See also line 456 below.</p><p>Line 455: 'within an interval of one to several hours' please provide more information whether this was randomly chosen or based on the birds performance. If random, what was the rationale for this large difference in block duration?</p><p>Line 456: 'after several days of training (average 33)' Please also provide the range and whether shorter training was related to age of the birds.</p><p>Line 461: 'three birds.…never progressed to full probe sequence either because they did not exhibit single-context learning or because of technical issues with consistent targeting of branch points'. Did two birds not learn and one had technical issue or other way round? How common is it in WN-escape experimental set-ups that birds do not learn? And what does 'single-context learning' mean? That they did not learn to associate yellow light with one target? This would imply that context 1 was learned first and then context 2, but in line 454 it sounds like both colors were paired with their particular target after one to several hours. Please explain.</p><p>Line 466: Please specify in the methods how many days the entire experiment lasted. How variable was the song output during that time and between individual birds? Did song output decline over time? Can the authors provide an estimate how many songs or bouts on average (and range) the birds sang?</p><p><italic>Reviewer 3:</italic></p><p>Although the present study describes the syllable sequence switching abilities of Bengalese finches within the framework of an elegantly designed behavioral paradigm, the links to the potential neural mechanisms are poorly presented or even obsolete since the authors do not provide any evidence about underlying brain dynamics. I recommend to rather discuss the results in a behavioral framework unless the authors add results from neural recordings or brain manipulations.</p><p>Line 34: Citation for reordering of finite elements to achieve infinite meaning, see Hauser, Chomsky and Fitch, 2002.</p><p>Line 47 f: Lipkind et al. 2017 showed that zebra finches can learn to re-order syllables during song learning. This paper is highly relevant and should be discussed.</p><p>Line 52: Reference to Doupe and Kuhl, 1999 should be moved to line 46?</p><p>Line 88 f: The authors decide to present most data in percentages. It would be useful to provide the actual number to assess the quality of the data.</p><p>Line 89: How reliable was the software in targeting syllables?</p><p>Line 284: The authors refer to white light as a neutral state. What is the color perception for Bengalese finches? Is white perceived rather as yellow or green? A novel light condition that the birds had not been exposed before would probably be better.</p><p>Line 306 f: Light cues are not arbitrary as the birds are initially trained to connect white noise with light of a certain color.</p><p>Line 444: What was the reason to specifically choose green and yellow as colors for this experiment?</p><p>Line 445: How much does visual perception of Bengalese finches differ between 520 and 600nm?</p><p>Line 453-455: What is the maximum duration of several hours? This would also help to understand the difference in amounts of switches in Figure 1 F and G.</p><p>Line 456: How did the white noise training look? Can learning curves be added?</p><p>Figure 1 B: It would be helpful to plot both probability curves (ab-d and ab-c) and color code them accordingly as a general probability plot (y axis).</p><p>Figure 1 F: Why is the amount of color switches different between baseline and training? When within the training did baseline days occur or was this prior to training?</p><p>Figure 1 G: Why are light phases for green/yellow differently long? The error bars are misleading, as they show the SEM of individual blocks rather than the entire sample.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61610.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. Please address the question, raised by Reviewer 2, about whether or not the production of the syllable initiating the target sequence was affected by the light context. i.e. if the training was abc vs abd, did the probability of producing a or ab change depending on context? Reviewer 2 wondered, if this is indeed the case, how this would affect the interpretations of the paper, especially &quot;postulated parallels to language?&quot;</p></disp-quote><p>We understand this to be a question about how specific to the target sequence were the changes in the overall transition structure of the song. We have added substantial new analysis, and a new figure (new Figure 4), to address the specificity of contextual differences to the branch points preceding the target sequences. These new analyses demonstrate that for the majority of birds, 80% or more of total contextual differences were restricted to the targeted branch points. With respect to the example cited in the question above, this means that the majority of change to sequencing in an experiment targeting ‘abc’ vs. ‘abd’ occurs to transitions at the branchpoint following syllable ‘b’ and that there is little or no contextual difference in the probability of producing an ‘a’ or ‘ab’. These new analyses indicate that the learned contextual changes to syllable sequencing reflect a capacity for modulation of specific sequences within song, rather than the kind of global modulation of structure that occurs (for example) between songs produced in different social contexts. While we have reduced comparisons with speech throughout, we note that this specificity parallels a feature of contextual modulation of sequencing in speech, which similarly reflects a capacity for flexible, local and specific reordering of elements.</p><disp-quote content-type="editor-comment"><p>2. Please also address the comments below, of which there are many. A point-by-point response will not be necessary, but it should be clear that all reviewers wanted some of the claims of novelty and connection to human cognition to be tempered, i.e. keep the conclusions closer to the data. This will mostly involve re-wording in the Introduction and Discussion. There are also several sources of confusion where the methods and analyses should be presented more clearly. Please see more details below.</p></disp-quote><p>We have attempted to address all comments, especially the points noted immediately above.</p><disp-quote content-type="editor-comment"><p>Reviewer 1:</p><p>In Figure 4: n=3 birds on Figure 4 is a bit thin. New data is not absolutely necessary because the effect is robust and consistent across birds, but the authors may want to replicate this in 1-2 more birds to really nail the finding that specific light cues can drive shifts bidirectionally.</p></disp-quote><p>We have added a sentence noting that the relevant conclusions would benefit from additional experiments beyond those presented in Figure 5 (previously Figure 4).</p><disp-quote content-type="editor-comment"><p>The discussion would be improved if it included examples of natural context-dependent changes in song syllable selection – for example song matching by buntings, great tits, and sparrows. In these cases, birds sequence and select syllables in a context dependent way, even depending on where they are in a territory. Thus there may be some precedent and evolutionarily context for the core finding here, i.e. that a 'place' representation can access the song system to influence syllable sequence and song selection. This consideration would fit in the &quot;Evolution of control over vocal sequencing&quot; section.</p></disp-quote><p>We have substantially expanded the discussion of natural context-dependent changes (l.57ff, l.385ff), including addition of concrete examples, and have adjusted the logic in the Introduction and Discussion (paragraphs on evolution) to explicitly note that these examples of natural context-dependent control suggest that birds might also be able to exert such control in response to more arbitrary, learned contexts:</p><p>L.416ff: “Such capacities for vocal control likely reflect evolutionary advantages of incorporating sensory and contextual information about conspecifics and the environment in generating increasingly sophisticated vocal signaling. […] Perhaps evolutionary pressures to develop nuanced social communication led to the elaboration of cortical (pallial) control over brainstem vocal circuitry (Hage and Nieder, 2016), and thereby established a conduit that facilitated the integration of progressively more abstract cues and internal states in that control.</p><disp-quote content-type="editor-comment"><p>Reviewer 2:</p><p>Line 28: 'parallels aspects of human cognitive control over speech'. I find that an overstatement, unless I misunderstand the data. The authors condition birds to avoid a particular sequence by punishing ('aversively reinforcing') it with white noise and link this to a visual stimulus. How does that parallel human cognitive control over speech? Can the authors please provide more explanation?</p><p>Line 78 'immediately, flexibly, and adaptively adjust their sequencing of vocal elements in response to learned contextual cues, in a manner that parallels key aspects of human cognitive control over speech' Same comment as to line 28. I think the authors are not doing themselves a favor in phrasing this claim so broadly and non-specifically.</p></disp-quote><p>We have tempered, specified, or removed comparisons to contextual control of human speech throughout the text, and have provided additional explanation about similarities we see to speech control in the Discussion. We particularly focus on what we see as a shared capacity for learned, context-dependent control over the sequencing of vocal elements that is immediate, flexible, and adaptive. For example, contextual shifts appear immediately after context switches (Figure 2), they are learned, in the appropriate, arbitrarily chosen, direction in response to cues, which do not elicit such changes without prior training (Figure 1, Figure 5), and they are adaptive, in that they avoid aversive WN. We do not suggest that the context-dependent learning we have demonstrated reflects a capacity for conveying the kind of rich semantic content that is central to human language. Rather, that the underlying ability manifest in speech motor control to immediately and flexibly reorganize sequences of constituent elements (phonemes/syllables/words) to achieve a communicative ‘goal’ has some formal similarities to the simpler contextual control of vocalizations demonstrated here. In particular, we construe both to include a capacity for learned, moment-by-moment, “top-down” influences on the organization and sequencing of vocal elements to achieve contextually appropriate, adaptive outcomes. For human speech, complex cognitive processes and semantic “intent” can inform those top-down influences with an adaptive goal of influencing the listener (“conveying meaning”). For our experiments, bird vocalizations are similarly deployed in a learned and contextually appropriate fashion to achieve an adaptive goal (escaping from white noise). Correspondingly, we suggest that context-dependent modulation of vocal sequencing in the Bengalese finch may provide a particularly tractable behavioral model for examining how different arbitrary learned cues can drive the kind of top-down control of vocal motor output that forms a building block of speech. However, we appreciate the reviewer’s perspective that it is a long way from the capacities demonstrated here to insights about speech motor control, and correspondingly have largely curtailed a discussion of these parallels.</p><disp-quote content-type="editor-comment"><p>Line 35. Please provide a reference with evidence for the part in italics (mine) in the following statement, or rephrase more evidence-based: 'This flexibility ingrained in human language stands in striking contrast to the largely innate and stereotypic vocalization patterns of most animal species, including our closest relatives, the non-human primates.' Most of the roughly 8.7 million animal species? How many have been analyzed? Of those, how many vocalization patterns are 'largely innate'? And stereotypic? At what level stereotypic?</p></disp-quote><p>We acknowledge that the previous statement was too broad, given that most of the 8.7 million species noted by the reviewer have not been characterized in depth. We have rephrased this section (l. 37f) to focus on primates, where there has been considerable prior work:</p><p>“This cognitive control over vocal production is thought to rely on the direct innervation of brainstem and midbrain vocal networks by executive control structures in the frontal cortex, which have become more elaborate over the course of primate evolution (Hage and Nieder, 2016, Simonyan and Horwitz 2011). However, because of the comparatively limited flexibility of vocal production in nonhuman primates (Nieder and Mooney, 2020), the evolutionary and neural circuit mechanisms that have enabled the development of this flexibility remain poorly understood.“</p><disp-quote content-type="editor-comment"><p>Line 57: '…affective behavior, elicited instinctually by contact with potential mates, rivals, or performed..' What do the authors mean by 'affective' and 'instinctually'? Human speech also has affective components (prosody for instance) and we instinctually change aspects of our speech (and language) when we talk to children, partners, strangers. This is an unsophisticated dichotomy 'humans/birdsong', please consider rephrasing (rethinking).</p></disp-quote><p>We did not mean to imply that human speech lacks affective components. Rather, we wanted to emphasize the flexible top-down control of human speech production, which is typically considered a cognitive process involving the reorganization of vocal elements to achieve some communicative intent. In contrast, contextual changes in birdsong have typically been ascribed to affective processes, such as hormonal and neuromodulatory changes related to the production of directed song, and these changes have been shown to be unaffected by learning (“instinctual”). We here test whether cognitive influences on birdsong exist beyond these possibly completely instinctual contextual changes, building, on the previously known examples of contextual differences in birdsong. We have revised the paragraph to better explain examples of naturally occurring contextual changes in birdsong (see also other Reviewer comments), and clarified the logic in Introduction and Discussion.</p><disp-quote content-type="editor-comment"><p>Line 59: 'There are differences between songs produced in distinct social contexts…' Refs to this sentence should include Heinig et al. 2014 Male mate preferences in mutual mate choice: finches modulate their songs across and within male-female interactions. Anim Behav. 97:1-12.</p></disp-quote><p>We have added the reference.</p><disp-quote content-type="editor-comment"><p>Line 62. ‚However, these social influences likely reflect a general modulation of song structure related to the animal's affective state (Berwick, Okanoya, Beckers, and Bolhuis, 2011). What is the concrete evidence for the 'likely' in this sentence?</p></disp-quote><p>Changes in directed song typically reflect a general or global modulation of song structure, such that song overall is faster, louder, higher pitched, and more stereotyped. We have rephrased and included references in the paragraph to clarify.</p><p>l. 57ff: “Contextual variation of song in natural settings, such as territorial counter-singing or female-directed courtship song, indicate that songbirds can rapidly alter aspects of their song, including syllable sequencing and selection of song types (Chen, Matheson, and Sakata, 2016; Heinig et al., 2014; King and McGregor, 2016; Sakata, Hampton, and Brainard, 2008; Searcy and Beecher, 2009; Trillo and Vehrencamp, 2005). […] For example, the presence of potential mates or rivals elicits a global and unlearned modulation of song intensity (James, Dai, and Sakata, 2018a) related to the singer’s level of arousal or aggression (Alcami, Ma, and Gahr, 2021; Heinig et al., 2014; Jaffe and Brainard, 2020).”</p><disp-quote content-type="editor-comment"><p>Line 64 'and do not reveal whether song can be modified more flexibly by different cognitive factors.' But the fact that Bengalese finches sing different song sequences to different females (Heinig et al. paper, above) raise the possibility that 'cognitive factors' could play a role, since it's all 'affective' courtship song but different depending on which female is being sung to.</p></disp-quote><p>To our understanding, the Heinig et al. paper shows that birds sing different <italic>intensity</italic> of directed song to different females, which is compatible with an interpretation that they have different levels of general motivation to sing directed song to different females. This and other papers about directed song, referenced in response to the previous comment, suggest that directed song can elicit global changes (increased speed, amplitude, stereotypy, including sequence stereotypy) that are not learned. In contrast, the changes we show here are learned in response to arbitrary contextual cues, and are specific to the targeted position in the song bout. We have added new analysis (Figure 4) that demonstrates this specificity of the contextual changes in the current experiment, and have also clarified logic in Introduction and Discussion, to note that song changes elicited in natural contexts – including song type selection in other species – raise the possibility that learned, cognitive factors could play a role in modulating vocal output, an idea that we attempt to specifically test in our study.</p><disp-quote content-type="editor-comment"><p>line 110: 'alternating blocks' first mentioned here. Please include a section in the methods about blocks. I found it hard to extract the information from various points in the text how long blocks were, how many blocks per day on average (from the figures it seems that the entire day was used for data acquisition?) and what 'short probe blocks' (line 227) meant in terms of timing. Also, why were there many more block switches (Figure 1F) during baseline than during training (Figure 1G)?</p><p>Line 455: 'within an interval of one to several hours' please provide more information whether this was randomly chosen or based on the birds performance. If random, what was the rationale for this large difference in block duration?</p></disp-quote><p>These details have been added to the “<italic>Training procedure and blocks”</italic> section of the Methods.</p><disp-quote content-type="editor-comment"><p>Line 125: Figure 1A: it would help to point out the individual 'songs' in that figure, since song is defined differently in different species. In zebra finches a song as defined by the authors would be called 'a motif': (line 69) a 'song consist of ca 5-12 acoustically distinct elements'. Where do songs start and end? How is that determined? This relates also to my question above, whether sequences are modified in probability of occurrence or songs (or song types).</p></disp-quote><p>We believe these questions are mainly based on a misunderstanding of the referenced sentence in l.72. For clarification, we have rephrased as follows: “Each Bengalese finch song repertoire included ~5-12 acoustically distinct elements (‘syllables’) that are strung together into long sequences in variable but non-random order”. A song, or song bout, (which we colloquially use interchangeably but have now tried to exclusively use song bout in the manuscript) is defined as is typical in the field, for zebra finch and Bengalese finch, as a period of continuous vocalizations separated by 2s of silence (now defined in the Methods). There are no different song types in Bengalese finches, and each song bout typically contains several renditions of both target sequences.</p><disp-quote content-type="editor-comment"><p>Line 237: 'Figure 3A,B shows song bouts for one example bird'. Since song bouts are defined by authors as 'separated by at least 2 sec' I would like to know whether the shown spectrogram is the entire bout and the silence before and after are just not shown or whether A and B show part of a bout. If so, can you show the entire bout, including the time when the light changes?</p></disp-quote><p>The spectrograms do not show the entire bout, but show an exemplary section of the bout, to make it easier to recognize target sequences. We have added a supplementary to Figure 3 to show the entire bout. We cannot show the time of the light change, as the recording program was set up to never change lights in the middle of song, i.e. the light changed as soon as the recording for the first bout ended, and before the recording for any following bout started.</p><disp-quote content-type="editor-comment"><p>Line 332: 'The ability to alter syllable sequencing in a flexible fashion also contrasts with prior studies that have demonstrated modulation of vocalizations in more naturalistic settings.(…). In contrast, here we show that birds can learn to locally modulate specific features of their songs (i.e. individually targeted syllable transitions) in response to arbitrarily assigned contextual stimuli that have no prior ethological relevance.' Could the authors please comment on the following conundrum: If flexible use of song sequences under natural conditions were 'hard-wired/innate/reflexive/affective' as the authors suggest, how would the ability to pair an arbitrary contextual cue with a particular song sequence have evolved in Bengalese finches? Why would neural connections exist that allow this pairing of visual input to motor output? Isn't it more parsimonious to postulate that under natural conditions, visual stimuli do lead to different vocal motor responses because in addition to the known 'affective' mediators (hormones, dopamine etc) there is some 'top down', 'cognitive' control? (Reviewer 1 agrees with this point).</p><p>Line 348: ' Evolution of control over vocal sequencing' section is in line with my above comment, e.g. suggests what some animals might use the ability to use contextual visual information for adaptive motor output but again negates that Bengalese finches actually use it in their current behavior, instead the authors call it 'latent capacity'. I do not follow their logic.</p></disp-quote><p>We largely agree with this interpretation and have now added discussion to clarify this logic in the “Evolution” paragraph, and explicitly state that some examples of natural contextual variation likely also involve more cognitive processing.</p><p>l. 420f: “and suggest that some of the contextual control observed in natural settings may likewise rely on learned associations and other cognitive factors.”</p><disp-quote content-type="editor-comment"><p>Line 367: Neural implementation. Does the two process model relate to human speech and language? Please explain.</p></disp-quote><p>We have added references to speech motor adaptation and language selection studies, and clarified that the rest of this paragraph concerns models related to more general motor control processes.</p><disp-quote content-type="editor-comment"><p>Line 428: add 'male'. The manuscript does not mention anywhere whether males and females in Bengalese finches sing….Or add it to line 23 in the Abstract.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>Line 429: 'age range 128-320 days' was there any age-related difference in learning? Looking at the figures some birds seemed to have performed quite a bit better than others. See also line 456 below.</p><p>Line 456: 'after several days of training (average 33)' Please also provide the range and whether shorter training was related to age of the birds.</p></disp-quote><p>We have added analyses in Sup. Figure 4 to examine whether the bird’s performance depended on age and other possible explanatory variables. We did not find a significant correlation with any tested variables, although that may be expected given the small sample size and idiosyncratic features of each song and choice of branch point. Follow-up studies would need to be performed which systematically test learning ability at different branch points of the same bird.</p><p>We now also note that training duration (range 12-79 days) was not varied across birds in a fashion that was explicitly related to magnitude of sequence changes, and indeed was not a tightly controlled variable in these experiments.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-resp-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Line 461: 'three birds.…never progressed to full probe sequence either because they did not exhibit single-context learning or because of technical issues with consistent targeting of branch points'. Did two birds not learn and one had technical issue or other way round? How common is it in WN-escape experimental set-ups that birds do not learn? And what does 'single-context learning' mean? That they did not learn to associate yellow light with one target? This would imply that context 1 was learned first and then context 2, but in line 454 it sounds like both colors were paired with their particular target after one to several hours. Please explain.</p></disp-quote><p>We have clarified that one bird did not learn in single context training, one bird was abandoned due to technical difficulties, and one bird exhibited a loss of sequence variability during initial training that prevented further differential training.</p><p>We first tested each bird to ensure that it was capable of “single context learning” before initiating context dependent training. Single context learning as now defined in results means learning in one direction with constant light color, as in Figure 1C,D, and we construed this as a likely pre-requisite for context-dependent learning. It happens sometimes that birds do not learn in WN-escape experiments, typically because of some higher-order structure in the song (such as history dependence as described by Warren et al. 2012). However, for one pilot bird that did not exhibit single context learning in these initial experiments (similar to 1C,D) we nonetheless initiated dual context-dependent training to see if it might develop learning over time. We never saw evidence of learning in that bird, and training was abandoned.</p><p>A second bird was excluded because of technical issues with maintaining accurate targeting of syllables through template matching (see Methods) to deliver WN.</p><p>The third excluded bird learned well in single context training, but lost sequence variability over the course of initial training for reasons that are unclear and in a manner that was not observed in other birds. This resulted in the elimination of one of the target sequences, precluding differential training, and the bird was abandoned. These further details are now noted in Methods.</p><disp-quote content-type="editor-comment"><p>Line 466: Please specify in the methods how many days the entire experiment lasted. How variable was the song output during that time and between individual birds? Did song output decline over time? Can the authors provide an estimate how many songs or bouts on average (and range) the birds sang?</p></disp-quote><p>Consistent with prior observations (Yamahachi et al., 2020 Plos One), we found that some birds increased and others decreased the average number of song bouts per day. On average, birds sang 226 (range 66-356) bouts during baseline days and 258 (range 171-368) bouts per day during the period of probe collection at the end of training (14% increase). The average duration of song bouts also changed little, with both the average number of target sequences per bout (8.7 during baseline, 7.7 during probes, 7% decrease) and the average number of syllables per bout (74 during baseline, 71 during probes, 2% decrease) decreasing slightly. These numbers are now included in Methods.</p><fig id="respfig2"><label>Author response image 2.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61610-resp-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewer 3:</p><p>Although the present study describes the syllable sequence switching abilities of Bengalese finches within the framework of an elegantly designed behavioral paradigm, the links to the potential neural mechanisms are poorly presented or even obsolete since the authors do not provide any evidence about underlying brain dynamics. I recommend to rather discuss the results in a behavioral framework unless the authors add results from neural recordings or brain manipulations.</p></disp-quote><p>We have expanded Introduction and Discussion on behavioral studies in birds and humans, and have reserved any speculation about neural mechanisms for the discussion. We retained some discussion of this point, as songbirds are an extensively studied model for neural mechanisms of vocal motor control; this large prior body of work on neural mechanisms enables some informed speculation about how the ability to rapidly adjust song in response to learned, visual cues could be accomplished by the song system, and we felt this would be of potential interest to more mechanistically inclined readers.</p><disp-quote content-type="editor-comment"><p>Line 34: Citation for reordering of finite elements to achieve infinite meaning, see Hauser, Chomsky and Fitch, 2002</p><p>Line 47 f: Lipkind et al. 2017 showed that zebra finches can learn to re-order syllables during song learning. This paper is highly relevant and should be discussed.</p><p>Line 52: Reference to Doupe and Kuhl, 1999 should be moved to line 46?</p></disp-quote><p>We have added these references.</p><disp-quote content-type="editor-comment"><p>Line 88 f: The authors decide to present most data in percentages. It would be useful to provide the actual number to assess the quality of the data.</p></disp-quote><p>We have clarified that the measure used throughout this study, sequence probability, is in units of percent. The changes that we describe in percentages are absolute changes in sequence probability, which reflect the proportion of each target sequence, not percent changes. For example, the plots in Figure 2 A-D (and similar ones throughout the manuscript) show raw, absolute values of sequence probability. To provide a measure of the quality of the data, we provide error bars, reflecting s.e.m. across song bouts.</p><p>We previously had not provided error bars on Figure 1 B-D, as these data points are based on all target sequences sung on a given day (i.e. there is only one data point per day). We have now added confidence intervals to Figure 1 B-D, estimated from normal approximation of the binomial probability of the proportion of ‘abd’ and ‘abc’ target sequences. The actual number of either target sequence are 1854 per day during baseline, 808 per day during ab-d targeting (fewer, because day 1 and day 4 are not full days of singing, but belong partly to baseline or ab-c targeting), 1888 per day during ab-c targeting.</p><disp-quote content-type="editor-comment"><p>Line 89: How reliable was the software in targeting syllables?</p></disp-quote><p>The reliability of the templates was checked continuously throughout the experiment, but we did not keep careful notes on this for each bird. We have therefore retroactively assessed the specificity and sensitivity of the template by hand-checking 20 randomly selected song bouts from a single day of training for each bird, and added this information to Methods:</p><p>“We determined sensitivity and specificity of the template to the target sequence on a randomly selected set of 20 song bouts on which labels and delivery of WN was hand-checked. […] The average template specificity was 96.7% (range 90.6-100% ).”</p><disp-quote content-type="editor-comment"><p>Line 284: The authors refer to white light as a neutral state. What is the color perception for Bengalese finches? Is white perceived rather as yellow or green? A novel light condition that the birds had not been exposed before would probably be better.</p><p>Line 306 f: Light cues are not arbitrary as the birds are initially trained to connect white noise with light of a certain color.</p><p>Line 444: What was the reason to specifically choose green and yellow as colors for this experiment?</p><p>Line 445: How much does visual perception of Bengalese finches differ between 520 and 600nm?</p></disp-quote><p>We mean by ‘arbitrary’ that these colors have no prior ethological meaning for the behavior of the bird. Hence, the color perception should not matter, as long as the birds are able to discriminate the colors. We set out to demonstrate that the birds would be able to associate song changes with any arbitrary visual stimulus. There was no reason to choose these specific colors. The white light is “neutral” only insofar as the birds had learned that aversive WN would never occur in the white context, not because it is spectrally in the middle of green and yellow. We did use a white LED light which was different from the home light in the cage, which was also white and the birds might have had experience with prior to any context training.</p><disp-quote content-type="editor-comment"><p>Line 453-455: What is the maximum duration of several hours? This would also help to understand the difference in amounts of switches in Figure 1 F and G.</p><p>Figure 1 F: Why is the amount of color switches different between baseline and training? When within the training did baseline days occur or was this prior to training?</p><p>Figure 1 G: Why are light phases for green/yellow differently long? The error bars are misleading, as they show the SEM of individual blocks rather than the entire sample.</p></disp-quote><p>We have added further explanation of block durations in the Methods (see also Reviewer2). Probes were collected before WN training. The training schedule was changed between 1F and 1G. The individual light phases are drawn from random intervals, therefore, it might randomly happen that on one day the yellow contexts appear longer than the green context (or vice versa), but the two colors are drawn from the same intervals, so this should even out over time. We think that the SEM per block should be informative about effect reliability, and have expanded the legend for Figure 1G so as to clarify this measure.</p><disp-quote content-type="editor-comment"><p>Line 456: How did the white noise training look? Can learning curves be added?</p></disp-quote><p>Learning data are shown in Figure 1 C and D for single context training, and Figure 2 G,H for the contextual training protocol.</p><disp-quote content-type="editor-comment"><p>Figure 1 B: It would be helpful to plot both probability curves (ab-d and ab-c) and color code them accordingly as a general probability plot (y axis).</p></disp-quote><p>We now have added the probability for the other target sequence; thank you for the suggestion.</p></body></sub-article></article>