<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86224</article-id><article-id pub-id-type="doi">10.7554/eLife.86224</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Designing optimal behavioral experiments using machine learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-303694"><name><surname>Valentin</surname><given-names>Simon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2039-8928</contrib-id><email>simonvalentin@me.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-329780"><name><surname>Kleinegesse</surname><given-names>Steven</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-303735"><name><surname>Bramley</surname><given-names>Neil R</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-303736"><name><surname>Seriès</surname><given-names>Peggy</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-303737"><name><surname>Gutmann</surname><given-names>Michael U</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-303738"><name><surname>Lucas</surname><given-names>Christopher G</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>School of Informatics, University of Edinburgh</institution></institution-wrap><addr-line><named-content content-type="city">Edinburgh</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>Department of Psychology, University of Edinburgh</institution></institution-wrap><addr-line><named-content content-type="city">Edinburgh</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>01</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>13</volume><elocation-id>e86224</elocation-id><history><date date-type="received" iso-8601-date="2023-01-17"><day>17</day><month>01</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2023-11-19"><day>19</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2023-05-12"><day>12</day><month>05</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2305.07721"/></event></pub-history><permissions><copyright-statement>© 2024, Valentin, Kleinegesse et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Valentin, Kleinegesse et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86224-v1.pdf"/><abstract><p>Computational models are powerful tools for understanding human cognition and behavior. They let us express our theories clearly and precisely and offer predictions that can be subtle and often counter-intuitive. However, this same richness and ability to surprise means our scientific intuitions and traditional tools are ill-suited to designing experiments to test and compare these models. To avoid these pitfalls and realize the full potential of computational modeling, we require tools to design experiments that provide clear answers about what models explain human behavior and the auxiliary assumptions those models must make. Bayesian optimal experimental design (BOED) formalizes the search for optimal experimental designs by identifying experiments that are expected to yield informative data. In this work, we provide a tutorial on leveraging recent advances in BOED and machine learning to find optimal experiments for any kind of model that we can simulate data from, and show how by-products of this procedure allow for quick and straightforward evaluation of models and their parameters against real experimental data. As a case study, we consider theories of how people balance exploration and exploitation in multi-armed bandit decision-making tasks. We validate the presented approach using simulations and a real-world experiment. As compared to experimental designs commonly used in the literature, we show that our optimal designs more efficiently determine which of a set of models best account for individual human behavior, and more efficiently characterize behavior given a preferred model. At the same time, formalizing a scientific question such that it can be adequately addressed with BOED can be challenging and we discuss several potential caveats and pitfalls that practitioners should be aware of. We provide code to replicate all analyses as well as tutorial notebooks and pointers to adapt the methodology to different experimental settings.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>experimental design</kwd><kwd>machine learning</kwd><kwd>computational modeling</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000848</institution-id><institution>University of Edinburgh</institution></institution-wrap></funding-source><award-id>Principal's Career Development Scholarship</award-id><principal-award-recipient><name><surname>Valentin</surname><given-names>Simon</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>EPSRC Centre for Doctoral Training in Data Science</institution></institution-wrap></funding-source><award-id>EP/L016427/1</award-id><principal-award-recipient><name><surname>Kleinegesse</surname><given-names>Steven</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Recent advances in Bayesian optimal experimental design have made it possible to improve the efficiency and informativeness of experiments using machine learning, and shed new light on considerations that affect machine learning assisted experimental designs and computational models in general.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Computational modeling of behavioral phenomena is currently experiencing rapid growth, in particular with respect to methodological improvements. For instance, seminal work by <xref ref-type="bibr" rid="bib88">Wilson and Collins, 2019</xref> has been crucial in raising methodological standards and bringing attention to how computational analyses can add value to the study of human (and animal) behavior. Meanwhile, in most instances, computational analyses are applied to data collected from experiments that were designed based on intuition and convention. That is, while experimental designs are usually motivated by scientific questions in mind, they are often chosen without explicitly and quantitatively considering how informative these data might be for the computational analyses and, finally, the scientific questions being studied. This can, in the worst case, completely undermine the research effort, especially as particularly valuable experimental designs can be counter-intuitive. Today, advancements in machine learning (ML) open up the possibility of applying computational methods when deciding how experiments should be designed in the first place, to yield data that are maximally informative with respect to the scientific question at hand.</p><p>In this work, we provide an introduction to modern BOED and a step-by-step tutorial on how these advancements can be combined and leveraged to find optimal experimental designs for any computational model that we can simulate data from, and show how by-products of this procedure allow for quick and straightforward evaluation of models and their parameters against real experimental data. Formalizing the scientific goal of an experiment such that our design aligns with our intention can be difficult. BOED forces the researcher to make explicit, and engage with, various assumptions and constraints that would otherwise remain implicit, which helps in making experimental research more rigorous but can also lead to unexpected optimal designs. We discuss several issues related to experimental design that practitioners may encounter no matter how they design their experiments. Importantly, these issues are made more salient by formalizing the process.</p></sec><sec id="s2"><title>Why optimize experimental designs?</title><p>Experiments are the bedrock of scientific data collection, making it possible to discriminate between different theories or models, and discovering what specific commitments a model must make to align with reality by means of parameter estimation. The process of designing experiments involves making a set of complex decisions, where the space of possible experimental designs is typically navigated based on a combination of researchers’ prior experience, intuitions about what designs may be informative, and convention.</p><p>This approach has been successful for centuries across various scientific fields, but it has important limitations that are amplified as theories become richer, more nuanced, and more complex. Often, a theory does not make a concrete prediction but rather has free parameters that license a variety of possible predictions of varying plausibility. For example, if we want to model human behavior, it is important to recognize that different participants in a task might have different strategies or priorities, which can be captured by parameters in a model. As we expand models to accommodate more complex effects, it becomes ever more time-consuming and difficult to design experiments that distinguish between models, or allow for effective parameter estimation. As an example from cognitive science, there are instances where models turned out to be empirically indistinguishable under certain conditions, a fact that was only recognized after a large number of experiments had already been done (<xref ref-type="bibr" rid="bib29">Jones and Dzhafarov, 2014</xref>). Such problems also play an important role in the larger context of the replication crisis (e.g. <xref ref-type="bibr" rid="bib55">Pashler and Wagenmakers, 2012</xref>).</p><p>Collecting experimental data is typically a costly and time-consuming process, especially in the case of studies involving resource-intensive recording techniques, such as neuroimaging or eye-tracking (e.g. <xref ref-type="bibr" rid="bib12">Dale, 1999</xref>; <xref ref-type="bibr" rid="bib42">Lorenz et al., 2016</xref>). There is a strong economic as well as ethical case for conducting experiments that are expected to lead to maximally informative data with respect to the scientific question at hand. For instance, in computational psychiatry, researchers are often interested in estimating parameters describing people’s traits and how they relate, often doing so under constrained resources in terms of the number of participants and their time. Additionally, researchers and practitioners alike require that experiments are sufficiently informative to provide confidence that they would reveal effects if they are there and also that negative results are true negatives (e.g. <xref ref-type="bibr" rid="bib26">Huys et al., 2013</xref>; <xref ref-type="bibr" rid="bib32">Karvelis et al., 2018</xref>). Consequently, as theories of natural phenomena become more realistic and complex, researchers must expand their methodological toolkit for designing scientific experiments.</p></sec><sec id="s3"><title>Bayesian optimal experimental design</title><p>As a potential solution, <italic>Bayesian optimal experimental design</italic> (BOED; see <xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Rainforth et al., 2023</xref>, for reviews) provides a principled framework for optimizing the design of experiments. Broadly speaking, BOED rephrases the task of finding optimal experimental designs to that of solving an optimization problem. That is, the researcher specifies all controllable parameters of an experiment, also known as experimental designs, for which they wish to find optimal settings, which are then determined by maximizing a utility function. Exactly what we might want to optimize depends on our experiment, but can include any aspect of the design that we can specify, such as which stimuli to present, when or where measurements should be taken in a quasi-experiment, or, as an example, how to reward risky choices in a behavioral experiment. The utility function is selected to measure the quality of a given experimental design with respect to the scientific goal at hand, such as model discrimination, parameter estimation or prediction of future observations, with typical choices including expected information gain, uncertainty reduction, and many others (<xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>), as explained below. Importantly, BOED approaches require that theories are formalized via computational models of the underlying natural phenomena.</p></sec><sec id="s4"><title>Computational models</title><p>Scientific theories are increasingly formalized via computational models (e.g. <xref ref-type="bibr" rid="bib25">Guest and Martin, 2021</xref>). These models take many forms, but one desideratum for a model is that, given a set of experimental data, we can compare it to other models based on how well it predicts that data. To that end, many computational models are constructed in a way that allows for the analytic evaluation of the likelihood of the model given data, by means of a likelihood function. However, this imposes strong constraints on the space of models we can consider. Models that are rich enough to accurately describe complex phenomena—like human behavior—often have intractable likelihood functions (e.g. <xref ref-type="bibr" rid="bib41">Lintusaari et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Cranmer et al., 2020</xref>). In other words, we cannot compute likelihoods due to the prohibitive computational cost of doing so, or the inability to express the likelihood function mathematically. This has at least two implications (1) When we develop rich models, we often lack the tools to evaluate and compare them directly, and are left with coarse, qualitative proxy measures–like the “the model curves look like the human curves for some parameter settings!”; (2) when we are intent on conducting a systematic comparison of models, we are often forced to use simplified models that have tractable likelihoods, even when the simplifications lead to consequential departures from the theories we might want our models to capture.</p><sec id="s4-1"><title>Simulator models: What if the likelihood cannot be computed?</title><p>Many realistically complex models have the feature that we can <italic>simulate</italic> data from them, which allows for using simulation-based inference methods, such as approximate Bayesian computation (ABC) and many others (e.g. <xref ref-type="bibr" rid="bib41">Lintusaari et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Cranmer et al., 2020</xref>). There has been a growing interest in this class of models, often referred to as <italic>generative</italic>, <italic>implicit,</italic> or <italic>simulator</italic> models (e.g. <xref ref-type="bibr" rid="bib43">Marjoram et al., 2003</xref>; <xref ref-type="bibr" rid="bib72">Sisson et al., 2007</xref>; <xref ref-type="bibr" rid="bib41">Lintusaari et al., 2017</xref>; <xref ref-type="bibr" rid="bib5">Brehmer et al., 2020</xref>; <xref ref-type="bibr" rid="bib10">Cranmer et al., 2020</xref>). We will use the term simulator model throughout this paper to refer to any model from which we can simulate data. We note that some generative models have tractable likelihood functions, but this class of models is subsumed by simulator models, which do not make any structural assumptions on the likelihood and include important cases where the likelihood is too expensive to be computed, or cannot be computed at all (simulator models also include the common case of a potentially complex deterministic simulator with an additional observation noise component, as is used in many areas of science). Simulator models have become ubiquitous in the sciences, including physics (<xref ref-type="bibr" rid="bib67">Schafer and Freeman, 2012</xref>), biology (<xref ref-type="bibr" rid="bib62">Ross et al., 2017</xref>), economics (<xref ref-type="bibr" rid="bib23">Gouriéroux et al., 2010</xref>), epidemiology (<xref ref-type="bibr" rid="bib9">Corander et al., 2017</xref>), and cognitive science (<xref ref-type="bibr" rid="bib52">Palestro et al., 2018</xref>), among others.</p><p>The study of human, and animal, perception and cognition is experiencing a surge in the use of computational modeling (e.g. <xref ref-type="bibr" rid="bib24">Griffiths, 2015</xref>). Here, simulator models can formalize complex theories about latent psychological processes to arrive at quantitative predictions about observable behavior. As such, simulator models appear across cognitive science, including for example, many Bayesian and connectionist models, a wide range of process models, cognitive architectures, or many reinforcement learning models and physics engines (see, e.g. <xref ref-type="bibr" rid="bib76">Turner and Van Zandt, 2012</xref>; <xref ref-type="bibr" rid="bib77">Turner and Sederberg, 2014</xref>; <xref ref-type="bibr" rid="bib2">Bitzer et al., 2014</xref>; <xref ref-type="bibr" rid="bib79">Turner et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bramley et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Ullman et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Palestro et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Kangasrääsiö et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Gebhardt et al., 2021</xref>). The applicability and usefulness of experimental design optimization for parameter estimation and model comparison has been demonstrated in many scientific disciplines, but this has often been restricted to settings with simple and tractable classes of models, for instance in cognitive science (e.g. <xref ref-type="bibr" rid="bib47">Myung and Pitt, 2009</xref>; <xref ref-type="bibr" rid="bib92">Zhang and Lee, 2010</xref>; <xref ref-type="bibr" rid="bib50">Ouyang et al., 2018</xref>).</p></sec></sec><sec id="s5"><title>An overview of this tutorial</title><p>In this tutorial, we present a flexible workflow that combines recent advances in ML and BOED in order to optimize experimental designs for any computational model from which we can simulate data. A high-level overview of the underlying ML-based BOED method is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, including researchers’ inputs and resulting outputs. The presented approach is applicable to any model from which we can simulate data and scales well to realistic numbers of design variables, experimental trials and blocks. Furthermore, as well as optimized designs, the presented approach also results in tractable amortized posterior inference—that is, it allows researchers to use their actual experimental data to easily compute posterior distributions, which may otherwise be computationally expensive or intractable to compute. Moreover, we note that, to our knowledge, there has been no study that performs BOED for simulator models without likelihoods and then uses the optimal designs to run a real experiment. Instead, most practical applications of BOED have been limited to simple scientific theories of nature (e.g. <xref ref-type="bibr" rid="bib39">Liepe et al., 2013</xref>). This paucity of real-world applications is partly because the situations where BOED would be most useful are those where it has previously been computationally intractable to use. The ML-based approach presented in this work aims to overcome these limitations.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>A high level overview of our approach that uses machine learning (ML) to optimize the design of experiments.</title><p>The inputs to our method are a model from which we can simulate data and a prior distribution over the model parameters. Our method starts by drawing a set of samples from the prior and initializing an experimental design. These are used to simulate artificial data using the simulator model. We then use ML to estimate the expected information gain of those data, which is used as a metric to search over the design space. Finally, this is repeated until convergence of the expected information gain. The outputs of our method are optimal experimental designs, an estimate of the expected information gain when performing the experiment and an amortized posterior distribution, which can be used to cheaply compute approximate posterior distributions once real-world data are observed. Other useful by-products of our method include a set of approximate sufficient summary statistics and a fitted utility surface of the expected information gain.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-fig1-v1.tif"/></fig><p>As with many new computational tools, however, the techniques for using BOED are not trivial to implement from scratch. In an effort to help other scientists understand these methods and use them to unlock the full potential of BOED in their own research, we next present a case study in using BOED to better understand human decision-making. Specifically, we (1) describe new simulator models that generalize earlier decision-making models that were constrained by the need to have tractable likelihoods; (2) walk through the steps to use BOED to design experiments comparing these simulator models and estimating the psychologically interpretable parameters they contain; and (3) contrast our new, optimized experiments to previous designs and illustrate their advantages and the new lessons they offer. All our steps and analyses are accompanied by code that we have commented and structured to be easily adapted by other researchers.</p></sec><sec id="s6"><title>Case study: Multi-armed bandit tasks</title><p>As a case study application, we consider the question of how people balance their pursuit of short-term reward (‘exploitation’) against learning how to maximize reward in the longer term (‘exploration’). This question has been studied at length in psychology, neuroscience, and computer science, and one of the key frameworks for investigating it is the multi-armed bandit decision-making task (e.g. <xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Dezfouli et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Schulz et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Gershman, 2018</xref>). Multi-armed bandits have a long history in statistics and machine learning (e.g. <xref ref-type="bibr" rid="bib61">Robbins, 1952</xref>; <xref ref-type="bibr" rid="bib74">Sutton and Barto, 2018</xref>) and formalize a general class of sequential decision problems—repeatedly choosing between a set of options under uncertainty with the goal of maximizing cumulative reward (for more background, see <xref ref-type="box" rid="box1">Box 1</xref>).</p><boxed-text id="box1"><label>Box 1.</label><caption><title>Bandit tasks.</title></caption><p>Bandit tasks constitute a minimal reinforcement learning task, in which the agent is faced with the problem of balancing exploration and exploitation. Various algorithms have been proposed for maximizing the reward in bandit tasks, and some of these algorithms have been used directly or in modified form as models of human behavior. A selection of such algorithms indeed captures important psychological mechanisms in solving bandit tasks (<xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>). However, a richer account of human behavior in bandit tasks would seem to be required to accommodate flexibility and nuance of human behavior (e.g. see <xref ref-type="bibr" rid="bib13">Dezfouli et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Lee et al., 2011</xref>), leading to simulator models that do not necessarily permit familiar likelihood-based inference, due to, for example unobserved latent states, and which complicate the task of intuiting informative experiments. See <xref ref-type="fig" rid="box1fig1">Box 1—figure 1</xref> for a schematic of the data-generating process of multi-armed bandit tasks.</p><fig position="float" id="box1fig1"><label>Box 1—figure 1.</label><caption><title>Schematic of the data-generating process for the example of multi-armed bandit tasks.</title><p>For a given design <bold>d</bold> (here the reward probabilities associated with the bandit arms in each individual block) and a sample from the prior over the model parameters <inline-formula><mml:math id="inf1"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>, the simulator generates observed data <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, corresponding to the actions and rewards from <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi></mml:mstyle></mml:math></inline-formula> blocks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box1-fig1-v1.tif"/></fig></boxed-text><p>We demonstrate the applicability of our method by optimizing the design of a multi-armed bandit decision-making task, considering three flexible extensions of previously proposed models of human behavior. In our experiments, we consider two general scientific tasks: model discrimination and parameter estimation. We then evaluate the optimal designs found using our method empirically with simulations and real human behavioral data collected from online experiments, and compare them to designs commonly used in the literature. We find that, as compared to commonly used designs, our optimal experimental designs yield significantly better model recovery, more informative posterior distributions and improved model parameter disentanglement.</p></sec><sec id="s7"><title>Workflow summary</title><p>In the following sections, we describe and walk through our workflow step-by-step, using the aforementioned multi-armed bandit task setting as a concrete case study to further illustrate ML-based BOED. At a high level, our workflow comprises the following steps:</p><list list-type="order"><list-item><p>Defining a scientific goal, for example, which model, among a set of models, best describes a natural phenomenon.</p></list-item><list-item><p>Formalizing a theory, or theories, as a computational model(s) that can be sampled from.</p></list-item><list-item><p>Setting up the design optimization problem and deciding which aspects of the experimental design need to be optimized.</p></list-item><list-item><p>Constructing the required ML models and training them with simulated data.</p></list-item><list-item><p>Validating the obtained optimal designs in silico and, potentially, re-evaluating design and modeling choices.</p></list-item><list-item><p>After confirming and validating the optimal design, the real experiment can be performed. The trained ML models used for BOED can be used afterwards to easily compute posterior distributions.</p></list-item></list><p>We provide a more detailed description of the above workflow in the following sections. The outputs of the ML-based BOED method are optimal experimental designs, an estimate of the (maximum) expected information gain and an amortized posterior distribution, which allows for straightforward computation of posterior distributions from collections of real-world data and saves us a potentially computationally expensive (likelihood-free) inference step. Readers may find that many of these steps should be part of the design of experiments no matter whether using BOED or not. However, BOED here provides the additional advantage of making all of these steps, which are sometimes carried out heuristically, explicit and formal.</p></sec><sec id="s8"><title>Step 1: Formulate your scientific goal</title><p>The first step in setting up the experimental design optimization is to define our scientific goal. Although there may be many different goals for an experiment, they often fall into the broad categories of model discrimination or parameter estimation.</p><sec id="s8-1"><title>Model discrimination</title><p>Computational models provide formal and testable implementations of theories about nature. Model discrimination (also often referred to as <italic>model comparison</italic>) lies at the core of many scientific questions and amounts to the problem of deciding which of a set of different models best explains some observed data. A rigorous formulation of this problem is given by the Bayesian approach: Starting from a prior belief about the plausibility of different models and their parameters, we consider how well different models can explain the observed data and use this information to update our prior beliefs. Crucially, more flexible theories are penalized automatically via the notion of the Bayesian Ockham’s razor. This ensures that in cases where two theories can explain the data equally well, we should favor the simpler one. Formally, this requires computing the marginal likelihood of our different models, which may mean integrating (or for discrete parameters, summing) over a large number of model parameters, which is often intractable, and thus often tackled via information criteria, which may, however, suffer from various problems (e.g. <xref ref-type="bibr" rid="bib57">Pitt and Myung, 2002</xref>; <xref ref-type="bibr" rid="bib14">Dziak et al., 2020</xref>; <xref ref-type="bibr" rid="bib66">Schad et al., 2021</xref>). This computational complexity of computing marginal model likelihoods is exacerbated when model parameter likelihood is intractable, as is the case for many simulator models.</p><p>With respect to our case study, we may, for instance, be interested in comparing which of the three simulator models presented in <xref ref-type="box" rid="box2">Box 2</xref> best explains human behavior in multi-armed bandit tasks.</p><boxed-text id="box2"><label>Box 2.</label><caption><title>Models of human behavior in bandit tasks.</title></caption><p>We study models of human choice behavior in bandit tasks to demonstrate and validate our method. We generalize previously proposed models (see Appendix 3 for details), and consider three novel computational models for which likelihoods are intractable.</p><p><bold>Win-Stay Lose-Thompson-Sample (WSLTS)</bold></p><p>Posits that people tend to repeat choices that resulted in a reward, and otherwise to explore by selecting options with a probability proportional to their posterior probability of being the best (Thompson sampling). This model generalizes the Win-Stay Lose-Shift model that has been considered in previous work (<xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>), but accommodates the possibility that people might be more likely to shift to more promising alternatives than uniformly at random.</p><p><bold>Auto-regressive</bold> <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula><bold>-Greedy (AEG)</bold></p><p>Formalizes the idea that people might greedily choose the option with the highest estimated reward in each trial with a certain probability or otherwise explore randomly, but also permits that people may be ‘sticky’, that is, having some tendency to re-select whatever option was chosen on the previous trial, or ‘anti-sticky’, that is, preferring to switch to a new option on each trial. This generalizes the <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-greedy model studied previously (<xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>), further accommodating the possibility that people preferentially stick to or avoid their previous selection.</p><p><bold>Generalized Latent State (GLS)</bold></p><p>Captures the idea that people might have an internal latent exploration-exploitation state that influences their choices when encountering situations in which they are faced with an explore-exploit dilemma. Transitions between these states may depend on previously encountered rewards and the previous latent state. This generalizes previous latent-state models that assume people shift between states independently of previous states and observed rewards <xref ref-type="bibr" rid="bib91">Zhang et al., 2009</xref>, or only once per task (<xref ref-type="bibr" rid="bib37">Lee et al., 2011</xref>). As these models are novel and to illustrate the method, we do not specify strong prior beliefs about any model parameters and describe the priors distribution in Appendix 1.</p></boxed-text></sec><sec id="s8-2"><title>Parameter estimation</title><p>Researchers are often interested in learning about model parameters of a particular, single model. Similar to the model discrimination approach, a Bayesian approach of learning about these parameters would involve starting with a prior distribution and then updating our belief via Bayes’ rule using observed data. For instance, we may be interested in estimating an individual’s (or population’s) working memory capacity given a working memory model, response time distribution in a model for selective attention, choice probability of choosing between different options in a risky choice model, inclination to explore versus exploit in a reinforcement task, or associations between biases in probabilistic visual integration tasks and autistic/schizotypal traits (<xref ref-type="bibr" rid="bib32">Karvelis et al., 2018</xref>) to name a few examples. Model parameters may here be continuous or discrete, which matters for how we set up up the optimization procedure in Step 4.</p><p>As an example, for the Auto-regressive <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϵ</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy (AEG) simulator model in our case study, we are, among many other things, interested in estimating peoples’ inclination to re-select previously chosen options as opposed to avoiding repeating their own past behavior, which is characterized by a specific model parameter.</p></sec><sec id="s8-3"><title>Other goals</title><p>While we here focus on the problems of model discrimination and parameter estimation, we point out that the overall BOED approach via mutual information (MI) estimation is more general, and can easily be adapted to other tasks, such as improving future predictions (e.g. <xref ref-type="bibr" rid="bib35">Kleinegesse and Gutmann, 2021</xref>) and many others (see <xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>).</p></sec><sec id="s8-4"><title>How do we measure the value of an experimental design?</title><p>We formalize the quality of our experimental design via a utility function, which thereby serves as a quantitative way of measuring the value of one experimental design over any another and provides the objective function for our optimization problem. Following recent work in BOED (e.g. <xref ref-type="bibr" rid="bib33">Kleinegesse and Gutmann, 2019</xref>; <xref ref-type="bibr" rid="bib34">Kleinegesse et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Kleinegesse and Gutmann, 2021</xref>; <xref ref-type="bibr" rid="bib18">Foster et al., 2019</xref>), we use MI (<xref ref-type="bibr" rid="bib40">Lindley, 1956</xref>; also known as the <italic>expected information gain</italic>) as our utility function <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for measuring the value of an experimental design <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a variable we wish to estimate and <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the observed data (actions and rewards in our case study). Note that the probability density function <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the posterior distribution and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the prior distribution of the variable of interest.</p><p>For the model discrimination task, the variable of interest <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in our utility function corresponds to a scalar model indicator variable <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> (where <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> corresponds to a model 1, <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> to some model 2 and so on for as many models as we are comparing). For the parameter estimation task, the variable of interest <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in our utility function is the vector of model parameters <inline-formula><mml:math id="inf18"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> for a given model <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>MI has several appealing properties (<xref ref-type="bibr" rid="bib54">Paninski, 2005</xref>) that make it a popular utility function in BOED (<xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>). Intuitively, MI quantifies the amount of information our experiment is expected to provide about the variable of interest. Furthermore, the MI utility function can easily be adapted to various scientific goals by changing the variable of interest (<xref ref-type="bibr" rid="bib35">Kleinegesse and Gutmann, 2021</xref>). Optimal designs are then found by maximizing this utility function, that is <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-OP"><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. Although a useful quantity, estimating <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is typically extremely difficult, especially for simulator models that have intractable likelihoods <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (e.g. <xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>). Nonetheless, in the later steps of our workflow, we shall explain how we can approximate this utility function using ML-based approaches.</p></sec></sec><sec id="s9"><title>Step 2: Cast your theory as a computational model</title><p>After defining our scientific goal, we need to ensure that our scientific theories are, or can be, formalized as computational models. This either requires us to formalize them from the ground up, or leverage existing computational models from literature that support our theories. In the former case, this entails making all aspects of the theory explicit, which is therefore a highly useful and established practice; we refer the reader to <xref ref-type="bibr" rid="bib88">Wilson and Collins, 2019</xref> for a more general guidance on building computational models. For the presented ML-based BOED approach, we require that our theories are cast as computational models that allow us to simulate data, that is simulator models. Further we need to decide on a general experimental paradigm that needs to be able to interact with our computational models.</p><sec id="s9-1"><title>Building a computational model</title><p>Formally, a computational model defines a generative model <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> that allows sampling of synthetic data <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, given values of its model parameters <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the experimental design <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. We assume that sampling from this model is feasible, but make no assumptions about whether evaluating the likelihood function <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is tractable or intractable. Even if the likelihood is tractable and simple, important quantities such as the marginal likelihood may nevertheless be intractable, since evaluating the marginal likelihood requires integrating over all free parameters. This problem is naturally exacerbated for more complex models that may have intractable likelihood functions.</p><p>See <xref ref-type="box" rid="box2">Box 2</xref> for a set of simulator models from our case study. These models describe human behavior in a multi-armed bandit setting, where the observed data <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the sequences of chosen bandit arms along with their observed reward.</p></sec><sec id="s9-2"><title>What are our prior beliefs?</title><p>In addition to formalizing our theories as simulator models, we need to complete this step by defining prior distributions over the (free) model parameters, as would be required for any analysis. We point the reader to <xref ref-type="bibr" rid="bib88">Wilson and Collins, 2019</xref>; <xref ref-type="bibr" rid="bib66">Schad et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Mikkola et al., 2021</xref> for guidance on specifying prior distributions, as this is a broad research topic and a detailed treatment is beyond the scope of this work. Generally, prior distributions should reflect what is known about the model parameters prior to running the experiment, based on either empirical data or general domain knowledge. This is crucial for the design of experiments, as we may otherwise spend experimental resources and effort on learning what is already known about our models.</p></sec></sec><sec id="s10"><title>Step 3: Decide which aspect of your experiment to optimize</title><p>Having formalized our scientific theories as simulator models and decided on our scientific goals, we need to think about which parts of the experiment we want to optimize. In principle, any controllable aspect of the experimental apparatus could be tuned as part of the experimental design optimization. Concretely, in order to solve an experimental design problem by means of BOED, our experimental designs <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> need to be formalized as part of the simulator model <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Instead of optimizing all controllable aspects of an experiment, it may then help to leverage domain knowledge and prior work to choose a sufficient experimental design set, in order to reduce the dimensionality of the design problem.</p><p>In our case study, for instance, we have chosen the reward probabilities of different bandit arms to be the experimental designs to optimize over. This choice posits that certain reward environments yield more informative data than others, in the context of model discrimination and parameter estimation. Our task is then to find the optimal reward environment that yields maximally informative data.</p><p>There may be other experimental choices that we will have to make, in particular to limit variability in the observed data, to control for confounding effects, and to reduce non-stationarity, among many other reasons. In our multi-armed bandit setting, for example, we may choose to limit the number of trials in the experiment to avoid participant fatigue, which is usually not accounted for in computational models of behavioral phenomena. Additionally, we may limit the number of available choices in a discrete choice task or impose constraints on any other aspects suggested by domain knowledge, or where we want to ensure comparability with prior research. For example, we may have reason to believe that certain designs are more ecologically valid than others—we discuss such considerations in the <italic>Caveats</italic> Section.</p><sec id="s10-1"><title>Static and adaptive designs</title><p>We consider the common case of <italic>static</italic> BOED, with the possibility of having multiple blocks of trials. That is, we find a set of optimal designs prior to actually performing the experiment. In some cases, however, a scientist may wish to optimize the designs sequentially as the participant progresses through the experiment. This is called <italic>sequential</italic>, or <italic>adaptive</italic>, BOED (see <xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Rainforth et al., 2023</xref>) and has been explored previously for tractable models of human behavior (<xref ref-type="bibr" rid="bib48">Myung et al., 2013</xref>), or in the domain of adaptive testing (e.g. <xref ref-type="bibr" rid="bib86">Weiss and Kingsbury, 1984</xref>). More recently, sequential BOED has also been extended to deal with implicit models (<xref ref-type="bibr" rid="bib34">Kleinegesse et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Ivanova et al., 2021</xref>). At this point, sequential BOED is an active area of research and may be a consideration for researchers in the future. Generally, sequential designs can be more efficient, as they allow to adapt the experiment ‘on-the-fly’. However, sequential designs also make building the actual experiment more complicated, because the experiment needs to be updated in almost real-time to provide a seamless experience. Further, in some settings, participants may notice that the experiment adapts to their actions, which may inadvertently alter their behavior. As we discuss below in the <italic>Caveats</italic> Section, the effects of model misspecification may be especially severe with sequential designs, contributing to our focus on static designs. For future applications, which approach to follow is likely going to be a choice that uniquely depends on the experiment and research questions at hand, while the overall workflow outlined in this tutorial will be relevant to both (<xref ref-type="box" rid="box3">Box 3</xref>).</p><boxed-text id="box3"><label>Box 3.</label><caption><title>Experimental settings in the case study.</title></caption><p>Behavioral experiments are often set up with several experimental blocks. We here make the common assumption of exchangeability with respect to the experimental blocks in our analyses and randomize block order in all experiments involving human participants. In our experiments, the bandit task in each experimental block has 3 choice options (bandit arms) and 30 trials (sequential choices). For each block of trials, the data <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> consists of 30 actions and 30 rewards, leading to a dimensionality of 60 per block. For a given block, the experimental design <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> we wish to optimize are the reward probabilities of the multi-armed bandit, which, in our setting, are three-dimensional (corresponding to independent scalar Bernoulli reward probabilities for each bandit arm). The generative process for models of human behavior in bandit tasks is illustrated in <xref ref-type="box" rid="box2">Box 2</xref>. We demonstrate the optimization of reward probabilities for multi-armed bandit tasks, with the scientific goals of <italic>model discrimination</italic> and <italic>parameter estimation</italic>. We consider five blocks of 30 trials per participant, yielding a fairly short but still informative experiment. We divide this into two blocks for model discrimination followed by three blocks for parameter estimation, where the parameter estimation blocks are conditional on the winning model for the participant in the first two blocks. In other words, for the parameter estimation task, a different experimental design (which corresponds to a particular setting of the reward probabilities associated with each bandit arm) is presented to a participant depending on the simulator model that best described them in the first two blocks. Since we consider three-armed bandits, the design space is constituted by the possible combinations of reward probabilities. This is six-dimensional for model discrimination and nine-dimensional for PE. Similarly, the data for a single block is 60-dimensional, so the dimensionality of the data is 120 for model discrimination and 180 for PE. Our method applies to any prior distribution over model parameters or the model indicator, but for demonstration purposes, we assume uninformative prior distributions on all model parameters (see Appendix 1 for details). As our baseline designs, we sample all reward probabilities from a <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>Beta</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> distribution, following a large and well-known experiment on bandit problems with 451 participants (<xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>). More information about the experimental settings and detailed descriptions of the algorithms can be found in the Appendix.</p></boxed-text></sec></sec><sec id="s11"><title>Step 4: Use machine learning to design the experiments</title><p>As previously explained, MI shown in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is an effective means of measuring the value (i.e. the <italic>utility</italic>) of an experimental design setting <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> in BOED (<xref ref-type="bibr" rid="bib54">Paninski, 2005</xref>; <xref ref-type="bibr" rid="bib65">Ryan et al., 2016</xref>). Unfortunately, it is generally an expensive, or intractable, quantity to compute, especially for simulator models. The reason for this is twofold: Firstly, the MI is defined via a high-dimensional integral. Secondly, the MI requires density evaluations of either the posterior distribution or the marginal likelihood, both of which are expensive to compute for general statistical models, and intractable for simulator models. Recent advances in BOED for simulator models thus advocate the use of cheaper MI lower bounds instead (e.g. <xref ref-type="bibr" rid="bib18">Foster et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Kleinegesse et al., 2020</xref>, <xref ref-type="bibr" rid="bib35">Kleinegesse and Gutmann, 2021</xref>; <xref ref-type="bibr" rid="bib28">Ivanova et al., 2021</xref>).</p><p>Many recent innovations in BOED rely on ML to accurately and efficiently estimate functions. ML is currently revolutionizing large parts of the natural sciences, with applications ranging from understanding the spread of viruses (<xref ref-type="bibr" rid="bib11">Currie et al., 2020</xref>) to discovering new molecules (<xref ref-type="bibr" rid="bib30">Jumper et al., 2021</xref>), forecasting climate change (<xref ref-type="bibr" rid="bib64">Runge et al., 2019</xref>), and developing new theories of human decision-making (<xref ref-type="bibr" rid="bib56">Peterson et al., 2021</xref>). Due to its versatility, ML can be integrated effectively into a wide range of scientific workflows, facilitating theory building, data modeling and analysis. In particular, ML allows us to automatically discover patterns in high-dimensional data sets. Many recent state-of-the-art methods in natural science, therefore, deeply integrate ML into their data analysis pipelines (e.g. <xref ref-type="bibr" rid="bib3">Blei and Smyth, 2017</xref>). However, there has been little focus on applying ML to improve the data collection process, which ultimately determines the quality of data, and thus the efficacy of downstream analyses and inferences. In this work, we use ML methods as part of our optimization scheme, where they serve as flexible function approximators.</p><p>The step of training a good ML model currently requires some previous expertise with (or willingness to learn about) applying ML methods. Fortunately, this step is less experiment-specific than it may first appear. Many different experiments generate similar data, and the architecture we provide can deal with multiple blocks and automatically learns summary statistics, as we explain above. Many excellent resources on the basics of ML exist, and a detailed treatment is beyond the scope of this tutorial. However, we provide pointers to several aspects in the code repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/simonvalentin/boed-tutorial">https://github.com/simonvalentin/boed-tutorial</ext-link> (copy archived at <xref ref-type="bibr" rid="bib82">Valentin, 2023</xref>).</p><sec id="s11-1"><title>Approximating mutual information with machine learning</title><p>In this work, we leverage the MINEBED method (<xref ref-type="bibr" rid="bib34">Kleinegesse et al., 2020</xref>), which allows us to efficiently estimate the MI using ML. Specifically, for a particular design <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, we first simulate data from the computational models under consideration, (shown in Box <xref ref-type="box" rid="box2">Box 2</xref>). The approach works by constructing a lower bound on the MI that is parameterized via an ML model. In our case study, we use a neural network with an architecture customized for behavioral experiments (shown in <xref ref-type="box" rid="box4">Box 4</xref>), but any other ML model would work in place. We then train the ML model by using the simulated data as input and the lower bound as an objective function, which gradually tightens the lower bound.</p><boxed-text id="box4"><label>Box 4.</label><caption><title>Bespoke neural network architecture for behavioral experiments.</title></caption><fig position="float" id="box4fig1"><label>Box 4—figure 1.</label><caption><title>Neural network architecture for behavioral experiments.</title><p>For each block of data we have a small sub-network (shown in blue) that outputs summary statistics S. These are concatenated with the variable of interest <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (e.g. corresponding to a model indicator for model discrimination tasks) and passed to a larger neural network (shown in green). <xref ref-type="fig" rid="box4fig1">Box 4—figure 1</xref> has been adapted from Figure 1 in <xref ref-type="bibr" rid="bib81">Valentin et al., 2021</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box4-fig1-v1.tif"/></fig></boxed-text><p>More concretely, this method works by training a neural network <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, or any other ML model, where <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are the neural network parameters and the data <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is simulated at design <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> with samples from the prior <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We train this neural network by maximizing the following objective function, which is a <italic>lower bound</italic> of the MI,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/></mml:math></disp-formula></p><p>The above lower bound is also known as the NWJ lower bound and has several appealing bias-variance properties (<xref ref-type="bibr" rid="bib58">Poole et al., 2019</xref>; we note that alternative bounds on MI can be used in the place of the NWJ bound as our loss function, as discussed by <xref ref-type="bibr" rid="bib35">Kleinegesse and Gutmann, 2021</xref>). The expectations in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> are usually approximated using (Monte-Carlo) sample-averages.</p><p>Once we have trained the neural network <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> on <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we estimate the MI at <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> by computing <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> using a held-out test set of data simulated from the computational model(s).</p></sec><sec id="s11-2"><title>Dealing with high-dimensional observations</title><p>We can efficiently deal with high-dimensional data and do not have to construct (approximately sufficient) summary statistics based on domain expertise, as is commonly done (<xref ref-type="bibr" rid="bib47">Myung and Pitt, 2009</xref>; <xref ref-type="bibr" rid="bib92">Zhang and Lee, 2010</xref>; <xref ref-type="bibr" rid="bib50">Ouyang et al., 2018</xref>), and which can be prohibitively difficult for complex simulator models (<xref ref-type="bibr" rid="bib7">Chen et al., 2021</xref>). In fact, the previously described method can allow us to automatically learn summary statistics as a by-product, as explained below.</p><p>Critical to the performance of our method in our case study, as with any application of neural networks, is the choice of architecture (see <xref ref-type="bibr" rid="bib16">Elsken et al., 2019</xref> and <xref ref-type="bibr" rid="bib60">Ren et al., 2021</xref> for recent reviews of the challenges and solutions in neural architecture search). In order to develop an effective neural network architecture, we leverage the method of neural approximate sufficient statistics (<xref ref-type="bibr" rid="bib7">Chen et al., 2021</xref>) and propose them as a novel and practical alternative to hand-crafted summary statistics in the context of BOED. Behavioral data are often collected in several blocks that can have different designs but should be analyzed jointly, which can be challenging due to the high dimensionality of the joint data, which typically comprises numerous choices, judgments or other measurements (including high-resolution and/or high-frequency neuroimaging data). In order to deal with a multi-block data structure effectively, we propose an architecture devised specifically for application to behavioral experiments. Our architecture, visualized in <xref ref-type="fig" rid="box4fig1">Box 4—figure 1</xref>, incorporates a sub-network for each block of an experiment. The outputs of these sub-networks are then concatenated and passed as an input to a larger neural network. In doing so, besides being able to reduce the dimensionality of the input data, each sub-network conveniently learns to approximate sufficient summary statistics of the data from each block in the experiment (<xref ref-type="bibr" rid="bib7">Chen et al., 2021</xref>).</p><p>In this work, we develop a bespoke feed-forward neural network architecture, which allows us to efficiently deal with multiple blocks of data. The code we are providing is straightforwardly adaptable to other problems ‘out of the box’, as the architecture can scale to multiple blocks of data and realistic complexity (dimensionality) of the observed data. However, the overall approach is not restricted to this particular ML method; in fact, any sufficiently flexible ML model may be used, as long as the loss function can be specified. In some settings, researchers may be interested in using ML models that only require little tuning, such as tree-based ensemble methods like Random Forests (<xref ref-type="bibr" rid="bib6">Breiman, 2001</xref>) or gradient boosted trees (<xref ref-type="bibr" rid="bib19">Friedman, 2001</xref>). On the other hand, when dealing with high-dimensional but structured data, such as in eye-tracking or neuroimaging (or combinations of different types of data, see <xref ref-type="bibr" rid="bib78">Turner et al., 2017</xref>), one may use other appropriate architectures, such as convolutional (<xref ref-type="bibr" rid="bib36">LeCun et al., 2015</xref>), recurrent (<xref ref-type="bibr" rid="bib63">Rumelhart et al., 1986</xref>), transformers, (<xref ref-type="bibr" rid="bib84">Vaswani et al., 2017</xref>), or graph neural networks (<xref ref-type="bibr" rid="bib93">Zhou et al., 2020</xref>). For further pointers to practical recommendations, see the code repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/simonvalentin/boed-tutorial">https://github.com/simonvalentin/boed-tutorial</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib82">Valentin, 2023</xref>).</p></sec><sec id="s11-3"><title>Search over the space of possible experimental designs</title><p>Once we have obtained an estimate of <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> at a candidate design <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, we need to update the design appropriately when searching the design space to solve the experimental design problem <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo>∗</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Unfortunately, in the setting of discrete observed data <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (such as choices between a set of options), gradients with respect to designs are generally unavailable, requiring the use of gradient-free optimization techniques. We here optimize the design <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> by means of Bayesian optimization (BO; see <xref ref-type="bibr" rid="bib70">Shahriari et al., 2015</xref>, for a review), which has been used effectively in the experimental design context before (e.g. <xref ref-type="bibr" rid="bib44">Martinez-Cantin, 2014</xref>; <xref ref-type="bibr" rid="bib34">Kleinegesse et al., 2020</xref>). We specifically use a Gaussian Process (GP) as our probabilistic surrogate model with a Matérn-5/2 kernel and Expected Improvement as the acquisition function (these are standard choices). A formal summary of our BOED approach is shown in Appendix 4.</p><p>For higher dimensional continuous design spaces, this search may become difficult and necessitate the use of more scalable BO variants (e.g. <xref ref-type="bibr" rid="bib51">Overstall and Woods, 2017</xref>; <xref ref-type="bibr" rid="bib49">Oh et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Eduardo and Gutmann, 2023</xref>). For discrete design spaces or combinations of discrete and continuous design spaces, one can straightforwardly parallelize the optimization for each level of the discrete variable. If the observed data are continuous, such as participants’ response times, and one is able to compute gradients with respect to the designs (i.e. the simulator is differentiable), the design space may also be explored with gradient-based optimization (<xref ref-type="bibr" rid="bib35">Kleinegesse and Gutmann, 2021</xref>). In some settings, we may have some non-trivial constraints on permissible experimental designs, and these can be enforced as part of the search, making sure that impermissible designs are rejected and/or not explored. Thus, if we want our designs to have certain properties, we need to encode these constraints. For example, in our bandit tasks, we could enforce a certain minimum degree of randomness associated with each bandit arm’s reward probability (although we do not do so in the present work), such that each reward probability is within in a specified range.</p></sec></sec><sec id="s12"><title>Step 5: Validate the optimal experimental design in silico</title><p>Once we have converged in our search over experimental designs, we can simulate the behavior of our theories under our optimal experimental design(s). Irrespective of the final experiment that will be run, model simulations can already provide useful information for theory building, for example by noticing that the simulator models generate implausible data for certain experimental designs. This means that we can use simulations to assess whether we can recover our models, or individual model parameters, given the experimental design, as we did in our simulation study. We highlight this again as a critical step, since issues at this stage warrant revising the models under consideration and, importantly, any corresponding real-world experiment would be expected to yield uninformative data and waste resources (<xref ref-type="bibr" rid="bib88">Wilson and Collins, 2019</xref>).</p><p>As we discuss in the <italic>Caveats</italic> Section, BOED may sometimes lead to unusual yet effective experimental designs, and we suggest that surprising optimal designs may deserve careful consideration, as they may reveal subtle differences in predicted behaviors or model misspecifications. One appeal of optimal designs is that some of the ways of obtaining information about which models is correct correspond to finding situations in which one of them makes extreme predictions. BOED can thereby be seen as facilitating good prior predictive checking. We have illustrated this step using our case study in <xref ref-type="box" rid="box5">Box 5</xref> and <xref ref-type="box" rid="box6">Box 6</xref>, where we present simulation study results for model discrimination and parameter estimation, respectively.</p><boxed-text id="box5"><label>Box 5.</label><caption><title>Model discrimination.</title></caption><p>Simulation study Our method reveals that the optimal reward probabilities for the model discrimination task are, approximately, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> for one block of trials and <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> for the other block. These optimal designs stand in stark contrast to the usual reward probabilities used in such behavioral experiments, which characteristically use less extreme values (<xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>). Contrary to our initial intuitions, and, presumably those of previous experiment designers, the extreme probabilities in these optimal designs are effective at distinguishing between particular theoretical commitments of our different models. For instance, the AEG model (see <xref ref-type="box" rid="box2">Box 2</xref>) can break ties between options with equivalent (observed) reward rates via the ‘stickiness’ parameter. To illustrate this effect, consider the bandit with <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> reward probabilities, as found above. Here, switching between the two options that always produce a reward is different from switching to the option that never produces a reward. Specifically, switching between the winning options can be seen as ‘anti-stickiness’, motivated for example, out of boredom or epistemic curiosity about subtle differences in the true reward rates of the winning options. This type of strategy is less apparent when the bandit arms are stochastic. As such, these mechanisms can be investigated most effectively by not introducing additional variability due to stochastic rewards and instead isolating their effects. Hence, we can interpret the, perhaps counter-intuitive, optimal experimental designs as effective choices for isolating distinctive mechanisms of our behavioral models or their parameters under the assumed priors.</p><p>Using the neural network that was trained at that optimal design, we can cheaply compute approximate posterior distributions of the model indicator (as described in Materials and methods). This yields the confusion matrices in <xref ref-type="fig" rid="box5fig1">Box 5—figure 1</xref>, which show that our optimal designs result in considerably better model recovery than the baseline designs.</p><fig position="float" id="box5fig1"><label>Box 5—figure 1.</label><caption><title>Simulation study results for the model discrimination task, showing the confusion matrices of the inferred behavioral models, for optimal (left) and baseline (right) designs.</title><p><xref ref-type="fig" rid="box5fig1">Box 5—figure 1</xref> has been adapted from Figure 2 in <xref ref-type="bibr" rid="bib81">Valentin et al., 2021</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box5-fig1-v1.tif"/></fig></boxed-text><boxed-text id="box6"><label>Box 6.</label><caption><title>Parameter estimation: Simulation study.</title></caption><p>We next discuss the parameter estimation results, focusing on the WSLTS model; the results for the AEG and GLS model can be found in Appendix 5. We find that the optimal reward probabilities for the WSLTS model are [0, 1, 0], [0, 1, 1] and [1, 0, 1] for the three blocks. Similar to the model discrimination task, these optimal designs take extreme values, unlike commonly used reward probabilities in the literature. In <xref ref-type="fig" rid="box6fig1">Box 6—figure 1</xref> we show posterior distributions of the WSLTS model parameters for optimal and baseline designs. We find that our optimal designs yield data that result in considerably improved parameter recovery, as compared to baseline designs.</p><fig position="float" id="box6fig1"><label>Box 6—figure 1.</label><caption><title>Simulation study results for the parameter estimation task of the WSLTS model, showing the marginal posterior distributions of the three WSLTS model parameters for optimal (green) and baseline (orange) designs, averaged over 1,000 simulated observations.</title><p>The ground-truth parameter values were chosen in accordance with previous work on simpler versions of the WSLTS model (<xref ref-type="bibr" rid="bib92">Zhang and Lee, 2010</xref>) and as plausible population values for the posterior reshaping parameter.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box6-fig1-v1.tif"/></fig></boxed-text></sec><sec id="s13"><title>Step 6: Run the real experiment</title><p>Once we are satisfied with the found optimal experimental design, we can run the actual experiment. Here, the ML models we used to approximate MI allow us to cheaply perform posterior inference. After running the real-world experiment, we can use our trained ML models to cheaply compute approximate posterior distributions over models and their parameters. This procedure provides a valuable addition to a Bayesian modeling workflow <xref ref-type="bibr" rid="bib66">Schad et al., 2021</xref>; <xref ref-type="bibr" rid="bib88">Wilson and Collins, 2019</xref>, as computing posterior distributions is a computationally expensive step, which is especially difficult for models with intractable likelihoods. We explain theoretical details below, and showcase an analysis of a real experiment with our case study in <xref ref-type="box" rid="box7">Box 7</xref>, <xref ref-type="box" rid="box8">Box 8</xref>, <xref ref-type="box" rid="box9">Box 9</xref>.</p><boxed-text id="box7"><label>Box 7.</label><caption><title>Human participant study: Methodology.</title></caption><p>To validate our method on empirical data, we collected a sample of <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>326</mml:mn></mml:mstyle></mml:math></inline-formula> participants from Prolific (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.com/">https://www.prolific.com/</ext-link>; all experiments were certified according to the University of Edinburgh Informatics Research Ethics Process, RT number 2019/58792). Participants were randomly allocated to our optimal designs or to one of ten baseline designs (the same as in the simulation study); we refer to the former as the <italic>optimal</italic> design group and to the latter as the <italic>baseline</italic> group. The first two blocks correspond to the model discrimination task, that is they were used to identify the model that matches a participant’s behavior best. This was done by computing the posterior distribution over the model indicator <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> using the trained neural networks (see Appendix 1) and then selecting the model with the highest posterior probability, that is the <italic>maximum</italic> a posteriori estimate. This inference process was done online without a noticeable interruption to the experiment. The following three blocks were then used for the parameter estimation task. Participants in the optimal design group were allocated the optimal design according to their inferred model (as provided by the model discrimination task). Participants in the baseline group were again allocated to baseline designs. This data collection process facilitates gathering real-world data for both the model discrimination and parameter estimation tasks, allowing us to effectively compare our optimal designs to baseline designs. See Appendix 2 for more details about the human participant study setup.</p><p>Here, we follow the rationale that more useful experimental designs are those that lead to more information (or lower posterior uncertainty) about the variable of interest. The belief about our variable of interest is provided by its posterior distribution, which we can estimate using the neural network output. We assess the quality of these posterior distributions by estimating their entropy, which quantifies the amount of <italic>information</italic> they encode (<xref ref-type="bibr" rid="bib71">Shannon, 1948</xref>). This is an effective and prominent metric to evaluate distributions, as it directly relates to the uncertainty about the variable of interest. For the model discrimination task, we specifically consider the Shannon entropy as a metric, since the model indicator <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> is a discrete random variable. As the continuous analogue of Shannon entropy, we use differential entropy for measuring the entropy of posterior distributions for the parameter estimation task, since the model parameters are continuous random variables. This allows us to quantitatively evaluate our optimal designs and compare their efficacy to that of baseline designs.</p></boxed-text><boxed-text id="box8"><label>Box 8.</label><caption><title>Human participant study: Results.</title></caption><p>We here briefly present the results of our human participant study. For the model discrimination tasks, we find that our optimal designs yield posterior distributions that tend to have lower Shannon entropy than the baseline designs, as shown in <xref ref-type="fig" rid="box8fig1">Box 8—figure 1</xref>. Our ML-based approach thus allows us to obtain more informative beliefs about which model matches real individual human behavior best, as compared to baselines.</p><p>Having determined the best model to explain a given participant’s behavior, we turn to the parameter estimation task, focusing our efforts on estimating the model parameters that explain their specific strategy. Similar to the model discrimination task, we here assess the quality of joint posterior distributions using entropy, where smaller values directly correspond to smaller uncertainty. <xref ref-type="fig" rid="box8fig2">Box 8—figure 2</xref> shows the distribution of differential posterior entropies across participants that were assigned to the AEG simulator model; we show corresponding plots for the WSLTS and GLS models in Appendix 5. Similar to the model discrimination task, we find that our optimal designs provide more informative data, that is they result in joint posterior distributions that tend to have lower differential entropy, as compared to the ones resulting from baseline designs, as shown in the figure.</p><p>Moreover, the proportions of participants allocated to the WSLTS, AEG, and GLS models for the optimal design were 62(37%), 75(45%) and 29(18%), respectively. For the baseline designs, the proportions were 57(36%), 22(14%) and 81(51%), for the WSLTS, AEG, and GLS models, respectively. The finding that the largest proportion of participants for the optimal designs were best described by the AEG model differs from prior work, where most participants were best described by Win-Stay Lose-Shift (a special case of our WSLTS model) <xref ref-type="bibr" rid="bib73">Steyvers et al., 2009</xref>. This suggests that a reinterpretation of previous results is required, where the human tendency to stick with previous choices is less about myopically repeating the last successful choice, and more about balancing reward maximization, exploration, and a drive to be consistent over recent runs of choices. Interestingly, we find different distributions over models for the optimal as compared to the baseline designs (A chi-square test revealed a significant difference between the distributions of WSLTS, AEG, and GLS in the Optimal and Baseline groups, <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>χ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>53.66</mml:mn></mml:mstyle></mml:math></inline-formula>, p &lt; 0.0001.). In particular, while, descriptively, the proportions of participants allocated to the WSLTS model are very close (37%, and 36% for the optimal and baseline designs, respectively), the proportions allocated to the AEG and GLS model differ more substantially. This may suggest that people’s strategy depends on the experimental design, potentially with some participants favoring strategies similar to the AEG model with more reliable reward environments, and the GLS in more unreliable (baseline) environments. The models we have considered in our case study do not account for the possibility of transferring knowledge across experimental blocks. However, learning about the reward probabilities in one (or several) experimental blocks may influence people’s expectations about the reward distribution of future block(s). For instance, having inferred (correctly or not), that at least one arm bandit arm always pays a reward, a learner would do well to quickly identify this arm in a future block (see, e.g. <xref ref-type="bibr" rid="bib85">Wang et al., 2018</xref>, for a related line of research). Our space of computational models does not include such strategy switching mechanisms, and we discuss the impact of such potential misspecifications below in more detail. While exploring these findings in detail from a cognitive science perspective is beyond the scope of the present work, we view this as a fruitful avenue for future work.</p><p>The extreme designs here may be surprising, considering the typically used stochastic rewards in bandit problems. However, these designs allow the experiment to focus on particular aspects of people’s behavior, which turns out to be most informative for distinguishing between the computational models we consider. For instance, while our AEG model performs uniformly random exploration, the WSLTS model performs uncertainty-guided exploration. Meanwhile, our optimal designs raise the question of ecological validity, as it is unclear which kinds of ‘reward environments’ are more reflective of real-world situations. On the one hand, research in causal reasoning suggests that people expect relationships to be deterministic and reliable (e.g. <xref ref-type="bibr" rid="bib68">Schulz and Sommerville, 2006</xref>). On the other hand, some environments are known to be random and more akin to a classic, stochastic bandit problem, such as the stock market. We view the question of the ecological validity of stochastic rewards as an important direction for future work, and discuss this more below in the <italic>Caveats</italic> Section.</p><fig position="float" id="box8fig1"><label>Box 8—figure 1.</label><caption><title>Human-participant study results for the model discrimination (MD) task, showing thedistribution of posterior Shannon entropies obtained for optimal (green) and baseline (orange)designs (lower is better).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box8-fig1-v1.tif"/></fig><fig position="float" id="box8fig2"><label>Box 8—figure 2.</label><caption><title>Human-participant study results for the parameter estimation (PE) task of the AEG model,showing the distribution of posterior differential entropies obtained for optimal (green) and baseline(orange) designs (lower is better).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box8-fig2-v1.tif"/></fig></boxed-text><sec id="s13-1"><title>Posterior estimation</title><p>Once the neural network is trained, we can conveniently compute an approximate posterior distribution with a single forward-pass (see <xref ref-type="bibr" rid="bib34">Kleinegesse et al., 2020</xref>, for a derivation and further explanations). The NWJ lower bound is tight when <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. By rearranging this, we can thus use our trained neural network <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to compute a (normalized) estimate of the posterior distribution, This can be done with the following equation,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Neural networks are notorious for large variations in performance, mainly due to the random initialization of network parameters. The quality of the posterior distribution estimated using <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> is therefore quite sensitive to the initialization of the neural network parameters. To overcome this limitation and to obtain a more robust estimate of the posterior distribution, we suggest to use <italic>ensemble learning</italic> to compute <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. This is done by training several neural networks (50 in our case study) and then averaging estimates of the posteriors obtained from each trained neural network.</p></sec><sec id="s13-2"><title>Summary statistics</title><p>Additionally, as another component of our approach, we also obtain automatically-learned (approximate) sufficient summary statistics for data observed with the optimal experimental designs. Summary statistics are often crafted manually by domain experts, which tends to be a time-consuming and difficult task. In fact, hand-crafted summary statistics are becoming less effective and informative as our models naturally become more complex as science advances (<xref ref-type="bibr" rid="bib7">Chen et al., 2021</xref>). Our learned summary statistics could be used for various downstream tasks, for example as an input for approximate inference techniques (such as ABC) or as means to interpret and analyze the data more effectively, for example as a lower dimensional representation used to visualize the data.</p><boxed-text id="box9"><label>Box 9.</label><caption><title>Exploring model parameter disentanglement.</title></caption><p>In addition to measuring the efficacy of our optimal experiments via the differential entropy of posterior distributions, we can ask how well they isolate, or <italic>disentangle</italic>, the effects of individual model parameters. Specifically, we measure this by means of the (Pearson) correlation between model parameters in the joint posterior distribution, which we present in <xref ref-type="fig" rid="box9fig1">Box 9—figure 1</xref> for all simulator models and for both optimal designs and baseline designs. Here, a correlation of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> means that there is no (linear) relationship between the respective pair of inferred model parameters across individuals, thereby providing evidence that the data under this design allow us to separate the natural mechanisms corresponding to those parameters. On the other hand, correlations closer to <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> would imply that the model parameters essentially encode the same mechanism. As shown descriptively in <xref ref-type="fig" rid="box9fig1">Box 9—figure 1</xref>, our optimal designs are indeed able to isolate the mechanisms of individual model parameters more effectively than baseline designs. Note that these are not confusion matrices, as displayed in <xref ref-type="fig" rid="box5fig1">Box 5—figure 1</xref>, but rather average correlation matrices of the model parameter posterior distributions.</p><p>To assess this observation quantitatively, we first <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>-transform (<xref ref-type="bibr" rid="bib17">Fisher, 1915</xref>) the correlation coefficients (i.e. all entries below the main diagonal in the correlation matrices) and compute the average absolute <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>-score over the model parameters within each participant. The resulting average of absolute <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>-scores is an expression for the average magnitude of linear dependency between the posterior model parameters for each participant. Next, we assess whether the average absolute <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi></mml:mstyle></mml:math></inline-formula>-scores are significantly different between those participants allocated to our optimal designs and those allocated to the baseline designs. We compute this statistical significance by conducting two-sided Welch’s <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>-tests (<xref ref-type="bibr" rid="bib87">Welch, 1947</xref>). We find that our optimal designs significantly outperform baseline designs for all three models, that is for the WSLTS model we find a p-value of p &lt; 0.001 (<inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>60.12</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>13.89</mml:mn></mml:mstyle></mml:math></inline-formula>), for the AEG model we have p = 0.004 (<inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>27.30</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3.12</mml:mn></mml:mstyle></mml:math></inline-formula>) and for the GLS model we have p &lt; 0.001 (<inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>91.01</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>8.44</mml:mn></mml:mstyle></mml:math></inline-formula>). This provides additional evidence that our optimal designs are better able to disentangle the effects of individual model parameters than baseline designs. As explained earlier, we argue that this isolation of effects is facilitated by the extreme values of our optimal designs.</p><fig position="float" id="box9fig1"><label>Box 9—figure 1.</label><caption><title>Human-participant study average linear correlations in the posterior distribution of model parameters for all models in the parameter estimation task, and for both optimal (top) and baseline (bottom) designs.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-box9-fig1-v1.tif"/></fig></boxed-text></sec><sec id="s13-3"><title>Model checking</title><p>As also emphasized by previous works on computational modeling, a model-based data analysis (such as via the amortized posterior distributions discussed above) should go hand-in-hand with model checking, also referred to as validation (e.g. <xref ref-type="bibr" rid="bib88">Wilson and Collins, 2019</xref>; <xref ref-type="bibr" rid="bib53">Palminteri et al., 2017</xref>). A detailed treatment of methods and techniques for model checking is beyond the scope of this paper. However, it should be noted that model checking is just as important with data collected through optimized experimental designs as through hand-crafted designs.</p></sec></sec><sec id="s14"><title>Caveats</title><p>We now turn to more high-level considerations and discuss several categories of potential issues and caveats related to BOED. We first discuss how the optimality of an experimental design is always defined with respect to a specific goal. Second, we consider how posterior inferences always depend on the experimental design and how this impacts BOED considerations. Third, we bring attention to the important issue of model misspecification in the context of BOED. Fourth, we discuss the topic of ecological validity and generalizability of our inferences with BOED. Finally, we cover the interpretability of our optimal designs. While we discuss these issues in light of BOED, they are just as important with hand-crafted experiments. Importantly, however, BOED makes these issues more salient, by turning the design of experiments into a more explicit process. Therefore, these points should be relevant to any researcher collecting or analyzing experimental data using computational models.</p><sec id="s14-1"><title>No single design to rule them all</title><p>The process of making all of our assumptions explicit and leaving the process of proposing concrete designs to the optimization procedure can provide us with assurance that our design is optimal with respect to our assumptions. Therefore, this process may also reduce bias in the design of experiments, as the optimization process can easily be replicated. We believe that these properties will help tackle the notorious <italic>replication crisis</italic> (<xref ref-type="bibr" rid="bib27">Ioannidis, 2005</xref>; <xref ref-type="bibr" rid="bib55">Pashler and Wagenmakers, 2012</xref>; <xref ref-type="bibr" rid="bib1">Baker, 2016</xref>) and potentially setting new standards.</p><p>However, it is important to point out that the designs found using our optimization procedure are not assured to be optimal for all scenarios. That is, there is no ‘free lunch’ (<xref ref-type="bibr" rid="bib89">Wolpert, 1996</xref>): No design suits all situations, as the usefulness of experimental designs depends on the theories under consideration, prior beliefs, and utility function. For instance, if we already have strong prior beliefs about a particular model parameter (such as people being strongly inclined to explore instead of exploit), there may be little to learn about this parameter and the BOED procedure may instead focus on parameters we have weaker prior beliefs about, which may yield different optimal designs. More generally, different scientific goals can lead to different optimal designs.</p><p>As an illustrative example, we consider the design of a survey item for political orientation. For simplicity, we assume a one-dimensional spectrum with extremes to either side of the scale being equally likely in the population, with most people falling in the middle of the scale. In one scenario the goal may be to estimate a given person’s political belief along the spectrum. Here, an informative question is likely going to be moderate, since extreme items on either side of the spectrum ignore the other side, have a low prior probability and are therefore less useful. However, if the goal is to distinguish ‘extremist’ participants on one side of the spectrum from the rest of the population, or if the scientific question implies that a participant can only belong to either side of the one-dimensional political spectrum then polarizing items are likely more informative than moderate items that do not reveal such extremist beliefs. This emphasizes the importance of clearly defining the scientific goal, making sure that the computational models reflect the theories under consideration, as described in Steps 1 and 2.</p></sec><sec id="s14-2"><title>Inferences are implicitly conditioned on the experimental design</title><p>An important fact that is easy to forget is that all parameter inferences, be they Bayesian or frequentist, are inherently conditioned on the experimental designs used to generate the observed data. That is, the inferred belief about some parameter <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula> is determined by <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and not <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is the design with which the data <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> was observed, even though this is usually not reflected in the notation explicitly, but rather kept implicit.</p><p>As we have seen in our case study, different experimental designs may lead to different posterior inferences. For example, for the task of model discrimination, we found different proportions of participants best explained by our optimal designs and our baseline designs. The question of when and why different experimental designs might support different models or theories is an important one that should concern anyone doing empirical research, but contrasting qualitatively different designs, as we do, makes this especially salient. We would expect the simplest and most compelling evidence for some design <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> being ‘better’ than another (<inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) to be posterior distributions that are in qualitative agreement having conducted experiments with both designs, but with lower posterior variances given design <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. This raises the question of what any differences in inferences may imply. In extreme cases, one might observe two different experiments leading to dramatically different conclusions—whatever the provenance of the two experimental designs. These differences may go so far as two experimental designs, <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> supporting different theories, such that <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> supports theory <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula>, but falsifies theory <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> supports theory <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi></mml:mstyle></mml:math></inline-formula>, but falsifies theory <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula>. Such cases might plausibly be the result of serious model misspecification, where no model under consideration gives a good account of human behavior across different designs. For example, in the present case study, if every model under consideration supposes that people use fixed strategies, but in reality people adapt to the structure or statistics of the specific task at hand, then different designs would support different models. It seems plausible that some participants in bandit tasks might perform such strategy selection (e.g. <xref ref-type="bibr" rid="bib38">Lieder and Griffiths, 2017</xref>): Having engaged with a block of deterministic bandit arms, some participants may approach the second block differently than if they had just engaged with a more stochastic design, or may even switch strategies within the same block. The problem of model misspecification is thus brought to light very clearly by BOED, as can be seen in the case study.</p><p>More generally, in cognitive science, we typically aspire to estimate parameters <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that describe some stable (with respect to time, but also context) cognitive attribute or process. If two designs do not support the same estimated parameters, then <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> likely does not describe the stable trait that we would like it to describe. Sometimes, this may suggest adopting a higher level model that decides when <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should have an effect (such as a strategy switching model, as discussed above), such that our new <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> describes a policy. Alternatively, we can explicitly limit the context (or domain) in which <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> should operate. For example, in the context of our case study, instead of generalizing our theory by proposing strategy-switching mechanisms, we may say that a theory on behavior in bandit problems should only cover stochastic reward environments, where no bandit arm has deterministic (0 or 1) rewards. These considerations depend very much on the concrete phenomena being studied and the theories used to describe them.</p></sec><sec id="s14-3"><title>Model misspecification and informativeness</title><p>Despite best efforts, computational models will suffer from some degree of model misspecification, failing to capture some aspects of people’s behavior. While the misspecification may only be subtle in some cases, it can also be more severe, which poses a very real and important problem for computational modeling. BOED takes the perspective that the scientific question and models should come first and thus puts the model more into central focus than traditional approaches, thereby providing a more formal way to analyze the impact of modeling choices on the utility of different designs and expected results. For instance, computational models may sometimes only provide reasonable predictions in limited regions of the space of possible experimental designs. We can inspect this in Step 5 of our proposed workflow and surprising predictions may motivate iterating on our theory/model building, or if we are committed to the model, can be exploited by BOED to perform powerful tests of competing theories.</p><p>While the issue of model misspecification is not specific to BOED, optimizing experimental designs based on misspecified models can have negative consequences. In particular, if the space of models is too narrow, our data may be less useful for later analyses with well-specified models. Two simple ways of lessening the impact of model misspecification are to enforce diversity constraints and to select qualitatively different high-utility designs to cover important behavioral phenomena (<xref ref-type="bibr" rid="bib53">Palminteri et al., 2017</xref>). As discussed, we can include constraints in our search over experimental designs, such that our designs are not too narrowly focused on a single region of the parameter space. We could, for example, enforce this on a block-level, to ensure that every participant is presented with a diverse set of experimental designs. Alternatively, we can perform a more manual selection and choose designs that have high utility, but are likely to elicit qualitatively different phenomena. We present exemplary ways of engaging with the estimated utility function in Appendix 6.</p></sec><sec id="s14-4"><title>Ecological validity and generalizability</title><p>We have seen that BOED may sometimes lead to extreme or atypical designs, as the present results illustrate, and in some cases these designs may be deemed undesirable. Similarly, we may sometimes want to use not only one experimental design for all units of observation, but for example, slightly vary the design across participants. As discussed by <xref ref-type="bibr" rid="bib90">Yarkoni, 2020</xref>, there is a common policy in experimental research of ignoring stimulus sampling variability, also referred to as the ‘fixed-effect fallacy’ <xref ref-type="bibr" rid="bib8">Clark, 1973</xref>. This policy can lead to problems with generalizing inferences to broader classes of stimuli, especially when the phenomenon under question is likely sensitive to contextual effects (<xref ref-type="bibr" rid="bib83">Van Bavel et al., 2016</xref>). Fortunately, the BOED workflow is inherently designed to handle such cases.</p><p>First, the optimization procedure over designs can include constraints (e.g. at most one deterministic bandit arm), which may be appropriate depending on the research question and can be viewed as inserting expert knowledge. Second, the approximation of the learned utility surface, allows for exploring the utility of different designs systematically. Rather than simply picking the design with maximal utility, practitioners can select from this surface to capture qualitatively different phenomena, or simply include slight variations of similar designs. For instance, researchers can pick the top-<inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> designs, or adopt a more sophisticated policy to ensure that the designs being used cover important parts of the design space to ensure robustness and generalizability of results, as argued for, for example, by <xref ref-type="bibr" rid="bib90">Yarkoni, 2020</xref>. If the goal is to provide more varied designs, instead of the present approach of optimizing the reward probabilities directly, it would be straightforward to parametrize the design space differently and optimize a design policy. For instance, in the context of our case study, we could optimize the parameters (<inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>) of a <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>Beta</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> distribution from which the reward probabilities for the bandit arms of a given block are drawn. This is clearly only one option, and different ways of parametrizing design policies are possible, as an alternative or complement to choosing designs from the estimated utility surface. The BOED procedure is agnostic to such choices, and decisions should be based on the concrete problem being studied. Researchers may here consider the trade-off between generalizabiltiy and variance control, which can be studied by simulating data from the model and performing parameter recovery analyses. We illustrate how the utility surface can be explored in Appendix 6, with code that shows how this can be accomplished in the related GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/simonvalentin/boed-tutorial">https://github.com/simonvalentin/boed-tutorial</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib82">Valentin, 2023</xref>).</p></sec><sec id="s14-5"><title>Interpretability</title><p>Simulating data from the computational models under consideration is of key importance in all stages of the experimental design process. In this work, we treat all models as simulators and, as argued, for example, by <xref ref-type="bibr" rid="bib53">Palminteri et al., 2017</xref>, simulated data are crucial in assessing and potentially falsifying specific model commitments. Prior predictive analyses are crucial for refining models and their priors, including detecting potential problems that may already be obvious from simulations alone, without having to run a real experiment. Beyond comparing models’ posterior probability, posterior predictive analyses can provide insights into where models are ‘right’ and where they may depart from human behavior, referring back to the previous theme of model misspecification. As proposed by <xref ref-type="bibr" rid="bib53">Palminteri et al., 2017</xref>, model comparison (through posterior probabilities) and falsification through simulation of behavioral effects thus serve complementary roles. Such simulations can serve as rigorous tests of theories, as a failure of a model to generate an effect of interest can be used as a criterion for rejecting the model (or a particular mechanism encoded in the model) in that form. These simulations should be considered a key part of the workflow presented in this work, beyond just inspecting posterior distributions (over models or their parameters).</p></sec></sec><sec id="s15"><title>Conclusion and outlook</title><p>This tutorial provides a flexible step-by-step workflow to finding optimal experimental designs, by leveraging recent advances in ML and BOED. As a first step, the proposed workflow suggests to define a scientific goal, such as model discrimination or parameter estimation, thereby expressing the value of a particular design as a measurable quantity using a utility function. Secondly, the scientific theories under investigation need to be cast as a computational model from which we can sample synthetic data. The methodologies discussed in this tutorial make no assumption, however, on whether or not the likelihood function of the computational model needs be tractable, which opens up the space of scientific theories that can be tested considerably. In the third step, the design optimization problem needs to be set up, which includes deciding which experimental design variables need to be optimized and whether there are any additional constraints that should be included. The fourth step of our proposed workflow then involves using ML to estimate and optimize the utility function. In doing so, we discussed a recent approach that leverages neural networks to learn a mapping between experimental designs and expected information gain, that is MI, and provided a novel extension that efficiently deals with behavioral data. In the fifth step, the discovered optimal designs are then validated in silico using synthetic data. This ensures that the optimal designs found are sound and that no experimental resources are wasted, possibly also if it is found that the computational models need to be revisited after this step. Lastly, in the sixth step, the real experiment can be performed using the discovered optimal design. Using the discussed methodologies, the trained neural networks can then conveniently be used to compute posterior distributions and summary statistics, immediately after the data collection.</p><p>This tutorial and the accompanying case study demonstrate the usefulness of modern ML and BOED methods for improving the way in which we design experiments and thus collect empirical data. Furthermore, the methodologies discussed in this tutorial optimize experimental designs for models of cognition without requiring computable likelihoods, or marginal likelihoods. This is critical to provide the methodological support required for scientific theories and models of human behavior that become increasingly realistic and complex. As part of our case study, using simulations and real-world data, we showed that the proposed workflow and the discussed methodologies yield optimal designs that outperform human-crafted designs found in the literature, offering more informative data for model discrimination as well as parameter estimation. We showed that adopting more powerful methodological tools allows us to study more realistic theories of human behavior, and our results provide empirical support for the expressiveness of the simulator models we studied in our case study. BOED is currently an active area of research and future work will likely make some steps easier and more computationally efficient, such as automatic tuning of the ML methods used to estimate MI, variations of the loss function, improved ways of searching over the design space, new ways of dealing with model misspecification or computationally efficient sequential BOED (<xref ref-type="bibr" rid="bib59">Rainforth et al., 2023</xref>). What will remain despite such technical advancements, however, is the need to formulate sound scientific questions and actively engaging with tools that help to automate designs. We thus believe that the steps and considerations outlined in the workflow presented in this work will stay relevant. More broadly, ML has seen success in behavioral research regarding the analysis of large datasets and discovering new theories with promising results (e.g. <xref ref-type="bibr" rid="bib56">Peterson et al., 2021</xref>) but its potential for data collection and in particular experimental design has been explored remarkably little. We view the present work as an important step in this direction.</p></sec></body><back><sec sec-type="additional-information" id="s16"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Supervision, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Supervision, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Supervision, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Fiona Knäussel for her help in creating the schematics used in this paper, Maximilian Harkotte for valuable feedback during preparation of the paper and three anonymous reviewers for their careful reading of our manuscript and their many insightful comments and suggestions. SV was supported by a Principal’s Career Development Scholarship, awarded by the University of Edinburgh. SK was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>1,500 scientists lift the lid on reproducibility</article-title><source>Nature</source><volume>533</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1038/533452a</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitzer</surname><given-names>S</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Perceptual decision making: drift-diffusion model is equivalent to a Bayesian model</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>102</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00102</pub-id><pub-id pub-id-type="pmid">24616689</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Smyth</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Science and data science</article-title><source>PNAS</source><volume>114</volume><fpage>8689</fpage><lpage>8692</lpage><pub-id pub-id-type="doi">10.1073/pnas.1702076114</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bramley</surname><given-names>NR</given-names></name><name><surname>Gerstenberg</surname><given-names>T</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Gureckis</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Intuitive experimentation in the physical world</article-title><source>Cognitive Psychology</source><volume>105</volume><fpage>9</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2018.05.001</pub-id><pub-id pub-id-type="pmid">29885534</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brehmer</surname><given-names>J</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name><name><surname>Pavez</surname><given-names>J</given-names></name><name><surname>Cranmer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mining gold from implicit models to improve likelihood-free inference</article-title><source>PNAS</source><volume>117</volume><fpage>5242</fpage><lpage>5249</lpage><pub-id pub-id-type="doi">10.1073/pnas.1915980117</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Random forests</article-title><source>Machine Learning</source><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural approximate sufficient statistics for implicit models</article-title><conf-name>In International Conference on Learning Representations (ICLR</conf-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>The language-as-fixed-effect fallacy: A critique of language statistics in psychological research</article-title><source>Journal of Verbal Learning and Verbal Behavior</source><volume>12</volume><fpage>335</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/S0022-5371(73)80014-3</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corander</surname><given-names>J</given-names></name><name><surname>Fraser</surname><given-names>C</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name><name><surname>Arnold</surname><given-names>B</given-names></name><name><surname>Hanage</surname><given-names>WP</given-names></name><name><surname>Bentley</surname><given-names>SD</given-names></name><name><surname>Lipsitch</surname><given-names>M</given-names></name><name><surname>Croucher</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Frequency-dependent selection in vaccine-associated pneumococcal population dynamics</article-title><source>Nature Ecology &amp; Evolution</source><volume>1</volume><fpage>1950</fpage><lpage>1960</lpage><pub-id pub-id-type="doi">10.1038/s41559-017-0337-x</pub-id><pub-id pub-id-type="pmid">29038424</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cranmer</surname><given-names>K</given-names></name><name><surname>Brehmer</surname><given-names>J</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The frontier of simulation-based inference</article-title><source>PNAS</source><volume>117</volume><fpage>30055</fpage><lpage>30062</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id><pub-id pub-id-type="pmid">32471948</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Currie</surname><given-names>CSM</given-names></name><name><surname>Fowler</surname><given-names>JW</given-names></name><name><surname>Kotiadis</surname><given-names>K</given-names></name><name><surname>Monks</surname><given-names>T</given-names></name><name><surname>Onggo</surname><given-names>BS</given-names></name><name><surname>Robertson</surname><given-names>DA</given-names></name><name><surname>Tako</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How simulation modelling can help reduce the impact of COVID-19</article-title><source>Journal of Simulation</source><volume>14</volume><fpage>83</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1080/17477778.2020.1751570</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Optimal experimental design for event-related fMRI</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:2/3&lt;109::AID-HBM7&gt;3.0.CO;2-W</pub-id><pub-id pub-id-type="pmid">10524601</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dezfouli</surname><given-names>A</given-names></name><name><surname>Griffiths</surname><given-names>K</given-names></name><name><surname>Ramos</surname><given-names>F</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Models that learn how humans learn: The case of decision-making and its disorders</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006903</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006903</pub-id><pub-id pub-id-type="pmid">31185008</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dziak</surname><given-names>JJ</given-names></name><name><surname>Coffman</surname><given-names>DL</given-names></name><name><surname>Lanza</surname><given-names>ST</given-names></name><name><surname>Li</surname><given-names>R</given-names></name><name><surname>Jermiin</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sensitivity and specificity of information criteria</article-title><source>Briefings in Bioinformatics</source><volume>21</volume><fpage>553</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1093/bib/bbz016</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eduardo</surname><given-names>A</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Bayesian optimization with informative covariance</article-title><source>Transactions on Machine Learning Research</source><fpage>1</fpage><lpage>37</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsken</surname><given-names>T</given-names></name><name><surname>Metzen</surname><given-names>JH</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural architecture search: A survey</article-title><source>Journal of Machine Learning Research</source><volume>20</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-05318-5</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1915">1915</year><article-title>Frequency distribution of the values of the correlation coeffients in samples from an indefinitely large popu;ation</article-title><source>Biometrika</source><volume>10</volume><fpage>507</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1093/biomet/10.4.507</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>A</given-names></name><name><surname>Jankowiak</surname><given-names>M</given-names></name><name><surname>Bingham</surname><given-names>E</given-names></name><name><surname>Horsfall</surname><given-names>P</given-names></name><name><surname>Teh</surname><given-names>YW</given-names></name><name><surname>Rainforth</surname><given-names>T</given-names></name><name><surname>Goodman</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Variational Bayesian optimal experimental design</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Greedy function approximation: A gradient boosting machine</article-title><source>The Annals of Statistics</source><volume>29</volume><elocation-id>1232</elocation-id><pub-id pub-id-type="doi">10.1214/aos/1013203451</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gebhardt</surname><given-names>C</given-names></name><name><surname>Oulasvirta</surname><given-names>A</given-names></name><name><surname>Hilliges</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hierarchical reinforcement learning explains task interleaving behavior</article-title><source>Computational Brain &amp; Behavior</source><volume>4</volume><fpage>284</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1007/s42113-020-00093-9</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deconstructing the human algorithms for exploration</article-title><source>Cognition</source><volume>173</volume><fpage>34</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2017.12.014</pub-id><pub-id pub-id-type="pmid">29289795</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Origin of perseveration in the trade-off between reward and complexity</article-title><source>Cognition</source><volume>204</volume><elocation-id>104394</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104394</pub-id><pub-id pub-id-type="pmid">32679270</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouriéroux</surname><given-names>C</given-names></name><name><surname>Phillips</surname><given-names>PCB</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Indirect inference for dynamic panel models</article-title><source>Journal of Econometrics</source><volume>157</volume><fpage>68</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.jeconom.2009.10.024</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Manifesto for a new (computational) cognitive revolution</article-title><source>Cognition</source><volume>135</volume><fpage>21</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2014.11.026</pub-id><pub-id pub-id-type="pmid">25497482</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guest</surname><given-names>O</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How computational modeling can force theory building in psychological science</article-title><source>Perspectives on Psychological Science</source><volume>16</volume><fpage>789</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1177/1745691620970585</pub-id><pub-id pub-id-type="pmid">33482070</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJ</given-names></name><name><surname>Pizzagalli</surname><given-names>DA</given-names></name><name><surname>Bogdan</surname><given-names>R</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mapping anhedonia onto reinforcement learning: a behavioural meta-analysis</article-title><source>Biology of Mood &amp; Anxiety Disorders</source><volume>3</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1186/2045-5380-3-12</pub-id><pub-id pub-id-type="pmid">23782813</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Why most published research findings are false</article-title><source>PLOS Medicine</source><volume>2</volume><elocation-id>e124</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id><pub-id pub-id-type="pmid">16060722</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ivanova</surname><given-names>DR</given-names></name><name><surname>Foster</surname><given-names>A</given-names></name><name><surname>Kleinegesse</surname><given-names>S</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name><name><surname>Rainforth</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods</article-title><conf-name>35th Conference on Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>M</given-names></name><name><surname>Dzhafarov</surname><given-names>EN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Unfalsifiability and mutual translatability of major modeling schemes for choice reaction time</article-title><source>Psychological Review</source><volume>121</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1037/a0034190</pub-id><pub-id pub-id-type="pmid">24079307</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Žídek</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Ballard</surname><given-names>AJ</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Nikolov</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>J</given-names></name><name><surname>Back</surname><given-names>T</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Reiman</surname><given-names>D</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Pacholska</surname><given-names>M</given-names></name><name><surname>Berghammer</surname><given-names>T</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Senior</surname><given-names>AW</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><pub-id pub-id-type="pmid">34265844</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kangasrääsiö</surname><given-names>A</given-names></name><name><surname>Jokinen</surname><given-names>JPP</given-names></name><name><surname>Oulasvirta</surname><given-names>A</given-names></name><name><surname>Howes</surname><given-names>A</given-names></name><name><surname>Kaski</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Parameter inference for computational cognitive models with approximate bayesian computation</article-title><source>Cognitive Science</source><volume>43</volume><elocation-id>e12738</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.12738</pub-id><pub-id pub-id-type="pmid">31204797</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karvelis</surname><given-names>P</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name><name><surname>Lawrie</surname><given-names>SM</given-names></name><name><surname>Seriès</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Autistic traits, but not schizotypy, predict increased weighting of sensory information in Bayesian visual integration</article-title><source>eLife</source><volume>7</volume><elocation-id>e34115</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34115</pub-id><pub-id pub-id-type="pmid">29757142</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kleinegesse</surname><given-names>S</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Efficient Bayesian experimental design for implicit models</article-title><conf-name>In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</conf-name></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kleinegesse</surname><given-names>S</given-names></name><name><surname>Drovandi</surname><given-names>C</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sequential bayesian experimental design for implicit models via mutual information</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.09379">https://arxiv.org/abs/2003.09379</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kleinegesse</surname><given-names>S</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Gradient-based Bayesian experimental design for implicit models using mutual information lower bounds</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2105.04379">https://arxiv.org/abs/2105.04379</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>MD</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Munro</surname><given-names>M</given-names></name><name><surname>Steyvers</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Psychological models of human and optimal performance in bandit problems</article-title><source>Cognitive Systems Research</source><volume>12</volume><fpage>164</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.cogsys.2010.07.007</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Strategy selection as rational metareasoning</article-title><source>Psychological Review</source><volume>124</volume><fpage>762</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1037/rev0000075</pub-id><pub-id pub-id-type="pmid">29106268</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liepe</surname><given-names>J</given-names></name><name><surname>Filippi</surname><given-names>S</given-names></name><name><surname>Komorowski</surname><given-names>M</given-names></name><name><surname>Stumpf</surname><given-names>MPH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Maximizing the information content of experiments in systems biology</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002888</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002888</pub-id><pub-id pub-id-type="pmid">23382663</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindley</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>On a Measure of the Information Provided by an Experiment</article-title><source>The Annals of Mathematical Statistics</source><volume>27</volume><fpage>986</fpage><lpage>1005</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177728069</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lintusaari</surname><given-names>J</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name><name><surname>Dutta</surname><given-names>R</given-names></name><name><surname>Kaski</surname><given-names>S</given-names></name><name><surname>Corander</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fundamentals and recent developments in approximate bayesian computation</article-title><source>SYSTEMATIC BIOLOGY</source><volume>66</volume><fpage>e66</fpage><lpage>e82</lpage><pub-id pub-id-type="doi">10.1093/sysbio/syw077</pub-id><pub-id pub-id-type="pmid">28175922</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenz</surname><given-names>R</given-names></name><name><surname>Monti</surname><given-names>RP</given-names></name><name><surname>Violante</surname><given-names>IR</given-names></name><name><surname>Anagnostopoulos</surname><given-names>C</given-names></name><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Montana</surname><given-names>G</given-names></name><name><surname>Leech</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Automatic Neuroscientist: A framework for optimizing experimental design with closed-loop real-time fMRI</article-title><source>NeuroImage</source><volume>129</volume><fpage>320</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.01.032</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marjoram</surname><given-names>P</given-names></name><name><surname>Molitor</surname><given-names>J</given-names></name><name><surname>Plagnol</surname><given-names>V</given-names></name><name><surname>Tavaré</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Markov chain Monte Carlo without likelihoods</article-title><source>PNAS</source><volume>100</volume><fpage>15324</fpage><lpage>15328</lpage><pub-id pub-id-type="doi">10.1073/pnas.0306899100</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Cantin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesopt: a bayesian optimization library for nonlinear optimization, experimental design and bandits</article-title><source>Journal of Machine Learning Research: JMLR</source><volume>15</volume><fpage>3735</fpage><lpage>3739</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mikkola</surname><given-names>P</given-names></name><name><surname>Martin</surname><given-names>OA</given-names></name><name><surname>Chandramouli</surname><given-names>S</given-names></name><name><surname>Hartmann</surname><given-names>M</given-names></name><name><surname>Pla</surname><given-names>OA</given-names></name><name><surname>Thomas</surname><given-names>O</given-names></name><name><surname>Pesonen</surname><given-names>H</given-names></name><name><surname>Corander</surname><given-names>J</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Kaski</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Prior Knowledge Elicitation: The Past, Present, and Future</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2112.01380">https://arxiv.org/abs/2112.01380</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Simulation-based optimal design</article-title><source>Bayesian Statistics</source><volume>6</volume><fpage>459</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1093/oso/9780198504856.001.0001</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myung</surname><given-names>JI</given-names></name><name><surname>Pitt</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Optimal experimental design for model discrimination</article-title><source>Psychological Review</source><volume>116</volume><fpage>499</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1037/a0016104</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myung</surname><given-names>JI</given-names></name><name><surname>Cavagnaro</surname><given-names>DR</given-names></name><name><surname>Pitt</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A tutorial on adaptive design optimization</article-title><source>Journal of Mathematical Psychology</source><volume>57</volume><fpage>53</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2013.05.005</pub-id><pub-id pub-id-type="pmid">23997275</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>C</given-names></name><name><surname>Gavves</surname><given-names>E</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>BOCK: Bayesian optimization with cylindrical kernels</article-title><conf-name>In Proceedings of the 35th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>L</given-names></name><name><surname>Tessler</surname><given-names>MH</given-names></name><name><surname>Ly</surname><given-names>D</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>webppl-oed: A practical optimal experiment design system</article-title><conf-name>In Proceedings of the annual meeting of the cognitive science society</conf-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overstall</surname><given-names>AM</given-names></name><name><surname>Woods</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bayesian design of experiments using approximate coordinate exchange</article-title><source>Technometrics</source><volume>59</volume><fpage>458</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1080/00401706.2016.1251495</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Palestro</surname><given-names>JJ</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Osth</surname><given-names>AF</given-names></name><name><surname>Van Zandt</surname><given-names>T</given-names></name><name><surname>Turner</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Likelihood-Free Methods for Cognitive Science</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-72425-6</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Importance of falsification in computational cognitive modeling</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Asymptotic theory of information-theoretic experimental design</article-title><source>Neural Computation</source><volume>17</volume><fpage>1480</fpage><lpage>1507</lpage><pub-id pub-id-type="doi">10.1162/0899766053723032</pub-id><pub-id pub-id-type="pmid">15901405</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashler</surname><given-names>H</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Editors’ introduction to the special section on replicability in psychological science: a crisis of confidence?</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>528</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1177/1745691612465253</pub-id><pub-id pub-id-type="pmid">26168108</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>JC</given-names></name><name><surname>Bourgin</surname><given-names>DD</given-names></name><name><surname>Agrawal</surname><given-names>M</given-names></name><name><surname>Reichman</surname><given-names>D</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Using large-scale experiments and machine learning to discover theories of human decision-making</article-title><source>Science</source><volume>372</volume><fpage>1209</fpage><lpage>1214</lpage><pub-id pub-id-type="doi">10.1126/science.abe2629</pub-id><pub-id pub-id-type="pmid">34112693</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitt</surname><given-names>MA</given-names></name><name><surname>Myung</surname><given-names>IJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>When a good fit can be bad</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>421</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)01964-2</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Ozair</surname><given-names>S</given-names></name><name><surname>Den Oord</surname><given-names>A</given-names></name><name><surname>Alemi</surname><given-names>A</given-names></name><name><surname>Tucker</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>On variational bounds of mutual information</article-title><conf-name>In Proceedings of the 36th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rainforth</surname><given-names>T</given-names></name><name><surname>Foster</surname><given-names>A</given-names></name><name><surname>Ivanova</surname><given-names>DR</given-names></name><name><surname>Smith</surname><given-names>FB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Modern bayesian experimental design</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2302.14545">https://arxiv.org/abs/2302.14545</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>P</given-names></name><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Chang</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A comprehensive survey of neural architecture search: Challenges and solutions</article-title><source>ACM Computing Surveys</source><volume>54</volume><elocation-id>3447582</elocation-id><pub-id pub-id-type="doi">10.1145/3447582</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robbins</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>Some aspects of the sequential design of experiments</article-title><source>Bulletin of the American Mathematical Society</source><volume>58</volume><fpage>527</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1090/S0002-9904-1952-09620-8</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>RJH</given-names></name><name><surname>Baker</surname><given-names>RE</given-names></name><name><surname>Parker</surname><given-names>A</given-names></name><name><surname>Ford</surname><given-names>MJ</given-names></name><name><surname>Mort</surname><given-names>RL</given-names></name><name><surname>Yates</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Using approximate Bayesian computation to quantify cell-cell adhesion parameters in a cell migratory process</article-title><source>NPJ Systems Biology and Applications</source><volume>3</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1038/s41540-017-0010-7</pub-id><pub-id pub-id-type="pmid">28649436</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runge</surname><given-names>J</given-names></name><name><surname>Bathiany</surname><given-names>S</given-names></name><name><surname>Bollt</surname><given-names>E</given-names></name><name><surname>Camps-Valls</surname><given-names>G</given-names></name><name><surname>Coumou</surname><given-names>D</given-names></name><name><surname>Deyle</surname><given-names>E</given-names></name><name><surname>Glymour</surname><given-names>C</given-names></name><name><surname>Kretschmer</surname><given-names>M</given-names></name><name><surname>Mahecha</surname><given-names>MD</given-names></name><name><surname>Muñoz-Marí</surname><given-names>J</given-names></name><name><surname>van Nes</surname><given-names>EH</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Quax</surname><given-names>R</given-names></name><name><surname>Reichstein</surname><given-names>M</given-names></name><name><surname>Scheffer</surname><given-names>M</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Spirtes</surname><given-names>P</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Zscheischler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Inferring causation from time series in Earth system sciences</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2553</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10105-3</pub-id><pub-id pub-id-type="pmid">31201306</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>EG</given-names></name><name><surname>Drovandi</surname><given-names>CC</given-names></name><name><surname>McGree</surname><given-names>JM</given-names></name><name><surname>Pettitt</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A review of modern computational algorithms for bayesian optimal design</article-title><source>International Statistical Review</source><volume>84</volume><fpage>128</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1111/insr.12107</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schad</surname><given-names>DJ</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Vasishth</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Toward a principled Bayesian workflow in cognitive science</article-title><source>Psychological Methods</source><volume>26</volume><fpage>103</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1037/met0000275</pub-id><pub-id pub-id-type="pmid">32551748</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schafer</surname><given-names>CM</given-names></name><name><surname>Freeman</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Likelihood-free inference in cosmology: potential for the estimation of luminosity functions</chapter-title><person-group person-group-type="editor"><name><surname>Feigelson</surname><given-names>E</given-names></name><name><surname>Babu</surname><given-names>G</given-names></name></person-group><source>Statistical Challenges in Modern Astronomy V</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name><fpage>3</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-3520-4</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>LE</given-names></name><name><surname>Sommerville</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>God does not play dice: causal determinism and preschoolers’ causal inferences</article-title><source>Child Development</source><volume>77</volume><fpage>427</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8624.2006.00880.x</pub-id><pub-id pub-id-type="pmid">16611182</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Franklin</surname><given-names>NT</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Finding structure in multi-armed bandits</article-title><source>Cognitive Psychology</source><volume>119</volume><elocation-id>101261</elocation-id><pub-id pub-id-type="doi">10.1016/j.cogpsych.2019.101261</pub-id><pub-id pub-id-type="pmid">32059133</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahriari</surname><given-names>B</given-names></name><name><surname>Swersky</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>de Freitas</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Taking the human out of the loop: a review of bayesian optimization</article-title><source>Proceedings of the IEEE</source><volume>104</volume><fpage>148</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2015.2494218</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A mathematical theory of communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sisson</surname><given-names>SA</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name><name><surname>Tanaka</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sequential monte carlo without likelihoods</article-title><source>PNAS</source><volume>104</volume><fpage>1760</fpage><lpage>1765</lpage><pub-id pub-id-type="doi">10.1073/pnas.0607208104</pub-id><pub-id pub-id-type="pmid">17264216</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steyvers</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>MD</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A Bayesian analysis of human decision-making on bandit problems</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>168</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2008.11.002</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="1933">1933</year><article-title>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</article-title><source>Biometrika</source><volume>25</volume><fpage>285</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1093/biomet/25.3-4.285</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>BM</given-names></name><name><surname>Van Zandt</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A tutorial on approximate Bayesian computation</article-title><source>Journal of Mathematical Psychology</source><volume>56</volume><fpage>69</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2012.02.005</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>BM</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A generalized, likelihood-free method for posterior estimation</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>227</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0530-0</pub-id><pub-id pub-id-type="pmid">24258272</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>BM</given-names></name><name><surname>Forstmann</surname><given-names>BU</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Palmeri</surname><given-names>TJ</given-names></name><name><surname>Van Maanen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Approaches to analysis in model-based cognitive neuroscience</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>65</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.01.001</pub-id><pub-id pub-id-type="pmid">31745373</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>BM</given-names></name><name><surname>Schley</surname><given-names>DR</given-names></name><name><surname>Muller</surname><given-names>C</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Competing theories of multialternative, multiattribute preferential choice</article-title><source>Psychological Review</source><volume>125</volume><fpage>329</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1037/rev0000089</pub-id><pub-id pub-id-type="pmid">29265855</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>TD</given-names></name><name><surname>Stuhlmüller</surname><given-names>A</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning physical parameters from dynamic scenes</article-title><source>Cognitive Psychology</source><volume>104</volume><fpage>57</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2017.05.006</pub-id><pub-id pub-id-type="pmid">29653395</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Valentin</surname><given-names>S</given-names></name><name><surname>Kleinegesse</surname><given-names>S</given-names></name><name><surname>Bramley</surname><given-names>NR</given-names></name><name><surname>Gutmann</surname><given-names>MU</given-names></name><name><surname>Lucas</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bayesian optimal experimental design for simulator models of cognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.15632">https://arxiv.org/abs/2110.15632</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Valentin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Boed-Tutorial</data-title><version designator="swh:1:rev:94f32a04693763ec6af5d31247caf129bfe99142">swh:1:rev:94f32a04693763ec6af5d31247caf129bfe99142</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:18d09063fd478a0d6b69251334b52ffe9c7f66d8;origin=https://github.com/simonvalentin/boed-tutorial;visit=swh:1:snp:f53dfc8f8da5a38fe0e90465366f408bb1489f01;anchor=swh:1:rev:94f32a04693763ec6af5d31247caf129bfe99142">https://archive.softwareheritage.org/swh:1:dir:18d09063fd478a0d6b69251334b52ffe9c7f66d8;origin=https://github.com/simonvalentin/boed-tutorial;visit=swh:1:snp:f53dfc8f8da5a38fe0e90465366f408bb1489f01;anchor=swh:1:rev:94f32a04693763ec6af5d31247caf129bfe99142</ext-link></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Bavel</surname><given-names>JJ</given-names></name><name><surname>Mende-Siedlecki</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>WJ</given-names></name><name><surname>Reinero</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Contextual sensitivity in scientific reproducibility</article-title><source>PNAS</source><volume>113</volume><fpage>6454</fpage><lpage>6459</lpage><pub-id pub-id-type="doi">10.1073/pnas.1521897113</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention is all you need</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Tirumala</surname><given-names>D</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>860</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0147-8</pub-id><pub-id pub-id-type="pmid">29760527</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>DJ</given-names></name><name><surname>Kingsbury</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Application of computerized adaptive testing to educational problems</article-title><source>Journal of Educational Measurement</source><volume>21</volume><fpage>361</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1111/j.1745-3984.1984.tb01040.x</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welch</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1947">1947</year><article-title>The generalisation of student’s problems when several different population variances are involved</article-title><source>Biometrika</source><volume>34</volume><fpage>28</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1093/biomet/34.1-2.28</pub-id><pub-id pub-id-type="pmid">20287819</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id><pub-id pub-id-type="pmid">31769410</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The lack of a priori distinctions between learning algorithms</article-title><source>Neural Computation</source><volume>8</volume><fpage>1341</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.7.1341</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The generalizability crisis</article-title><source>The Behavioral and Brain Sciences</source><volume>45</volume><elocation-id>e1</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X20001685</pub-id><pub-id pub-id-type="pmid">33342451</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>MD</given-names></name><name><surname>Munro</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Human and optimal exploration and exploitation in bandit problems</article-title><source>Ratio</source><volume>13</volume><elocation-id>14</elocation-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimal experimental design for a class of bandit problems</article-title><source>Journal of Mathematical Psychology</source><volume>54</volume><fpage>499</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2010.08.002</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Cui</surname><given-names>G</given-names></name><name><surname>Hu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Graph neural networks: A review of methods and applications</article-title><source>AI Open</source><volume>1</volume><fpage>57</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s17"><title>Experiments</title><sec sec-type="appendix" id="s17-1"><title>Priors</title><p>We use a uniform categorical prior over the model indicator <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>, that is <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We generally use uninformative priors <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for all model parameters, except for the temperature parameter of the WSLTS model that has a <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>LogNorm</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> prior, as it acts as an exponent in reshaping the posterior. We generate 50,000 samples from the prior and then simulate corresponding synthetic data <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at every design <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s17-2"><title>Sub-networks</title><p>For all our experiments, we use sub-networks <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> that consist of two hidden layers with 64 and 32 hidden units, respectively, and ReLU activation functions. The number of sufficient statistics we wish to learn for each block of behavioral data is given by the number of dimensions in the output layer of the sub-networks. These are 6, 8, 6 and 8 units for the model discrimination, parameter estimation (WSLTS), parameter estimation (AEG) and parameter estimation (GLS) experiments, respectively. The flexibility of a sub-network is naturally improved when increasing the number of desired summary statistics, but the computational cost increases accordingly. When the number of summary statistics is too low, the summary statistics we learn may not be sufficient. We have found the above numbers of summary statistics to be effective middle-grounds and refer to <xref ref-type="bibr" rid="bib7">Chen et al., 2021</xref> for more detailed guidance on how to select the number of summary statistics.</p></sec><sec sec-type="appendix" id="s17-3"><title>Main network</title><p>The main network <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> consists of the concatenated outputs of the sub-networks for each block of behavioral data and the variable of interest. This is then followed by two fully-connected layers with ReLU activation functions. For the model discrimination experiment, we use 32 hidden units for the two hidden layers, while we use 64 and 32 hidden units for the parameter estimation experiments. See <xref ref-type="fig" rid="box4fig1">Box 4—figure 1</xref> for a visualization of this bespoke neural network architecture.</p></sec><sec sec-type="appendix" id="s17-4"><title>Training</title><p>We use the Adam optimizer to maximize the lower bound shown in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, with a learning rate of 10<sup>-3</sup> and a weight decay of 10<sup>-3</sup> (except for the parameter estimation (WSLTS) experiments where we use a weight decay of 10<sup>-4</sup>). We additionally use a plateau learning rate scheduler with a decay factor of 05 and a patience of 25 epochs. We train the neural network for 200, 400, 300 and 300 epochs for the model discrimination, parameter estimation (WSLTS), parameter estimation (AEG) and parameter estimation (GLS) experiments, respectively. At every design we simulate 50,000 samples from the data-generating distribution (one for every prior sample) and randomly hold out 10,000 of those as a validation set, which are then used to compute an estimate of the mutual information via <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. During the BO procedure we select an experimental budget of 400 evaluations (80 of which were initial evaluations), which is more than double needed to converge.</p></sec></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s18"><title>Human participant study</title><sec sec-type="appendix" id="s18-1"><title>Task</title><p>Participants completed the multi-armed bandit tasks in online experiments. After going through instructions on the interface and setup of the task, participants were required to pass a series of five comprehension questions (in a true-false format). If any of the comprehension questions were answered incorrectly, the participant was sent back to the instructions and could only progress to the task once they answered all questions correctly. An example screen-shot of the task interface can be found in Appendix 5.</p></sec><sec sec-type="appendix" id="s18-2"><title>Two-phase design</title><p>To allocate participants to the parameter estimation designs for the respective model that best matched their behavior in the model discrimination blocks, we implemented a simple API that uses the ensemble of trained neural networks and performs approximate MAP inference over the model indicator (see the previous sub-section on posterior estimation). As we have obtained amortized posterior distributions as by-products in the BOED procedure, this inference can be done efficiently: Forward-passes through the neural networks are computationally cheap, only taking a fraction of a second, which means that there was no noticeable delay for the allocation of the optimal model for each participant.</p></sec><sec sec-type="appendix" id="s18-3"><title>Participants</title><p><inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>326</mml:mn></mml:mstyle></mml:math></inline-formula> adults (154 female, mean age <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>35.61</mml:mn></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>12.59</mml:mn></mml:mstyle></mml:math></inline-formula>) participated in the experiment in return for a basic payment of £0.60 and performance related bonuses of up to £1.00. Participants took, on average, 8.8 (<inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2.89</mml:mn></mml:mstyle></mml:math></inline-formula>) minutes to complete the task.</p></sec></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s19"><title>Computational models</title><p>We here provide further details about the computational models of human behavior in bandit tasks, which we briefly presented in the main text. We consider the multi-armed bandit setting where, at each trial <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>, participants have to make an action <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>, which consists of choosing any of the <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi></mml:mstyle></mml:math></inline-formula> bandit arms, and subsequently observe a reward <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. After a participant has gone through all <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> trials, we summarize their behavior by a set of actions <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and observed rewards <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Whether a participant observes a reward of 0 or 1 when making a particular choice <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> depends on the specified reward probability. We here assume that each bandit arm <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> is associated with a Bernoulli reward distribution. Each of these reward distributions has a (potentially) different reward probability, which is given by the corresponding entry in the design vector <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. In other words, the reward <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> a participant receives when making a particular choice <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> is sampled via <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>-th element of <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Computational models of human behavior in bandit tasks only differ in how they model the choices of a participant depending on the previous actions and rewards. To help us describe the mechanisms of such computational models, we here define the vectors <inline-formula><mml:math id="inf122"><mml:mi mathvariant="bold-italic">α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:mi mathvariant="bold-italic">β</mml:mi></mml:math></inline-formula>, which store the number of observed 0 and 1 rewards for all <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi></mml:mstyle></mml:math></inline-formula> arms. That is, <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> refers to the number of times the <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>-th arm was selected and subsequently generated a reward. Similarly, <inline-formula><mml:math id="inf127"><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> refers to the number of times a participant did not observe a reward when selecting the <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>-th bandit arm. Below, we describe each of the computational models in more detail and include corresponding pseudo-code.</p></sec><sec sec-type="appendix" id="s20"><title>Win-Stay Lose-Thompson-Sample (WSLTS)</title><p>Here, we propose Win-Stay Lose-Thompson-Sample (WSLTS) as an amalgamation of Win-Stay Lose-Shift (WSLS; <xref ref-type="bibr" rid="bib61">Robbins, 1952</xref>) and Thompson Sampling (<xref ref-type="bibr" rid="bib75">Thompson, 1933</xref>). WSLTS has three model parameters <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> that regulate different aspects of exploration and exploitation behavior. Specifically, <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> corresponds to the probability of staying after winning, that is the probability of re-selecting the previous arm <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> after having observed <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. However, with probability <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> the agent may decide to choose a different bandit arm even after having observed a reward of 1 at the previous trial. In this case, the agent performs Thompson Sampling, using a temperature parameter <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, to decide which bandit arm to select (see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). Conversely, <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> corresponds to the probability of switching to another arm when observing a loss in the previous trial, that is when <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>, in which case the agent again performs Thompson Sampling to select another bandit arm. However, the agent may also re-select the previous bandit arm with probability <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> even after having observed a loss.</p><p>During the exploration phases mentioned above, the WSLTS agent performs Thompson Sampling from a reshaped posterior, which is controlled via a temperature parameter <inline-formula><mml:math id="inf138"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. The corresponding choice of bandit arm is then given by<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow class="MJX-TeXAtom-OP"><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ω</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>ω</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>Beta</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mtext/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mtext/></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> correspond to the number of times a participant observed a reward of 1 and 0, respectively, for a bandit arm <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>. This stands in contrast to standard WSLS, where the agent shifts to another arm uniformly at random. We note that, for <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> values close to 1 we recover standard Thompson-Sampling (excluding the previous arm), while for <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mstyle></mml:math></inline-formula> we recover standard WSLS, and for <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> we obtain a greedy policy. We provide pseudo-code for the WSLTS computational model in Algorithm 1.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups" id="AL1"><thead><tr><th align="left" valign="bottom">Algorithm 1 Win-Stay Lose-Thompson-Sample (WSLTS)</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Input</bold>: Parameter <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, design <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><break/><bold>Output</bold>: Actions <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, rewards <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>1: Initialize pseudo-counts <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to 1 for all bandit arms <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>.<break/>2: <bold>for</bold> t = 1, …, T <bold>do</bold><break/>3:   Sample <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>4:   <bold>if</bold> t=1 <bold>then</bold><break/>5:    Select the first bandit arm <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> uniformly at random.<break/>6:   <bold>else if</bold> <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>7:    <bold>if</bold> <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>8:     Re-select the previous bandit arm <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.<break/>9:    <bold>else</bold><break/>10:     Thomson-Sample according to <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>.<break/>11:   <bold>else</bold><break/>12:    <bold>if</bold> <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>13:     Thomson-Sample according to <xref ref-type="disp-formula" rid="equ4">equation 4</xref>.<break/>14:    <bold>else</bold><break/>15:     Re-select the previous previous bandit arm <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.<break/>16:  Sample the reward <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>17:  Increment <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s21"><title>Autoregressive <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy (AEG)</title><p>Standard <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy (e.g. <xref ref-type="bibr" rid="bib74">Sutton and Barto, 2018</xref>) is a ubiquitous method in reinforcement learning, where the agent selects the arm with the highest expected reward with probability <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula> is a model parameter. Conversely, with probability <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>, the agent performs exploration by uniformly selecting a bandit arm. Here, we propose Auto-regressive <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy (AEG) as a generalization of <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy, which allows for modeling people’s tendency towards auto-regressive behavior (e.g. <xref ref-type="bibr" rid="bib22">Gershman, 2020</xref>).</p><p>Specifically, the AEG model has two model parameters <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> corresponds to the same <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula> parameter as in <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy. However, as opposed to randomly selecting any bandit arm, the probability of selecting the previous arm, in order to break ties between options with the same expected reward, is specifically controlled via the second model parameter <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. We provide pseudo-code for the AEG computational model in Algorithm 2.</p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups" id="AL2"><thead><tr><th align="left" valign="bottom">Algorithm 2 Autoregressive <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-Greedy (AEG)</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Input</bold>: Parameter <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, design <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><break/><bold>Output</bold>: Actions <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, rewards <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>1: Initialize pseudo-counts <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to 1 for all bandit arms <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>.<break/>2: <bold>for</bold> t=1, …, T <bold>do</bold><break/>3:     Sample <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>4:     Update the estimated reward probability for all <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi></mml:mstyle></mml:math></inline-formula> bandit arms as <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:math></inline-formula>.<break/>5:   Let <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> be the set of bandit arms with maximal expected reward.<break/>6:   <bold>if</bold> t=1 <bold>then</bold><break/>7:     Select the first bandit arm <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> uniformly at random.<break/>8:   <bold>else if</bold> <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>9:       <bold>if</bold> <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>10:          Re-select the previous bandit arm <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.<break/>11:     <bold>else</bold><break/>12:          Randomly select another bandit arm.<break/>13:   <bold>else</bold><break/>14:       <bold>if</bold> <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> <bold>and</bold> <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>15:         Re-select the previous bandit arm <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>.<break/>16:       <bold>else</bold><break/>17:          Randomly select another bandit arm among the set <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>.<break/>18:    Sample the reward <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>19:   Increment <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s22"><title>Generalized Latent State (GLS)</title><p><xref ref-type="bibr" rid="bib37">Lee et al., 2011</xref> proposed a latent state model for bandit tasks whereby a learner can be in either an <italic>explore</italic> or an <italic>exploit</italic> state and switch between these as they go through the task. Here, we propose the Generalized Latent State (GLS) model, which unifies and extends latent-state and latent-switching models, previously studied in <xref ref-type="bibr" rid="bib37">Lee et al., 2011</xref>, allowing for more flexible and structured transitions (we thank Patrick Laverty for contributing towards this). The transition distribution over whether the agent is in an exploit state is specified by four parameters, each of which specify the probability of being in an exploit state at trial <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>. This transition distribution is denoted by <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi></mml:mstyle></mml:math></inline-formula> is the previous latent state and <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> the reward observed in the previous trial. We here refer to trials where a bandit arm was selected but failed to produce a reward as <italic>failures</italic>. We provide pseudo-code for our GLS computational model in Algorithm 3.</p><table-wrap id="inlinetable3" position="anchor"><table frame="hsides" rules="groups" id="AL3"><thead><tr><th align="left" valign="bottom">Algorithm 3 Generalized Latent State (GLS)</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Input</bold>: Parameter <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, design <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><break/><bold>Output</bold>: Actions <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, rewards <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>1: Initialize pseudo-counts <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to 1 for all bandit arms <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>.<break/>2: Sample the initial latent state <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>3: <bold>for</bold> t=1, … <bold>do</bold><break/>4:   Let <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mo stretchy="false">←</mml:mo></mml:mstyle></mml:math></inline-formula> all <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> such that <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁢</mml:mo><mml:mi>max</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (arms with the maximal number of rewards).<break/>5:   Let <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>F</mml:mi><mml:mo stretchy="false">←</mml:mo></mml:mstyle></mml:math></inline-formula> all <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> such that <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁢</mml:mo><mml:mi>min</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (arms with the minimal number of failures).<break/>6:   Let <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mi>R</mml:mi><mml:mo>∩</mml:mo><mml:mi>F</mml:mi></mml:mstyle></mml:math></inline-formula>.<break/>7:   Sample <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>8:   <bold>if</bold> t=1 <bold>then</bold><break/>9:    Select the first bandit arm <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> uniformly at random.<break/>10:  <bold>else if</bold> <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>11:   Randomly select a bandit arm <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> from the set <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi></mml:mstyle></mml:math></inline-formula>.<break/>12:  <bold>else if</bold> <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>13:   <bold>if</bold> <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>14:     Select the bandit arm <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mstyle></mml:math></inline-formula> <break/>15:     <bold>else</bold><break/>16:        Randomly select another bandit arm.<break/>17:  <bold>else if</bold> <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>18:    <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>                <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:math></inline-formula> Latent state is <italic>exploit</italic><break/>19:    <bold>if</bold> <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>20:    Let <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> be the set of bandit arms in <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi></mml:mstyle></mml:math></inline-formula> with the minimal number of failures.<break/>21:      Randomly select a bandit arm <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> from the set <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.<break/>22:    <bold>else</bold><break/>23:      Randomly select another bandit arm.<break/>24:  <bold>else</bold><break/>25:     <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi><mml:mo stretchy="false">←</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>               <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>▹</mml:mo></mml:mstyle></mml:math></inline-formula> Latent state is <italic>explore</italic><break/>26:     <bold>if</bold> <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold><break/>27:        Let <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> be the set of bandit arms in <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>F</mml:mi></mml:mstyle></mml:math></inline-formula> with the maximal number of rewards.<break/>28:      Randomly select a bandit arm <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> from the set <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.<break/>29:   <bold>else</bold><break/>30:      Randomly select another bandit arm.<break/>31:<break/>32:      Sample the reward <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.<break/>33:      Increment <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s23"><title>BOED Algorithm</title><p>Algorithm 4 describes the BOED procedure used in our work to determine optimal experiments in bandit tasks.</p><table-wrap id="inlinetable4" position="anchor"><table frame="hsides" rules="groups" id="AL4"><thead><tr><th align="left" valign="bottom">Algorithm 4 BOED</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Input</bold>: Simulator model <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, prior distribution <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, neural network (NN) architecture for <inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/><bold>Output</bold>: Optimal design <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, trained NN <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> <break/>1: Randomly initialize the experimental designs <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> <break/>2: Initialize the Gaussian Process for Bayesian optimization (BO)<break/>3: <bold>while</bold> <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> not converged <bold>do</bold><break/>4:     Sample from the prior: <inline-formula><mml:math id="inf258"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> <break/>5:     Sample from the simulator: <inline-formula><mml:math id="inf260"><mml:mrow><mml:msup><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> <break/>6:     Randomly initialize the NN parameters <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> <break/>7:     <bold>while</bold> <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">d</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with fixed <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> not converged <bold>do</bold><break/>8:       Compute a sample average of the lower bound (see the main text)<break/>9:       Estimate gradients of the sample average with respect to <inline-formula><mml:math id="inf265"><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> <break/>10:      Update <inline-formula><mml:math id="inf266"><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> using any gradient-based optimizer<break/>11:    Use <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to update the Gaussian Process<break/>12:    Use BO to determine which <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to evaluate next</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s24"><title>Additional figures and results</title><p>In this section, we provide additional figures and results that supplement our main text. <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref> shows a screenshot of the bandit task that participants were presented with in our human experiment. <xref ref-type="fig" rid="app5fig2">Appendix 5—figures 2</xref> and <xref ref-type="fig" rid="app5fig3">3</xref> show the parameter estimation results for the AEG and GLS simulator models, respectively, of our simulation study. Specifically, these figures show marginal posterior distributions of the model parameters, averaged over several (simulated) observations, for baseline designs and our optimal designs. With regards to the real experiment with human participants, <xref ref-type="fig" rid="app5fig4">Appendix 5—figures 4</xref> and <xref ref-type="fig" rid="app5fig5">5</xref> show distributions of the posterior entropies in the parameter estimation task for the WSLTS and GLS simulator models, respectively. These distributions compare the differential entropy of the posteriors obtained using our optimal designs with those obtained using the baseline designs. Lastly, <xref ref-type="fig" rid="app5fig6">Appendix 5—figures 6</xref> and <xref ref-type="fig" rid="app5fig7">7</xref> show example posteriors of human participants assigned to the WSLTS model, while <xref ref-type="fig" rid="app5fig8">Appendix 5—figures 8</xref> and <xref ref-type="fig" rid="app5fig9">9</xref> show example posteriors of human participants assigned to the GLS model.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Screenshot of bandit task that participants completed in online experiments.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig1-v1.tif"/></fig><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>Results for the parameter estimation task of the AEG model, showing the marginal posterior distributions of the three AEG model parameters for optimal (green) and baseline (orange) designs, averaged over 1000 observations.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig2-v1.tif"/></fig><fig id="app5fig3" position="float"><label>Appendix 5—figure 3.</label><caption><title>Results for the parameter estimation task of the GLS model, showing the marginal posterior distributions of the three GLS model parameters for optimal (green) and baseline (orange) designs, averaged over 1000 observations.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig3-v1.tif"/></fig><fig id="app5fig4" position="float"><label>Appendix 5—figure 4.</label><caption><title>Results for the parameter estimation task of the WSLTS model, showing the distribution of posterior differential entropies obtained for optimal (green) and baseline (orange) designs (lower is better).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig4-v1.tif"/></fig><fig id="app5fig5" position="float"><label>Appendix 5—figure 5.</label><caption><title>Results for the parameter estimation task of the GLS model, showing the distribution of posterior differential entropies obtained for optimal (green) and baseline (orange) designs (lower is better).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig5-v1.tif"/></fig><fig id="app5fig6" position="float"><label>Appendix 5—figure 6.</label><caption><title>Marginal posterior distributions (green) of the WSLTS model parameters for example participant 1.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig6-v1.tif"/></fig><fig id="app5fig7" position="float"><label>Appendix 5—figure 7.</label><caption><title>Marginal posterior distributions (green) of the WSLTS model parameters for example participant 2.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig7-v1.tif"/></fig><fig id="app5fig8" position="float"><label>Appendix 5—figure 8.</label><caption><title>Marginal posterior distributions (green) of the GLS model parameters for example participant 1.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig8-v1.tif"/></fig><fig id="app5fig9" position="float"><label>Appendix 5—figure 9.</label><caption><title>Marginal posterior distributions (green) of the GLS model parameters for example participant 2.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app5-fig9-v1.tif"/></fig></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s25"><title>Exploring the utility surface and locally optimal designs</title><p>In this section, we provide an example of how our framework allows for exploring the surface of the utility function and (locally) optimal designs. To do so, we consider the example of the model discrimination task where the optimal design was found to be around <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> for the first bandit arm and <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> for the second bandit arm. Our framework maximizes a probabilistic surrogate model, for example a Gaussian process (GP), over the estimated mutual information (MI) values in order to find optimal designs. It is straightforward to extract the learned GP from the final round of training, which allows us to utilize it for post-training analysis and exploration. Since the design space is six-dimensional, it is difficult to visualize the entirety of the utility function, provided by the mean function of the learned GP, at once. However, it is possible to explore the utility function by means of slicing. An example of visualizing such a slice is shown in <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1</xref>. Furthermore, it is possible to systematically search for local optima of the utility function by running stochastic gradient ascent (SGA) over the mean function of the GP, with several restarts. We have done this for 20 restarts for the above example and summarized the (unique) local optima of the utility function in <xref ref-type="table" rid="app6table1">Appendix 6—table 1</xref>. Note that instead of using SGA, we could also treat the utility as an unnormalized density and sample designs proportional to this density (see, e.g. <xref ref-type="bibr" rid="bib46">Müller, 1999</xref>).</p><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>Example of how to explore the high-dimensional utility function for the model discrimination (MD) task, by slicing the Gaussian process (GP) learned during the design optimization step.</title><p>The left plot shows the mean of the GP and the right plot shows the standard deviation of the GP. Shown is the slice corresponding to the design,<inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> which contains the global optimum.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app6-fig1-v1.tif"/></fig><table-wrap id="app6table1" position="float"><label>Appendix 6—table 1.</label><caption><title>A ranking of local design optima for the model discrimination task.</title><p>Shown are the rank, the mutual information (MI) estimate computed via the Gaussian process (GP) mean, and the locally optimal designs. Note that, for this task, the designs for the first bandit arm were <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> for the second bandit arm. The 5 unique optima were obtained by running stochastic gradient ascent on the GP mean with 20 restarts.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Rank of Optimum</th><th align="left" valign="top">MI</th><th align="left" valign="top"><inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.672</td><td align="char" char="." valign="top">0.000</td><td align="char" char="." valign="top">0.000</td><td align="char" char="." valign="top">0.607</td><td align="char" char="." valign="top">1.000</td><td align="char" char="." valign="top">1.000</td><td align="char" char="." valign="top">0.000</td></tr><tr><td align="char" char="." valign="top">2</td><td align="char" char="." valign="top">0.526</td><td align="char" char="." valign="top">1.000</td><td align="char" char="." valign="top">0.006</td><td align="char" char="." valign="top">1.000</td><td align="char" char="." valign="top">0.140</td><td align="char" char="." valign="top">0.266</td><td align="char" char="." valign="top">0.528</td></tr><tr><td align="char" char="." valign="top">3</td><td align="char" char="." valign="top">0.485</td><td align="char" char="." valign="top">0.000</td><td align="char" char="." valign="top">1.000</td><td align="char" char="." valign="top">0.774</td><td align="char" char="." valign="top">0.275</td><td align="char" char="." valign="top">0.399</td><td align="char" char="." valign="top">0.630</td></tr><tr><td align="char" char="." valign="top">4</td><td align="char" char="." valign="top">0.479</td><td align="char" char="." valign="top">0.683</td><td align="char" char="." valign="top">0.775</td><td align="char" char="." valign="top">0.000</td><td align="char" char="." valign="top">0.216</td><td align="char" char="." valign="top">0.717</td><td align="char" char="." valign="top">0.128</td></tr><tr><td align="char" char="." valign="top">5</td><td align="char" char="." valign="top">0.475</td><td align="char" char="." valign="top">0.009</td><td align="char" char="." valign="top">0.710</td><td align="char" char="." valign="top">0.583</td><td align="char" char="." valign="top">0.842</td><td align="char" char="." valign="top">0.000</td><td align="char" char="." valign="top">0.109</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s26"><title>Example participants</title><fig id="app7fig1" position="float"><label>Appendix 7—figure 1.</label><caption><title>Inferred marginal posterior model parameters for human participants best described by the AEG model.</title><p>Marginal posterior distributions (green) of the AEG model parameters for example participant 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app7-fig1-v1.tif"/></fig><fig id="app7fig2" position="float"><label>Appendix 7—figure 2.</label><caption><title>Posterior distribution (green) of the AEG model parameters for example participant 2.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app7-fig2-v1.tif"/></fig><fig id="app7fig3" position="float"><label>Appendix 7—figure 3.</label><caption><title>Mean values of the parameter posterior distributions for each of the 75 participants best described by the AEG model; the square and circle markers represent example participants 1 and 2, respectively.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86224-app7-fig3-v1.tif"/></fig><p>We now explore the data from our optimal design group in the human participant study, specifically focusing on participants that were best described by the AEG model as an example. We qualitatively investigate their behavior by visualizing the (marginal) posterior distribution of the AEG model, as shown in <xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1</xref> and <xref ref-type="fig" rid="app7fig2">Appendix 7—figure 2</xref> and for two example participants. The posterior distribution for the first participant (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1</xref>) indicates small values for the first parameter 0 of the AEG model, that is the ε parameter, which implies that the participant tends to select the arm with the highest probability of receiving a reward as opposed to making random exploration decisions. Larger values of the second parameter 1 of the AEG model, that is the ‘stickiness’ parameter, imply that the participant is biased towards re-selecting the previously chosen bandit arm. The second example participant (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1</xref>) shows markedly different behavior. As indicated by the marginal posterior of the 0 parameter, this participant engages in considerably more random choices. Moreover, this participant shows ‘anti-sticky’ behavior, as implied by low values of the second parameter 1. Lastly, we compute the posterior mean for each participant best described by the AEG model and visualize them in a scatter plot in <xref ref-type="fig" rid="app7fig3">Appendix 7—figure 3</xref>. We find that most participants best described by the AEG model show behavior somewhat similar to the first example participant, but as expected, there is inter-individual variation, for example, with some participants displaying ‘anti-sticky’ behavior.</p></sec></app></app-group></back></article>