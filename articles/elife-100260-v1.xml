<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">100260</article-id><article-id pub-id-type="doi">10.7554/eLife.100260</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100260.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The emergence of visual category representations in infants’ brains</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Yan</surname><given-names>Xiaoqian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4711-7428</contrib-id><email>xqyan@fudan.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Tung</surname><given-names>Sarah Shi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Fascendini</surname><given-names>Bella</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7690-6752</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yulan Diana</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Norcia</surname><given-names>Anthony M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Grill-Spector</surname><given-names>Kalanit</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Wu Tsai Neurosciences Institute, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Neurosciences Program, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP100260</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-06-14"><day>14</day><month>06</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-06-14"><day>14</day><month>06</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.05.11.539934"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-02"><day>02</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100260.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-21"><day>21</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100260.2"/></event></pub-history><permissions><copyright-statement>© 2024, Yan et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Yan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-100260-v1.pdf"/><abstract><p>Organizing the continuous stream of visual input into categories like places or faces is important for everyday function and social interactions. However, it is unknown when neural representations of these and other visual categories emerge. Here, we used steady-state evoked potential electroencephalography to measure cortical responses in infants at 3–4 months, 4–6 months, 6–8 months, and 12–15 months, when they viewed controlled, gray-level images of faces, limbs, corridors, characters, and cars. We found that distinct responses to these categories emerge at different ages. Reliable brain responses to faces emerge first, at 4–6 months, followed by limbs and places around 6–8 months. Between 6 and 15 months response patterns become more distinct, such that a classifier can decode what an infant is looking at from their brain responses. These findings have important implications for assessing typical and atypical cortical development as they not only suggest that category representations are learned, but also that representations of categories that may have innate substrates emerge at different times during infancy.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>infant vision</kwd><kwd>object categorization</kwd><kwd>EEG</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014373</institution-id><institution>Wu Tsai Neurosciences Institute, Stanford University</institution></institution-wrap></funding-source><award-id>139471</award-id><principal-award-recipient><name><surname>Norcia</surname><given-names>Anthony M</given-names></name><name><surname>Grill-Spector</surname><given-names>Kalanit</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Scalp electroencephalography combined with a frequency tagging method reveals that distinct responses to daily categories emerge at different ages in infants.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Visual categorization is important for everyday activities and is amazingly rapid: adults categorize the visual input in about one-tenth of a second (<xref ref-type="bibr" rid="bib80">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib33">Grill-Spector and Kanwisher, 2005</xref>). In adults and school-age children, this key behavior is supported by both clustered and distributed responses to visual categories in high-level visual cortex in ventral temporal and lateral occipitotemporal cortex (VTC and LOTC, respectively) (<xref ref-type="bibr" rid="bib34">Grill-Spector and Weiner, 2014</xref>; <xref ref-type="bibr" rid="bib12">Bugatus et al., 2017</xref>). A visual category consists of items that share common visual features and configurations (<xref ref-type="bibr" rid="bib33">Grill-Spector and Kanwisher, 2005</xref>; <xref ref-type="bibr" rid="bib64">Nordt et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Margalit et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Gomez et al., 2017</xref>; <xref ref-type="bibr" rid="bib77">Stigliani et al., 2015</xref>); e.g., corridors share features of floors, walls, and ceilings, with a typical spatial relationship. Clustered regions in VTC and LOTC (<xref ref-type="bibr" rid="bib12">Bugatus et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>) respond more strongly to items of ecologically relevant categories (faces, bodies, places, words) than other stimuli (<xref ref-type="bibr" rid="bib64">Nordt et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib23">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="bib22">Downing et al., 2001</xref>; <xref ref-type="bibr" rid="bib29">Golarai et al., 2007</xref>; <xref ref-type="bibr" rid="bib74">Scherf et al., 2007</xref>; <xref ref-type="bibr" rid="bib20">Dehaene-Lambertz et al., 2018</xref>) and distributed neural responses across VTC and LOTC (<xref ref-type="bibr" rid="bib12">Bugatus et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>) are reliable across items of a category but distinct across items of different categories. However, it is unknown when these visual category representations emerge in infants’ brains.</p><p>Behaviorally, infants can perform some level of visual categorization within the first year of life. Measurements of infants’ looking preferences and looking times suggest that visual saliency impacts young infants’ viewing patterns (<xref ref-type="bibr" rid="bib76">Spriet et al., 2022</xref>): between 4 and 10 months of age, infants can behaviorally distinguish between faces and objects (<xref ref-type="bibr" rid="bib76">Spriet et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Mondloch et al., 1999</xref>) and between different animals like cats and dogs (<xref ref-type="bibr" rid="bib70">Quinn et al., 1993</xref>; <xref ref-type="bibr" rid="bib90">Younger and Fearing, 1999</xref>). Later on, between 10 and 19 months, infants behaviorally distinguish broader-level animate vs. inanimate categories (<xref ref-type="bibr" rid="bib76">Spriet et al., 2022</xref>). Neurally, electroencephalographic (EEG) studies have found stronger responses to images of faces vs. objects or textures in 4- to 12-month-olds (<xref ref-type="bibr" rid="bib17">Conte et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Farzin et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>) and that stimulus category can be decoded from distributed responses slightly but significantly above chance in 6- to 15-month-olds (<xref ref-type="bibr" rid="bib7">Bayet et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Xie et al., 2022</xref>). Functional magnetic resonance imaging (fMRI) studies have found stronger responses to videos of faces (<xref ref-type="bibr" rid="bib18">Deen et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>), bodies (<xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>), and places (<xref ref-type="bibr" rid="bib18">Deen et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>) vs. objects in clustered regions in VTC and LOTC of 2- to 10-month-olds. However, because prior studies used different types of stimuli and age ranges, it is unknown when representations to various categories emerge during the first year of life. To address this key open question, we examined when neural representations to different visual categories emerge during infancy using EEG in infants of four age groups spanning 3–15 months of age.</p><p>We considered two main hypotheses regarding the developmental trajectories of category representations. One possibility is that representations to multiple categories emerge together because infants need to organize the barrage of visual input to understand what they see. Supporting this hypothesis are findings of (i) selective responses to faces, places, and body parts in VTC and LOTC of 2- to 10-month-olds (<xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>), and (ii) above chance classification of distributed EEG responses to toys, bodies, faces, houses in 6- to 8-month-olds (<xref ref-type="bibr" rid="bib87">Xie et al., 2022</xref>) as well as animals and body parts in 12- to 15-month-olds (<xref ref-type="bibr" rid="bib7">Bayet et al., 2020</xref>).</p><p>Another possibility is that representations of different categories may emerge at different times during infancy. This may be due to two reasons. First, representations of ecologically relevant categories like faces, body parts, and places may be innate because of their evolutionary importance (<xref ref-type="bibr" rid="bib46">Kanwisher, 2010</xref>; <xref ref-type="bibr" rid="bib78">Sugita, 2009</xref>; <xref ref-type="bibr" rid="bib55">Mahon and Caramazza, 2011</xref>; <xref ref-type="bibr" rid="bib9">Bi et al., 2016</xref>), whereas representations for other categories may develop later only with learning (<xref ref-type="bibr" rid="bib64">Nordt et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>; <xref ref-type="bibr" rid="bib8">Behrmann and Plaut, 2015</xref>). Supporting this hypothesis are findings that newborns and young infants tend to orient to faces (<xref ref-type="bibr" rid="bib66">Pascalis et al., 1995</xref>) and face-like stimuli (<xref ref-type="bibr" rid="bib44">Johnson et al., 1991</xref>), as well as have cortical responses to face-like stimuli (<xref ref-type="bibr" rid="bib13">Buiatti et al., 2019</xref>), but word representations only emerge in childhood with the onset of reading instruction (<xref ref-type="bibr" rid="bib64">Nordt et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>; <xref ref-type="bibr" rid="bib19">Dehaene et al., 2010</xref>). Second, even if visual experience is necessary for the development category representations (including faces; <xref ref-type="bibr" rid="bib5">Arcaro et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Scott and Arcaro, 2023</xref>; <xref ref-type="bibr" rid="bib54">Livingstone et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Arcaro et al., 2017</xref>), categories that are seen more frequently earlier in infancy may develop before others. Measurements using head-mounted cameras suggest that infants’ visual diet (composition of visual input) varies across categories and age: The visual diet of 0- to 3-month-olds contains ~25% faces and &lt;10% hands, that of 12- to 15-month-olds contains ~20% faces and ~20% hands (<xref ref-type="bibr" rid="bib26">Fausey et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Jayaraman et al., 2017</xref>), and that of 24-month-olds contains ~10% faces, and ~25% hands. Thus, looking behavior in infants predicts that representations of faces may emerge before that of limbs.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>45 infants from four age groups: 3–4 months (n=17, 7 females), 4–6 months (n=14, 7 females), 6–8 months (n=15, 6 females), and 12–15 months (n=15, 4 females) participated in EEG experiments. Twelve participants were part of an ongoing longitudinal study and came for several sessions spanning at least 3 months apart. Infants viewed gray-scale images from five visual categories present in infants’ environments (faces, limbs, corridors, characters, and cars) while EEG was recorded. Different from prior infant studies (<xref ref-type="bibr" rid="bib17">Conte et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>; <xref ref-type="bibr" rid="bib7">Bayet et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Xie et al., 2022</xref>; <xref ref-type="bibr" rid="bib18">Deen et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>), we used images that have been widely used in fMRI studies (<xref ref-type="bibr" rid="bib64">Nordt et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Gomez et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Lerma-Usabiaga et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Jagadeesh and Gardner, 2022</xref>) and are largely controlled for low-level properties such as luminance, contrast, similarity, and spatial frequency (<xref ref-type="fig" rid="fig1">Figure 1B</xref> and <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>). We use a steady-state visual evoked potential (<xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>; <xref ref-type="bibr" rid="bib25">Farzin et al., 2012</xref>; <xref ref-type="bibr" rid="bib40">Heinrich et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Liu-Shuang et al., 2014</xref>) (SSVEP) paradigm: In each 70 s sequence, images from five categories were shown every 0.233 s; one of the categories was the target, so different images from that category appeared every 1.167 s, and the rest of the images were drawn from the other four categories in a random order (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Images of all categories appeared at equal probability and no images were repeated (<xref ref-type="bibr" rid="bib77">Stigliani et al., 2015</xref>). Infants participated in five conditions, which varied by the target category. We used the EEG-SSVEP approach because: (i) it affords a high signal-to-noise ratio with short acquisitions making it effective for infants (<xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>; <xref ref-type="bibr" rid="bib25">Farzin et al., 2012</xref>), (ii) it has been successfully used to study responses to faces in infants (<xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>; <xref ref-type="bibr" rid="bib25">Farzin et al., 2012</xref>; <xref ref-type="bibr" rid="bib71">Rekow et al., 2021</xref>), and (iii) it enables measuring both general visual response to images by examining responses at the image presentation frequency (4.286 Hz) and category-selective responses by examining responses at the category frequency (0.857 Hz, <xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and stimuli analysis.</title><p>(<bold>A</bold>) Example segments of presentation sequences in which faces (top panel) and limbs (bottom panel) were the target category. Images spanning 12° containing gray-level images of items from different categories on a phase-scrambled background appeared for 233 ms (frequency: 4.286 Hz). A different exemplar from a single category appeared every fifth image (frequency: 0.857 Hz). Between the target category images, randomly drawn images from the other four categories were presented. Sequences consisted of 20% images from each of the five categories and no images were repeated. Each category condition lasted for 14 s and contained 12 such cycles. Participants viewed in random order 5 category conditions: faces, limbs, corridors, characters, and cars forming a 70 s presentation sequence. (<bold>B</bold>) Images were controlled for several low-level properties using the SHINE toolbox as explained in <xref ref-type="bibr" rid="bib77">Stigliani et al., 2015</xref>. Metrics are colored by category (see legend). <italic>Contrast</italic>: mean standard deviation of gray-level values in each image, averaged across 144 images of a category. <italic>Luminance</italic>: mean gray-level of each image, averaged across 144 images of a category. <italic>Similarity:</italic> mean pixel wise similarity between all pairs of images in a category. For all three metrics, boxplots indicate median, 25%, 75% percentiles, range, and outliers. Significant differences between categories are indicated by asterisks, for contrast and luminance (nonparametric permutation t-test p&lt;0.05, Bonferroni corrected); for image similarity, all categories are significantly different than others (nonparametric permutation testing, p&lt;0.05, Bonferroni corrected, except for corridors vs. cars, p=0.24). <italic>Spatial frequency: Solid lines:</italic> distribution of spectral amplitude in each frequency averaged across 144 images in each category. <italic>Shaded area</italic>: standard deviation. Spatial frequency distributions are similar across categories.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig1-v1.tif"/></fig><p>As the EEG-SSVEP paradigm is novel and we are restricted in the amount of data we can obtain in infants, we first tested if we can use this paradigm and a similar amount of data to detect category-selective responses in adults. Results in adults validate the SSVEP paradigm for measuring category selectivity: as they show that (i) category-selective responses can be reliably measured using EEG-SSVEP with the same amount of data as in infants (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), and that (ii) category information from distributed spatiotemporal response patterns can be decoded with the same amount of data as in infants (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>).</p><p>As infants have lower cortical visual acuity, we also tested if the stimuli are distinguishable to infants. Thus, we simulated how they may look to infants by filtering the images to match the cortical acuity of 3-month-olds (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). Despite being blurry, images of different categories are readily distinguishable by adults (<xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video5">5</xref>), suggesting that there is sufficient visual information in the lower spatial frequencies of the stimuli for infants to distinguish visual categories.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100260-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Movie showing Gaussian low-pass filtered face stimuli shown in the experiment at 5 cycles per degree (cpd).</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100260-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Movie showing Gaussian low-pass filtered limb stimuli shown in the experiment at 5 cycles per degree (cpd).</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100260-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Movie showing Gaussian low-pass filtered corridor stimuli shown in the experiment at 5 cycles per degree (cpd).</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100260-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Movie showing Gaussian low-pass filtered character stimuli shown in the experiment at 5 cycles per degree (cpd).</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-100260-video5.mp4" id="video5"><label>Video 5.</label><caption><title>Movie showing Gaussian low-pass filtered car stimuli shown in the experiment at 5 cycles per degree (cpd).</title></caption></media><sec id="s2-1"><title>Robust visual responses in occipital regions to visual stimuli in all infant age groups</title><p>We first tested if there are significant visual responses to our stimuli in infants’ brains by evaluating the amplitude of responses at the image presentation frequency (4.286 Hz) and its first three harmonics. We found that in all age groups, visual responses were concentrated spatially over occipital electrodes (<xref ref-type="fig" rid="fig2">Figure 2A–D</xref>, left panel, <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Quantification of the mean visual response amplitude over a region of interest (ROI) spanning nine electrodes over early visual cortex (occipital ROI) revealed significant responses in all infant age groups at the image frequency and its first three harmonics (response amplitudes significantly above zero with false discovery rate [FDR] corrected at four levels; except for the first harmonic at 8.571 Hz in 6- to 8-month-olds; <xref ref-type="fig" rid="fig2">Figure 2A–D</xref>, right panel). Analysis of visual responses separately by category condition revealed that visual responses were not significantly different across category conditions (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>; no significant main effect of category, β<sub>category</sub> = 0.08, 95% CI: –0.08–0.24, t<sub>(301)</sub> = 0.97, p=0.33, or category by age interaction, β<sub>category × age</sub> = -0.04, 95% CI: –0.11–0.03, t<sub>(301)</sub> = –1.09, p=0.28, linear mixed model (LMM) on response amplitude to 4.286 Hz and its first three harmonics). We also tested if experimental noise varied across age groups. Noise level was estimated in the occipital electrodes by measuring the amplitude of response in frequencies up to 8.571 Hz excluding image presentation frequencies (4.286 Hz and harmonics) and category frequencies (0.857 Hz and harmonics) as this frequency range includes the relevant main harmonics. We found no significant difference in noise across age groups (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). These analyses indicate that infants were looking at the stimuli as there are significant visual responses even in the youngest 3- to 4-month-old infants’ and there are no significant differences in noise levels across infants of different ages.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Strong visual responses over occipital cortex at the image-update frequency and its harmonics in all age groups.</title><p>Each panel shows mean responses across infants in an age group. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds, n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. <italic>Left panels in each row:</italic> spatial distribution of the visual response at the image-update frequency and its first three harmonics. <italic>Middle panels in each row:</italic> mean Fourier amplitude spectrum across nine occipital electrodes of the occipital region of interest (ROI) showing high activity at harmonics of the image-update frequency marked out by thicker lines. Data are first averaged in each participant and each condition and then across participants. <italic>Error bars:</italic> standard error of the mean across participants. <italic>Asterisks:</italic> response amplitudes significantly larger than zero, p&lt;0.01, false discovery rate (FDR) corrected. <italic>Colored bars:</italic> amplitude of response at category frequency and its harmonics. <italic>White bars:</italic> amplitude of response at noise frequencies. (<bold>E</bold>) Noise amplitudes in the frequency range up to 8.571 Hz (except for the visual response frequencies and visual category response frequencies) from the amplitude spectra in (<bold>A</bold>) for each age group (white bars on the spectra). <italic>Error bars:</italic> standard error of the mean across participants. (<bold>F</bold>) Mean image-update response over occipital electrodes for each age group. Waveforms are cycle averages over the period of the individual image presentation time (233 ms). <italic>Lines:</italic> mean response. <italic>Shaded areas:</italic> standard error of the mean across participants of each group. <italic>Horizontal lines colored by age group:</italic> significant responses vs. zero (p&lt;0.05 with a cluster-based analysis, see Methods). (<bold>G</bold>) Peak latency for the first peak in the 60–90 ms interval after stimulus onset. Each dot is a participant; dots are colored by age group. <italic>Line</italic>: linear mixed model (LMM) estimate of peak latency as a function of log10(age). <italic>Shaded area:</italic> 95% confidence interval (CI). (<bold>H</bold>) Same as (<bold>G</bold>) but for the second peak in the 90–160 ms interval for 3- to 4-month-olds, and 90–110 ms for older infants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig2-v1.tif"/></fig><p>Prior EEG data (<xref ref-type="bibr" rid="bib17">Conte et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Taylor et al., 1999</xref>) suggest that the timing and waveform of visual responses may vary across development. To complement the frequency domain analysis, we transformed the responses at image frequency and its harmonics to the time domain using an inverse Fourier transformation for two reasons. First, the time domain provides access to information about response timing and waveform that is not directly accessible from an analysis of responses of individual harmonics. Second, the total visual response is better reflected in the time domain as the individual harmonic amplitudes can sum constructively.</p><p>We observed that during the 233 ms image presentation, temporal waveforms had two deflections in 3- to 4-month-olds (one negativity and one positivity, <xref ref-type="fig" rid="fig2">Figure 2F</xref>) and four deflections for infants older than 4 months (two minima and two maxima, <xref ref-type="fig" rid="fig2">Figure 2F</xref>). To evaluate developmental effects, we examined the latency and amplitude of the peak visual response during two time windows related to the first deflection (60–90 ms), and the second deflection (90–160 ms for 3- to 4-month-olds, and 90–110 ms for other age groups). In general, we find that the latency of the peak deflection decreased from 3 to 15 months (<xref ref-type="fig" rid="fig2">Figure 2G and H</xref>). As data includes both cross-sectional and longitudinal measurements and we observed larger development earlier than later in infancy, we used an LMM to model peak latency as a function of the logarithm of age (see Methods). Results reveal that the latency of the peak deflection significantly and differentially decreased with age in the two time windows (β<sub>age × time window</sub> = –45.78, 95% CI: –58.39 to –33.17, t<sub>(118)</sub> = –7.19, p=6.39 × 10<sup>–11</sup>; LMM with age and time window as fixed effects, and participant as a random effect, all stats in <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref>, <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref>). There were larger decreases in the peak latency in the second than first time window (<xref ref-type="fig" rid="fig2">Figure 2G and H</xref>, first:β<sub>age</sub> = –7.44, 95% CI: –13.82 to –1.06, t<sub>(118)</sub> = –2.33, p<sub>FDR</sub>&lt;0.05; second: β<sub>age</sub> = –46.91, 95% CI: –56.56 to –37.27, t<sub>(59)</sub> = –9.73, p<sub>FDR</sub>&lt;0.001). Peak amplitude also differentially develops across the two windows (β<sub>age × time window</sub> = –4.90, 95% CI: –8.66 to –1.14, t<sub>(118)</sub> = –2.58, p=0.01, <xref ref-type="table" rid="app1table6">Appendix 1—table 6</xref>, <xref ref-type="table" rid="app1table7">Appendix 1—table 7</xref>). The decrease in peak amplitude with age was significant only for the second deflection (β<sub>age</sub> = –3.59, 95% CI: –6.38 to –0.81, t<sub>(59)</sub> = –2.58, p=0.01, LMM). These data suggest that the temporal dynamics of visual responses over occipital cortex develop from 3 to 15 months of age.</p></sec><sec id="s2-2"><title>What is the nature of category-selective responses in infants?</title><p>We next examined if in addition to visual responses to the rapid image stream, there are also category-selective responses in infants, by evaluating the amplitude of responses at the category frequency (0.857 Hz) and its harmonics. This is a selective response as it reflects the relative response to images of category above the general visual response. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the spatial distribution and amplitude of the mean category response for faces and its harmonics in each age group. Mean category-selective responses to limbs, cars, corridors, and words are shown in <xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref>. We analyzed mean responses over two ROIs spanning seven electrodes each over the left (LOT) and right occipitotemporal (ROT) cortex where high-level visual regions are located (<xref ref-type="bibr" rid="bib86">Xie et al., 2019</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Face responses emerge over occipitotemporal electrodes after 4 months of age.</title><p>Each panel shows mean responses at the category frequency and its harmonics across infants in an age group. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds; n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. Left panels in each row: spatial distribution of response to category frequency at the 0.857 Hz and its first harmonic. Harmonic frequencies are indicated on the top. Right two panels in each row: mean Fourier amplitude spectrum across two regions of interest (ROIs): seven left and seven right occipitotemporal electrodes (shown in black on the left panel). Data are first averaged across electrodes in each participant and then across participants. Error bars: standard error of the mean across participants of an age group. Asterisks: significant amplitude vs. zero (p&lt;0.05, false discovery rate [FDR] corrected at two levels). Black bars: image frequency and harmonics; colored bars: category frequency and harmonics. White bars: noise frequencies. Responses for the other categories (limbs, corridors, characters, and cars) in <xref ref-type="fig" rid="app1fig5">Appendix 1—figures 5</xref>–<xref ref-type="fig" rid="app1fig8">8</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig3-v1.tif"/></fig><p>We found significant group-level category responses to some but not all categories and a differential development of category-selective responses during infancy. The largest and earliest developing category-selective responses were to faces. In contrast to visual responses, which were centered over occipital electrodes (<xref ref-type="fig" rid="fig2">Figure 2A–D</xref>, left panel), significant categorical responses to faces (at 0.857 Hz and its first harmonic, 1.714 Hz) were observed over lateral occipitotemporal electrodes (<xref ref-type="fig" rid="fig3">Figure 3A–D</xref>, left panel). Notably, there were significant responses to faces over bilateral occipitotemporal electrodes in 4- to 6-month-olds at 0.857 Hz (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, response amplitudes significantly above zero with Hotelling’s T2 statistic, p<sub>FDR</sub>&lt;0.05, FDR corrected over two levels: the category frequency and its first harmonic), as well as 6- to 8-month-olds and 12- to 15-month-olds at the category frequency and its first harmonic (<xref ref-type="fig" rid="fig3">Figure 3C and D</xref>, both p<sub>FDR</sub>&lt;0.05). However, there were no significant responses to faces in 3- to 4-month-olds at either the category frequency or its harmonics (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right panel). These data suggest that face-selective responses start to reliably emerge over lateral occipitotemporal cortex between 4 and 6 months of age.</p><p>We did not find significant group-level category-selective responses that survived FDR correction to any of the other categories before 6 months of age (<xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref>, except for a weak but statistically significant response for cars in the ROT ROI in 3- to 4-month-olds). Instead, we found significant category-selective responses that survived FDR correction for (i) limbs in 6- to 8-month-olds in the ROT ROI (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>), (ii) corridors in 6- to 8-month-olds and 12- to 15-months-old in the left occipitotemporal (LOT) ROI (<xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>), and (iii) characters in 6- to 8-month-olds in the ROT ROI, and in 12- to 15-month-olds in bilateral occipitotemporal ROI (<xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>).</p><p>We next examined the development of the category-selective responses separately for the right and left lateral occipitotemporal ROIs. The response amplitude was quantified by the root mean square (RMS) amplitude value of the responses at the category frequency (0.857 Hz) and its first harmonic (1.714 Hz) for each category condition and infant. With an LMM analysis, we found significant development of response amplitudes in both the occipitotemporal ROIs which varied by category (LOT ROIs: β<sub>category × age</sub> = –0.21, 95% CI: –0.39 to –0.04, t<sub>(301)</sub> = –2.40, p<sub>FDR</sub>&lt;0.05; ROT ROIs: β<sub>category × age</sub> = –0.26, 95% CI: –0.48 to –0.03, t<sub>(301)</sub> = –2.26, p<sub>FDR</sub>&lt;0.05, LMM as a function of log (age) and category; participant: random effect).</p><p>We evaluated the temporal dynamics of category-selective waveforms by transforming the data at the category frequency and its harmonics to the time domain. This analysis was done separately for each of the LOT and ROT ROIs for each category and age group. Consistent with frequency domain analyses, average temporal waveforms over lateral-occipital ROIs show significant responses to faces that emerge at ~4 months of age (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, significant responses relative to zero, cluster-based nonparametric permutation 10,000 times, two-tailed t-test, at p&lt;0.05). The temporal waveforms of responses to faces in infants show an initial positive deflection peaking ~500 ms after stimulus onset followed by a negative deflection peaking at ~900 ms. Notably, mean waveforms associated with limbs, corridors, and characters in lateral occipital ROIs are different from faces: there is only a single negative deflection that peaks at ~500 ms after stimulus onset, which is significant only in 6- to 8- and 12- to 15-month-olds (<xref ref-type="fig" rid="fig4">Figure 4B–D</xref>). There was no significant category response to cars in infants except for a late (~1000 ms) positive response in 4- to 6-month-olds (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). These results show that both the timing and waveform differ across categories, which suggests that there might be additional category information in the distributed spatiotemporal response.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Temporal dynamics of category-selective responses as a function of age.</title><p>Category-selective responses to (<bold>A</bold>) faces, (<bold>B</bold>) limbs, (<bold>C</bold>) corridors, (<bold>D</bold>) characters, and (<bold>E</bold>) cars over left and right occipitotemporal region of interest (ROI). Data are averaged across electrodes of an ROI and across individuals. Left four panels in each row show the responses in the time domain for the four age groups. <italic>Colored lines:</italic> mean responses in the right occipitotemporal ROI. <italic>Gray lines:</italic> mean responses in the left occipitotemporal ROI. <italic>Colored horizontal lines above x-axis:</italic> significant responses relative to zero for the right OT ROI. <italic>Gray horizontal lines above x-axis:</italic> significant responses relative to zero for the left OT ROI. <italic>Top</italic>: 3D topographies of the spatial distribution of the response to target category stimuli at a 483–500 ms time window after stimulus onset. <italic>Right panel in each row:</italic> amplitude of the peak deflection defined in a 400–700 ms time interval after stimulus onset. Each dot is a participant; dots are colored by age group. <italic>Red line:</italic> linear mixed model (LMM) estimate of peak amplitude as a function of log10(age). <italic>Shaded area:</italic> 95% CI.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig4-v1.tif"/></fig><p>We next examined the development of the peak response and latency of the category waveforms separately for the right and left lateral occipitotemporal ROIs. We found significant development of the peak response in the right lateral occipitotemporal ROI which varied by category (β<sub>category × age</sub> = –1.09, 95% CI: –2.00 to –0.14, t<sub>(301)</sub> = –2.26, p<sub>FDR</sub>&lt;0.05, LMM as a function of log(age) and category; participant: random effect). Post hoc analyses revealed that the peak response for faces significantly increased from 3 to 15 months (<xref ref-type="fig" rid="fig4">Figure 4A</xref> right, β<sub>age</sub> = 7.27, 95% CI: 4.03–10.51, t<sub>(59)</sub> = 4.50, p<sub>FDR</sub>&lt;0.05, LMM as a function of log(age); participant: random effect) and the peak response for limbs significantly decreased (<xref ref-type="fig" rid="fig4">Figure 4B</xref> right, β<sub>age</sub> = –2.90, 95% CI: –5.41 to –0.38, t<sub>(59)</sub> = –2.31, p=0.02, not significant after FDR correction over five category levels). There were no other significant developments of peak amplitude (<xref ref-type="table" rid="app1table8">Appendix 1—table 8</xref>, <xref ref-type="table" rid="app1table9">Appendix 1—table 9</xref>).</p><p>Additionally, for all categories, the latency of the peak response in the ROT ROI significantly decreased from 3 to 15 months of age (β<sub>age</sub> = –173.17, 95% CI: –284.73 to –61.61, t<sub>(301)</sub> = –3.05, p=0.002, LMM as a function of log(age) and category; participant: random effect). We found no significant development of peak latency in the LOT ROI (<xref ref-type="table" rid="app1table8">Appendix 1—table 8</xref>, <xref ref-type="table" rid="app1table9">Appendix 1—table 9</xref>).</p></sec><sec id="s2-3"><title>Are spatiotemporal patterns of responses to visual categories consistent across infants?</title><p>As we observed different mean waveforms over the lateral occipital ROIs for the five categories (<xref ref-type="fig" rid="fig4">Figure 4</xref>), we asked whether the distributed spatiotemporal patterns of brain responses evoked by each category are unique and reliable across infants. We reasoned that if different categories generated consistent distributed spatiotemporal responses, an independent classifier would be able to predict the category an infant was viewing from their distributed spatiotemporal pattern of response. Thus, we used a leave-one-out-cross-validation (LOOCV) approach (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and tested if a classifier can decode the category a left-out infant viewed based on the similarity of their distributed spatiotemporal response to the mean response to each of the categories in the remaining N–1 infants. We calculated for each infant the mean category waveform (same as <xref ref-type="fig" rid="fig4">Figure 4</xref>) across the occipital and lateral occipitotemporal ROIs and concatenated the waveforms across the three ROIs to generate the distributed spatiotemporal response to a category (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The classifier was trained and tested separately for each age group.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Successful decoding of faces from mean spatiotemporal responses starting from 4 months of age.</title><p>(<bold>A</bold>) An illustration of winner-takes-all leave-one-out-cross-validation (LOOCV) classifier from mean spatiotemporal response patterns of each category. Spatiotemporal patterns of response for each category are generated by concatenating the mean time courses from N–1 infants from three regions of interest (ROIs): left occipitotemporal (LOT), occipital (OCC), and right occipitotemporal (ROT). At each iteration, we train the classifier with the mean spatiotemporal patterns of each category from N–1 infants, and test how well it predicts the category the left-out infant is viewing from their spatiotemporal brain response. The winner-take-all (WTA) classifier determines the category based on the training vector that has highest pairwise correlation with the test vectors. (<bold>B</bold>) Mean decoding accuracies across all five categories in each age group. <italic>Asterisks:</italic> significant decoding above chance level (p&lt;0.01, Bonferroni corrected, one-tailed). (<bold>C</bold>) Percentage of infants in each age group we could successfully decode for each category. <italic>Dashed lines</italic>: chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig5-v1.tif"/></fig><p>Results reveal two main findings. First, the LOOCV classifier decoded category information from brain responses significantly above the 20% chance level in infants aged 6 months and older but not in younger infants (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, 6- to 8-month-olds, significant above chance: t<sub>(14)</sub> = 4.1, p<sub>FDR</sub>&lt;0.01, one-tailed, FDR corrected over four age groups; 12- to 15-month-olds, t<sub>(14)</sub> = 3.4, p<sub>FDR</sub>&lt;0.01). This suggests that spatiotemporal patterns of responses to different categories become reliable across infants after 6 months of age. Second, examination of classification by category shows that the LOOCV classifier successfully determined from spatiotemporal responses when infants were viewing faces in 64% of 4- to 6-month-olds, in 93% of 6- to 8-month-olds, and 87% of 12- to 15-month-olds (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). In contrast, classification performance was numerically lower for the other categories (successful classification in less than 40% of the infants). This suggests that a reliable spatiotemporal response to faces that is consistent across infants develops after 4 months of age and dominates classification performance.</p></sec><sec id="s2-4"><title>What is the nature of categorical spatiotemporal patterns in individual infants?</title><p>While the prior analyses leverage the power of averaging across electrodes and infants, this averaging does not provide insight to fine-grained neural representations within individual infants. To examine the finer-grain representation of category information within each infant’s brain, we examined the distributed spatiotemporal responses to each category across the 23 electrodes spanning the LOT and ROT cortex in each infant. We tested: (i) if categorical representations in an infant’s brain are reliable across different images of a category, and (ii) if category representations become more distinct during the first year of life. We predicted that if representations become more similar across items of a category and more dissimilar between items of different categories then category distinctiveness (defined as the difference between mean within and between category similarity) would increase from 3 to 15 months of age.</p><p>To examine the representational structure, we calculated representation similarity matrices (RSMs) across odd/even split-halves of the data in each infant. Each cell in the RSM quantifies the similarity between two spatiotemporal patterns: On-diagonal cells of the RSM quantify the similarity of distributed spatiotemporal responses to different images from the same category and off-diagonal cells quantify the similarity of spatiotemporal responses to images from different categories. Categorical structure will manifest in RSMs as positive on diagonal values indicating reliable within-category spatiotemporal responses which are higher than off-diagonal between category similarity (<xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3B</xref>, and <xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Individual split-half spatiotemporal pattern analyses reveal category information slowly emerges in the visual cortex after 6 months of age.</title><p>(<bold>A</bold>) Representation similarity matrices (RSMs) generated from odd/even split-halves of the spatiotemporal patterns of responses in individual infants. Spatiotemporal patterns for each category are generated by concatenating the mean time courses of each of 23 electrodes across left occipitotemporal (LOT), occipital (OCC), and right occipitotemporal (ROT). (<bold>B</bold>) Category distinctiveness calculated for each infant and category by subtracting the mean between-category correlation values from the within-category correlation value. (<bold>C</bold>) Distinctiveness as a function of age; panels by category; each dot is a participant. Dots are colored by age group. <italic>Red line:</italic> linear mixed model (LMM) estimates of distinctiveness as a function of log10(age). <italic>Shaded area:</italic> 95% CI.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig6-v1.tif"/></fig><p>Examination of mean RSMs in each age group reveals no reliable category information in individuals at 3- to 4-month-olds or 4- to 6-month-olds, as within-category similarity is not significantly above zero (<xref ref-type="fig" rid="fig6">Figure 6A</xref> and 3- to 4-month-olds: on-diagonal, –0.03 ±0.06, p=0.96, one-tailed; 4- to 6-month-olds: on-diagonal: 0.009 ± 0.11, p=0.38). However, starting around 6 months some category structure emerges in the RSMs. In particular, distributed responses to faces become reliable as within category similarity for faces is significantly above zero in 6- to 8-month-olds (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, 0.31 ± 0.24, t<sub>(14)</sub> = 5.1, p<sub>FDR</sub>&lt;0.05, FDR corrected over five category levels), and stays reliable in 12- to 15-month-olds (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, 0.26 ± 0.24, t<sub>(14)</sub> = 4.18, p<sub>FDR</sub>&lt;0.05). Distributed responses to limbs become reliable later on as within category similarity for limbs is significantly above zero in 12- to 15-months-olds (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, 0.11 ± 0.21, t<sub>(14)</sub> = 1.98, p=0.03, but not surviving FDR correction at five levels).</p><p>Next, we evaluated the development of category distinctiveness, which was calculated for each category and infant. Individual infants’ category distinctiveness is shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref> (infants ordered by age) and in the scatterplots in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. In infants younger than 4 months (120 days) category distinctiveness is largely close to zero or even negative, suggesting no differences between spatiotemporal responses to one category vs. another. Category distinctiveness increases with age and becomes more positive from 84 to 445 days of age (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>). The biggest increase is for faces where after ~6 months of age (194 days) face distinctiveness is consistently positive in individual infants (13/15 infants aged 194–364 days and 12/15 infants aged 365–445 days). The increase in distinctiveness is more modest for other categories and appears later in development. For example, positive distinctiveness for limbs and cars in individual infants is consistently observed after 12 months of age (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>; limbs: 9/15 infants aged 365–445 days vs. 5/15 infants aged 194–364 days; cars: 12/15 365–445 days vs. 7/15 194–364 days).</p><p>Using LMMs we determined if distinctiveness significantly changed with age (log transformed) and category (participant, random factor). Results indicate that category distinctiveness significantly increased from 3 to 15 months of age (β<sub>ββage</sub> = 0.77, 95% CI: 0.54–1.00, t<sub>(301)</sub> = 6.62, p=1.67×10<sup>–10</sup>), and further that development significantly varies across categories (β<sub>age × category</sub> = –0.13, 95% CI: –0.2 to –0.06, t<sub>(301)</sub> = –3.61, p=3.5×10<sup>–4</sup>; main effect of category, β<sub>category</sub> = 0.27, 95% CI: 0.11–0.43, t<sub>(301)</sub> = 3.38, p=8.2×10<sup>–4</sup>). Post hoc analyses for each category (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) reveal that distinctiveness significantly increased with age for faces (β<sub>age</sub> = 0.9, 95% CI: 0.6–1.1, t<sub>(59)</sub> = 6.8, p<sub>FDR</sub>&lt;0.001), limbs (β<sub>age</sub> = 0.4, 95% CI: 0.2–0.6, t<sub>(59)</sub> = 5.0, <italic>p<sub>FDR</sub></italic>&lt;0.001), characters (β<sub>age</sub> = 0.2, 95% CI: 0.02–0.3, t<sub>(59)</sub> = 2.2, p<sub>FDR</sub>&lt;0.05), and cars (β<sub>age</sub> = 0.4, 95% CI: 0.2–0.5, t<sub>(59)</sub> = 3.7, p<sub>FDR</sub>&lt;0.001). Post hoc t-tests show that for faces, the category distinctiveness is significantly above zero after 6 months of age (6- to 8-month-olds, t<sub>(14)</sub> = 6.73, p<sub>FDR</sub>&lt;0.05; 12- to 15-month-olds, t<sub>(14)</sub> = 5.30, p<sub>FDR</sub>&lt;0.05) and for limbs and cars at 12–15 months of age (limbs: t<sub>(14)</sub> = 2.19, p<sub>FDR</sub>&lt;0.05; cars: t<sub>(14)</sub> = 4.53, p<sub>FDR</sub>&lt;0.05). This suggests that category distinctiveness slowly emerges in the visual cortex of infants from 3 to 15 months of age, with the largest and fastest development for faces.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We find that both selective responses to items of a category over others across lateral occipital ROIs and the distinctiveness of distributed visual category representations progressively develop from 3 to 15 months of age. Notably, we find a differential development of category-selective responses (<xref ref-type="fig" rid="fig7">Figure 7</xref>), whereby responses to faces emerge the earliest, at 4–6 months of age and continue to develop through the first year of life. Category-selective responses to limbs, corridors, and characters follow, emerging at 6–8 months of age. Our analysis of the distinctiveness of the distributed spatiotemporal patterns to each category also finds that distributed representations to faces become more robust in 6- to 8-month-olds and remain robust in 12- to 15-month-olds. While the distinctiveness of distributed patterns to limbs and cars only become reliable at 12–15 months of age. Together these data suggest a rethinking of the development of category representations during infancy as they not only suggest that category representations are learned, but also that representations of categories that may have innate substrates such as faces, bodies, and places emerge at different times during infancy.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Selective responses to items of a category and distinctiveness in distributed patterns develop at different times during the first year of life.</title><p><italic>Blue arrows</italic>: presence of significant mean region of interest (ROI) category-selective responses in lateral occipital ROIs, combining results of analyses in the frequency and time domains. <italic>Yellow arrows</italic>: presence of significantly above zero distinctiveness in the distributed spatiotemporal response patterns across occipital and lateral occipital electrodes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-fig7-v1.tif"/></fig><sec id="s3-1"><title>Reliable category representations start to emerge at 4 months of age</title><p>While 3- to 4-month-old infants have significant and reliable evoked visual responses over early visual cortex, we find no reliable category representations of faces, limbs, corridors, or characters in these young infants. Both analyses of average responses across lateral occipital ROIs and analyses of distributed spatiotemporal responses across visual cortex find no reliable category representations in 3- to 4-month-olds, either when examining mean response across an ROI or in distributed spatiotemporal patterns across visual cortex. The earliest categorical responses we find are for faces, and they emerge at 4–6 months of age.</p><p>Is it possible that there are some category representations in 3- to 4-month-olds, but we lack the sensitivity to measure them? We believe this is unlikely, because (i) we can measure significant visual responses from the same 3- to 4-month-olds, (ii) with the same amount of data, we can measure category-selective responses and decode category information from distributed spatiotemporal responses in infants older than 4 months and in adults. As using SSVEP to study high-level representations is a nascent field (<xref ref-type="bibr" rid="bib27">Gentile and Rossion, 2014</xref>; <xref ref-type="bibr" rid="bib72">Retter et al., 2020</xref>; <xref ref-type="bibr" rid="bib69">Peykarjou, 2022</xref>), future work can further examine how SSVEP parameters such as stimulus and target category presentation rate may affect the sensitivity of measurements in infants (see review by <xref ref-type="bibr" rid="bib69">Peykarjou, 2022</xref>).</p><p>Our findings together with a recent fMRI study in 2- to 10-month-olds (<xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>) provide accumulating evidence for multiple visual categories representations in infants’ brains before the age of one. However, there are also differences across studies. The earliest we could find reliable group-level category-selective responses for faces was 4- to 6-month-olds and for limbs and corridors only after 6 months of age. In contrast, <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>, report category-selective responses to faces, bodies, and scenes in example 4- to 5-month-olds. Group average data in their study found significant face- and place-selective responses in infants’ VTC but not in LOTC, and significant body-selective responses in LOTC, but not VTC. Because <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>, report group-averaged data across infants spanning 8 months, their study does not provide insights to the time course of this development. We note that, the studies differ in several ways: (i) measurement modalities (fMRI in <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>, and EEG here), (ii) the types of stimuli infants viewed: in <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>, infants viewed isolated, colored, and moving stimuli, but in our study, infants viewed still, gray-level images on phase-scrambled backgrounds, which were controlled for several low-level properties, and (iii) contrasts used to detect category-selective responses, whereby in <xref ref-type="bibr" rid="bib49">Kosakowski et al., 2022</xref>, the researchers identified within predefined parcels – the top 5% of voxels that responded to the category of interest vs. objects, here we contrasted the category of interest vs. all other categories the infant viewed. Thus, future research is necessary to determine whether differences between findings are due to differences in measurement modalities, stimulus format, and data analysis choices.</p></sec><sec id="s3-2"><title>Face representations emerge around 4–6 months of age</title><p>Recognizing faces (e.g. a caregiver’s face) is crucial for infants’ daily lives. Converging evidence from many studies suggest that infants have significant and reliable face-selective neural responses at 4–6 months of age (<xref ref-type="bibr" rid="bib25">Farzin et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>; <xref ref-type="bibr" rid="bib18">Deen et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Guy et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Halit et al., 2004</xref>). While some studies report responses to face-like (high-contrast paddle-like) stimuli in newborns (<xref ref-type="bibr" rid="bib44">Johnson et al., 1991</xref>; <xref ref-type="bibr" rid="bib13">Buiatti et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Goren et al., 1975</xref>) and significant visual evoked responses to faces in 3-month-olds (<xref ref-type="bibr" rid="bib37">Halit et al., 2003</xref>; <xref ref-type="bibr" rid="bib15">Cassia et al., 2006</xref>; <xref ref-type="bibr" rid="bib69">Peykarjou, 2022</xref>; <xref ref-type="bibr" rid="bib82">Tzourio-Mazoyer et al., 2002</xref>), these studies have largely compared responses to an isolated face vs. another isolated object. In contrast, we do not find reliable face-selective responses (<xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig4">4</xref>) or reliable distributed representations (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6</xref>) to faces in 3- to 4-month-olds when responses to faces are contrasted to many other items and when stimuli are shown on a background rather than in isolation. Our findings are consistent with longitudinal research in macaques showing that robust cortical selectivity to faces takes several months to emerge (<xref ref-type="bibr" rid="bib54">Livingstone et al., 2017</xref>) and support the hypothesis that experience with faces is necessary for the development of cortical face selectivity (<xref ref-type="bibr" rid="bib5">Arcaro et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Livingstone et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Arcaro et al., 2017</xref>).</p><p>Our data also reveal that face-selective responses and distributed representations to faces become more robust in 6- to 8-month-olds and remain robust in 12- to 15-month-olds. For example, successful decoding of faces in the group level was observed in 80% of individual infants based on several minutes of EEG data. Reliable distributed spatiotemporal responses to different images of faces become significantly different from responses to images from different categories. This robust decoding has important clinical ramifications as it may serve as an early biomarker for cortical face processing, which is important for early detection of social and cognitive developmental disorders such as Autism (<xref ref-type="bibr" rid="bib73">Rossion, 2020</xref>; <xref ref-type="bibr" rid="bib83">Vettori et al., 2019</xref>) and Williams syndrome (<xref ref-type="bibr" rid="bib24">Farran et al., 2020</xref>). Future research is necessary for elucidating the relationship between the development of brain responses to faces to infant behavior. For example, it is interesting that at 6 months of age, when we find robust face representations, infants also start to exhibit recognition of familiar faces (like parents) and stranger anxiety (<xref ref-type="bibr" rid="bib47">Kobayashi et al., 2020</xref>).</p><p>One fascinating aspect of the development of cortical face selectivity is that among the categories we tested, selectivity to faces seems to emerge the earliest at around 4 months of age, yet the development of selectivity and distributed representations to faces is protracted compared to objects and places (<xref ref-type="bibr" rid="bib29">Golarai et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Peelen et al., 2009</xref>; <xref ref-type="bibr" rid="bib74">Scherf et al., 2007</xref>). Indeed, in both our data and prior work, face-selective responses and distributed representations to faces in infants are immature compared to adults (<xref ref-type="bibr" rid="bib17">Conte et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Xie et al., 2022</xref>), and a large body of work has shown that face selectivity (<xref ref-type="bibr" rid="bib64">Nordt et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Golarai et al., 2007</xref>; <xref ref-type="bibr" rid="bib74">Scherf et al., 2007</xref>; <xref ref-type="bibr" rid="bib79">Taylor et al., 1999</xref>; <xref ref-type="bibr" rid="bib67">Peelen et al., 2009</xref>; <xref ref-type="bibr" rid="bib14">Cantlon et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Cohen et al., 2019</xref>) and distributed representations to faces (<xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>) continue to develop during childhood and adolescence. This suggests that not only experience during infancy but also life-long experience with faces, sculpts cortical face selectivity. We speculate that the extended cortical plasticity for faces may be due to both the expansion of social circles (family, friends, and acquaintances) across the lifespans and also the changing statistics of faces we socialize with (e.g. child and adult faces have different appearance).</p></sec><sec id="s3-3"><title>A new insight about cortical development: different category representations emerge at different times during infancy</title><p>To our knowledge, our study is the first to examine the development of both ROI level and spatiotemporal distributed responses in infants across the first year of life. We note that both analyses find that category information to faces develops before other categories. However, there are also some differences across analyses (<xref ref-type="fig" rid="fig7">Figure 7</xref>). For example, for limbs and corridors we find significant category-selective responses at the ROI level in lateral occipitotemporal ROIs starting at 6–8 months but no reliable distinct distributed responses across visual cortex at this age. In contrast, for cars, we find an opposite pattern where there is a distinct spatiotemporal pattern in 12- to 15-month-olds even as there is no significant car-selective response in the ROI level. As these approaches have different sensitivities, they reveal insights to the nature of the underlying representations. For example, as visible in <xref ref-type="fig" rid="fig4">Figure 4</xref>, limbs and corridor have a clear category-selective waveform in both in 6- to 8- and 12- to 15-months-olds, but the waveform of limbs and its spatial distribution is not that different from that to corridors, which may explain why distinctiveness of spatiotemporal patterns for limbs is low in 6- to 8-month-olds (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Likewise, even as there is no significant response for cars (<xref ref-type="fig" rid="fig4">Figure 4e</xref>), its spatiotemporal pattern is consistently different than for other categories giving rise to a distinctive spatiotemporal response by 12 months (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>In sum, the key finding from our study is that the development of category selectivity during infancy is non-uniform: face-selective responses and representations of distributed patterns develop before representations to limbs and other categories. We hypothesize that this differential development of visual category representations may be due to differential visual experience with these categories during infancy. This hypothesis is consistent with behavioral research using head-mounted cameras that revealed that the visual input during early infancy is dense with faces, while hands become more prevalent in the visual input later in development and especially when in contact with objects (<xref ref-type="bibr" rid="bib26">Fausey et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Jayaraman et al., 2017</xref>). Additionally, a large body of research has suggested that young infants preferentially look at faces and face-like stimuli (<xref ref-type="bibr" rid="bib76">Spriet et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Mondloch et al., 1999</xref>; <xref ref-type="bibr" rid="bib66">Pascalis et al., 1995</xref>; <xref ref-type="bibr" rid="bib44">Johnson et al., 1991</xref>), as well as look longer at faces than other objects (<xref ref-type="bibr" rid="bib26">Fausey et al., 2016</xref>), indicating that not only the prevalence of faces in babies’ environments but also longer looking times may drive the early development of face representations. Further supporting the role of visual experience in the formation of category selectivity is a study that found that infant macaques that are reared without seeing faces do not develop face selectivity but develop selectivity to other categories in their environment like body parts (<xref ref-type="bibr" rid="bib4">Arcaro et al., 2017</xref>). An alternative hypothesis is that differential development of category representations is of maturational origin. For example, we found differences in the temporal dynamics of visual responses among four infant age groups, which suggests that the infant’s visual system is still developing during the first year of life. While underlying maturational mechanisms are yet unknown they may include myelination and cortical tissue maturation (<xref ref-type="bibr" rid="bib35">Grotheer et al., 2022</xref>; <xref ref-type="bibr" rid="bib59">Natu et al., 2021</xref>; <xref ref-type="bibr" rid="bib81">Tooley et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Adibpour et al., 2024</xref>; <xref ref-type="bibr" rid="bib50">Lebenberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Gilmore et al., 2018</xref>). Future studies can test experience-driven vs. maturational alternatives by examining infants’ visual diet, looking behavior, and anatomical brain development and examine responses using additional behaviorally relevant categories such as food (<xref ref-type="bibr" rid="bib6">Bannert and Bartels, 2022</xref>; <xref ref-type="bibr" rid="bib42">Jain et al., 2023</xref>; <xref ref-type="bibr" rid="bib68">Pennock et al., 2023</xref>). These measurements can test how environmental and individual differences in visual experiences may impact infants’ developmental trajectories. Specifically, a visual experience account predicts that differences in visual experience would translate into differences in brain development, but a maturational account predicts that visual experience will have no impact on the development of category representations.</p><p>Together our findings not only suggest that visual experience is necessary for the development of visual category representations, including faces, but also necessitate a rethinking of how visual category representations develop in infancy. Moreover, this differential development during infancy is evident even for categories that have evolutionary importance and may have innate substrates such as faces, bodies, and places (<xref ref-type="bibr" rid="bib46">Kanwisher, 2010</xref>; <xref ref-type="bibr" rid="bib78">Sugita, 2009</xref>; <xref ref-type="bibr" rid="bib55">Mahon and Caramazza, 2011</xref>; <xref ref-type="bibr" rid="bib9">Bi et al., 2016</xref>). Finally, our findings have important ramifications for theoretical and computational models of visual development as well as for the assessment of atypical infant development.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Ethical permission for the study was obtained from the Institutional Review Board of Stanford University. Parents of the infant participants provided written informed consent prior to their first visit and also prior to each session if they came for multiple sessions. Participants were paid 20$/hr for participation. Participants were recruited via ads on social media (Facebook and Instagram).</p><p>Sixty-two full-term, typically developing infants were recruited. Twelve participants were part of an ongoing longitudinal study that obtained both anatomical MRI and EEG data in infants. Some of the infants participated in both studies and some only in one of the studies. Infants were recruited at around newborn, 3 months, 6 months, and 12 months. We did not recruit infants between 8 and 12 months of age because around 9 months there is little contrast between gray and white matter in anatomical MRI scans that were necessary for the MRI study. Infants came for several sessions spanning ~3 months apart (seven 3- to 4-month-olds, three 4- to 6-month-olds, eight 6- to 8-month-olds, and twelve 12- to 15-month-olds). Data from 19 infants (nine 3- to 4-month-olds, six 4- to 6-month-olds, and eight 6- to 8-month-olds; among whom seven were longitudinal) were acquired in two visits within a 2-week span to obtain a sufficient number of valid data epochs. <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> contains participants’ demographic information (sex and race). The youngest infants were 3 months of age, as the EEG setup requires the infants to be able to hold their head and look at the screen in front of them. 23 adults (14 females) also participated in the study. All participants had normal/corrected-to-normal vision and provided written informed consent.</p><p>Data e<italic>xclusion criteria:</italic> We excluded participants who had less than 20 valid epochs (1.1667 s/epoch) per category, had noise/muscular artifacts during the EEG recordings, couldn’t record data, or had no visual responses over the occipital electrodes. As such, we excluded (i) five infants due to an insufficient number of epochs, (ii) two infants who had no visual responses, (iii) ten infants due to technical issues during data collection, and (iv) three adults due to excessive noise/muscular artifacts during EEG. In total, we report data from 45 infants (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) and 20 adults (13 females, 19–38 years) that met inclusion criteria.</p></sec><sec id="s4-2"><title>Visual stimuli</title><p>Natural grayscale images of adult faces, limbs, corridors, characters, and cars are used as stimuli, with 144 images per category from the fLOC image database (<ext-link ext-link-type="uri" xlink:href="https://github.com/VPNL/fLoc">https://github.com/VPNL/fLoc</ext-link>, copy archived at <xref ref-type="bibr" rid="bib89">Yan, 2024</xref>; <xref ref-type="bibr" rid="bib77">Stigliani et al., 2015</xref>). The size, view, and retinal position of the items varied, and the items were overlaid on phase-scrambled backgrounds that were generated from a randomly drawn image in the stimulus set. The images were also controlled for multiple low-level differences between stimuli of different categories including their luminance, contrast, similarity, and spatial frequency power distributions using the SHINE toolbox (<xref ref-type="bibr" rid="bib85">Willenbockel et al., 2010</xref>). As only five of ten categories from <xref ref-type="bibr" rid="bib77">Stigliani et al., 2015</xref>, were used, we evaluated the stimuli used in our experiments to test if they differed in (i) contrast, (ii) luminance, (iii) similarity, and (iv) spatial frequency. Results show that categories were largely matched on most metrics (<xref ref-type="fig" rid="fig1">Figure 1B</xref> and Appendix). The stimuli were presented on a gamma-corrected OLED monitor screen (SONY PVM-2451; SONY Corporation, Tokyo Japan) at a screen resolution of 1920 ×1080 pixels and a monitor refresh rate of 60 Hz. When viewed from 70 cm away, each image extended a visual angle of approximately 12°.</p></sec><sec id="s4-3"><title>EEG protocol</title><p>The experiments were conducted in a calm, dimly illuminated lab room. Stimuli were presented using custom stimulus presentation software with millisecond timing precision. During testing, infant participants were seated on their parent’s laps in front of the screen at a distance of 70 cm. One experimenter stood behind the presentation screen to monitor where the infant was looking. The visual presentation was paused if the infant looked away from the screen and was continued when the infant looked again at the center of the screen. To motivate infants to fixate and look at the screen, we presented at the center of the screen small (~1°) colored cartoon images such as butterflies, flowers, and ladybugs. They were presented in random order with durations uniformly distributed between 1 and 1.5 s. For adults, we used a fixation cross of the same size instead of the cartoons and asked the participants to fixate and indicate when the fixation’s color changed by pressing a space bar key on a keyboard. EEG measurements for infant participants continued until the infant no longer attended the screen and we obtained between 2 and 12 different 70 s sequences per individual. For adult participants, we acquired 12 sequences per individual.</p><p>A frequency-tagging paradigm (<xref ref-type="bibr" rid="bib21">de Heering and Rossion, 2015</xref>; <xref ref-type="bibr" rid="bib25">Farzin et al., 2012</xref>) was used to measure brain responses. In the experiment, randomly selected images from five categories were presented sequentially at a rate of 4.286 Hz (~233 ms per image) with no inter stimulus interval during each 70 s sequence. For each condition, one category was determined as the target category; for this category random selected images from that category were presented first and followed by four images randomly drawn from the other four categories with no regular order (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The target images are therefore presented periodically at 0.857 Hz (i.e. 4.286 Hz/5), but the intervals between sequential presentations of images from the other four categories was not periodic. The probability of image occurrences across categories was equal at 20%. The experiment had five conditions, one for each of the following target categories: faces, limbs, corridors, characters, and cars. Each 70 s experimental sequence was composed of five 14 s long conditions which included a 1.1667 s stimulus fade-in and a 1.1667 s stimulus fade-out. The order of the category conditions was randomized within each 70 s sequence. No image was repeated within a sequence. Two presentation frequencies were embedded in the experiment: (i) the image frequency (4.286 Hz), which is predicted to elicit visual responses to all stimuli over occipital visual cortex, and, (ii) the category frequency (0.857 Hz), which is predicted to elicit a category-selective response over lateral occipital-temporal electrodes.</p></sec><sec id="s4-4"><title>EEG acquisition</title><p>EEG data were recorded at 500 Hz from a 128-channel EGI High-Density Geodesic Sensor Net. For infants, the net was connected to a NetAmps 300 amplifier (Electrical Geodesics, Inc, Eugene, OR, USA). For the adults, the net was connected to a NetAmps400 amplifier. The EEG recording was referenced online to a single vertex (electrode Cz) and the channel impedance was kept below 50 KΩ.</p></sec><sec id="s4-5"><title>Pre-processing</title><p>EEG recordings were down-sampled to 420 Hz and were filtered using a 0.03–50 Hz bandpass filter with custom signal processing software. Artifact rejection was performed in two steps. For infants, first, channels with more than 20% of samples exceeding a 100–150 μV amplitude threshold were replaced with the average amplitude of its six nearest-neighbor channels. The continuous EEG signals were then re-referenced to the common average of all channels and segmented into 1166.7 ms epochs (i.e. duration of five stimuli starting with a target category image followed with four images drawn from the rest four categories). Epochs with more than 15% of time samples exceeding threshold (150–200 μV) were excluded further on a channel-by-channel basis (<xref ref-type="bibr" rid="bib63">Norcia et al., 2017</xref>). For adults, the two-step artifact rejection was performed with different criteria as EEG response amplitudes are lower in adults than infants (<xref ref-type="bibr" rid="bib63">Norcia et al., 2017</xref>). EEG channels with more than 15% of samples exceeding a 30 μV amplitude threshold were replaced by the average value of their neighboring channels. Then the EEG signals were re-referenced to the common average of all channels and segmented into 1.1667 s epochs. Epochs with more than 10% of time samples exceeding threshold (30–80 μV) were excluded on a channel-by-channel basis (<xref ref-type="bibr" rid="bib48">Kohler et al., 2020</xref>).</p><p><xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref> shows the number of epochs (1.1667 s each) we acquired before and after data pre-processing summing across all five categories. We used data after pre-processing for further analyses. There was no significant difference in the number of pre-processed epochs across infant age groups (F<sub>(3,57)</sub> = 1.5, p=0.2). The number of electrodes being interpolated for each age group were 10.0±4.8 for 3- to 4-month-olds, 9.9 ± 3.7 for 4- to 6-month-olds, 9.9 ±3.9 for 6- to 8-month-olds, and 7.7 ±4.7 for 12- to 15-month-olds. There was no significant difference in the number of electrodes being interpolated across infant age groups (F<sub>(3,55)</sub> = 0.78, p=0.51).</p></sec><sec id="s4-6"><title>Univariate EEG analyses</title><p>Both image-update and categorical EEG visual responses are reported in the frequency and time domain over three ROIs: two occipito-temporal ROIs (LOT: channels 57, 58, 59, 63, 64, 65, and 68; ROT channels: 90, 91, 94, 95, 96, 99, and 100) and one occipital ROI (channels 69, 70, 71, 74, 75, 76, 82, 83, and 89). These ROIs were selected a priori based on a previously published study (<xref ref-type="bibr" rid="bib86">Xie et al., 2019</xref>). We further removed several channels in these ROIs for two reasons: (i) Three outer rim channels (i.e. 73, 81, and 88) were not included in the occipital ROI for further data analysis for both infant and adult participants because they were consistently noisy. (ii) Three channels (66, 72, and 84) in the occipital ROI, one channel (50) in the LOT ROI, and one channel (101) in the ROT ROI were removed because they did not show substantial responses in the group-level analyses.</p><sec id="s4-6-1"><title>Frequency domain analysis</title><p>Individual participant’s pre-processed EEG signals for each stimulus condition were averaged over two consecutive epochs (2.3334 s). The averaged time courses for each participant were then converted to the frequency domain at a frequency resolution of 0.4286 Hz via a discrete Fourier transform (DFT). The frequency bins of interest are at exactly every other bin in the frequency spectrum. The real and imaginary Fourier coefficients for each of the categorical and image-update responses for each condition were averaged across participants (vector averaging) to obtain a group-level estimate. The amplitudes of response were then computed from the coherently averaged vector. Hotelling’s T2 statistic (<xref ref-type="bibr" rid="bib84">Victor and Mast, 1991</xref>) was used to test whether response amplitudes were significantly different from zero. We used Benjamini’s &amp; Hochberg’s FDR procedure to correct for multiple comparisons.</p><sec id="s4-6-1-1"><title>Image-update visual responses (image frequency)</title><p>The amplitude and phase of the evoked response at the image presentation rate and its first three harmonics (4.286 Hz, 8.571 Hz, 12.857 Hz, and 17.143 Hz).</p></sec><sec id="s4-6-1-2"><title>Categorical responses (category frequency)</title><p>The amplitude and phase of the response at the category repetition frequency and its second harmonic (0.857 Hz, 1.714 Hz) for each category condition.</p></sec></sec></sec><sec id="s4-7"><title>Time domain analyses</title><p>Pre-processed time domain EEG signals of each participant were low-passed filtered with a 30 Hz cut-off. The raw EEG signals contain many components including categorical responses (0.857 Hz and harmonics), general visual responses (4.286 Hz and harmonics), and noise. To separate out the temporal waveforms of these two responses, we first transformed the epoch-averaged (2.3334 s) time series of each condition into frequency domain using a DFT. Then, we used an inverse DFT to transform back to the time domain keeping only the responses at the category frequency and its harmonics, zeroing the other frequencies. The same approach was used to separate the visual responses by using an inverse DFT transform of the responses at 4.286 Hz and harmonics.</p></sec><sec id="s4-8"><title>Categorical responses</title><p>We kept responses at frequencies of interest (0.857 Hz and its harmonics up to 30 Hz, excluding the harmonic frequencies that overlapped with the image frequency and its harmonics) and zeroed responses in other frequencies. Then we applied an inverse Fourier transform to transform the data to the time domain. We further segmented the time series into 1.1667 s epochs and averaged across these epochs for each condition and individual. The mean and standard error across participants were computed for each condition at each time point.</p></sec><sec id="s4-9"><title>Image-update visual responses</title><p>A similar procedure was performed except that frequencies of interest are 4.286 Hz and its harmonics, and the rest were zeroed. As temporal waveforms for image-update responses were similar across different category conditions, we averaged waveforms across all five conditions and report the mean response (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec><sec id="s4-10"><title>Statistical analyses</title><p>To determine time windows in which amplitudes were significantly different from zero for each condition, we used a cluster-based nonparametric permutation t-test, 10,000 permutations, with a threshold of p&lt;0.05, two-tailed on the post-stimulus onset time points (0–1167 ms) (<xref ref-type="bibr" rid="bib3">Appelbaum et al., 2006</xref>; <xref ref-type="bibr" rid="bib10">Blair and Karniski, 1993</xref>). The null hypothesis is that the evoked waveforms are not different from zero at any time point. For each permutation, we assigned random signs to the data of individual participants and computed the group-level difference against zero using a t-test. We then calculated the cluster-level statistic as the sum of t-values in the consecutive time points with p-values less than 0.05 (<xref ref-type="bibr" rid="bib57">Maris and Oostenveld, 2007</xref>). We calculated the maximum cluster-level statistic for each permutation to generate a nonparametric reference distribution of cluster-level statistics. We rejected the null hypothesis if the cluster-level statistic for any consecutive time points in the original data was larger than 97.5% or smaller than 2.5% of the values in the null distribution.</p></sec><sec id="s4-11"><title>Decoding analyses</title><sec id="s4-11-1"><title>Group level</title><p>We used an LOOCV classifier to test if spatiotemporal responses patterns to each of the five categories were reliable across participants. The classifier was trained on averaged data from N–1 participants and tested on how well it predicted the category the left-out participant was viewing from their brain activations. This procedure was repeated for each left-out participant. We calculated the averaged category temporal waveform for each category across channels of our three ROIs: seven LOT, nine occipital, and seven ROT, as the exact location of the channels varies across individuals. Then, we concatenated the waveform from the three ROIs to form a spatiotemporal response vector (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). At each iteration, the LOOCV classifier computed the correlation between each of the five category vectors from the left-out participant (test data, for an unknown stimulus) and each of the mean spatiotemporal vectors across the N–1 participants (training data, labeled data). The winner-take-all classifier classifies the test vector to the category of the training vector that yields the highest correlation with the training vector (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). For a given test pattern, correct classification yields a score of 1 and an incorrect classification yields a score of 0. For each left-out infant, we computed the percentage correct across all categories, and then the mean decoding performance across all participants in an age group (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p></sec><sec id="s4-11-2"><title>Individual level</title><p>Similar to group level with two differences: (i) All analyses were done within an individual using a split-half approach. That is, the classifier was trained on one half of the data (i.e. odd or even trials) and tested on the other half of the data. (ii) Spatiotemporal patterns for each category used the concatenated waveforms across 23 channels spanning the occipital and bilateral occipitotemporal ROIs.</p></sec><sec id="s4-11-3"><title>Category distinctiveness (<xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>)</title><p>Category distinctiveness is defined as the difference between the similarity (correlation coefficient) of spatiotemporal responses within a category across odd and even splits using different images and the average between-category similarity of spatiotemporal responses across odd and even splits (<xref ref-type="bibr" rid="bib65">Nordt et al., 2023</xref>). Distinctiveness is higher when the within-category similarity is positive and the between-category similarity is negative and varies from –2 (no category information) to 2 (maximal category information). We computed category distinctiveness for each of the five categories as in each infant and determined if it varied from 3 to 15 months of age.</p></sec></sec><sec id="s4-12"><title>Statistical analyses of developmental effects</title><p>To quantify developmental effects, we used LMMs (<xref ref-type="bibr" rid="bib11">Bosker and Snijders, 2011</xref>), with the ‘fitlme’ function in MATLAB version 2021b (MathWorks, Inc). LMMs allow explicit modeling of both within-subject effects (e.g. longitudinal measurements) and between-subject effects (e.g. cross-sectional data) with unequal number of points per participants, as well as examine main and interactive effects of both continuous (age) and categorical (e.g. stimulus category) variables. We used random-intercept models that allow the intercept to vary across participants (term: 1|participant). In all LMMs, we measured development as a function of log 10(age in days) as development is typically faster earlier on. Indirect evidence comes from neuroimaging and post-mortem studies showing that the structural development of infants’ brains is nonlinear, with development in the first 2 years being rapid, especially the first year (<xref ref-type="bibr" rid="bib28">Gilmore et al., 2018</xref>).</p><p>We report slope (rate of development), interaction effects, and their significance. <xref ref-type="table" rid="table1">Table 1</xref> summarizes LMMs used in this study.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Linear mixed models (LMMs).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Variable</th><th align="left" valign="bottom">LMM formula</th><th align="left" valign="bottom">Results</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Peak latency: latency of the peak waveform in each time window; window 1: 60–90 ms or window 2: 90–160 ms for 3- to 4-month-olds, and 90–110 ms for other age groups</bold></td><td align="left" valign="bottom">Peak latency ~1 + log10(age in days)×time window + (1|participant)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figure 2G</xref>.<break/><xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Peak latency ~1 + log10(age in days) + (1|participant)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figure 2H</xref>. <xref ref-type="table" rid="app1table5">Appendix 1—table 5</xref></td></tr><tr><td align="left" valign="bottom"><bold>Category-selective response amplitude:</bold><break/><bold>root mean square (RMS) of category-selective response at category frequency (0.857 Hz) and its first harmonic (1.714 Hz</bold>)</td><td align="left" valign="bottom">Response amplitude ~1 + log10(age in days)×category + (1|participant)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>Peak amplitude: peak response in a 400–700 ms time window</bold></td><td align="left" valign="bottom">Peak amplitude ~1 + log10(age in days)×category + (1|participant)</td><td align="left" valign="bottom"><xref ref-type="table" rid="app1table7">Appendix 1—table 7</xref></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Peak amplitude ~1 + log10(age in days) + (1|participant)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4</xref>. <xref ref-type="table" rid="app1table8">Appendix 1—table 8</xref></td></tr><tr><td align="left" valign="bottom"><bold>Category distinctiveness of spatiotemporal responses for each of the five categories</bold></td><td align="left" valign="bottom">Category distinctiveness ~log10(age in days)×category + (1|participant)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Category distinctiveness ~log10(age in days) + (1|participant)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6B</xref></td></tr></tbody></table></table-wrap><sec id="s4-12-1"><title>Analysis of noise</title><p>To test whether EEG noise levels vary with age, e.g. whether noise in the EEG data is larger in the younger infants than older ones, we quantified the response amplitudes in the occipital ROI in the frequency domain, at frequency bins next to the category and image frequency bins (white bars in <xref ref-type="fig" rid="fig2">Figure 2A–D</xref>, right panel). The noise level was quantified as the amplitude of response up to 8.571 Hz excluding image presentation frequencies (4.286 Hz and harmonics) and category frequencies (0.857 Hz and harmonics) as this frequency range includes the relevant main harmonics (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). We used a LMM to test if noise varies with age, with participant as a random effect:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mn>10</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We found no significant differences in noise amplitude across infant age groups (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, mean amplitude across the first five noise bins: β<sub>age</sub> = –0.005, 95% CI: –0.12 to –0.11, t<sub>(59)</sub> = –0.09, p=0.93; mean noise across the first 10 bins: β<sub>age</sub> = .04, 95% CI: –0.03 to –0.12, t<sub>(59)</sub> = 1.16, p=0.25, LMMs with age (log transformed) as fixed effects and participant as a random effect).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation</p></fn><fn fn-type="con" id="con3"><p>Data curation</p></fn><fn fn-type="con" id="con4"><p>Data curation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Ethical permission (eprotocol number: 48634) for the study was obtained from the Institutional Review Board of Stanford University. Parents of the infant participants provided written informed consent prior to their first visit and also prior to each session if they came for multiple sessions.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-100260-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Individual preprocessed EEG data, and code for all analyses are available on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/VPNL/InfantObjectCategorization">https://github.com/VPNL/InfantObjectCategorization</ext-link> (copy archived at <xref ref-type="bibr" rid="bib88">Yan, 2023</xref>). Individual EEG data is also available on OSF (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/G5FTA">https://doi.org/10.17605/OSF.IO/G5FTA</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Infant object categorization</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/G5FTA</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by grants from the Wu Tsai Neurosciences Institute of Stanford University, the Human Centered Artificial Intelligence Institute of Stanford University to KGS and AMN.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Adibpour</surname><given-names>P</given-names></name><name><surname>Nasser</surname><given-names>H</given-names></name><name><surname>Pedoux</surname><given-names>A</given-names></name><name><surname>Devisscher</surname><given-names>L</given-names></name><name><surname>Elbaz</surname><given-names>N</given-names></name><name><surname>Ghozland</surname><given-names>C</given-names></name><name><surname>Hinnekens</surname><given-names>E</given-names></name><name><surname>Neumane</surname><given-names>S</given-names></name><name><surname>Kabdebon</surname><given-names>C</given-names></name><name><surname>Lefebvre</surname><given-names>A</given-names></name><name><surname>Kaminska</surname><given-names>A</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name><name><surname>Heneau</surname><given-names>A</given-names></name><name><surname>Sibony</surname><given-names>O</given-names></name><name><surname>Alison</surname><given-names>M</given-names></name><name><surname>Delanoë</surname><given-names>C</given-names></name><name><surname>Delorme</surname><given-names>R</given-names></name><name><surname>Barbu-Roth</surname><given-names>M</given-names></name><name><surname>Biran</surname><given-names>V</given-names></name><name><surname>Dubois</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Characterizing the Temporal Dynamics and Maturation of Brain Activity during Sleep: An EEG Microstate Study in Preterm and Full-Term Infants</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.03.19.585608</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00962-x</pub-id><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appelbaum</surname><given-names>LG</given-names></name><name><surname>Wade</surname><given-names>AR</given-names></name><name><surname>Vildavski</surname><given-names>VY</given-names></name><name><surname>Pettet</surname><given-names>MW</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cue-invariant networks for figure and background processing in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>11695</fpage><lpage>11708</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2741-06.2006</pub-id><pub-id pub-id-type="pmid">17093091</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Ponce</surname><given-names>CR</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing faces is necessary for face-domain formation</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1404</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1038/nn.4635</pub-id><pub-id pub-id-type="pmid">28869581</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Universal mechanisms and the development of the face network: what you see is what you get</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>341</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014917</pub-id><pub-id pub-id-type="pmid">31226011</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannert</surname><given-names>MM</given-names></name><name><surname>Bartels</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual cortex: big data analysis uncovers food specificity</article-title><source>Current Biology</source><volume>32</volume><fpage>R1012</fpage><lpage>R1015</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.08.068</pub-id><pub-id pub-id-type="pmid">36220088</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayet</surname><given-names>L</given-names></name><name><surname>Zinszer</surname><given-names>BD</given-names></name><name><surname>Reilly</surname><given-names>E</given-names></name><name><surname>Cataldo</surname><given-names>JK</given-names></name><name><surname>Pruitt</surname><given-names>Z</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal dynamics of visual representations in the infant brain</article-title><source>Developmental Cognitive Neuroscience</source><volume>45</volume><elocation-id>100860</elocation-id><pub-id pub-id-type="doi">10.1016/j.dcn.2020.100860</pub-id><pub-id pub-id-type="pmid">32932205</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrmann</surname><given-names>M</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A vision of graded hemispheric specialization</article-title><source>Annals of the New York Academy of Sciences</source><volume>1359</volume><fpage>30</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1111/nyas.12833</pub-id><pub-id pub-id-type="pmid">26199998</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Object domain and modality in the ventral visual pathway</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>282</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.02.002</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blair</surname><given-names>RC</given-names></name><name><surname>Karniski</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>An alternative method for significance testing of waveform difference potentials</article-title><source>Psychophysiology</source><volume>30</volume><fpage>518</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1993.tb02075.x</pub-id><pub-id pub-id-type="pmid">8416078</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bosker</surname><given-names>R</given-names></name><name><surname>Snijders</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling</source><publisher-name>Sage</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bugatus</surname><given-names>L</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task alters category representations in prefrontal but not high-level visual cortex</article-title><source>NeuroImage</source><volume>155</volume><fpage>437</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.062</pub-id><pub-id pub-id-type="pmid">28389381</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buiatti</surname><given-names>M</given-names></name><name><surname>Di Giorgio</surname><given-names>E</given-names></name><name><surname>Piazza</surname><given-names>M</given-names></name><name><surname>Polloni</surname><given-names>C</given-names></name><name><surname>Menna</surname><given-names>G</given-names></name><name><surname>Taddei</surname><given-names>F</given-names></name><name><surname>Baldo</surname><given-names>E</given-names></name><name><surname>Vallortigara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical route for facelike pattern processing in human newborns</article-title><source>PNAS</source><volume>116</volume><fpage>4625</fpage><lpage>4630</lpage><pub-id pub-id-type="doi">10.1073/pnas.1812419116</pub-id><pub-id pub-id-type="pmid">30755519</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cantlon</surname><given-names>JF</given-names></name><name><surname>Pinel</surname><given-names>P</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Pelphrey</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical representations of symbols, objects, and faces are pruned back during early childhood</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>191</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq078</pub-id><pub-id pub-id-type="pmid">20457691</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cassia</surname><given-names>VM</given-names></name><name><surname>Kuefner</surname><given-names>D</given-names></name><name><surname>Westerlund</surname><given-names>A</given-names></name><name><surname>Nelson</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A behavioural and ERP investigation of 3-month-olds’ face preferences</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>2113</fpage><lpage>2125</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.11.014</pub-id><pub-id pub-id-type="pmid">16368116</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Weigelt</surname><given-names>S</given-names></name><name><surname>Feather</surname><given-names>J</given-names></name><name><surname>Kell</surname><given-names>AJ</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Zöllei</surname><given-names>L</given-names></name><name><surname>Wald</surname><given-names>L</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Representational similarity precedes category selectivity in the developing ventral visual pathway</article-title><source>NeuroImage</source><volume>197</volume><fpage>565</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.010</pub-id><pub-id pub-id-type="pmid">31077844</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conte</surname><given-names>S</given-names></name><name><surname>Richards</surname><given-names>JE</given-names></name><name><surname>Guy</surname><given-names>MW</given-names></name><name><surname>Xie</surname><given-names>W</given-names></name><name><surname>Roberts</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Face-sensitive brain responses in the first year of life</article-title><source>NeuroImage</source><volume>211</volume><elocation-id>116602</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116602</pub-id><pub-id pub-id-type="pmid">32044434</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deen</surname><given-names>B</given-names></name><name><surname>Richardson</surname><given-names>H</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Organization of high-level visual cortex in human infants</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13995</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13995</pub-id><pub-id pub-id-type="pmid">28072399</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Pegado</surname><given-names>F</given-names></name><name><surname>Braga</surname><given-names>LW</given-names></name><name><surname>Ventura</surname><given-names>P</given-names></name><name><surname>Nunes Filho</surname><given-names>G</given-names></name><name><surname>Jobert</surname><given-names>A</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Kolinsky</surname><given-names>R</given-names></name><name><surname>Morais</surname><given-names>J</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>How learning to read changes the cortical networks for vision and language</article-title><source>Science</source><volume>330</volume><fpage>1359</fpage><lpage>1364</lpage><pub-id pub-id-type="doi">10.1126/science.1194140</pub-id><pub-id pub-id-type="pmid">21071632</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Monzalvo</surname><given-names>K</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The emergence of the visual word form: Longitudinal evolution of category-specific ventral visual areas during reading acquisition</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004103</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004103</pub-id><pub-id pub-id-type="pmid">29509766</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heering</surname><given-names>A</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rapid categorization of natural face images in the infant right hemisphere</article-title><source>eLife</source><volume>4</volume><elocation-id>e06564</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06564</pub-id><pub-id pub-id-type="pmid">26032564</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farran</surname><given-names>EK</given-names></name><name><surname>Mares</surname><given-names>I</given-names></name><name><surname>Papasavva</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Ewing</surname><given-names>L</given-names></name><name><surname>Smith</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Characterizing the neural signature of face processing in Williams syndrome via multivariate pattern analysis and event related potentials</article-title><source>Neuropsychologia</source><volume>142</volume><elocation-id>107440</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107440</pub-id><pub-id pub-id-type="pmid">32179101</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farzin</surname><given-names>F</given-names></name><name><surname>Hou</surname><given-names>C</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Piecing it together: infants’ neural responses to face and object structure</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1167/12.13.6</pub-id><pub-id pub-id-type="pmid">23220577</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fausey</surname><given-names>CM</given-names></name><name><surname>Jayaraman</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>From faces to hands: Changing visual input in the first two years</article-title><source>Cognition</source><volume>152</volume><fpage>101</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.03.005</pub-id><pub-id pub-id-type="pmid">27043744</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentile</surname><given-names>F</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal frequency tuning of cortical face-sensitive areas for individual face perception</article-title><source>NeuroImage</source><volume>90</volume><fpage>256</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.053</pub-id><pub-id pub-id-type="pmid">24321556</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilmore</surname><given-names>JH</given-names></name><name><surname>Knickmeyer</surname><given-names>RC</given-names></name><name><surname>Gao</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging structural and functional brain development in early childhood</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>123</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1038/nrn.2018.1</pub-id><pub-id pub-id-type="pmid">29449712</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golarai</surname><given-names>G</given-names></name><name><surname>Ghahremani</surname><given-names>DG</given-names></name><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name><name><surname>Reiss</surname><given-names>A</given-names></name><name><surname>Eberhardt</surname><given-names>JL</given-names></name><name><surname>Gabrieli</surname><given-names>JDE</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differential development of high-level visual cortex correlates with category-specific recognition memory</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>512</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/nn1865</pub-id><pub-id pub-id-type="pmid">17351637</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname><given-names>J</given-names></name><name><surname>Barnett</surname><given-names>MA</given-names></name><name><surname>Natu</surname><given-names>V</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Palomero-Gallagher</surname><given-names>N</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Microstructural proliferation in human cortex is coupled with the development of face processing</article-title><source>Science</source><volume>355</volume><fpage>68</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1126/science.aag0311</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goren</surname><given-names>CC</given-names></name><name><surname>Sarty</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>PY</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Visual following and pattern discrimination of face-like stimuli by newborn infants</article-title><source>Pediatrics</source><volume>56</volume><fpage>544</fpage><lpage>549</lpage><pub-id pub-id-type="pmid">1165958</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieve</surname><given-names>PG</given-names></name><name><surname>Emerson</surname><given-names>RG</given-names></name><name><surname>Fifer</surname><given-names>WP</given-names></name><name><surname>Isler</surname><given-names>JR</given-names></name><name><surname>Stark</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Spatial correlation of the infant and adult electroencephalogram</article-title><source>Clinical Neurophysiology</source><volume>114</volume><fpage>1594</fpage><lpage>1608</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(03)00122-6</pub-id><pub-id pub-id-type="pmid">12948788</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual recognition: as soon as you know it is there, you know what it is</article-title><source>Psychological Science</source><volume>16</volume><fpage>152</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.00796.x</pub-id><pub-id pub-id-type="pmid">15686582</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grotheer</surname><given-names>M</given-names></name><name><surname>Rosenke</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Kular</surname><given-names>H</given-names></name><name><surname>Querdasi</surname><given-names>FR</given-names></name><name><surname>Natu</surname><given-names>VS</given-names></name><name><surname>Yeatman</surname><given-names>JD</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>White matter myelination during early infancy is linked to spatial gradients and myelin content at birth</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>997</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-28326-4</pub-id><pub-id pub-id-type="pmid">35194018</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guy</surname><given-names>MW</given-names></name><name><surname>Zieber</surname><given-names>N</given-names></name><name><surname>Richards</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The cortical development of specialized face processing in infancy</article-title><source>Child Development</source><volume>87</volume><fpage>1581</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1111/cdev.12543</pub-id><pub-id pub-id-type="pmid">27246260</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halit</surname><given-names>H</given-names></name><name><surname>de Haan</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical specialisation for face processing: face-sensitive event-related potential components in 3- and 12-month-old infants</article-title><source>NeuroImage</source><volume>19</volume><fpage>1180</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(03)00076-4</pub-id><pub-id pub-id-type="pmid">12880843</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halit</surname><given-names>H</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name><name><surname>Volein</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Face-sensitive cortical processing in early infancy</article-title><source>Journal of Child Psychology and Psychiatry, and Allied Disciplines</source><volume>45</volume><fpage>1228</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7610.2004.00321.x</pub-id><pub-id pub-id-type="pmid">15335343</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinrich</surname><given-names>SP</given-names></name><name><surname>Mell</surname><given-names>D</given-names></name><name><surname>Bach</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Frequency-domain analysis of fast oddball responses to visual stimuli: A feasibility study</article-title><source>International Journal of Psychophysiology</source><volume>73</volume><fpage>287</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2009.04.011</pub-id><pub-id pub-id-type="pmid">19426768</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jagadeesh</surname><given-names>AV</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Texture-like representation of objects in human visual cortex</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2115302119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2115302119</pub-id><pub-id pub-id-type="pmid">35439063</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Henderson</surname><given-names>MM</given-names></name><name><surname>Lin</surname><given-names>R</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Selectivity for food in human ventral visual cortex</article-title><source>Communications Biology</source><volume>6</volume><elocation-id>175</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-023-04546-2</pub-id><pub-id pub-id-type="pmid">36792693</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayaraman</surname><given-names>S</given-names></name><name><surname>Fausey</surname><given-names>CM</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Why are faces denser in the visual experiences of younger than older infants?</article-title><source>Developmental Psychology</source><volume>53</volume><fpage>38</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1037/dev0000230</pub-id><pub-id pub-id-type="pmid">28026190</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Dziurawiec</surname><given-names>S</given-names></name><name><surname>Ellis</surname><given-names>H</given-names></name><name><surname>Morton</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Newborns’ preferential tracking of face-like stimuli and its subsequent decline</article-title><source>Cognition</source><volume>40</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(91)90045-6</pub-id><pub-id pub-id-type="pmid">1786670</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional specificity in the human brain: a window into the functional architecture of the mind</article-title><source>PNAS</source><volume>107</volume><fpage>11163</fpage><lpage>11170</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id><pub-id pub-id-type="pmid">20484679</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>M</given-names></name><name><surname>Kakigi</surname><given-names>R</given-names></name><name><surname>Kanazawa</surname><given-names>S</given-names></name><name><surname>Yamaguchi</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Infants’ recognition of their mothers’ faces in facial drawings</article-title><source>Developmental Psychobiology</source><volume>62</volume><fpage>1011</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1002/dev.21972</pub-id><pub-id pub-id-type="pmid">32227340</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kohler</surname><given-names>PJ</given-names></name><name><surname>Barzegaran</surname><given-names>E</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>McCandliss</surname><given-names>BD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parietal contributions to abstract numerosity measured with steady state visual evoked potentials</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.06.239889</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosakowski</surname><given-names>HL</given-names></name><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Selective responses to faces, scenes, and bodies in the ventral visual pathway of infants</article-title><source>Current Biology</source><volume>32</volume><fpage>265</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.10.064</pub-id><pub-id pub-id-type="pmid">34784506</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebenberg</surname><given-names>J</given-names></name><name><surname>Mangin</surname><given-names>J-F</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Poupon</surname><given-names>C</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name><name><surname>Leroy</surname><given-names>F</given-names></name><name><surname>Adibpour</surname><given-names>P</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Dubois</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mapping the asynchrony of cortical maturation in the infant brain: A MRI multi-parametric clustering approach</article-title><source>NeuroImage</source><volume>185</volume><fpage>641</fpage><lpage>653</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.07.022</pub-id><pub-id pub-id-type="pmid">30017787</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerma-Usabiaga</surname><given-names>G</given-names></name><name><surname>Carreiras</surname><given-names>M</given-names></name><name><surname>Paz-Alonso</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Converging evidence for functional and structural segregation within the left ventral occipitotemporal cortex in reading</article-title><source>PNAS</source><volume>115</volume><fpage>E9981</fpage><lpage>E9990</lpage><pub-id pub-id-type="doi">10.1073/pnas.1803003115</pub-id><pub-id pub-id-type="pmid">30224475</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shi</surname><given-names>F</given-names></name><name><surname>Lyall</surname><given-names>AE</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Gilmore</surname><given-names>JH</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping longitudinal development of local cortical gyrification in infants from birth to 2 years of age</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4228</fpage><lpage>4238</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3976-13.2014</pub-id><pub-id pub-id-type="pmid">24647943</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu-Shuang</surname><given-names>J</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An objective index of individual face discrimination in the right occipito-temporal cortex by means of fast periodic oddball stimulation</article-title><source>Neuropsychologia</source><volume>52</volume><fpage>57</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.10.022</pub-id><pub-id pub-id-type="pmid">24200921</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Savage</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Development of the macaque face-patch system</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14897</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14897</pub-id><pub-id pub-id-type="pmid">28361890</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id><pub-id pub-id-type="pmid">21317022</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margalit</surname><given-names>E</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Vizioli</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>R-Y</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultra-high-resolution fmri of human ventral temporal cortex reveals differential representation of categories and domains</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3008</fpage><lpage>3024</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2106-19.2020</pub-id><pub-id pub-id-type="pmid">32094202</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mondloch</surname><given-names>CJ</given-names></name><name><surname>Lewis</surname><given-names>TL</given-names></name><name><surname>Budreau</surname><given-names>DR</given-names></name><name><surname>Maurer</surname><given-names>D</given-names></name><name><surname>Dannemiller</surname><given-names>JL</given-names></name><name><surname>Stephens</surname><given-names>BR</given-names></name><name><surname>Kleiner-Gathercoal</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Face perception during early infancy</article-title><source>Psychological Science</source><volume>10</volume><fpage>419</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00179</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Natu</surname><given-names>VS</given-names></name><name><surname>Rosenke</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Querdasi</surname><given-names>FR</given-names></name><name><surname>Kular</surname><given-names>H</given-names></name><name><surname>Lopez-Alvarez</surname><given-names>N</given-names></name><name><surname>Grotheer</surname><given-names>M</given-names></name><name><surname>Berman</surname><given-names>S</given-names></name><name><surname>Mezer</surname><given-names>AA</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Infants’ cortex undergoes microstructural growth coupled with myelination during development</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>1191</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02706-w</pub-id><pub-id pub-id-type="pmid">34650227</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Tyler</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatial frequency sweep VEP: visual acuity during the first year of life</article-title><source>Vision Research</source><volume>25</volume><fpage>1399</fpage><lpage>1408</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(85)90217-2</pub-id><pub-id pub-id-type="pmid">4090273</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Tyler</surname><given-names>CW</given-names></name><name><surname>Hamer</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Development of contrast sensitivity in the human infant</article-title><source>Vision Research</source><volume>30</volume><fpage>1475</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90028-j</pub-id><pub-id pub-id-type="pmid">2247957</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Development of vision in infancy</chapter-title><person-group person-group-type="editor"><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><source>Adler’s Physiology of the Eye</source><publisher-name>Elsevier Health Sciences</publisher-name><fpage>531</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1016/B978-0-323-05714-1.00038-8</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Kohler</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence for long-range spatiotemporal interactions in infant and adult visual cortex</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1167/17.6.12</pub-id><pub-id pub-id-type="pmid">28622700</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nordt</surname><given-names>M</given-names></name><name><surname>Gomez</surname><given-names>J</given-names></name><name><surname>Natu</surname><given-names>VS</given-names></name><name><surname>Rezai</surname><given-names>AA</given-names></name><name><surname>Finzi</surname><given-names>D</given-names></name><name><surname>Kular</surname><given-names>H</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cortical recycling in high-level visual cortex during childhood development</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>1686</fpage><lpage>1697</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01141-5</pub-id><pub-id pub-id-type="pmid">34140657</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nordt</surname><given-names>M</given-names></name><name><surname>Gomez</surname><given-names>J</given-names></name><name><surname>Natu</surname><given-names>VS</given-names></name><name><surname>Rezai</surname><given-names>AA</given-names></name><name><surname>Finzi</surname><given-names>D</given-names></name><name><surname>Kular</surname><given-names>H</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Longitudinal development of category representations in ventral temporal cortex predicts word and face recognition</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>8010</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-43146-w</pub-id><pub-id pub-id-type="pmid">38049393</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascalis</surname><given-names>O</given-names></name><name><surname>de Schonen</surname><given-names>S</given-names></name><name><surname>Morton</surname><given-names>J</given-names></name><name><surname>Deruelle</surname><given-names>C</given-names></name><name><surname>Fabre-Grenet</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Mother’s face recognition by neonates: a replication and an extension</article-title><source>Infant Behavior and Development</source><volume>18</volume><fpage>79</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/0163-6383(95)90009-8</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural mechanisms of rapid natural scene categorization in human visual cortex</article-title><source>Nature</source><volume>460</volume><fpage>94</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/nature08103</pub-id><pub-id pub-id-type="pmid">19506558</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennock</surname><given-names>IML</given-names></name><name><surname>Racey</surname><given-names>C</given-names></name><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Franklin</surname><given-names>A</given-names></name><name><surname>Bosten</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Color-biased regions in the ventral visual pathway are food selective</article-title><source>Current Biology</source><volume>33</volume><fpage>134</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.11.063</pub-id><pub-id pub-id-type="pmid">36574774</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peykarjou</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Frequency tagging with infants: the visual oddball paradigm</article-title><source>Frontiers in Psychology</source><volume>13</volume><elocation-id>1015611</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2022.1015611</pub-id><pub-id pub-id-type="pmid">36425830</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>PC</given-names></name><name><surname>Eimas</surname><given-names>PD</given-names></name><name><surname>Rosenkrantz</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Evidence for representations of perceptually similar natural categories by 3-month-old and 4-month-old infants</article-title><source>Perception</source><volume>22</volume><fpage>463</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1068/p220463</pub-id><pub-id pub-id-type="pmid">8378134</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rekow</surname><given-names>D</given-names></name><name><surname>Baudouin</surname><given-names>JY</given-names></name><name><surname>Poncet</surname><given-names>F</given-names></name><name><surname>Damon</surname><given-names>F</given-names></name><name><surname>Durand</surname><given-names>K</given-names></name><name><surname>Schaal</surname><given-names>B</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Leleu</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Odor-driven face-like categorization in the human infant brain</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2014979118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2014979118</pub-id><pub-id pub-id-type="pmid">34001601</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Retter</surname><given-names>TL</given-names></name><name><surname>Jiang</surname><given-names>F</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>All-or-none face categorization in the human brain</article-title><source>NeuroImage</source><volume>213</volume><elocation-id>116685</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116685</pub-id><pub-id pub-id-type="pmid">32119982</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Biomarkers of face perception in autism spectrum disorder: time to shift to fast periodic visual stimulation with electroencephalography?</article-title><source>Biological Psychiatry. Cognitive Neuroscience and Neuroimaging</source><volume>5</volume><fpage>258</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1016/j.bpsc.2020.01.008</pub-id><pub-id pub-id-type="pmid">32147110</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scherf</surname><given-names>KS</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name><name><surname>Humphreys</surname><given-names>K</given-names></name><name><surname>Luna</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual category-selectivity for faces, places and objects emerges along different developmental trajectories</article-title><source>Developmental Science</source><volume>10</volume><fpage>F15</fpage><lpage>F30</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2007.00595.x</pub-id><pub-id pub-id-type="pmid">17552930</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>LS</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A domain-relevant framework for the development of face processing</article-title><source>Nature Reviews Psychology</source><volume>2</volume><fpage>183</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1038/s44159-023-00152-5</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spriet</surname><given-names>C</given-names></name><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Hochmann</surname><given-names>JR</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual object categorization in infancy</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2105866119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2105866119</pub-id><pub-id pub-id-type="pmid">35169072</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stigliani</surname><given-names>A</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Temporal processing capacity in high-level visual cortex is domain specific</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12412</fpage><lpage>12424</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4822-14.2015</pub-id><pub-id pub-id-type="pmid">26354910</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugita</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Innate face processing</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>39</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.03.001</pub-id><pub-id pub-id-type="pmid">19339171</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>MJ</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name><name><surname>Saliba</surname><given-names>E</given-names></name><name><surname>Degiovanni</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>ERP evidence of developmental changes in processing of faces</article-title><source>Clinical Neurophysiology</source><volume>110</volume><fpage>910</fpage><lpage>915</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(99)00006-1</pub-id><pub-id pub-id-type="pmid">10400205</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tooley</surname><given-names>UA</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Mackey</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Environmental influences on the pace of brain development</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>372</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1038/s41583-021-00457-5</pub-id><pub-id pub-id-type="pmid">33911229</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name><name><surname>De Schonen</surname><given-names>S</given-names></name><name><surname>Crivello</surname><given-names>F</given-names></name><name><surname>Reutter</surname><given-names>B</given-names></name><name><surname>Aujard</surname><given-names>Y</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural correlates of woman face processing by 2-month-old infants</article-title><source>NeuroImage</source><volume>15</volume><fpage>454</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0979</pub-id><pub-id pub-id-type="pmid">11798279</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vettori</surname><given-names>S</given-names></name><name><surname>Dzhelyova</surname><given-names>M</given-names></name><name><surname>Van der Donck</surname><given-names>S</given-names></name><name><surname>Jacques</surname><given-names>C</given-names></name><name><surname>Steyaert</surname><given-names>J</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Boets</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reduced neural sensitivity to rapid individual face discrimination in autism spectrum disorder</article-title><source>NeuroImage. Clinical</source><volume>21</volume><elocation-id>101613</elocation-id><pub-id pub-id-type="doi">10.1016/j.nicl.2018.101613</pub-id><pub-id pub-id-type="pmid">30522972</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Mast</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A new statistic for steady-state evoked potentials</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>78</volume><fpage>378</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(91)90099-p</pub-id><pub-id pub-id-type="pmid">1711456</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Sadr</surname><given-names>J</given-names></name><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Horne</surname><given-names>GO</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Tanaka</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the SHINE toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>W</given-names></name><name><surname>McCormick</surname><given-names>SA</given-names></name><name><surname>Westerlund</surname><given-names>A</given-names></name><name><surname>Bowman</surname><given-names>LC</given-names></name><name><surname>Nelson</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural correlates of facial emotion processing in infancy</article-title><source>Developmental Science</source><volume>22</volume><elocation-id>e12758</elocation-id><pub-id pub-id-type="doi">10.1111/desc.12758</pub-id><pub-id pub-id-type="pmid">30276933</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Moeskops</surname><given-names>M</given-names></name><name><surname>Kayhan</surname><given-names>E</given-names></name><name><surname>Kliesch</surname><given-names>C</given-names></name><name><surname>Turtleton</surname><given-names>B</given-names></name><name><surname>Köster</surname><given-names>M</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual category representations in the infant brain</article-title><source>Current Biology</source><volume>32</volume><fpage>5422</fpage><lpage>5432</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.11.016</pub-id><pub-id pub-id-type="pmid">36455560</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>InfantObjectCategorization</data-title><version designator="swh:1:rev:299e90b1d435701db7b66e690a922a14062ebb5f">swh:1:rev:299e90b1d435701db7b66e690a922a14062ebb5f</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:abca84f8fd1b2878cd84211d8dc6f1b307bc4724;origin=https://github.com/VPNL/InfantObjectCategorization;visit=swh:1:snp:27cc7916e17e344645c77aafef7071c450ba5dd2;anchor=swh:1:rev:299e90b1d435701db7b66e690a922a14062ebb5f">https://archive.softwareheritage.org/swh:1:dir:abca84f8fd1b2878cd84211d8dc6f1b307bc4724;origin=https://github.com/VPNL/InfantObjectCategorization;visit=swh:1:snp:27cc7916e17e344645c77aafef7071c450ba5dd2;anchor=swh:1:rev:299e90b1d435701db7b66e690a922a14062ebb5f</ext-link></element-citation></ref><ref id="bib89"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>FLoc</data-title><version designator="swh:1:rev:de6a26cc269a2c7075461a4c839bfd628f225c95">swh:1:rev:de6a26cc269a2c7075461a4c839bfd628f225c95</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:91bd166396050320b1800544fc3e44e4f88cbe37;origin=https://github.com/VPNL/fLoc;visit=swh:1:snp:487c51f0dfbe508145e66c779a2c05257237ab9f;anchor=swh:1:rev:de6a26cc269a2c7075461a4c839bfd628f225c95">https://archive.softwareheritage.org/swh:1:dir:91bd166396050320b1800544fc3e44e4f88cbe37;origin=https://github.com/VPNL/fLoc;visit=swh:1:snp:487c51f0dfbe508145e66c779a2c05257237ab9f;anchor=swh:1:rev:de6a26cc269a2c7075461a4c839bfd628f225c95</ext-link></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Younger</surname><given-names>BA</given-names></name><name><surname>Fearing</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Parsing items into separate categories: developmental change in infant categorization</article-title><source>Child Development</source><volume>70</volume><fpage>291</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1111/1467-8624.00022</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Demographic information</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Demographic information.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"><sc>3–4</sc> months<sc>(N=17)</sc></th><th align="left" valign="bottom"><sc>4–6</sc> months<sc>(N=14)</sc></th><th align="left" valign="bottom"><sc>6–8</sc> months<sc>(N=15)</sc></th><th align="left" valign="bottom"><sc>12–15</sc> months<sc>(N=15)</sc></th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Age at test (days</bold>)</td><td align="left" valign="bottom">84–117</td><td align="left" valign="bottom">127–183</td><td align="left" valign="bottom">194–232</td><td align="left" valign="bottom">369–445</td></tr><tr><td align="left" valign="bottom"><bold>Sex</bold></td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td></tr><tr><td align="left" valign="bottom"> Female</td><td align="left" valign="bottom"> 7</td><td align="left" valign="bottom"> 7</td><td align="left" valign="bottom"> 6</td><td align="left" valign="bottom"> 4</td></tr><tr><td align="left" valign="bottom"> Male</td><td align="left" valign="bottom"> 10</td><td align="left" valign="bottom"> 7</td><td align="left" valign="bottom"> 9</td><td align="left" valign="bottom"> 11</td></tr><tr><td align="left" valign="bottom"><bold>Race</bold></td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td></tr><tr><td align="left" valign="bottom"> White</td><td align="left" valign="bottom"> 5</td><td align="left" valign="bottom"> 4</td><td align="left" valign="bottom"> 6</td><td align="left" valign="bottom"> 5</td></tr><tr><td align="left" valign="bottom"> Black</td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> </td></tr><tr><td align="left" valign="bottom"> Asian</td><td align="left" valign="bottom"> </td><td align="left" valign="bottom"> 1</td><td align="left" valign="bottom"> 1</td><td align="left" valign="bottom"> 2</td></tr><tr><td align="left" valign="bottom"> Mixed races</td><td align="left" valign="bottom"> 11</td><td align="left" valign="bottom"> 8</td><td align="left" valign="bottom"> 6</td><td align="left" valign="bottom"> 7</td></tr><tr><td align="left" valign="bottom"> Unknown</td><td align="left" valign="bottom"> 1</td><td align="left" valign="bottom"> 1</td><td align="left" valign="bottom"> 2</td><td align="left" valign="bottom"> 1</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Average number of valid epochs summed across all five categories for each age group before and after data pre-processing.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">3–4 months(N=17)</th><th align="left" valign="bottom">4–6 months(N=14)</th><th align="left" valign="bottom">6–8 months(N=15)</th><th align="left" valign="bottom">12–15 months(N=15)</th><th align="left" valign="bottom">Adults(N=20)</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Before pre-processing</bold></td><td align="left" valign="bottom">281 (± 103)</td><td align="left" valign="bottom">270 (± 86)</td><td align="left" valign="bottom">346 (± 111)</td><td align="left" valign="bottom">324 (± 78)</td><td align="left" valign="bottom">600 (± 0)</td></tr><tr><td align="left" valign="bottom"><bold>After pre-processing</bold></td><td align="left" valign="bottom">223 (± 89)</td><td align="left" valign="bottom">219 (± 73)</td><td align="left" valign="bottom">266 (± 91)</td><td align="left" valign="bottom">269 (± 77)</td><td align="left" valign="bottom">560 (± 37)</td></tr><tr><td align="left" valign="bottom"><bold>Ratio (after/before</bold>)</td><td align="left" valign="bottom">79%</td><td align="left" valign="bottom">81%</td><td align="left" valign="bottom">77%</td><td align="left" valign="bottom">83%</td><td align="left" valign="bottom">93%</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s9"><title>Low-level image properties analyses</title><p>As images of items of visual categories vary both on low-level and high-level properties, and both the EEG signal and babies’ attention may be affected by low-level properties like contrast or luminance, it is important to control for several low-level factors such as luminance, contrast, similarity spatial frequency, as well as high-level properties such as familiarity across categories. Here, we used a subset of categories from <xref ref-type="bibr" rid="bib77">Stigliani et al., 2015</xref>, that did not contain familiar stimuli to our participants and were controlled for several low-level factors using the SHINE toolbox (<xref ref-type="bibr" rid="bib85">Willenbockel et al., 2010</xref>). To validate the effect of employing SHINE, we evaluated several low-level metrics and tested if they differed across categories: (i) <italic>Luminance:</italic> mean gray level of each image; (ii) <italic>contrast:</italic> mean standard deviation of gray-level values of each image; (iii) <italic>similarity:</italic> mean pixel-wise similarity between gray levels across pairs of images of a category, and (iv) <italic>spatial frequency distribution</italic>: we transformed each image to the Fourier domain using FFT and calculated its circular amplitude spectrum. We calculated these metrics for each image, then tested if contrast, luminance, similarity metrics were significantly different across categories using pairwise nonparametric permutation t-tests (10,000 times, with Bonferroni correction). While it is difficult to completely control multiple low-level metrics across categories (<xref ref-type="bibr" rid="bib85">Willenbockel et al., 2010</xref>), our analyses show that images are largely controlled for several low-level metrics across categories, and that medians, means, and ranges are matched across categories (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>). With respect to differences in contrast and luminance, there are no significant differences across categories except that limbs have slightly but significantly lower luminance (ps&lt;0.05, Bonferroni corrected) and lower pairwise similarity (ps&lt;0.01) and higher contrast (ps&lt;0.05, except for corridors) than other categories. For similarity, the medians and ranges are similar across categories, but the outliers vary producing significant differences across categories (permutation tests, p&lt;0.05, Bonferroni corrected). For spatial frequency distribution, the means and standard deviations are similar across categories (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, right). We further ran Kolmogrov-Smirnov tests for each pair of categories with Bonferroni correction, our results revealed significantly different distributions across category pairs (ps&lt;0.001, except for limbs and corridors).</p><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Mean (± SD) values of contrast, luminance, and similarity metrics across images within each five stimuli categories.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Faces</th><th align="left" valign="bottom">Limbs</th><th align="left" valign="bottom">Corridors</th><th align="left" valign="bottom">Characters</th><th align="left" valign="bottom">Cars</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Contrast</bold></td><td align="left" valign="bottom">0.469<break/>(2.533e-4)</td><td align="left" valign="bottom">0.391<break/>(3.91e-4)</td><td align="left" valign="bottom">0.404<break/>(4.04e-4)</td><td align="left" valign="bottom">0.2856<break/>(2.856e-4)</td><td align="left" valign="bottom">0.3269<break/>(3.269e-4)</td></tr><tr><td align="left" valign="bottom"><bold>Luminance</bold></td><td align="left" valign="bottom">0.6733<break/>(0.0007)</td><td align="left" valign="bottom">0.6729<break/>(0.0011)</td><td align="left" valign="bottom">0.6732<break/>(0.0011)</td><td align="left" valign="bottom">0.6732<break/>(0.0008)</td><td align="left" valign="bottom">0.6733<break/>(0.0009)</td></tr><tr><td align="left" valign="bottom"><bold>Similarity</bold></td><td align="left" valign="bottom">0.9855<break/>(0.0063)</td><td align="left" valign="bottom">0.9555<break/>(0.0039)</td><td align="left" valign="bottom">0.9569<break/>(0.0051)</td><td align="left" valign="bottom">0.9563<break/>(0.0033)</td><td align="left" valign="bottom">0.9570<break/>(0.0044)</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s10"><title>Analyses of adult data</title><p>In the study, one concern is that the amount of empirical data typically collected in infants is less than in adults, which may compromise the experimental power to detect responses in infants as it may affect the noise bandwidth (sensitivity) of the frequency tagging analysis. Thus, we tested whether it is possible to measure category-selective responses in 20 adults using the same experiment (<xref ref-type="fig" rid="fig1">Figure 1</xref>) and the same amount of data collected in infants using three types of analyses. By using the same number of trials in adults as those obtained from our infant participants for data analyses, our goal was to test that we had sufficient power to detect categorical responses from infants using the experimental paradigm. We expect temporal and amplitude differences between adults and infants (<xref ref-type="bibr" rid="bib87">Xie et al., 2022</xref>) as infants have immature brains and skulls. For example, cortical gyrification, which determines the orientation of the electrical fields generated in a certain part of the brain region on the scalp, still undergoes development during the first 2 years of infants’ lives (<xref ref-type="bibr" rid="bib52">Li et al., 2014</xref>). Second, adults’ skulls are thicker and have lower conductivity than infants’ skulls, thus electrical signals on their scalp are lower than infants (<xref ref-type="bibr" rid="bib32">Grieve et al., 2003</xref>). Nonetheless, we tested if we could in principle detect category information in adults with the same amount of data as infants. We reasoned that if category information can be detected in adults and signals are stronger in infants then we should have the power to detect category information in infants.</p><sec sec-type="appendix" id="s10-1"><title>Visual responses</title><p>We averaged the pre-processed data across two consecutive epochs (2.3334 s) and across conditions for each individual to measure visual responses to all images that were presented at 4.286 Hz and then transformed the data to the frequency domain. <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref> shows the three-D topographies of group-averaged responses at 4.286 Hz and its first three harmonics (left panel) and the group-averaged Fourier amplitude spectrum over middle occipital and occipitotemporal ROIs (right panel). We used Hotelling’s T2 statistic to test the amplitude significance and found significant visual responses in adults at 4.286 Hz over the occipital ROI containing nine electrodes over early visual cortex (p&lt;0.05, FDR corrected at four levels) and significant visual responses at 4.286 Hz and its first two harmonics over the occipitotemporal ROI (14 electrodes) (ps&lt;0.05).</p><p>Examining the temporal waveform of the visual response (filtered at the image frequency and its harmonics), we found a relatively slow waveform with one peak, after 100 ms since stimulus onset (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Robust visual and categorical responses recorded over occipitotemporal and occipital cortex in 20 adults.</title><p>(<bold>A</bold>) <italic>Left panel:</italic> spatial distribution of visual response at 4.286 Hz and harmonic. Harmonic frequencies are indicated on the top. <italic>Right panel:</italic> mean Fourier amplitude spectrum across 14 electrodes in the occipitotemporal and 9 electrodes in the occipital regions of interest (ROIs). <italic>Error bars:</italic> standard error of the mean across participants. <italic>Black bars:</italic> image frequency and harmonics; Purple bars: category frequency and harmonics. Asterisks: significant response amplitude from zero at p<sub>FDR</sub>&lt;0.05. (<bold>B</bold>) <italic>Left:</italic> spatial distribution of visual responses at time window 145–155 ms. <italic>Right</italic>: Mean visual responses over two ROIs in the time domain. Waveforms are shown for a time window of 233 ms during which one image is shown. <italic>Shaded area</italic>: standard error of the mean across participants. Blank line at around y=–1.5: stimulus onset duration. To define time windows in which amplitudes were significantly different from zero, we used a cluster-based nonparametric permutation t-test (1000 permutations, with a threshold of p&lt;0.05, two-tailed) on the post-stimulus onset time points (0–1167 ms) (<xref ref-type="bibr" rid="bib3">Appelbaum et al., 2006</xref>; <xref ref-type="bibr" rid="bib10">Blair and Karniski, 1993</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig1-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10-2"><title>Visual category responses</title><p>As adults have mature category-selective regions, we next tested if using our SSVEP paradigm and the same amount of data as in infants, we identify significant category-selective responses in adults. First, we analyzed the mean response at the category frequency and its harmonics in lateral occipitotemporal ROIs. This is a selective response as it reflects the relative response to the target category (generalized across exemplars) above the general visual response to images of other categories. Despite lower response amplitudes in adults than infants, using the same amount of data as infants, adults show significant category-selective responses to each of these five categories (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2A</xref> for all categories). <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2A</xref> shows the group-averaged Fourier amplitude spectrum over LOT and ROT ROIs (top panels) and the three-D topographies of group-averaged responses at 0.857 Hz and its first harmonic (bottom panel). We used Hotelling’s T2 statistic to test the amplitude significance and found significant above-zero category-selective responses in adults at 0.857 Hz and its multiple harmonics over bilateral occipitotemporal ROIs containing seven electrodes each (ps&lt;0.05, FDR corrected over eight levels).</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Adult control: using the same amount of data as infants reveals strong category-selective responses in adults’ occipitotemporal cortex.</title><p>(<bold>A</bold>) Mean Fourier amplitude spectrum across seven (left OT: 57, 58, 59, 63, 64, 65, 68; right OT: 90, 91, 94, 95, 96, 99) electrodes in bilateral occipitotemporal regions of interest (ROIs). Data are first averaged in each participant and then across 20 participants. <italic>Error bars</italic>: standard error of the mean across participants. <italic>Black bars</italic>: visual response at image frequency and harmonics; <italic>Colored bars:</italic> categorical response at category frequency and harmonics. <italic>Asterisks:</italic> significant response amplitude from zero at p<sub>FDR</sub>&lt;0.05 for category harmonics. Crosses: significant response amplitude from zero at p&lt;0.05 with no FDR correction. <italic>Black dots:</italic> ROI channels used in analysis. (<bold>B</bold>) Mean category-selective responses in the time domain. Data are averaged across electrodes of each of the left and right occipitotemporal ROI in each participant and then across participants. <italic>Colored lines along x-axis at y=–1.5:</italic> significant deflections against zero (calculated with a cluster-based method, see Methods part). <italic>Black line above x-axis:</italic> stimulus onset duration. <italic>Bottom panel:</italic> spatial distribution of category-selective responses at time window 200–217 ms.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig2-v1.tif"/></fig><p>By transforming the filtered data at the category frequency and its harmonics to examine the temporal waveform of each category. Notably, each category generates a unique topography (bottom panels) at 200–217 ms after stimulus onset (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2B</xref>). For faces, we found an early negative deflection peaking ~200 ms after stimulus onset in both the LOT and ROT ROIs, with the ROT ROI showing a numerically larger mean response amplitude than the left (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2B</xref>, faces). Similarly, we found an early negative peak at around 200 ms for characters, and a left hemisphere dominance (at the 162–248 ms and 398–474 ms time windows <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2B</xref>, characters). For both limbs and corridors, there was an early positive waveform peaking at around 200 ms in both hemispheres with no hemispheric differences.</p><p>Second, we examined in the same 20 adults whether the spatiotemporal pattern of brain responses evoked by different visual categories are distinct from one another and are reliable across participants, using an LOOCV classifier approach with spatiotemporal time series concatenated with mean temporal waveforms from three ROIs: (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3A</xref>, left). Mean LOOCV classification performance was around 80% and significantly above chance in adults (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3A</xref>, right). This classification was associated with correct decoding of all five categories from distributed responses in the majority of participants.</p><p>Third, we examined in the same 20 adults if there is reliable category information in spatiotemporal distributed responses in each individual. Distributed spatiotemporal responses were measured in individual participants over 23 occipital and lateral occipital ROIs using split half of data. We computed the RSM across split-halves and then calculated category distinctiveness by subtracting mean between category similarity from within-category similarity for each category. Mean RSM across 20 adults shows that spatiotemporal patterns are more similar across items of a category than across items of different categories (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3B</xref>, left), and the category distinctiveness scores are significantly above zero for all five categories: faces, t<sub>(19)</sub> = 11.18, p<sub>FDR</sub>&lt;0.05; limbs, t<sub>(19)</sub> = 5.01, p<sub>FDR</sub>&lt;0.05; corridors, t<sub>(19)</sub> = 5.58, p<sub>FDR</sub>&lt;0.05; characters, t<sub>(19)</sub> = 6.56, p<sub>FDR</sub>&lt;0.05; and cars, t<sub>(19)</sub> = 10.59, p<sub>FDR</sub>&lt;0.05.</p><p>Together, these analyses suggest that the experimental paradigm has sufficient power to identify category representations both at the group and individual levels.</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Adult control: using the same amount of data as infants reveals distributed category-selective responses in adults’ occipitotemporal cortex.</title><p>(<bold>A</bold>) <italic>Left:</italic> An illustration of winner-takes-all leave-one-out-cross-validation (LOOCV) classifier using the spatiotemporal response patterns of each category. Spatiotemporal patterns of response for each category are generated by concatenating the mean time courses from each of the three regions of interest (ROIs): left occipitotemporal (LOT), occipital (OCC), and right occipitotemporal (ROT). At each iteration, we train the classifier with the mean spatiotemporal patterns of each category from N–1 participants, and test how well it predicts the category the left-out participant is viewing from their spatiotemporal brain response. This is a winner-take-all classifier which predicts the category based on the highest pairwise correlation between the training and testing vectors. <italic>Right: White:</italic> mean decoding accuracy across all five categories. In adults, this is significantly above chance level (t<sub>(19)</sub> = 15.4, p&lt;0.001). <italic>Colored:</italic> decoding accuracy per category. (<bold>B</bold>) <italic>Left:</italic> average adult representation similarity matrix (RSM) for odd/even splits of spatiotemporal patterns of categorical over 23 electrodes in the LOT, OCC, ROT. RSMs were generated in each participant and then averaged across all participants. <italic>Diagonal:</italic> correlation of distributed responses across different exemplars of the same category. <italic>Off-diagonal:</italic> correlations across different exemplars from different categories. <italic>Acronyms:</italic> F: faces; L: limbs; Corr: corridors; Char: characters; Car: Cars.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig3-v1.tif"/></fig></sec></sec><sec sec-type="appendix" id="s11"><title>Validation of experimental paradigm</title><p>As visual acuity develops during the first year of life (<xref ref-type="bibr" rid="bib60">Norcia and Tyler, 1985</xref>; <xref ref-type="bibr" rid="bib61">Norcia et al., 1990</xref>; <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>), one concern is that our controlled natural, gray-level stimuli may not be distinguishable to infants. Measurements of visual evoked potentials (<xref ref-type="bibr" rid="bib60">Norcia and Tyler, 1985</xref>; <xref ref-type="bibr" rid="bib61">Norcia et al., 1990</xref>) suggest that visual acuity in 3-month-olds is around 5–8 cycles per degree (cpd) and in 6-month-olds around 10–16 cpd (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). Thus, to simulate how our images may appear to infants, we filtered all images at 5 cpd. Despite being blurry, images of different categories are distinguishable and individual items retain their identity by visualization (Appendix 1 – <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video5">5</xref>).</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Grating acuity as a function of age measured with a swept spatial frequency technique combining electroencephalography (EEG).</title><p>(<bold>A</bold>) Acuity growth functions are similar across studies, with acuity increasing from 5 to 8 cycles per degree (cpd) in 3-month-olds to around 10–16 cpd in 6-month-olds. This figure is adapted from <xref ref-type="bibr" rid="bib62">Norcia, 2011</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig4-v1.tif"/></fig></sec><sec sec-type="appendix" id="s12"><title>Visual responses over occipital cortex per condition for all age groups</title><p>In the main analysis, we averaged the image-update visual responses across five conditions for each infant, as the same visual stimuli from all five stimuli categories were viewed by the infant. However, we are showing the mean Fourier amplitude spectrum over the occipital cortex for each condition for all age groups (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>). The response patterns across conditions at each age group is similar. To examine whether there are visual response amplitude differences between conditions by age groups, we quantified the RMS amplitude value of the responses at the image-update frequency (4.286 Hz) and its first three harmonics (8.571 Hz, 12.857 Hz, and 17.143 Hz) for each category condition and infant. Then, we used an LMM to test for an age by category interaction. The LMM was conducted over the posterior occipital ROI. Results of this analysis find no significant main effect of category (β<sub>category</sub> = 0.08, 95% CI: –0.08 to 0.24, t<sub>(301)</sub> = 0.97, p=0.33) or category by age interaction (β<sub>category × age</sub> = –0.04, 95% CI: –0.11 to 0.03, t<sub>(301)</sub> = –1.09, p=0.28), which means that the visual response amplitudes are consistent across category conditions.</p><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Visual responses over occipital cortex at the image-update frequency and its harmonics in five category conditions in all age groups.</title><p>Each column shows mean responses across infants in an age group for each condition. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds, n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. Graphs show the mean Fourier amplitude spectrum over the occipital region of interest (ROI). The visual response is at the image-update frequency (4.286 Hz) and its first three harmonics, with the mean topographies at these frequencies of interest shown on top.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig5-v1.tif"/></fig></sec><sec sec-type="appendix" id="s13"><title>LMM analyses of visual responses (associated with <xref ref-type="fig" rid="fig2">Figure 2</xref>)</title><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Peak latency of visual responses by age and time window (window 1: 60–90 ms; window 2: 90–160 ms for 3- to 4-month-olds, and 90–110 ms for other groups).</title><p><italic>Formula:</italic> Peak latency ~1 + log10(age) × time window + (1|participant). Significant effects are indicated by asterisks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><italic>Parameter</italic></th><th align="left" valign="bottom">β</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Intercept</italic></td><td align="left" valign="bottom">–54.61</td><td align="left" valign="bottom">−100.82, –8.41</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">–2.34</td><td align="left" valign="bottom">0.021*</td></tr><tr><td align="left" valign="bottom"><italic>Age</italic></td><td align="left" valign="bottom">39.77</td><td align="left" valign="bottom">19.51, 60.02</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">3.89</td><td align="left" valign="bottom">0.00017***</td></tr><tr><td align="left" valign="bottom"><italic>Window</italic></td><td align="left" valign="bottom">141.68</td><td align="left" valign="bottom">112.92, 170.45</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">9.75</td><td align="left" valign="bottom">7.69e-17***</td></tr><tr><td align="left" valign="bottom"><italic>Age×window</italic></td><td align="left" valign="bottom">–45.78</td><td align="left" valign="bottom">−58.39, –33.17</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">–7.19</td><td align="left" valign="bottom">6.39e-11**</td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Peak latency of visual responses by age at each of the two time windows.</title><p><italic>Formula:</italic> Peak latency ~1 + log10(age) + (1|participant). Significant effects are indicated by asterisks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><italic>Window</italic></th><th align="left" valign="bottom"><italic>Parameter</italic></th><th align="left" valign="bottom">β</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Window1</italic></td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">90.19</td><td align="left" valign="bottom">75.66, 104.72</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">12.42</td><td align="char" char="hyphen" valign="bottom">4.22e-18***</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–7.44</td><td align="left" valign="bottom">−13.82, –1.06</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–2.33</td><td align="char" char="." valign="bottom">0.02*</td></tr><tr><td align="left" valign="bottom"><italic>Window2</italic></td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">218.09</td><td align="left" valign="bottom">196.05, 240.13</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">19.80</td><td align="char" char="hyphen" valign="bottom">9.64e-28***</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–46.91</td><td align="left" valign="bottom">−56.56, –37.27</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–9.73</td><td align="char" char="hyphen" valign="bottom">7.06e-14***</td></tr></tbody></table></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1—table 6.</label><caption><title>Analysis of peak amplitude of visual responses by age and time window.</title><p><italic>Formula:</italic> Peak amplitude ~1 + log10(age) × time window + (1|participant). Significant effects are indicated by an asterisk.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><italic>Parameter</italic></th><th align="left" valign="bottom">β</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Intercept</italic></td><td align="left" valign="bottom">–18.69</td><td align="left" valign="bottom">−32.35, –5.03</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">–2.71</td><td align="left" valign="bottom">0.008**</td></tr><tr><td align="left" valign="bottom"><italic>Age</italic></td><td align="left" valign="bottom">3.93</td><td align="left" valign="bottom">–2.05, 9.92</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">1.30</td><td align="left" valign="bottom">0.20</td></tr><tr><td align="left" valign="bottom"><italic>Window</italic></td><td align="left" valign="bottom">17.24</td><td align="left" valign="bottom">8.66, 25.82</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">3.98</td><td align="left" valign="bottom">0.0001***</td></tr><tr><td align="left" valign="bottom"><italic>Age×window</italic></td><td align="left" valign="bottom">–4.90</td><td align="left" valign="bottom">−8.66, –1.14</td><td align="char" char="." valign="bottom">118</td><td align="left" valign="bottom">–2.58</td><td align="left" valign="bottom">0.011*</td></tr></tbody></table></table-wrap><table-wrap id="app1table7" position="float"><label>Appendix 1—table 7.</label><caption><title>Peak amplitude of visual responses by age at each of the two time windows.</title><p><italic>Formula:</italic> Peak amplitude ~1 + log10(age) + (1|participant). Significant effects are indicated by asterisks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><italic>Window</italic></th><th align="left" valign="bottom"><italic>Parameter</italic></th><th align="left" valign="bottom">β</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Window1</italic></td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">1.69</td><td align="left" valign="bottom">–3.42, 6.79</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">0.66</td><td align="left" valign="bottom">0.51</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">0.91</td><td align="left" valign="bottom">–1.33, 3.15</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">0.82</td><td align="left" valign="bottom">0.42</td></tr><tr><td align="left" valign="bottom"><italic>Window2</italic></td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">11.16</td><td align="left" valign="bottom">4.80, 17.51</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">3.51</td><td align="left" valign="bottom">0.0009***</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–3.59</td><td align="left" valign="bottom">−6.38, –0.81</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–2.58</td><td align="left" valign="bottom">0.012*</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s14"><title>Frequency domain analyses of infants’ categorical responses to limbs, corridors, characters, and cars (associated with <xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows the group-averaged categorical response to faces. <xref ref-type="fig" rid="app1fig6">Appendix 1—figures 6</xref>–<xref ref-type="fig" rid="app1fig9">9</xref> show group averaged categorical responses to the rest four conditions other than faces in four age groups. We found significant responses in 6- to 8-month-olds for limbs, corridors, and characters, and in 12–15 months for corridors and characters.</p><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Limb responses emerge over occipitotemporal electrodes after 6 months of age.</title><p>Each panel shows mean responses at the category frequency (0.857 Hz) and its harmonics across infants in an age group. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds; n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. <italic>Left panels in each row:</italic> spatial distribution of categorical response at 0.857 Hz and its first harmonic. Harmonic frequencies are indicated on the top. <italic>Right two panels in each row</italic>: mean Fourier amplitude spectrum across seven left occipitotemporal electrodes and seven right occipitotemporal (shown in black on the left panel). Data are first averaged in each participant and then across participants. <italic>Error bars:</italic> standard error of the mean across participants in an age group. <italic>Asterisk:</italic> significant amplitude vs. zero (p&lt;0.05, FDR corrected). Cross: significant amplitude vs. zero (p&lt;0.05, with no FDR correction). <italic>Black bars:</italic> image frequency and harmonics; <italic>colored bars:</italic> category frequency and harmonics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig6-v1.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Corridor responses emerge over occipitotemporal electrodes after 6 months of age.</title><p>Each panel shows mean responses at the category frequency and its harmonics across infants in an age group. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds; n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. <italic>Left panels in each row:</italic> spatial distribution of categorical response at 0.857 Hz and its first harmonic. Harmonic frequencies are indicated on the top. <italic>Right two panels in each row</italic>: mean Fourier amplitude spectrum across seven left occipitotemporal electrodes and seven right occipitotemporal (shown in black on the left panel). Data are first averaged in each participant and then across participants. <italic>Error bars:</italic> standard error of the mean across participants in an age group. <italic>Asterisks:</italic> significant amplitude vs. zero (p&lt;0.05, FDR corrected). Crosses: significant amplitude vs. zero (p&lt;0.05, with no FDR correction). <italic>Black bars: image frequency and harmonics; colored bars: category frequency and harmonics</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig7-v1.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Significant character responses found over occipitotemporal electrodes at 12–15 months of age.</title><p>Each panel shows mean responses at the category frequency (0.857 Hz) and its harmonics across infants in an age group. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds; n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. <italic>Left panels in each row:</italic> spatial distribution of categorical response at 0.857 Hz and its first harmonic. Harmonic frequencies are indicated on the top. <italic>Right two panels in each row</italic>: mean Fourier amplitude spectrum across seven left occipitotemporal electrodes and seven right occipitotemporal (shown in black on the left panel). Data are first averaged in each participant and then across participants. <italic>Error bars:</italic> standard error of the mean across participants in an age group. <italic>Asterisks:</italic> significant amplitude vs. zero (p&lt;0.05, FDR corrected). <italic>Black bars:</italic> image frequency and harmonics; <italic>colored bars:</italic> category frequency and harmonics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig8-v1.tif"/></fig><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Significant car responses found over occipitotemporal electrodes at 3–4 months of age.</title><p>Each panel shows mean responses at the category frequency and its harmonics across infants in an age group. (<bold>A</bold>) 3- to 4-month-olds, n=17; (<bold>B</bold>) 4- to 6-month-olds; n=14; (<bold>C</bold>) 6- to 8-month-olds, n=15; (<bold>D</bold>) 12- to 15-month-olds, n=15. <italic>Left panels in each row:</italic> spatial distribution of categorical response at 0.857 Hz and its first harmonic. Harmonic frequencies are indicated on the top. <italic>Right two panels in each row</italic>: mean Fourier amplitude spectrum across seven left occipitotemporal electrodes and seven right occipitotemporal (shown in black on the left panel). Data are first averaged in each participant and then across participants. <italic>Error bars:</italic> standard error of the mean across participants in an age group. <italic>Asterisk:</italic> significant amplitude vs. zero (p&lt;0.05, FDR corrected). Cross: significant amplitude vs. zero (p&lt;0.05, with no FDR correction). <italic>Black bars:</italic> image frequency and harmonics; <italic>colored bars:</italic> category frequency and harmonics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig9-v1.tif"/></fig></sec><sec sec-type="appendix" id="s15"><title>LMM analyses of category-selective responses</title><table-wrap id="app1table8" position="float"><label>Appendix 1—table 8.</label><caption><title>Analysis of peak amplitude of waveforms of category responses by age and category.</title><p>Separate linear mixed models (LMMs) were done separately for the left occipitotemporal (OT) and right OT regions of interest (ROIs). <italic>Formula:</italic> Peak amplitude ~1 + log10(age) × category + (1|participant); Peak latency ~1 + log10(age) × category + (1|participant). Significant effects are indicated by an asterisk.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">ROI/metric</th><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">β</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">Left OT/</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">–1.35</td><td align="left" valign="bottom">–7.36, 4.66</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–0.44</td><td align="left" valign="bottom">0.66</td></tr><tr><td align="left" valign="bottom">amplitude</td><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">2.62</td><td align="left" valign="bottom">–0.02, 5.25</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">1.95</td><td align="left" valign="bottom">0.052</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Category</td><td align="left" valign="bottom">0.18</td><td align="left" valign="bottom">–1.58, 1.93</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">0.20</td><td align="left" valign="bottom">0.84</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age×category</td><td align="left" valign="bottom">–0.20</td><td align="left" valign="bottom">–0.97, 0.57</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–0.51</td><td align="left" valign="bottom">0.61</td></tr><tr><td align="left" valign="bottom">Left OT/</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">730.29</td><td align="left" valign="bottom">477.96, 982.61</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">5.70</td><td align="left" valign="bottom">2.9e-8***</td></tr><tr><td align="left" valign="bottom">latency</td><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–97.17</td><td align="left" valign="bottom">–207.76, 13.43</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–1.73</td><td align="left" valign="bottom">0.08</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Category</td><td align="left" valign="bottom">–43.35</td><td align="left" valign="bottom">–119.43, 32.73</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–1.12</td><td align="left" valign="bottom">0.26</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age×category</td><td align="left" valign="bottom">20.24</td><td align="left" valign="bottom">–13.11, 53.58</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">1.19</td><td align="left" valign="bottom">0.23</td></tr><tr><td align="left" valign="bottom">Right OT/</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">–7.39</td><td align="left" valign="bottom">−14.44, –0.36</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–2.07</td><td align="left" valign="bottom">0.04*</td></tr><tr><td align="left" valign="bottom">amplitude</td><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">5.53</td><td align="left" valign="bottom">2.45, 8.62</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">3.53</td><td align="left" valign="bottom">0.0005***</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Category</td><td align="left" valign="bottom">2.19</td><td align="left" valign="bottom">0.06, 4.3</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">2.02</td><td align="left" valign="bottom">0.04*</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age×category</td><td align="left" valign="bottom">–1.09</td><td align="left" valign="bottom">−2.00, –0.14</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–2.26</td><td align="left" valign="bottom">0.02*</td></tr><tr><td align="left" valign="bottom">Right OT/</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">922.47</td><td align="left" valign="bottom">667.95, 1177</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">7.13</td><td align="left" valign="bottom">7.38e-12***</td></tr><tr><td align="left" valign="bottom">latency</td><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–173.17</td><td align="left" valign="bottom">−284.73, –61.61</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–3.05</td><td align="left" valign="bottom">0.002**</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Category</td><td align="left" valign="bottom">–64.41</td><td align="left" valign="bottom">–141.15, 12.33</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">–1.65</td><td align="left" valign="bottom">0.10</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age×category</td><td align="left" valign="bottom">28.49</td><td align="left" valign="bottom">–5.15, 62.12</td><td align="char" char="." valign="bottom">301</td><td align="char" char="." valign="bottom">1.67</td><td align="left" valign="bottom">0.10</td></tr></tbody></table></table-wrap><table-wrap id="app1table9" position="float"><label>Appendix 1—table 9.</label><caption><title>Analysis of peak amplitude of waveforms of category responses for each category in the right occipitotemporal (OT) region of interest (ROI).</title><p><italic>Formula:</italic> Peak amplitude ~age + (1|participant). Significant effects are indicated by an asterisk.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Category</th><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">β</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">df</th><th align="left" valign="bottom">t</th><th align="left" valign="bottom">p</th></tr></thead><tbody><tr><td align="left" valign="bottom">Faces</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">–10.43</td><td align="left" valign="bottom">−17.81, –3.05</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–2.83</td><td align="left" valign="bottom">0.006**</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">7.27</td><td align="left" valign="bottom">4.03, 10.51</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">4.50</td><td align="left" valign="bottom">3.30e-5***</td></tr><tr><td align="left" valign="bottom">Limbs</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">2.10</td><td align="left" valign="bottom">–3.62, 7.83</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">0.74</td><td align="left" valign="bottom">0.46</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–2.90</td><td align="left" valign="bottom">−5.41,–0.38</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–2.31</td><td align="left" valign="bottom">0.02*</td></tr><tr><td align="left" valign="bottom">Corridors</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">–4.65</td><td align="left" valign="bottom">–11.09, 1.81</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–1.44</td><td align="left" valign="bottom">0.15</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">0.35</td><td align="left" valign="bottom">–2.47, 3.18</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">0.25</td><td align="left" valign="bottom">0.80</td></tr><tr><td align="left" valign="bottom">Characters</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">–2.79</td><td align="left" valign="bottom">–8.06, 2.49</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–1.06</td><td align="left" valign="bottom">0.3</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">–0.66</td><td align="left" valign="bottom">–2.97, 1.65</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–0.57</td><td align="left" valign="bottom">0.57</td></tr><tr><td align="left" valign="bottom">Cars</td><td align="left" valign="bottom">Intercept</td><td align="left" valign="bottom">–4.87</td><td align="left" valign="bottom">–11.46, 1.73</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">–1.48</td><td align="left" valign="bottom">0.15</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Age</td><td align="left" valign="bottom">0.78</td><td align="left" valign="bottom">–2.11, 3.67</td><td align="char" char="." valign="bottom">59</td><td align="char" char="." valign="bottom">0.54</td><td align="left" valign="bottom">0.59</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s16"><title>Individual-level decoding analysis</title><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>Illustration of the winner-takes-all (WTA) classifier.</title><p>In each individual, the time series data is split into odd and even trials. We concatenate the time series data from 23 electrodes in the left occipitotemporal, occipital, and right occipitotemporal regions of interest (ROIs) into a pattern vector for each split half and each condition. The classifier is trained on one half of the data (i.e. odd or even trials) and tested on how well it could predict the rest half of the data (i.e. even or odd trials) for each individual. The bottom shows the representation similarity matrix (RSM) in an example infant. Each cell indicates the correlation between distributed responses to different images of the same (on-diagonal) or different (off-diagonal) categories. <italic>F: faces; L: limbs; Corr: corridors; Char: characters; Car: cars</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-100260-app1-fig10-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100260.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Beijing Normal University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study investigates the development of high-level visual responses in infants, finding that neural responses specific to faces are present by 4-6 months but not earlier. The study is methodologically <bold>convincing</bold>, using state-of-the-art experimental design and analysis approaches. The findings would be of broad interest to the cognitive neuroscience and developmental psychology research communities.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100260.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In the paper, Yan and her colleagues investigate at which stage of development different categorical signals can be detected with EEG using a Steady-state visual evoked potential paradigm. The study reports the development trajectory of selective responses to five categories (i.e., faces, limbs, corridors, characters, and cars) over the first 1.5 years of life. It reveals that while responses to faces show significant early development, responses to other categories (i.e., characters and limbs) develop more gradually and emerge later in infancy. The insights the study provides are important. The paper is well-written and enjoyable, and the content is well-motivated and solid.</p><p>Strengths:</p><p>(1) This study contains a rich dataset with a good amount of effort. It covers a large sample of infants across ages (N=45) asking an interesting question about when we can robustly detect visual category representations during the first year of life of human infants.</p><p>(2) The chosen category stimuli are appropriate and well-controlled. These categories are classic and important for situating the study in the field within a well-established theoretical framework.</p><p>(3) The brain measurements are solid. Visual periodicity allows for the dissociation of selective responses to image categories within the same rapid image stream, which appears at different intervals. This is important for the infant field, where brain measures often lack sensitivity due to the developing brain's low signal-to-noise ratio and short recording time. Considering the significant changes in the brain during infancy, this robust measure of ERPs has good interpretability.</p><p>Weaknesses:</p><p>(1) There is limited data available for each category per infant, with an average of only 5 trials/epochs per category per participant. This insufficient data for each individual weakens the study, as it limits the power of analysis and constrains our understanding of the research question. If more data were available for each tested category per individual, the findings would be more robust and our ability to answer the questions more effectively would be enhanced.</p><p>(2) The study would benefit from a more detailed explanation of analysis choices, limitations, and broader interpretations of the findings. This should include: (a) improving the treatment of bias from specific categories (e.g., faces) towards others; (b) justifying the specific experimental and data analysis choices; and (c) expanding the interpretation and discussion of the results. I believe that giving more attention to these aspects would improve the study and contribute positively to the field.</p><p>Comments on revised submission:</p><p>The authors thoroughly addressed my concerns, and I have no further issues with their response.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100260.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The current work investigates the neural signature of category representation in infancy. Neural responses during steady-state visually-evoked potentials (ssVEPs) were recorded in four age groups of infants between 3 and 15 months. Stimuli (i.e., faces, limbs, corridors, characters, and cars) were presented at 4.286 Hz with category changes occurring at a frequency of 0.857 Hz. Results of the category frequency analyses showed that reliable responses to faces emerge around 4-6 months, whereas response to libs, corridors, and characters emerge around 6-8 months. Additionally, the authors trained a classifier for each category to assess how consistent the responses were across participants (leave-one-out approach). Spatiotemporal responses to faces were more consistent than the responses to the remaining categories and increased with increasing age. Faces showed an advantage over other categories in two additional measures (i.e., representation similarity and distinctiveness). Together, these results suggest a different developmental timing of category representation.</p><p>Strengths:</p><p>The study design is well organized. The authors described and performed analyses on several measures of neural categorization, including innovative approaches to assess the organization of neural responses. Results are in support of one of the two main hypotheses on the development of category representation described in the introduction. Specifically, the results suggest a different timing in the formation of category representations, with earlier and more robust responses emerging for faces over the remaining categories. Graphic representations and figures are very useful when reading the results. The inclusion of the adult sample and results further validate the approach utilized with infants.</p><p>Comments on revised submission:</p><p>The revised manuscript satisfactorily addressed all my previous comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100260.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Yan et al. (&quot;When do visual category representations emerge in infant brains?&quot;) present an EEG study of category-specific visual responses in infancy from 3 to 15 months of age. In their experiment, infants viewed visually controlled images of faces and several non-face categories in a steady state evoked potential paradigm. The authors find visual responses at all ages, but face responses only at 4-6 months and older, and other category-selective responses at later ages. They find that spatiotemporal patterns of response can discriminate faces from other categories at later ages.</p><p>Overall, I found the study well-executed and a useful contribution to the literature. The study advances prior work by using well-controlled stimuli, subgroups at different ages, and new analytic approaches. The data and analyses support their conclusions regarding developmental change in neural responses to high-level visual stimuli.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.100260.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yan</surname><given-names>Xiaoqian</given-names></name><role specific-use="author">Author</role><aff><institution>Fudan University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Tung</surname><given-names>Sarah Shi</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Fascendini</surname><given-names>Bella</given-names></name><role specific-use="author">Author</role><aff><institution>Princeton University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yulan Diana</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Norcia</surname><given-names>Tony</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Grill-Spector</surname><given-names>Kalanit</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>In the paper, Yan and her colleagues investigate at which stage of development different categorical signals can be detected with EEG using a steady-state visual evoked potential paradigm. The study reports the development trajectory of selective responses to five categories (i.e., faces, limbs, corridors, characters, and cars) over the first 1.5 years of life. It reveals that while responses to faces show significant early development, responses to other categories (i.e., characters and limbs) develop more gradually and emerge later in infancy. The paper is well-written and enjoyable, and the content is well-motivated and solid.</p><p>Strengths:</p><p>(1) This study contains a rich dataset with a substantial amount of effort. It covers a large sample of infants across ages (N=45) and asks an interesting question about when visual category representations emerge during the first year of life.</p><p>(2) The chosen category stimuli are appropriate and well-controlled. These categories are classic and important for situating the study within a well-established theoretical framework.</p><p>(3) The brain measurements are solid. Visual periodicity allows for the dissociation of selective responses to image categories within the same rapid image stream, which appears at different intervals. This is important for the infant field, as it provides a robust measure of ERPs with good interpretability.</p><p>Weaknesses:</p><p>The study would benefit from a more detailed explanation of analysis choices, limitations, and broader interpretations of the findings. This includes:</p><p>a) improving the treatment of bias from specific categories (e.g., faces) towards others;</p><p>b) justifying the specific experimental and data analysis choices;</p><p>c) expanding the interpretation and discussion of the results.</p><p>I believe that giving more attention to these aspects would improve the study and contribute positively to the field.</p></disp-quote><p>We thank the reviewer for their clear summary of the work and their constructive feedback. To address the reviewer’s concerns, in the revised manuscript we now provide a detailed explanation of analysis choices, limitations, and broader interpretations, as summarized in the point-by-point responses in the section: Reviewer #1 (Recommendations For The Authors) below, for which we give here an overview in points (a), (b), and (c):</p><p>(a) The reviewer is concerned that using face stimuli as one of the comparison categories may hinder the detection of selective responses to other categories like limbs. Unfortunately, because of the frequency tagging design of our study we cannot compare the responses to one category vs. only some of the other categories (e.g. limbs vs objects but not faces). In other words, our experimental design does not enable us to do this analysis suggested by the reviewer. Nonetheless, we underscore that faces compromise only ¼ of contrast stimuli and we are able to detect significant selective responses to limbs, corridors and characters in infants after 6-8 months of age even as faces are included in the contrast and the response to faces continues to increase (see Fig 4). We discuss the reviewer’s point regarding how contrast can contribute to differences in findings in the discussion on pages 12-13, lines 344-351. Full details below in Reviewer 1: Recommendations for Authors - Frequency tagging category responses.</p><p>(b) We expanded the justification of specific experimental and data analysis choices, see details below in Reviewer 1: Recommendations for Authors -&gt;Specific choices for experiment and data analysis.</p><p>(c) We expand the interpretation and discussion, see details below in Reviewer 1: Recommendations for Authors -&gt; More interpretation and discussion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The current work investigates the neural signature of category representation in infancy. Neural responses during steady-state visually-evoked potentials (ssVEPs) were recorded in four age groups of infants between 3 and 15 months. Stimuli (i.e., faces, limbs, corridors, characters, and cars) were presented at 4.286 Hz with category changes occurring at a frequency of 0.857 Hz. The results of the category frequency analyses showed that reliable responses to faces emerge around 4-6 months, whereas responses to libs, corridors, and characters emerge at around 6-8 months. Additionally, the authors trained a classifier for each category to assess how consistent the responses were across participants (leave-one-out approach). Spatiotemporal responses to faces were more consistent than the responses to the remaining categories and increased with increasing age. Faces showed an advantage over other categories in two additional measures (i.e., representation similarity and distinctiveness). Together, these results suggest a different developmental timing of category representation.</p><p>Strengths:</p><p>The study design is well organized. The authors described and performed analyses on several measures of neural categorization, including innovative approaches to assess the organization of neural responses. Results are in support of one of the two main hypotheses on the development of category representation described in the introduction. Specifically, the results suggest a different timing in the formation of category representations, with earlier and more robust responses emerging for faces over the remaining categories. Graphic representations and figures are very useful when reading the results.</p><p>Weaknesses:</p><p>(1) The role of the adult dataset in the goal of the current work is unclear. All results are reported in the supplementary materials and minimally discussed in the main text. The unique contribution of the results of the adult samples is unclear and may be superfluous.</p><p>(2) It would be useful to report the electrodes included in the analyses and how they have been selected.</p></disp-quote><p>We thank the reviewer for their constructive feedback and for summarizing the strengths and weaknesses of our study. We revised the manuscript to address these two weaknesses.</p><p>(1) The reviewer indicates that the role of the adult dataset is unclear. The goal of testing adult participants was to validate the EEG frequency tagging paradigm. We chose to use adults because a large body of fMRI research shows that both clustered and distributed responses to visual categories are found in adults’ high-level visual cortex. Therefore, the goal of the adult data is to determine whether with the same amount of data as we collect on average in infants, we have sufficient power to detect categorical responses using the frequency tagging experimental paradigm as we use in infants. Because this data serves as a methodological validation purpose, we believe it belongs to the supplemental data.</p><p>We clarify this in the Results, second paragraph, page 5 where now write: “As the EEG-SSVEP paradigm is novel and we are restricted in the amount of data we can obtain in infants, we first tested if we can use this paradigm and a similar amount of data to detect category-selective responses in adults. Results in adults validate the SSVEP paradigm for measuring category-selectivity: as they show that (i) category-selective responses can be reliably measured using EEG-SSVEP with the same amount of data as in infants (Supplementary Figs S1-S2), and that (ii) category information from distributed spatiotemporal response patterns can be decoded with the same amount of data as in infants (Supplementary Fig S3).”</p><p>(2) The reviewer asks us to report the electrodes used in the analysis and their selection. We note that the selection of electrodes included in the analyses has been reported in our original manuscript (Methods, section: Univariate EEG analyses). On pages 18-19, lines 530-538, we write: “Both image update and categorical EEG visual responses are reported in the frequency and time domain over three regions-of-interest (ROIs): two occipito-temporal ROIs (left occipitotemporal (LOT): channels 57, 58, 59, 63, 64, 65 and 68; right occipitotemporal (ROT) channels: 90, 91, 94, 95, 96, 99, and 100) and one occipital ROI (channels 69, 70, 71, 74, 75, 76, 82, 83 and 89). These ROIs were selected a priori based on a previously published study51. We further removed several channels in these ROIs for two reasons: (1) Three outer rim channels (i.e., 73, 81, and 88) were not included in the occipital ROI for further data analysis for both infant and adult participants because they were consistently noisy. (2) Three channels (66, 72, and 84) in the occipital ROI, one channel (50) in the LOT ROI, and one channel (101) in the ROT ROI were removed because they did not show substantial responses in the group-level analyses.”</p><p>In the section Reviewer 2, Recommendations for the authors, we also addressed the reviewer’s minor points.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Yan et al. present an EEG study of category-specific visual responses in infancy from 3 to 15 months of age. In their experiment, infants viewed visually controlled images of faces and several non-face categories in a steady state evoked potential paradigm. The authors find visual responses at all ages, but face responses only at 4-6 months and older, and other category-selective responses at later ages. They find that spatiotemporal patterns of response can discriminate faces from other categories at later ages.</p><p>Overall, I found the study well-executed and a useful contribution to the literature. The study advances prior work by using well-controlled stimuli, subgroups of different ages, and new analytic approaches.</p><p>I have two main reservations about the manuscript: (1) limited statistical evidence for the category by age interaction that is emphasized in the interpretation; and (2) conclusions about the role of learning and experience in age-related change that are not strongly supported by the correlational evidence presented.</p></disp-quote><p>We thank the reviewer for their enthusiasm and their constructive feedback.</p><disp-quote content-type="editor-comment"><p>(1) The overall argument of the paper is that selective responses to various categories develop at different trajectories in infants, with responses to faces developing earlier. Statistically, this would be most clearly demonstrated by a category-by-age interaction effect. However, the statistical evidence for a category by interaction effect presented is relatively weak, and no interaction effect is tested for frequency domain analyses. The clearest evidence for a significant interaction comes from the spatiotemporal decoding analysis (p. 10). In the analysis of peak amplitude and latency, an age x category interaction is only found in one of four tests, and is not significant for latency or left-hemisphere amplitude (Supp Table 8). For the frequency domain effects, no test for category by age interaction is presented. The authors find that the effects of a category are significant in some age ranges and not others, but differences in significance don't imply significant differences. I would recommend adding category by age interaction analysis for the frequency domain results, and ensuring that the interpretation of the results is aligned with the presence or lack of interaction effects.</p></disp-quote><p>The reviewer is asking for additional evidence for age x category interaction by repeating the interaction analysis in the frequency domain. The reason we did not run this analysis in the original manuscript is that the categorical responses of interest are reflected in multiple frequency bins: the category frequency (0.857 Hz) and its harmonics, and there are arguments in the field as to how to quantify response amplitudes from multiple frequency bins (Peykarjou, 2022). Because there is no consensus in the field and also because how the different harmonics combine depends not just on their amplitudes but also on their phase, we chose to transform the categorical responses across multiple frequency bins from the frequency domain to the time domain. The transformed signal in the time domain includes both phase and amplitude information across the category frequency and its harmonics. Therefore, subsequent analyses and statistical evaluations were done in the time domain.</p><p>However, we agree with the reviewer that adding category by age interaction analysis for the frequency domain results can further solidify the results. Thus, in the revised manuscript we added a new analysis, in which we quantified the root mean square (RMS) amplitude value of the responses at the category frequency (0.857 Hz) and its first harmonic (1.714 Hz) for each category condition and infant. Then we used a LMM to test for an age by category interaction. The LMM was conducted separately for the left and right lateral occipitotemporal ROIs. Results of this analysis find a significant category by age interaction, that is, in both hemispheres, the development of response RMS amplitudes varied across category (left occipitotemporal ROIs: βcategory x age = -0.21, 95% CI: -0.39 – -0.04, <italic>t(301)</italic> = -2.40, <italic>pFDR</italic> &lt; .05; right occipitotemporal ROIs: βcategory x age = -0.26, 95% CI: -0.48 – -0.03, <italic>t(301)</italic> = -2.26, <italic>pFDR</italic> &lt; .05). We have added this analysis in the manuscript, pages 7-8, lines 186-193: “We next examined the development of the category-selective responses separately for the right and left lateral occipitotemporal ROIs. The response amplitude was quantified by the root mean square (RMS) amplitude value of the responses at the category frequency (0.857 Hz) and its first harmonic (1.714 Hz) for each category condition and infant. With a LMM analysis, we found significant development of response amplitudes in the both occipitotemporal ROIs which varied by category (left occipitotemporal ROIs: βcategory x age = -0.21, 95% CI: -0.39 – -0.04, t(301) = -2.40, pFDR &lt; .05; right occipitotemporal ROIs: βcategory x age = -0.26, 95% CI – -0.48 – -0.03, t(301) = -2.26, pFDR &lt; .05, LMM as a function of log (age) and category; participant: random effect).” We also added the formula for the LMM analysis in Table 1 in the Methods section, page 21.</p><disp-quote content-type="editor-comment"><p>(2) The authors argue that their results support the claim that category-selective visual responses require experience or learning to develop. However, the results don't bear strongly on the question of experience. Age-related changes in visual responses could result from experience or experience-independent maturational processes. Finding age-related change with a correlational measure does not favor either of these hypotheses. The results do constrain the question of experience, in that they suggest against the possibility that category-selectivity is present in the first few months of development, which would in turn suggest against a role of experience. However the results are still entirely consistent with the possibility of age effects driven by experience-independent processes. The manner in which the results constrain theories of development could be more clearly articulated in the manuscript, with care taken to avoid overly strong claims that the results demonstrate a role of experience.</p></disp-quote><p>Thanks for the comment. We agree with this nuanced point. It is possible that development of category-selective visual responses is a maturational process. In response to this comment, we have revised the manuscript to discuss both perspectives, see revised discussion section – A new insight about cortical development: different category representations emerge at different times during infancy, pages 14-15, lines 403-426, where we now write: “In sum, the key finding from our study is that the development of category selectivity during infancy is non-uniform: face-selective responses and representations of distributed patterns develop before representations to limbs and other categories. We hypothesize that this differential development of visual category representations may be due to differential visual experience with these categories during infancy. This hypothesis is consistent with behavioral research using head-mounted cameras that revealed that the visual input during early infancy is dense with faces, while hands become more prevalent in the visual input later in development and especially when in contact with objects 41,42. Additionally, a large body of research has suggested that young infants preferentially look at faces and face-like stimuli 17,18,33,34, as well as look longer at faces than other objects 41, indicating that not only the prevalence of faces in babies’ environments but also longer looking times may drive the early development of face representations. Further supporting the role of visual experience in the formation of category selectivity is a study that found that infant macaques that are reared without seeing faces do not develop face-selectivity but develop selectivity to other categories in their environment like body parts40. An alternative hypothesis is that differential development of category representations is maturational. For example, we found differences in the temporal dynamics of visual responses among four infant age groups, which suggests that the infant’s visual system is still developing during the first year of life. While the mechanisms underlying the maturation of the visual system in infancy are yet unknown, they may include myelination and cortical tissue maturation 66-71. Future studies can test these alternatives by examining infants’ visual diet, looking behavior, and brain development and examine responses using additional behaviorally relevant categories such as food 72–74. These measurements can test how environmental and individual differences in visual experiences may impact infants’ developmental trajectories. Specifically, a visual experience account predicts that differences in visual experience would translate into differences in development of cortical representations of categories, but a maturational account predicts that visual experience will have no impact on the development of category representations.”</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Major points:</p><p>Bias from faces to other categories:</p><p>- Frequency tagging category responses:</p><p>We see faces from non-face objects and limbs from non-limb objects. Non-limb objects include faces; I suspect that finding the effects of limbs is challenging with faces in the non-limbs category. How would you clarify the choice of categories, and to what extent are the negative (i.e., non-significant) effects on other categories not because of the heavy bias to faces?</p></disp-quote><p>The reviewer is concerned that using face stimuli as one of the comparison categories may hinder the ability to detect selective responses to other categories like limbs in our study. Unfortunately, because of the frequency tagging design of our study, we cannot compare the responses to one category to only some of the other categories (e.g. limbs vs objects but not faces), so our experimental design does not enable us to do the analysis suggested by the reviewer. Nonetheless, we underscore that faces compromise only ¼ of contrast stimuli in the category frequency tagging and we are able to detect significant selective responses to limbs, corridors and characters in infants after 6-8 months of age, when faces are included in the contrast and the responses to faces continue to increase more than for other categories (see Fig 4).</p><p>We address this point in the discussion where we consider differences between our findings and those of Kosakowski et al. 2022, on pages 12-13, lines 344-351 we write: “We note that, the studies differ in several ways: (i) measurement modalities (fMRI in 27 and EEG here), (ii) the types of stimuli infants viewed: in 27 infants viewed isolated, colored and moving stimuli, but in our study, infants viewed still, gray-level images on phase-scrambled backgrounds, which were controlled for several low level properties, and (iii) contrasts used to detect category-selective responses, whereby in 27 the researchers identified within predefined parcels – the top 5% of voxels that responded to the category of interest vs. objects, here we contrasted the category of interest vs. all other categories the infant viewed. Thus, future research is necessary to determine whether differences between findings are due to differences in measurement modalities, stimulus format, and data analysis choices.”</p><disp-quote content-type="editor-comment"><p>- Decoding analyses:</p><p>Figure 5 Winner-take-all classification. First, the classifier may be biased towards the categories with strong and clean data, similar to the last point, this needs clarification on the negative effect. Second, it could be helpful to see how exactly the below-chance decoded categories were being falsely classified to which categories at the group level. Decoding accuracy here means a 20% chance the selection will go to the target category, but the prediction and the exact correlation coefficient the winner has is not explicit; concerning a value of 0.01 correlation could take the winner among negative or pretty bad correlations with other categories. It would be helpful to report how exactly the category was correlated, as it could be a better way to define the classification bias, for example, correlation differences between hit and miss classification. Also, the noise ceiling of the correlation within each group should be provided. Third, this classifier needs improvement in distinguishing between noise and signals to identify the type of information it extracts. Do you have thoughts about that?</p></disp-quote><p>Thanks for the questions, answers below:</p><p>In the winner-take-all (WTA) classifier analysis, at each iteration, the LOOCV classifier computed the correlation between each of the five category vectors from the left-out participant (test data, for an unknown stimulus) and each of the mean spatiotemporal vectors across the N-1 participants (training data, labeled data). The winner-take-all (WTA) classifier classifies the test vector to the category that yields the highest correlation with the training vector. For a given test pattern, correct classification yielded a score of 1 and an incorrect classification yielded a score of 0. Then we computed the group mean decoding performance across all N iterations for each category and the group mean decoding accuracies across five categories.</p><p>For the classification data in Fig 5, the statistics and differences from chance are provided in 5B, where we report overall classification across all categories from an infant’s brain data. Like the reviewer, we were interested in assessing if successful classification is uniform across categories or is driven by some categories. As is visible in 5C, decoding success is non-uniform across categories, and is higher for faces than other categories. Because this is broken by category we cannot compare to chance, and what is reported in Fig 5c is percentage infants in each age group that a particular category was successfully decoded. Starting from 4 months of age, faces can be decoded from distributed brain data in a majority of infants, but other categories only in 20-40% of infants.</p><p>The reviewer also asks about what levels of correlations drive the classification. The analysis of RSMs in Fig 6a shows the mean correlations of distributed responses to different images within and between categories per age group. As is evident from the RSM, reproducible responses for a category only start to emerge at 4-6 months of age and the highest within category correlations are for faces. To quantify what drives the classification we measure distinctiveness - within category minus between-category correlations of distributed responses; all individual infant data per category are in Fig 6C. Distinctiveness values vary by age and category, see text related to Fig 6 in section: What is the nature of categorical spatiotemporal patterns in individual infants?</p><disp-quote content-type="editor-comment"><p>Figure 6 Category distinctiveness. An analysis that runs on a &quot;single item level&quot; would ideally warrant a more informative category distinction. Did you try that? Does it work?</p></disp-quote><p>Thanks for the question. We agree that doing an analysis at the single item level would be interesting. However, none of the images were repeated, so we do not have sufficient SNR to perform this analysis.</p><disp-quote content-type="editor-comment"><p>Specific choices for experiment and data analysis:</p><p>- Although using the SSVEP paradigm is familiar to the field, the choice could be detailed for understanding or evaluation of the effectiveness of the paradigm. For example, how the specific frequency for entrainment was chosen, and are there any theories or related warrants for studying in infants?</p></disp-quote><p>Thanks for the questions. We choose to use the SSVEP paradigm over traditional ERP designs for several reasons, as described which have been listed in our original manuscript (Results part, first paragraph, pages 4-5, lines 90-94): “We used the EEG-SSVEP approach because: (i) it affords a high signal-to-noise ratio with short acquisitions making it effective for infants 23,46, (ii) it has been successfully used to study responses to faces in infants23,46,49, and (iii) it enables measuring both general visual response to images by examining responses at the image presentation frequency (4.286 Hz), as well as category-selective responses by examining responses at the category frequency (0.857 Hz, Fig 1A).”</p><p>With regards to our choice of presentation rate, a previous study in 4-6-month-olds by de Heering and Rossion (2015) used SSVEP showing infants faces and objects presented the visual stimuli at 6 Hz (i.e. 167 ms per image) to study infants’ categorical responses to natural faces relative to objects. Here, we chose to use a relatively slower presentation rate, which was 4.286 Hz (i.e. 233 ms per image), so that our infant participants would have more time to process each image yet still unlikely to make eye movements across a stimulus. Both de Heering et (2015) and our study have found significant selective responses to faces relative to other categories in 4-6-month-olds, across these presentation rates. As discussed in a recent review of frequency tagging with infants: The visual oddball paradigm (Peykarjou, 2022), there are many factors to consider when adapting SSVEP paradigms to infants. We agree that an interesting direction for future studies is examination of how SSVEP parameters such as stimulus and oddball presentation rate, and overall duration of acquisition affects the sensitivity of the SSVEP paradigm in infants. We added a discussion point on this on page 12, lines 332-334 where we write: “As using SSVEP to study high-level representations is a nascent field52–54, future work can further examine how SSVEP parameters such as stimulus and target category presentation rate may affect the sensitivity of measurements in infants (see review by54).”</p><disp-quote content-type="editor-comment"><p>- There is no baseline mentioned in the study. How was the baseline considered in the paradigm and data analysis? The baseline is important for evaluating how robust/ reliable the periodic responses within each group are in the first place. It also helps us to see how different the SNR changes in the fast periodic responses from baseline across age groups. Would the results be stable if the response amplitudes were z-scored by a baseline?</p></disp-quote><p>Thanks for the question. Previous studies using a similar frequency tagging paradigm have compared response amplitude at stimulus-related frequencies to that of neighboring frequency bins as their baseline for differentiating signal from noise. We use a more statistically powerful method, the Hotelling’s T2 statistic to test whether response amplitudes were statistically different from 0 amplitude. Importantly, this method takes into consideration both the amplitude and phase information of the response. That is, a significant response is expected to have consistent phase information across participants as well as significant amplitude.</p><disp-quote content-type="editor-comment"><p>- Statistical inferences: could the variance of data be considered appropriately in your LLM? Why?</p></disp-quote><p>As we have explained in our original manuscript (Methods part, section-Statistical Analyses of Developmental Effects, page 21 lines 611-615): “LMMs allow explicit modeling of both within-subject effects (e.g., longitudinal measurements) and between-subject effects (e.g., cross-sectional data) with unequal number of points per participants, as well as examine main and interactive effects of both continuous (age) and categorical (e.g., stimulus category) variables. We used random-intercept models that allow the intercept to vary across participants (term: 1|participant).” This statistical model is widely used in developmental studies that combine both longitudinal and cross-sectional measurements (e.g. Nordt et al. 2022, 2023; Natu et al. 2021; Grotheer et al. 2022).</p><disp-quote content-type="editor-comment"><p>- The sampling of the age groups. Why are these age groups considered, as 8-12 months are not considered? Or did the study first go with an equal sampling of the ages from 3 to 15 months? Then how was the age group defined? The log scale of age makes sense for giving a simplified view of the effects, but the sampling procedure could be more detailed.</p></disp-quote><p>Thanks for the question. Our study recruited infants longitudinally for both anatomical MRI and EEG studies. Some of the infants participated in both studies and some only in one of the studies. Infants were recruited at around newborn, 3 months, 6 months, and 12 months. We did not recruit infants between 8-12 months of age because around 9 months there is little contrast between gray and white matter in anatomical MRI scans that were necessary for the MRI study. For the EEG study we binned the subjects by age group such that there were a similar number of participants across age groups to enable similar statistical power. The division of age groups was decided based on the distribution of the infants included in the analyses.</p><p>We have now added the sampling procedure details in the Methods, part, under section: Participants, pages 15-16, lines 440-445: “Sixty-two full-term, typically developing infants were recruited. Twelve participants were part of an ongoing longitudinal study that obtained both anatomical MRI and EEG data in infants. Some of the infants participated in both studies and some only in one of the studies. Infants were recruited at around newborn, 3 months, 6 months, and 12 months. We did not recruit infants between 8-12 months of age because around 9 months there is little contrast between gray and white matter in anatomical MRI scans that were necessary for the MRI study.”</p><disp-quote content-type="editor-comment"><p>- 30 Hz cutoff is arbitrary, but it makes sense as most EEG effects can be expected in a lower frequency band than higher. However, this specific choice is interesting and informative, when faced with developmental data and this type of paradigm. Would the results stay robust as the cutoff changes? Would the results benefit from going even lower into the frequency cutoff?</p></disp-quote><p>In the time domain analyses, we choose the 30 Hz cutoff to be consistent with previous EEG studies including those done with infants. However, as our results from the frequency domain (Fig 3, right panel, and supplementary Fig S6-S9) show that there are barely any selective categorical responses above about 6 Hz. Therefore, we expect that using a lower frequency cutoff, such as 10 Hz, will not lead to different results.</p><disp-quote content-type="editor-comment"><p>More interpretation and discussion:</p><p>- You report the robust visual responses in occipital regions, the responses that differ across age groups, and their characteristics (i.e., peak latency and amplitude) in time curves. This part of the results needs more interpretation to help the data be better situated in the field; I wondered whether this relates to the difference in the signal processing of the information. Could this be the signature of slow recurrence connection development? Or how could this be better interpreted?</p></disp-quote><p>Thanks for the question. Changes in speed of processing can arise from several related reasons including (i) myelination of white matter connections that would lead to faster signal transmission (Lebenberg et al. 2019; Grotheer et al. 2022), (ii) maturation of cortical visual circuits affecting temporal integration time, and (iii) development of feedback connections. Our data cannot distinguish among these different mechanisms. Future studies that combine functional high temporal resolution measurements with structural imaging of tissue properties could elucidate changes in cortical dynamics over development.</p><p>We added this as a discussion point, on page 15 lines 416-420 we write: “For example, we found differences in the temporal dynamics of visual responses among four infant age groups, which suggests that the infant’s visual system is still developing during the first year of life. While underlying maturational mechanisms are yet unknown, they may include myelination and cortical tissue maturation68–73.”</p><disp-quote content-type="editor-comment"><p>- The supplementary material includes a detailed introduction to the methods when facing the developing visual acuity, which justifies the choice of the paradigm. I appreciate this thorough explanation. Interestingly, high visual acuity has its potential developmental downside; for instance, low visual acuity would aid in the development of holistic processing associated with face recognition (as discussed by Vogelsang et al., 2018, in PNAS). How do you view this point in relation to the emergence of complex cognitive processes, as here the category-selective responses?</p></disp-quote><p>Thanks for linking this to the Vogelsang (2018) study. Just as faces are processed in a hierarchical manner, starting with low-level features (edges, contours) and progressing to high-level features (identity, expression), other complex visual categories like cars, scenes, and body parts follow similar hierarchies. Early holistic processing could provide a foundation for recognizing objects quickly and efficiently, while feature-based processing might allow for more precise recognition and categorization as acuity increases. Therefore, as visual acuity improves, an infant’s brain can integrate finer details into those holistic representations, supporting more refined and complex cognitive processes. The balance between low- and high-level visual acuity highlights the intricate interplay between sensory processing and cognitive development across various domains.</p><disp-quote content-type="editor-comment"><p>Minor points:</p><p>Paradigm:</p><p>- Are the colored cartoon images for motivating infants' fixation counterbalanced across categories in the paradigm? Or how exactly were the cartoon images presented in the paradigm?</p></disp-quote><p>Response: Yes, the small cartoon images that were presented at the center of the screen during stimuli presentation were used to engage infants’ attention and accommodation to the screen. For each condition, they were randomly drawn from a pool of 70 images (23 flowers, 22 butterflies, 25 birds) from categories unrelated to the ones under test. They were presented in random order with durations uniformly distributed between 1 and 1.5 s. We have added these details of the paradigm to the Methods section, page 17, lines 479-481: “To motivate infants to fixate and look at the screen, we presented at the center of the screen small (~1°) colored cartoon images such as butterflies, flowers, and ladybugs. They were presented in random order with durations uniformly distributed between 1 and 1.5 s.”</p><disp-quote content-type="editor-comment"><p>Analysis:</p><p>- Are the visual responses over the occipital cortex different across different category conditions in the first place? I guess this should not be different; this probably needs one more supplementary figure.</p></disp-quote><p>The visual responses reflect the responses to images that are randomly drawn from the five stimuli categories at a presentation frequency of 4.286 Hz. The only difference between the five conditions is that the stimuli presentation order is different. Therefore, the visual response over the occipital cortex across conditions should not be different within an age group.</p><p>In the revised manuscript, we have added Supplementary Figure S5 that shows the frequency spectra distribution and the response topographies of the visual response at 4.286 Hz and its first 3 harmonics separately for each condition and age group and a new Supplementary Materials section: 5. Visual responses over occipital cortex per condition for all age groups. On page 5, lines 116-120, we now write: “Analysis of visual responses in the occipital ROI separately by category condition revealed that visual responses were not significantly across category condition Supplementary Fig S5, no significant main effect of category (βcategory = 0.08, 95% CI: -0.08 – 0.24, t(301) = 0.97, p = .33), or category by age interaction (βcategory x age = -0.04, 95% CI: -0.11 – 0.03, t(301) = -1.09, p = .28, LMM on RMS of response to first three harmonics).”</p><disp-quote content-type="editor-comment"><p>- The summary of epochs used for each category for each age group needs to be included; this is important while evaluating whether the effects are due to not having enough data for categories or others.</p></disp-quote><p>This part of information is provided in the manuscript in the Methods section, page 18 lines 521-524, and supplementary Table S2. Our analysis shows that there was no significant difference in the number of pre-processed epochs across different age groups (<italic>F(3,57)</italic> = 1.5, <italic>p</italic> = .2).</p><disp-quote content-type="editor-comment"><p>- Numbers of channels of EEG being interpolated should be provided; is that a difference across age groups?</p></disp-quote><p>Thanks for the suggestion. We have now added information about the number of channels being interpolated for each age groups in the Methods section (page 18, lines 525-528): “The number of electrodes being interpolated for each age group were 10.0 ± 4.8 for 3-4-month-olds, 9.9 ± 3.7 for 4-6-month-olds, 9.9 ± 3.9 for 6-8-month-olds, and 7.7 ± 4.7 for 12-15-month-olds. There was no significant difference in the number of electrodes being interpolated across infant age-groups (F(3,55) = 0.78, p = .51).”</p><disp-quote content-type="editor-comment"><p>- I noticed that the removal of EEG artifacts (i.e., muscles and eye-blinks) for data analysis is missing; did the preprocessing pipeline involve any artifacts removing procedures that are typically used in both infants and adults SSVEP data analysis? If so, please provide more information.</p></disp-quote><p>In our analysis, artifact rejection was performed in two steps. First, the continuous filtered data were evaluated according to a sample-by-sample thresholding procedure to locate consistently noisy channels. Channels with more than 20% of samples exceeding a 100-150 μV amplitude threshold were replaced by the average of their six nearest spatial neighbors. Once noisy channels were interpolated in this fashion, the EEG was re-referenced from the Cz reference used during the recording to the common average of all sensors and segmented into epochs (1166.7-ms). Finally, EEG epochs that contained more than 15% of time samples exceeding threshold (150-200 microvolts) were excluded on a sensor-by-sensor basis. This method is provided in the manuscript under Methods section, page 18 lines 510-516.</p><disp-quote content-type="editor-comment"><p>Figure:</p><p>- Supplementary Figure 8. The illustration of the WTA classifier was not referred to anywhere in the main text.</p></disp-quote><p>Thanks for pointing this out. The supplementary Figure 8 should be noted as supplementary Figure 10 instead. We have now mentioned it in the manuscript, page 10, line 267.</p><disp-quote content-type="editor-comment"><p>- Figure 5 WTA classifier needed to be clarified. It was correlation-based but used to choose the most correlated response patterns averaged across the N-1 subjects for the leave-one-out subject. The change from correlation coefficients to decoding accuracy could be clearer as I spent some time making sense of it. The correlation coefficient here evaluates how correlated the two vectors are, but the actual decoding accuracy estimated at the end is the percentage of participants who can be assigned to the &quot;ground truth&quot; label, so one step in between is missing. Can this be better illustrated?</p></disp-quote><p>Thanks for surfacing that this is not described sufficiently clearly and for your suggestions. The spatiotemporal vector was calculated separately for each category. This is illustrated in Fig 5A. At each iteration, the LOOCV classifier computed the correlation between each of the five category vectors from the left-out participant (test data, for an unknown stimulus) and each of the mean spatiotemporal vectors across the N-1 participants (training data, labeled data). The winner-take-all (WTA) classifier classifies the test vector to the category that yields the highest correlation with the training vector. This is illustrated in Fig 5A, with spatiotemporal patterns and correlation values from an example infant shown. For a given test pattern, correct classification yields a score of 1 and an incorrect classification yields a score of 0. We compute the percentage correct across all categories for each left-out-infant, and then mean decoding performance across all participants in an age group (Fig 5B). We have now added these details in the Methods part, section – Decoding analyses, Group-level, page 20 lines 590-597, where we write: “At each iteration, the LOOCV classifier computed the correlation between each of the five category vectors from the left-out participant (test data, for an unknown stimulus) and each of the mean spatiotemporal vectors across the N-1 participants (training data, labeled data). The winner-take-all (WTA) classifier classifies the test vector to the category of the training vector that yields the highest correlation with the training vector (Fig 5A). For a given test pattern, correct classification yields a score of 1 and an incorrect classification yields a score of 0. For each left-out infant, we computed the percentage correct across all categories, and then the mean decoding performance across all participants in an age group (Fig 5B).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>I only have some minor comments.</p><p>Typo on line 90 (&quot;Infants participants in 5 conditions, which [...]&quot;).</p></disp-quote><p>Thanks for pointing this out. We have now corrected ‘participants’ to ‘participated’.</p><disp-quote content-type="editor-comment"><p>Typo on lines 330: &quot;[...] in example 4-5-months-olds.&quot;.</p></disp-quote><p>Thanks for pointing this out. We changed ‘4-5-months-olds’ to ‘4-5-month-olds’.</p><disp-quote content-type="editor-comment"><p>Figure 2 - bar plots: rotating and spacing out values on the x-axis may improve readability. Ditto for the line plots in Figure 4.</p></disp-quote><p>Thanks for the suggestions. In the revised manuscript, we have improved the readability of Figure 2.</p><disp-quote content-type="editor-comment"><p>Caption of Figure 6: description of the distinctiveness plots may refer to panel C, instead of the bottom panels of section B.</p></disp-quote><p>Thanks for pointing this out. We have now corrected this information in the manuscript.</p></body></sub-article></article>