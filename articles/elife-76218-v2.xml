<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">76218</article-id><article-id pub-id-type="doi">10.7554/eLife.76218</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Selfee, self-supervised features extraction of animal behaviors</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-264682"><name><surname>Jia</surname><given-names>Yinjun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8281-0669</contrib-id><email>jyj20@mails.tsinghua.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-161386"><name><surname>Li</surname><given-names>Shuaishuai</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-264683"><name><surname>Guo</surname><given-names>Xuan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-282048"><name><surname>Lei</surname><given-names>Bo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-264684"><name><surname>Hu</surname><given-names>Junqiang</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-161390"><name><surname>Xu</surname><given-names>Xiao-Hong</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-263377"><name><surname>Zhang</surname><given-names>Wei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0512-3096</contrib-id><email>wei_zhang@mail.tsinghua.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cve4549</institution-id><institution>School of Life Sciences, IDG/McGovern Institute for Brain Research, Tsinghua University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05kje8j93</institution-id><institution>Tsinghua-Peking Center for Life Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vpwhm04</institution-id><institution>Institute of Neuroscience, State Key Laboratory of Neuroscience, Chinese Academy of Sciences Center for Excellence in Brain Science and Intelligence Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0551a0y31</institution-id><institution>Shanghai Center for Brain Science and Brain-Inspired Intelligence Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zlatic</surname><given-names>Marta</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00tw3jy02</institution-id><institution>MRC Laboratory of Molecular Biology</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>VijayRaghavan</surname><given-names>K</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ht1xw27</institution-id><institution>National Centre for Biological Sciences, Tata Institute of Fundamental Research</institution></institution-wrap><country>India</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e76218</elocation-id><history><date date-type="received" iso-8601-date="2021-12-08"><day>08</day><month>12</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-06-15"><day>15</day><month>06</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-12-24"><day>24</day><month>12</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.12.24.474120"/></event></pub-history><permissions><copyright-statement>© 2022, Jia et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Jia et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-76218-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-76218-figures-v2.pdf"/><abstract><p>Fast and accurately characterizing animal behaviors is crucial for neuroscience research. Deep learning models are efficiently used in laboratories for behavior analysis. However, it has not been achieved to use an end-to-end unsupervised neural network to extract comprehensive and discriminative features directly from social behavior video frames for annotation and analysis purposes. Here, we report a self-supervised feature extraction (Selfee) convolutional neural network with multiple downstream applications to process video frames of animal behavior in an end-to-end way. Visualization and classification of the extracted features (Meta-representations) validate that Selfee processes animal behaviors in a way similar to human perception. We demonstrate that Meta-representations can be efficiently used to detect anomalous behaviors that are indiscernible to human observation and hint in-depth analysis. Furthermore, time-series analyses of Meta-representations reveal the temporal dynamics of animal behaviors. In conclusion, we present a self-supervised learning approach to extract comprehensive and discriminative features directly from raw video recordings of animal behaviors and demonstrate its potential usage for various downstream applications.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd><italic>Drosophila</italic></kwd><kwd>mouse</kwd><kwd>behavior</kwd><kwd>deep learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32022029</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Wei</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31871059</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Wei</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009592</institution-id><institution>Beijing Municipal Science and Technology Commission</institution></institution-wrap></funding-source><award-id>Z181100001518001</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Wei</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004147</institution-id><institution>Tsinghua University</institution></institution-wrap></funding-source><award-id>IDG/McGovern Institute for Brain Research - Brain + X</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Wei</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Chinese Institute for Brain Research, Beijing</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zhang</surname><given-names>Wei</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Young Thousand Talent Program of China</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zhang</surname><given-names>Wei</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Selfee, a self-supervised learning approach, is designed to extract comprehensive and discriminative features directly from raw videos of animal behaviors which can be used for in-depth analysis.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Extracting representative features of animal behaviors has long been an important strategy for studying the relationship between genes, neural circuits, and behaviors. Traditionally, human observations and descriptions are the primary solutions for animal behavior analysis (<xref ref-type="bibr" rid="bib28">Hall, 1994</xref>; <xref ref-type="bibr" rid="bib53">McGill, 1962</xref>; <xref ref-type="bibr" rid="bib70">Rubenstein and Alcock, 2019</xref>). Well-trained researchers would define a set of behavior patterns and compare their intensity or proportion between experimental and control groups. With the emergence and flourish of machine learning methodology, supervised learning has been assisting human annotations and achieved impressive results (<xref ref-type="bibr" rid="bib38">Jiang et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Segalin et al., 2020</xref>). Nevertheless, supervised learning is limited by prior knowledge (especially used for feature engineering) and manually assigned labels, thus could not identify behavioral features that are not annotated.</p><p>Other machine learning methods were then introduced to the field which was designed to extract representative features beyond human-defined labels. These methods can be generally divided into two major categories: one estimates animal postures with a group of pre-defined key points of the body parts, and the other directly transforms raw images. The former category marks representative key points of animal bodies, including limbs, joints, trunks, and/or other body parts of interest (<xref ref-type="bibr" rid="bib25">Graving et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Günel et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Mathis et al., 2018</xref>). Those features are usually sufficient to represent animal behaviors. However, it has been demonstrated that the key points generated by pose estimation are less effective for direct behavior classification or two-dimensional visualization (<xref ref-type="bibr" rid="bib50">Luxem et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Sun et al., 2021</xref>). Sophisticated post-processing like recurrent neural networks (RNNs) (<xref ref-type="bibr" rid="bib50">Luxem et al., 2020</xref>), non-locomotor movement decomposition (<xref ref-type="bibr" rid="bib33">Huang et al., 2021</xref>), or feature engineering (<xref ref-type="bibr" rid="bib75">Sun et al., 2021</xref>) can be applied to transform the key points into higher-level discriminative features. Additionally, the neglection of body parts could cause problems. For example, the position of the proboscis of a fly is commonly neglected in behavior studies using pose estimation software (<xref ref-type="bibr" rid="bib11">Calhoun et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Sun et al., 2021</xref>). Still, it is crucial for feeding (<xref ref-type="bibr" rid="bib89">Zhou et al., 2019</xref>), licking behavior during courtship (<xref ref-type="bibr" rid="bib56">Mezzera et al., 2020</xref>), and hardness detection for a substrate (<xref ref-type="bibr" rid="bib87">Zhang et al., 2020</xref>). Finally, best to our knowledge, there is no demonstration of these pose-estimation methods applied to multiple animals of the same color with intensive interactions. Thus, the application of pose estimation to mating behaviors of two black mice, a broadly adopted behavior paradigm (<xref ref-type="bibr" rid="bib5">Bayless et al., 2019</xref>; <xref ref-type="bibr" rid="bib80">Wei et al., 2018</xref>; <xref ref-type="bibr" rid="bib88">Zhang et al., 2021</xref>), could be limited because labeling body parts during mice mounting is challenging even for humans (see Discussion for more details). Therefore, using these feature extraction methods requires rigorously controlled experimental settings, additional feature engineering, and considerable prior knowledge of particular behaviors.</p><p>In contrast, the other category transforms pixel-level information without key point labeling, thus retaining more details and requiring less prior knowledge. Feature extraction of images could be achieved by wavelet transforms (<xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>) or Radon transforms <xref ref-type="bibr" rid="bib6">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Ravbar et al., 2019</xref> followed by principal component analysis (PCA), and these transforms can be applied to either 2D images or depth images. However, preprocessing such as segmentation and/or registration of the images is required to achieve spatial invariance in some implementations, a task that is particularly difficult for multi-agent videos (one method named ABRS solved this problem by using the spectra of Radon transforms, which is translation and rotation invariant; <xref ref-type="bibr" rid="bib67">Ravbar et al., 2019</xref>). Beyond translation or rotation invariance, the relative position between animals is also important to social behaviors. Animals could be of varies of relative positions when perform the same type of behaviors. Extracted features should be invariant to these variations and capture the major characteristics. Additionally, because these methods usually use unlearnable transforms, although it makes them highly transferrable from dataset to dataset, they could not be adaptive to images with different characteristics and thus select the most discriminative and relevant features across the dataset automatically. Flourished deep learning methods, especially convolutional neural networks (CNNs) (<xref ref-type="bibr" rid="bib44">Lecun et al., 1998</xref>), could adaptively extract features from diversified datasets. Also, they have been proven more potent than classic computer vision algorithms like wavelet transforms (<xref ref-type="bibr" rid="bib69">Romero et al., 2009</xref>) and Radon transforms (<xref ref-type="bibr" rid="bib3">Aradhya et al., 2007</xref>) on a famous grayscale dataset MNIST, even without supervising (<xref ref-type="bibr" rid="bib36">Ji et al., 2019</xref>). Therefore, we attempt to adopt CNNs to achieve end-to-end feature extractions of animal behaviors that are comprehensive and discriminative.</p><p>The cutting-edge self-supervised deep learning methods aim to extract representative features for downstream missions by comparing different augmentations of the same image and/or different images (<xref ref-type="bibr" rid="bib13">Caron et al., 2020</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>; <xref ref-type="bibr" rid="bib31">He et al., 2020</xref>; <xref ref-type="bibr" rid="bib83">Wu et al., 2018</xref>). Compared with previous techniques, these methods have three major advantages. First, self-supervised or unsupervised methods could completely avoid human biases. Second, the augmentations used to create positive samples promise invariance of the neural networks to object sizes, spatial orientations, and ambient laminations so that registration or other preprocessing is not required. Finally, the networks are optimized to export similar results for positive samples and separate negative ones, such that the extracted features are inherently discriminative. Even without negative samples, the networks can utilize differential information within batches to obtain remarkable results on downstream missions like classification or image segmentation (<xref ref-type="bibr" rid="bib16">Chen and He, 2021</xref>; <xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>; <xref ref-type="bibr" rid="bib86">Zbontar et al., 2021</xref>). These advances in self-supervised learning provide a promising way to analyze animal behaviors.</p><p>In this work, we develop Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) that adopts recently published self-supervised learning algorithms and CNNs to analyze animal behaviors. Selfee is trained on massive unlabeled behavior video frames (around five million frames from hundreds of videos) to avoid human bias in annotating animal behaviors, and it could capture a global character of animal behaviors even when detailed postures are hard to extract, similar to human perception. During the training process, Selfee learns to project images to a low-dimensional space without being affected by shooting conditions, image translation, and rotation, where cosine distance is proper to measure the similarities of original pictures. Selfee also provides potential for various downstream analyses. We demonstrate that the extracted features are suitable for t-SNE visualization, <italic>k</italic>-NN-based classification, <italic>k</italic>-NN-based anomaly detection, and dynamic time warping (DTW). We also show that further integrated modeling, like the autoregressive hidden Markov model (AR-HMM), is compatible with Selfee extracted Meta-representations. We apply Selfee to fruit flies, mice, and rats, three widely used model animals, and validate our results with manual annotations or pre-existed animal tracking methods. Discoveries of behavioral phenotypes in mutant flies by Selfee are consistent with either human observations or other animal tracking analysis, and can be validated by biological experiments. The performance of Selfee on these model species indicates its potential usage for behavioral studies of non-model animals as well as other tasks. We also provide an open-source Python project and pre-trained models of flies and mice to the community (see more in Code Availability).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Workflow of Selfee and its downstream analyses</title><p>Selfee is trained to generate Meta-representations at the frame level, then analyzed at different time scales. First, grayscale videos are decomposed into single frames, and three tandem frames are stacked into a live-frame to generate a motion-colored RGB picture (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). These live-frames preserve not only spatial information (e.g., postures of each individual or relative distances and angles between individuals) within each channel but also temporal information across different channels. Live-frames are used to train Selfee to produce comprehensive and discriminative representations at the frame level (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). These representations can be later used in numerous applications. For example, anomaly detection on mutant animals can discover new phenotypes compared with their genetic controls (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Also, the AR-HMM could be applied to model the micro-dynamics of behaviors, such as the duration of states or the probabilities of state transitions (<xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>). The AR-HMM splits videos into modules and yields behavioral state usages that visualize differences between genotypes (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). In contrast, DTW could compare the long-term dynamics of animal behaviors and capture global differences at the video level (<xref ref-type="bibr" rid="bib60">Myers et al., 1980</xref>) by aligning pairs of time series and calculating their similarities (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). These three demonstrations cover different time scales from frame to video level, and other downstream analyses could also be incorporated into the workflow of Selfee.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The framework of Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) and its downstream applications.</title><p>(<bold>A</bold>) One live-frame is composed of three tandem frames in R, G, and B channels, respectively. The live-frame could capture the dynamics of animal behaviors. (<bold>B</bold>) Live-frames are used to train Selfee, which adopts a backbone of ResNet-50. (C, D, and E) Representations produced by Selfee could be used for anomaly detection that could identify unusual animal postures in the query video compared with the reference videos. (<bold>C</bold>) AR-HMM (autoregressive hidden Markov model) that models the local temporal characteristics of behaviors and clusters frames into modules (states) and calculates stages usages of different genotypes (<bold>D</bold>) DTW (dynamic time warping) that aligns behavior videos to reveal differences of long-term dynamics (<bold>E</bold>) and other potential tasks including behavior classification, forecasting, or even image segmentation and pose estimation after appropriately modifying and fine-tuning of the neural networks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Beddings and backgrounds that affect training and inference of Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction).</title><p>(<bold>A</bold>) Textures on the damped filter paper would mislead Selfee to output features similar to copulation but not wing extension (ground truth). The left example showed a background that would not affect Selfee neural network, and the right example showed a background that could strongly affect classification accuracy. (<bold>B</bold>) Background inconsistency would affect the training process when Selfee was applied to mice behavior data. Therefore, backgrounds were removed from all frames to avoid potential defects. After background removal, illumination normalization was applied.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>t-SNE visualization of pose estimation derived features.</title><p>(<bold>A</bold>) Visualization of fly courtship live-frames with t-SNE dimension reduction of distances between key points, including head, tail, thorax, and wings. Each dot was colored based on human annotations. Points representing non-interactive behaviors (‘others’), chasing, wing extension, copulation attempt, and copulation were colored with red, yellow, green, blue, and violet, respectively. (<bold>B</bold>) Visualization of fly courtship live-frames with t-SNE dimension reduction of human-engineered features, including male head to female tail distance, male body to female body distance, male wing angle, female wing angle, and angle between male body axis. Each dot was colored based on human annotations. Points representing non-interactive behaviors (‘others’), chasing, wing extension, copulation attempt, and copulation were colored with red, yellow, green, blue, and violet, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Animal tracking with DLC, FlyTracker, and SLEAP.</title><p>(<bold>A</bold>) Visualization of DLC tracking results on intensive interactions between mice during mating behavior. The nose, ears, body center, hips, and bottom were labeled. DLC tended to detect two animals as one due to occluding. (<bold>B</bold>) Visualization of FlyTracker tracking results provided in Fly-vs-Fly dataset. The head, tail, center, and wings were colored in red, blue, purple, and green, respectively. When two flies were close, wings became hard to be detected correctly. (<bold>C</bold>) Visualization of SLEAP tracking results on close interactions during fly courtship behavior. Five body parts were marked with points, including head, tail, thorax, and wings, and head to tail, thorax to wings were linked by lines. Female was labeled in red, and the male was in blue. Body parts were wrongly assigned when two animals were close.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig1-figsupp3-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig1-video1.mp4" id="fig1video1"><label>Figure 1—video 1.</label><caption><title>Visualization of DLC tracking results on intensive interactions between mice during mating behavior.</title><p>The nose, ears, body center, hips, and bottom were labeled. DLC worked great when two animals were separated, but it tended to detect two animals as one during mounting or intromission due to occluding.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig1-video2.mp4" id="fig1video2"><label>Figure 1—video 2.</label><caption><title>A tracking example of FlyTracker of Fly-vs-Fly dataset.</title><p>Visualization of FlyTracker tracking results provided in Fly-vs-Fly dataset. The head, tail, center, and wings were colored in red, blue, purple, and green, respectively. The tracking result was very competent even compared with deep learning-based methods. However, when two flies were close, wings, even bodies, became hard to be detected correctly.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig1-video3.mp4" id="fig1video3"><label>Figure 1—video 3.</label><caption><title>A tracking example of SLEAP on fly courtship behavior.</title><p>Visualization of SLEAP tracking results on fly courtship behavior. Five body parts were marked with points, including head, tail, thorax, and wings, and head to tail, thorax to wings were linked by lines. Female was labeled in red, and the male was in blue. In general, the tracking result was good. However, body parts were wrongly assigned when two animals were close, and performance was significantly impaired during copulation attempt and copulation.</p></caption></media></fig-group><p>Compared with previous machine learning frameworks for animal behavior analysis, Selfee has three major advantages. First, Selfee and the Meta-representations could be used for various tasks. The contrastive learning process of Selfee would allow output features to be appropriately compared by cosine similarity. Therefore, distance-based applications, including classification, clustering, and anomaly detection, would be easily realized. It was also reported that with some adjustment of backbones, self-supervised learning would facilitate tasks such as pose estimation (<xref ref-type="bibr" rid="bib21">Dahiya et al., 2021</xref>) and object segmentation (<xref ref-type="bibr" rid="bib14">Caron et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">He et al., 2020</xref>). Those findings indicate that Selfee could be generalized, modified, and fine-tuned for animal pose estimation or segmentation tasks. Second, Selfee is a fully unsupervised method developed to annotate animal behaviors. Although some other techniques also adopt semi-supervised or unsupervised learning, they usually require manually labeled pre-defined key points of the images (<xref ref-type="bibr" rid="bib33">Huang et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Luxem et al., 2020</xref>); some methods also require expert-defined programs for better performance (<xref ref-type="bibr" rid="bib75">Sun et al., 2021</xref>). Key point selection and program incorporation require a significant amount of prior knowledge and are subject to human bias. In contrast, Selfee does not need any prior knowledge. Finally, Selfee is relatively hardware-inexpensive compared with other self-supervised learning methods (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>). Training Selfee only takes 8 hr on a single RTX 3090 GPU (graphic card), and the inference speed could reach 800 frames per second. Selfee could accept top-view 2D grayscale video frames as inputs so that neither depth cameras (<xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>) nor fine-calibrated multi-view camera arrays (<xref ref-type="bibr" rid="bib33">Huang et al., 2021</xref>) are required. Therefore, Selfee can be trained and used with routinely collected behavior videos on ordinary desktop workstations, warranting its accessibility to biology laboratories.</p></sec><sec id="s2-2"><title>Siamese CNNs capture discriminative representations of animal posture</title><p>Selfee contains a pair of Siamese CNNs trained to generate discriminative representations for live-frames. ResNet-50 (<xref ref-type="bibr" rid="bib30">He et al., 2016</xref>) is chosen as the backbone whose classifier layer is replaced by a three-layer multi-layer perceptron (MLP). These MLPs are called projectors which yield final representations during the inference stage. There are two branches in Selfee. The main branch is equipped with an additional predictor, while the reference branch is a copy of the main branch (the SimSiam style; <xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>). To the best of our knowledge, the SimSiam style is the most straightforward Siamese CNN frameworks with only one term of loss. Both branches contain group discriminators after projectors and perform dimension reduction on extracted features for online clustering (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The network structure of Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction).</title><p>(<bold>A</bold>) The architecture of Selfee networks. Each live-frame is randomly transformed twice before being fed into Selfee. Data augmentations include crop, rotation, flip, Turbo, and color jitter. (<bold>B</bold>) Selfee adopts a SimSiam-style network structure with additional group discriminators. Loss 1 is canonical negative cosine loss, and loss 2 is the newly proposed CLD (cross-level instance-group discrimination) loss. (<bold>C</bold>) A brief illustration of two loss terms used in Selfee. The first term of loss is negative cosine loss, and the outcome from the reference branch is detached from the computational graph to prevent mode collapse. The second term of loss is the CLD loss. All data points are colored based on the clustering result of the upper branch, and points representing the same instance are attached by lines. For one instance, the orange triangle, its representation from one branch is compared with cluster centroids of another branch and yields affinities (green arrows). Loss 2 is calculated as the cross-entropy between the affinity vector and the cluster label of its counterpart (blue arrows).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Different augmentations used for Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) training.</title><p>(<bold>A</bold>) Visualization of each augmentation. For data augmentations, crop, rotation, flip, Turbo, and color jitter were applied. Each live-frame was randomly cropped into a smaller version containing more than 49% (70%×70%) of the original image; then the image was randomly (clockwise or anticlockwise) rotated for an angle smaller than the acute angle formed by the diagonal line and the vertical line, then the image would be vertically flipped, horizontally flipped, and/or applied the Turbo lookup table at the probability of 50%, respectively; and finally, the brightness, contrast, saturation, and hue were randomly adjusted within 10% variation. Detailed descriptions of each augmentation could be found in the Materials and methods and source codes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig2-figsupp1-v2.tif"/></fig></fig-group><p>During the training stage, batches of live-frames are randomly transformed twice and fed into the main branch and reference branch, respectively. Augmentations applied to live-frames include crop, rotation, flip, and application of the Turbo lookup table (<xref ref-type="bibr" rid="bib57">Mikhailov, 2019</xref>) followed by color jitters (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The reference branch yields a representation of received frames, while the main branch predicts the outcome of the reference branch. This is the first objective of the training process, which optimizes the cosine similarity between the outcome of the reference branch and its prediction given by the main branch. To prevent mode collapse, the reference branch will not receive gradient information during optimization, which means the outcome of the reference branch is detached from the computational graph and treated as ground truth (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The second objective is to optimize the clustering results of representations from two discriminators. For one instance, its representation from one branch is compared with cluster centroids of another branch and yields affinities. The affinity vector is optimized to be consistent with the cluster label of its counterpart. In the original publication (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>), this loss is termed cross-level instance-group discrimination (CLD) loss (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Because the main branch and reference branch are symmetric, similar loss calculations can also be done after swapping the identity of these two branches. The final loss is the average loss under the two conditions. In this way, Selfee is trained to be invariant to those transforms and focus on critical information to yield discriminative representations.</p><p>After the training stage, we evaluated the performance of Selfee with t-SNE visualization and <italic>k</italic>-NN classification. To investigate whether our model captured human-interpretable features, we manually labeled one clip of <italic>Drosophila</italic> courtship video and visualized those representations with t-SNE dimension reduction. On the t-SNE map, human-annotated courtship behaviors, including chasing, wing extension, copulation attempt, copulation, and non-interactive behaviors (‘others’), were grouped in a non-random pattern (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Then, we would like to know if our neural network could capture fine-grained features beyond human-defined labels. Using cutting-edge animal tracking software SLEAP (<xref ref-type="bibr" rid="bib64">Pereira et al., 2022</xref>), flies’ wings, heads, tails, and thoraxes were tracked throughout the clip automatically with manual proofreading (<xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>). Three straight-forward features were visualized on the t-SNE map. Distances between male heads and female tails could indicate chasing intensity; wing angles of male flies were correlated to wing extension behavior, and distances of male flies away from the chamber center could reflect the trade-off between their thigmotaxis (<xref ref-type="bibr" rid="bib7">Besson and Martin, 2005</xref>) and courtship motivation. We found that the features extracted by Selfee separated wing extension behaviors based on the wing angles (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>) and male head to female tail distance (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D</xref>). The result also showed that chasing behavior could be separated based on the positions of male flies in the chamber, which might indicate the thigmotaxis level. In conclusion, Selfee is capable of extracting comprehensive features that consist of human observations or animal tracking results, and in this way, Selfee uniforms natural-languages-based human descriptions and engineered features from tracking results in different units (e.g., rad for angle and mm for distance) and scales in a single discriminative Meta-representation.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The validation of Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) with human annotations.</title><p>(<bold>A</bold>) Visualization of fly courtship live-frames with t-SNE dimension reduction. Each dot was colored based on human annotations. Points representing chasing, wing extension, copulation attempt, copulation, and non-interactive behaviors (‘others’) were colored with yellow, green, blue, violet and red, respectively. (<bold>B</bold>) The confusion matrix of the <italic>k</italic>-NN classifier for fly courtship behavior, normalized by the numbers of each behavior in the ground truth. The average <italic>F</italic><sub>1</sub> score of the sevenfold cross-validation was 72.4%, and mAP was 75.8%. The recall of each class of behaviors was indicated on the diagonal of the confusion matrix. (<bold>C</bold>) A visualized comparison of labels produced by the <italic>k</italic>-NN classifier and human annotations of fly courtship behaviors. The <italic>k</italic>-NN classifier was constructed with data and labels of all seven videos used in the cross-validation, and the <italic>F</italic><sub>1</sub> score was 76.1% and mAP was 76.1%. (<bold>D</bold>) Visualization of live-frames of mice mating behaviors with t-SNE dimension reduction. Each dot is colored based on human annotations. Points representing non-interactive behaviors (‘others’), social interest, mounting, intromission, and ejaculation were colored with red, yellow, green, blue, and violet, respectively. (<bold>E</bold>) The confusion matrix of the LightGBM (Light Gradient Boosting Machine) classifier for mice mating behaviors, normalized by the numbers of each behavior in the ground truth. For the LightGBM classifier, the average <italic>F</italic><sub>1</sub> score of the eightfold cross-validation was 67.4%, and mAP was 69.1%. The recall of each class of behaviors was indicated on the diagonal of the confusion matrix. (<bold>F</bold>) A visualized comparison of labels produced by the LightGBM classifier and human annotations of mice mating behaviors. An ensemble of eight trained LightGBM was used, and the <italic>F</italic><sub>1</sub> sore was 68.1% and mAP was not available for this ensembled classifier due to the voting mechanism.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) captured fine-grained features related to animal postures and positions.</title><p>(<bold>A</bold>) Visualization of fly courtship live-frames with t-SNE dimension reduction. Each dot was colored based on human annotations. Points representing non-interactive behaviors (‘others’), chasing, wing extension, copulation attempt, and copulation were colored with red, yellow, green, blue, and violet, respectively. Same as <xref ref-type="fig" rid="fig3">Figure 3A</xref>. (<bold>B</bold>) Fly skeletons were semi-automated labeled, and three features were used in the following panels. Five body parts were marked with points, including head, tail, thorax, wings, and head to tail, thorax to wings were linked by lines. Females were indicated by blue color and males were indicated by orange color. Three features were male head to female tail distance, male thorax to chamber center, and the angle between male wings, which were indicated in violet. (<bold>C</bold>) Male wing angles were visualized on the t-SNE map same as panel A. Frames of wing extension behaviors in the red box were of relatively smaller wing angles than those in the orange box. Some examples from these two groups were exhibited on the right, with their angle values below each image. (<bold>D</bold>) Male head to female tail distances were visualized on the t-SNE map same as panel A. Frames of wing extension behaviors in the red box were of relatively shorter distance than those in the orange and violet boxes. Some examples from these three groups were exhibited on the right, with their distance values below each image. (<bold>E</bold>) Male thorax to chamber center distances were visualized on the t-SNE map same as panel A. Frames of chasing behaviors in the violet box were of relatively shorter distance than those in the orange box. Some examples from these two groups were exhibited on the right, with their distance values below each image.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Difficulties on fly courtship behavior classification.</title><p>(<bold>A</bold>) Some wing extension frames are hard to distinguish from chasing behaviors. Images in the first row were labeled as no wing extension; images in the second to the fourth rows were labeled as wing extension. Images in the second row were of relatively weak wing extension (blue indicators pointed at slightly extended wings), and the fourth row showed a process from no wing extension to strong wing extension. (<bold>B</bold>) The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in inferred labels. The average <italic>F</italic><sub>1</sub> score of the sevenfold cross-validation was 72.4%, and mAP was 75.8%. The precision of each class of behaviors was indicated on the diagonal of the confusion matrix.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Classification of mice mating behaviors with Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) extracted features.</title><p>(<bold>A</bold>) For the <italic>k</italic>-NN classifier, the average <italic>F</italic><sub>1</sub> score of the eightfold cross-validation was 59.0%, and mAP was 53.0%. The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in the ground truth. The recall of each class of behaviors was indicated on the diagonal of the confusion matrix. (<bold>B</bold>) The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in inferred labels. The precision of each class of behaviors was indicated on the diagonal of the confusion matrix. (<bold>C</bold>) The confusion matrix of the LightGBM (Light Gradient Boosting Machine) classifier, normalized by the numbers of each behavior in inferred labels. The precision of each class of behaviors was indicated on the diagonal of the confusion matrix. The LightGBM classifier had a much better performance compared with the <italic>k</italic>-NN classifier.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title><italic>k</italic>-NN classification of rat behaviors with Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) trained on mice datasets.</title><p>(<bold>A</bold>) The average <italic>F</italic><sub>1</sub> score of the ninefold cross-validation was 49.6%, and mAP was 46.6%. The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in the ground truth. The recall of each class of behaviors was indicated on the diagonal of the confusion matrix. (<bold>B</bold>) The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in inferred labels. The precision of each class of behaviors was indicated on the diagonal of the confusion matrix.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig3-figsupp4-v2.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Ablation test of Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) training process on fly datasets.</title><p>(<bold>A</bold>) The distribution of different behaviors in wild-type flies courtship videos. (<bold>B</bold>) Visualization of the same live-frames as <xref ref-type="fig" rid="fig3">Figure 3A</xref> with t-SNE dimension reduction. Used representations were extracted by models trained without cross-level instance-group discrimination (CLD) loss. Each dot is colored based on human annotations. The legend is shared with panel A. (<bold>C</bold>) The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in the ground truth. The recall of each class of behaviors was indicated on the diagonal of the confusion matrix. Used representations were extracted by models trained without CLD loss. (<bold>D</bold>) Collapse levels during the training process. Collapse level was calculated as one minus to the average standard deviation of each channel of the representation multiplied by the square root of the channel number. One means maximum collapse, while zero means no collapse. Without CLD loss, Selfee suffered from catastrophic mode collapse. Details for collapse level calculation could be found in Materials and methods. (<bold>E</bold>) Visualization of the same live-frames as <xref ref-type="fig" rid="fig3">Figure 3A</xref> with t-SNE dimension reduction. Used representations were extracted by models trained without Turbo transformation. Each dot is colored based on human annotations. The legend is shared with panel A. (<bold>F</bold>) The confusion matrix of the <italic>k</italic>-NN classifier, normalized by the numbers of each behavior in the ground truth. The recall of each class of behaviors was indicated on the diagonal of the confusion matrix. Used representations were extracted by models trained without Turbo transformation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig3-figsupp5-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig3-video1.mp4" id="fig3video1"><label>Figure 3—video 1.</label><caption><title>Pose estimation of fly courtship behaviors.</title><p>Flies’ wings, heads, tails, and thoraxes were tracked throughout the clip automatically using cutting-edge animal tracking software SLEAP, and each frame was carefully manual proofread by human researchers.</p></caption></media></fig-group><p>Meta-representations can also be used for behavior classification. We manually labeled seven 10,000-frame videos (around 5 min each) as a pilot dataset. A weighed <italic>k</italic>-NN classifier was then constructed as previously reported (<xref ref-type="bibr" rid="bib83">Wu et al., 2018</xref>). Sevenfold cross-validation was performed on the dataset with the <italic>k</italic>-NN classifier, which achieved a mean <italic>F</italic><sub>1</sub> score of 72.4% and achieved a similar classification result as human annotations (<xref ref-type="fig" rid="fig3">Figure 3B and C</xref>). The classifier had the worst recall score on wing extension behaviors (67% recall, <xref ref-type="fig" rid="fig3">Figure 3B</xref>), likely because of the ambiguous intermediate states between chasing and wing extension (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). The precisions also showed that this <italic>k</italic>-NN classifier tended to have strict criteria for wing extension and copulation and relatively loose criteria for chasing and copulation attempts (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). It was reported that independent human experts could only reach agreements on around 70% of wing extension frames (<xref ref-type="bibr" rid="bib45">Leng et al., 2020</xref>), comparable to the performance of our <italic>k</italic>-NN classifier.</p><p>Next, we compared Selfee extracted features with animal-tracking derived features. In a previous study <xref ref-type="bibr" rid="bib45">Leng et al., 2020</xref>, labeled wing extension intensities of male flies for three clips of male-female interaction videos: each frame was scored from 0 (no wing extension) to 3 (strong wing extension) by two experienced researchers. Here, we used their summarized score as ground-truth labels. FlyTracker (<xref ref-type="bibr" rid="bib22">Fleet et al., 2014</xref>) was used to track each fly’s body, wings and legs, and the identity swap between male and female flies was manually corrected in the previous work. We used four types of post-processing for the tracking results. Features from FlyTracker and JAABA (<xref ref-type="bibr" rid="bib39">Kabra et al., 2013</xref>) were from the work by <xref ref-type="bibr" rid="bib45">Leng et al., 2020</xref>. We also constructed pure distance-based features recording distances between all key points (heads, tails, thoraxes, wings, and with or without legs). For a fair comparison, we evaluated the performance of weight <italic>k</italic>-NN classifiers in sixfold cross-validations for all types of features, and none of the additional temporal information aggregation was used (e.g., sliding window voting used in Selfee, bout features used in FlyTracker, or window features used in JAABA). Three evaluation metrics were applied, including a robust version of Pearson’s correlation coefficient (<xref ref-type="bibr" rid="bib42">Lai et al., 2019</xref>), <italic>F</italic><sub>1</sub> score, and average precision. We found that Selfee extracted features achieved comparable results with FlyTracker features or JAABA features, and it achieved the best performance evaluated by Pearson’s correlation and <italic>F</italic><sub>1</sub> score (<xref ref-type="table" rid="table1">Table 1</xref>). We also found that additional irrelevant key points marking fly legs would strongly interfere the performance of pure distance-based features, indicating that key point choice and feature engineering were crucial for downstream classification. The comparison also yielded an interesting result that the pure distance-based feature could even outperform human-engineered features like FlyTracker features. Distances between key points indeed preserved detailed information on animal behaviors, but it remained unclear if they could capture the universals of behavioral stereotypes. For further investigation, we visualized distance-based features and human-engineered features on the same clip as in <xref ref-type="fig" rid="fig3">Figure 3A</xref> with t-SNE dimension reduction. Results showed that distance-based features were overfocused on subtle differences between frames and neglected differences between major types of behaviors. In contrast, human-engineered features formed distinct clusters corresponding to human annotations on the t-SNE map (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), so did Selfee features (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Therefore, although pure distance-based features could outperform human-engineered features and Selfee features using highly non-linear <italic>k</italic>-NN classifier, they were less abstractive. Overall, Selfee extracted features are as discriminative as classic animal-tracking derived features, but could be used more easily without careful key points definition or dedicated feature engineering.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>A comparison between Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) extracted features and animal-tracking derived features.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Evaluationssetups</th><th align="left" valign="bottom">Pearson’s R</th><th align="left" valign="bottom"><italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">AP</th></tr></thead><tbody><tr><td align="left" valign="bottom">Selfee</td><td align="char" char="." valign="bottom"><bold>0.774</bold><xref ref-type="table-fn" rid="table1fn1"><sup>*</sup></xref></td><td align="char" char="." valign="bottom"><bold>0.629</bold><xref ref-type="table-fn" rid="table1fn1"><sup>*</sup></xref></td><td align="char" char="." valign="bottom">0.354</td></tr><tr><td align="left" valign="bottom">FlyTracker -&gt;FlyTracker</td><td align="char" char="." valign="bottom">0.756</td><td align="char" char="." valign="bottom">0.571</td><td align="char" char="." valign="bottom">0.330</td></tr><tr><td align="left" valign="bottom">FlyTracker -&gt;JAABA</td><td align="char" char="." valign="bottom">0.755</td><td align="char" char="." valign="bottom">0.613</td><td align="char" char="." valign="bottom">0.346</td></tr><tr><td align="left" valign="bottom">FlyTracker (w/o legs) -&gt;distance</td><td align="char" char="." valign="bottom">0.771</td><td align="char" char="." valign="bottom">0.613</td><td align="char" char="." valign="bottom"><bold>0.374</bold><xref ref-type="table-fn" rid="table1fn1">*</xref></td></tr><tr><td align="left" valign="bottom">FlyTracker (w/ legs) -&gt;distance</td><td align="char" char="." valign="bottom">0.629</td><td align="char" char="." valign="bottom">0.400</td><td align="char" char="." valign="bottom">0.256</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>Best results of different feature extractors under each evaluation metric are indicated in bold values.</p></fn></table-wrap-foot></table-wrap><p>We then asked whether Selfee can be generalized to analyze behaviors of other species. We fine-tuned fly video pre-trained Selfee with mice mating behavior data. The mating behavior of mice can be defined mainly into five categories (<xref ref-type="bibr" rid="bib53">McGill, 1962</xref>), including social interest, mounting, intromission, ejaculation, and others (see Materials and methods for detailed definitions). With t-SNE visualization, we found that five types of behaviors could be separated by Selfee, although mounting behaviors were rare and not concentrated (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). We then used eight human-annotated videos to test the <italic>k</italic>-NN classification performance of Selfee-generated features. We achieved an <italic>F</italic><sub>1</sub> score of 59.0% (Table 3-Replication 1, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Mounting, intromission, and ejaculation share similar static characteristics but are different in temporal dynamics. Therefore, we asked if more temporal information would assist the classification. Using the LightGBM (Light Gradient Boosting Machine) classifier (<xref ref-type="bibr" rid="bib40">Ke et al., 2017</xref>), we achieved a much higher classification performance by incorporating slide moving average and standard deviation of 81-frame time windows, the main frequencies, and their energy within 81-frame time windows. The average <italic>F</italic><sub>1</sub> score of eightfold cross-validation could reach 67.4%, and the classification results of the ensembled classifier (see Materials and methods) were closed to human observations (<xref ref-type="fig" rid="fig3">Figure 3E and F</xref>). Nevertheless, it was still difficult to distinguish between mounting, intromission, and ejaculation because mounting and ejaculation are much rarer than social body contact or intromission.</p><p>Selfee is more robust than the vanilla SimSiam networks when applied to the behavioral data. Behavioral data often suffer from severe imbalance. For example, copulation attempts are around sixfold rarer than wing extension during fly courtship (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5A</xref>). Therefore, we added group discriminators to vanilla SimSiam networks, which were reported to fight against the long-tail effect proficiently (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>). As noted in the original publication of CLD loss, <italic>k</italic>-means clustering lifted weights of rare behaviors in batches from the reciprocal of the batch size to the reciprocal of the cluster number, for which the imbalance between majority and minority classes can be ameliorated (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>). Aside from overcoming the long-tail effect, we also found group discriminators helpful for preventing mode collapse during ablation studies (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5B-D</xref> and <xref ref-type="table" rid="table2">Table 2</xref>). We hypothesized that the convergence could be easily reached on images of similar objects (two flies), by which CNNs may not be well trained to extract good representations. Without CLD loss, it could be easier to output similar representations than to distinguish images apart, because only the attraction between positive samples was applied. Using CLD loss, Selfee could explicitly utilize negative samples and encourage both repulsion and attraction. Therefore, the mode collapse could be largely avoided. Aside from CLD loss, Turbo transformation was another customed modification. Applying the Turbo lookup table on grayscale frames brought more complexity and made color distortions more powerful on grayscale images, one of the most critical type of augmentations reported before (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>). Because hue and saturation of a grayscale image are meaningless, color jitter can be strongly enhanced after Turbo transforms. Selfee would capture more useful features with this Turbo augmentation (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5E, F</xref> and <xref ref-type="table" rid="table2">Table 2</xref>). Live-frames used by Selfee were also helpful to capture temporal information of animal behaviors, especially for highly dynamic ones such as mice mating behaviors. We found that Selfee features extracted from single frames were less discriminative than those extracted from live-frames (<xref ref-type="table" rid="table3">Table 3</xref>). In summary, our modifications to original SimSiam designs, including live-frames extracting temporal dynamics, Turbo transformation customed for grayscale images, and CLD loss coping with the long-tailed nature of behavioral data, provide a significant performance boost in the case of animal behavior analysis.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>An ablation test of Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) training process on fly datasets.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom" colspan="2">Pre-trained ResNet-50 with random projectors</th><th align="left" valign="bottom" colspan="2">Selfee</th><th align="left" valign="bottom" colspan="2">Selfee without CLD loss</th><th align="left" valign="bottom" colspan="2">Selfee without Turbo transformation</th></tr><tr><th align="left" valign="bottom">Evaluation</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th></tr></thead><tbody><tr><td align="left" valign="bottom">Replication 1</td><td align="char" char="." valign="bottom">0.586</td><td align="char" char="." valign="bottom">0.580</td><td align="char" char="." valign="bottom">0.724</td><td align="char" char="." valign="bottom">0.758</td><td align="char" char="." valign="bottom">0.227</td><td align="char" char="." valign="bottom">0.227</td><td align="char" char="." valign="bottom">0.604</td><td align="char" char="." valign="bottom">0.550</td></tr><tr><td align="left" valign="bottom">Replication 2</td><td align="char" char="." valign="bottom">0.597</td><td align="char" char="." valign="bottom">0.570</td><td align="char" char="." valign="bottom">0.676</td><td align="char" char="." valign="bottom">0.683</td><td align="char" char="." valign="bottom">0.163</td><td align="char" char="." valign="bottom">0.200</td><td align="char" char="." valign="bottom">0.574</td><td align="char" char="." valign="bottom">0.551</td></tr><tr><td align="left" valign="bottom">Replication 3</td><td align="char" char="." valign="bottom">0.596</td><td align="char" char="." valign="bottom">0.586</td><td align="char" char="." valign="bottom">0.714</td><td align="char" char="." valign="bottom">0.754</td><td align="char" char="." valign="bottom">0.172</td><td align="char" char="." valign="bottom">0.214</td><td align="char" char="." valign="bottom">0.517</td><td align="char" char="." valign="bottom">0.497</td></tr><tr><td align="left" valign="bottom">Best</td><td align="char" char="." valign="bottom">0.597</td><td align="char" char="." valign="bottom">0.586</td><td align="char" char="." valign="bottom"><bold>0.724</bold><xref ref-type="table-fn" rid="table2fn1"><sup>*</sup></xref></td><td align="char" char="." valign="bottom"><bold>0.758</bold><xref ref-type="table-fn" rid="table2fn1"><sup>*</sup></xref></td><td align="char" char="." valign="bottom">0.227</td><td align="char" char="." valign="bottom">0.227</td><td align="char" char="." valign="bottom">0.604</td><td align="char" char="." valign="bottom">0.551</td></tr></tbody></table><table-wrap-foot><fn id="table2fn1"><label>*</label><p>Best results of different training setups under each evaluation metric are indicated in bold values.</p></fn></table-wrap-foot></table-wrap><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>An ablation test of Selfee training process on mice datasets.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom" colspan="2">Single frame + KNN</th><th align="left" valign="bottom" colspan="2">Live-frame + KNN</th><th align="left" valign="bottom" colspan="2">Single frame + LGBM</th><th align="left" valign="bottom" colspan="2">Live-frame + LGBM</th></tr><tr><th align="left" valign="bottom">Evaluation</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th><th align="left" valign="bottom">Mean <italic>F</italic><sub>1</sub> score</th><th align="left" valign="bottom">Mean AP</th></tr></thead><tbody><tr><td align="left" valign="bottom">Replication 1</td><td align="char" char="." valign="bottom">0.554</td><td align="char" char="." valign="bottom">0.498</td><td align="char" char="." valign="bottom">0.590</td><td align="char" char="." valign="bottom">0.530</td><td align="char" char="." valign="bottom">0.645</td><td align="char" char="." valign="bottom">0.671</td><td align="char" char="." valign="bottom">0.674</td><td align="char" char="." valign="bottom">0.691</td></tr><tr><td align="left" valign="bottom">Replication 2</td><td align="char" char="." valign="bottom">0.574</td><td align="char" char="." valign="bottom">0.508</td><td align="char" char="." valign="bottom">0.599</td><td align="char" char="." valign="bottom">0.549</td><td align="char" char="." valign="bottom">0.653</td><td align="char" char="." valign="bottom">0.663</td><td align="char" char="." valign="bottom">0.663</td><td align="char" char="." valign="bottom">0.699</td></tr><tr><td align="left" valign="bottom">Replication 3</td><td align="char" char="." valign="bottom">0.566</td><td align="char" char="." valign="bottom">0.514</td><td align="char" char="." valign="bottom">0.601</td><td align="char" char="." valign="bottom">0.539</td><td align="char" char="." valign="bottom">0.652</td><td align="char" char="." valign="bottom">0.692</td><td align="char" char="." valign="bottom">0.663</td><td align="char" char="." valign="bottom">0.700</td></tr><tr><td align="left" valign="bottom">Mean</td><td align="char" char="." valign="bottom">0.565</td><td align="char" char="." valign="bottom">0.507</td><td align="char" char="." valign="bottom">0.597</td><td align="char" char="." valign="bottom">0.539</td><td align="char" char="." valign="bottom">0.650</td><td align="char" char="." valign="bottom">0.675</td><td align="char" char="." valign="bottom"><bold>0.667</bold><xref ref-type="table-fn" rid="table3fn1"><sup>*</sup></xref></td><td align="char" char="." valign="bottom"><bold>0.697</bold><xref ref-type="table-fn" rid="table3fn1"><sup>*</sup></xref></td></tr><tr><td align="left" valign="bottom">Best</td><td align="char" char="." valign="bottom">0.574</td><td align="char" char="." valign="bottom">0.514</td><td align="char" char="." valign="bottom">0.601</td><td align="char" char="." valign="bottom">0.549</td><td align="char" char="." valign="bottom">0.653</td><td align="char" char="." valign="bottom">0.692</td><td align="char" char="." valign="bottom"><bold>0.674</bold><xref ref-type="table-fn" rid="table3fn1"><sup>*</sup></xref></td><td align="char" char="." valign="bottom"><bold>0.700</bold><xref ref-type="table-fn" rid="table3fn1"><sup>*</sup></xref></td></tr></tbody></table><table-wrap-foot><fn id="table3fn1"><label>*</label><p>Best results of different training setups under each evaluation metric are indicated in bold values.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-3"><title>Anomaly detection at the frame level identifies rare behaviors at the sub-second time scale</title><p>The representations produced by Selfee could be directly used for anomaly detection without further post-processing. During the training step, Selfee learns to compare Meta-representations of frames with cosine distance which is also used for anomaly detection. When given two groups of videos, namely the query group and the reference group, the anomaly score of each live-frame in the query group is calculated in two steps (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). First, distances between the query live-frame and all reference live-frames are measured, and the <italic>k</italic>-nearest distance is referred to as its inter-group score (IES). Without further specification, <italic>k</italic> equals 1 in all anomaly detections in this work, which is a trivial and intuitive case of the <italic>k</italic>-NN algorithm to avoid parameter search of <italic>k</italic>. Some false positives occurred when only the IES was used as the anomaly score (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). The reason could be that two flies in a chamber could be in mathematically infinite relative positions and form a vast event space. However, each group usually only contains several videos, and each video is only recorded for several minutes. For some rare postures, even though the probability of observing them is similar in both the query and reference group, they might only occur in the query group but not in the reference group. Therefore, an intra-group score (IAS) is introduced in the second step to eliminate these false-positive effects. We assume that those rare events should not be sampled frequently in the query groups either. Thus, the IAS is defined as the <italic>k</italic>-nearest distance of the query frame against all other frames within its group, except those within the time window of ±50 frames, because representations for frames beyond 50 frames were less similar to the current frame (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). The final anomaly score is defined as the IES minus the IAS.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Anomalous posture detection using Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction)-produced features.</title><p>(<bold>A</bold>) The calculation process of anomaly scores. Each query frame is compared with every reference frame, and the nearest distance was named IES (the thickness of lines indicates distances). Each query frame is also compared with every query frame, and the nearest distance is called IAS. The final anomaly score of each frame equals IES minus IAS. (<bold>B</bold>) Anomaly detection results of 15 fly lines with mutations in neurotransmitter genes or with specific neurons silenced ( n = 10,9,10,7,12,15,29,7,16,8,8,6,7,7,9,7, respectively). RA is short for CCHa2-R-RA, and RB is short for CCHa2-R-RB. CCHa2-R-RB<sup>Gal4</sup>&gt;Kir2.1, q&lt;0.0001; <italic>Trh<sup>Gal4</sup></italic>, q=0.0432; one-way ANOVA with Benjamini and Hochberg correction. (<bold>C</bold>) Examples of mixed tussles and copulation attempts identified in CCHa2-R-RB<sup>Gal4</sup>&gt;Kir2.1 flies. (<bold>D</bold>) The temporal dynamic of anomaly scores during the mixed behavior, centralized at 1.67 s. SEM is indicated with the light color region. (<bold>E</bold>) Examples of close body contact behaviors identified in <italic>Trh<sup>Gal4</sup></italic> flies. (<bold>F</bold>) The cosine similarity between the center frame of the close body contact behaviors (1.67 s) and their local frames. SEM is indicated with the light color region. (<bold>G</bold>) The kicking index of <italic>Trh<sup>Gal4</sup></italic> flies (<italic>n</italic>=30) was significantly lower than <italic>w<sup>1118</sup></italic> flies (<italic>n</italic>=27), p=0.0034, Mann-Whitney test. (<bold>H</bold>) Examples of social aggregation behaviors of <italic>Trh<sup>Gal4</sup></italic> flies and <italic>w<sup>1118</sup></italic> flies. Forty male flies were transferred into a vertically placed triangle chamber (blue dashed lines), and the photo was taken after 20 min. A fly was indicated by a blue arrow. The lateral sides of the chamber were 16.72 cm. (<bold>I</bold>) Social distances of <italic>Trh<sup>Gal4</sup></italic> flies (<italic>n</italic>=6) and <italic>w<sup>1118</sup></italic> flies (<italic>n</italic>=6). <italic>Trh<sup>Gal4</sup></italic> flies had much closer social distances with each other compared with <italic>w<sup>1118</sup></italic> flies; nearest, p=0.0043; median, p=0.002; average, p=0.0087; all Mann-Whitney test. (<bold>J</bold>) Distributions of the median social distance of <italic>Trh<sup>Gal4</sup></italic> flies and <italic>w<sup>1118</sup></italic> flies. Distributions were calculated within each replication. Average distributions were indicated with solid lines, and SEMs were indicated with light color regions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Using intra-group score (IAS) to eliminate false-positive results in anomaly detections.</title><p>(<bold>A</bold>) Anomaly scores without IAS of wild-type male-male interactions with the same genotype as references. The blue region indicates the max anomaly score when using IAS; blue dots indicate anomaly scores without IAS that fall into the blue region; red dots indicate false-positive anomaly scores. (<bold>B</bold>) The cosine similarity between the center frame of wild-type courtship behaviors (1.67 s) and their local frames. SEM is indicated with the light color region. Seven videos containing 70,000 frames were split into non-overlapping 100-frame fragments for calculations. Beyond ±50 frames, the cosine similarity dropped to a much lower level, not affecting anomaly detection.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig4-figsupp1-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig4-video1.mp4" id="fig4video1"><label>Figure 4—video 1.</label><caption><title>The anomaly detection on male-male interactions of RB-Gal4 &gt;Kir2.1 flies.</title><p>The anomalous behavior was brief tussle behavior mixed with copulation attempts. This behavior was ultra-fast and lasted for less than a quarter second.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig4-video2.mp4" id="fig4video2"><label>Figure 4—video 2.</label><caption><title>The anomaly detection on male-male interactions of <italic>Trh<sup>Gal4</sup></italic> flies.</title><p>The anomalous behavior was short-range body interactions. These social interactions could last for around half to 1 s on average.</p></caption></media></fig-group><p>To test whether our methods could detect anomalous behavior in real-world data, we performed anomaly detection to 15 previously recorded neurotransmitter-related mutant alleles or neuron-silenced lines (with UAS-Kir2.1; <xref ref-type="bibr" rid="bib62">Paradis et al., 2001</xref>; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Their male-male interaction videos were inferred by Selfee trained on male-female courtship videos. Since we aimed to find interactions distinct from male-male courtship behaviors, a baseline of ppk23 &gt;Kir2.1 flies was established because this line exhibits strong male-male courtship behaviors (<xref ref-type="bibr" rid="bib76">Thistle et al., 2012</xref>). We compared the top-100 anomaly scores from sets of videos from experimental groups and wild-type control flies. The results revealed that one line, CCHa2-R-RB&gt;Kir2.1, showed a significantly high anomaly score. By manually going through all anomalous live-frames, we further identified its phenotype as a brief tussle behavior mixed with copulation attempts (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>, 0.2× play speed). This behavior was ultra-fast and lasted for less than a quarter second (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), making it difficult to be detected by human observers. Up to this point, we have demonstrated that the frame-level anomaly detection could capture sub-second behavior episodes that human observers tend to neglect.</p><p>Selfee also revealed that <italic>Trh</italic> (<italic>Tryptophan hydroxylase</italic>) knock-out flies had close body contact compared to wild-type. <italic>Trh</italic> is the crucial enzyme for serotonin biosynthesis (<xref ref-type="bibr" rid="bib19">Coleman and Neckameyer, 2005</xref>), and its mutant flies showed a statistically significantly higher anomaly score (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) than the wild-type control. Selfee identified 60 frames of abnormal behaviors within 42,000 input frames, occupying less than 0.15% of the total recording time. By manually going through all these frames, we concluded most of them as short-range body interactions (<xref ref-type="fig" rid="fig4">Figure 4E</xref> and <xref ref-type="video" rid="fig4video2">Figure 4—video 2</xref>, 0.2× play speed). These social interactions could last for around 1 s on average (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Even though serotonin signals were well studied for controlling aggression behavior in flies (<xref ref-type="bibr" rid="bib2">Alekseyenko et al., 2014</xref>), to the best of our knowledge, the close body contact of flies and serotonergic neurons’ role in this behavior has not been reported yet. Considering this behavior is not as fast as the ones of CCHa2-R-RB&gt;Kir2.1 flies for humans, a possible reason is that this behavior is too scarce to be noticed by human experts.</p><p>To further ask whether these close body contacts have biological significance, we performed corresponded behavior assays on mutant flies. Based on the fact that the <italic>Trh</italic> mutant male flies have a higher tolerance to body touch, we hypothesized that they would have a decreased defensive behavior. As previously reported, fruit flies show robust defensive behavior to mechanical stimuli on their wings (<xref ref-type="bibr" rid="bib46">Li et al., 2016</xref>; <xref ref-type="bibr" rid="bib48">Liu et al., 2020</xref>). Decapitated flies would kick with their hind legs when a thin probe stimulates their wings. This stimulation mimics the invasion of parasitic mites and could be used to test its defensive behavior. Our results showed that <italic>Trh</italic> knock-out flies had a significantly lower kicking rate than control flies (<xref ref-type="fig" rid="fig4">Figure 4G</xref>), indicating a reduction of self-defensive intensity. Next, we performed social behavior assay (<xref ref-type="bibr" rid="bib55">McNeil et al., 2015</xref>; <xref ref-type="bibr" rid="bib73">Simon et al., 2012</xref>) on the mutant flies because the close body contact can also be explained by reduced social distance. We measured the nearest distance, median distance, and average distance of each male fly in a 40-individual group placed in a vertical triangular chamber (<xref ref-type="fig" rid="fig4">Figure 4H</xref>). By comparing median values of these distances of each replication, <italic>Trh</italic> knock-out flies kept significantly shorter distances from others than the control group (<xref ref-type="fig" rid="fig4">Figure 4H, I</xref>). The probability density function of their median distances also showed that knock-out flies had a closer social distance than control flies (<xref ref-type="fig" rid="fig4">Figure 4J</xref>). Therefore, we concluded that <italic>Trh</italic> knock-out flies had reduced self-defensive behavior and social distance, which validated the anomaly detected by Selfee. Taken together, Selfee is capable of discovering novel features of animal behaviors with biological relevance when a proper baseline is defined.</p></sec><sec id="s2-4"><title>Modeling motion structure of animal behaviors</title><p>Animal behaviors have long-term structures beyond single-frame postures. The duration and proportions of each bout and transition probabilities of different behaviors have been proven to have biological significance (<xref ref-type="bibr" rid="bib58">Mueller et al., 2019</xref>; <xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>). To better understand those long-term characteristics, we introduce AR-HMM and DTW analyses to model the temporal structure of animal behaviors. AR-HMM is a powerful method for analyzing stereotyped behavioral data (<xref ref-type="bibr" rid="bib71">Rudolph et al., 2020</xref>; <xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Wiltschko et al., 2020</xref>). It discovers modules of behaviors and describes the modules with autoregressive matrixes. In other words, each embedding of the frame is predicted by a linear combination of embeddings of several previous frames, and frames within each module share the same coefficients of the linear combination, which is called autoregressive matrixes. In an AR-HMM, these fitted autoregressive patterns are utilized as hidden states of the HMM. The transition between each hidden state (autoregressive pattern) is determined by the transition matrix of the HMM. By the definition of Markov property, the transition from the current state to the next state is only determined by the current state and transition probabilities (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In this way, AR-HMM could capture local structures (autoregressive pattern) of animal behaviors as well as syntaxes (transition probabilities).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Time-series analyses using Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction)-produced features.</title><p>(<bold>A</bold>) A brief illustration of the autoregressive hidden Markov model (AR-HMM). The local autoregressive property is determined by <italic>β<sub>t</sub></italic>, the autoregressive matrix, which is yield based on the current hidden state of the HMM. The transition between each hidden state is described by the transition matrix (<italic>p<sub>ij</sub></italic>). (<bold>B</bold>) Principal component analysis (PCA) visualization of state usages of mice in control groups (<italic>n</italic>=17, blue points) and chronic immobilization stress (CIS) groups (<italic>n</italic>=17, red points). (<bold>C</bold>) State usages of 10 modules. Module No.0 and No.3 showed significantly different usages in wild-type and mutant flies; p=0.00065, q=0.003 and p=0.015, q=0.038, respectively, Mann-Whitney test with Benjamini and Hochberg correction. (<bold>D</bold>) The differences spotted by the AR-HMM could be explained by the mice’s position. Mice distances to the two nearest walls were calculated in each frame. Distance distributions (the bin width was 1 cm) throughout open-field test (OFT) experiments were plotted in solid lines, and SEMs were indicated with light color regions. Green blocks indicated bins with statistic differences between the CIS group and control groups. Frames assigned to modules No.0 and No.3 were isolated, and their distance distributions were plotted in blue and yellow bars, respectively. Frames of module No.0 were enriched in bins of larger values, while frames of module No.3 were enriched in bins of smaller values. (<bold>E</bold>) A brief illustration of the dynamic time warping (DTW) model. The transformation from a rounded rectangle to an ellipse could contain six steps (gray reference shapes). The query transformation lags at step 2 but surpasses at step 4. The dynamic is visualized on the right panel. (<bold>F</bold>) <italic>NorpA<sup>36</sup></italic> flies (<italic>n</italic>=6) showed a significantly longer copulation latency than wild-type flies (<italic>n</italic>=7), p=0.0495, Mann-Whitney test. (<bold>G</bold>) <italic>NorpA<sup>36</sup></italic> flies had delayed courtship dynamics than wild-type flies with DTW visualization. Dynamic of wild-type flies and <italic>NorpA</italic> mutant flies were indicated by blue and red lines, respectively, and SEMs were indicated with light color regions. The red line was laid below the blue line, showing a delayed dynamic of <italic>NorpA</italic> mutant flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig5-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig5-video1.mp4" id="fig5video1"><label>Figure 5—video 1.</label><caption><title>A video example of Module No.0.</title><p>An exploratory-like behavior of mice.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-76218-fig5-video2.mp4" id="fig5video2"><label>Figure 5—video 2.</label><caption><title>A video example of Module No.3.</title><p>Mice walking alongside walls of the arena.</p></caption></media></fig-group><p>We asked if we could detect the dynamic changes in mice behaviors after chronic immobilization stress (CIS) during the open-field test (OFT). The CIS model is well established to study the depression-like behavior of experimental animals, and the anxiety level of mice can be evaluated with OFT. Mice prefer to walk near the wall, and the time spent in the center of the arena is considered to be related with anxiety behavior (<xref ref-type="bibr" rid="bib20">Crusio et al., 2013</xref>; <xref ref-type="bibr" rid="bib65">Prut and Belzung, 2003</xref>). We tested the OFT performance of mice with or without the CIS treatment. After preprocessing, videos were processed with Selfee trained with mice mating behavior. An AR-HMM with five modules (No.0 to No.4) was fitted to analyze behaviors during OFT. PCA of state usages revealed an apparent difference between mice with and without CIS experience (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Usages of two modules (No.0 and No.3) showed statistically significant differences between the two groups (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). By watching sampled fragments of these two behaviors, we found module No.0 might be exploratory-like behavior, while module No.3 contained mice walking alongside walls (<xref ref-type="video" rid="fig5video1 fig5video2">Figure 5—videos 1 and 2</xref>). To further confirm these observations, videos were analyzed by an animal tracking program, whose results were proofread manually. Mice in the CIS group spent more time near walls while mice in the control group spent more time in the central area (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, blue and red lines), and similar observations had been reported before (<xref ref-type="bibr" rid="bib66">Ramirez et al., 2015</xref>). Then, we analyzed whether these two modules were related to mice’s position in the arena. All frames belonging to each module were extracted, and distances to the two nearest walls were calculated, the same as what was performed on all videos. The result indicated that mice performing behavior No.0 were relatively distant from walls, while in module No. 3, mice were closer to the border (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, cyan and yellow bars). These results showed that Selfee with AR-HMM successfully distinguished mice in the control group from the CIS group, and the differences spotted by Selfee were consistent with previous observations (<xref ref-type="bibr" rid="bib66">Ramirez et al., 2015</xref>). It is also established that Selfee with AR-HMM could discover the differences in proportions of behaviors, similar to what could be achieved with classic manual analysis or animal tracking software.</p><p>The AR-HMM modeling does not necessarily capture the difference in long-term dynamics intuitively, such as the latency of certain behaviors. To solve this problem, we introduce DTW analysis. DTW is a well-known algorithm for aligning time series, which returns the best-matched path and the matching similarity (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). The alignment can be simplified as follows. When given the same start state and end state, it optimally maps all indices from the query series to the reference series monotonically. Pairs of mapped indices form a path to visualize the dynamic difference. The points above the diagonal line indicate that the current time point in the query group is matched to a future time point in the reference group so that the query group has faster dynamics and vice versa. Our experiments use cosine similarities of Selfee extracted representations to calculate warping paths.</p><p>Previously, DTW was widely applied to numerical measures of animal behaviors, including trajectory (<xref ref-type="bibr" rid="bib18">Cleasby et al., 2019</xref>), audios (<xref ref-type="bibr" rid="bib41">Kohlsdorf et al., 2016</xref>), and acceleration (<xref ref-type="bibr" rid="bib4">Aurasopon, 2016</xref>). For the first time, we applied DTW to image data, with the aid of Selfee, to study the prolonged dynamic of animal behaviors. We applied DTW to analyze representations of <italic>NorpA</italic> mutant flies. Visual cues are essential for male flies to locate female flies during courtship (<xref ref-type="bibr" rid="bib68">Ribeiro et al., 2018</xref>), and mutant flies of <italic>NorpA,</italic> which have defective visual transduction (<xref ref-type="bibr" rid="bib8">Bloomquist et al., 1988</xref>), have a prolonged courtship latency in our experiments (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), similar to previously findings (<xref ref-type="bibr" rid="bib51">Markow and Manning, 1980</xref>). When wild-type flies were used as the reference for the DTW, the group of <italic>NorpA</italic> mutant flies yielded a curve lower than the diagonal line, indicating a delay in their courtship behaviors (<xref ref-type="fig" rid="fig5">Figure 5G</xref>). In this way, our experiments confirm that Selfee and DTW could capture differences in long-term dynamics such as behavior latency. In conclusion, DTW and AR-HMM could capture temporal differences between control and experimental groups beyond single-frame postures, making Selfee a competent unsupervised method for traditional analyses like courtship index or copulation latency.</p></sec><sec id="s2-5"><title>A brief demonstration of the whole Selfee pipeline</title><p>Selfee is a powerful end-to-end unsupervised behavior analysis tool. We went through the whole pipeline using the mice OFT discussed in previous sections as a demonstration. The first step to use Selfee is setting up a development environment containing packages in Key resources table. When recording conditions are consistent for different sets of videos, preprocessing can be simple frame extraction (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, the dashed line). As mice open-field videos were recorded with some variations in our case, arenas in the video were first cropped; backgrounds were removed, and the luminance was normalized (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, solid lines).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Application of the Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) pipeline to mice open-field test (OFT) videos.</title><p>(<bold>A</bold>) Image preprocessing for Selfee. The area of the behavior chamber was cropped, and the background was extracted. Illumination normalization was performed after background subtraction. This preprocessing could be skipped if the background was consistent in each video, as our pipeline for fly videos (dashed lines). (<bold>B</bold>) Anomaly detection of mice OFT videos after chronic immobilization stress (CIS) experiences. Only 12 frames (red points, indicated by arrows) were detected based on a threshold constructed with control mice (the blue region), and anomaly scores were slightly higher than the threshold. (<bold>C</bold>) Dynamic time warping (DTW) analysis of mice OFT videos after CIS experiences. The dynamic difference between control groups and CIS groups was visualized, and positive values indicated a delay of the reference (control groups). Results from Selfee features and animal positions were similar (red and blue lines, respectively). (<bold>D</bold>) Autoregressive hidden Markov model (AR-HMM) analysis of mice OFT videos after CIS experiences. Principal component analysis (PCA) visualization of state usages of mice in control groups (<italic>n</italic>=17, blue points) and CIS groups (<italic>n</italic>=17, red points). Same as <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Anomaly detection of chronic immobilization stress (CIS) mice.</title><p>(<bold>A</bold>) Anomaly scores of CIS mice with half of mice in the control groups as references and another half as negative controls. The blue region indicates the max anomaly score of negative controls; blue dots indicate anomaly scores fall into the blue region; red dots indicate detected anomalies (12 points in total). Detected anomalies were only of scores slightly higher than the threshold. (<bold>B</bold>) Twelve anomalous frames. These frames contained mice near walls with their head facing the center of the arena. All frames seemed normal.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Dynamic time warping (DTW) analysis of chronic immobilization stress (CIS) and control mice.</title><p>(<bold>A</bold>) DTW analysis using Selfee (<bold><underline>Sel</underline></bold>f-supervised <bold><underline>Fe</underline></bold>atures <bold><underline>E</underline></bold>xtraction) features of mice in CIS groups with control mice as reference. The red line represents CIS groups, and the blue line represents control groups. The red line is slightly above the blue line. (<bold>B</bold>) DTW analysis using mice distances to two nearest walls of mice in CIS groups with control mice as reference. The red line represents CIS groups, and the blue line represents control groups. The red line is slightly above the blue line.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76218-fig6-figsupp2-v2.tif"/></fig></fig-group><p>After preprocessing, image embeddings were extracted with pre-trained Selfee. Features were then grouped according to experimental designs, and features for mice in the control group and CIS group were sent to the following modules. First, features of the control group were randomly assigned as references or negative controls. The maximum anomaly score of negative controls was set as the threshold. In the CIS group, 12 frames of anomaly behaviors were sorted out (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>). However, these anomaly frames with their score slightly higher than the threshold only contributed to less than 0.01% of all frames, and occurred only in 2 out of 17 videos. Therefore, these frames were classified as false-positive after being examined manually (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>). Second, the features of the control group and CIS group were analyzed with AR-HMM. As previously showed, AR-HMM separated two groups apart, and the major differences were related to mice’s distances to the arena walls (<xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p><p>Finally, we compared the long-range dynamics of these two groups of mice with DTW. DTW results showed a relatively similar dynamic between the two groups, with a minor delay occurring in the control group (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2A</xref>). Inspired by the result of AR-HMM, we wondered if the delay could also be explained by the distance to walls. Therefore, DTW analysis was applied to mice distances to the arena walls, and the result appeared similar (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2B</xref>). When delays of the control group were isolated, we found results generated from Selfee embeddings were strongly consistent with those from mice positions (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Although the observed delay was related to mice positions in the arena, it was only several seconds, and the biological significance was unclear. Further experiments were required to determine whether this result indeed revealed the behavioral dynamic of mice with CIS experiences, or just a deviation due to small sample sizes. In conclusion, our pipeline with Selfee and downstream analysis could evaluate animal behaviors in a purely unsupervised way. The method requires no human definition of either animal behaviors, or key points of animal skeletons, and thus subjective factors that would bias the analysis are avoided.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we use cutting-edge self-supervised learning methods and CNNs to extract Meta-representations from animal behavior videos. Siamese CNNs have proven their capability to learn comprehensive representations (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>). The cosine similarity, part of its loss function used for training, is rational and well suited to measure similarities between the raw images. Besides, CNNs are trained end-to-end so that preprocessing steps like segmentation or key points extraction is unnecessary. By incorporating Selfee with different post-processing methods, we can identify phenotypes of animal behaviors at different time scales. In the current work, we demonstrate that the extracted representations could be used not only for straightforward distance-based analyses such as t-SNE visualization or <italic>k</italic>-NN anomaly detection but also for sophisticated post-processing methods like AR-HMM. These validations confirm that the extracted Meta-representations are meaningful and valuable. Besides anomaly detection, AR-HMM, and DTW discussed here, other methods could also be used to process features produced by Selfee. For classification, as mentioned before, temporal features could be engineered, such as bout features used in FlyTracker, or window features used in JAABA. Also, other unsupervised learning methods developed for skeleton features can be used downstream of Selfee. For example, UMAP non-linear transformations (<xref ref-type="bibr" rid="bib54">McInnes et al., 2018</xref>) and HDBSCAN (<xref ref-type="bibr" rid="bib12">Campello et al., 2013</xref>) clustering proposed in B-SOiD (<xref ref-type="bibr" rid="bib32">Hsu and Yttri, 2021</xref>) and dynamic time alignment kernel proposed in Behavior Atlas (<xref ref-type="bibr" rid="bib33">Huang et al., 2021</xref>). Therefore, Selfee features could be used flexibly with different machine learning methods for various experimental purposes.</p><p>By applying our method to mice mating behavior and fly courtship behaviors, we show that Selfee could serve as a helpful complement of the widely used pose estimation methods in multi-animal behavior analysis, and vice versa. First, Selfee features are proved to be comparably discriminative as key points derived human-engineered features. Second, the famous DeepLabCut (<xref ref-type="bibr" rid="bib43">Lauer et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Mathis et al., 2018</xref>) and similar methods face problems coping with animals of the same color recorded at a compromised resolution and with intensive body contacts. We found that the current version of DeepLabCut could hardly extract useful features during intromission behaviors of two black mice (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A</xref>, <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>). The reason was that it was extremely difficult to unambiguously label body parts like nose, ears and hips when two mice were close enough, a task challenging even for human experts. Similar results were also observed in fly videos. We visualized tracking results provided by the Fly-vs-Fly dataset (<xref ref-type="bibr" rid="bib22">Fleet et al., 2014</xref>), which used classic computer vision techniques for tracking, and inaccurate tracking of fly wings and fly bodies was observed (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref>, <xref ref-type="video" rid="fig1video2">Figure 1—video 2</xref>). To investigate if cutting-edge deep learning methods would avoid such problems, we labeled 3000 frames to train a SLEAP (<xref ref-type="bibr" rid="bib64">Pereira et al., 2022</xref>) model and inferred on a clip of a fly courtship video. Unfortunately, the same type of error occurred when flies showed intensive body contact (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3C</xref>, <xref ref-type="video" rid="fig1video3">Figure 1—video 3</xref>). Those wrongly tracked frames were either hard for humans to detect or rare postures that were not covered in the training data. By testing these three representative pose estimation programs, we argue that key point detection for closely interacting or overlapped animals is still challenging. By contrast, our methods could capture global characteristics of behaviors like human perception, making it robust to these confusing occlusions. Nevertheless, Selfee features appear less explainable than key points, and we have shown that animal tracking was very useful for interpreting Selfee features. Therefore, Selfee strongly complements the incapability of pose estimation methods processing closely contacted animals, and could be further improved to be more explainable when assisted by animal tracking.</p><p>We also demonstrate that the cutting-edge self-supervised learning model is accessible to biology labs. Modern self-supervised learning neural networks usually require at least eight modern GPUs (<xref ref-type="bibr" rid="bib14">Caron et al., 2021</xref>; <xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>) even TPUs (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>) for training, take advantage of batch sizes larger than 1024, and training time varies from several days to a week. In contrast, our model can be trained on only one RTX 3090 GPU with a batch size of 256 within only 8 hr with the help of the newly proposed CLD loss function (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>) and other improvements (see Materials and methods for further details). Despite our effort to improve training speed and lower hardware requirement, the current development of self-supervised learning could not make training as accessible as cutting-edge supervised key points detection networks. Nevertheless, we found that our model could achieve zero-shot domain transfer. We demonstrated that Selfee trained for mating behavior of a pair of mice could also be applied to OFTs of single animal. Assisted by AR-HMM, Selfee captured the major differences between mice in the control and CIS groups. Even though the network was trained in a translation-invariant way, and apparent landmarks in the arena were removed during preprocessing, Selfee still identified two distinct behaviors related to mice position in the arena in a zero-shot way. Furthermore, when the model pre-trained with mice videos was applied to rat behaviors, we were able to achieve a zero-shot classification of five major types of social behaviors (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). Although the <italic>F</italic><sub>1</sub> score was only 49.6%, it still captured the major differences between similar behaviors, such as allogrooming and social nose contact (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). These results showed that Selfee could be used in a zero-shot training-free way even without GPUs. Thus, we have demonstrated that self-supervised learning could be easily achieved with limited computation resources and a much shorter time and could be directly transferred to datasets that share similar visual characteristics and save more resources.</p><p>Despite those advantages, there are some limitations of Selfee. First, because each live-frame only contains three raw frames, our model could not capture much information on the animal motion. It becomes more evident when Selfee is applied to highly dynamic behaviors such as mice mating behaviors. This can be overcome with updated hardware because 3D convolution (<xref ref-type="bibr" rid="bib35">Ji et al., 2013</xref>) or spatial-temporal attention (<xref ref-type="bibr" rid="bib1">Aksan et al., 2020</xref>) is good at dynamic information extraction but requires much more computational resources. Second, as previously reported, CNNs are highly vulnerable to image texture (<xref ref-type="bibr" rid="bib24">Geirhos et al., 2019</xref>). We observed that certain types of beddings of the behavior chamber could profoundly affect the performance of our neural networks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), so in some cases, background removal is necessary (see Materials and methods for further details). Lastly, Selfee could only use discriminative features within each batch, without any negative samples provided, so minor irrelevant differences could be amplified and cause inconsistent results (named mode-split). This mode-split may increase variations of downstream analyses. One possible solution is using some labels to fine-tune the network (cite instance learning). However, the fine-tuning would break the fully unsupervised setup. Another solution is to make the representations more explainable, so that the causes of mode-split can be spotted and corrected. We propose that by combining unsupervised semantic segmentation (<xref ref-type="bibr" rid="bib17">Cho et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Hamilton et al., 2022</xref>; <xref ref-type="bibr" rid="bib85">Xu et al., 2022</xref>) and bag-of-features (BagNet) (<xref ref-type="bibr" rid="bib10">Brendel and Bethge, 2019</xref>) training objective, it is possible to produce disentangle features that could map each dimension to pixels. Therefore, whenever mode-split happens and corresponding dimensions are identified, and human interference to either recoding setup or training hyperparameters can be applied.</p><p>We can envision at least two possible future directions for Selfee. One is to optimize our designs of self-supervised learning method. On the one hand, advanced self-supervised learning methods like DINO (<xref ref-type="bibr" rid="bib14">Caron et al., 2021</xref>) (with visual transformers, ViTs) could separate objects from the background and extract more explainable representations. Besides, by using ViTs, the neural network could be more robust against distractive textures (<xref ref-type="bibr" rid="bib61">Naseer et al., 2021</xref>). At the same time, more temporal information can also be incorporated for a better understanding of motions. Combining these two, equipping ViTs with spatial-temporal attention could extract better features. On the other hand, although Siamese networks are popular choices for self-supervised learning, they require two times more computational resources than single-branch designs. A recent work on instance learning shed light on self-supervised learning with smaller datasets and simpler architectures. This could be a promising direction in which self-supervised learning for animal behaviors could be more accessible for biologists. In summary, possible improvements of Selfee designs can either bring in more advanced and complex architectures for better performance or try more simplified instance learning techniques to achieve easier deployment.</p><p>Another direction will be explainable behavior forecasting for a deeper understanding of animal behaviors. For a long time, behavior forecasting has been a field with extensive investigations in which RNNs, LSTMs, or transformers are usually applied (<xref ref-type="bibr" rid="bib1">Aksan et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Fragkiadaki, 2015</xref>; <xref ref-type="bibr" rid="bib75">Sun et al., 2021</xref>). However, most of these works use coordinates of key points as inputs. Therefore, the trained model might predominantly focus on spatial movement information and discover fewer behavioral syntaxes. By representation learning, spatial information is essentially condensed so that more syntaxes might be highlighted. Transformer models for forecasting could capture correlations between sub-series as well as long-term trends like seasonality (<xref ref-type="bibr" rid="bib84">Wu et al., 2021</xref>). These deep learning methods would provide behavioral neuroscientists with powerful tools to identify behavior motifs and syntaxes that organize stereotyped motifs beyond the Markov property.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>w<sup>1118</sup></italic></td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">Female, <xref ref-type="fig" rid="fig3">Figure 3A–C</xref> &amp; <xref ref-type="fig" rid="fig5">Figure 5F–G</xref>; male, <xref ref-type="fig" rid="fig4">Figure 4G–J</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CS</italic></td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">Male, <xref ref-type="fig" rid="fig3">Figure 3A–C</xref>, <xref ref-type="fig" rid="fig4">Figure 4B</xref> &amp; <xref ref-type="fig" rid="fig5">Figure 5F–G</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila. melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CCHa1<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84458</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}CCHa1[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CCHa1-R<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84459</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}CCHa1-R[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CCHa2<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84460</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}CCHa2[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CCHa2-R<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84461</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}CCHa2-R[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CCHa2-R-RA<sup>Gal4</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84603</td><td align="left" valign="bottom">TI{2 A-GAL4}CCHa2-R[2 A-A.GAL4]; with Kir2.1, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CCHa2-R-RB<sup>Gal4</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84604</td><td align="left" valign="bottom">TI{2 A-GAL4}CCHa2-R[2A-B.GAL4]; with Kir2.1, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>CNMa<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84485</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}CNMa[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>Oamb<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84555</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}Oamb[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>Dop2R<sup>KO</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84720</td><td align="left" valign="bottom">TI{TI}Dop2R[KO]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>DopEcR<sup>Gal4</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84717</td><td align="left" valign="bottom">TI{GAL4}DopEcR[KOGal4.w-]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>SerT<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84572</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}SerT[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>Trh<sup>Gal4</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">86146</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=2 A-GAL4}Trh[GKO]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>TK<sup>attP</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">84579</td><td align="left" valign="bottom">w[*]; TI{RFP[3xP3.cUa]=TI}Tk[attP]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>UAS-Kir2.1</italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">6595</td><td align="left" valign="bottom">w[*]; P{w[+mC]=UAS-Hsap\KCNJ2.EGFP}7; with Gal4, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>NorpA<sup>36</sup></italic></td><td align="left" valign="bottom">BDRC</td><td align="char" char="." valign="bottom">9048</td><td align="left" valign="bottom">w[*] norpA[P24]; male, <xref ref-type="fig" rid="fig5">Figure 5F–G</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom"><italic>Tdc2<sup>RO54</sup></italic></td><td align="left" valign="bottom">Pan Lab at SEU</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Tdc2[RO54]; male, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Drosophila melanogaster</italic>)</td><td align="left" valign="bottom">Taotie-Gal4</td><td align="left" valign="bottom">Zhu Lab at IBP</td><td align="left" valign="bottom"/><td align="left" valign="bottom">w[*]; P{w[+mC]=Gr28 b.b-GAL4.4.7}10; with Kir2.1, <xref ref-type="fig" rid="fig4">Figure 4B</xref></td></tr><tr><td align="left" valign="bottom">Genetic reagent (<italic>Mus musculus</italic>)</td><td align="left" valign="bottom">C57BL/6J</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom">–</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3D–F</xref> &amp; <xref ref-type="fig" rid="fig5">Figure 5B–D</xref></td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">python</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">3.8.8</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">numpy</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">1.19.2</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">matplotlib</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">3.4.1</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">av</td><td align="left" valign="bottom">conda-forge</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">8.0.3</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">scipy</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">1.6.2</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">cudatoolkit</td><td align="left" valign="bottom">conda-forge</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">11.1.1</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">pytorch</td><td align="left" valign="bottom">pytorch</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">1.8.1</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">torchvision</td><td align="left" valign="bottom">pytorch</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">0.9.1</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">pillow</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">8.2.0</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">scikit-learn</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">0.24.2</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">pandas</td><td align="left" valign="bottom">Anaconda</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">1.2.4</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">lightgbm</td><td align="left" valign="bottom">conda-forge</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">3.2.1</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">opencv-python</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">4.5.3.56</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">psutil</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">5.8.0</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">pytorch-metric-learning</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">0.9.99</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">pyhsmm</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">0.1.6</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">autoregressive</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">0.1.2</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">dtw-python</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">1.1.10</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">SLEAP</td><td align="left" valign="bottom">conda-forge</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">1.2.2</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">DEEPLABCUT</td><td align="left" valign="bottom">PyPI</td><td align="left" valign="bottom">–</td><td align="char" char="." valign="bottom">2.2.0.2</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Fly stocks</title><p>All fly strains were maintained under a 12 hr/12 hr light/dark cycle at 25°C and 60% humidity (PERCIVAL incubator). The following fly lines were acquired from Bloomington <italic>Drosophila</italic> Stock Center: <italic>CCHa1<sup>attP</sup></italic> (84458), <italic>CCHa1-R<sup>attP</sup></italic> (84459), <italic>CCHa2<sup>attP</sup></italic> (84460), <italic>CCHa2-R<sup>attP</sup></italic> (84461), CCHa2-R-RA<sup>Gal4</sup> (84603), CCHa2-R-RB<sup>Gal4</sup> (84604), <italic>CNMa<sup>attP</sup></italic> (84485), <italic>Oamb<sup>attP</sup></italic> (84555), <italic>Dop2R<sup>KO</sup></italic> (84720), <italic>DopEcR<sup>Gal4</sup></italic> (84717), <italic>SerT<sup>attP</sup></italic> (84572), <italic>Trh<sup>Gal4</sup></italic> (86146), <italic>TK<sup>attP</sup></italic> (84579), <italic>NorpA<sup>36</sup></italic> (9048), UAS-Kir2.1 (6595). <italic>Tdc2<sup>RO54</sup></italic> was a gift from Dr Yufeng Pan at Southeast University, China. Taotie-Gal4 was a gift from Dr Yan Zhu at Institute of Biophysics, Chinese Academy of Sciences, China.</p></sec><sec id="s4-2"><title>Fly courtship behavior and male-male interaction</title><p>Virgin female flies were raised for 4–6 days in 15-fly groups, and naïve male flies were kept in isolated vials for 8–12 days. All behavioral experiments were done under 25°C and 45–50% humidity. Flies were transferred into a customized chamber of 3 mm in height and 10 mm in diameter by a homemade aspirator. Fly behaviors were recorded using a stereoscopic microscope mounted with a CCD camera (Basler ORBIS OY-A622f-DC) at the resolution of 1000×500 (for two chambers at the same time), or 640×480 (for individual chambers) and a frame rate of 30 Hz. Five types of behaviors were annotated manually, including ‘chasing’ (a male fly follows a female fly), ‘wing extension’ a male fly extends unilateral wing and orientates to the female to sing courtship son, ‘copulation attempt’ (a male fly bends its abdomen toward the genitalia of the female or the unstable state that male fly mounts on a female with its wings open), and ‘copulation’ (male fly mounts on a female in a stable posture for several minutes).</p></sec><sec id="s4-3"><title>Fly defensive behavior assay</title><p>The kicking behavior was tested based on previously reported paradigms (<xref ref-type="bibr" rid="bib46">Li et al., 2016</xref>; <xref ref-type="bibr" rid="bib48">Liu et al., 2020</xref>). Briefly, flies were raised in groups for 3–5 days. Flies were anesthetized on ice, and then male flies were decapitated and transferred to 35 mm Petri dishes with damped filter paper on the bottom to keep the moisture. Flies were allowed to recover for around 30 min in the dishes. The probe for stimulation was homemade from a heat-melt yellow pipette tip, and the probe’s tip was 0.3 mm. Each side of flies’ wing margin was gently touched five times, and the kicking behavior was recorded manually. The statistical analysis was performed with the Mann-Whitney test with GraphPad Prism Software.</p></sec><sec id="s4-4"><title>Social behavior assay for flies</title><p>The social distance was tested based on the previously reported method (<xref ref-type="bibr" rid="bib55">McNeil et al., 2015</xref>). Briefly, flies were raised in groups for 3 days. Flies were anesthetized paralyzed on ice, and male flies were picked and transferred to new vials (around 40 flies per vial). Flies were allowed to recover for 1 day. The vertical triangular chambers were cleaned with 75% ethanol and dried with paper towels. After assembly, flies were transferred into the chambers by a homemade aspirator. The photos were taken after 20 min, and the positions of each fly were manually marked in ImageJ. The social distances were measured with the lateral sides of the chambers (16.72 cm) as references, and the median values of the nearest, median, and average distance of each replication are calculated. The statistical analysis was performed with the Mann-Whitney test in GraphPad Prism Software.</p></sec><sec id="s4-5"><title>Mice mating behavior assay</title><p>Wild-type mice of C57BL/6J were purchased from Slac Laboratory Animal (Shanghai). Adult (8–24 weeks of age) male mice were used for sexual behavior analysis. All animals were housed under a reversed 12 hr/12 hr light-dark cycle with water and food ad libitum in the animal facility at the Institute of Neuroscience, Shanghai, China. All experiments were approved by the Animal Care and Use Committee of the Institute of Neuroscience, Chinese Academy of Sciences, Shanghai, China (IACUC No. NA-016-2016).</p><p>Male mice were singly housed for at least 3 days prior to sexual behavioral tests. All tests were initiated at least 1 hr after lights were switched off. Behavioral assays were recorded using infrared cameras at the frame rate of 30 Hz. Female mice were surgically ovariectomized and supplemented with hormones to induce receptivity. Hormones were suspended in sterile sunflower seed oil (Sigma-Aldrich, S5007) and injected 10 mg (in 50 mL oil) and 5 mg (in 50 mL oil) of 17b-estradiol benzoate (Sigma-Aldrich, E8875) 48 and 24 hr preceding the test, respectively. On the day of the test, 50 mg of progesterone (Sigma-Aldrich, P0130; in 50 mL oil) was injected 4–6 hr prior to the test. Male animals were adapted 10 min to behavioral testing rooms where a recording chamber equipped with video acquisition systems was located. A hormonal primed ovariectomized C57BL/6J female (OVX) was introduced to the home cage of male mice and videotaped for 30 min. Mating behavior tests were repeated three times with different OVX at least 3 days apart. Videos were manually scored using a custom-written MATLAB program. The following criteria were used for behavioral annotation: active nose contacts initiated by male mouse toward the female’s genitals, body area, faces were defined collectively as ‘social interest’; male mouse climbs the back of the female and moves the pelvis were defined as ‘mount’; rhythmic pelvic movements after mount were defined as ‘intromission’; a body rigidity posture after final deep thrust were defined as ‘ejaculation’.</p></sec><sec id="s4-6"><title>Mice OFT</title><p>All experiments were performed using the principles outlined in <italic>the Guide for the Care and Use of Laboratory Animals of Tsinghua University</italic>. C57BL/6J male mice, aged 8–12 weeks, were used for behavior test. Mice were housed five per cage with free access to food and water and under a 12 hr light-dark cycle (light on from 7 p.m. to 7 a.m.). All mice were purchased and maintained under standard conditions by the Animal Research Centre of Tsinghua University.</p><p>All studies and experimental protocols were approved by Institutional Animal Care and Use Committee (IACUC) at Tsinghua University (No. 19-ZY1). Specifically, the OFT was conducted in an open plastic arena (50 cm × 50 cm × 40 cm). Mice were first placed in the peripheral area with their head toward the wall. Exploration time during 10 min in the peripheral and central regions, respectively, were measured using an automated animal tracking program.</p><p>The animal tracking program was coded in Python 3 with Open-CV. Each frame was first preprocessed, and then a median filter with a kernel size of 5 and a threshold of 150 was performed sequentially. Connected components with the maximum area were extracted, and the center of gravities and borders were visualized with original images for manual proofreading. Tracking results were saved in plain text format.</p></sec><sec id="s4-7"><title>Data preprocessing, augmentation, and sampling</title><p>Fly behavior videos were decomposed into frames by FFmpeg, and only the first 10,000 frames of each video were preserved and resized into images with a resolution of 224×224. For model training of <italic>Drosophila</italic> courtship behavior, each video was manually checked to ensure successful copulations within 10,000 frames.</p><p>Mice behavior videos were decomposed into frames by FFmpeg, and only frames of the first 30 min of each video were preserved. Frames were then preprocessed with OpenCV (<xref ref-type="bibr" rid="bib9">Bradski, 2000</xref>) in Python. Behavior chambers in each video were manually marked, segmented, and resized into images of a resolution of 256 × 192 (mating behavior) or 500 × 500 (OFT). For background removal, the average frame of each video was subtracted from each frame, and noises were removed by a threshold of 25 and the median filter with a kernel size of 5. Finally, the contrast was adjusted with histogram equalization.</p><p>For data augmentations, crop, rotation, flip, Turbo, and color jitter were applied. For a given frame, it formed a live-frame with its preceding and succeeding frames. For flies’ behavior video, three frames were successive, and for mice, the preceding or succeeding frame is one frame away from the current frame due to their slower dynamics (<xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>). Each live-frame was randomly cropped into a smaller version containing more than 49% (70%×70%) of the original image; then the image was randomly (clockwise or anticlockwise) rotated for an angle smaller than the acute angle formed by the diagonal line and the vertical line, then the image would be vertically flipped, horizontally flipped, and/or applied the Turbo lookup table (<xref ref-type="bibr" rid="bib57">Mikhailov, 2019</xref>) at the probability of 50%, respectively; and finally, the brightness, contrast, saturation, and hue were randomly adjusted within 10% variation. Notably, since the Turbo transformation is designed for grayscale images, for a motion-colored RGB image, each channel was transformed individually. After Turbo transformation, their corresponded channels were composited to form a new image.</p><p>For fly data sampling, all images of all videos were randomly ranked, and each batch contained 256 images from different videos. For mice data sampling, all images of each video were randomly ranked, and each batch contained 256 images from the same video. This strategy was designed to eliminate the inconsistency of recording conditions of mice that was more severe than flies.</p></sec><sec id="s4-8"><title>Selfee neural network and its training</title><p>All training and inference were accomplished on a workstation with 128 GB RAM, AMD Ryzen 7 5800×, and one NVIDIA GeForce RTX 3090. Selfee neural network was constructed based on publications and source codes of BYOL (<xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>), SimSiam (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>), and CLD (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>) with PyTorch (<xref ref-type="bibr" rid="bib63">Paszke et al., 2019</xref>). In brief, the last layer of ResNet-50 was removed, and a three-layer 2048-dimension MLP was added as the projector. Hidden layers of the projector were followed by batch normalization (BN) and ReLU activation, and the output layer only had BN. The predictor was constructed with a two-layer bottleneck MLP with a 512-dimension hidden layer and a 2048-dimension output layer. The hidden layer but not the output layer of the predictor had BN and ReLU. As for the group discriminator for CLD loss, it had only one normalized fully connected layer that projected 2048-dimension output to 1024 dimensions, followed by a customized normalization layer that was described in the paper of CLD (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>). The collapse level was monitored as one minus to average standard deviation of each channel of the normalized representation multiplied by the square root of the channel number. If a collapse happens, the standard deviation becomes zero, and the collapse level should be one. If no collapse happens, each channel should obey standard normal distribution, and the average standard deviation is one. The normalization operation cancels out the square root of the channel number. In this way, the collapse level is zero.</p><p>The loss function of Selfee had two major parts. The first part was the negative cosine loss (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>), and the second part was the CLD loss (<xref ref-type="bibr" rid="bib79">Wang et al., 2021</xref>). For a batch of <italic>n</italic> samples, <italic>Z</italic>, <italic>P</italic>, <italic>V</italic> represented the output of projector, predictor, and group discriminator of the main branch, respectively; <italic>Z’</italic>, <italic>P’</italic>, <italic>V’</italic> represented the output of the reference branch; and <italic>sg</italic> as the stop-gradient operator. After <italic>k</italic>-means clustering of <italic>V</italic>, the centroids of <italic>k</italic> classes were given by <italic>M</italic>, and labels of each sample were provided in the one-hot form as <italic>L</italic>. The hyperparameter <italic>θ</italic> was 0.07, and <italic>λ</italic> was 2. The loss function was given by the following equations:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">∣</mml:mo><mml:mo>∣</mml:mo><mml:mi>m</mml:mi><mml:mo>∣</mml:mo><mml:mo stretchy="false">∣</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac><mml:mo>.</mml:mo><mml:mfrac><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">∣</mml:mo><mml:mo>∣</mml:mo><mml:mi>n</mml:mi><mml:mo>∣</mml:mo><mml:mo stretchy="false">∣</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow/></mml:msup><mml:msub><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow/></mml:msup><mml:msub><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>∣∣</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:msub><mml:mo>∣</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>θ</mml:mi></mml:mfrac><mml:mo>.</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow/></mml:msup><mml:msup><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mrow/></mml:msup><mml:msub><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>θ</mml:mi></mml:mfrac><mml:mo>.</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>l</mml:mi><mml:mrow/></mml:msup><mml:msub><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>λ</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>For all training processes, the Selfee network was trained for 20,000 steps with the SDG optimizer with a momentum of 0.9 and a weight decay of 1e-4. The learning rate was adjusted in the one-cycle learning rate policy (<xref ref-type="bibr" rid="bib74">Smith and Topin, 2017</xref>) with base learning rates and a pct start of 0.025. The model for <italic>Drosophila</italic> courtship behavior was initialized with ResNet-50 pre-trained on the ImageNet, and the base learning rate was 0.025 per batch size of 256. As for the mating behaviors of mice, the model was initialized with weights trained on the fly dataset, and the base learning rate was 0.05 per batch size of 256.</p><p>For fly courtship behavior, 516 and 7 videos (4,607,274 and 55,708 frames) were used as train set and test set, respectively; for mice mating behavior, 118 and 13 videos (4,943,101 and 548,993 frames) were used as train set and test set, respectively. For comparison with animal tracking methods, Selfee was fine-tuned with 11 and 1 videos (1,188,550 and 108,050 frames).</p></sec><sec id="s4-9"><title>t-SNE visualization</title><p>Video frames for t-SNE visualization were all processed by Selfee. Embeddings of three tandem frames were averaged to eliminate potential noises. All embeddings were transformed using t-SNE provided in the scikit-learn (<xref ref-type="bibr" rid="bib63">Paszke et al., 2019</xref>) package in Python without further tuning of parameters. Results were visualized with the Matplotlib (<xref ref-type="bibr" rid="bib34">Hunter, 2007</xref>) package in Python, and their colors were assigned based on human annotations of video frames.</p></sec><sec id="s4-10"><title>Classification</title><p>Two kinds of classification methods were implemented, including the <italic>k</italic>-NN classifier and the LightGBM classifier. The weighed <italic>k</italic>-NN classifier was constructed based on the previous reports (<xref ref-type="bibr" rid="bib15">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib83">Wu et al., 2018</xref>). LightGBM classifier (<xref ref-type="bibr" rid="bib40">Ke et al., 2017</xref>) was provided by its official package in Python. The <italic>F</italic><sub>1</sub> score and mAP were calculated with the scikit-learn (<xref ref-type="bibr" rid="bib63">Paszke et al., 2019</xref>) package in Python. Scores for different type of behaviors were averaged in the ‘macro’ way, while scores for behaviors of different intensity were averaged in the ‘micro’ way.</p><p>For fly behavior classification, seven 10,000-frame videos were annotated manually. Sevenfold cross-validation was performed using embeddings generated by Selfee and the <italic>k</italic>-NN classifier. Inferred labels were forced to be continuous through time using inferred labels of 21 neighbor frames to determine the final result. The neighborhood length was determined by plotting the similarity between neighbor frames and the current frame and choosing the frame number where similarities dropped to the half between the peak and the bottom. Then, a video independent of the cross-validation was annotated and inferred by a <italic>k</italic>-NN classifier using all 70,000 samples, and the last 3000 frames were used for the raster plot.</p><p>To compare different types of features, three published labeled videos were used. Each frame of video was labelled with a wing extension score from 0 to 6 (scored from 0 to 3 by two individuals). Each video of 30 min was evenly split into two parts, and six video samples were used for sixfold cross-validation. Voting by 21 neighbors was not used. FlyTracker features and JAABA features were obtained from the original publication (<ext-link ext-link-type="uri" xlink:href="https://library.ucsd.edu/dc/object/bb20197654">https://library.ucsd.edu/dc/object/bb20197654</ext-link>), and distances between key points were calculated from the tracking results of FlyTracker.</p><p>For rat behavior classification, the RatSI dataset (<xref ref-type="bibr" rid="bib49">Lorbach et al., 2018</xref>) (a kind gift from Noldus Information Technology bv) contains nine manually annotated videos. We neglected three rarest annotated behaviors: moving away, nape attacking, and pinning, and we combined approaching and following into a larger category. Therefore, we used five kinds of behaviors, including allogrooming, approaching or following, social nose contact, solitary, and others. Ninefold cross-validation was performed using embeddings generated by Selfee and the <italic>k</italic>-NN classifier. Inferred labels were forced to be continuous through time by using inferred labels of 81 neighbor frames to determine the final result.</p><p>For mice behavior classification, eight videos were annotated manually. Eightfold cross-validation was performed using embeddings generated by Selfee and the <italic>k</italic>-NN classifier. To incorporate more temporal information, the LightGBM classifier and additional features were also used. Additional features include slide moving average and standard deviation of 81-frame time windows, the main frequencies, and their energy (using short-time Fourier transform in SciPy; <xref ref-type="bibr" rid="bib78">Virtanen et al., 2020</xref>) within 81-frame time windows. Early-stop was used to prevent over-fitting. Inferred labels were forced to be continuous through time by using inferred labels of 81 neighbor frames to determine the final result. Then, a video independent of the cross-validation was annotated and inferred by an ensemble classifier of eight previously constructed classifiers, and all frames were used for the raster plot.</p></sec><sec id="s4-11"><title>Anomaly detection</title><p>For a group of query embeddings of sequential frames <italic>q</italic><sub>1</sub>, <italic>q</italic><sub>2</sub>, <italic>q</italic><sub>3</sub>, …, <italic>q<sub>n</sub></italic>, and a group of reference embeddings of sequential frames <italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>, <italic>r</italic><sub>3</sub>, …, <italic>r<sub>m</sub></italic>, the anomaly score of each query frame was given by the following equation:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">∣</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">∣</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>50</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A PyTorch implementation of cosine similarity (<xref ref-type="bibr" rid="bib59">Musgrave et al., 2020</xref>) was used for accelerated calculations.</p><p>The anomaly score of each video was the average anomaly score of the top 100 anomalous frames. The statistical analysis of the genetic screening was performed with one-way ANOVA with Benjamini and Hochberg correction in GraphPad Prism Software.</p><p>If negative controls are provided, anomalous frames are defined as frames with higher anomaly scores than the maximum anomaly score of frames in negative control videos.</p></sec><sec id="s4-12"><title>Autoregressive hidden Markov model</title><p>All AR-HMMs were built with the implementation of MoSeq (<xref ref-type="bibr" rid="bib81">Wiltschko et al., 2015</xref>) (<ext-link ext-link-type="uri" xlink:href="https://github.com/mattjj/pyhsmm-autoregressive">https://github.com/mattjj/pyhsmm-autoregressive</ext-link>; <xref ref-type="bibr" rid="bib47">Linderman, 2018</xref>). A PCA model that could explain 95% of variance of the control group was built and used to transform both control and experiment groups. The max module number was set as 10 for all experiments unless indicated otherwise. Each model was sampled for 1000 iterations. We kept other hyperparameters the same as the examples provided by this package. State usages of each module in control and experimental groups were analyzed by Mann-Whitney test with SciPy <xref ref-type="bibr" rid="bib78">Virtanen et al., 2020</xref> followed with Benjamini and Hochberg correction. The state usages were also visualized after PCA dimensional reduction with scikit-learn (<xref ref-type="bibr" rid="bib63">Paszke et al., 2019</xref>) and Matplotlib (<xref ref-type="bibr" rid="bib34">Hunter, 2007</xref>) for the exclusion of possible obvious outliners or batch effects.</p></sec><sec id="s4-13"><title>Dynamic time warping</title><p>DTW was modified from the Python implementation (<xref ref-type="bibr" rid="bib77">Toni, 2009</xref>) (<ext-link ext-link-type="uri" xlink:href="https://dynamictimewarping.github.io/python/">https://dynamictimewarping.github.io/python/</ext-link>). Specifically, PyTorch implementation of cosine similarity (<xref ref-type="bibr" rid="bib59">Musgrave et al., 2020</xref>) was used for accelerated calculations.</p></sec><sec id="s4-14"><title>Pose estimation with SLEAP</title><p>We used the official implementation of SLEAP. For explanation for t-SNE plot of Selfee features, we labeled the same 3000 frames with the help of SLEAP. First, we labeled around 400 frames and trained a SLEAP model. The trained SLEAP model was used to infer all 3000 frames, tracking was performed using the ‘simple’ method, and the target number of instances per frame was set to 2. Inferred skeletons and tracking results were manually proofread and corrected. These results were used for the following analysis of the t-SNE plot. Then, all 3000 frames were used to train a new SLEAP model. This new model was used for the pose estimation of another clip of a fly courtship video (shown in <xref ref-type="video" rid="fig1video1 fig1video2 fig1video3">Figure 1—videos 1–3</xref> and <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</p></sec><sec id="s4-15"><title>Visualization tracking results from FlyTracker</title><p>Tracking results of the Fly-vs-Fly dataset were obtained from its official website. The head and tail coordinates were calculated from the center, the orientation, and the major axis length. Tracking results were visualized with OpenCV.</p></sec><sec id="s4-16"><title>Pose estimation with DeepLabCut</title><p>We used the official implementation of DeepLabCut (<xref ref-type="bibr" rid="bib43">Lauer et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Mathis et al., 2018</xref>). For training, 120 frames of a mating behavior video were labeled manually, and 85% were used as the training set. Marked body parts included nose, ears, body center, hips, and bottom, following previous publications (<xref ref-type="bibr" rid="bib72">Segalin et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Sun et al., 2021</xref>). The model (ResNet-50 as the backbone) was trained for 100,000 iterations, with a batch size of 16. We kept other hyperparameters the same as default settings.</p></sec><sec id="s4-17"><title>Data and code availability statement</title><p>Major datasets that support the findings of this study are available from Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.brv15dvb8">https://doi.org/10.5061/dryad.brv15dvb8</ext-link>). Other data are available from the corresponding author upon reasonable request, because they are too large to upload to a particular server. Source codes used in this article are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/EBGU/Selfee">https://github.com/EBGU/Selfee</ext-link>; <xref ref-type="bibr" rid="bib37">Jia, 2022</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2a27178ad1671ca020588a7013dc38ecc49697c3;origin=https://github.com/EBGU/Selfee;visit=swh:1:snp:2ea2417cdd1d3ceeb712124e63b6d282c92bfb2a;anchor=swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c">swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c</ext-link>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Resources, Software, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Resources</p></fn><fn fn-type="con" id="con4"><p>Resources</p></fn><fn fn-type="con" id="con5"><p>Data curation, Software</p></fn><fn fn-type="con" id="con6"><p>Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Project administration, Supervision, Writing - original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All mating experiments were approved by the Animal Care and Use Committee of the Institute of Neuroscience, CAS Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China (IACUC No. NA-016-2016) All studies and experimental protocols of CIS and OFT were approved by Institutional Animal Care and Use Committee (IACUC) at Tsinghua University (No. 19-ZY1). Experiments were performed using the principles outlined in the Guide for the Care and Use of Laboratory Animals of Tsinghua University.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-76218-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Major data used in this study were uploaded to Dryad, including pretrained weights. Data can be accessed via: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.brv15dvb8">https://doi.org/10.5061/dryad.brv15dvb8</ext-link>. With the uploaded dataset and pretrained weights, our experiments can be replicated. However, due to its huge size and the limited internet service resources, we are currently not able to share our full training dataset. The full dataset is as large as 400GB, which is hard to upload to a public server and will be difficult for others users to download. The training dataset, is available from the corresponding author upon reasonable request (<ext-link ext-link-type="uri" xlink:href="https://www.ie.tsinghua.edu.cn/eng/info/1017/1347.htm">https://www.ie.tsinghua.edu.cn/eng/info/1017/1347.htm</ext-link>), and then we can discuss how to transfer the dataset. No project proposal is needed as long as the dataset is not used for any commercial purpose. Our Python scripts can be accessed on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/EBGU/Selfee">https://github.com/EBGU/Selfee</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2a27178ad1671ca020588a7013dc38ecc49697c3;origin=https://github.com/EBGU/Selfee;visit=swh:1:snp:2ea2417cdd1d3ceeb712124e63b6d282c92bfb2a;anchor=swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c">swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c</ext-link>. Other software used in our project include ImageJ (<ext-link ext-link-type="uri" xlink:href="https://imagej.net/software/fiji/">https://imagej.net/software/fiji/</ext-link>) and GraphPad Prism (<ext-link ext-link-type="uri" xlink:href="https://www.graphpad.com/">https://www.graphpad.com/</ext-link>). All data used to plot graphs and charts in the manuscript can be fully accessed on Dryad (DOI <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.brv15dvb8">10.5061/dryad.brv15dvb8</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Lei</surname><given-names>B</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data from: Selfee: Self-supervised features extraction of animal behaviors</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.brv15dvb8</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Eyrun</surname><given-names>E</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Burgos-Artizzu</surname><given-names>P</given-names></name><name><surname>Hoopfer</surname><given-names>ED</given-names></name><name><surname>Schor</surname><given-names>J</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Fly-vs-Fly</data-title><source>CaltechDATA</source><pub-id pub-id-type="accession" xlink:href="https://data.caltech.edu/records/1893">1893</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset3"><person-group person-group-type="author"><name><surname>Xubo</surname><given-names>L</given-names></name><name><surname>Margot</surname><given-names>W</given-names></name><name><surname>Kenichi</surname><given-names>I</given-names></name><name><surname>Pavan</surname><given-names>N</given-names></name><name><surname>Kenta</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Quantifying influence of human choice on the automated detection of <italic>Drosophila</italic> behavior by a supervised machine learning algorithm</data-title><source>LIBRARY DIGITAL COLLECTIONS</source><pub-id pub-id-type="doi">10.6075/J0QF8RDZ</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset4"><person-group person-group-type="author"><name><surname>Lorbach</surname><given-names>M</given-names></name><name><surname>Kyriakou</surname><given-names>EI</given-names></name><name><surname>Poppe</surname><given-names>R</given-names></name><name><surname>van Dam</surname><given-names>EA</given-names></name><name><surname>Noldus</surname><given-names>LPJJ</given-names></name><name><surname>Veltkamp</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>RatSI</data-title><source>Noldus Information Technology</source><pub-id pub-id-type="accession" xlink:href="https://www.noldus.com/form/ratsi-dataset">ratsi-dataset</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank members of the Zhang lab for discussions. This work was supported by grants 31871059 and 32022029 from the National Natural Science Foundation of China, grant Z181100001518001 from the Beijing Municipal Science &amp; Technology Commission, and a ‘Brain + X’ seed grant from the IDG/McGovern Institute for Brain Research at Tsinghua to WZ. WZ is supported by Chinese Institute for Brain Research, Beijing. WZ is an awardee of the Young Thousand Talent Program of China.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Aksan</surname><given-names>E</given-names></name><name><surname>Kaufmann</surname><given-names>M</given-names></name><name><surname>Cao</surname><given-names>P</given-names></name><name><surname>Hilliges</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Attention, Please: A Spatio-Temporal Transformer for 3D Human Motion Prediction</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2004.08692</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alekseyenko</surname><given-names>OV</given-names></name><name><surname>Chan</surname><given-names>YB</given-names></name><name><surname>Fernandez</surname><given-names>M</given-names></name><name><surname>Bülow</surname><given-names>T</given-names></name><name><surname>Pankratz</surname><given-names>MJ</given-names></name><name><surname>Kravitz</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Single serotonergic neurons that modulate aggression in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>24</volume><fpage>2700</fpage><lpage>2707</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.09.051</pub-id><pub-id pub-id-type="pmid">25447998</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Aradhya</surname><given-names>VNM</given-names></name><name><surname>Kumar</surname><given-names>GH</given-names></name><name><surname>Noushath</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Robust Unconstrained Handwritten Digit Recognition using Radon Transform.International Conference on Signal Processing, Communications and Networking</article-title><conf-name>IEEE</conf-name><fpage>22</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1109/ICSCN.2007.350685</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aurasopon</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dynamic Time Warping for classifying cattle behaviors and reducing acceleration data size</article-title><source>Agricultural Engineering International: The CIGR Journal</source><volume>18</volume><fpage>293</fpage><lpage>300</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayless</surname><given-names>DW</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Mason</surname><given-names>MM</given-names></name><name><surname>Susanto</surname><given-names>AAT</given-names></name><name><surname>Lobdell</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Limbic neurons shape sex recognition and social behavior in sexually naive males</article-title><source>Cell</source><volume>176</volume><fpage>1190</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.12.041</pub-id><pub-id pub-id-type="pmid">30712868</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Choi</surname><given-names>DM</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id><pub-id pub-id-type="pmid">25142523</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besson</surname><given-names>M</given-names></name><name><surname>Martin</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Centrophobism/thigmotaxis, a new role for the mushroom bodies in <italic>Drosophila</italic></article-title><source>Journal of Neurobiology</source><volume>62</volume><fpage>386</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1002/neu.20111</pub-id><pub-id pub-id-type="pmid">15547935</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloomquist</surname><given-names>BT</given-names></name><name><surname>Shortridge</surname><given-names>RD</given-names></name><name><surname>Schneuwly</surname><given-names>S</given-names></name><name><surname>Perdew</surname><given-names>M</given-names></name><name><surname>Montell</surname><given-names>C</given-names></name><name><surname>Steller</surname><given-names>H</given-names></name><name><surname>Rubin</surname><given-names>G</given-names></name><name><surname>Pak</surname><given-names>WL</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Isolation of a putative phospholipase c gene of <italic>Drosophila</italic>, norpA, and its role in phototransduction</article-title><source>Cell</source><volume>54</volume><fpage>723</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1016/S0092-8674(88)80017-5</pub-id><pub-id pub-id-type="pmid">2457447</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The OpenCV Library</article-title><source>Dr. Dobb’s Journal of Software Tools</source><volume>120</volume><fpage>122</fpage><lpage>125</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Approximating CNNs with Bag-of-Local-Features Models Works Surprisingly Well on ImageNet</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1904.00760</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>AJ</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unsupervised identification of the internal states that shape natural behavior</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>2040</fpage><lpage>2049</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0533-x</pub-id><pub-id pub-id-type="pmid">31768056</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Campello</surname><given-names>RG</given-names></name><name><surname>Moulavi</surname><given-names>D</given-names></name><name><surname>Sander</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Density-Based Clustering Based on Hierarchical Density Estimates</article-title><conf-name>Advances in Knowledge Discovery and Data Mining</conf-name><conf-loc>Gold Coast, Australia</conf-loc><fpage>160</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-37456-2</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>M</given-names></name><name><surname>Misra</surname><given-names>I</given-names></name><name><surname>Mairal</surname><given-names>J</given-names></name><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Bojanowski</surname><given-names>P</given-names></name><name><surname>Joulin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2006.09882</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>M</given-names></name><name><surname>Touvron</surname><given-names>H</given-names></name><name><surname>Misra</surname><given-names>I</given-names></name><name><surname>Jegou</surname><given-names>H</given-names></name><name><surname>Mairal</surname><given-names>J</given-names></name><name><surname>Bojanowski</surname><given-names>P</given-names></name><name><surname>Joulin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Emerging Properties in Self-Supervised Vision Transformers</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2104.14294</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Simple Framework for Contrastive Learning of Visual Representations</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2002.05709</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>He</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Exploring Simple Siamese Representation Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2011.10566</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>JH</given-names></name><name><surname>Mall</surname><given-names>U</given-names></name><name><surname>Bala</surname><given-names>K</given-names></name><name><surname>Hariharan</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering</article-title><conf-name>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><conf-loc>Nashviille, United States</conf-loc><fpage>16789</fpage><lpage>16799</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01652</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cleasby</surname><given-names>IR</given-names></name><name><surname>Wakefield</surname><given-names>ED</given-names></name><name><surname>Morrissey</surname><given-names>BJ</given-names></name><name><surname>Bodey</surname><given-names>TW</given-names></name><name><surname>Votier</surname><given-names>SC</given-names></name><name><surname>Bearhop</surname><given-names>S</given-names></name><name><surname>Hamer</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using time-series similarity measures to compare animal movement trajectories in ecology</article-title><source>Behavioral Ecology and Sociobiology</source><volume>73</volume><elocation-id>151</elocation-id><pub-id pub-id-type="doi">10.1007/s00265-019-2761-1</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coleman</surname><given-names>CM</given-names></name><name><surname>Neckameyer</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Serotonin synthesis by two distinct enzymes in <italic>Drosophila melanogaster</italic></article-title><source>Archives of Insect Biochemistry and Physiology</source><volume>59</volume><fpage>12</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1002/arch.20050</pub-id><pub-id pub-id-type="pmid">15822093</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Crusio</surname><given-names>WE</given-names></name><name><surname>Sluyter</surname><given-names>F</given-names></name><name><surname>Gerlai</surname><given-names>RT</given-names></name><name><surname>Pietropaolo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>The genetics of exploratory behavior</chapter-title><source>Genetics of Behavioral Phenotypes</source><publisher-name>Cambridge University Press</publisher-name><fpage>148</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1017/CBO9781139541022.016</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dahiya</surname><given-names>A</given-names></name><name><surname>Spurr</surname><given-names>A</given-names></name><name><surname>Hilliges</surname><given-names>O</given-names></name></person-group><article-title>Exploring self-supervised learning techniques for hand pose estimation</article-title><conf-name>NeurIPS 2020 Workshop on Pre-registration in Machine Learning, PMLR</conf-name><year iso-8601-date="2021">2021</year><fpage>255</fpage><lpage>271</lpage><ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v148/dahiya21a.html">https://proceedings.mlr.press/v148/dahiya21a.html</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fleet</surname><given-names>D</given-names></name><name><surname>Pajdla</surname><given-names>T</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name><name><surname>Tuytelaars</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Computer Vision – ECCV 2014</article-title><conf-name>European Conference on Computer Vision 2014</conf-name><fpage>772</fpage><lpage>787</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10605-2_50</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fragkiadaki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Recurrent Network Models for Kinematic Tracking</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1508.00271</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><name><surname>Rubisch</surname><given-names>P</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Wichmann</surname><given-names>F</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ImageNet-Trained CNNs Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1811.12231</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Grill</surname><given-names>JB</given-names></name><name><surname>Strub</surname><given-names>F</given-names></name><name><surname>Altch’e</surname><given-names>F</given-names></name><name><surname>Tallec</surname><given-names>C</given-names></name><name><surname>Richemond</surname><given-names>PH</given-names></name><name><surname>Buchatskaya</surname><given-names>E</given-names></name><name><surname>Doersch</surname><given-names>C</given-names></name><name><surname>Pires</surname><given-names>BA</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Azar</surname><given-names>MG</given-names></name><name><surname>Piot</surname><given-names>B</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Munos</surname><given-names>R</given-names></name><name><surname>Valko</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2006.07733</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Günel</surname><given-names>S</given-names></name><name><surname>Rhodin</surname><given-names>H</given-names></name><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Campagnolo</surname><given-names>J</given-names></name><name><surname>Ramdya</surname><given-names>P</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic></article-title><source>eLife</source><volume>8</volume><elocation-id>e48571</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48571</pub-id><pub-id pub-id-type="pmid">31584428</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The mating of a fly</article-title><source>Science</source><volume>264</volume><fpage>1702</fpage><lpage>1714</lpage><pub-id pub-id-type="doi">10.1126/science.8209251</pub-id><pub-id pub-id-type="pmid">8209251</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Hariharan</surname><given-names>B</given-names></name><name><surname>Snavely</surname><given-names>N</given-names></name><name><surname>Freeman</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unsupervised Semantic Segmentation by Distilling Feature Correspondences</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2203.08414</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Momentum Contrast for Unsupervised Visual Representation Learning</article-title><conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><conf-loc>Seattle, WA, USA</conf-loc><fpage>9726</fpage><lpage>9735</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00975</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5188</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id><pub-id pub-id-type="pmid">34465784</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Pan</surname><given-names>H</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Yi</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A hierarchical 3D-motion learning framework for animal spontaneous behavior mapping</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2784</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22970-y</pub-id><pub-id pub-id-type="pmid">33986265</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: A 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>3D convolutional neural networks for human action recognition</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>35</volume><fpage>221</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.59</pub-id><pub-id pub-id-type="pmid">22392705</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>X</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Henriques</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Invariant Information Clustering for Unsupervised Image Classification and Segmentation</article-title><conf-name>2019 IEEE/CVF International Conference on Computer Vision</conf-name><conf-loc>Seoul, Korea (South)</conf-loc><fpage>9864</fpage><lpage>9873</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2019.00996</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Selfee: Self-supervised Features Extraction of animal behaviors</data-title><version designator="swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c">swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2a27178ad1671ca020588a7013dc38ecc49697c3;origin=https://github.com/EBGU/Selfee;visit=swh:1:snp:2ea2417cdd1d3ceeb712124e63b6d282c92bfb2a;anchor=swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c">https://archive.softwareheritage.org/swh:1:dir:2a27178ad1671ca020588a7013dc38ecc49697c3;origin=https://github.com/EBGU/Selfee;visit=swh:1:snp:2ea2417cdd1d3ceeb712124e63b6d282c92bfb2a;anchor=swh:1:rev:3af2d1ed2dfcf3bd1d1c18d488ed87f7b826529c</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Z</given-names></name><name><surname>Chazot</surname><given-names>PL</given-names></name><name><surname>Celebi</surname><given-names>ME</given-names></name><name><surname>Crookes</surname><given-names>D</given-names></name><name><surname>Jiang</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Social behavioral phenotyping of <italic>Drosophila</italic> with a 2D–3D hybrid CNN framework</article-title><source>IEEE Access</source><volume>7</volume><fpage>67972</fpage><lpage>67982</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2917000</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Rivera-Alba</surname><given-names>M</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ke</surname><given-names>G</given-names></name><name><surname>Meng</surname><given-names>Q</given-names></name><name><surname>Finley</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Ma</surname><given-names>W</given-names></name><name><surname>Ye</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>TY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</article-title><conf-name>Advances in Neural Information Processing Systems 30 (NIPS 2017</conf-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohlsdorf</surname><given-names>D</given-names></name><name><surname>Herzing</surname><given-names>D</given-names></name><name><surname>Starner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Methods for discovering models of behavior: A case study with wild Atlantic spotted dolphins</article-title><source>Animal Behavior and Cognition</source><volume>3</volume><fpage>265</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.12966/abc.06.11.2016</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>CS</given-names></name><name><surname>Tao</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>F</given-names></name><name><surname>Ng</surname><given-names>WWY</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Yuan</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>C</given-names></name><name><surname>Lai</surname><given-names>LL</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Locatelli</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A robust correlation analysis framework for imbalanced and dichotomous data with uncertainty</article-title><source>Information Sciences</source><volume>470</volume><fpage>58</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2018.08.017</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>S</given-names></name><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Nath</surname><given-names>T</given-names></name><name><surname>Rahman</surname><given-names>MM</given-names></name><name><surname>Di Santo</surname><given-names>V</given-names></name><name><surname>Soberanes</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Lauder</surname><given-names>G</given-names></name><name><surname>Dulac</surname><given-names>C</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multi-Animal Pose Estimation and Tracking with DeepLabCut</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.04.30.442096</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>P</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><conf-name>Proceedings of the IEEE</conf-name><year iso-8601-date="1998">1998</year><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leng</surname><given-names>X</given-names></name><name><surname>Wohl</surname><given-names>M</given-names></name><name><surname>Ishii</surname><given-names>K</given-names></name><name><surname>Nayak</surname><given-names>P</given-names></name><name><surname>Asahina</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying influence of human choice on the automated detection of <italic>Drosophila</italic> behavior by a supervised machine learning algorithm</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0241696</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0241696</pub-id><pub-id pub-id-type="pmid">33326445</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Jan</surname><given-names>LY</given-names></name><name><surname>Jan</surname><given-names>YN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A defensive kicking behavior in response to mechanical stimuli mediated by <italic>Drosophila</italic> wing margin bristles</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>11275</fpage><lpage>11282</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1416-16.2016</pub-id><pub-id pub-id-type="pmid">27807168</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Linderman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>pyhsmm-autoregressive</data-title><version designator="swh:1:rev:19899d5531ae6222111e5881ddbf8e51b4d2da7e">swh:1:rev:19899d5531ae6222111e5881ddbf8e51b4d2da7e</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mattjj/pyhsmm-autoregressive">https://github.com/mattjj/pyhsmm-autoregressive</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A neural circuit encoding mating states tunes defensive behavior in <italic>Drosophila</italic></article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3962</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17771-8</pub-id><pub-id pub-id-type="pmid">32770059</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorbach</surname><given-names>M</given-names></name><name><surname>Kyriakou</surname><given-names>EI</given-names></name><name><surname>Poppe</surname><given-names>R</given-names></name><name><surname>van Dam</surname><given-names>EA</given-names></name><name><surname>Noldus</surname><given-names>LPJJ</given-names></name><name><surname>Veltkamp</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning to recognize rat social behavior: Novel dataset and cross-dataset application</article-title><source>Journal of Neuroscience Methods</source><volume>300</volume><fpage>166</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.05.006</pub-id><pub-id pub-id-type="pmid">28495372</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Luxem</surname><given-names>K</given-names></name><name><surname>Mocellin</surname><given-names>P</given-names></name><name><surname>Fuhrmann</surname><given-names>F</given-names></name><name><surname>Kürsch</surname><given-names>J</given-names></name><name><surname>Remy</surname><given-names>S</given-names></name><name><surname>Bauer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Identifying Behavioral Structure from Deep Variational Embeddings of Animal Motion</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.14.095430</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markow</surname><given-names>TA</given-names></name><name><surname>Manning</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Mating success of photoreceptor mutants of <italic>Drosophila melanogaster</italic></article-title><source>Behavioral and Neural Biology</source><volume>29</volume><fpage>276</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/S0163-1047(80)90612-3</pub-id><pub-id pub-id-type="pmid">6770840</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGill</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Sexual behavior in three inbred strains of mice</article-title><source>Behaviour</source><volume>19</volume><fpage>341</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1163/156853962X00087</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Saul</surname><given-names>N</given-names></name><name><surname>Großberger</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1802.03426</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNeil</surname><given-names>AR</given-names></name><name><surname>Jolley</surname><given-names>SN</given-names></name><name><surname>Akinleye</surname><given-names>AA</given-names></name><name><surname>Nurilov</surname><given-names>M</given-names></name><name><surname>Rouzyi</surname><given-names>Z</given-names></name><name><surname>Milunovich</surname><given-names>AJ</given-names></name><name><surname>Chambers</surname><given-names>MC</given-names></name><name><surname>Simon</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Conditions affecting social space in <italic>Drosophila melanogaster</italic></article-title><source>Journal of Visualized Experiments</source><volume>105</volume><elocation-id>53242</elocation-id><pub-id pub-id-type="doi">10.3791/53242</pub-id><pub-id pub-id-type="pmid">26575105</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mezzera</surname><given-names>C</given-names></name><name><surname>Brotas</surname><given-names>M</given-names></name><name><surname>Gaspar</surname><given-names>M</given-names></name><name><surname>Pavlou</surname><given-names>HJ</given-names></name><name><surname>Goodwin</surname><given-names>SF</given-names></name><name><surname>Vasconcelos</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ovipositor extrusion promotes the transition from courtship to copulation and signals female acceptance in <italic>Drosophila melanogaster</italic></article-title><source>Current Biology</source><volume>30</volume><fpage>3736</fpage><lpage>3748</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.06.071</pub-id><pub-id pub-id-type="pmid">32795437</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Mikhailov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Turbo, An Improved Rainbow Colormap for Visualization</article-title><ext-link ext-link-type="uri" xlink:href="https://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html">https://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html</ext-link><date-in-citation iso-8601-date="2019-08-20">August 20, 2019</date-in-citation></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname><given-names>JM</given-names></name><name><surname>Ravbar</surname><given-names>P</given-names></name><name><surname>Simpson</surname><given-names>JH</given-names></name><name><surname>Carlson</surname><given-names>JM</given-names></name><name><surname>Calhoun</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title><italic>Drosophila melanogaster</italic> grooming possesses syntax with distinct rules at different temporal scales</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007105</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007105</pub-id><pub-id pub-id-type="pmid">31242178</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Musgrave</surname><given-names>K</given-names></name><name><surname>Belongie</surname><given-names>SJ</given-names></name><name><surname>Lim</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>PyTorch Metric Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2008.09164</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>C</given-names></name><name><surname>Rabiner</surname><given-names>L</given-names></name><name><surname>Rosenberg</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Performance tradeoffs in dynamic time warping algorithms for isolated word recognition</article-title><conf-name>IEEE Transactions on Acoustics, Speech, and Signal Processing</conf-name><fpage>623</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1109/TASSP.1980.1163491</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Naseer</surname><given-names>M</given-names></name><name><surname>Ranasinghe</surname><given-names>K</given-names></name><name><surname>Khan</surname><given-names>S</given-names></name><name><surname>Hayat</surname><given-names>M</given-names></name><name><surname>Khan</surname><given-names>F</given-names></name><name><surname>Yang</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Intriguing Properties of Vision Transformers</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2105.10497</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paradis</surname><given-names>S</given-names></name><name><surname>Sweeney</surname><given-names>ST</given-names></name><name><surname>Davis</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Homeostatic control of presynaptic release is triggered by postsynaptic membrane depolarization</article-title><source>Neuron</source><volume>30</volume><fpage>737</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00326-9</pub-id><pub-id pub-id-type="pmid">11430807</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Köpf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title><conf-name>NIPS’19: Proceedings of the 33rd International Conference on Neural Information Processing Systems</conf-name><fpage>8026</fpage><lpage>8037</lpage><pub-id pub-id-type="doi">10.5555/3454287.3455008</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Tabris</surname><given-names>N</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>DM</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Ravindranath</surname><given-names>S</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Normand</surname><given-names>E</given-names></name><name><surname>Deutsch</surname><given-names>DS</given-names></name><name><surname>Wang</surname><given-names>ZY</given-names></name><name><surname>McKenzie-Smith</surname><given-names>GC</given-names></name><name><surname>Mitelut</surname><given-names>CC</given-names></name><name><surname>Castro</surname><given-names>MD</given-names></name><name><surname>D’Uva</surname><given-names>J</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name><name><surname>Kocher</surname><given-names>SD</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title><source>Nature Methods</source><volume>19</volume><fpage>486</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id><pub-id pub-id-type="pmid">35379947</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prut</surname><given-names>L</given-names></name><name><surname>Belzung</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The open field as a paradigm to measure the effects of drugs on anxiety-like behaviors: a review</article-title><source>European Journal of Pharmacology</source><volume>463</volume><fpage>3</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/s0014-2999(03)01272-x</pub-id><pub-id pub-id-type="pmid">12600700</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramirez</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>MacDonald</surname><given-names>CJ</given-names></name><name><surname>Moffa</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Redondo</surname><given-names>RL</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Activating positive memory engrams suppresses depression-like behaviour</article-title><source>Nature</source><volume>522</volume><fpage>335</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1038/nature14514</pub-id><pub-id pub-id-type="pmid">26085274</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravbar</surname><given-names>P</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Simpson</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An automatic behavior recognition system classifies animal behaviors using movements and their temporal context</article-title><source>Journal of Neuroscience Methods</source><volume>326</volume><elocation-id>108352</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108352</pub-id><pub-id pub-id-type="pmid">31415845</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>IMA</given-names></name><name><surname>Drews</surname><given-names>M</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Machacek</surname><given-names>C</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual projection neurons mediating directed courtship in <italic>Drosophila</italic></article-title><source>Cell</source><volume>174</volume><fpage>607</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.020</pub-id><pub-id pub-id-type="pmid">30033367</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Romero</surname><given-names>D</given-names></name><name><surname>Ruedin</surname><given-names>A</given-names></name><name><surname>Seijas</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Wavelet-based feature extraction for handwritten numeral</article-title><conf-name>International Conference on Image Analysis and Processing</conf-name><fpage>374</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-04146-4_41</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rubenstein</surname><given-names>DR</given-names></name><name><surname>Alcock</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Animal Behavior</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudolph</surname><given-names>S</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Osorno</surname><given-names>T</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Krauss</surname><given-names>JM</given-names></name><name><surname>Nyitrai</surname><given-names>H</given-names></name><name><surname>Flaquer</surname><given-names>I</given-names></name><name><surname>El-Rifai</surname><given-names>M</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Regehr</surname><given-names>WG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cerebellum-specific deletion of the GABA<sub>A</sub> receptor δ subunit leads to sex-specific disruption of behavior</article-title><source>Cell Reports</source><volume>33</volume><elocation-id>108338</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2020.108338</pub-id><pub-id pub-id-type="pmid">33147470</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Zelikowsky</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Mouse Action Recognition System (MARS): A Software Pipeline for Automated Analysis of Social Behaviors in Mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.26.222299</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>AF</given-names></name><name><surname>Chou</surname><given-names>MT</given-names></name><name><surname>Salazar</surname><given-names>ED</given-names></name><name><surname>Nicholson</surname><given-names>T</given-names></name><name><surname>Saini</surname><given-names>N</given-names></name><name><surname>Metchev</surname><given-names>S</given-names></name><name><surname>Krantz</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A simple assay to study social behavior in <italic>Drosophila</italic>: measurement of social space within A group</article-title><source>Genes, Brain, and Behavior</source><volume>11</volume><fpage>243</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1111/j.1601-183X.2011.00740.x</pub-id><pub-id pub-id-type="pmid">22010812</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>LN</given-names></name><name><surname>Topin</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://ui.adsabs.harvard.edu/abs/2017arXiv170807120S">https://ui.adsabs.harvard.edu/abs/2017arXiv170807120S</ext-link><date-in-citation iso-8601-date="2017-08-01">August 1, 2017</date-in-citation></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Zhan</surname><given-names>E</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Task Programming: Learning Data Efficient Behavior Representations</article-title><conf-name>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><conf-loc>Nashville, United States</conf-loc><fpage>2875</fpage><lpage>2884</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00290</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thistle</surname><given-names>R</given-names></name><name><surname>Cameron</surname><given-names>P</given-names></name><name><surname>Ghorayshi</surname><given-names>A</given-names></name><name><surname>Dennison</surname><given-names>L</given-names></name><name><surname>Scott</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Contact chemoreceptors mediate male-male repulsion and male-female attractionduring <italic>Drosophila</italic> Courtship</article-title><source>Cell</source><volume>149</volume><fpage>1140</fpage><lpage>1151</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2012.03.045</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toni</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Computing and visualizing dynamic time warping alignments in R: The dtw package</article-title><source>Journal of Statistical Software</source><volume>31</volume><elocation-id>i07</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v031.i07</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>SX</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination</article-title><conf-name>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><fpage>12581</fpage><lpage>12590</lpage><pub-id pub-id-type="doi">10.48550/conf.2008.03813</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>YC</given-names></name><name><surname>Wang</surname><given-names>SR</given-names></name><name><surname>Jiao</surname><given-names>ZL</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>JK</given-names></name><name><surname>Li</surname><given-names>XY</given-names></name><name><surname>Li</surname><given-names>SS</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>XH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Medial preoptic area in mice is capable of mediating sexually dimorphic behaviors regardless of gender</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>279</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02648-0</pub-id><pub-id pub-id-type="pmid">29348568</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>JM</given-names></name><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Abraira</surname><given-names>VE</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping sub-second structure in mouse behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Tsukahara</surname><given-names>T</given-names></name><name><surname>Zeine</surname><given-names>A</given-names></name><name><surname>Anyoha</surname><given-names>R</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1433</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00706-3</pub-id><pub-id pub-id-type="pmid">32958923</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Xiong</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>SX</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unsupervised Feature Learning via Non-Parametric Instance-Level Discrimination</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1805.01978</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Long</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2106.13008</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Ouyang</surname><given-names>W</given-names></name><name><surname>Bennamoun</surname><given-names>M</given-names></name><name><surname>Boussaid</surname><given-names>F</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2203.02891</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zbontar</surname><given-names>J</given-names></name><name><surname>Jing</surname><given-names>L</given-names></name><name><surname>Misra</surname><given-names>I</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Deny</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Barlow Twins: Self-Supervised Learning via Redundancy Reduction</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parallel mechanosensory pathways direct oviposition decision-making in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>30</volume><fpage>3075</fpage><lpage>3088</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.05.076</pub-id><pub-id pub-id-type="pmid">32649914</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>SX</given-names></name><name><surname>Lutas</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Diaz</surname><given-names>A</given-names></name><name><surname>Fluhr</surname><given-names>H</given-names></name><name><surname>Nagel</surname><given-names>G</given-names></name><name><surname>Gao</surname><given-names>S</given-names></name><name><surname>Andermann</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hypothalamic dopamine neurons motivate mating through persistent cAMP signalling</article-title><source>Nature</source><volume>597</volume><fpage>245</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03845-0</pub-id><pub-id pub-id-type="pmid">34433964</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Cao</surname><given-names>LH</given-names></name><name><surname>Sui</surname><given-names>XW</given-names></name><name><surname>Guo</surname><given-names>XQ</given-names></name><name><surname>Luo</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mechanosensory circuits coordinate two opposing motor actions in <italic>Drosophila</italic> feeding</article-title><source>Science Advances</source><volume>5</volume><elocation-id>eaaw5141</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aaw5141</pub-id><pub-id pub-id-type="pmid">31131327</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76218.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zlatic</surname><given-names>Marta</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00tw3jy02</institution-id><institution>MRC Laboratory of Molecular Biology</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.12.24.474120" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.12.24.474120"/></front-stub><body><p>Jia et al., present a valuable machine learning framework, Selfee, based on deep neural networks for analyzing video recordings of animal behavior, which is efficient and runs in an unsupervised fashion, and requires very little pre-processing. Selfee should be of broad interest to researchers studying quantitative animal behavior.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76218.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zlatic</surname><given-names>Marta</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00tw3jy02</institution-id><institution>MRC Laboratory of Molecular Biology</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Klein</surname><given-names>Mason</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02dgjyy92</institution-id><institution>University of Miami</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Ravbar</surname><given-names>Primoz</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.12.24.474120">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.12.24.474120v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Selfee: Self-supervised Features Extraction of animal behaviors&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and K VijayRaghavan as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Mason Klein (Reviewer #1); Primoz Ravbar (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>The Reviewers agree that the paper presents a really exciting method that could have broad impact on researchers studying quantitative animal behavior. However there are some unresolved issues for establishing the credibility of the method that we would like you to address.</p><p>Essential revisions:</p><p>1) Chose one or two strong and clean datasets and take them all the way through the analysis pipeline (selfee-&gt;anomaly score-&gt;AR-HMM-&gt;DTW) instead of using different experiments for explaining different points. We suggest removing IR76b data from the story unless the experiment can be somehow justified.</p><p>2) Also show Selfee's results on a simpler data set (with <italic>Drosophila</italic> or some other animal) with single-animal video.</p><p>3) Explain the rationale behind the choice of parameters for &quot;Selfee&quot;. Please explain why Selfee performs so poorly without the CLD component. Would a simpler architecture, with the addition of CLD, achieve similar performance as the Selfee?</p><p>4) Explore the behaviors found in different parts of the t-SNE maps and evaluate the effect of the irrelevant features on their distributions. The t-SNE maps display several clusters representing the same behaviors (Figure 2D) which suggests strong overfitting (weak generalization). The authors should at least show some examples taken from different clusters. This would illustrate why Selfee is separating behaviors that humans perceive as a single category.</p><p>5) Provide a formal comparison with one of the pre-existing methods that would in theory work on the same videos (e.g. JAABA) (Posted 6th Apr 2022).</p><p>6) Provide a supplementary table showing full genotype of all experimental animals.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Overall I like the paper! I will try to give as many specific, detailed comments as possible in this section.</p><p>First, to clarify some of the items I listed under &quot;weaknesses&quot; in the public review:</p><p>(1) Since the paper feels mainly like a Selfee methods paper, I think spending more time describing what exactly Selfee is and how it works would be appropriate. I would recommend putting an expanded version of the Methods section paragraph (lines 925-933) in the main body of the paper, briefly explaining the techniques and existing software as you go through each step. It would give a reader with little machine learning experience a better understanding of the process.</p><p>(2) Efficiency was talked about in the introduction and occasionally brought up when highlighting that Selfee doesn't need large computational/processing power to operate, but I didn't come away with a sense of how much Selfee improves on other approaches in this regard. It would be helpful to be more specific.</p><p>(3) The comparisons with DeepLabCut make sense -- if it would be straightforward to compare to other software, that might help highlight unique aspects of Selfee a bit more effectively.</p><p>(4) I don't usually recommend new experiments when I review papers, but perhaps showing Selfee's results on a simpler data set (with <italic>Drosophila</italic> or some other animal), say with single-animal video, before showing the courtship-type results, could ease a reader into the process more effectively.</p><p>(5) It would also be great, if possible, to include a small section that could operate like a guide for a novice user to try Selfee on their own data.</p><p>A few general comments:</p><p>(1) I am not a machine learning expert myself, so I hope another reviewer could evaluate Selfee's uniqueness compared to other methods, as I am taking that at face value in my review. In particular, (line 17-19), is this true that unsupervised analysis has not been used this way before?</p><p>(2) A fair amount of grammar/word usage errors, not really a problem and I assume copy editors will address? (e.g. Fast --&gt; Quickly or Rapidly on line 15).</p><p>(3) I generally like the figures, but found Figure 2 and Figure 3 pretty difficult to read at that size. Rearranging the panels and/or using larger fonts would help readability considerably.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This work is very relevant but has some serious unresolved issues before warranting further consideration. The overall format of the paper will also have to be massively restructured to make it palatable for a wide, life-science research audience.</p><p>Major points of concern that need to be addressed:</p><p>1) In the way it is presented, the rationale behind choice of several of the parameters for &quot;Selfee&quot; are unclear to me. E.g.</p><p>a. Why do the live frames consist of 3 tandem frames? Is 3 somehow a special number that is easy to compute or represent as an RGB image? Why not 4 with CMYK or a higher number with a distinct colormap as is commonly done for representing depth coding.</p><p>b. The choice of AR-HMM and DTW as post-processing steps after the Selfee analysis is also a bit unclear to me. Especially because the authors do not discuss any alternatives and moreover, they don't even show great promise in the examples later in the manuscript.</p><p>2) The authors have surprisingly failed to acknowledge and cite the state of the field that directly translates to their novelty claims (Lines 56-59: &quot;There are no pose estimation paradigms that are designed to handle multi-animal contexts&quot;). Both multi-animal DLC (Lauer et al., 2021) and SLEAP (Perreira et al., 2020) were explicitly designed for this purpose. Not to mention a plethora tools for multi-animal behavioral classification, especially during social interactions that provide pretty much all the extracted information obtained using Selfee in this manuscript. Listing a few <italic>Drosophila</italic> specific tools:</p><p>a. JAABA (Kaabra et al., Nat Methods, 2013)</p><p>b. Matebook (Ribeiro et al., Cell 2018)</p><p>c. Flytracker (Hoopfer et al., <italic>eLife</italic> 2015)</p><p>3) The authors must provide a sup table showing full genotype of all experimental animals. Wherever possible test and controls should be in the same genetic background (this is especially true for loss of function experiments where vision and locomotion is an integral part of the behavior, like the ones done in this manuscript). Legends for videos should also be provided.</p><p>4) Figure 2 demonstrates how good Selfee is at extracting information. To do this, authors have to either look at how data clusters with respect to human annotated labels on a t-sne map or use a k-nn classification step to classify the data. Honestly, the t-sne maps in Figure 2D don't look that promising. K-nn classification looks better with around 70% F1 score. But at this point there should be a formal comparison with one of the pre-exisiting methods that would in theory work on the same videos. I suggest JAABA (or Flytracker+JAABA) since similar labels have been previously extracted using this toolkit. Same holds true for mouse data (2G-I) and especially more important because the F1 scores here are a bit low. Also panels 2F and 2I likely represent cherry picked cases where results are much better than what the scores indicate and hence are not really informative.</p><p>5) The rationale for definition of &quot;anomaly score&quot; (Figure 3, lines 230-249) is again poorly defended in the text. Could authors clearly define what &quot;meta-representations of frames&quot; means. Also, why restrict k=1 for IES? How is the +-50 frame window chosen for IAS score. Are these empirically derived values after some trial and error, or based on some priors. Since the anomaly score is quite central to the overall utility of this tool, it needs to be proposed in a much more credible way.</p><p>6) The unusual subset of neurotransmitter mutants/silenced lines used in Figure 3 data makes one wonder if the provided data is a cherry-picked subset of the full data to make the story easier. I would really appreciate if all data were shown clearly and rationally. The authors really need to establish credibility for persuading the reader to use their method.</p><p>7) Trh mutant phenotype is interesting but interpretations of secondary assays warrant some more thought. Trh neurons directly innervate <italic>Drosophila</italic> legs (Howard et al., 2019, Curr Biol.) and impact locomotion. Making a claim about social repulsion is a bit too ambitious without further segregating impact on other behaviors like walking that could impact the phenotype observed by the authors.</p><p>8) Given that the utility of AR-HMM rests on IR76b experiments which seem to be based on a false premise, it would be best if authors repeat this with one of the myriad of receptors shown to be involved in courtship. Or rather just do AR-HMM on one of the previous datasets.</p><p>9) DTW is a cool method but using increased copulation latency of blind flies to show its usability is a bit of a short-sell. It does not in any way increase the credibility of this method and I would rather suggest using DTW to look at longer scale dynamics in previous experiments (e.g. Figure 3 data), provided these are done in the correct genetic background).</p><p>Recommendation for improvement: The point of this paper is the method and not the biological findings. But the method loses credibility because there are serious problems with the way the experiments are performed and data is presented. I would recommend the authors chose one strong and clean dataset and take it all the way through their analysis pipeline (selfee-&gt;anomaly score-&gt;AR-HMM-&gt;DTW) instead of using different experiments for explaining different points. Even if the results are less interesting, if the rationale is clear and all data is shown properly this will encourage other people to try out this tool.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The figures are in a good shape, except where specifically mentioned (and assuming that the final published figures will have better resolution).</p><p>Major concerns (2):</p><p>1) Reference [9] is critically important for the understanding of how the CLD is implemented in this architecture, however, I find it difficult to access the reference. The authors should provide for an alternative. Adding more visualization of how the CLD is implemented, in Figure 2, would be helpful. The authors should compare the classification performance of Selfee with and without the CLD (for fly courtship behavior in Figure 2). This way the reader could understand the weight of the CLD in the overall architecture. Next, the authors should discuss alternative architectures with CLD and show why the SimSiam has been chosen.</p><p>2) How much of the performance of the classification (and possibly other utilities) is lost to strict end-to-end processing? The authors should show the effects of the following pre-processing steps on the t-SNE maps, the confusion matrices, and the F1 scores: a) use the raw frames instead of live-frames; b) perform simple background subtraction; c) create narrow areas of interest around the animals in each frame. Authors can add other steps such as segmentation. The prediction would be that the classification performance will improve greatly, the t-SNE maps should have fewer yet more distinct clusters, and rare behaviors should be classified better than, for example, in Figure 2—figure supplement 5 F.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76218.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Chose one or two strong and clean datasets and take them all the way through the analysis pipeline (selfee-&gt;anomaly score-&gt;AR-HMM-&gt;DTW) instead of using different experiments for explaining different points. We suggest removing IR76b data from the story unless the experiment can be somehow justified.</p></disp-quote><p>First, we removed experiments done on Ir76b mutant flies and replaced them with analyses of open-field tests (OFT) performed on mice after chronic immobilization stress (CIS) treatment. We also used this experiment for the whole pipeline (Selfee-&gt;anomaly score-&gt;ARHMM-&gt;DTW), and the results are shown in the last section of Results.</p><disp-quote content-type="editor-comment"><p>2) Also show Selfee's results on a simpler data set (with <italic>Drosophila</italic> or some other animal) with single-animal video.</p></disp-quote><p>We have used Selfee to analyze the video from single-mouse open-field tests. The results are now included in the manuscript.</p><disp-quote content-type="editor-comment"><p>3) Explain the rationale behind the choice of parameters for &quot;Selfee&quot;. Please explain why Selfee performs so poorly without the CLD component. Would a simpler architecture, with the addition of CLD, achieve similar performance as the Selfee?</p></disp-quote><p>For neural network design, we followed the ResNet50 architecture and SimSiam framework as cited in the manuscript. For classifications, we used the previously published weighted k-NN classifier and LightGBM classifier without changing parameters. The rationale for choosing other parameters was explained in the manuscript as well as in the following point-by-point response to reviewers’ comments.</p><p>The contribution of the CLD loss was discussed in detail in the last paragraph of Siamese convolutional neural networks capture discriminative representations of animal posture, the second section of Results. Further optimization of the neural network architectures was discussed in the Discussion section.</p><p>A simpler architecture, with the addition of CLD, may not be able to achieve a similar performance as Selfee. In short, the CLD loss needs a Siamese structure, and SimSiam is the simplest Siamese neural network as far as we know, which has only one ResNet50, one threelayer projector and one term of loss. We don’t envision any Siamese CNNs could be significantly simpler than SimSiam.</p><disp-quote content-type="editor-comment"><p>4) Explore the behaviors found in different parts of the t-SNE maps and evaluate the effect of the irrelevant features on their distributions. The t-SNE maps display several clusters representing the same behaviors (Figure 2D) which suggests strong overfitting (weak generalization). The authors should at least show some examples taken from different clusters. This would illustrate why Selfee is separating behaviors that humans perceive as a single category.</p><p>5) Provide a formal comparison with one of the pre-existing methods that would in theory work on the same videos (e.g. JAABA) (Posted 6th Apr 2022).</p></disp-quote><p>Thanks for the suggestion. We compared Selfee extracted features with FlyTracker or JAABA engineered features, and we also visualized pose estimation results of animals during intensive social interactions from three widely used toolkits, DeepLabCut, SLEAP and FlyTracker. We found our method could extract features as discriminative as FlyTracker or JAABA features. The outcomes for the pre-existing methods and Selfee were outlined in Table 1.</p><disp-quote content-type="editor-comment"><p>6) Provide a supplementary table showing full genotype of all experimental animals.</p></disp-quote><p>We have provided a detailed resources table before the Methods section.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Overall I like the paper! I will try to give as many specific, detailed comments as possible in this section.</p><p>First, to clarify some of the items I listed under &quot;weaknesses&quot; in the public review:</p><p>(1) Since the paper feels mainly like a Selfee methods paper, I think spending more time describing what exactly Selfee is and how it works would be appropriate. I would recommend putting an expanded version of the Methods section paragraph (lines 925-933) in the main body of the paper, briefly explaining the techniques and existing software as you go through each step. It would give a reader with little machine learning experience a better understanding of the process.</p></disp-quote><p>We expanded the method between lines 925 to 933 in the second paragraph of Siamese convolutional neural networks capture discriminative representations of animal posture, the second section of Results. We also explained more about how our pipeline works in the last section of Results and Figure 6.</p><disp-quote content-type="editor-comment"><p>(2) Efficiency was talked about in the introduction and occasionally brought up when highlighting that Selfee doesn't need large computational/processing power to operate, but I didn't come away with a sense of how much Selfee improves on other approaches in this regard. It would be helpful to be more specific.</p></disp-quote><p>We didn’t compare our methods with other self-supervised learning methods formally, because we used our own datasets but not the widely used ImageNet, and training a new model could be computationally expensive. However, as we claimed in the third paragraph of the Discussion, Selfee made self-supervised learning accessible to biology labs by training with only a workstation equipped with one GPU but not multi-GPU servers. We also showed that our method could be transferred from one behavioral assay to another without training.</p><disp-quote content-type="editor-comment"><p>(3) The comparisons with DeepLabCut make sense -- if it would be straightforward to compare to other software, that might help highlight unique aspects of Selfee a bit more effectively.</p></disp-quote><p>We compared Selfee feature extraction with features from FlyTracker or JAABA, two widely used software. We also visualized tracking results of SLEAP and FlyTracker in complement to the DeepLabCut experiment.</p><disp-quote content-type="editor-comment"><p>(4) I don't usually recommend new experiments when I review papers, but perhaps showing Selfee's results on a simpler data set (with <italic>Drosophila</italic> or some other animal), say with single-animal video, before showing the courtship-type results, could ease a reader into the process more effectively.</p></disp-quote><p>We used Selfee with single-mouse OFT of mice after CIS treatment and demonstrated increased thigmotaxis behavior in CIS treated mice. Our results were consistent with previous literature and animal tracking analysis.</p><disp-quote content-type="editor-comment"><p>(5) It would also be great, if possible, to include a small section that could operate like a guide for a novice user to try Selfee on their own data.</p></disp-quote><p>We wrote in the last section of Results to showcase how Selfee works on behavior videos. For further clarity, a more detailed step-by-step guide was uploaded to the GitHub repository.</p><disp-quote content-type="editor-comment"><p>A few general comments:</p><p>(1) I am not a machine learning expert myself, so I hope another reviewer could evaluate Selfee's uniqueness compared to other methods, as I am taking that at face value in my review. In particular, (line 17-19), is this true that unsupervised analysis has not been used this way before?</p></disp-quote><p>As far as we know, most unsupervised methods required specified equipment or processing procedures, and could not be used in an end-to-end way. For example, MotionMapper requires image registration; ABRS requires “a representative training data set” for dimension reduction, and MoSeq requires depth images but not RGB images. However, we also rewrote lines 17-19 to avoid potential overclaim.</p><disp-quote content-type="editor-comment"><p>(2) A fair amount of grammar/word usage errors, not really a problem and I assume copy editors will address? (e.g. Fast --&gt; Quickly or Rapidly on line 15).</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>(3) I generally like the figures, but found Figure 2 and Figure 3 pretty difficult to read at that size. Rearranging the panels and/or using larger fonts would help readability considerably.</p></disp-quote><p>For Figure 2, we rearranged it, used a larger front, and split it into two separate figures. We also added some annotation to the original Figure 3 (the new Figure 4).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This work is very relevant but has some serious unresolved issues before warranting further consideration. The overall format of the paper will also have to be massively restructured to make it palatable for a wide, life-science research audience.</p><p>Major points of concern that need to be addressed:</p><p>1) In the way it is presented, the rationale behind choice of several of the parameters for &quot;Selfee&quot; are unclear to me. E.g.</p><p>a. Why do the live frames consist of 3 tandem frames? Is 3 somehow a special number that is easy to compute or represent as an RGB image? Why not 4 with CMYK or a higher number with a distinct colormap as is commonly done for representing depth coding.</p></disp-quote><p>There are mainly three reasons. First, as we claimed in the manuscript, we followed the original ResNet50 network architecture and we adopt its public pre-trained weights, so the input channel was set to three. Second, best to our knowledge, we don’t know any examples that use CYMK coding for image inputs of a CNN. As for depth, RGBD is a common choice, but our data did not contain depth information. Last, to implement Turbo transformation whose importance was discussed in the Results, the input has to be an RGB input, which is obvious in Figure 2 and Figure 2—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>b. The choice of AR-HMM and DTW as post-processing steps after the Selfee analysis is also a bit unclear to me. Especially because the authors do not discuss any alternatives and moreover, they don't even show great promise in the examples later in the manuscript.</p></disp-quote><p>We added some discussion about the alternatives, such as UMAP non-linear transformations, HDBSCAN clustering and dynamic time alignment kernel (DTAK).</p><disp-quote content-type="editor-comment"><p>2) The authors have surprisingly failed to acknowledge and cite the state of the field that directly translates to their novelty claims (Lines 56-59: &quot;There are no pose estimation paradigms that are designed to handle multi-animal contexts&quot;). Both multi-animal DLC (Lauer et al., 2021) and SLEAP (Perreira et al., 2020) were explicitly designed for this purpose. Not to mention a plethora tools for multi-animal behavioral classification, especially during social interactions that provide pretty much all the extracted information obtained using Selfee in this manuscript. Listing a few <italic>Drosophila</italic> specific tools:</p><p>a. JAABA (Kaabra et al., Nat Methods, 2013)</p><p>b. Matebook (Ribeiro et al., Cell 2018)</p><p>c. Flytracker (Hoopfer et al., eLife 2015)</p></disp-quote><p>Our original claim was “there is no demonstration of these pose-estimation methods applied to multiple animals of the same color with intensive interactions”, where we intended to note that the current pose-estimation method was not optimized for multiple animals of the same color with intensive interactions. In the original manuscript, we showed the pose estimation results of DLC for our videos on two mice with the same color. In this revision, we added results from SLEAP and FlyTracker. We think that current tracking or pose-estimation algorithms are not suited for intensive social interactions of multiple animals of the same color.</p><p>Second, we would like to discuss more about JAABA, the most famous fly tool, aside from what we discussed in the manuscript. JAABA can be generally split into four parts: animal tracking and pose estimation, spatial feature engineering, temporal feature engineering, and classifier. For JAABA, the first part can be replaced FlyTracker, and SLEAP or DeepLabCut with certain modifications. Selfee could be in the same position of the first and the second part. Our method is just an engineering-free feature extractor without specified temporal feature engineering and classifier. The k-NN classifier used in the manuscript was a demonstration that the feature is discriminative. It is weaker compared with the gentle boost machine used in JAABA, or SVM used in FlyTracker, but it has fewer parameters and was a common choice in the previous publications of self-supervised learning. In this version, we showed that our feature was as discriminative as the engineered features from JAABA in some cases.</p><disp-quote content-type="editor-comment"><p>3) The authors must provide a sup table showing full genotype of all experimental animals. Wherever possible test and controls should be in the same genetic background (this is especially true for loss of function experiments where vision and locomotion is an integral part of the behavior, like the ones done in this manuscript). Legends for videos should also be provided.</p></disp-quote><p>A table containing all genetic information was added. Details of genetic background are discussed in previous sections. Legends for videos were provided.</p><disp-quote content-type="editor-comment"><p>4) Figure 2 demonstrates how good Selfee is at extracting information. To do this, authors have to either look at how data clusters with respect to human annotated labels on a t-sne map or use a k-nn classification step to classify the data. Honestly, the t-sne maps in Figure 2D don't look that promising. K-nn classification looks better with around 70% F1 score. But at this point there should be a formal comparison with one of the pre-exisiting methods that would in theory work on the same videos. I suggest JAABA (or Flytracker+JAABA) since similar labels have been previously extracted using this toolkit. Same holds true for mouse data (2G-I) and especially more important because the F1 scores here are a bit low. Also panels 2F and 2I likely represent cherry picked cases where results are much better than what the scores indicate and hence are not really informative.</p></disp-quote><p>We added a comparison between our method and JAABA+FlyTracker on a previously constructed fly dataset, which contained not only features from these two methods but also finegrained scores from two human experts. Our results were comparable with JAABA and FlyTracker. We did not do the same thing on mice videos, because as we showed in Figure 1— figure supplement 3, the tracking result on copulating mice is not informative.</p><p>The main reason why the raster plot looked more promising than the F<sub>1</sub> score is the imbalance of animal behaviors. When we reported F<sub>1</sub> scores in the original Figure 2 (the new Figure 3), we used the macro average of each behavior, because all behaviors were biologically significant. For classes like copulation attempts or mounting that contained much fewer samples, they are extremely difficult to machine learning models but they contributed to the score equally. However, they did not contribute equally to the appearance of the raster plot due to smaller sample numbers. If we change macro average to micro average (each sample was treated equally), the F1 score of mice behavior would be 86.0% for eight-fold cross-validation. It might make the original Figure 2I (the new Figure 3F) more understandable.</p><p>For the original Figure 2F (the new Figure 3C), F<sub>1</sub> score is 76.14% and mAP is 76.09%, where F<sub>1</sub> score is a bit higher than seven-fold cross-validation (76.1% vs 72.4%) and mAP is nearly identical (76.1 vs 75.8). However, when conducting this experiment, we used all data for crossvalidation, but not a leave-one-out setup, so a performance boost is not surprised. For Figure the original 2I (the new Figure 3F), F1 sore is 68.1% and mAP is not available for this ensembled classifier. Compared with eight-fold cross-validation, it is only a bit higher (68.1% vs 67.4%), but it is also not supervising because we used an ensemble classifier from crossvalidation experiments.</p><disp-quote content-type="editor-comment"><p>5) The rationale for definition of &quot;anomaly score&quot; (Figure 3, lines 230-249) is again poorly defended in the text. Could authors clearly define what &quot;meta-representations of frames&quot; means. Also, why restrict k=1 for IES? How is the +-50 frame window chosen for IAS score. Are these empirically derived values after some trial and error, or based on some priors. Since the anomaly score is quite central to the overall utility of this tool, it needs to be proposed in a much more credible way.</p></disp-quote><p>Meta-representation means our feature contains spatial features and short temporal features of animal behaviors, and uniforms natural-languages-based human descriptions and engineered features from tracking results in different units (e.g., rad for angle and mm for distance) and scales in a single discriminative Meta-representation (the new Figure 3, Figure 3—figure supplement 1, Table 3).</p><p>For IES, which implements a k-NN anomaly detection algorithm, setting k=1 is a trivial and intuitive case of the k-NN algorithm to avoid parameter search of k. We didn’t have any empirical experience for the k value, and we had no validation set for k searching, so k=1 was only a showcase to demonstrate this anomaly detection could work for Selfee features.</p><p>For IAS score, as we showed in the original Figure 3—figure supplement 1B (the new Figure 4—figure supplement 1B), the similarity between the central frame and frames that were 50 frames away was quite low, dropping to half of its maximum value. We didn’t test other values, but 50 was a safe choice according to this figure.</p><disp-quote content-type="editor-comment"><p>6) The unusual subset of neurotransmitter mutants/silenced lines used in Figure 3 data makes one wonder if the provided data is a cherry-picked subset of the full data to make the story easier. I would really appreciate if all data were shown clearly and rationally. The authors really need to establish credibility for persuading the reader to use their method.</p></disp-quote><p>As we explained previously, we only used pre-existing videos in our lab which are mainly focused on nutrient sensing and behavior homeostasis. Also, most of the important neurotransmitter mutants are not viable.</p><disp-quote content-type="editor-comment"><p>7) Trh mutant phenotype is interesting but interpretations of secondary assays warrant some more thought. Trh neurons directly innervate <italic>Drosophila</italic> legs (Howard et al., 2019, Curr Biol.) and impact locomotion. Making a claim about social repulsion is a bit too ambitious without further segregating impact on other behaviors like walking that could impact the phenotype observed by the authors.</p></disp-quote><p>We changed our claim from reduced social repulsion to reduced social distance. We don’t think that changed locomotion likely contributed to the reduced social distance. According to Howard et al., Trh mutant flies had faster walking speeds and increased locomotion. In our aggregation behavior assay, increased locomotion could have induced a hyperactive phenotype leading to movements from the top of the triangle chamber to a distant zone of a more comfortable social distance, which we did not observe.</p><disp-quote content-type="editor-comment"><p>8) Given that the utility of AR-HMM rests on IR76b experiments which seem to be based on a false premise, it would be best if authors repeat this with one of the myriad of receptors shown to be involved in courtship. Or rather just do AR-HMM on one of the previous datasets.</p></disp-quote><p>The results of Ir76b were removed. AR-HMM was done with a dataset from mouse open field test.</p><disp-quote content-type="editor-comment"><p>9) DTW is a cool method but using increased copulation latency of blind flies to show its usability is a bit of a short-sell. It does not in any way increase the credibility of this method and I would rather suggest using DTW to look at longer scale dynamics in previous experiments (e.g. Figure 3 data), provided these are done in the correct genetic background).</p></disp-quote><p>We agreed that DTW applied to NorpA mutants was a trivial case, but we intended to demonstrate our method, rather than to make a new biological discovery. In the videos used in the original Figure 3 (the new Figure 4), we discussed male-male interactions of flies. In such an assay we didn’t observe the long-term dynamic of behaviors.</p><disp-quote content-type="editor-comment"><p>Recommendation for improvement: The point of this paper is the method and not the biological findings. But the method loses credibility because there are serious problems with the way the experiments are performed and data is presented. I would recommend the authors chose one strong and clean dataset and take it all the way through their analysis pipeline (selfee-&gt;anomaly score-&gt;AR-HMM-&gt;DTW) instead of using different experiments for explaining different points. Even if the results are less interesting, if the rationale is clear and all data is shown properly this will encourage other people to try out this tool.</p></disp-quote><p>We used different experiments for demonstrations of different methods because it was extremely hard to find an animal that had all short-term anomalies, which can be detected by anomaly detection, micro-dynamic changes, which is the field of AR-HMM, and long-term behavioral difference, suitable for DTW analysis. Most mutant animals we tested only had behavioral changes at one time scale, and strong mutants that were far beyond normal could be easily spotted by human observers and may be too trivial to demonstrate the power of automated self-supervised behavior analysis. We went through our Selfee pipeline with mice OFT assays.</p><p>We didn’t find any short-term anomaly in CIS-treated mice, but we found their increased thigmotaxis behavior could be proved by animal tracking.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The figures are in a good shape, except where specifically mentioned (and assuming that the final published figures will have better resolution).</p><p>Major concerns (2):</p><p>1) Reference [9] is critically important for the understanding of how the CLD is implemented in this architecture, however, I find it difficult to access the reference. The authors should provide for an alternative. Adding more visualization of how the CLD is implemented, in Figure 2, would be helpful. The authors should compare the classification performance of Selfee with and without the CLD (for fly courtship behavior in Figure 2). This way the reader could understand the weight of the CLD in the overall architecture. Next, the authors should discuss alternative architectures with CLD and show why the SimSiam has been chosen.</p></disp-quote><p>The CLD paper can be accessed from two sources:</p><p>UCB official website http://people.eecs.berkeley.edu/~xdwang/papers/CVPR2021_CLD.pdf and the preprint version https://arxiv.org/abs/2008.03813v4 .</p><p>The GitHub repository is at:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/frank-xwang/CLD-UnsupervisedLearning">https://github.com/frank-xwang/CLD-UnsupervisedLearning</ext-link>.</p><p>The comparison results between with and without CLD were in the original Figure 2—figure supplement 6 (the new Table 2).</p><p>In our paper, we simply chose a simpler model SimSiam but not a more complex one like BYOL. In our GitHub repository, we implemented the BYOL architecture and it would work. However, we were afraid that it may cause confusion to biological researchers if more neural network structure was talked about. We intended to focus on self-supervised learning but not one specific network design.</p><disp-quote content-type="editor-comment"><p>2) How much of the performance of the classification (and possibly other utilities) is lost to strict end-to-end processing? The authors should show the effects of the following pre-processing steps on the t-SNE maps, the confusion matrices, and the F1 scores: a) use the raw frames instead of live-frames; b) perform simple background subtraction; c) create narrow areas of interest around the animals in each frame. Authors can add other steps such as segmentation. The prediction would be that the classification performance will improve greatly, the t-SNE maps should have fewer yet more distinct clusters, and rare behaviors should be classified better than, for example, in Figure 2—figure supplement 5 F.</p></disp-quote><p>For raw frames instead of live-frames, results were added in the new Table 3, and there was around one to three percent performance loss. For background subtraction, we actually used it for all mice videos and fly videos when comparisons with JAABA and FlyTracker were made.</p><p>We found that our method would not work without background subtraction for mice mating videos because the beddings caused a lot of distraction. Segmentation of two animals was actually very difficult. Because the relative position between two animals was difficult to preserve after the segmentation, and segmenting intensively interacting animals was also very hard. Even more, thigmotaxis behavior became hard to identify after segmentation, which was very important to evaluate mice anxiety. Therefore, we did not implement animal segmentation.</p></body></sub-article></article>