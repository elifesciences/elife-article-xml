<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75323</article-id><article-id pub-id-type="doi">10.7554/eLife.75323</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Prefrontal cortex supports speech perception in listeners with cochlear implants</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-259871"><name><surname>Sherafati</surname><given-names>Arefeh</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2543-0851</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-266371"><name><surname>Dwyer</surname><given-names>Noel</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-266372"><name><surname>Bajracharya</surname><given-names>Aahana</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7361-6020</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-266373"><name><surname>Hassanpour</surname><given-names>Mahlega Samira</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-266374"><name><surname>Eggebrecht</surname><given-names>Adam T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-176406"><name><surname>Firszt</surname><given-names>Jill B</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-266375"><name><surname>Culver</surname><given-names>Joseph P</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-4395"><name><surname>Peelle</surname><given-names>Jonathan E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9194-854X</contrib-id><email>j.peelle@northeastern.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Department of Radiology, Washington University in St. Louis</institution></institution-wrap><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Department of Otolaryngology, Washington University in St. Louis</institution></institution-wrap><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03r0ha626</institution-id><institution>Moran Eye Center, University of Utah</institution></institution-wrap><addr-line><named-content content-type="city">Salt Lake City</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Department of Electrical &amp; Systems Engineering, Washington University in St. Louis</institution></institution-wrap><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Department of Biomedical Engineering, Washington University in St. Louis</institution></institution-wrap><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Division of Biology and Biomedical Sciences, Washington University in St. Louis</institution></institution-wrap><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01yc7t268</institution-id><institution>Department of Physics, Washington University in St. Louis</institution></institution-wrap><addr-line><named-content content-type="city">St. Louis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kj2bm70</institution-id><institution>University of Newcastle</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75323</elocation-id><history><date date-type="received" iso-8601-date="2021-11-05"><day>05</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-06-04"><day>04</day><month>06</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-10-16"><day>16</day><month>10</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.10.16.464654"/></event></pub-history><permissions><copyright-statement>© 2022, Sherafati et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Sherafati et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75323-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75323-figures-v2.pdf"/><abstract><p>Cochlear implants are neuroprosthetic devices that can restore hearing in people with severe to profound hearing loss by electrically stimulating the auditory nerve. Because of physical limitations on the precision of this stimulation, the acoustic information delivered by a cochlear implant does not convey the same level of acoustic detail as that conveyed by normal hearing. As a result, speech understanding in listeners with cochlear implants is typically poorer and more effortful than in listeners with normal hearing. The brain networks supporting speech understanding in listeners with cochlear implants are not well understood, partly due to difficulties obtaining functional neuroimaging data in this population. In the current study, we assessed the brain regions supporting spoken word understanding in adult listeners with right unilateral cochlear implants (n=20) and matched controls (n=18) using high-density diffuse optical tomography (HD-DOT), a quiet and non-invasive imaging modality with spatial resolution comparable to that of functional MRI. We found that while listening to spoken words in quiet, listeners with cochlear implants showed greater activity in the left prefrontal cortex than listeners with normal hearing, specifically in a region engaged in a separate spatial working memory task. These results suggest that listeners with cochlear implants require greater cognitive processing during speech understanding than listeners with normal hearing, supported by compensatory recruitment of the left prefrontal cortex.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech perception</kwd><kwd>diffuse optical tomography</kwd><kwd>cochlear implant</kwd><kwd>fNIRS</kwd><kwd>HD-DOT</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21DC015884</award-id><principal-award-recipient><name><surname>Peelle</surname><given-names>Jonathan E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21DC016086</award-id><principal-award-recipient><name><surname>Peelle</surname><given-names>Jonathan E</given-names></name><name><surname>Culver</surname><given-names>Joseph P</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K01MH103594</award-id><principal-award-recipient><name><surname>Eggebrecht</surname><given-names>Adam T</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21MH109775</award-id><principal-award-recipient><name><surname>Eggebrecht</surname><given-names>Adam T</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS090874</award-id><principal-award-recipient><name><surname>Culver</surname><given-names>Joseph P</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS109487</award-id><principal-award-recipient><name><surname>Culver</surname><given-names>Joseph P</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01DC019507</award-id><principal-award-recipient><name><surname>Peelle</surname><given-names>Jonathan E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The use of high-density optical brain imaging in listeners with cochlear implants shows increased activity in frontal cortex during speech perception compared to those with normal hearing.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Cochlear implants (CIs) are neuroprosthetic devices that can restore hearing in people with severe to profound hearing loss by electrically stimulating the auditory nerve. Because of physical limitations on the precision of this stimulation—including, for example, the spatial spread of electrical current (<xref ref-type="bibr" rid="bib22">Garcia et al., 2021</xref>)—the auditory stimulation delivered by a CI does not convey the same level of acoustic detail as normal hearing. As a result, speech understanding in listeners with CIs is poorer than in listeners with normal hearing (<xref ref-type="bibr" rid="bib20">Firszt et al., 2004</xref>). Notably, even in quiet, listeners with CIs report increased effort during listening (<xref ref-type="bibr" rid="bib12">Dwyer and Firszt, 2014</xref>). Despite these challenges, many listeners with CIs attain significant success in understanding auditory speech. This remarkable success raises the question of how listeners with CIs make sense of a degraded acoustic signal.</p><p>One area of key importance is understanding the degree to which listeners with CIs rely on non-linguistic cognitive mechanisms to compensate for a degraded acoustic signal. In listeners with normal hearing, cognitive demands increase when speech is acoustically challenging (<xref ref-type="bibr" rid="bib35">Peelle, 2018</xref>). For example, even when speech is completely intelligible, acoustically degraded speech can reduce later memory for what has been heard (<xref ref-type="bibr" rid="bib7">Cousins et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Koeritzer et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Rabbitt, 1968</xref>; <xref ref-type="bibr" rid="bib61">Ward et al., 2016</xref>). These findings suggest that to understand acoustically challenging speech, listeners need to engage domain-general cognitive resources during perception. In a limited capacity cognitive system (<xref ref-type="bibr" rid="bib65">Wingfield, 2016</xref>), such recruitment necessarily reduces the resources available for other tasks, including memory encoding. Importantly, even speech presented in quiet (i.e., without background noise) is degraded by the time it reaches the auditory system of a listener with a CI.</p><p>Cognitive demands during speech understanding are supported by several brain networks that supplement classic frontotemporal language regions. The cingulo-opercular network, for example, is engaged during particularly challenging speech (<xref ref-type="bibr" rid="bib13">Eckert et al., 2009</xref>; <xref ref-type="bibr" rid="bib60">Vaden et al., 2017</xref>) and supports successful comprehension during difficult listening (<xref ref-type="bibr" rid="bib59">Vaden et al., 2013</xref>). Recruitment of prefrontal cortex (PFC) complements that in the cingulo-opercular network and varies parametrically with speech intelligibility (<xref ref-type="bibr" rid="bib8">Davis and Johnsrude, 2003</xref>). Activity in PFC, particularly dorsolateral regions, is associated with cognitive demands in a wide range of tasks (<xref ref-type="bibr" rid="bib11">Duncan, 2010</xref>), consistent with domain-general cognitive control (<xref ref-type="bibr" rid="bib5">Braver, 2012</xref>). We thus hypothesized that listeners with CIs would rely more on PFC during listening than listeners with normal hearing, and in particular regions of PFC associated with non-linguistic tasks. However, the functional anatomy of PFC is also complex (<xref ref-type="bibr" rid="bib32">Noyce et al., 2017</xref>), and dissociating nearby language and domain-general processing regions is challenging (<xref ref-type="bibr" rid="bib17">Fedorenko et al., 2012</xref>).</p><p>A central question concerns the degree to which listeners with CIs rely on cognitive processing outside core speech regions, such as dorsolateral PFC. Obtaining precise spatially localized images of regional brain activity has been difficult in listeners with CIs, given that functional MRI (fMRI) is not possible (or subject to artifact) due to the CI hardware. Thus, optical brain imaging (<xref ref-type="bibr" rid="bib34">Peelle, 2017</xref>) has become a method of choice for studying functional activity in CI listeners (<xref ref-type="bibr" rid="bib1">Anderson et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Lawler et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Lawrence et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Olds et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Zhou et al., 2018</xref>). In the current study, we use high-density diffuse optical tomography (HD-DOT) (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib68">Zeff et al., 2007</xref>), previously validated in speech studies in listeners with normal hearing (<xref ref-type="bibr" rid="bib25">Hassanpour et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Hassanpour et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Schroeder et al., 2020</xref>). HD-DOT provides high spatial resolution and homogenous sensitivity over a field of view that captures known speech-related brain regions (<xref ref-type="bibr" rid="bib62">White and Culver, 2010</xref>). We examine the brain regions supporting single word processing in listeners with a right unilateral CI relative to that in a group of matched, normal-hearing controls. We hypothesized that listeners with CIs would exhibit greater recruitment of PFC compared to normal-hearing controls.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Multi-session single-subject results</title><p>Due to the variability across CI users and difficulties in defining single-subject regions of interest (ROIs), we performed a small multi-session study from one CI subject for six sessions (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We collected two runs of spoken word perception per session (for six sessions) and one run of spatial working memory task per session (for four sessions). This multi-session analysis enabled localizing the left and right PFC based on the non-verbal spatial working memory task for this subject (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). It also revealed the engagement of regions beyond the auditory cortex, including PFC, during the word perception task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Time-traces of oxyhemoglobin concentration change show a clear event-related response for four selected regions in the word perception results.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Single-subject data from one cochlear implant (CI) user across multiple sessions.</title><p>(<bold>A</bold>) A CI user wearing the high-density diffuse optical tomography (HD-DOT) cap. (<bold>B</bold>) Response to the spoken words across six sessions (36 min of data). Hemodynamic response time-traces are plotted for peak activation values across six sessions for four brain regions. The seed colors match the plot boundaries with error bars indicating the standard error of the mean over n=12 runs of data. Gray shaded region indicates period during which words are presented. (<bold>C</bold>) Response to the spatial working memory task for the same CI user across four sessions (32 min of data).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Mapping the brain response to spoken words</title><p>We first investigated the degree of auditory activation in both control and CI groups by assessing the activity in a block design single word presentation task. We found strong bilateral superior temporal gyrus (STG) activations in controls similar to our previous studies using the same paradigm (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib51">Sherafati et al., 2020</xref>), as well as a strong left STG and a reduced right STG activation for the CI users (<xref ref-type="fig" rid="fig2">Figure 2A–B</xref>). In addition, we observed strong left-lateralized activations in regions beyond the auditory cortex, including parts of the PFC, in the CI group (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Spoken word recognition group maps.</title><p>Response to the spoken words in (<bold>A</bold>) 18 controls and (<bold>B</bold>) 20 right ear cochlear implant (CI) users. (<bold>C</bold>) Differential activation in response to the spoken words task in CIs&gt;controls highlights the group differences. The first column shows unthresholded β maps and the second column shows t-statistic maps thresholded at voxelwise p&lt;0.05 (uncorrected) for each group.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Group results for different hemoglobin contrasts.</title><p>β maps of oxy (column 1, HbO), deoxy (column 2, HbR), and total (column 3, HbT) hemoglobin for the spoken word recognition task for (<bold>A</bold>) all controls, (<bold>B</bold>) all cochlear implant (CI) users, (<bold>C</bold>) CIs&gt;controls, and (<bold>D</bold>) Controls&gt;CIs. Note that the first three rows in column 1 (HbO) are identical to the results shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> column 1. The redundancy is to guide the reader for side-by-side comparisons with HbR and HbT results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig2-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> provides the β maps of oxyhemoglobin (HbO), deoxyhemoglobin (HbR), and total hemoglobin (HbT) for controls (panel A), CI users (panel B), CIs&gt;controls (panel C), and controls&gt;CIs (panel D).</p><p>For statistical analysis, we focused on our predefined ROIs, as shown in Figure 6. <xref ref-type="fig" rid="fig3">Figure 3A-C</xref> shows unthresholded t-maps, masked by our ROIs. We averaged β values within each ROI, and statistically tested for group differences correcting for multiple comparisons across the three ROIs (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The temporal profile of the hemodynamic response in three selected ROIs also suggests the increased activity in the left PFC in the CI users relative to controls, and a decreased activity in both left and right auditory regions. Two sample t-statistics for the mean β values in each ROI support a statistically significant difference between the control and CI groups in left PFC, t(27) = 2.3, p=0.015 (one-tailed) and right auditory cortex, t(23) = 3.54, p=0.0017 (two-tailed) (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The observed change in the left auditory cortex was not statistically significant, t(36) = 1.46, p=0.15 (two-tailed). The threshold for statistical significance, Bonferroni corrected for multiple comparisons across three ROI analyses, was 0.016.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Region of interest (ROI)-based statistical analysis for spoken word recognition task.</title><p>Unthresholded t-maps in response to the spoken words spatially masked in the three ROIs for (<bold>A</bold>) 18 controls, (<bold>B</bold>) 20 right ear cochlear implant (CI) users, and (<bold>C</bold>) CIs&gt;controls, highlight the group differences in certain brain areas. (<bold>D</bold>) Temporal profile of the hemodynamic response in three selected ROIs (left prefrontal cortex [PFC], left auditory, and right auditory cortices). The error bars indicate the standard error of the mean over n=20 for CI users and n=18 for controls. Two-sample t-tests for mean β value in each ROI have been calculated between controls and the CI user group, confirming a significant increase in left PFC (p=0.015) and a significant decrease in the right auditory cortex (p=0.0017) in CI users (indicated by an asterisk above their corresponding box plots, corrected for multiple comparisons). The observed change in the left auditory cortex was not significant (p=0.15).</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Noisy source-detector numbers are provided along with the threshold used for identifying them for each cochlear implant (CI) user.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-75323-fig3-data1-v2.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Coupling coefficients of each source and detector is shown in a flat view for all cochlear implant (CI) users included in the study.</title><p>The dark red areas show the location of the signal drop for each CI user. The maximum coupling coefficient for each subject is shown above each plot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Coupling coefficients of each source and detector is shown in a flat view for all controls included in the study.</title><p>The maximum coupling coefficient for each subject is shown above each plot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Effect of the simulated cochlear implant (CI) transducer in controls in the right auditory region of interest (ROI) analysis.</title><p>(<bold>A</bold>) Replicating the ROI analysis for the right auditory cortex using 100 different shuffles of pre-processing for 18 controls by simulating the effect of the CI transducer. The error bars indicate the standard error of the mean over n=20 for CI users and n=18 for controls. (<bold>B</bold>) Histogram of the p-values shows statistical significance (p&lt;0.016 corrected) for all 100 shuffles across CI users and controls as found in the original pre-processing in the right auditory ROI in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig3-figsupp3-v2.tif"/></fig></fig-group><p>To further investigate whether the significant decrease in the right auditory cortex might be due to lower light level values around the CI transducer, we first performed a simulation of the HD-DOT sensitivity profile by blocking the optodes around the CI transducer. We found minimal overlap between the CI-related signal loss and the right auditory ROI (Appendix 2). We also performed an additional simulation analysis by first calculating the mean value of optical light power for each measurement. Then, for each source-detector location, we defined the approximate coupling coefficient as the mean over the first nearest neighbor measurements that have that source or detector in common. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s2">2</xref> show the coupling coefficients of each source and detector for all CI users and controls in the flat view, corrected by the average sensitivity of the avalanche photodiode detectors (APDs) plotted on a log base 10 scale to create linearly distributed color scale. We defined noisy optodes in the CI cohort as ones that had a coupling coefficient of 30% of maximum or lower based on the values for each participant across their two speech runs. If no optode was identified using this threshold, we lowered the threshold by 10% increments until at least one source or detector was identified (under the logic that the CI transducer must block some light). <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref> shows the source-detector numbers and the threshold used for identifying them for each CI user. For controls, no source or detector had a coupling coefficient of less than 30% of the maximum.</p><p>After identifying the noisy source-detector numbers for all 20 CI users, we pre-processed the data for 18 controls 100 times. For each of these analyses, we blocked the identified sources and detectors in a selected CI user for one control, assigned by a random permutation, effectively simulating the CI-based dropout in our control participants. Then, we replicated the ROI analysis for the right auditory cortex in <xref ref-type="fig" rid="fig3">Figure 3</xref>, far right panel, and found that in all 100 shuffles, the right auditory cortex in control subjects still had a significantly larger activation compared to the CI users (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p></sec><sec id="s2-3"><title>Behavioral measures</title><p>An important consideration in studying CI users is the variability in their speech perception abilities, hearing thresholds, and the relationship with brain activity. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows exploratory analyses between the magnitude of the activation in the left PFC ROI for the CI cohort with respect to the speech perception score, left ear hearing threshold unaided, left ear hearing threshold at test (aided if the subject used a hearing aid), and right ear hearing threshold (CI-aided).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Relationship between the magnitude of activation in left prefrontal cortex (PFC) and behavioral scores in cochlear implant (CI) users.</title><p>Plots of the Pearson correlation r between the magnitude of the mean β value in the left PFC region of interest (ROI) are shown with respect to speech perception score, left ear hearing threshold unaided, left ear hearing threshold (aided if the subject used a hearing aid), and right ear CI-aided hearing threshold. Hearing threshold was defined as four-frequency pure-tone average (4fPTA) at four frequencies, 500, 1000, 2000, and 4000 Hz.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Audiograms for left and right ears.</title><p>Individual subject (thin lines) and group mean (bold lines) hearing at 250, 500, 1000, 2000, 3000, 4000, and 6000 Hz for (<bold>A</bold>) left ear unaided, (<bold>B</bold>) left ear aided, and (<bold>C</bold>) right ear (unaided for controls and with a cochlear implant [CI] for CI users).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig4-figsupp1-v2.tif"/></fig></fig-group><p>Using p&lt;0.05 (uncorrected) as a statistical threshold, left PFC activation positively correlated with left ear unaided thresholds (p=0.01, Pearson r=0.55) and negatively correlated with right ear CI-aided thresholds (p=0.02, Pearson r=–0.49). Left PFC activation did not correlate with speech perception score (p=0.4, Pearson r=0.17) and aided hearing threshold for the left ear (p=0.1, Pearson r=0.36). <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows the hearing thresholds (audiograms) for left and right ears for each control and CI participant. These scores were unavailable for one control participant, who was thus not included in these analyses.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using high-density optical brain imaging, we examined the brain networks supporting spoken word recognition in listeners with CIs relative to a matched group of controls with bilateral normal hearing. We found that relative to controls, when listening to words in quiet, listeners with CIs showed reduced activity in the right auditory cortex and—critically—increased activity in left PFC. We review these two findings in turn below.</p><sec id="s3-1"><title>Increased PFC activity in CI users</title><p>When listening to spoken words in quiet, listeners with normal hearing typically engage the left and right superior temporal cortex, including primary and secondary auditory regions (<xref ref-type="bibr" rid="bib2">Binder et al., 2000</xref>; <xref ref-type="bibr" rid="bib41">Price et al., 1992</xref>; <xref ref-type="bibr" rid="bib44">Rogers et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Wiggins et al., 2016</xref>). Our current results for controls show this same pattern. However, when listeners with CIs performed the same task, we found that they also engaged left PFC significantly more than the controls.</p><p>Although we only tested a single level of speech difficulty (i.e., speech in quiet), prior studies have parametrically varied speech intelligibility and found intelligibility-dependent responses in the PFC. Use of several types of signal degradation (<xref ref-type="bibr" rid="bib8">Davis and Johnsrude, 2003</xref>) revealed a classic ‘inverted-U’ shape response in the PFC as a function of speech intelligibility, with activity increasing until the speech became very challenging and then tapering off. A similar pattern was reported in functional near-infrared spectroscopy (fNIRS) (<xref ref-type="bibr" rid="bib30">Lawrence et al., 2018</xref>).</p><p>A pervasive challenge for understanding the role of PFC in speech understanding is the close anatomical relationship of core language processing regions and domain-general regions of PFC (<xref ref-type="bibr" rid="bib17">Fedorenko et al., 2012</xref>). We attempted to add some degree of functional specificity to our interpretation by including a spatial working memory task presumed to strongly engage domain-general regions with minimal reliance on language processing (<xref ref-type="bibr" rid="bib11">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib66">Woolgar et al., 2015</xref>). Ideally, we would have used functional ROIs individually created for each subject. However, we were not convinced that our data for this task were sufficiently reliable at the single-subject level, as we only had one run per subject. Furthermore, we did not have spatial working memory task data for all subjects. Thus, our functional localization relies on group-average spatial working memory responses. The region we identified—centered in left inferior frontal sulcus—corresponds well to other investigations of non-language tasks (e.g., <xref ref-type="bibr" rid="bib11">Duncan, 2010</xref>), and supports our preferred interpretation of engagement of domain-general regions of dorsolateral PFC during listening. However, the ROI also extends into the dorsal portion of inferior frontal gyrus (IFG), and we cannot rule out the possibility that this frontal activity relates to increased language (as opposed to domain-general) processing.</p></sec><sec id="s3-2"><title>Reduced auditory cortical activity in CI users</title><p>We found reduced activity in the right auditory cortex in CI users relative to controls, which we attribute to differences in auditory stimulation. We limited our sample to CI listeners with unilateral right-sided implants but did not restrict left ear hearing. Most of our subjects with CIs had poor hearing in their left ears, which would result in reduced auditory information being passed to the contralateral (right) auditory cortex. This was as opposed to controls who had bilateral hearing. Prior fNIRS studies have also shown that activity in the superior temporal cortex corresponds with stimulation and comprehension (<xref ref-type="bibr" rid="bib33">Olds et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Zhou et al., 2018</xref>). We also leave open the possibility that the CI hardware in the right hemisphere interfered with the signal strength, although our simulation studies suggest this cannot fully explain the group differences (Appendix 2 and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>).</p><p>What is potentially more interesting is the hint at a lower level of activity in the left auditory cortex of the CI users compared to controls, even though all CI listeners were receiving adequate stimulation of their right auditory nerve with a right CI. There are several possible explanations for this finding. First, activity in superior temporal cortex does not reflect only ‘basic’ auditory stimulation, but processing related to speech sounds, word meaning, and other levels of linguistic analysis. Thus, although subjects with CIs were certainly receiving stimulation and speech intelligibility scores were generally good, some variability was still present (mean speech perception score in quiet = 0.88, SD = 0.09). The overall level of speech processing was significantly (p=0.00005) lower for CI users than controls (mean speech perception score = 0.99, SD = 0.01), resulting in decreased activity (indeed, because the depth of HD-DOT includes only about 1 cm of the brain, much of primary auditory cortex is not present in our field of view, and the observed group differences were localized in non-primary regions of STG and MTG).</p><p>Perhaps the most provocative explanation is that a reduction in top-down modulatory processes (<xref ref-type="bibr" rid="bib9">Davis and Johnsrude, 2007</xref>) plays out as reduced activity in the temporal cortex. That is, given that effortful listening depends on attention (<xref ref-type="bibr" rid="bib64">Wild et al., 2012</xref>), it might be that processes related to top-down prediction (<xref ref-type="bibr" rid="bib6">Cope et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Sohoglu et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Sohoglu et al., 2014</xref>) are muted when too much cognitive control is required for perceptual analysis. Reconciling this interpretation with predictive coding accounts of speech perception (<xref ref-type="bibr" rid="bib3">Blank and Davis, 2016</xref>; <xref ref-type="bibr" rid="bib55">Sohoglu and Davis, 2020</xref>) will require additional work. We emphasize that the group differences in left auditory regions were not significant, and thus our interpretation is speculative.</p></sec><sec id="s3-3"><title>Individual differences in PFC activation during spoken word recognition</title><p>Because of the variability of outcomes in CI users (<xref ref-type="bibr" rid="bib20">Firszt et al., 2004</xref>; <xref ref-type="bibr" rid="bib27">Holden et al., 2013</xref>), one promising thought is that individual differences in brain activation may help explain variability in speech perception ability. Although our study was not powered for individual difference analysis (<xref ref-type="bibr" rid="bib67">Yarkoni and Braver, 2010</xref>), we conducted exploratory correlations to investigate this avenue of inquiry. Interestingly, we saw a trend such that poorer hearing in the left (non-CI) ear was correlated with increased activity in PFC. Our subjects with CIs had significant variability in left ear hearing. Because the speech task was conducted using loudspeakers, we would expect both ears to contribute to accurate perception. Thus, poorer hearing in the left ear would create a greater acoustic challenge, with a correspondingly greater drain on cognitive resources. This interpretation will need additional data to be properly tested.</p></sec><sec id="s3-4"><title>Comparison with previous fNIRS studies in CI users</title><p>Due to limitations of using fMRI and EEG in studying CI users, fNIRS-based neuroimaging is an attractive method for studying the neural correlates of speech perception in this population (<xref ref-type="bibr" rid="bib25">Hassanpour et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Lawler et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Saliba et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Sevy et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Zhou et al., 2018</xref>). Some prior studies have looked at relationships of behavioral performance and brain activity in CI users. For example, <xref ref-type="bibr" rid="bib33">Olds et al., 2016</xref>, studied temporal lobe activity and showed that both normal hearing and CI users with good speech perception exhibited greater cortical activations to natural speech than to unintelligible speech. In contrast, CI users with poor speech perception had large, indistinguishable cortical activations to all stimuli. <xref ref-type="bibr" rid="bib69">Zhou et al., 2018</xref>, found that regions of temporal and frontal cortex had significantly different mean activation levels in response to auditory speech in CI listeners compared with normal-hearing listeners, and these activation levels were negatively correlated with CI users’ auditory speech understanding.</p><p>Our current study differs in that we were able to simultaneously measure responses in temporal and frontal lobes with a high-density array. Our finding of increased recruitment of left PFC in CI listeners is broadly consistent with the recruitment of PFC in fNIRS studies in normal-hearing participants listening to simulated distorted speech (<xref ref-type="bibr" rid="bib10">Defenderfer et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Lawrence et al., 2018</xref>). Potential differences in anatomical localization could reflect the difference between the type of the distortion created by an actual CI compared to simulated degraded speech listening scenarios in the previous studies, or in source localization accuracy across different imaging hardware. To our knowledge, our study is the first fNIRS-based study to use a non-verbal spatial working memory task for localizing the PFC ROI in the same population, and it may be that increased use of cross-domain functional localizers will prove to be a useful approach in future work.</p></sec><sec id="s3-5"><title>Caveats, considerations, and future directions</title><p>As with all fNIRS-based functional brain imaging, our ability to image neural activity is limited by the placement of sources and detectors, and in depth to approximately 1 cm of the cortex. Our HD-DOT cap covers the regions of superficial cortex commonly identified in fMRI studies of speech processing, including bilateral superior (STG) and middle temporal gyri (MTG), and pars triangularis of the left IFG (<xref ref-type="bibr" rid="bib8">Davis and Johnsrude, 2003</xref>; <xref ref-type="bibr" rid="bib43">Rodd et al., 2005</xref>; <xref ref-type="bibr" rid="bib44">Rogers et al., 2020</xref>; <xref ref-type="bibr" rid="bib64">Wild et al., 2012</xref>). We also have good coverage of occipital cortex and middle frontal gyrus. However, in the present study we do not image pars opercularis of the IFG, the cerebellum, or any subcortical structures. Thus, it is certainly possible that additional regions not identified here play different roles in understanding speech for CI listeners, a possibility that will require fNIRS setups with improved cortical coverage and converging evidence from other methods to explore.</p><p>We also note that some care should be taken in the degree to which we have identified PFC. As we have pointed out, the functional organization of the frontal cortex is complex and differs from person to person; in particular, fMRI studies have demonstrated that regions responding to language and non-language tasks lie nearby each other (<xref ref-type="bibr" rid="bib17">Fedorenko et al., 2012</xref>). Although we attempted to improve the interpretation of our results by using a functional localizer, we were not able to define subject-specific ROIs, which would be preferable. Future work with single-subject localizers and functional ROIs is needed to more clearly resolve this issue.</p><p>Cognitive demand during speech understanding frequently comes up in discussions of effortful listening (<xref ref-type="bibr" rid="bib37">Pichora-Fuller et al., 2016</xref>). Although understandable, it is important to keep in mind that the construct of listening effort is not clearly defined (<xref ref-type="bibr" rid="bib58">Strand et al., 2021</xref>) and different measures of ‘effort’ do not always agree with each other (<xref ref-type="bibr" rid="bib57">Strand et al., 2018</xref>). Here, we interpret increased activity in PFC as reflecting greater cognitive demand. Although we did not include an independent measure of cognitive challenge, we note that simulated CI speech (i.e., noise-vocoded speech) is associated with delayed word recognition (<xref ref-type="bibr" rid="bib31">McMurray et al., 2017</xref>) and poorer memory for what has been heard (<xref ref-type="bibr" rid="bib61">Ward et al., 2016</xref>), consistent with increased dependence on shared cognitive processes (<xref ref-type="bibr" rid="bib38">Piquado et al., 2010</xref>). Relating activity in PFC to cognitive demand is also consistent with <italic>decreased</italic> activity in PFC when speech becomes unintelligible (<xref ref-type="bibr" rid="bib8">Davis and Johnsrude, 2003</xref>).</p><p>Finally, we emphasize that in the current study we only measured responses to speech in quiet. Everyday communication frequently occurs in the presence of background noise, which can be particularly challenging for many listeners with CIs (<xref ref-type="bibr" rid="bib20">Firszt et al., 2004</xref>). Exploring how activity in PFC and other regions fluctuates in response to speech at various levels of background noise would be a highly interesting future extension of this work. Based on parametric modulations of acoustic challenge in listeners with normal hearing, we might expect increasing activity in PFC as speech gets more challenging (<xref ref-type="bibr" rid="bib8">Davis and Johnsrude, 2003</xref>), followed by cingulo-opercular activity once intelligibility is significantly hampered (<xref ref-type="bibr" rid="bib59">Vaden et al., 2013</xref>).</p><p>In summary, using high-density optical neuroimaging, we found increased activity in PFC in listeners with CIs compared to listeners with normal hearing while listening to words in quiet. Our findings are consistent with a greater reliance on domain-general cognitive processing and provide a potential framework for the cognitive effort that many CI users need to expend during speech perception.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>We recruited 21 adult CI patients aged 26–79 years (17 right-handed, 2 left-handed, 2 not available), and 19 age- and sex-matched controls (18 right-handed, 1 left-handed) (demographic information in <xref ref-type="table" rid="table1">Table 1</xref>). We excluded one CI user due to poor signal quality (evaluated as mean band limited signal-to-noise ratio (SNR) of all source-detectors) and one control due to excessive motion (see Appendix 1 for details). All CI patients had a unilateral right CI (manufacturer: 11 Cochlear, 6 Advanced Bionics, 3 Med-El). All subjects were native speakers of English with no self-reported history of neurological or psychiatric disorders. All aspects of these studies were approved by the Human Research Protection Office (HRPO) of the Washington University School of Medicine. Subjects were recruited from the Washington University campus and the surrounding community (IRB 201101896, IRB 201709126). All subjects gave informed consent and were compensated for their participation in accordance with institutional and national guidelines.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Demographic information.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Measure</th><th align="left" valign="top">Control</th><th align="left" valign="top">CI users</th></tr></thead><tbody><tr><td align="left" valign="top">Number of subjects (# of females)</td><td align="char" char="." valign="top">18 (11)</td><td align="char" char="." valign="top">20 (11)</td></tr><tr><td align="left" valign="top">Mean age at test in years (std)</td><td align="char" char="." valign="top">57.57 (12.74)</td><td align="char" char="." valign="top">56.80 (14.09)</td></tr><tr><td align="left" valign="top">Mean years of CI use (std)</td><td align="left" valign="top">–</td><td align="char" char="." valign="top">8.10 (6.51)</td></tr><tr><td align="left" valign="top">Mean speech perception score (AzBio sentences in quiet) (std)</td><td align="char" char="." valign="top">0.99 (0.01)</td><td align="char" char="." valign="top">0.88 (0.09)</td></tr><tr><td align="left" valign="top">Mean right ear 4fPTA (std)</td><td align="char" char="." valign="top">16.02 (6.74)</td><td align="char" char="." valign="top">21.85 (5.30) with CI on</td></tr><tr><td align="left" valign="top">Mean left ear 4fPTA (std)</td><td align="char" char="." valign="top">16.61 (7.67)</td><td align="char" char="." valign="top">91.25 (26.77) unaided</td></tr><tr><td align="left" valign="top">Mean left ear 4fPTA at test<xref ref-type="table-fn" rid="table1fn2"><sup>*</sup></xref><sup>,</sup><xref ref-type="table-fn" rid="table1fn3"><sup>†</sup></xref> (std)</td><td align="left" valign="top">–</td><td align="char" char="." valign="top">73.28 (37.72)</td></tr><tr><td align="left" valign="top">Mean duration of deafness right ear</td><td align="left" valign="top">–</td><td align="char" char="." valign="top">12.58 (11.74)</td></tr></tbody></table><table-wrap-foot><fn><p>If no response at a given frequency, a value of 120 dB HL was assigned.</p></fn><fn id="table1fn2"><label>*</label><p>With hearing aid, if the subject used amplification. Eight out of 20 CI (cochlear implant) users used hearing aids.</p></fn><fn id="table1fn3"><label>†</label><p>4fPTA (four-frequency pure-tone average), average pure tone threshold at four frequencies (500, 1000, 2000, 4000 Hz).</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4-2"><title>HD-DOT system</title><p>Data were collected using a continuous-wave HD-DOT system comprised of 96 sources (LEDs, at both 750 and 850 nm) and 92 detectors (coupled to APDs, Hamamatsu C5460-01) to enable HbO and HbR spectroscopy (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>). The design of this HD-DOT system provides more than 1200 usable source-detector measurements per wavelength at a 10 Hz full-field frame rate. This system has been validated for successfully mapping cortical responses to language and naturalistic stimuli with comparable sensitivity and specificity to fMRI (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Fishell et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Hassanpour et al., 2015</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>High-density diffuse optical tomography (HD-DOT) system and the experimental design.</title><p>(<bold>A</bold>) Schematic of a subject wearing the HD-DOT cap along with an illustration of the task design. (<bold>B</bold>) Simplified illustration of the HD-DOT system (far left), regional distribution of source-detector light levels (middle), and source-detector pair measurements (~1200 pairs) as gray solid lines illustrated in a flat view of the HD-DOT cap (far right). (<bold>C</bold>) An example point spread function (PSF) and the HD-DOT sensitivity profile illustrated on the volume and spatially registered on the cortical view of a standard atlas in lateral and posterior views.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig5-v2.tif"/></fig></sec><sec id="s4-3"><title>Experimental design</title><p>Subjects were seated on a comfortable chair in an acoustically isolated room facing an LCD screen located 76 cm from them, at approximately eye level. The auditory stimuli were presented through two speakers located approximately 150 cm away at about ±21° from the subjects’ ears at a sound level of approximately 65 dBA. Subjects were instructed to fixate on a white crosshair against a gray background while listening to the auditory stimuli, holding a keyboard on their lap for the stimuli that required their response (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left panel). The HD-DOT cap was fitted to the subject’s head to maximize optode-scalp coupling, assessed via real-time coupling coefficient readouts using an in-house software. The stimuli were presented using Psychophysics Toolbox 3 (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002881">SCR_002881</ext-link>) in MATLAB 2010b.</p><p>The spoken word recognition paradigm consisted of six blocks of spoken words per run. Each block contained 15 s of spoken words (one word per second), followed by 15 s of silence. Two runs were performed in each study session with a total of 180 words in about 6 min (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, middle panel). This task was first introduced by <xref ref-type="bibr" rid="bib36">Petersen et al., 1988</xref>, and subsequently replicated in other studies with the same HD-DOT system used in this study (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Fishell et al., 2019</xref>; <xref ref-type="bibr" rid="bib51">Sherafati et al., 2020</xref>). All sound files were generated from a list of short simple nouns (one to four syllables) pronounced with a female digital-voice audio using AT&amp;T Natural Voices text-to-speech generator with the ‘Lauren’ digital voice.</p><p>To better understand the left PFC activity we observed in our first few CI users, we adopted a spatial working memory task introduced in previous studies (<xref ref-type="bibr" rid="bib16">Fedorenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib18">Fedorenko et al., 2013</xref>) in the remaining subjects to aid in functionally localizing domain-general regions of PFC. In this spatial working memory task, subjects were asked to remember four locations (easy condition) or eight locations (hard condition) in a 3×4 grid, appearing one at a time. Following each trial, subjects had to choose the pattern they saw among two choices, one with correct and one with incorrect locations. This task requires keeping sequences of elements in memory for a brief period and has been shown to activate PFC. Each run for the spatial working memory task was about 8 min, with a total of 48 trials in the run (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right panel).</p></sec><sec id="s4-4"><title>Data processing</title><p>HD-DOT data were pre-processed using the NeuroDOT toolbox (<xref ref-type="bibr" rid="bib15">Eggebrecht and Culver, 2019</xref>). Source-detector pair light level measurements were converted to log-ratio by calculating the temporal mean of a given source-detector pair measurement as the baseline for that measurement. Noisy measurements were empirically defined as those that had greater than 7.5% temporal standard deviation in the least noisy (lowest mean motion) 60 s of each run (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib51">Sherafati et al., 2020</xref>). The data were next high pass filtered at 0.02 Hz. The global superficial signal was estimated as the average across the first nearest neighbor measurements (13 mm source-detector pair separation) and regressed from all measurement channels (<xref ref-type="bibr" rid="bib23">Gregg et al., 2010</xref>). The optical density time-traces were then low pass filtered with a cutoff frequency of 0.5 Hz to the physiological brain signal band and temporally downsampled from 10 to 1 Hz. A wavelength-dependent forward model of light propagation was computed using an anatomical atlas including the non-uniform tissue structures: scalp, skull, CSF, gray matter, and white matter (<xref ref-type="bibr" rid="bib19">Ferradal et al., 2014</xref>; <xref ref-type="fig" rid="fig5">Figure 5C</xref>). The sensitivity matrix was inverted to calculate relative changes in absorption at the two wavelengths via reconstruction using Tikhonov regularization and spatially variant regularization (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>). Relative changes in the concentrations of oxygenated, deoxygenated, and total hemoglobin (ΔHbO, HbR, ΔHbT) were then obtained from the absorption coefficient changes by the spectral decomposition of the extinction coefficients of oxygenated and deoxygenated hemoglobin at the two wavelengths (750 and 850 nm). After post-processing, we resampled all data to a 3 × 3 × 3 mm<sup>3</sup> standard atlas using a linear affine transformation for group analysis. In addition to the standard HD-DOT pre-processing steps used in the NeuroDOT toolbox, we used a comprehensive data quality assessment pipeline (see Appendix 1) to exclude the subjects with low pulse SNR or high motion levels.</p><p>After pre-processing, the response for the speech task was estimated using a standard general linear model (GLM) framework. The design matrix was constructed using onsets and durations of each stimulus presentation convolved with a canonical hemodynamic response function (HRF). This HRF was created using a two-gamma function (2 s delay time, 7 s time to peak, and 17 s undershoot) fitted to the HD-DOT data described in a previous study (<xref ref-type="bibr" rid="bib25">Hassanpour et al., 2015</xref>). We included both runs for each subject in one design matrix using custom MATLAB scripts (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>).</p><p>For modeling the spatial working memory task, we used a design matrix with two columns representing easy and hard conditions. The duration of each easy or hard trial was modeled as the total time of stimulus presentation and evaluation. Events were convolved with the same canonical HRF described in the spoken word perception task to model hemodynamic responses (<xref ref-type="bibr" rid="bib24">Hassanpour et al., 2014</xref>). We used the easy+hard response maps as a reference for defining the PFC ROI, which was more robust than the hard&gt;easy contrast previously used for younger populations.</p><p>After estimating the response (β map) for each subject for each task, we performed a second-level analysis in SPM12 (Wellcome Trust Centre for Neuroimaging) version 7487 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_007037">SCR_007037</ext-link>). Extracted time-traces for each subject were then calculated using a finite impulse response model.</p><p>We only present the ΔHbO results in the main figures as we have found that the ΔHbO signal exhibits a higher contrast-to-noise ratio compared to ΔHbR or ΔHbT (<xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Hassanpour et al., 2014</xref>).</p></sec><sec id="s4-5"><title>Functionally defined ROIs</title><p>To perform a more focused comparison between controls and CI users, we defined three ROIs, independent from our spoken word recognition dataset, for statistical analysis.</p><p>To accurately localize the elevated PFC activation in the CI group, we collected HD-DOT data from nine subjects (four controls and five CI users in 13 sessions) using a spatial working memory task (<xref ref-type="bibr" rid="bib17">Fedorenko et al., 2012</xref>). The visual spatial working memory task robustly activates PFC due to its working memory demands (and visual cortex because of its visual aspect). We chose this task to localize the PFC ROI for performing an ROI-based statistical analysis between controls and CI users. Our logic is that in prior work this task shows activity that dissociates from nearby language-related activity (<xref ref-type="bibr" rid="bib17">Fedorenko et al., 2012</xref>), and thus the region of PFC localized by this task is more likely to reflect domain-general processing (as opposed to language-specific processing). Our results show strong bilateral visual and PFC activations in response to this task (<xref ref-type="fig" rid="fig6">Figure 6A</xref> left). We then defined the left PFC ROI as the cluster of activation in the left PFC (<xref ref-type="fig" rid="fig6">Figure 6A</xref> right). This region was centered in inferior frontal sulcus, extending into both dorsal IFG and inferior MFG. The location is broadly consistent with domain-general (‘multiple demand’) activation (e.g., meta-analysis in <xref ref-type="bibr" rid="bib11">Duncan, 2010</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Defining functional regions of interest (ROIs).</title><p>(<bold>A</bold>) Spatial working memory activation for five CI users and four controls over 13 sessions. The prefrontal cortex (PFC) ROI was defined as the cluster of activation in the PFC region, after p&lt;0.05 (uncorrected) voxelwise thresholding. (<bold>B</bold>) Seed-based correlation map for a seed located in the left auditory cortex (left map). Right auditory ROI defined by masking the correlation map to include only the right hemisphere (right map). (<bold>C</bold>) Seed-based correlation map for a seed located in the right auditory cortex (left map). Left auditory ROI defined by masking the correlation map to include only the left hemisphere (right map).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-fig6-v2.tif"/></fig><p>To define the left and right auditory ROIs, we used fMRI resting state data from a previously published paper (<xref ref-type="bibr" rid="bib51">Sherafati et al., 2020</xref>) that was masked using the field of view of our HD-DOT system. We defined the left and right auditory ROIs by selecting a 5 mm radius seed in the contralateral hemisphere and finding the Pearson correlation between the time-series of the seed region with all other voxels in the field of view. Correlation maps in individuals were Fisher z-transformed and averaged across subjects (<xref ref-type="fig" rid="fig6">Figure 6B–C</xref>, left). Right/left auditory ROIs were defined by masking the correlation map to include only the right/left hemisphere (<xref ref-type="fig" rid="fig6">Figure 6B–C</xref> right). These ROIs extend well beyond primary auditory cortex, which is important to capture responses to words often observed in lateral regions of STG and MTG.</p></sec><sec id="s4-6"><title>Speech perception score</title><p>To measure auditory-only speech perception accuracy, we presented each participant with two lists of AzBio sentences in quiet (<xref ref-type="bibr" rid="bib56">Spahr et al., 2012</xref>) (except one participant who only heard one set). We calculated speech perception accuracy as the proportion of correctly repeated words across all sentences.</p></sec><sec id="s4-7"><title>Hearing threshold</title><p>We summarized hearing thresholds using a four-frequency pure-tone average (4fPTA), averaging thresholds at 500, 1000, 2000, and 4000 Hz. For unaided testing, a value of 120 dB was assigned if there was no response at a given frequency; for aided or CI testing, a value of 75 dB was used. For CI users, hearing was tested with the right ear CI alone, left ear alone unaided, and left ear with a hearing aid, if worn at the time of testing. <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows the audiograms for both controls and CI users.</p></sec><sec id="s4-8"><title>Quantification and statistical analysis</title><p>Based on prior optical neuroimaging studies with similar speech-related tasks (<xref ref-type="bibr" rid="bib10">Defenderfer et al., 2021</xref>; <xref ref-type="bibr" rid="bib14">Eggebrecht et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Hassanpour et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Pollonini et al., 2014</xref>; <xref ref-type="bibr" rid="bib69">Zhou et al., 2018</xref>), we anticipated 15–20 subjects per group would be sufficient to detect a moderate effect size while also within our resource limitations. We therefore aimed to have at least 15 subjects in each group. We performed a one-tailed two-sample t-test for our main hypothesis (increased recruitment of PFC in CI users), for which we had a strong directional hypothesis (greater activity in listeners with CIs than controls), and a two-tailed t-test for the left and right auditory cortex changes for which we did not have directional hypotheses. We adjusted for unequal variances for left PFC and right auditory ROIs based on the significance of Levene’s test.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Methodology, Software, Validation, Visualization, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology, Software, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Investigation, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Investigation, Methodology, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Investigation, Methodology, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Resources, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects were native speakers of English with no self-reported history of neurological or psychiatric disorders. All aspects of these studies were approved by the Human Research Protection Office (HRPO) of the Washington University School of Medicine. Subjects were recruited from the Washington University campus and the surrounding community (IRB 201101896, IRB 201709126). All subjects gave informed consent and were compensated for their participation in accordance with institutional and national guidelines.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-75323-transrepform1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Stimuli, data, and analysis scripts are available from <ext-link ext-link-type="uri" xlink:href="https://osf.io/nkb5v/">https://osf.io/nkb5v/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Sherafati</surname><given-names>A</given-names></name><name><surname>Bajracharya</surname><given-names>A</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Prefrontal cortex supports speech perception in listeners with cochlear implants</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/nkb5v/">https://osf.io/nkb5v/</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>AS would like to thank Abraham Z Snyder, Andrew K Fishell, Kalyan Tripathy, Karla M Bergonzi, Zachary E Markow, Tracy M Burns-Yocum, Mariel M Schroeder, Monalisa Munsi, Emily Miller, Timothy Holden, and Sarah McConkey for helpful discussions. We also want to thank our participants for their time and interest in our study.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>CA</given-names></name><name><surname>Wiggins</surname><given-names>IM</given-names></name><name><surname>Kitterick</surname><given-names>PT</given-names></name><name><surname>Hartley</surname><given-names>DEH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adaptive benefit of cross-modal plasticity following cochlear implantation in deaf adults</article-title><source>PNAS</source><volume>114</volume><fpage>10256</fpage><lpage>10261</lpage><pub-id pub-id-type="doi">10.1073/pnas.1704785114</pub-id><pub-id pub-id-type="pmid">28808014</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Frost</surname><given-names>JA</given-names></name><name><surname>Hammeke</surname><given-names>TA</given-names></name><name><surname>Bellgowan</surname><given-names>PSF</given-names></name><name><surname>Springer</surname><given-names>JA</given-names></name><name><surname>Kaufman</surname><given-names>JN</given-names></name><name><surname>Possing</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>512</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.5.512</pub-id><pub-id pub-id-type="pmid">10847601</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname><given-names>H</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction Errors but Not Sharpened Signals Simulate Multivoxel fMRI Patterns during Speech Perception</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002577</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002577</pub-id><pub-id pub-id-type="pmid">27846209</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897x00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The variable nature of cognitive control: a dual mechanisms framework</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.12.010</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname><given-names>TE</given-names></name><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Jones</surname><given-names>PS</given-names></name><name><surname>Wiggins</surname><given-names>J</given-names></name><name><surname>Dawson</surname><given-names>C</given-names></name><name><surname>Grube</surname><given-names>M</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Rowe</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence for causal top-down frontal contributions to predictive processes in speech perception</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>2154</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01958-7</pub-id><pub-id pub-id-type="pmid">29255275</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cousins</surname><given-names>KAQ</given-names></name><name><surname>Dar</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Acoustic masking disrupts time-dependent mechanisms of memory encoding in word-list recall</article-title><source>Memory &amp; Cognition</source><volume>42</volume><fpage>622</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.3758/s13421-013-0377-7</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Hierarchical processing in spoken language comprehension</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>3423</fpage><lpage>3431</lpage><pub-id pub-id-type="pmid">12716950</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hearing speech sounds: top-down influences on the interface between audition and speech perception</article-title><source>Hearing Research</source><volume>229</volume><fpage>132</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.01.014</pub-id><pub-id pub-id-type="pmid">17317056</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Defenderfer</surname><given-names>J</given-names></name><name><surname>Forbes</surname><given-names>S</given-names></name><name><surname>Wijeakumar</surname><given-names>S</given-names></name><name><surname>Hedrick</surname><given-names>M</given-names></name><name><surname>Plyler</surname><given-names>P</given-names></name><name><surname>Buss</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Frontotemporal activation differs between perception of simulated cochlear implant speech and speech in background noise: An image-based fNIRS study</article-title><source>NeuroImage</source><volume>240</volume><elocation-id>118385</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118385</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.004</pub-id><pub-id pub-id-type="pmid">20171926</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dwyer</surname><given-names>NY</given-names></name><name><surname>Firszt</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Effects of unilateral input and mode of hearing in the better ear: self-reported performance using the speech, spatial and qualities of hearing scale</article-title><source>Ear and Hearing</source><volume>35</volume><fpage>126</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3182a3648b</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckert</surname><given-names>MA</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name><name><surname>Walczak</surname><given-names>A</given-names></name><name><surname>Ahlstrom</surname><given-names>J</given-names></name><name><surname>Denslow</surname><given-names>S</given-names></name><name><surname>Horwitz</surname><given-names>A</given-names></name><name><surname>Dubno</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>At the heart of the ventral attention system: the right anterior insula</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>2530</fpage><lpage>2541</lpage><pub-id pub-id-type="doi">10.1002/hbm.20688</pub-id><pub-id pub-id-type="pmid">19072895</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Ferradal</surname><given-names>SL</given-names></name><name><surname>Robichaux-Viehoever</surname><given-names>A</given-names></name><name><surname>Hassanpour</surname><given-names>MS</given-names></name><name><surname>Dehghani</surname><given-names>H</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Hershey</surname><given-names>T</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping distributed brain function and networks with diffuse optical tomography</article-title><source>Nature Photonics</source><volume>8</volume><fpage>448</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1038/nphoton.2014.107</pub-id><pub-id pub-id-type="pmid">25083161</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>WUSTL-ORL/NeuroDOT_Beta</data-title><version designator="4423adf">4423adf</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/WUSTL-ORL/NeuroDOT_Beta">https://github.com/WUSTL-ORL/NeuroDOT_Beta</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Behr</surname><given-names>MK</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specificity for high-level linguistic processing in the human brain</article-title><source>PNAS</source><volume>108</volume><fpage>16428</fpage><lpage>16433</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id><pub-id pub-id-type="pmid">21885736</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Language-selective and domain-general regions lie side by side within Broca’s area</article-title><source>Current Biology</source><volume>22</volume><fpage>2059</fpage><lpage>2062</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.09.011</pub-id><pub-id pub-id-type="pmid">23063434</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Broad domain generality in focal regions of frontal and parietal cortex</article-title><source>PNAS</source><volume>110</volume><fpage>16616</fpage><lpage>16621</lpage><pub-id pub-id-type="doi">10.1073/pnas.1315235110</pub-id><pub-id pub-id-type="pmid">24062451</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferradal</surname><given-names>SL</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Hassanpour</surname><given-names>M</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Atlas-based head modeling and spatial normalization for high-density diffuse optical tomography: in vivo validation against fMRI</article-title><source>NeuroImage</source><volume>85</volume><fpage>117</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.069</pub-id><pub-id pub-id-type="pmid">23578579</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Firszt</surname><given-names>JB</given-names></name><name><surname>Holden</surname><given-names>LK</given-names></name><name><surname>Skinner</surname><given-names>MW</given-names></name><name><surname>Tobey</surname><given-names>EA</given-names></name><name><surname>Peterson</surname><given-names>A</given-names></name><name><surname>Gaggl</surname><given-names>W</given-names></name><name><surname>Runge-Samuelson</surname><given-names>CL</given-names></name><name><surname>Wackym</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Recognition of speech presented at soft to loud levels by adult cochlear implant recipients of three cochlear implant systems</article-title><source>Ear and Hearing</source><volume>25</volume><fpage>375</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1097/01.aud.0000134552.22205.ee</pub-id><pub-id pub-id-type="pmid">15292777</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fishell</surname><given-names>AK</given-names></name><name><surname>Burns-Yocum</surname><given-names>TM</given-names></name><name><surname>Bergonzi</surname><given-names>KM</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mapping brain function during naturalistic viewing using high-density diffuse optical tomography</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>11115</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-45555-8</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>C</given-names></name><name><surname>Goehring</surname><given-names>T</given-names></name><name><surname>Cosentino</surname><given-names>S</given-names></name><name><surname>Turner</surname><given-names>RE</given-names></name><name><surname>Deeks</surname><given-names>JM</given-names></name><name><surname>Brochier</surname><given-names>T</given-names></name><name><surname>Rughooputh</surname><given-names>T</given-names></name><name><surname>Bance</surname><given-names>M</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Panoramic ECAP Method: Estimating Patient-Specific Patterns of Current Spread and Neural Health in Cochlear Implant Users</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>22</volume><fpage>567</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1007/s10162-021-00795-2</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gregg</surname><given-names>NM</given-names></name><name><surname>White</surname><given-names>BR</given-names></name><name><surname>Zeff</surname><given-names>BW</given-names></name><name><surname>Berger</surname><given-names>AJ</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Brain specificity of diffuse optical imaging: improvements from superficial signal regression and tomography</article-title><source>Frontiers in Neuroenergetics</source><volume>2</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fnene.2010.00014</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassanpour</surname><given-names>MS</given-names></name><name><surname>White</surname><given-names>BR</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Ferradal</surname><given-names>SL</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Statistical analysis of high density diffuse optical tomography</article-title><source>NeuroImage</source><volume>85</volume><fpage>104</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.105</pub-id><pub-id pub-id-type="pmid">23732886</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassanpour</surname><given-names>MS</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping cortical responses to speech using high-density diffuse optical tomography</article-title><source>NeuroImage</source><volume>117</volume><fpage>319</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.058</pub-id><pub-id pub-id-type="pmid">26026816</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassanpour</surname><given-names>MS</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping effective connectivity within cortical networks with diffuse optical tomography</article-title><source>Neurophotonics</source><volume>4</volume><elocation-id>041402</elocation-id><pub-id pub-id-type="doi">10.1117/1.NPh.4.4.041402</pub-id><pub-id pub-id-type="pmid">28744475</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holden</surname><given-names>LK</given-names></name><name><surname>Finley</surname><given-names>CC</given-names></name><name><surname>Firszt</surname><given-names>JB</given-names></name><name><surname>Holden</surname><given-names>TA</given-names></name><name><surname>Brenner</surname><given-names>C</given-names></name><name><surname>Potts</surname><given-names>LG</given-names></name><name><surname>Gotter</surname><given-names>BD</given-names></name><name><surname>Vanderhoof</surname><given-names>SS</given-names></name><name><surname>Mispagel</surname><given-names>K</given-names></name><name><surname>Heydebrand</surname><given-names>G</given-names></name><name><surname>Skinner</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Factors Affecting Open-Set Word Recognition in Adults With Cochlear Implants</article-title><source>Ear &amp; Hearing</source><volume>34</volume><fpage>342</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3182741aa7</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koeritzer</surname><given-names>MA</given-names></name><name><surname>Rogers</surname><given-names>CS</given-names></name><name><surname>Van Engen</surname><given-names>KJ</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Impact of Age, Background Noise, Semantic Ambiguity, and Hearing Loss on Recognition Memory for Spoken Sentences</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>61</volume><fpage>740</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1044/2017_JSLHR-H-17-0077</pub-id><pub-id pub-id-type="pmid">29450493</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawler</surname><given-names>CA</given-names></name><name><surname>Wiggins</surname><given-names>IM</given-names></name><name><surname>Dewey</surname><given-names>RS</given-names></name><name><surname>Hartley</surname><given-names>DEH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The use of functional near-infrared spectroscopy for measuring cortical reorganisation in cochlear implant users: A possible predictor of variable speech outcomes?</article-title><source>Cochlear Implants International</source><volume>16</volume><fpage>S30</fpage><lpage>S32</lpage><pub-id pub-id-type="doi">10.1179/1467010014Z.000000000230</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>RJ</given-names></name><name><surname>Wiggins</surname><given-names>IM</given-names></name><name><surname>Anderson</surname><given-names>CA</given-names></name><name><surname>Davies-Thompson</surname><given-names>J</given-names></name><name><surname>Hartley</surname><given-names>DEH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical correlates of speech intelligibility measured using functional near-infrared spectroscopy (fNIRS</article-title><source>Hearing Research</source><volume>370</volume><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2018.09.005</pub-id><pub-id pub-id-type="pmid">30292959</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMurray</surname><given-names>B</given-names></name><name><surname>Farris-Trimble</surname><given-names>A</given-names></name><name><surname>Rigler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Waiting for lexical access: Cochlear implants or severely degraded input lead listeners to process speech less incrementally</article-title><source>Cognition</source><volume>169</volume><fpage>147</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2017.08.013</pub-id><pub-id pub-id-type="pmid">28917133</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noyce</surname><given-names>AL</given-names></name><name><surname>Cestero</surname><given-names>N</given-names></name><name><surname>Michalka</surname><given-names>SW</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Somers</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sensory-Biased and Multiple-Demand Processing in Human Lateral Frontal Cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8755</fpage><lpage>8766</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0660-17.2017</pub-id><pub-id pub-id-type="pmid">28821668</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olds</surname><given-names>C</given-names></name><name><surname>Pollonini</surname><given-names>L</given-names></name><name><surname>Abaya</surname><given-names>H</given-names></name><name><surname>Larky</surname><given-names>J</given-names></name><name><surname>Loy</surname><given-names>M</given-names></name><name><surname>Bortfeld</surname><given-names>H</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Oghalai</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical Activation Patterns Correlate with Speech Understanding After Cochlear Implantation</article-title><source>Ear and Hearing</source><volume>37</volume><fpage>e160</fpage><lpage>e172</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000258</pub-id><pub-id pub-id-type="pmid">26709749</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optical neuroimaging of spoken language</article-title><source>Language, Cognition and Neuroscience</source><volume>32</volume><fpage>847</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1080/23273798.2017.1290810</pub-id><pub-id pub-id-type="pmid">30555845</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Listening Effort: How the Cognitive Consequences of Acoustic Challenge Are Reflected in Brain and Behavior</article-title><source>Ear and Hearing</source><volume>39</volume><fpage>204</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000494</pub-id><pub-id pub-id-type="pmid">28938250</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Posner</surname><given-names>MI</given-names></name><name><surname>Mintun</surname><given-names>M</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Positron emission tomographic studies of the cortical anatomy of single-word processing</article-title><source>Nature</source><volume>331</volume><fpage>585</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/331585a0</pub-id><pub-id pub-id-type="pmid">3277066</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>MK</given-names></name><name><surname>Kramer</surname><given-names>SE</given-names></name><name><surname>Eckert</surname><given-names>MA</given-names></name><name><surname>Edwards</surname><given-names>B</given-names></name><name><surname>Hornsby</surname><given-names>BWY</given-names></name><name><surname>Humes</surname><given-names>LE</given-names></name><name><surname>Lemke</surname><given-names>U</given-names></name><name><surname>Lunner</surname><given-names>T</given-names></name><name><surname>Matthen</surname><given-names>M</given-names></name><name><surname>Mackersie</surname><given-names>CL</given-names></name><name><surname>Naylor</surname><given-names>G</given-names></name><name><surname>Phillips</surname><given-names>NA</given-names></name><name><surname>Richter</surname><given-names>M</given-names></name><name><surname>Rudner</surname><given-names>M</given-names></name><name><surname>Sommers</surname><given-names>MS</given-names></name><name><surname>Tremblay</surname><given-names>KL</given-names></name><name><surname>Wingfield</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hearing Impairment and Cognitive Energy: The Framework for Understanding Effortful Listening (FUEL</article-title><source>Ear and Hearing</source><volume>37</volume><fpage>5S</fpage><lpage>27S</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000312</pub-id><pub-id pub-id-type="pmid">27355771</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piquado</surname><given-names>T</given-names></name><name><surname>Cousins</surname><given-names>KAQ</given-names></name><name><surname>Wingfield</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Effects of degraded sensory input on memory for speech: Behavioral data and a test of biologically constrained computational models</article-title><source>Brain Research</source><volume>1365</volume><fpage>48</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2010.09.070</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollonini</surname><given-names>L</given-names></name><name><surname>Olds</surname><given-names>C</given-names></name><name><surname>Abaya</surname><given-names>H</given-names></name><name><surname>Bortfeld</surname><given-names>H</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Oghalai</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Auditory cortex activation to natural speech and simulated cochlear implant speech measured with functional near-infrared spectroscopy</article-title><source>Hearing Research</source><volume>309</volume><fpage>84</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.11.007</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Barnes</surname><given-names>KA</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion (vol 59, pg 2142, 2012</article-title><source>NeuroImage</source><volume>63</volume><elocation-id>999</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.069</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>C</given-names></name><name><surname>Wise</surname><given-names>R</given-names></name><name><surname>Ramsay</surname><given-names>S</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Howard</surname><given-names>D</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Frackowiak</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Regional response differences within the human auditory cortex when listening to words</article-title><source>Neuroscience Letters</source><volume>146</volume><fpage>179</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/0304-3940(92)90072-f</pub-id><pub-id pub-id-type="pmid">1491785</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabbitt</surname><given-names>PMA</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Channel-Capacity Intelligibility and Immediate Memory</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>20</volume><elocation-id>241</elocation-id><pub-id pub-id-type="doi">10.1080/14640746808400158</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodd</surname><given-names>JM</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The neural mechanisms of speech comprehension: fMRI studies of semantic ambiguity</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1261</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi009</pub-id><pub-id pub-id-type="pmid">15635062</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>CS</given-names></name><name><surname>Jones</surname><given-names>MS</given-names></name><name><surname>McConkey</surname><given-names>S</given-names></name><name><surname>Spehar</surname><given-names>B</given-names></name><name><surname>Van Engen</surname><given-names>KJ</given-names></name><name><surname>Sommers</surname><given-names>MS</given-names></name><name><surname>Peelle</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Age-Related Differences in Auditory Cortex Activity During Spoken Word Recognition</article-title><source>Neurobiology of Language (Cambridge, Mass.)</source><volume>1</volume><fpage>452</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1162/nol_a_00021</pub-id><pub-id pub-id-type="pmid">34327333</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saliba</surname><given-names>J</given-names></name><name><surname>Bortfeld</surname><given-names>H</given-names></name><name><surname>Levitin</surname><given-names>DJ</given-names></name><name><surname>Oghalai</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional near-infrared spectroscopy for neuroimaging in cochlear implant recipients</article-title><source>Hearing Research</source><volume>338</volume><fpage>64</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2016.02.005</pub-id><pub-id pub-id-type="pmid">26883143</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>ML</given-names></name><name><surname>Sherafati</surname><given-names>A</given-names></name><name><surname>Ulbrich</surname><given-names>RL</given-names></name><name><surname>Fishell</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mapping Cortical Activations Underlying Naturalistic Language Generation Without Motion Censoring Using HD-DOT</article-title><source>Optical Tomography and Spectroscopy</source><volume>2020</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1364/OTS.2020.STu2D.6</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sevy</surname><given-names>ABG</given-names></name><name><surname>Bortfeld</surname><given-names>H</given-names></name><name><surname>Huppert</surname><given-names>TJ</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Tonini</surname><given-names>RE</given-names></name><name><surname>Oghalai</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuroimaging with near-infrared spectroscopy demonstrates speech-evoked activity in the auditory cortex of deaf children following cochlear implantation</article-title><source>Hearing Research</source><volume>270</volume><fpage>39</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2010.09.010</pub-id><pub-id pub-id-type="pmid">20888894</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sherafati</surname><given-names>A</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Burns-Yocum</surname><given-names>TM</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A global metric to detect motion artifacts in optical neuroimaging data (Conference Presentation</article-title><conf-name>In Neural Imaging and Sensing</conf-name></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sherafati</surname><given-names>A</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Bergonzi</surname><given-names>KM</given-names></name><name><surname>Burns-Yocum</surname><given-names>TM</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Improvements in Functional Diffuse Optical Tomography Maps by Global Motion Censoring Techniques</article-title><conf-name>Clinical and Translational Biophotonics</conf-name><pub-id pub-id-type="doi">10.1364/TRANSLATIONAL.2018.JW3A.51</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sherafati</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Separating Signal from Noise in High-Density Diffuse Optical Tomography</source><publisher-name>Washington University in St. Louis</publisher-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherafati</surname><given-names>A</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Eggebrecht</surname><given-names>AT</given-names></name><name><surname>Bergonzi</surname><given-names>KM</given-names></name><name><surname>Burns-Yocum</surname><given-names>TM</given-names></name><name><surname>Lugar</surname><given-names>HM</given-names></name><name><surname>Ferradal</surname><given-names>SL</given-names></name><name><surname>Robichaux-Viehoever</surname><given-names>A</given-names></name><name><surname>Smyser</surname><given-names>CD</given-names></name><name><surname>Palanca</surname><given-names>BJ</given-names></name><name><surname>Hershey</surname><given-names>T</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Global motion detection and censoring in high-density diffuse optical tomography</article-title><source>Human Brain Mapping</source><volume>41</volume><fpage>4093</fpage><lpage>4112</lpage><pub-id pub-id-type="doi">10.1002/hbm.25111</pub-id><pub-id pub-id-type="pmid">32648643</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smyser</surname><given-names>CD</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Neil</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional connectivity MRI in infants: exploration of the functional organization of the developing brain</article-title><source>NeuroImage</source><volume>56</volume><fpage>1437</fpage><lpage>1452</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.02.073</pub-id><pub-id pub-id-type="pmid">21376813</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive top-down integration of prior knowledge during speech perception</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>8443</fpage><lpage>8453</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5069-11.2012</pub-id><pub-id pub-id-type="pmid">22723684</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Top-down influences of written text on perceived clarity of degraded speech</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>40</volume><fpage>186</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1037/a0033206</pub-id><pub-id pub-id-type="pmid">23750966</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rapid computations of spectrotemporal prediction error support perception of degraded speech</article-title><source>eLife</source><volume>9</volume><elocation-id>e58077</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58077</pub-id><pub-id pub-id-type="pmid">33147138</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spahr</surname><given-names>AJ</given-names></name><name><surname>Dorman</surname><given-names>MF</given-names></name><name><surname>Litvak</surname><given-names>LM</given-names></name><name><surname>Van Wie</surname><given-names>S</given-names></name><name><surname>Gifford</surname><given-names>RH</given-names></name><name><surname>Loizou</surname><given-names>PC</given-names></name><name><surname>Loiselle</surname><given-names>LM</given-names></name><name><surname>Oakes</surname><given-names>T</given-names></name><name><surname>Cook</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Development and Validation of the AzBio Sentence Lists</article-title><source>Ear &amp; Hearing</source><volume>33</volume><fpage>112</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e31822c2549</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strand</surname><given-names>JF</given-names></name><name><surname>Brown</surname><given-names>VA</given-names></name><name><surname>Merchant</surname><given-names>MB</given-names></name><name><surname>Brown</surname><given-names>HE</given-names></name><name><surname>Smith</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Measuring Listening Effort: Convergent Validity, Sensitivity, and Links With Cognitive and Personality Measures</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>61</volume><fpage>1463</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1044/2018_JSLHR-H-17-0257</pub-id><pub-id pub-id-type="pmid">29800081</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strand</surname><given-names>JF</given-names></name><name><surname>Ray</surname><given-names>L</given-names></name><name><surname>Dillman-Hasso</surname><given-names>NH</given-names></name><name><surname>Villanueva</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Understanding Speech amid the Jingle and Jangle: Recommendations for Improving Measurement Practices in Listening Effort Research</article-title><source>Auditory Perception &amp; Cognition</source><volume>3</volume><fpage>169</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1080/25742442.2021.1903293</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaden</surname><given-names>KI</given-names></name><name><surname>Kuchinsky</surname><given-names>SE</given-names></name><name><surname>Cute</surname><given-names>SL</given-names></name><name><surname>Ahlstrom</surname><given-names>JB</given-names></name><name><surname>Dubno</surname><given-names>JR</given-names></name><name><surname>Eckert</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The cingulo-opercular network provides word-recognition benefit</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18979</fpage><lpage>18986</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1417-13.2013</pub-id><pub-id pub-id-type="pmid">24285902</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaden</surname><given-names>KI</given-names></name><name><surname>Teubner-Rhodes</surname><given-names>S</given-names></name><name><surname>Ahlstrom</surname><given-names>JB</given-names></name><name><surname>Dubno</surname><given-names>JR</given-names></name><name><surname>Eckert</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cingulo-opercular activity affects incidental memory encoding for speech in noise</article-title><source>NeuroImage</source><volume>157</volume><fpage>381</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.028</pub-id><pub-id pub-id-type="pmid">28624645</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>CM</given-names></name><name><surname>Rogers</surname><given-names>CS</given-names></name><name><surname>Van Engen</surname><given-names>KJ</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Effects of Age, Acoustic Challenge, and Verbal Working Memory on Recall of Narrative Speech</article-title><source>Experimental Aging Research</source><volume>42</volume><fpage>97</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1080/0361073X.2016.1108785</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>BR</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Quantitative evaluation of high-density diffuse optical tomography: in vivo resolution and mapping performance</article-title><source>Journal of Biomedical Optics</source><volume>15</volume><elocation-id>026006</elocation-id><pub-id pub-id-type="doi">10.1117/1.3368999</pub-id><pub-id pub-id-type="pmid">20459251</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiggins</surname><given-names>IM</given-names></name><name><surname>Anderson</surname><given-names>CA</given-names></name><name><surname>Kitterick</surname><given-names>PT</given-names></name><name><surname>Hartley</surname><given-names>DEH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Speech-evoked activation in adult temporal cortex measured using functional near-infrared spectroscopy (fNIRS): Are the measurements reliable?</article-title><source>Hearing Research</source><volume>339</volume><fpage>142</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2016.07.007</pub-id><pub-id pub-id-type="pmid">27451015</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wild</surname><given-names>CJ</given-names></name><name><surname>Yusuf</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>DE</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effortful listening: the processing of degraded speech depends critically on attention</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>14010</fpage><lpage>14021</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1528-12.2012</pub-id><pub-id pub-id-type="pmid">23035108</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wingfield</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evolution of Models of Working Memory and Cognitive Resources</article-title><source>Ear and Hearing</source><volume>37</volume><fpage>35S</fpage><lpage>43S</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000310</pub-id><pub-id pub-id-type="pmid">27355768</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Jackson</surname><given-names>J</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>How Domain General Is Information Coding in the Brain. A Meta-Analysis Of</source><publisher-name>Department of Cognitive Science</publisher-name><pub-id pub-id-type="doi">10.3389/conf.fnhum.2015.217.00350</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Cognitive neuroscience approaches to individual differences in working memory and executive control: conceptual and methodological issues</chapter-title><person-group person-group-type="editor"><name><surname>Gruszka</surname><given-names>A</given-names></name></person-group><source>In Handbook of Individual Differences in Cognition</source><publisher-name>Springer</publisher-name><fpage>87</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-1210-7</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeff</surname><given-names>BW</given-names></name><name><surname>White</surname><given-names>BR</given-names></name><name><surname>Dehghani</surname><given-names>H</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Culver</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Retinotopic mapping of adult human visual cortex with high-density diffuse optical tomography</article-title><source>PNAS</source><volume>104</volume><fpage>12169</fpage><lpage>12174</lpage><pub-id pub-id-type="doi">10.1073/pnas.0611266104</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Seghouane</surname><given-names>AK</given-names></name><name><surname>Shah</surname><given-names>A</given-names></name><name><surname>Innes-Brown</surname><given-names>H</given-names></name><name><surname>Cross</surname><given-names>W</given-names></name><name><surname>Litovsky</surname><given-names>R</given-names></name><name><surname>McKay</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical Speech Processing in Postlingually Deaf Adult Cochlear Implant Users, as Revealed by Functional Near-Infrared Spectroscopy</article-title><source>Trends in Hearing</source><volume>22</volume><elocation-id>2331216518786850</elocation-id><pub-id pub-id-type="doi">10.1177/2331216518786850</pub-id><pub-id pub-id-type="pmid">30022732</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Data quality assessment steps</title><sec sec-type="appendix" id="s8-1"><title>Motion artifact detection</title><p>Motion artifacts were detected in a time-point by time-point manner on the 10 Hz bandpass filtered first nearest neighbor (13 mm) measurements, using a previously described motion detection method in HD-DOT, the global variance of the temporal derivatives (GVTD) (<xref ref-type="bibr" rid="bib49">Sherafati et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Sherafati et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Sherafati et al., 2020</xref>).</p><p>GVTD, similar to DVARS (derivative of variance) in fMRI (<xref ref-type="bibr" rid="bib40">Power et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">Smyser et al., 2011</xref>), is a vector <inline-formula><mml:math id="inf1"><mml:mi>g</mml:mi></mml:math></inline-formula> that is defined as the RMS of the temporal derivatives <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> across a set of optical density measurements.<disp-formula id="equ1">,<label> (1)</label><mml:math id="m1"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>N is the number of first nearest neighbor measurements, and M is the number of time-points. GVTD indexes the global instantaneous change in the time-traces. Higher GVTD values indicate high motion levels. We defined the motion criterion (g<sub>thresh</sub>) (red line in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and 4) based on the GVTD distribution mode (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>κ</mml:mi><mml:mo>∼</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) plus a constant c = 10 times the standard deviation computed on the left (low) side of the mode (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mi>κ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> shows two example GVTD time-traces for a low-motion and a high-motion run. The GVTD threshold was calculated as <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mover><mml:mi>κ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and is shown as a red line.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Examples of global variance of the temporal derivative (GVTD) time-traces for a low-motion and a high-motion spoken word recognition high-density diffuse optical tomography (HD-DOT) data.</title><p>The red lines indicate the GVTD threshold of <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>κ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of each run.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-app1-fig1-v2.tif"/></fig></sec></sec><sec sec-type="appendix" id="s9"><title>Including motion regressors in the design matrix</title><p>After determining the time-points that passed the GVTD threshold, we then included a motion regressor as a column for each time-point that passed the motion threshold with one for the noisy time-point, and zeros for every other point in the design matrix, a method commonly known as one-hot encoding. An example design matrix including two runs in a session of spoken word recognition task along with their motion regressors are shown in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Including motion regressors in the design matrix.</title><p>(<bold>A</bold>) An example design matrix for the general linear model (GLM) for the spoken word recognition task, including a constant column (cst), the task times for run1, one-hot encoding columns for time-points passing the global variance of the temporal derivative (GVTD) threshold for run 1, task times for run2, and one-hot encoding columns for time-points passing the GVTD threshold for run 2. (<bold>B</bold>) The GVTD time-traces and the temporal masks for excluding the time-points passing the GVTD threshold are shown in (<bold>B</bold>) for run 1 and (<bold>C</bold>) for run 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-app1-fig2-v2.tif"/></fig></sec><sec sec-type="appendix" id="s10"><title>Heartbeat (pulse) SNR across HD-DOT field of view</title><p>One indicator of detecting physiological signal in optical neuroimaging is the detection of heartbeat frequency (pulse) (<xref ref-type="bibr" rid="bib50">Sherafati, 2020</xref>). If the data has a very low heartbeat SNR it indicates that the lower frequencies emergent from the hemodynamic fluctuations might also have a low SNR. Therefore, we excluded the subjects that showed a very low mean pulse SNR across the field of view, as it indicates that their brain signal is also less reliable.</p><p>The underlying reason behind low pulse SNR is either a poor optode-scalp coupling or high levels of motion. If the optode-scalp coupling is good, normally pulse SNR is only low during high-motion (identified as high GVTD) epochs of data. However, if the optode-scalp coupling is poor, we will see low pulse SNR throughout the run. Note that there are not well-studied definitions and cutoffs for the calculation of pulse SNR in fNIRS-based methods. Therefore, since we already regressed time-points contaminated with motion using the GVTD index, here we only exclude the subjects that had a very low mean pulse SNR across the cap compared to other subjects.</p><p>We calculated the pulse SNR using the NeuroDOT function PlotPhysiologyPower.m (<ext-link ext-link-type="uri" xlink:href="https://github.com/WUSTL-ORL/NeuroDOT_Beta">https://github.com/WUSTL-ORL/NeuroDOT_Beta</ext-link>; <xref ref-type="bibr" rid="bib15">Eggebrecht and Culver, 2019</xref>). This function first calculates the fast Fourier transform (FFT) of the optical density signal (log mean ratio). Then, it calculates the pulse SNR based on the ratio of the band-limited pulse power ~1 Hz (P) and the noise floor (N).</p><p>The band-limited pulse power (P) is defined as the sum of the squares of the FFT magnitudes of a small frequency band around the peak of FFT magnitudes in the 0.5–2 Hz frequency bandwidth. The noise floor (N) is then defined as the median of the squares of the FFT magnitudes of the FFT indices in the same 0.5–1 Hz frequency band, excluding the indices defined as the peak frequency window (used for the heartbeat pulse power). Finally, the pulse SNR was defined as <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">g</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For pulse SNR calculation, we only used the second nearest neighbor measurements which penetrate through the cortex (30 mm source-detector separation), second wavelength (850 nm), and good measurements (ones with &lt;7.5% std). <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref> shows an example for high SNR values across the field of view (top) and its signature in a subset of the measurements for the same run (bottom) (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3A</xref>) and an example for low SNR values across the field of view (top) and a subset of measurements for that run (bottom) (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3B</xref>).</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Examples of a good and a bad pulse signal-to-noise ratio (SNR) in high-density diffuse optical tomography (HD-DOT) data.</title><p>Example pulse SNR plot and a selection of the measurements from the HD-DOT array for (<bold>A</bold>) a high-quality pulse SNR, and (<bold>B</bold>) a low-quality pulse SNR. Note the heartbeat frequency (~1 Hz) that appears as around 10 peaks in 10 s.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-app1-fig3-v2.tif"/></fig></sec><sec sec-type="appendix" id="s11"><title>Exclusion of subjects based on low pulse and high motion</title><p>We rank ordered all subjects based on their mean band limited SNR across the field of view and their mean GVTD values (averaged over two spoken word recognition runs for each subject). We then excluded one of the CI users due to a very low band limited SNR (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4A</xref>) and one control due to a very high motion level (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4B</xref>).</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Exclusion of subjects with very low band limited signal-to-noise ratio (SNR) and very high motion levels.</title><p>Sorting all subjects based on (<bold>A</bold>) mean band limited SNR value across the high-density diffuse optical tomography (HD-DOT field of view, and (<bold>B</bold>)) mean global variance of the temporal derivative (GVTD) values across spoken word recognition runs. The red boxes indicate the subjects excluded based on each quality score.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-app1-fig4-v2.tif"/></fig></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s12"><title>Simulating the effect of the CI transducer blocking the optodes</title><p>We identified the approximate area of the HD-DOT field of view that fell under the transducer across people. In most subjects, the transducer only blocked three to five optodes. Due to the high density of the optodes in HD-DOT systems (13 mm nearest source-detector distance), even when an optode is blocked, the light still penetrates through that region from the neighboring sources. Therefore, in most cases, light level plots reveal sensitivity reduction in only two or three optodes. <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A</xref> shows the location of the CI transducer in one CI user and the optode numbers. <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1B</xref> shows the sensitivity of the HD-DOT cap if all the optodes are fully coupled with the scalp. <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1C and D</xref> demonstrate that the sensitivity drop due to the CI transducer is mainly affecting the IFG and MTG when four and six source-detector pairs are simulated to be completely blocked. Note that the right auditory ROI (<xref ref-type="fig" rid="fig6">Figure 6B</xref>) mainly includes the STG.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Effects of the cochlear implant (CI) transducer on high-density diffuse optical tomography (HD-DOT) sensitivity.</title><p>(<bold>A</bold>) A right ear CI user wearing the HD-DOT cap. The yellow contours around the CI transducer with respect to the ear fiducials illustrate the source numbers (encircled in red) and detector numbers (encircled in blue) around the transducer. (<bold>B</bold>) The left panel shows the HD-DOT source-detector grid, overlaid on the mesh used in the study for image reconstruction. The right panel shows the sensitivity of the HD-DOT cap around the cortex including all optodes. (<bold>C</bold>) The left panel shows four sources (in red) and four detectors (in blue) excluded from the sensitivity calculation. The right panel shows the sensitivity of the HD-DOT cap excluding those sources and detectors. (<bold>D</bold>) The left panel shows six sources (in red) and six detectors (in blue) excluded from the sensitivity calculation. The right panel shows the sensitivity of the HD-DOT cap excluding those sources and detectors. Note that the exclusion of sources and detectors in (<bold>C</bold>) and (<bold>D</bold>) only resulted in sensitivity drop off in the inferior and middle temporal gyri.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-app2-fig1-v2.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75323.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kj2bm70</institution-id><institution>University of Newcastle</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.10.16.464654" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.16.464654"/></front-stub><body><p>The work establishes use of a specific extra area of prefrontal cortex during word listening by CI users and supports a hypothesis based on the multiple demand network that can be tested using other techniques that look at the rest of the network. The revision provides further points of clarity required and better acknowledges the limitations of the technique.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75323.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kj2bm70</institution-id><institution>University of Newcastle</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kj2bm70</institution-id><institution>University of Newcastle</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Gander</surname><given-names>Phillip E</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/036jqmy94</institution-id><institution>University of Iowa</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.10.16.464654">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.10.16.464654v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Dorsolateral prefrontal cortex supports speech perception in listeners with cochlear implants&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Timothy D Griffiths as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Phillip E Gander (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The authors should address the following major points in a revision that are highlighted in detail in the individual referee reports.</p><p>1. Acknowledgement of and discussion of technical limitations. These are related to the depth of signal and field of view which means that the whole of the network for speech perception is not analysed.</p><p>2. Discussion of task effects. There is an effect shown in the frontal cortex listening to the world in quiet but no measurement of correlates of more challenging listening that might show this with the greatest power or demonstrate additional areas. And although listening effort is invoked to explain the data there are no physiological measurements of this.</p><p>3. Clearer definition and justification of the ROI within which extra activation occurs. The result in the frontal ROI is just significant without correction for multiple comparisons so it is important that the ROI is clearly justified and explained</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I would like the authors to consider these comments. Basically, I think the demonstration of the DLPFC signal increase during speech perception by the CI group supports models based on the use of additional resources beyond the traditional speech network. An issue for me is that the technique does not allow the examination of other candidate areas.</p><p>1. The technique is an advance on fNIRS used previously in terms of spatial resolution but suffers from the same issue with respect to the demonstration of deep sources. This is helpfully shown by the authors in the volumetric field of view diagram in figure 1. The demonstration of the DLPFC involvement in the listening task is robust in the group data including the ROI analysis: this area can be optimally imaged with the technique. The field of view excludes deep parts of the superior temporal plane including the medial parts of the primary auditory cortex and planum temporale behind it, and the deep opercular part of IFG. This prevents the comparison of the signal between CI users and controls in all of the conventional parts of the language network (and other parts of the multiple demand network discussed below).</p><p>2. With respect to the decrease in signal in the left auditory cortex in the CI users I think the interpretation is limited because the medial primary cortex is not imaged. I agree with the authors' suggestion that the change here likely reflects a decrease in high-level processing of speech sounds (in the cortex near the convexity) but the discussion might include acknowledgement of the technical limitation due to field of view.</p><p>3. The demonstration of increased signal in the left DLPFC is the most robust finding here and is interesting in and of itself. This is consistent with the idea advanced by the senior author that more difficult listening engages anterior cingulate, frontal operculum, premotor cortex, DLPFC and parietal cortex (eg PMID: 28938250 figure 3). Of these areas, only the DLPFC is adequately imaged by this technique (although the authors might comment on whether the inferior parietal cortex is within FOV and demonstrated by contrast in figure 4C). The discussion here focuses more on the multiple-demand network idea and uses a WM task similar to Duncan and others to define this but from this perspective the imaging of the deep operculum, frontal lobe and medial frontal lobe is still missing.</p><p>4. I found it hard to interpret the between-subject effect of behavioural scores in figure 6. The effect of left ear hearing (non-CI side) goes in a direction I would predict if the DLPFC signal reflected listening effect and the right ear hearing (CI side) goes in the opposite direction. I guess the range of hearing deficit is broader on the left and this has been emphasised more in the discussion. The authors might comment.</p><p>5. Do the authors have any data on varying behaviour within subjects like presenting the sound in noise? Again, any effect on DLPFC would be interesting. But I think the main point here is the use of a different network as a function of CI use even in quiet.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. Hypothesis/results interpretation:</p><p>Can the justification for the hypothesis be unpacked a little more, for this particular case? If challenging listening requires more processing in a wider array of brain regions to take over for greater cognitive demand, for example in DLPFC. And CI users experience greater cognitive demand during listening, one might hypothesize that listening to single words without background noise would be the hardest condition in which to find an effect (despite the finding of Dwyer et al. 2014). Alternatively, one might compare words in quiet with words in noise, since speech in noise understanding is the main complaint of people with hearing loss (especially CI users). And the brain mechanisms required under these conditions might be different to what is a less challenging task. Perhaps the authors wanted to start in the most &quot;trivial&quot; case and test more conditions later.</p><p>Figure 4. The difference t-map has a somewhat loose threshold. Presumably this is because the differences in the frontal cortex would be lost. The authors have previously used more stringent thresholding, but perhaps there is justification here.</p><p>The CI&gt;Control t-map shows the strongest differences in the visual/visual association cortex, auditory cortex. On the left it appears that a relevant frontal activation occurs in the inferior frontal gyrus, as might be predicted from the literature.</p><p>Reduced activation in the left auditory cortex between CI and control could still be due to threshold differences ~5dB (may be significant). Peripheral fidelity is much lower in the CI group, with also poor contribution ipsilaterally.</p><p>The authors propose that lower left auditory cortex activation could also be related to lower speech perception scores, however these scores are drawn from the AzBio test (sentences in noise) which has the potential for indexing very different processes than a word in the quiet test as was the case in this study.</p><p>2. ROI definition:</p><p>Choosing an ROI functionally defined from a spatial WM task to look for overlap in brain regions sharing a putative cognitive process is unclear if the comparison is a verbal task. If the goal was to find common cognitive resources the referenced paper by Federenko and colleagues achieved such a goal, providing an ROI with which to test against a common set of cognitive processes. Further, the claim that the task activates a brain region, DLFPC, which does seem to include dorsal aspects of inferior frontal gyrus on pars triangularis. Yet the activation found include this region in this study, while this region of pars triangularis does not appear in the work by Federenko and colleagues leave the reader confused about why these differences would exist, unless it was for other reasons, like localization differences between HD-DOT and fMRI.</p><p>The results might change considerably if anatomical ROIs were used instead, i.e., IFG or a DLPFC ROI.</p><p>It is fine to use the logic that one task (A) primarily indexes cognitive function X (hopefully) and let's see if another task (B) also shares some of that presumed function by masking B with A. That shows (hopefully) that the functions are shared across tasks. In this case, the function of (spatial) working memory is shared between a spatial working memory and speech task in CI users.</p><p>DLPFC is an anatomical label, not a functional one. In this case, the authors have used a functional ROI and so need to name it as such, e.g., 'WM', while reference to the term DLPFC is not helpful and should be removed from the manuscript (and title) except perhaps in the discussion.</p><p>Line 194. It is slightly confusing to define auditory ROIs from a fMRI resting state paradigm. The referenced paper used a 'word listening' activation for both fMRI and HD-DOT. So are the authors using the acoustic word task rather than rest? And further, if there is data using HD-DOT with a similar paradigm then that might be a more sensible ROI definition?</p><p>Line 195-199. The procedure for the definition of the ROI is not clearly motivated, in particular, because the data are obtained from a previous publication which displays thresholded values (at least for the auditory task) that clearly defines the auditory cortex. A correlational approach might (appropriately) be very broad and include areas outside of 'canonical' auditory temporal cortex. Was this intended? What was the threshold for the correlation?</p><p>Line 240-245. This text implies the ROI was defined with the current data set rather than another as written in the methods. Please clarify.</p><p>Figure 3A. The 'DLPFC' ROI seems to have a large proportion of the cluster within pars triangularis, which is not quite the same as the fMRI overlap representation in Figure 2 of Federnko et al. 2013, in which the bulk of activation is within the middle frontal gyrus.</p><p>3. HD-DOT sensitivity:</p><p>As stated in the referenced White and Culver paper: &quot;The high-density array has an effective resolution of about 1.3 cm, which should prevent identification to the wrong gyrus.&quot;</p><p>This means that localization errors to middle frontal gyrus, as opposed to inferior frontal gyrus, could be a concern in this case in which the activation appears to be across a sulcal boundary.</p><p>Figure 1. The sensitivity profile appears to show that greater sensitivity exists for the superior aspect of inferior frontal gyrus and inferior aspect of the middle frontal gyrus (and visual cortex). Is that a simple misinterpretation from the figure? Or do the authors know of issues with respect to the HD-DOT technique weighting responses to the regions of greater sensitivity?</p><p>Figure 1 supplement. Is it possible there may be some concern about the drop in signal on the right due to variability in the location of the CI transducer across participants? Is it possible the drop in signal in right middle/superior temporal and inferior frontal is also due to loss of measurement here across the group of CI users?</p><p>Specific comments:</p><p>1. Lines 56-57: Please clarify. Are you saying that listening to degraded input reduces the ability for certain other processing, for instance, performance on a second task that involves memory encoding? Or is it that memory encoding on the speech is limited, or both? It is not entirely clear from the text.</p><p>2. What kind of CI device? Since it is not specified, I assume this is a standard length implant and not one for hearing preservation.</p><p>3. What was the distribution of CI duration? Is it possible there may be some people under 1 year of use? If so this may be difficult with respect to interpretation for those users, since their performance with the device may not be their best, i.e., they may still yet improve. This process of adaptation to the implant could easily involve different brain regions other than measured in this study, or a different weighting depending on adaptation.</p><p>4. Similarly, does the range of unaided hearing in the CI group get to the level of some &quot;functional&quot; residual hearing, or does the skew go to higher thresholds? If there are people with PTAs of ~60 they may perform differently than people without functional residual hearing? Perhaps residual hearing could be added as a covariate to the analyses.</p><p>There may also be clues based on performance on the AzBio task. Is there a relationship between residual hearing and AzBio score? The thrust of these questions is that residual hearing likely has an impact on performance, which similarly may influence brain mechanisms.</p><p>5. Figure 1. The Spatial WM task appears to have multiple components within it that would be beyond just WM, for example, reward (from the feedback), decision making, response execution, which would all be tied into the level of temporal resolution of the imaging technique. This might temper conclusions about WM per se.</p><p>6. Visual activation appears only on the left, which is unclear (compare to Figure 3A). Was the display directly in-front of the participant, or something about their vision?</p><p>7. Figure 1. In the spatial working memory task the wait time range of 4000-250ms is unclear.</p><p>8. Experimental design. Please provide more information about the following: presentation level for acoustic stimuli, types of words spoken, e.g., monosyllabic, nouns, single talker, gender, etc.</p><p>9. Line 149. Confusing text: &quot;…regressed from the all measurement…&quot;</p><p>10. Line 176. Was the 'easy+hard' contrast used to provide better SNR?</p><p>Alternatively, why not use the ROI defined previously? Did they vary, because if they did that is instructive, perhaps as implied by aging effects?</p><p>11. Line 191. Which group of subjects? Is this not the study group as implied on line 176?</p><p>12. Line 260. Why does the supplemental analysis not include a contrast of control &gt; CI?</p><p>13. Line 304-305. Incomplete sentence.</p><p>14. Line 359. The cited literature of similar findings in normal listeners, mostly highlights the inferior frontal gyrus, however, middle frontal gyrus is included in Defenderfer et al.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Introduction</p><p>Line 81: Why predict effects in DLPFC only? There could be a range of 'non-core' brain regions that support speech perception. Were the effects lateralized to left DLPFC predicted in advance of the study?</p><p>Methods</p><p>Line 91: How were the sample sizes of the CI group and the control group determined? I note that page 7 explains that 15-20 subjects per group was thought to be sufficient, based on similar studies. Did these previous studies include a cross-sectional design?</p><p>Page 4. Table 1 reports left ear 4fPTA (unaided). The footnote for Table 1 reports that 8/20 CI users had a hearing aid but did they use them during the speech task?</p><p>Line 120. Where were the 2 speakers located relative to the participants? +/- 45 degrees? Which sound level were the words presented at?</p><p>Lines 127-130. More details about the spoken word task is needed. Is this a standardized test or a novel test that the authors developed themselves?</p><p>Line 131. Which 'preliminary results'?</p><p>Line 191. &quot;…based on the response … in a group of subjects…&quot;. Which subjects? Merge with relevant information on page 8.</p><p>Line 216. Is &quot;Pollonini et al. 2014&quot; cited correctly?</p><p>Line 238. Why define only left DLPFC as an ROI?</p><p>Page 9. The visual spatial working memory task also activated 'auditory' regions in the right hemisphere that look like they overlap with the 'right auditory ROI' created by the authors. Do the authors consider this overlap to be problematic?</p><p>Page 9. Legend for Figure 3. &quot;…in the DLPFC region, survived after…&quot;. A word is missing.</p><p>Page 10. Figure 4C suggests that CI &gt; controls contrast identified quite a few group differences. So why then focus on left DLPFC?</p><p>Page 11. Results of two-sample t-tests are reported in the legend of Figure 5. This information should be taken out of the legend and reported in full. Why were t-tests more appropriate than an ANOVA? Please report results of statistical analyses fully i.e. not just p-values. Were reported p-vals corrected for the number of t-tests that were used?</p><p>Page 11. The authors report the results of correlation analyses. Please add further details e.g. are Pearson's correlation coefficients reported?</p><p>Page 11. The authors report that the threshold used for statistical significance in the correlation analyses was 'uncorrected'. Presumably this means that reported p-vals were not corrected for the 4 correlation analyses shown?</p><p>Page 12. Correlation analyses for CI users. The rationale for these analyses is missing. Why carry out simple correlation analyses on data for CI users only? Potential predictors of speech processing (4PTA) could have been used as a covariate in the main analyses (e.g. Figure 5).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75323.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The authors should address the following major points in a revision that are highlighted in detail in the individual referee reports.</p><p>1. Acknowledgement of and discussion of technical limitations. These are related to the depth of signal and field of view which means that the whole of the network for speech perception is not analysed.</p></disp-quote><p>Thank you for encouraging us to clarify the limitations in our approach. Of course, we completely agree that though we have good coverage relative to other optical neuroimaging approaches, we do not cover the full brain, and there are indeed brain regions we might expect to be involved in speech perception that we do not image. We have added the following to the Discussion:</p><p>“As with all fNIRS-based functional neuroimaging, our ability to image neural activity is limited by the placement of sources and detectors, and in depth to approximately 1 cm of the cortex. Our HD-DOT cap covers the regions of superficial cortex commonly identified in fMRI studies of speech processing, including bilateral superior and middle temporal gyri, and pars triangularis of the left inferior frontal gyrus (Davis and Johnsrude 2003; Rodd, Davis, and Johnsrude 2005; Rogers et al. 2020; Wild et al. 2012). We also have good coverage of occipital cortex and middle frontal gyrus. However, in the present study we do not image pars opercularis of the inferior frontal gyrus, the cerebellum, or any subcortical structures. Thus, it is certainly possible that additional regions not identified here play different roles in understanding speech for CI listeners, a possibility that will require fNIRS setups with improved cortical coverage and converging evidence from other methods to explore.”</p><disp-quote content-type="editor-comment"><p>2. Discussion of task effects. There is an effect shown in the frontal cortex listening to the world in quiet but no measurement of correlates of more challenging listening that might show this with the greatest power or demonstrate additional areas. And although listening effort is invoked to explain the data there are no physiological measurements of this.</p></disp-quote><p>Thank you for raising these two important points.</p><p>Regarding the first point, it is important to note that both CI simulations (i.e., noise vocoding) and data from CI listeners show difficulty understanding speech in quiet (Willis, Helen; 2018; The feasibility of the dual-task paradigm as a framework for a clinical test of listening effort in cochlear implant users. Doctoral thesis, University College London). Thus, our perspective is that speech may be “in quiet” in the room, but to the auditory system of a listener with a cochlear implant, the acoustic signal is degraded, and it is precisely the brain’s response to that degraded signal we are measuring. We readily agree that further exploring responses to speech in noise (which, of course, is common in everyday listening, and can be particularly challenging for many listeners with cochlear implants) is highly interesting, and we hope a future extension of this work. We now explicitly note these points in the introduction (to clarify the logic behind studying speech in quiet) and in the Discussion:</p><p>“Finally, we emphasize that we report here only responses to speech in quiet. Everyday communication frequently occurs in the presence of background noise, which can be particularly challenging for many listeners with cochlear implants (Firszt et al. 2004). Exploring how activity in PFC and other regions fluctuates in response to speech at various levels of background noise would be a highly interesting future extension of this work. Based on parametric modulations of acoustic challenge in listeners with normal hearing, we might expect increasing activity in PFC as speech gets more challenging (Davis and Johnsrude 2003), followed by cingulo-opercular activity once intelligibility is significantly hampered (Vaden et al. 2013).”</p><p>With respect to the second point, it is entirely correct that we do not have a non-imaging measure of cognitive demand during listening. That being said, we think there are good reasons to think that differences here are driven by a response to acoustic challenge, which we believe falls comfortably under the “listening effort” umbrella. However, there are two important clarifications we should make. First, we readily acknowledge that listening effort is not clearly defined (Strand et al., 2020), and in its fullest definition encompasses more than just cognitive challenge (Pichora-Fuller et al., 2016). In fact, in the manuscript we never use the term “listening effort” to describe our paradigm for precisely this reason. We have also done our best to explain where our cognitive-demand-centered interpretation comes from. In short, from imaging studies showing parametric responses to acoustic challenge (including noise vocoding) (e.g., Davis and Johnsrude, 2003), behavioral studies showing that acoustic challenge hinders memory for words (Rabbitt, 1968) and stories (Ward et al., 2016); and the many reports of listeners with CIs having difficulty understanding speech (Firszt et al., 2004, McMurray et al. 2017, and many others). We have added these points to the Discussion:</p><p>“Cognitive demand during speech understanding frequently comes up in discussions of effortful listening (Pichora-Fuller et al. 2016). Although understandable, it is important to keep in mind that the construct of listening effort is not clearly defined (Strand et al. 2020) and different measures of “effort” do not always agree with each other (Strand et al. 2018). Here, we interpret increased activity in PFC as reflecting greater cognitive demand. Although we did not include an independent measure of cognitive challenge, we note that simulated CI speech (i.e., noise-vocoded speech) is associated with delayed word recognition (McMurray, Farris-Trimble, and Rigler 2017) and poorer memory for what has been heard (Ward et al., 2016), consistent with increased dependence on shared cognitive processes (Piquado et al. 2010). Relating activity in PFC to cognitive demand is also consistent with decreased activity in PFC when speech becomes unintelligible (Davis and Johnsrude 2003).”</p><disp-quote content-type="editor-comment"><p>3. Clearer definition and justification of the ROI within which extra activation occurs. The result in the frontal ROI is just significant without correction for multiple comparisons so it is important that the ROI is clearly justified and explained</p></disp-quote><p>Thank you for giving us the opportunity to clarify these important points. We start by discussing the definition and justification for the PFC region of interest, followed by a description of a revised analysis.</p><p>We have added some additional detail to the justification of the prefrontal cortex ROI. The full revised text reads:</p><p>“To accurately localize the elevated prefrontal cortex activation in the CI group, we collected HD-DOT data from 9 subjects (4 controls and 5 CI users in 13 sessions) using a spatial working memory task (Fedorenko, Duncan, and Kanwisher 2012). The visual spatial working memory task robustly activates PFC due to its working memory demands (and visual cortex because of its visual aspect). We chose this task to localize the PFC ROI for performing an ROI-based statistical analysis between controls and CI users. Our logic is that in prior work this task shows activity that dissociates from that in inferior frontal gyrus (Fedorenko, Duncan, and Kanwisher 2012), and thus the region of PFC localized by this task is more likely to reflect domain-general processing (as opposed to language-specific processing). Our results show strong bilateral visual and PFC activations in response to this task (Figure 6A left). We then defined the left PFC ROI as the cluster of activation in the left PFC region (Figure 6A right). This region was centered in inferior frontal sulcus, extending into both dorsal IFG and inferior MFG. The location is broadly consistent with domain-general (“multiple demand”) activation (e.g., meta-analysis in Duncan, 2010).”</p><p>In terms of anatomy, our frontal cortex ROI is centered in the inferior frontal sulcus (IFS), which also corresponds to peak activity in meta analyses of “multiple demand” tasks—for example, in this meta-analysis from Duncan (2010, Trends in Cognitive Science).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-sa2-fig1-v2.tif"/></fig><p>Being centered in IFS, there is some overlap with both IFG and MFG. If we trust our source localization, we don’t think the IFG overlap is inferior enough to suggest language-related processing, although we concede we do not have definitive evidence for this. The ROI is unquestionably in the prefrontal cortex. In our view, centered in IFS is sufficient for considering this dorsolateral prefrontal cortex. However, we have gone with the more general “prefrontal cortex” in the title and text of the manuscript because we don’t think this point affects our main findings or interpretation.Regarding correction for multiple comparisons, we have revisited this section and clarified that the critical comparison in PFC is corrected for the ROIs looked at.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I would like the authors to consider these comments. Basically, I think the demonstration of the DLPFC signal increase during speech perception by the CI group supports models based on the use of additional resources beyond the traditional speech network. An issue for me is that the technique does not allow the examination of other candidate areas.</p><p>1. The technique is an advance on fNIRS used previously in terms of spatial resolution but suffers from the same issue with respect to the demonstration of deep sources. This is helpfully shown by the authors in the volumetric field of view diagram in figure 1. The demonstration of the DLPFC involvement in the listening task is robust in the group data including the ROI analysis: this area can be optimally imaged with the technique. The field of view excludes deep parts of the superior temporal plane including the medial parts of the primary auditory cortex and planum temporale behind it, and the deep opercular part of IFG. This prevents the comparison of the signal between CI users and controls in all of the conventional parts of the language network (and other parts of the multiple demand network discussed below).</p></disp-quote><p>We acknowledge the limitation of our system field of view in covering the entirety of brain networks involved in speech processing, particularly deep structures. We agree this is an important point, and now note this explicitly in the Discussion.</p><disp-quote content-type="editor-comment"><p>2. With respect to the decrease in signal in the left auditory cortex in the CI users I think the interpretation is limited because the medial primary cortex is not imaged. I agree with the authors' suggestion that the change here likely reflects a decrease in high-level processing of speech sounds (in the cortex near the convexity) but the discussion might include acknowledgement of the technical limitation due to field of view.</p></disp-quote><p>This comment is very important and we thank the reviewer for reminding us of this fact. We have added a sentence in the Discussion section to address this point.</p><disp-quote content-type="editor-comment"><p>3. The demonstration of increased signal in the left DLPFC is the most robust finding here and is interesting in and of itself. This is consistent with the idea advanced by the senior author that more difficult listening engages anterior cingulate, frontal operculum, premotor cortex, DLPFC and parietal cortex (eg PMID: 28938250 figure 3). Of these areas, only the DLPFC is adequately imaged by this technique (although the authors might comment on whether the inferior parietal cortex is within FOV and demonstrated by contrast in figure 4C). The discussion here focuses more on the multiple-demand network idea and uses a WM task similar to Duncan and others to define this but from this perspective the imaging of the deep operculum, frontal lobe and medial frontal lobe is still missing.</p></disp-quote><p>We focused on the PFC ROI for two reasons. First, our reading of prior fMRI literature (and work from our own group) suggests that when speech is challenging to understand, but still intelligible, increases are seen in many regions of the dorsolateral prefrontal cortex. However, when intelligibility begins to suffer (i.e., errors are introduced), the cingulo-opercular network is engaged (see compelling fMRI work from Eckert, Vaden, and colleagues). Because we were presenting speech in quiet, to CI users with fairly good auditory-only perception, we focused on the PFC activity. As you point out, we also do not have the ability to image deep areas—and so we are not particularly well-suited for studying the cingulo-opercular network.</p><p>We had considered including an inferior parietal ROI, which as you say, is also often implicated in multiple-demand tasks. However, we did not have a clear ROI defined based on our spatial working memory task. Thus, we did not feel justified in including one. (It is also unclear whether our parietal coverage adequately covers IPS—it may, but the absence of strong functional activity here led us to abandon this idea for now.)</p><disp-quote content-type="editor-comment"><p>4. I found it hard to interpret the between-subject effect of behavioural scores in figure 6. The effect of left ear hearing (non-CI side) goes in a direction I would predict if the DLPFC signal reflected listening effect and the right-ear hearing (CI side) goes in the opposite direction. I guess the range of hearing deficit is broader on the left and this has been emphasised more in the discussion. The authors might comment.</p></disp-quote><p>We agree with this comment. We also think that our rather small sample size and the small variability in the right ear hearing levels, makes our PFC correlation with right ear 4fPTA harder to interpret. We added a sentence to state this more clearly in the discussion.</p><disp-quote content-type="editor-comment"><p>5. Do the authors have any data on varying behaviour within subjects like presenting the sound in noise? Again, any effect on DLPFC would be interesting. But I think the main point here is the use of a different network as a function of CI use even in quiet.</p></disp-quote><p>Because we have brain imaging data for speech in quiet, we felt that the strongest justification for brain-behavior correlations would be to use behavioral scores from a similar condition.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. Hypothesis/results interpretation:</p><p>Can the justification for the hypothesis be unpacked a little more, for this particular case? If challenging listening requires more processing in a wider array of brain regions to take over for greater cognitive demand, for example in DLPFC. And CI users experience greater cognitive demand during listening, one might hypothesize that listening to single words without background noise would be the hardest condition in which to find an effect (despite the finding of Dwyer et al. 2014). Alternatively, one might compare words in quiet with words in noise, since speech in noise understanding is the main complaint of people with hearing loss (especially CI users). And the brain mechanisms required under these conditions might be different to what is a less challenging task. Perhaps the authors wanted to start in the most &quot;trivial&quot; case and test more conditions later.</p></disp-quote><p>As we note above, speech in quiet is still degraded from the perspective of the auditory system of a CI user. The brain responses to degraded speech are not uniform, but depend (among other things) on the overall challenge and intelligibility. We focused on speech in quiet to (a) demonstrate differences in processing for listeners with CIs “even” in quiet, and (b) to maximize intelligibility, which in our view helps interpretation (i.e., we answer the question: what are listeners doing to understand speech when they are being successful?).</p><disp-quote content-type="editor-comment"><p>Figure 4. The difference t-map has a somewhat loose threshold. Presumably this is because the differences in the frontal cortex would be lost. The authors have previously used more stringent thresholding, but perhaps there is justification here.</p></disp-quote><p>Because we had rather specific anatomically-constrained hypotheses, we constructed the study first and foremost around an ROI-based analysis. However, we also thought it was important to show non-ROI results. Hence, in Figure 2 we include unthresholded maps of parameter estimates, and thresholded (but not corrected) t maps. We do this to give a sense of the extent of the data and to be transparent about what is, and isn’t showing up at the whole-brain level. It is correct that the result in PFC does not survive correcting for multiple comparisons at the whole brain level.</p><disp-quote content-type="editor-comment"><p>The CI&gt;Control t-map shows the strongest differences in the visual/visual association cortex, auditory cortex. On the left it appears that a relevant frontal activation occurs in the inferior frontal gyrus, as might be predicted from the literature.</p><p>Reduced activation in the left auditory cortex between CI and control could still be due to threshold differences ~5dB (may be significant). Peripheral fidelity is much lower in the CI group, with also poor contribution ipsilaterally.</p></disp-quote><p>An additional reason we provide the whole-brain maps is to help readers think about additional hypotheses and guide future research. We agree that activity in IFG is also of interest.</p><disp-quote content-type="editor-comment"><p>The authors propose that lower left auditory cortex activation could also be related to lower speech perception scores, however these scores are drawn from the AzBio test (sentences in noise) which has the potential for indexing very different processes than a word in the quiet test as was the case in this study.</p></disp-quote><p>We used the AzBio sentences, but these were presented in quiet to get a measure of speech-in-quiet understanding. We have clarified this in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>2. ROI definition:</p><p>Choosing an ROI functionally defined from a spatial WM task to look for overlap in brain regions sharing a putative cognitive process is unclear if the comparison is a verbal task. If the goal was to find common cognitive resources the referenced paper by Federenko and colleagues achieved such a goal, providing an ROI with which to test against a common set of cognitive processes. Further, the claim that the task activates a brain region, DLFPC, which does seem to include dorsal aspects of inferior frontal gyrus on pars triangularis. Yet the activation found include this region in this study, while this region of pars triangularis does not appear in the work by Federenko and colleagues leave the reader confused about why these differences would exist, unless it was for other reasons, like localization differences between HD-DOT and fMRI.</p><p>The results might change considerably if anatomical ROIs were used instead, i.e., IFG or a DLPFC ROI.</p><p>It is fine to use the logic that one task (A) primarily indexes cognitive function X (hopefully) and let's see if another task (B) also shares some of that presumed function by masking B with A. That shows (hopefully) that the functions are shared across tasks. In this case, the function of (spatial) working memory is shared between a spatial working memory and speech task in CI users.</p><p>DLPFC is an anatomical label, not a functional one. In this case, the authors have used a functional ROI and so need to name it as such, e.g., 'WM', while reference to the term DLPFC is not helpful and should be removed from the manuscript (and title) except perhaps in the discussion.</p></disp-quote><p>We have updated the term to “prefrontal cortex” because this region is unambiguously in the prefrontal cortex. In our view, it is in the dorsal and lateral aspect of the prefrontal cortex, which motivated our use of DLPFC. However we acknowledge there is no clear consensus on DLPFC boundaries and we don’t find this label critical to our argument.</p><disp-quote content-type="editor-comment"><p>Line 194. It is slightly confusing to define auditory ROIs from a fMRI resting state paradigm. The referenced paper used a 'word listening' activation for both fMRI and HD-DOT. So are the authors using the acoustic word task rather than rest? And further, if there is data using HD-DOT with a similar paradigm then that might be a more sensible ROI definition?</p></disp-quote><p>We appreciate the reviewer's attention to this point. In the referenced paper, both word listening and resting state data were presented. We made <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> to show that there is a large overlap between the left and right auditory ROIs found using the seed based resting state maps and the auditory maps in response to the hearing words task. The dice coefficient between our proposed resting state based ROI and the hearing words based ROI (thresholded at p = 0.05) is 0.7 and the voxelwise similarity of the two is also 0.7. We chose the resting state based ROIs based on fMRI data, because it is independent of any particular task, although we see the alternative logic of using the same task, as well. In practice regions defined both ways are similar, and do not affect our conclusions.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-sa2-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Line 195-199. The procedure for the definition of the ROI is not clearly motivated, in particular, because the data are obtained from a previous publication which displays thresholded values (at least for the auditory task) that clearly defines the auditory cortex. A correlational approach might (appropriately) be very broad and include areas outside of 'canonical' auditory temporal cortex. Was this intended? What was the threshold for the correlation?</p></disp-quote><p>It is not uncommon for speech-related activity to extend beyond the superior temporal plane, through STG and into STS (and sometimes MTG). Our aim was to define an ROI that covered this span of auditory responses and corresponded to our prior work using this particular DOT system (see, e.g., Eggebrecht et al. 2014). We found the ROIs defined this way showed good general correspondence to the activation in our speech tasks, which was our specific use case. For thresholding the ROIs we used a threshold of z(r) = 0.1.</p><disp-quote content-type="editor-comment"><p>Line 240-245. This text implies the ROI was defined with the current data set rather than another as written in the methods. Please clarify.</p></disp-quote><p>Thanks for bringing this ambiguity to our attention. To make this point clear, we consolidated “Region of interest analysis” in Methods and “Functionally defined ROIs” in Results to ““Functionally defined regions of interest”” in Methods.</p><p>There, we clarified that:</p><p>“To accurately localize the elevated PFC activation in the CI group, we collected HD-DOT data from 9 subjects (4 controls and 5 CI users in 13 sessions) using a spatial working memory task (Fedorenko et al., 2012)” …. “To define the left and right auditory ROIs, we used a previously published fMRI resting state dataset (Sherafati et al., 2020)”</p><disp-quote content-type="editor-comment"><p>Figure 3A. The 'DLPFC' ROI seems to have a large proportion of the cluster within pars triangularis, which is not quite the same as the fMRI overlap representation in Figure 2 of Federnko et al. 2013, in which the bulk of activation is within the middle frontal gyrus.</p></disp-quote><p>One strength of using a functional localizer is that our inferences are most strongly driven by the overlap in activity, rather than putative anatomy. Though, we also note that our ROI is centered on IFS, which is often the center of overlap in multiple-demand network studies. However, there certainly could be some anatomical overlap with portions of the IFG.</p><disp-quote content-type="editor-comment"><p>3. HD-DOT sensitivity:</p><p>As stated in the referenced White and Culver paper: &quot;The high-density array has an effective resolution of about 1.3 cm, which should prevent identification to the wrong gyrus.&quot;</p><p>This means that localization errors to middle frontal gyrus, as opposed to inferior frontal gyrus, could be a concern in this case in which the activation appears to be across a sulcal boundary.</p></disp-quote><p>Our interpretation of the prefrontal activity in CI users is not based primarily on the anatomical landmark, but by the functional definition of the region (which we interpret in an anatomical context).</p><disp-quote content-type="editor-comment"><p>Figure 1. The sensitivity profile appears to show that greater sensitivity exists for the superior aspect of inferior frontal gyrus and inferior aspect of the middle frontal gyrus (and visual cortex). Is that a simple misinterpretation from the figure? Or do the authors know of issues with respect to the HD-DOT technique weighting responses to the regions of greater sensitivity?</p></disp-quote><p>The variation in sensitivity profile proceeds from the arrangement of the sources and detectors. Near the edge of the field of view, sensitivity begins to drop. With our current cap configuration, this does result in small sensitivity differences (Appendix 2– figure 1B). However, we apply a more stringent field of view mask which was created using the intersection of sensitivity across multiple participants to all our data to exclude any voxels with a sensitivity less than around 10% of the maximum sensitivity (Figure 5C). This avoids including any voxels that might be less reliable due to inhomogeneous sensitivity.</p><disp-quote content-type="editor-comment"><p>Figure 1 supplement. Is it possible there may be some concern about the drop in signal on the right due to variability in the location of the CI transducer across participants? Is it possible the drop in signal in right middle/superior temporal and inferior frontal is also due to loss of measurement here across the group of CI users?</p></disp-quote><p>Thank you for raising this important point, which is something we’ve worried about a lot. Although we cannot rule out the effects of lower sensitivity in the magnitude of the activation in certain areas, we were fairly confident that the ROIs we explored are not overlapping with low sensitivity areas.</p><p>To quantify the expected change in signal resulting from the CI hardware, in addition to our simulation of two levels of optode blocking (moderate case 4 sources and 4 detectors, and extreme case 6 sources and 6 detectors, now in Appendix 2 – figure 1), we added another simulation study (Figure 3—figure supplements 1-3). Briefly, for each CI user, we calculated how many sources and detectors were affected by the hardware. We then excluded this number of sources/detectors from a control participant. We repeated this process 100 times with different random assignments of excluded channels to the control data to get a sense of what the time course would be for control data (but missing the same number of channels as our CI data). We found a small drop in auditory activation that was still significantly above what we observed in the CI listeners. We note that we cannot completely rule out the effects of CI hardware on signals in the right temporal and parietal regions; however, we do not think it can fully account for our findings in the right auditory cortex. (Because we only studied CI users with a right unilateral implant, regions in the left hemisphere are unaffected.)</p><disp-quote content-type="editor-comment"><p>Specific comments:</p><p>1. Lines 56-57: Please clarify. Are you saying that listening to degraded input reduces the ability for certain other processing, for instance, performance on a second task that involves memory encoding? Or is it that memory encoding on the speech is limited, or both? It is not entirely clear from the text.</p></disp-quote><p>When speech is degraded, listeners have trouble remembering it. For example, in Ward et al. (2016) we played normal-hearing participants short stories that were either normal speech, or degraded (but intelligible) due to noise vocoding (16 or 20 channels). We asked participants to repeat back as much of the stories as they could, and found poorer memory for the degraded stories (despite an intelligibility test indicating the stories were intelligible). Perhaps even more compelling evidence comes from studies of word list memory, in which not only the degraded word, but <italic>subsequent</italic> words are remembered more poorly. Because the subsequent words are not degraded, it is difficult to come up with an acoustic explanation for the memory difficulty—thus pointing towards some shared cognitive resource needed to understand degraded speech, that is thus less available for memory encoding.</p><disp-quote content-type="editor-comment"><p>2. What kind of CI device? Since it is not specified, I assume this is a standard length implant and not one for hearing preservation.</p></disp-quote><p>11 used Cochlear, 6 used Advanced Bionics, 3 used Med EI. We have added this information to the methods.</p><disp-quote content-type="editor-comment"><p>3. What was the distribution of CI duration? Is it possible there may be some people under 1 year of use? If so this may be difficult with respect to interpretation for those users, since their performance with the device may not be their best, i.e., they may still yet improve. This process of adaptation to the implant could easily involve different brain regions other than measured in this study, or a different weighting depending on adaptation.</p></disp-quote><p>We agreed the duration of use is important to consider. We only included CI users if they had at least 1 year of experience with their implant. The years of experience ranged between 1-27.</p><fig id="sa2fig3" position="float"><label>Author response image 3.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-sa2-fig3-v2.tif"/></fig><p>We performed some exploratory analyses using this but did not see anything compelling. In general we don’t think this study is adequately powered for individual difference analysis, but experience is certainly a variable we are keeping squarely in mind for future studies.</p><disp-quote content-type="editor-comment"><p>4. Similarly, does the range of unaided hearing in the CI group get to the level of some &quot;functional&quot; residual hearing, or does the skew go to higher thresholds? If there are people with PTAs of ~60 they may perform differently than people without functional residual hearing? Perhaps residual hearing could be added as a covariate to the analyses.</p><p>There may also be clues based on performance on the AzBio task. Is there a relationship between residual hearing and AzBio score? The thrust of these questions is that residual hearing likely has an impact on performance, which similarly may influence brain mechanisms.</p></disp-quote><p>We agree regarding the importance of residual hearing and its variability.</p><p>We found no significant correlation between the speech perception score (average AzBio score in quiet) and the residual hearing, either defined as left ear 4fPTA unaided, or left ear 4fPTA (aided).</p><fig id="sa2fig4" position="float"><label>Author response image 4.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-sa2-fig4-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>5. Figure 1. The Spatial WM task appears to have multiple components within it that would be beyond just WM, for example, reward (from the feedback), decision making, response execution, which would all be tied into the level of temporal resolution of the imaging technique. This might temper conclusions about WM per se.</p></disp-quote><p>We agree the spatial working memory task encompasses multiple processes. In interpreting this task, and the areas involved, we are relying on the fairly extensive multiple-demand literature tied to these regions, and the work of Fedorenko and colleagues showing that—in single-subject applications—regions activated by this task dissociate from those activated by language tasks. But, also, we have tried to remain somewhat agnostic about the specific cognitive processes engaged. We have made some educated guesses based on the broader literature, but we agree that our own current data have to be understood in terms of what we actually tested.</p><disp-quote content-type="editor-comment"><p>6. Visual activation appears only on the left, which is unclear (compare to Figure 3A). Was the display directly in-front of the participant, or something about their vision?</p></disp-quote><p>The display was directly in front of the participant. The crosshair at the center of the monitor has been aligned to the middle point between the eyes of the participant. We do see a stronger visual activation on the left side, however, we did not collect any vision data and just have verbal confirmation of the participant’s normal vision.</p><p>However, <xref ref-type="fig" rid="sa2fig5">Author response image 5</xref> is the same spatial working memory activation (same map depicted in Figure 1C) with a lower t threshold (p = 0.05, t=1.6) showing some activation on the right side which did not survive the more stringent thresholding. Thus we are reluctant to make too much of this qualitative asymmetry.</p><fig id="sa2fig5" position="float"><label>Author response image 5.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75323-sa2-fig5-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>7. Figure 1. In the spatial working memory task the wait time range of 4000-250ms is unclear.</p></disp-quote><p>We apologize for the confusion. The time is 3750 ms (i.e., 4000 – 250), further subtracting the response time. In this way, the “response + feedback + fixation” portion of each trial is kept to 4000 ms on each trial. We modified the figure text to clarify this.</p><disp-quote content-type="editor-comment"><p>8. Experimental design. Please provide more information about the following: presentation level for acoustic stimuli, types of words spoken, e.g., monosyllabic, nouns, single talker, gender, etc.</p></disp-quote><p>Now added to the revised manuscript.</p><disp-quote content-type="editor-comment"><p>9. Line 149. Confusing text: &quot;…regressed from the all measurement…&quot;</p></disp-quote><p>Thank you for catching this, now repaired.</p><disp-quote content-type="editor-comment"><p>10. Line 176. Was the 'easy+hard' contrast used to provide better SNR?</p></disp-quote><p>That is correct.</p><disp-quote content-type="editor-comment"><p>Alternatively, why not use the ROI defined previously? Did they vary, because if they did that is instructive, perhaps as implied by aging effects?</p></disp-quote><p>If you mean the ROI defined in past studies using fMRI, we had three reasons for using our own ROI. First, we are moving towards subject-specific localizers, and as such, need to collect these data in all of our own participants. Second, our participants indeed differ in age from most prior reports with this task, and this may affect the extent of activation. Third, given the intricate anatomy of the prefrontal cortex we wanted to use a within-modality definition for this ROI.</p><disp-quote content-type="editor-comment"><p>11. Line 191. Which group of subjects? Is this not the study group as implied on line 176?</p></disp-quote><p>As mentioned in lines 359-362, we did not plan on collecting data for the spatial working memory task in the beginning of this study. “To better understand the left PFC activity we observed in our first few CI users, we adopted a spatial working memory task introduced in previous studies (Fedorenko et al., 2011; Fedorenko et al., 2013) in the remaining subjects to aid in functionally localizing domain-general regions of prefrontal cortex.” Since in the literature the variability across individuals has been reported for this task, we aimed at defining an ROI based on at least a subset of our own demographic cohort of interest. Second, we wanted to use the same imaging modality to localize this activation.</p><disp-quote content-type="editor-comment"><p>12. Line 260. Why does the supplemental analysis not include a contrast of control &gt; CI?</p></disp-quote><p>Added.</p><disp-quote content-type="editor-comment"><p>13. Line 304-305. Incomplete sentence.</p></disp-quote><p>Thank you for catching this, now fixed.</p><disp-quote content-type="editor-comment"><p>14. Line 359. The cited literature of similar findings in normal listeners, mostly highlights the inferior frontal gyrus, however, middle frontal gyrus is included in Defenderfer et al.</p></disp-quote><p>We categorize this activity as “prefrontal cortex” to cover both IFG and MFG here.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Introduction</p><p>Line 81: Why predict effects in DLPFC only? There could be a range of 'non-core' brain regions that support speech perception. Were the effects lateralized to left DLPFC predicted in advance of the study?</p></disp-quote><p>Indeed, as expanded upon in greater detail in Peelle (2018) and elsewhere, we agree there are many “non-core” regions that support speech perception in a variety of contexts. Briefly, our focus on DLPFC is motivated by (a) many fMRI studies, including from our group (not all published), showing DLPFC activity for degraded speech; (b) the work of other groups in the frameworks of cognitive control, multiple-demand, or frontoparietal attention networks implicating DLPFC in a variety of demanding tasks; (c) behavioral evidence for domain-general demands when processing acoustically-degraded speech, which we view as consistent with a role for DLPFC (as frequently characterized).</p><p>The lateralized DLPFC activity was predicted prior to starting data collection.</p><disp-quote content-type="editor-comment"><p>Methods</p><p>Line 91: How were the sample sizes of the CI group and the control group determined? I note that page 7 explains that 15-20 subjects per group was thought to be sufficient, based on similar studies. Did these previous studies include a cross-sectional design?</p></disp-quote><p>In these referenced studies (Defenderfer et al. 2021; Pollonini et al. 2014; Zhou et al. 2018b), the normal hearing cohort in an effortful listening/unintelligible speech situation was considered to be representing the CI user cohort.</p><disp-quote content-type="editor-comment"><p>Page 4. Table 1 reports left ear 4fPTA (unaided). The footnote for Table 1 reports that 8/20 CI users had a hearing aid but did they use them during the speech task?</p></disp-quote><p>Yes, all listeners performed the test in their everyday listening configuration.</p><disp-quote content-type="editor-comment"><p>Line 120. Where were the 2 speakers located relative to the participants? +/- 45 degrees? Which sound level were the words presented at?</p></disp-quote><p>We have added this to the text.</p><disp-quote content-type="editor-comment"><p>Lines 127-130. More details about the spoken word task is needed. Is this a standardized test or a novel test that the authors developed themselves?</p></disp-quote><p>This task was first introduced by Peterson et al. 1988 (Petersen, S. E., Fox, P. T., Posner, M. I., Mintun, M. and Raichle, M. E. Positron emission tomographic studies of the cortical anatomy of single-word processing. Nature 331, 585–589 (1988)), It was later replicated using HD-DOT by Eggebrecht et al. 2014, Sherafati et al. 2020, Fishel et al., 2020, in multiple studies to correctly localize the auditory cortex activation using both fMRI and HD-DOT.</p><disp-quote content-type="editor-comment"><p>Line 131. Which 'preliminary results'?</p></disp-quote><p>Clarified and changed to “To better understand the left PFC activity we observed in our first few CI users, we adopted a spatial working memory task introduced in previous studies”.</p><disp-quote content-type="editor-comment"><p>Line 191. &quot;…based on the response … in a group of subjects…&quot;. Which subjects? Merge with relevant information on page 8.</p></disp-quote><p>Implemented.</p><disp-quote content-type="editor-comment"><p>Line 216. Is &quot;Pollonini et al. 2014&quot; cited correctly?</p></disp-quote><p>Yes.</p><disp-quote content-type="editor-comment"><p>Line 238. Why define only left DLPFC as an ROI?</p></disp-quote><p>In our experience with fMRI studies of acoustically-challenging speech, frontal activity seems qualitatively stronger and/or more common in the left hemisphere (see e.g., Davis and Johnsrude, 2003, Journal of Neuroscience).</p><disp-quote content-type="editor-comment"><p>Page 9. The visual spatial working memory task also activated 'auditory' regions in the right hemisphere that look like they overlap with the 'right auditory ROI' created by the authors. Do the authors consider this overlap to be problematic?</p></disp-quote><p>Our focus is on the left PFC region involved in spatial working memory. The observation of left temporal lobe activity is somewhat puzzling but does not affect our ROI definition.</p><disp-quote content-type="editor-comment"><p>Page 9. Legend for Figure 3. &quot;…in the DLPFC region, survived after…&quot;. A word is missing.</p></disp-quote><p>Now fixed.</p><disp-quote content-type="editor-comment"><p>Page 10. Figure 4C suggests that CI &gt; controls contrast identified quite a few group differences. So why then focus on left DLPFC?</p></disp-quote><p>Based on prior literature and our work in the area we had a specific hypothesis regarding left DLPFC. We certainly allow that there may be other areas involved in speech processing for CI users not engaged in controls, but that was not the focus of the current paper.</p><disp-quote content-type="editor-comment"><p>Page 11. Results of two-sample t-tests are reported in the legend of Figure 5. This information should be taken out of the legend and reported in full. Why were t-tests more appropriate than an ANOVA? Please report results of statistical analyses fully i.e. not just p-values. Were reported p-vals corrected for the number of t-tests that were used?</p></disp-quote><p>We have ensured the t statistics and degrees of freedom are in the revised manuscript. The ROI results in Figure 3 (main results) are Bonferroni corrected for multiple comparisons across the three ROIs tested, now also clarified in the text.</p><disp-quote content-type="editor-comment"><p>Page 11. The authors report the results of correlation analyses. Please add further details e.g. are Pearson's correlation coefficients reported?</p></disp-quote><p>Yes, Pearson correlations—now specified.</p><disp-quote content-type="editor-comment"><p>Page 11. The authors report that the threshold used for statistical significance in the correlation analyses was 'uncorrected'. Presumably this means that reported p-vals were not corrected for the 4 correlation analyses shown?</p></disp-quote><p>Correct.</p><disp-quote content-type="editor-comment"><p>Page 12. Correlation analyses for CI users. The rationale for these analyses is missing. Why carry out simple correlation analyses on data for CI users only? Potential predictors of speech processing (4PTA) could have been used as a covariate in the main analyses (e.g. Figure 5).</p></disp-quote><p>The CI listeners were our primary group of interest, and we also assumed they would show more variability in these covariate measures (supported by the data). We completely agree that some of these covariates might also be informative in the control listeners, but we worried there might be a confound (if controls and CI users differ both in group membership and 4PTA, for example, it would cause problems with interpretation). So, we opted to restrict these exploratory analyses to the patient group of interest.</p></body></sub-article></article>