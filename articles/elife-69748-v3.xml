<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">69748</article-id><article-id pub-id-type="doi">10.7554/eLife.69748</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Sex differences in learning from exploration</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-235924"><name><surname>Chen</surname><given-names>Cathy S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2506-8522</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-236667"><name><surname>Knep</surname><given-names>Evan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-236668"><name><surname>Han</surname><given-names>Autumn</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-51290"><name><surname>Ebitz</surname><given-names>R Becket</given-names></name><email>rebitz@gmail.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-182917"><name><surname>Grissom</surname><given-names>Nicola M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3630-8130</contrib-id><email>ngrissom@umn.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, University of Minnesota</institution><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Neurosciences, University of Montreal</institution><addr-line><named-content content-type="city">Quebec</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>19</day><month>11</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e69748</elocation-id><history><date date-type="received" iso-8601-date="2021-04-24"><day>24</day><month>04</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-11-18"><day>18</day><month>11</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2020-12-29"><day>29</day><month>12</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.12.29.424773"/></event></pub-history><permissions><copyright-statement>© 2021, Chen et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Chen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-69748-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-69748-figures-v3.pdf"/><abstract><p>Sex-based modulation of cognitive processes could set the stage for individual differences in vulnerability to neuropsychiatric disorders. While value-based decision making processes in particular have been proposed to be influenced by sex differences, the overall correct performance in decision making tasks often show variable or minimal differences across sexes. Computational tools allow us to uncover latent variables that define different decision making approaches, even in animals with similar correct performance. Here, we quantify sex differences in mice in the latent variables underlying behavior in a classic value-based decision making task: a restless two-armed bandit. While male and female mice had similar accuracy, they achieved this performance via different patterns of exploration. Male mice tended to make more exploratory choices overall, largely because they appeared to get ‘stuck’ in exploration once they had started. Female mice tended to explore less but learned more quickly during exploration. Together, these results suggest that sex exerts stronger influences on decision making during periods of learning and exploration than during stable choices. Exploration during decision making is altered in people diagnosed with addictions, depression, and neurodevelopmental disabilities, pinpointing the neural mechanisms of exploration as a highly translational avenue for conferring sex-modulated vulnerability to neuropsychiatric diagnoses.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>When faced with a decision to make, humans and other animals reflect on past experiences of similar situations to choose the best option. However, in an uncertain situation, this decision process requires balancing two competing priorities: exploiting options that are expected to be rewarding (exploitation), and exploring alternatives that could be more valuable (exploration).</p><p>Decision making and exploration are disrupted in many mental disorders, some of which can differ in either presentation or risk of development across women and men. This raises the question of whether sex differences in exploration and exploitation could contribute to the vulnerability to these conditions. To shed light on this question, Chen et al. studied exploration in male and female mice as they played a video game.</p><p>The mice had the option to touch one of two locations on a screen for a chance to win a small reward. The likelihood of success was different between the two options, and so the mice were incentivized to determine which was the more rewarding button. While the mice were similarly successful in finding rewards regardless of sex, on average male mice were more likely to keep exploring between the options while female mice more quickly gained confidence in an option. These differences were stronger during uncertain periods of learning and exploration than when making choices in a well-known situation, indicating that periods of uncertainty are when the influence of sex on cognition are most visible.</p><p>However, not every female or male mouse was the same – there was as much variability within a sex as was seen between sexes. These results indicate that sex mechanisms, along with many other influences cause individual differences in the cognitive processes important for decision making. The approach used by Chen et al. could help to study individual differences in cognition in other species, and shed light on how individual differences in decision-making processes could contribute to risk and resilience to mental disorders.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>sex differences</kwd><kwd>exploration</kwd><kwd>decision making</kwd><kwd>explore-exploit</kwd><kwd>reinforcement learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01MH123661</award-id><principal-award-recipient><name><surname>Grissom</surname><given-names>Nicola M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P50MH119569</award-id><principal-award-recipient><name><surname>Grissom</surname><given-names>Nicola M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000874</institution-id><institution>Brain and Behavior Research Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Ebitz</surname><given-names>R Becket</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Mistletoe Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Ebitz</surname><given-names>R Becket</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000156</institution-id><institution>Fonds de Recherche du Québec - Santé</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Ebitz</surname><given-names>R Becket</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007249</institution-id><institution>University of Minnesota</institution></institution-wrap></funding-source><award-id>University of Minnesota start-up funds</award-id><principal-award-recipient><name><surname>Grissom</surname><given-names>Nicola M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new computational analysis of decision making in mice shows sex biases in value-updating while exploring unknown options, with male mice tending to explore longer than females due to updating values more slowly.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Almost every neuropsychiatric condition shows sex and/or gender biases in risk, presentation, etiology, and prognosis (<xref ref-type="bibr" rid="bib24">Green et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Grissom and Reyes, 2019</xref>; <xref ref-type="bibr" rid="bib44">Shansky, 2019</xref>). This raises the possibility that sex-modulated biological mechanisms could modulate cognitive processes that confer vulnerability and/or resilience to mental health challenges. However, sex differences in cognitive task performance can be difficult to detect and even more variable than would be expected given the non-dichotomous, overlapping impacts of sex mechanisms on cognition (<xref ref-type="bibr" rid="bib37">Maney, 2016</xref>). An underrecognized source of variability in cognitive tasks is that there can be multiple ways to achieve the same level of performance on the primary dependent variables used to assess these tasks, such as ‘number of correct responses’. This means that equivalent levels of performance could mask individual differences in how males and females are solving the same problem. Indeed, we have recently shown that examining the latent strategies underlying task performance --rather than differences in final performance--can reveal that individual males and females can take very different strategic paths to the learning of action-outcome associations (<xref ref-type="bibr" rid="bib13">Chen et al., 2021b</xref>). Here, we applied computational tools to characterize sex differences in the latent variables underlying behavior to understand sex differences in a key cognitive process regulating reward-guided behaviors: balancing exploration and exploitation.</p><p>In an uncertain world, we must balance two goals: exploiting rewarding options when they are available, but also exploring alternatives that could be more rewarding or provide new information about the world. Too little exploration makes behavior inflexible and perseverative. Too much makes it impossible to sustain rewarding behaviors. Exploration is dysregulated in numerous neuropsychiatric disorders (<xref ref-type="bibr" rid="bib2">Addicott et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Wilson et al., 2021</xref>), many of which are also sex-biased (<xref ref-type="bibr" rid="bib24">Green et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Grissom and Reyes, 2019</xref>; <xref ref-type="bibr" rid="bib44">Shansky, 2019</xref>). This suggests that sex differences in exploration and exploitation could contribute to sex-linked vulnerability to these conditions, though we do not yet understand how exploration and exploitation differ with sex. Because exploration is a major source of errors in task performance more broadly (<xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Pisupati et al., 2019</xref>), sex-differences in exploration could contribute to performance differences and variability seen across tasks and speciess (<xref ref-type="bibr" rid="bib25">Grissom and Reyes, 2019</xref>; <xref ref-type="bibr" rid="bib45">van den Bos et al., 2013</xref>).</p><p>To examine whether there are sex differences in exploration, we trained male and female mice on a classic explore/exploit task, a spatial restless two-armed bandit (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). Males showed higher levels of exploration than females throughout the task. This was because males were more likely to get ‘stuck’ in extended periods of exploration before committing to a favored choice. On the other hand, females showed elevated reward learning specifically during bouts of exploration, making exploratory trials more informative, which allowed them to start exploiting a favored choice earlier than males. Together, these results demonstrate that while the overall performance was similar, males and females exhibited different patterns of exploration while interacting with the same uncertain environment.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Age-matched male and female wild-type mice (n = 32, 16 per sex, strain B6129SF1/J, powered to detect differences in decision making <xref ref-type="bibr" rid="bib13">Chen et al., 2021b</xref>) were trained to perform a restless two-armed spatial bandit task in touch-screen operant chambers (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Animals were presented with two physically identical targets (squares) on the left and right of the screen each trial and indicated their choices by nose poking at one of two target locations. Each location offered some probability of reward, which changed slowly and randomly across trials, and independently across targets. The dynamic reward contingencies encouraged the animals to constantly balance exploration and exploitation. The animals had to exploit a good option when it was found, but also occasionally explore the other option, whose drifting values meant that it could become better at any time. Mice performed two repetitions of four consecutive sessions of the restless bandit task, measuring eight sessions in total. Each session consisted of 300 trials.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Male and female mice showed different exploratory strategies in a restless bandit task - males explored more than females, and they explored for longer periods of time once started.</title><p>(<bold>A</bold>) Schematic of the mouse touchscreen chamber with the restless two-armed bandit task and trial structure. (<bold>B</bold>) Average probability of obtaining reward compared to the chance probability of reward across individuals (dots). (<bold>C</bold>) Average probability of obtaining reward compared to the chance probability of reward across sexes. (<bold>D</bold>) Average response time across sexes. Females responded significantly faster than did males. (<bold>E</bold>) (left) A hidden Markov model that labeled exploration and exploitation as latent goal states underlying observed choices. This model includes an exploitation state for each arm and an exploration state where the subject chooses one of the arms randomly. (right) Reward probabilities (lines) and choices (dots) for 300 example trials for a given mouse. Shaded areas highlight explore-labeled choices. (<bold>F, G</bold>) Average (<bold>F</bold>) and distribution (<bold>G</bold>) of the percentage of Hidden Markov Model (HMM)-labeled exploratory trials in females and males. (<bold>H</bold>) Dynamic landscape of the fitted HMMs for males and females. The model fit to males had deeper exploratory states, with higher activation energy between the states. * indicates p &lt; 0.05. Graphs depict mean ± SEM across animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig1-v3.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Male and female mice had reached asymptotic performance.</title><p>There is no change in reward acquisition, response time, and reward retrieval time across days. (<bold>A</bold>) Average probability of obtaining reward compared to the chance probability of reward across days in male and female mice. (<bold>B</bold>) Average response time across days in male and female mice. (<bold>C</bold>) Average reward retrieval time across days in male and female mice.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig1-figsupp1-v3.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Two time constants combined best describe the rate of switching choices in animals’ choice behavior and Hidden Markov model validation.</title><p>Related to <xref ref-type="fig" rid="fig1">Figure 1D</xref>. (<bold>A</bold>) The tetrachoric correlation (r) between RL model-inferred explore-exploit states and HMM-inferred states. (<bold>B</bold>) The standardized regression coefficient (beta coefficients) of RL model-inferred states and HMM-inferred states in predicting response time. (<bold>C</bold>) The distribution of times between switch decisions (inter-switch intervals). A single probability of switching would produce exponentially distributed inter-switch intervals. Orange line, the maximum likelihood fit for a single discrete exponential distribution. Solid blue line, a mixture of two exponential distributions, with each component distribution in dotted blue. The two components reflect one fast-switching time constant (average interval, 1.7 trials) and one persistent time constant (6.8 trials). The right plot is the same as the left, but with a log scale. Inset is the log likelihood of mixtures of different numbers of exponential distributions. (<bold>D</bold>) Probability of choice as a function of value differences between choices for exploratory and exploitative states. (<bold>E</bold>) Difference in choice response time between explore and exploit choices. (<bold>F</bold>) The probability of animals switching targets on the next trial, given the current trial’s outcome and latent state. (<bold>G</bold>) Difference in choice response time between explore and exploit choices. There is no significant difference in retrieval time between two latent states, suggesting that exploration was not merely disengagement from the task. * indicates p &lt; 0.05. Graphs depict mean ± SEM across animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig1-figsupp2-v3.tif"/></fig></fig-group><p>It is worth noting that unlike other versions of bandit tasks such as the reversal learning task, in the restless bandit task, animals were encouraged to learn about the most rewarding choice(s). There is no asymptotic performance during the task because the reward probability of each choice constantly changes. The performance is best measured by the amount of obtained reward. Prior to data collection, both male and female mice had learned to perform this task in the touchscreen operant chamber. To examine whether mice had learned the task, we first calculated the average probability of reward acquisition across sessions in males and females (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). There was no significant change in the reward acquisition performance across sessions in both sexes, demonstrating that both males and females have learned to perform the task and had reached an asymptotic level of performance (two-way repeated measure ANOVA, main effect of session, p = 0.71). Then we examine two other primary behavioral metrics across sessions that are associated with learning: response time and reward retrieval time (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A,C</xref>). Response time was calculated as the time elapsed between the display onset and the time when the nose poke response was completed. Reward retrieval time was measured as the time elapsed between nose-poke response and magazine entry for reward collection. There was no significant change in response time (two-way repeated measure ANOVA, main effect of session, p = 0.39) and reward retrieval time (main effect of session, p = 0.71) across sessions in both sexes, which again demonstrated that both sexes have learned how to perform the task. Since both sexes have learned to perform the task prior to data collection, variabilities in task performance are results of how animals learned and adapted their choices in response to the changing reward contingencies.</p><p>To examine task performance, we first calculated the average probability of reward obtained in males and females. Because reward schedules were stochastic, sessions could differ slightly in the amount of reward that was available. We therefore compared performance against the average probability of reward if chosen randomly within each session. Regardless of sex, mice were able to earn reward more frequently than chance (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, two-way ANOVA, F(1, 60) = 228.9, p &lt; 0.0001). There was no significant sex difference in the probability of rewards acquired above chance (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, main effect of sex, F(1, 30) = 0.05, p = 0.83). While the mean of percent reward obtained did not differ across sexes, we consider the possibility that the distribution of reward acquisition in males and females might be different. We conducted the Kolmogorov-Smirnov (KS) test, which takes into account not only the means of the distributions but also the shapes of the distributions. The KS test suggested that males and females are not just not significantly different in their reward acquisition performance (Kolmogorov-Smirnov D = 0.1875, p = 0.94), but that males and females have the same distributions for reward acquisition. This result demonstrates equivalently strong understanding and identical performance of the task in males and females.</p><p>Similar levels of accuracy do not require or imply a similar approach to the task. Our previous study suggested that males and females could achieve similar learning performance via divergent decision making strategies (<xref ref-type="bibr" rid="bib13">Chen et al., 2021b</xref>). However, different strategies might take different amounts of time to execute (<xref ref-type="bibr" rid="bib13">Chen et al., 2021b</xref>; <xref ref-type="bibr" rid="bib20">Filipowicz et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Kool et al., 2010</xref>; <xref ref-type="bibr" rid="bib35">Kurdi et al., 2019</xref>). Therefore, we examined the response time, which was calculated as time elapsed between choice display onset and nose poke response as recorded by the touchscreen chamber, in both males and females. If males and females had adopted different strategies here, then we might expect response time to systematically differ between males and females, despite the similarities in learning performance. Indeed, females responded significantly faster than did males (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, main effect of sex, t (30) = 3.52, p = 0.0014), suggesting that decision making computations may differ across sexes and, if so, that the strategies that tended to be used by females resulted in faster choice response time than those used by males.</p><sec id="s2-1"><title>A hidden Markov Model (HMM) identifies distinct features of exploratory and exploitative choices in mice</title><p>Despite similar performance, response time differences suggested that males and females employed different strategies in this task. One possible difference was sex differences in the level of exploration. Prior research has shown that exploratory choices take longer than exploitative choices (<xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). Therefore, perhaps males took longer to make a choice because a greater proportion of their choices were exploratory. To test this hypothesis, we first need a method to label each choice as an exploratory choice or an exploitative choice. In some previous studies, reinforcement learning (RL) models were used to quantify exploration (<xref ref-type="bibr" rid="bib14">Cinotti et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib29">Ishii et al., 2002</xref>; <xref ref-type="bibr" rid="bib41">Pearson et al., 2009</xref>) via labeling choices that deviate from model values as exploratory. This approach is based on the rationale that exploration is a non-reward maximizing goal. However, a non-reward maximizing goal would produce choices that are orthogonal to reward value, not errors of reward maximization (<xref ref-type="bibr" rid="bib4">Averbeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). Therefore, recent studies have turned to an approach, which models exploration as a latent state underlying behavior via a Hidden Markov model (HMM), rather than inferring it from assumptions about values and learning (<xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Muller et al., 2019</xref>).</p><p>The HMM method has not previously been used to quantify exploration in mice, so we first asked whether it was appropriate here. The method works because sequences of exploratory decisions look very different from exploitative ones, at least in reinforcement learning agents and primates (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). When agents exploit, they repeatedly sample the same option, switching only very rarely. However, because exploration requires investigatory samples, runs of exploratory choices tend not to repeat the same option. They tend to switch far more frequently, closer to what we would expect from random samples from the environment (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). Therefore, if mice were alternating between exploration and exploitation in this task, we would expect to see evidence of two distinct patterns of switching in their behavior. Indeed, choice run durations (i.e. the distribution of inter-switch intervals) were parsimoniously described as a mixture of two different patterns: one regime where choices switched quickly (mean switching time = 1.7 trials, compared to random choices at two trials; 80 % of choice runs) and one regime where they changed slowly (mean switching time = 6.8 trials; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). Thus, mice had evidence of fast-switching (putatively explore) and slow-switching (putatively exploit) regimes in their behavior. Note that explore-labeled choices are more likely to also be switch choices, but not all explore-labeled choices are switches, and not all exploit-labeled choices are stay decisions.</p><p>To determine whether the novel HMM method produced more accurate labels than the previous RL method, we conducted a side-by-side comparison to examine how well each set of labels accounted for behavior. We first examined the correlation between explore-exploit states inferred by the HMM model and the RL model. We calculated the tetrachoric correlation between HMM-inferred and RL-inferred states (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>). The tetrachoric correlation (r<sub>tet</sub>) is specifically used to measure rater agreement for binary data and reveals how strong the association is between labels by two methods. The mean correlation was 0.42 with a standard deviation of 0.14, which is medium level agreement.</p><p>Next, to examine whether the states inferred by these models also produced differences in behavioral metrics other than choices, we computed average response time for explore trials and exploit trials labeled by the RL model and HMM model. The result suggested that response time was significantly longer during HMM-inferred exploration than exploitation (paired t-test, t(31) = 3.66, p = 0.0009), which is consistent with previous findings that exploration slows down decision making (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). Like HMM-inferred states, RL inferred explore-exploit states showed similar effects on response time - response time was significantly longer during exploration than exploitation (paired t-test, t(31) = 2.08, p = 0.046). However, the effect size of HMM labels on response time was over twice as big as that of RL labels (HMM: R<sup>2</sup> = 0.30; RL: R<sup>2</sup> = 0.12).</p><p>Finally, we calculated the standardized regression coefficients to measure how much of the response time is explained by states labeled by HMM model and RL model (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). The result suggested that the HMM-inferred states explained significantly more variance in response time than the RL-inferred states in explaining response time. The HMM allows us to make statistical inferences about the probability that each choice was due to exploration or exploitation via modeling these as the latent goal states underlying choice (see Materials and methods). Because this approach to infer exploration is agnostic to the generative computations and depends only on the temporal statistics of choices (<xref ref-type="bibr" rid="bib18">Ebitz et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Wilson et al., 2021</xref>), it is particularly ideal for circumstances like this one, where we suspect that the generative computations may differ across groups.</p><p>Since various factors could influence state of the next trial, we considered a simple two parameter HMM that models only two states (exploration and exploitation), a four-parameter input-output HMM (ioHMM) that allows reward outcome to influence the probability of transitioning between states, and a four-parameter unrestricted HMM with no promoter tying (ntHMM) that allows biased exploitation (see Materials and methods). The model comparisons have shown that the two parameter HMM was the simplest, most interpretable, and best fit model (AIC: 2 parameter HMM, AIC = 2976.1; ioHMM, AIC = 3117; ntHMM, AIC = 3101.5, see more statistics reported in Materials and methods). Therefore, we selected the simple two-parameter HMM to infer the likelihood that each choice was part of the exploratory regime, or the exploitative one (see Materials and methods). To evaluate the face validity of the HMM labels, we asked whether HMM-labeled exploratory choices matched the normative definition of the term. First, by definition, exploration is a pattern of non-reward-maximizing choices whose purpose is learning about rewards. This means exploratory choices should be (1) orthogonal to reward value, and (2) exhibit enhanced reward learning.</p><p>Explore-labeled choices were non-reward-maximizing: they were orthogonal to reward value (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>; the average value of choices chosen during exploration was not different from chance; one sample t-test, t(10) = 0.16, p = 0.87). Reward learning was also elevated during exploration. During HMM-labeled exploratory states, the outcome of choices had more influence on the subsequent decision - animals were more likely to stay with the same choice if rewarded and switch if not rewarded (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1F</xref>, two-way RM ANOVA, interaction term, F(1,31) = 51.2, p &lt; 0.001).</p><p>Differences in response time across HMM-inferred states suggested that these labels produced meaningful differences in primary behavioral metrics. To eliminate the possibility that exploration was merely disengagement from the task, we examined average reward retrieval time during exploratory and exploitative states. There was no significant difference in reward retrieval time between two states (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1G</xref>, t-test, t(31) = 0.05, p = 0.95), suggesting that animals were not more disengaged from the task during exploration than exploitation. They were only slower in making a decision. Together, these results demonstrated that HMM-labeled exploration was meaningful, non-reward-maximizing, and accompanied by enhanced reward learning, matching the normative definition of exploration.</p></sec><sec id="s2-2"><title>Males made more exploratory choices than females because they explored for longer periods of time once they started</title><p>With more confidence in the validity of HMM-inferred states, we found that males, on average, were more likely to be in the exploratory regime than the exploitative one, with 72.9% ± 11.5% STD of trials labeled as exploratory (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Females explored much less with only 55.4% ± 20.4% STD of trials being exploratory (<xref ref-type="fig" rid="fig1">Figure 1F</xref>, t-test, t(30) = 2.98, p = 0.0056; 95% CI for the difference between the sexes = [5.5%, 29.3%]). As groups, males and females were reasonably, but not perfectly discriminable in terms of the proportion of exploratory choices (<xref ref-type="fig" rid="fig1">Figure 1G</xref>, receiver operating characteristic analysis, AUC = 0.76 ± 0.09, 95% CI for the difference = [0.59, 0.93], p = 0.013). These differences were largely driven by the greater male tendency to keep exploring once they started. Males repeated exploration 92.1 % ( ± 3.4 % STD) of the time, while females stopped exploring and committed to a choice more quickly, repeating exploration only 83.1 % ( ± 16.8 % STD) of the time (t-test, t(30) = 2.09, <italic>P</italic> = 0.045, 95% CI for the difference = [0.2%, 17.8%]). There were no significant differences in the other parameter of the HMM (probability of repeating exploitation: males = 83.5% ± 3.7%; females = 79.7 ± 22.1 %; t(30) = 0.69, p = 0.5). Since males had more exploratory trials, which took longer, we tested the possibility that the sex difference in response time was due to prolonged exploration in male by calculating a two-way ANOVA between explore-exploit state and sex in predicting response time. There was a significant main effect of state (main effect of state: F (1,30) = 13.07, p = 0.0011), but males were slower during females during both exploitation and exploration (main effect of sex, F(1,30) = 14.15, p = 0.0007) and there was no significant interaction (F (1,30) = 0.279, p = 0.60). We also examine whether the probability of exploration changed over trials or across sessions by calculating the probability of exploration early, mid, and late within one session and across sessions. However, we failed to see changes in the amount of exploration within sessions and across sessions in both males and females.</p><p>Although sex differences in model parameters were modest, analyzing the full dynamics of the fitted HMMs again supported a robust sex difference in the tendency to explore (see Materials and methods). In models fit to males, exploration was a deeper, more ‘sticky’ behavioral state (<xref ref-type="fig" rid="fig1">Figure 1H</xref>, stationary probability of exploration = 68.0% ± 8.5% STD), compared to models fit to females, where exploration and exploitation were more closely matched (54.4% ± 18.4% STD; different from males: t(30) = 2.68, p = 0.012, 95% CI for the difference = [3.2%, 23.9%]). This suggests that males were more likely to get ‘stuck’ in an extended exploratory period, requiring more energy to escape from exploring and start exploiting a good choice.</p></sec><sec id="s2-3"><title>Multiple variables in reinforcement learning models may be the cause of increased exploration</title><p>The results from the HMM analyses suggest that males were, on average, more exploratory than females, and not because they were more likely to initiate exploration, but because they were more likely to become ‘stuck’ in exploration. This suggests that there were sex differences in the animals’ approach to this task. However, a crucial question remained unanswered: what computational differences made the males more exploratory? To address this question, we turned to reinforcement learning (RL) modeling to look for individual variability in latent cognitive parameters that could influence exploration and exploitation (<xref ref-type="bibr" rid="bib15">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib29">Ishii et al., 2002</xref>; <xref ref-type="bibr" rid="bib32">Jepma and Nieuwenhuis, 2011</xref>; <xref ref-type="bibr" rid="bib41">Pearson et al., 2009</xref>).</p><p>RL models include multiple parameters that could influence exploration. Consider a simple, two-parameter RL model, with one learning rate parameter (α) and one parameter for decision noise (inverse temperature β). Traditionally, only the latter parameter is thought to be related to exploration, and many previous studies of exploration have focused exclusively on this inverse temperature parameter (<xref ref-type="bibr" rid="bib7">Beeler et al., 2010</xref>; <xref ref-type="bibr" rid="bib14">Cinotti et al., 2019</xref>). However, exploration in an RL model should be a function of both the difference in subjective values and the decision-noise in the model. This is because both parameters increase the likelihood that agents will make non-reward-maximizing decisions (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). In the case of decision-noise, this happens because more choices deviate from reward-maximizing policies as noise increases. In the case of learning rate, this happens because the learning rate controls how quickly agents can move away from the regime in which decision-noise is highest. To test our intuition, we simulated data from a simple two-parameter RL model, then used the HMM to infer when and why exploration occurred. As expected, changing the decision-noise parameter (β) robustly changed the probability of exploration (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; GLM, main effect of inverse temperature, β1 = –0.11, p &lt; 0.0001): the larger the inverse temperature, the lower the decision noise and the lower probability of exploration. Critically, learning rate (ɑ) also influenced the probability of exploration (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; GLM, main effect of learning rate, β2 = –0.10, p &lt; 0.0001). In fact, there were some values for learning rate (ɑ) at which changing decision noise had no effect on exploration whatsoever: ɑ and β interacted to influence exploration (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; GLM, interaction term, β3 = –0.38, p &lt; 0.0001). This occurred because when the learning rate was very low, agents failed to move away from the high-decision-noise regime at all, meaning that there was little effect of any additional noise. Thus, even in this simple two-parameter RL model, multiple latent, cognitive variables can influence exploration. Because of this ambiguity, it was not clear whether males explored more frequently because they had more decision-noise, because they learned less from rewards, or because there were changes in other decision-making or learning computations, like the tendency to simply repeat past choices. Fortunately, we can distinguish these possibilities via fitting RL models to the data.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Multiple reinforcement learning parameters can influence the probability of exploration.</title><p>(<bold>A</bold>) Exploration occurs most often when option values are close together, illustrated by the gray shaded boxes in the value-choice functions. Both decreasing inverse temperature (β) and decreasing learning rate increases exploration because each manipulation changes the amount of time spent in the high exploration zone, although the mechanisms are different. Decreasing inverse temperature (β) widens the zone by flattening the value-choice function and increasing decision noise. Decreasing learning rates (α) keeps learners in the zone for longer. (<bold>B</bold>) Probability of exploration from 10,000 different reinforcement learning agents performing this task, initialized at different random combinations of inverse temperatures (β) and learning rates (α). Marginal relationships between decision noise (top) and learning rate (bottom) are shown here. (<bold>C</bold>) Heatmap of all pairwise combinations of learning rate and inverse temperature.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig2-v3.tif"/></fig></sec><sec id="s2-4"><title>Sex differences in exploration was due to changes in the learning rate - females had higher learning rates than males</title><p>The HMM suggested that males and females had different levels of exploration, but it did not provide insight into the latent, cognitive processes behind these differences. Fortunately, RL models allow us to identify differences in a variety of latent cognitive variables that could influence exploration, either alone or in combination. However, our ability to make inferences about changes in model parameters is highly sensitive to the correct specification of the model, so we first had to identify the best-fitting RL model for these animals.</p><p>There are many ways to parameterize RL models (<xref ref-type="bibr" rid="bib33">Katahira, 2018</xref>), the majority of which can be put in three categories: value-dependent learning terms, value-independent bias terms, and decision noise/randomness terms. Previous studies have shown the effect of various RL parameters on decision making, including learning terms such as asymmetrical learning rate (<xref ref-type="bibr" rid="bib21">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Gershman, 2016</xref>), bias terms such as choice bias (<xref ref-type="bibr" rid="bib33">Katahira, 2018</xref>; <xref ref-type="bibr" rid="bib46">Wilson and Collins, 2019</xref>), noise terms such as inverse temperature and lapse rate (<xref ref-type="bibr" rid="bib19">Economides et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Wilson and Collins, 2019</xref>). Here, we compared seven reinforcement learning models that made different assumptions about the latent processes mice might be employing via different combinations of learning, bias, and noise terms. These models included: (1) a ‘random’ model with some overall bias for one choice over the other, (2) a ‘noisy win stay lose shift’ model that assumes a win stay lose shift policy with some level of randomness, (3) a two-parameter ‘RL’ model with a consistent learning rate and some inverse temperature that captures decision noise, (4) a three-parameter ‘RLε’ model with a consistent learning rate, and inverse temperature that captures value-based decision noise, and a value-independent noise, (5) a four-parameter ‘RLCK’ model that captures both value-based and value-independent decision with separate parameters for learning rate, decision noise, choice bias, and choice stickiness, (6) a five-parameter ‘RLCKγ’ model that incorporates differential learning rate for rewarded and unrewarded outcomes on top of the ‘RLCK’ model, (7) a five-parameter ‘RLCKη’ model that adds a parameter that tunes the weight between value-based and choice-based decision to the ‘RLCK’ model (see Materials and methods, <xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Sex differences in learning rate, but not decision noise, drove differences in explore-exploit decisions.</title><p>(<bold>A</bold>) A diagram of latent parameters that capture learning (α), bias (α<sub>c</sub>), inverse temperature (β) in reinforcement learning models. The models tested used a combination of these parameters (see Materials and methods). (<bold>B</bold>) Model comparison across seven reinforcement learning models with various parameter combinations for males and females. The four-parameter reinforcement learning-choice kernel (RLCK) model has the highest relative likelihood in both sexes. (<bold>C</bold>) Model agreement across seven reinforcement learning models, which measures how well a model predicts the actual choices of animals. (<bold>D</bold>) All four parameters in the best fit RLCK model across sexes. Learning rate (α) was significantly higher in females than males. (<bold>E</bold>) Distribution of learning rate across sexes. (<bold>F</bold>) (left) Simulation of reward acquisition of RL agent with different combinations of learning rate (α) and decision noise (β<sup>-1</sup>). Different combinations of learning rate and decision noise can result in the same level of reward performance. Average learning rate and decision noise is overlaid on the heatmap for males and females. (right) relationship between reward acquisition and learning rate or decision noise separately. High learning rate is not equivalent to better learning performance. (<bold>G</bold>) Learning rate in females increased across days, suggestive of meta learning. * indicates p &lt; 0.05. Graphs depict mean ± SEM across animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>The best fit model, the four-parameter reinforcement learning-choice kernel (RLCK) model, captured both value-dependent and value-independent choice behaviors.</title><p>Actual choices (gray) and simulated choices (green) from the best fit model (RLCK) of two example animals. Predictions from the matching law are illustrated as a contrast to the best-fitting RL model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig3-figsupp1-v3.tif"/></fig></fig-group><p>Although model fitting was slightly different across sexes, in both males and females, the “RLCK” model, four-parameter model with value and choice kernel updating policies, best characterized animals’ choice behaviors in this task among all seven models (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The fact that the ‘RLCK’ model was the best-fit model in both males and females does not mean both sexes had the same strategy or that RL modeling cannot capture those strategies. Instead, this may suggest that strategic differences between sexes may be more a matter of degree (i.e.: differences in the specific values of model parameters), rather than a matter of categorically distinct computation. This interpretation also makes the most sense in light of the biology of sex differences, which produce few (if any) truly categorically distinct changes in neural function, but rather serve to bias neural systems across sexes in multiple complex ways.</p><p>To quantify how well each RL model was at predicting animals’ choices, we measured the model agreement for each model, which was calculated as the probability of choice correctly predicted by the optimized model parameters for each model (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Then we conducted a multiple comparison across model agreement of RL models (test statistics reported in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). The results suggested that the RL models with parameter(s) that account for choice bias (RLCK, RLCKγ, RLCKη) were significantly better at predicting animals’ actual choices than the models that do not account for choice bias and non-RL models (random, noisy WSLS, RL, RLε). There was no significant difference in model agreement between RLCK, RLCKγ, and RLCKη. Based on the result of model comparison (AIC) and model agreement, we decided that the four-parameter RLCK model is the simplest, best-fit model that best predicted animal’ actual choices. Finally, to visualize how well the RLCK model was at predicting choices of animals with different learning performance, we plotted the simulated choices and actual choices against the matching law (<xref ref-type="bibr" rid="bib43">Poling et al., 2017</xref>), which dictates that the probability of choice is proportional to the probability of reward. The figure showed that this four-parameter model was able to characterize animals’ choice behaviors regardless of the value-based learning performance (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>The RLCK model had four parameters, which we then compared across sexes. We found that females had significantly higher learning rate (α) than males (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, t(30) = 2.40, p = 0.02) but there was no significant difference across sexes in other parameters (β: t(30) = 1.44, p = 0.16; α<sub>c</sub>: t(30) = 1.40, <italic>P</italic> = 0.17; β<sub>c</sub>: t(30) = 1.73, p = 0.09). To examine whether the higher learning rate in females was driven by a few individuals with extremely high learning rates, we plotted the distribution and calculated the separability of learning rates of two sexes. As groups, males and females were reasonably discriminable in terms of the learning rate (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, receiver operating characteristic analysis, AUC = 0.72 ± 0.09, 95% CI for the difference = [0.54, 0.90], p = 0.035). These results suggested that the difference in the level of exploration between males and females was not due to differences in decision noise, but instead due to differences in learning rate.</p><p>While females had significantly higher learning rate (α) than males, they did not obtain more rewards than males. This is because the learning rate parameter in an RL model does not equate to the learning performance, which is better measured by the number of rewards obtained. The learning rate parameter reflects the rate of value updating from past outcomes. Performing well in this task requires both the ability to learn new information and the ability to hang onto the previously learned information. That occurs when the learning rate is moderate but not maximal. When the learning rate is maximal (<italic>α</italic> = 1), only the outcome of the immediate past trial is taken into account for the current choice. This essentially reduces the strategy to a win-stay lose-shift strategy, where choice is fully dependent on the previous outcome. A higher learning rate in a RL model does not translate to better reward acquisition performance. To illustrate that different combinations of learning rate and decision noise can result in the same reward acquisition performance. We conducted computer simulations of 10,000 RL agents defined by different combinations of learning rate (ɑ) and inverse temperature (β<sup>-1</sup>) and plotted their reward acquisition performance for the restless bandit task (<xref ref-type="fig" rid="fig3">Figure 3F</xref>, temperature instead of inverse temperature was plotted for the ease of presentation). This figure demonstrates that (1) different learning rate and inverse temperature combinations can result in similar performance, (2) the optimal reward acquisition is achieved when learning rate is moderate. This result suggested that not only did males and females had identical performance, their optimized RL parameters put them both within the same predicted performance gradient in this plot.</p><p>One interesting finding is that, when compared learning rate across sessions within sex, females, but not males, showed increased learning rate over experience with task (<xref ref-type="fig" rid="fig3">Figure 3G</xref>, repeated measures ANOVA, female: main effect of time, F (2.26,33.97) = 5.27, p = 0.008; male: main effect of time, F(2.5,37.52) = 0.23, p = 0.84). This points to potential sex differences in meta-learning that could contribute to the differential strategies across sexes.</p></sec><sec id="s2-5"><title>Females learned more during exploratory choices than males</title><p>The results of HMM model and RL models revealed significant sex differences in exploration, paralleled by sex differences in rate of learning. What remains unclear is how sex, explore-exploit states, and reward outcomes all interact together to influence the animals’ choices. Therefore, we conducted a four-way repeated measures ANOVA to examine how (1) positive and negative outcomes, (2) explore-exploit states, (3) sex, and (4) subject identity (nested in sex) all came together to influence choice: whether animals would repeat their last choice (stay) or try a different option (switch; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). This four-way repeated measure ANOVA allowed us to understand the main effect of sex, state, and outcome, as well as all pair and triplet-wise interaction effects, on how animals learned from previous rewards. Note that in previous analyses, we used subject averaged data but since subject average (16 subjects each sex) is underpowered to detect a three-way interaction effect, we used session averaged data to increase the power to detect any effects across sex, state, and outcome.</p><p>The results revealed an expected significant main effect of outcome on stay-switch decisions (main effect of outcome, p &lt; 0.00001). This effect was driven by the tendency of animals to repeat the previous choice (i.e.: not switching) after obtaining a reward (post hoc t-test compared to chance at 0.50, mean = 0.75, 95% CI = [0.74, 0.77], t(255)=34.33, p &lt; 0.0001) and a much smaller tendency to switch more often than chance after reward omission (mean = 0.52, 95% CI = [0.50, 0.55], post hoc t-test, t(255) = 2.05, p = 0.04). The tendency to switch or stay also differed by sex, with females more likely to repeat a previous choice and males more likely to switch (main effect of sex, p &lt; 0.00001; post-hoc t-test on p(switch): t(254) = 4.12, p &lt; 0.0001). There was also a significant interaction effect between sex and outcome (sex X reward interaction, p &lt; 0.00001). To understand how reward and reward omission differentially affect choice across sexes, we conducted post-hoc win-stay lose-shift analyses. We found that female mice displayed more win-stay behaviors, indicating that they were more likely than the males to repeat behaviors that produced reward on the previous trial (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, sex X reward interaction, p &lt; 0.000001, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>; post hoc t-test: t(254) = 5.53, p &lt; 0.0001). As groups, males and females were reasonably, but not perfectly discriminable in terms of the proportion of win stay choices (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, receiver operating characteristic analysis, AUC = 0.74 ± 0.09, 95% CI for the difference = [0.56, 0.92], p = 0.0195). In contrast, male mice tended to shift even when the previous choice was rewarded.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Females used more information about past outcomes and past choices to make decisions, and learned more during exploration.</title><p>(<bold>A, B</bold>) Percent win stay behavior (A: average; B: distribution) reveals that females were more likely to stay with the same choice after a reward. (<bold>C</bold>) Average percentage of lose shift behavior across sexes. (<bold>D</bold>) Probability of shifting after a loss during explore or exploit trials. (<bold>E</bold>) Probability of staying after a win during explore or exploit trials. (<bold>F</bold>) The probability of males and females switching targets on the next trial, given the current trial’s outcome and latent state. Females learned more only during exploratory trials. (<bold>G, H</bold>) Average (<bold>G</bold>) and distribution (<bold>H</bold>) of percentage of mutual information across all trials in females and males reveals that females use more information about past trials (choice and outcome) in making future decisions. * indicates p &lt; 0.05. Graphs depict mean ± SEM across animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Reward learning in explore vs. exploit state across sexes.</title><p>The probability of males and females switching targets on the next trial, given the current trial’s outcome and latent state. Females showed increased reward learning only during exploratory state (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, sex X reward X state interaction, p = 0.0438).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69748-fig4-figsupp1-v3.tif"/></fig></fig-group><p>There was no significant sex difference in learning from losses (<xref ref-type="fig" rid="fig4">Figure 4C</xref> and t(25) = 1.40, p = 0.16), but this did not mean that sex differences in learning were solely due to sex differences in learning from wins. There were at least two ways that we could observe an equivalent tendency to lose-shift across sexes in this task. One possibility is that females only learn more from positive outcomes, but not negative ones. However, the other possibility is that this lack of a difference in lose-shift behaviors between males and females was an artifact of the tendency of males to explore more frequently. Since males spend more time exploring (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) and learning from both wins and losses is enhanced during exploration (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2F</xref>), males could lose-shift less frequently than females during both exploration and exploitation, yet still lose-shift exactly as much as females because a greater proportion of their choices were exploratory.</p><p>To dissociate these possibilities, we next examined the effects of exploration and exploitation. The ANOVA revealed a significant main effect of state, resonating with the result of HMM model validation that animals were more likely to switch during exploratory state than during exploitative state (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, main effect of state, p &lt; 0.00001; sex X state interaction, p = 0.0667; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2F</xref>). Critically, there was also a significant three-way interaction between sex, explore/exploit state, and reward (<xref ref-type="fig" rid="fig4">Figure 4F</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, sex X reward X state interaction, p = 0.0438). This could imply sex-linked differences in reward learning across exploration and exploitation. To determine if this was true, we separated out the probability of lose-shift according to whether it happened during exploration or exploitation, as inferred from the Hidden Markov model. Males switched less after losses than females within exploratory states (<xref ref-type="fig" rid="fig4">Figure 4D</xref>; post hoc t-test on session averages: sex difference within exploration: p &lt; 0.001, t(251) = 3.39), though there was no significant sex difference within exploitation (p = 0.06, t(243) = 1.87; note that differing degrees of freedom are due to the fact that exploitation was not observed in some sessions for some animals). This supported the second hypothesis that males lose-shift less than females both when exploring and when exploiting, but that there was no difference in lose-shift overall because males spent more time in a state in which both win-stay and lose-shift choices occur more frequently (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2F</xref>). We also found that the increased tendency to win-stay that we observed in females was driven by the explore choices (<xref ref-type="fig" rid="fig4">Figure 4E</xref>; post-hoc t-test: p = 0.015, t(251) = 2.55). There was no significant difference in win-stay between males and females during exploit choices (post-hoc t-test: p = 0.09, t(244) = 1.68). Together these results suggest that females were better explorers (i.e.: they had increased reward learning during exploration), whereas males learned slower during exploration but compensated for this learning disadvantage by exploring more frequently.</p><p>These effects were not driven by idiosyncratic strategic differences between the sexes (e.g. shifting only after two losses). We used a model-free approach to quantify the extent to which behavior was structured without making strong assumptions about what form this structure might take. We calculated conditioned mutual information for all sessions across sexes (<xref ref-type="bibr" rid="bib36">Leao et al., 2004</xref>; <xref ref-type="bibr" rid="bib48">Wyner, 1978</xref>), to examine how choice behavior was influenced by information of past choice history, given the immediate outcome. The result suggested that mutual information was higher in females than males, suggesting that females were using more information from the past choice and outcome to make their current decision. (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, t(30) = 2.65, p = 0.013). As groups, males and females were reasonably discriminable in terms of mutual information (<xref ref-type="fig" rid="fig4">Figure 4H</xref>, receiver operating characteristic analysis, AUC = 0.74 ± 0.09, 95% CI for the difference = [0.57, 0.92], p = 0.0195). Together, these results reinforced our conclusion that females were learning more: utilizing more information from the past trial to make current choices.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Sex mechanisms biasing the preferred approaches taken during cognitive tasks are potentially significant contributors to task performance. We used a combination of computational modeling approaches to characterize sex differences in a canonical explore/exploit task. While males and females had similar performance, they used different latent explore-exploit strategies to learn about the dynamic environment. Males explored more than females and were more likely to ‘get stuck’ in an extended period of exploration before committing to a favored choice. In contrast, females were more likely to show elevated learning during the exploratory period, making exploration more efficient and allowing them to start exploiting a favored choice earlier. Furthermore, the learning rate increased over days in females but stayed stable in males. Such meta-learning in females permitted learning about the current task (which option provides the best reward outcome), as well as the structure of the task (the reward probability of choices changes over time). This allowed them to shift more quickly to exploit a rewarding option when they found one and only explored when the current option failed to provide valuable rewards. Together, these results demonstrate that while the overall performance was similar, males and females tended to adopt different strategies for interacting with the same uncertain environment. The difference in explore-exploit strategies across sexes may provide us insight into potential sex-modulated mechanisms that are implicated in learning under uncertainty.</p><p>Our major finding that males learned less during exploration and explored for longer is consistent with two explanations. First, males learned more slowly during exploration and as a result, they had to explore for longer to learn which option was worth exploiting. Another possibility is that males got ‘stuck’ in extended periods of exploration that prohibited them from applying the knowledge they have learned and committing to a rewarding option. In this view, it takes some energy to stop exploring and transition to exploit. As we have shown in our result, males had ‘deeper’ exploratory states and they were more likely to keep exploring once started. These two explanations are not mutually exclusive because changes in learning could contribute to changes in the stickiness of exploration and vice versa. This essentially presents a chicken and egg problem. It is difficult to distinguish from behavior alone whether slower learning drives longer exploration, or vice versa, if being stuck in exploration results in slower learning. Neural measures during explore and exploit choices across sexes may help us differentiate learning signals from signals that drive exploration, and whether these signals are sex-different.</p><p>Answering these neural questions will require a way to reliably identify latent exploration and exploitation states. The Hidden Markov model (HMM) has been used to infer trial-by-trial exploration and exploitation in non-human primates (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). The HMM inferred latent goal states explained more variance in neural activity than other decision-variables (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). In our data, we have shown for the first time that the HMM model was able to label a meaningful exploratory state that matches normative definitions of exploration in the mouse model. In the future, this computational tool can complement neurobiological recording techniques to examine for neural correlates of exploration on a trial-by-trial basis, and permit the visualization of dynamic landscapes of choice behavior across individuals (as in <xref ref-type="fig" rid="fig1">Figure 1H</xref>) or with pharmacological or other challenges (<xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>).</p><p>Reinforcement learning (RL) models have also been used in the past to identify levels of exploration across individuals (<xref ref-type="bibr" rid="bib15">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib41">Pearson et al., 2009</xref>). However, our findings indicate that multiple latent parameters can influence how much these models explore. Here, we found that differences in exploration between the sexes were due to differences in learning rates, not due to differences in the decision noise parameter, which is more commonly associated with exploration. While RL models are helpful for understanding cognitive or computational mechanisms, they are limited in their ability to identify when exploration is happening. The HMM model, conversely, provides no insight into mechanisms, but can tell us precisely when exploration is occurring, both in animal behaviors and RL models (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). By combining the HMM and RL approaches, we capitalized on the advantages of both frameworks: linking changes in exploration across sexes to underlying mechanisms of exploration. Since the broader reinforcement learning model framework is highly adaptive and amenable, in the future, the HMM and kinds of model-free analyses we completed here could also inform the design of RL models to capture explore-exploit decisions more precisely.</p><p>Future work is needed to understand the neurobiological bases of these observations. One neuromodulator that is implicated in reinforcement learning, including the transition between exploration and exploitation, <italic>and</italic> is strongly sex-modulated is dopamine (<xref ref-type="bibr" rid="bib7">Beeler et al., 2010</xref>; <xref ref-type="bibr" rid="bib31">Jenni et al., 2017</xref>). Studies have shown that dopamine signals regulate exploration via mechanisms of action selection and learning (<xref ref-type="bibr" rid="bib7">Beeler et al., 2010</xref>; <xref ref-type="bibr" rid="bib22">Frank et al., 2009</xref>; <xref ref-type="bibr" rid="bib28">Humphries et al., 2012</xref>). However, due to the exclusive use of males in many foundational experiments (<xref ref-type="bibr" rid="bib7">Beeler et al., 2010</xref>; <xref ref-type="bibr" rid="bib14">Cinotti et al., 2019</xref>), the fact that dopamine function on decision making is strongly modulated by sex and sex-linked mechanisms is often overlooked. For example, estradiol has been demonstrated to exert both acute and chronic modulatory effects on dopamine release, reuptake, and receptor binding (<xref ref-type="bibr" rid="bib49">Yoest et al., 2014</xref>), allowing enhanced DA release and faster DA clearance in females (<xref ref-type="bibr" rid="bib6">Becker, 1999</xref>). This mechanism could contribute to increased reward learning observed in females during exploration. The prefrontal cortex (PFC), which receives dopamine projections, is a target brain region to understand exploration and exploitation (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Jenni et al., 2017</xref>). Our previous study implicated PFC in the differences in learning strategy between males and females (<xref ref-type="bibr" rid="bib13">Chen et al., 2021b</xref>). It is possible that prefrontal cortical dopamine is particularly engaged in implementing explore-exploit strategies via sex-biased mechanisms of learning.</p><p>Rodent operant testing is frequently used to assess cognitive functions. This is critical for translational work in animals needed to link pharmacology, genetics, and other potential biological contributors to behavior (<xref ref-type="bibr" rid="bib25">Grissom and Reyes, 2019</xref>). However, many classic rodent cognitive tasks are species-specific: they were not designed to assess the same cognitive processes across species, and this limits their translational ability. Currently, there is an emerging exciting trend among rodent researchers with adopting tasks with translational potentials in rodents, such as reversal learning and various versions of bandit task (<xref ref-type="bibr" rid="bib30">Izquierdo et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Groman et al., 2016</xref>; <xref ref-type="bibr" rid="bib5">Bari et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Grossman et al., 2021</xref>), where we can assess the same cognitive processes across species and across animal models of diseases. Here, we establish that explore-exploit decisions are shared cognitive processes across species, making this restless bandit task an ideal tool for translational research. Parallel approaches in humans have been used to examine the explore-exploit strategic phenotype of neuropsychiatric disorders, including ADHD, addiction, and depression (<xref ref-type="bibr" rid="bib3">Addicott et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Addicott et al., 2012</xref>; <xref ref-type="bibr" rid="bib8">Beeler et al., 2012</xref>; <xref ref-type="bibr" rid="bib11">Blanco et al., 2013</xref>). The computational modeling used here permits fine grained quantification of individual variability in latent parameters that capture adaptive changes in exploration in changing environments. The computational approaches we develop here could help identify behavioral endophenotypes across species underlying a variety of neuropsychiatric disorders and open up new avenues for understanding as well as rescuing dysfunction in value-based decision making.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Strain, strain background (mouse)</td><td align="left" valign="bottom">B6129SF1/J</td><td align="left" valign="bottom">The Jackson Laboratory</td><td align="left" valign="bottom">JAX: 101,043</td><td align="left" valign="bottom"> </td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Python 3</td><td align="left" valign="bottom">Python</td><td align="left" valign="bottom">SCR_008394</td><td align="left" valign="bottom"> </td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">MATLAB R2013a</td><td align="left" valign="bottom">MathWorks</td><td align="left" valign="bottom">SCR_001622</td><td align="left" valign="bottom"> </td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Animals</title><p>Thirty-two BL6129SF1/J mice (16 males and 16 females) were obtained from Jackson Laboratories (stock #101043). Mice arrived at the lab at 7 weeks of age, and they were housed in groups of four with ad libitum access to water while being mildly food restricted (85–95% of free feeding weight) for the experiment. Animals engaging in operant testing were housed in a 0900–2100 hours reversed light cycle to permit testing during the dark period. Before operant chamber training, animals were food restricted to 85–90% of free feeding body weight. Operant testing occurred five days per week (Monday-Friday). All animals were cared for according to the guidelines of the National Institution of Health and the University of Minnesota.</p></sec><sec id="s4-2"><title>Apparatus</title><p>Sixteen identical triangular touchscreen operant chambers (Lafayette Instrument Co., Lafayette, IN) were used for training and testing. Two walls were black acrylic plastic. The third wall housed the touchscreen and was positioned directly opposite the magazine. The magazine provided liquid reinforcer (Ensure) delivered by a peristaltic pump, typically 7 µl (280 ms pump duration). ABET-II software (Lafayette Instrument Co., Lafayette, IN) was used to program operant schedules and to analyze all data from training and testing.</p></sec><sec id="s4-3"><title>Behavioral task</title><sec id="s4-3-1"><title>Two-armed spatial restless bandit task</title><p>Animals were trained to perform a two-armed spatial restless bandit task in the touchscreen operant chamber. Each trial, animals were presented with two identical squares on the left and right side of the screen. Nose poke to one of the target locations on the touchscreen was required to register a response. Each location is associated with some probability of reward, which changes independently over time. For every trial, there is a 10 % chance that the reward probability of a given arm will increase or decrease by 10 %. All the walks were generated randomly with a few criteria: (1) the overall reward probabilities of two arms are within 2 % of each other, preventing one arm being overly better than the other, (2) the reward probability cannot go down to 0 % or go up to 100%, (3) there are no 30 consecutive trials where the reward probabilities of both arms are lower than 20 % to ensure motivation. Animals ran a simple deterministic schedule on Monday to re-adapt to operant chamber after weekends off and ran a different restless bandit task each day from Tuesday to Friday. Animals ran for two rounds of four consecutive days and within each day, animals completed either 300 trials or spent a maximum of two hours in the operant chamber. On average across all sessions, animals performed 276.5 trials with a standard deviation of 8.6 trials (male average: 253.7 trials, sd = 15.4; female average 299.3 trials, sd = 0.74). Data was recorded by the ABET II system and was exported for further analysis. All computational modeling was conducted using python. All behavioral data have been deposited in generic database (Dyrad) with accession link <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.z612jm6c0">https://doi.org/10.5061/dryad.z612jm6c0</ext-link>. Codes used can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/CathySChen/restlessBandit2021">https://github.com/CathySChen/restlessBandit2021</ext-link> (<xref ref-type="bibr" rid="bib12">Chen, 2021a</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:57931052452f122677d3155e11498335b0d1c672;origin=https://github.com/CathySChen/restlessBandit2021;visit=swh:1:snp:b7d93688ef0aec8bf0ce93907a9d453e9ec71b69;anchor=swh:1:rev:a0a377707627f93f3637e1520f9d1304121dcf1a">swh:1:rev:a0a377707627f93f3637e1520f9d1304121dcf1a</ext-link>).</p></sec></sec><sec id="s4-4"><title>Data analysis</title><sec id="s4-4-1"><title>General analysis techniques</title><p>Data was analyzed with custom PYTHON, MATLAB, and Prism eight scripts. Generalized linear models, ANOVA, and t-test were used to determine sex differences over time, unless otherwise specified. p Values were compared against the standard ɑ = 0.05 threshold. The sample size is n = 16 for both males and females for all statistical tests. No animal was excluded from the experiment. One outlier was removed in one analysis using ROUT method (with Q set to 1%). This outlier was from the animal that ran the lowest number of total trials. Statistics for both no outlier removal and outlier removal were reported in the result. All statistical tests used and statistical details were reported in the results. All figures depict mean ± SEM.</p></sec><sec id="s4-4-2"><title>Mixture model</title><p>We first asked whether there were different behavioral dynamics that might correspond to exploration and exploitation. Exploration and exploitation take place on different time scales. In RL agents, for example, exploration is typically implemented via adding noise or indeterminacy to a decision-rule. The identity of choices that are caused by this noise—the exploratory choices—will thus switch more frequently than the identity of choices that depend on option value. We should see short runs of exploratory choices and long runs of exploitative ones (<xref ref-type="bibr" rid="bib16">Ebitz et al., 2018</xref>). To the extent that choice runs end probabilistically (an assumption of the HMM framework), choice run durations (inter-switch intervals) will be exponentially distributed (<xref ref-type="bibr" rid="bib9">Berg, 1993</xref>). Since there exist multiple causal regimes (such as exploration and exploitation), inter-switch intervals will be distributed as a mixture of multiple exponential distributions (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). Because trials are discrete, rather than continuous, we fit mixtures of the discrete equivalent to the exponential distribution, the geometric distribution. We examined the distribution of 24,836 interswitch intervals. Adding a second mixing distribution significantly improved model fit (one-component, one-parameter mixture log-likelihood: –44555, two-component, three-parameter: –41656; likelihood ratio test, p &lt; 10<sup>–32</sup>). Adding additional mixing distributions continued to improve model fit, a common observation in mixture modeling. However, the continued improvement was substantially less than the leap from one to two components (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>; three-component: –41431, four-component: –41412) and additional mixtures beyond two had weights below 3 %. This suggests that a mixture of one fast-switching regime and one slow-switching regime was the most parsimonious explanation for the data (<xref ref-type="bibr" rid="bib38">McLachlan and Peel, 2004</xref>).</p></sec><sec id="s4-4-3"><title>Hidden Markov model (HMM)</title><p>In order to identify when animals were exploring or exploiting, we fit an HMM. In an HMM framework, choices (y) are ‘emissions’ that are generated by an unobserved decision process that is in some latent, hidden state (z). Latent states are defined by both the probability of making each choice (k, out of N<sub>k</sub> possible options), and by the probability of transitioning from each state to every other state. Our model consisted of two types of states, the explore state and the exploit state. The emissions model for the explore state was uniform across the options. The emissions model for the explore state was uniform across the options:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>This is simply the maximum entropy distribution for a categorical variable - the distribution that makes the fewest number of assumptions about the true distribution and thus does not bias the model towards or away from any particular type of high-entropy choice period. This does not require, imply, impose, or exclude that decision-making happening under exploration is random. <xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref> have shown that exploration was highly structured and information-maximizing, despite being modeled as a uniform distribution over choices (<xref ref-type="bibr" rid="bib18">Ebitz et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>). Because exploitation involves repeated sampling of each option, exploit states only permitted choice emissions that matched one option. That is:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∉</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The latent states in this model are Markovian, meaning that they are time-independent. They depend only on the most recent state (z<sub>t</sub>):<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∨</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This means that we can describe the entire pattern of dynamics in terms of a single transition matrix. This matrix is a system of stochastic equations describing the one-time-step probability of transitioning between every combination of past and future states (i, j).<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, there were three possible states (two exploit states and one explore state) but parameters were tied across exploit states such that each exploit state had the same probability of beginning (from exploring) and of sustaining itself. Transitions out of the exploration, into exploitative states, were similarly tied. The model also assumed that the mice had to pass through exploration in order to start exploiting a new option, even if only for a single trial. This is because the utility of exploration is to maximize information about the environment, as defined in both animal foraging literature and reinforcement learning models (<xref ref-type="bibr" rid="bib39">Mehlhorn et al., 2015</xref>). If an animal switches from a bout of exploiting one option to another option, that very first trial after switching should be exploratory because the outcome or reward contingency of that new option is unknown and that behavior of switching aims to gain information. Through fixing the emissions model, constraining the structure of the transmission matrix, and tying the parameters, the final HMM had only two free parameters: one corresponding to the probability of exploring, given exploration on the last trial, and one corresponding to the probability of exploiting, given exploitation on the last trial.</p><p>The model was fit via expectation-maximization using the Baum Welch algorithm (<xref ref-type="bibr" rid="bib10">Bilmes, 1998</xref>). This algorithm finds a (possibly local) maxima of the complete-data likelihood. A complete set of parameters θ includes the emission and transition models, discussed already, but also initial distribution over states, typically denoted as PI. Because the mice had no knowledge of the environment at the first trial of the session, we assumed they began by exploring, rather than adding another parameter to the model here. The algorithm was reinitialized with random seeds 20 times, and the model that maximized the observed (incomplete) data log likelihood across all the sessions for each animal was ultimately taken as the best. To decode latent states from choices, we used the Viterbi algorithm to discover the most probable a posteriori sequence of latent states.</p><p>To account for the effect of reward on choice dynamics, we extended the two-parameter HMM model to an input-output HMM model (4-parameter ioHMM), whose framework allows inputs, such as reward outcomes, to influence the probability of transitioning between states (Bengio and Frasconi; <xref ref-type="bibr" rid="bib17">Ebitz et al., 2019</xref>). The ioHMM model improved model fit (two-parameter original HMM: log-likelihood = –1424.0; 4-parameter ioHMM: log-likelihood = –1430.5). Typically, improved model fit is expected with the addition of parameters. To determine whether it’s a meaningful improvement of model fit that justifies doubling the number of parameters, we calculated AIC and BIC for model comparison. The result of model comparison using both AIC suggested that the original two-parameter model was the better model (AIC: two-parameter original HMM: AIC = 2976.1; four-parameter ioHMM: AIC = 3117; relative likelihood (AIC weight) of the four-parameter ioHMM &lt;10^–30). BIC test has a even larger penalty for more parameters and therefore selected against the four-parameter ioHMM (BIC: two-parameter original HMM: BIC = 3562.8; two-parameter ioHMM: BIC = 4290.4; relative likelihood (BIC weight) of the four-parameter ioHMM &lt;10^–150). While reward outcomes could affect the probability of state transitions, the model comparison suggested that the extra input layer did not explain more variance in choice dynamics, and therefore, we would favor the original, simpler two-parameter HMM model.</p><p>To account for the effect of biased exploitation on the probability of transitioning between states, we also considered an unrestricted HMM model with no parameter tying (four parameter ntHMM), where we treat exploiting the left side and exploiting the right side as two separate exploit states and allow differential transition probability to each exploit state. However, the ntHMM did not improve model fit with almost identical log-likelihood (two-parameter original HMM: log-likelihood = –1424.0; four-parameter ntHMM: log-likelihood = –1423.8). Then we compared two models by calculating the AIC/BIC values for both models, which penalized extra parameters in the no parameter-tying HMM. The AIC test favored the original two-parameter model (AIC: two-parameter original HMM: AIC = 2976.1; four-parameter ntHMM: AIC = 3103.5; relative likelihood (AIC weight) of the four-parameter ntHMM &lt;10^–28). The BIC test also favored the simpler two-parameter model (BIC: two-parameter original HMM: BIC = 3562.8; four-parameter ntHMM: BIC = 4276.9; relative likelihood (BIC weight) of the four-parameter ioHMM &lt;10^–155). In the light of the model comparison results, we decided to fit the simpler two-parameter HMM model.</p></sec><sec id="s4-4-4"><title>Analyzing model dynamics</title><p>In order to understand how exploration and exploitation changed across males and females, we analyzed the HMMs. The term ‘dynamics’ means the rules or laws that govern how a system evolves over time. Here, the system of interest was decision making behavior, specifically at the level of the hidden explore and exploit goals. In fitting our HMMs, we were fitting a set of equations that describe the dynamics of these goals: the probability of transitions between exploration and exploitation and vice versa. Of course, having a set of fitted equations is a far cry from understanding them. To develop an intuition for how sex altered the dynamics of exploration, we therefore turned to analytical tools that allowed us to directly characterize the energetic landscape of behavior (<xref ref-type="fig" rid="fig1">Figure 1H</xref>).</p><p>In statistical mechanics, processes within a system (like a decision-maker at some moment in time) occupy states (like exploration or exploitation). States have energy associated with them, related to the long-time scale probability of observing a process in those states. A low-energy state is one that is very stable and deep, much like a valley between two high peaks. Low-energy states will be over-represented in the system. A high energy state, like a ledge on the side of a mountain, is considerably less stable. High-energy states will be under-represented in the long-term behavior of the system. The probability of observing a process in a given state i will is related to the energy of that state (E<sub>i</sub>) via the Boltzman distribution:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where Z is the partition function of the system, k<sub>B</sub> is the Boltzman constant, and T is the temperature of the system. If we focus on the ratio between two state probabilities, the partition functions cancel out and the relative occupancy of the two states is now a simple function of the difference in energy between them:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Rearranging, we can now express the difference in energy between two states as a function of the difference in the long-term probability of those states being occupied:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Meaning that the difference in the energetic depth of the states is proportional to the natural log of the probability of each state, up to some multiplicative factor k<sub>B</sub>T. To calculate the probability of exploration and exploitation (p<sub>i</sub> and p<sub>j</sub>), we calculated the stationary distribution of the fitted HMMs. The stationary distribution is the equilibrium probability distribution over states. This means that this distribution is the relative frequency of each state that we would observe if the model’s dynamics were run for an infinite period of time. Each entry of the model’s transition matrix reflects the probability that the mice would move from one state (e.g. exploring) to another (e.g. exploiting one of the options) at each moment in time. Because the parameters for all the exploitation states were tied, each transition matrix effectively had two states—an explore state and a generic exploit that described the dynamics of all exploit states. Each of the k animals had its own transition matrix (A<sub>k</sub>), which describes how the entire system—an entire probability distribution over states—would evolve from time point to time point. We observe how the dynamics evolve any probability distribution over states (;;) by applying the dynamics to this distribution:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Over many time steps, ergodic systems will reach a point where the state distributions are unchanged by continued application of the transition matrix as the distribution of states reaches its equilibrium. That is, in stationary systems, there exists a stationary distribution, ;;<sup>*</sup>, such that:<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow/></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mrow/></mml:msup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>If it exists, this distribution is a (normalized) left eigenvector of the transition matrix A<sub>k</sub> with an eigenvalue of 1, so we solved for this eigenvector to determine the stationary distribution of each A<sub>k</sub>. We then took an average of these stationary distributions within each sex, plugged them back into the Boltzman equations to calculate the relative energy (depth) of exploration and exploitation as illustrated in <xref ref-type="fig" rid="fig1">Figure 1H</xref>.</p><p>In order to understand the dynamics of our coarse-grained system, we need to not only understand the depth of the two states, but also the height of the energetic barrier between them: the activation energy required to transition from exploration to exploitation and back again. Here, we build on an approach from chemical kinetics that relates the rate of transition between different conformational states to the energy required to affect these transitions. The Arrhenius equation relates the rate of transitions away from a state (k) to the activation energy required to escape that state (E<sub>a</sub>):<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where A is a constant pre-exponential factor related to the readiness of reactants to undergo the transformation. We will set this to one for convenience. Again, k<sub>B</sub>T is the product of temperature and the Boltzman constant. Note the similarities between this equation and the Boltzman distribution illustrated earlier. Rearranging this equation to solve for activation energy yields:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>k</mml:mi><mml:mi>A</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mi>T</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Thus, much like the relative depth of each state, activation energy is also proportional to some measurable function of behavior, up to some multiplicative factor k<sub>B</sub>T. Note that our approach has only identified the energy of three discrete states (an explore state, an exploit state, and the peak of the barrier between them). These are illustrated by tracing a continuous potential through these three points only to provide a physical intuition for these effects.</p></sec><sec id="s4-4-5"><title>Reinforcement learning models</title><p>We fitted seven reinforcement learning (RL) models that could potentially characterize animals’ choice behaviors, with details of each RL model as below. To identify the model that best captured the computations used by the animals, we compared model fits across six reinforcement learning models with different combinations of latent parameters. AIC weights were calculated from AIC values of each model for each sex and compared across models to determine the best model with the highest relative likelihood.</p><p>The first model assumes that animals choose between two arms randomly with some overall bias for one side over the other. This choice bias for choosing left side over right side is captured with a parameter <italic>b</italic>. The probability of choosing left side on trial <italic>t</italic> is:<disp-formula id="equ12"><label>[1] “random”</label><mml:math id="m12"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The second model is a noisy win-stay lose-shift (WSLS) model that adapts choices with regards to outcomes. This model assumes a win-stay lose-shift policy that is to repeat a rewarded choice and to switch to the other choice if not rewarded. Furthermore, this model includes a parameter ϵ that captures the level of randomness, allowing a stochastic application of the win-stay lose-shift policy. The probability of choosing arm <italic>k</italic> on trial <italic>t</italic> is:</p><p><inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∨</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>k</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>k</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∨</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>∧</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>[2] “noisy WSLS”</p><p>c<sub>t</sub> indicates the choice on trial <italic>t</italic> and r<sub>t</sub> is a binary variable that indicates whether or not trial <italic>t</italic> was rewarded.</p><p>The third model is a basic delta-rule reinforcement learning (RL) model. This two-parameter model assumes that animals learn by consistently updating Q values, which are values defined for options (left and right side). These Q values, in turn, dictate what choice to make next. For example, in a multi-armed bandit task, <italic>Q<sub>t</sub><sup>k</sup></italic> is the value estimation of how good arm <italic>k</italic> at trial <italic>t</italic>, and is updated based on the reward outcome of each trial:<disp-formula id="equ13"><label>[3] ”RL”</label><mml:math id="m13"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In each trial, r<sub>t</sub> – Q<sub>t</sub><sup>k</sup> captures the reward prediction error (RPE), which is the difference between expected outcome and the actual outcome. The parameter <italic>a</italic> is the learning rate, which determines the rate of updating RPE. With Q values defined for each arm, choice selection on each trial was performed based on a Softmax probability distribution:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>, where inverse temperature β determines the level of random decision noise.</p><p>The fourth model incorporates a lapse rate parameter (ε), which reduces the influence of value-independent choices on the estimation of the remaining parameters, capturing any noises outside the softmax value function.<disp-formula id="equ15"><label>[4] “RLε”</label><mml:math id="m15"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ε</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ε</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The fifth model incorporates a choice updating rules in addition to the value updating rule in model 3. The model assumes that choice kernel, which captures the outcome-independent tendency to repeat a previous choice, also influences decision making. The choice kernel updating rule is similar to the value-updating rule:<disp-formula id="equ16"><label>[5] “RLCK”</label><mml:math id="m16"><mml:mrow><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>, where a<sub>t</sub><sup>k</sup> is a binary variable that indicates whether or not arm <italic>k</italic> was chosen on trial <italic>t</italic> and <italic>a</italic><sub>t</sub> is choice kernel updating rate, characterizing choice persistence. The value and choice kernel term were combined to compute the probability of choosing arm <italic>k</italic> on trial <italic>t</italic>:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>, where β<sub>c</sub> is the inverse temperature associated with the choice kernel, capturing the stickiness of choice.</p><p>The sixth model is the same as the fourth model, except that this model includes another parameter γ that modulates learning rate when the choice is not rewarded. This model assumes asymmetrical learning that the learning rate is different for rewarded and unrewarded trials.<disp-formula id="equ18"><label>[6] “RLCKγ”</label><mml:math id="m18"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo>×</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The seventh model is also similar to model 4, except that this model includes another parameter η that tunes the balance between the value updating rule and the choice kernel updating rule. This model assumes that animals could be using two policies (value and choice kernel) to different extent, that is some animals could depend their choices more heavily on values and some animals could be more dependent on choice preference. In this model, the probability of choosing arm <italic>k</italic> on trial <italic>t</italic>:<disp-formula id="equ19"><label>[7] “RLCKη”</label><mml:math id="m19"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-6"><title>Conditional mutual information</title><p>We quantified the extent to which choice history was informative about current choices as the conditional mutual information between the current choice (C<sub>t</sub>) and the last choice (C<sub>t-1</sub>), conditioned on the reward outcome of the last trial (R):<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where the set of choice options (C) represented the two options (left/right).</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Validation, Visualization, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Methodology, Supervision, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Methodology, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (#1912-37717A) of the University of Minnesota.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Four-way Repeated Measures ANOVA showing main effects and interaction effects (pairwise and 3-way) of sex, outcome, latent state, and subject identity.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-69748-supp1-v3.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Tukey’s multiple comparison test of model agreement across RL models showing how well each model predicts animals’ actual choices.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-69748-supp2-v3.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-69748-transrepform1-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All behavioral data have been deposited in generic database (Dyrad) with accession link <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.z612jm6c0">https://doi.org/10.5061/dryad.z612jm6c0</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Sex differences in learning from exploration</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.z612jm6c0</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIMH R01 MH123661, NIMH P50 MH119569, and NIMH T32 training grant MH115886, startup funds from the University of Minnesota (NMG), a Young Investigator Grant from the Brain and Behavior Foundation (RBE), an Unfettered Research Grant from the Mistletoe Foundation (RBE), and Fonds de Recherche du Québec Santé, Chercheur-Boursier Junior 1, #284,309 (RBE). Thank you to Briana E Mork and Lisa S Curry-Pochy for help improve this manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addicott</surname><given-names>MA</given-names></name><name><surname>Baranger</surname><given-names>DAA</given-names></name><name><surname>Kozink</surname><given-names>RV</given-names></name><name><surname>Smoski</surname><given-names>MJ</given-names></name><name><surname>Dichter</surname><given-names>GS</given-names></name><name><surname>McClernon</surname><given-names>FJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Smoking withdrawal is associated with increases in brain activation during decision making and reward anticipation: a preliminary study</article-title><source>Psychopharmacology</source><volume>219</volume><fpage>563</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1007/s00213-011-2404-3</pub-id><pub-id pub-id-type="pmid">21766170</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addicott</surname><given-names>MA</given-names></name><name><surname>Pearson</surname><given-names>JM</given-names></name><name><surname>Sweitzer</surname><given-names>MM</given-names></name><name><surname>Barack</surname><given-names>DL</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Primer on Foraging and the Explore/Exploit Trade-Off for Psychiatry Research</article-title><source>Neuropsychopharmacology</source><volume>42</volume><fpage>1931</fpage><lpage>1939</lpage><pub-id pub-id-type="doi">10.1038/npp.2017.108</pub-id><pub-id pub-id-type="pmid">28553839</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addicott</surname><given-names>MA</given-names></name><name><surname>Pearson</surname><given-names>JM</given-names></name><name><surname>Schechter</surname><given-names>JC</given-names></name><name><surname>Sapyta</surname><given-names>JJ</given-names></name><name><surname>Weiss</surname><given-names>MD</given-names></name><name><surname>Kollins</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attention-deficit/hyperactivity disorder and the explore/exploit trade-off</article-title><source>Neuropsychopharmacology</source><volume>46</volume><fpage>614</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1038/s41386-020-00881-8</pub-id><pub-id pub-id-type="pmid">33040092</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>B</given-names></name><name><surname>Costa</surname><given-names>V</given-names></name><name><surname>Eisenberg</surname><given-names>D</given-names></name><name><surname>Czarapata</surname><given-names>J</given-names></name><name><surname>Berman</surname><given-names>K</given-names></name><name><surname>Murray</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>376. Subcortical Contributions to the Explore-Exploit Tradeoff</article-title><source>Biological Psychiatry</source><volume>81</volume><elocation-id>S154</elocation-id><pub-id pub-id-type="doi">10.1016/j.biopsych.2017.02.393</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Grossman</surname><given-names>CD</given-names></name><name><surname>Lubin</surname><given-names>EE</given-names></name><name><surname>Rajagopalan</surname><given-names>AE</given-names></name><name><surname>Cressy</surname><given-names>JI</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stable Representations of Decision Variables for Flexible Behavior</article-title><source>Neuron</source><volume>103</volume><fpage>922</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.001</pub-id><pub-id pub-id-type="pmid">31280924</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Gender differences in dopaminergic function in striatum and nucleus accumbens</article-title><source>Pharmacology, Biochemistry, and Behavior</source><volume>64</volume><fpage>803</fpage><lpage>812</lpage><pub-id pub-id-type="doi">10.1016/s0091-3057(99)00168-9</pub-id><pub-id pub-id-type="pmid">10593204</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beeler</surname><given-names>JA</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name><name><surname>Frazier</surname><given-names>CRM</given-names></name><name><surname>Zhuang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Tonic dopamine modulates exploitation of reward learning</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>4</volume><elocation-id>170</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2010.00170</pub-id><pub-id pub-id-type="pmid">21120145</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beeler</surname><given-names>JA</given-names></name><name><surname>Frazier</surname><given-names>CRM</given-names></name><name><surname>Zhuang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Putting desire on a budget: dopamine and energy expenditure, reconciling reward and resources</article-title><source>Frontiers in Integrative Neuroscience</source><volume>6</volume><elocation-id>49</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2012.00049</pub-id><pub-id pub-id-type="pmid">22833718</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>Random Walks in Biology</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bilmes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>A Gentle Tutorial of the EM Algorithm and Its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models</source><publisher-name>International computer science institute</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanco</surname><given-names>NJ</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name><name><surname>Beevers</surname><given-names>CG</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The influence of depression symptoms on exploratory decision-making</article-title><source>Cognition</source><volume>129</volume><fpage>563</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2013.08.018</pub-id><pub-id pub-id-type="pmid">24055832</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2021">2021a</year><data-title>RestlessBandit2021</data-title><version designator="a0a3777">a0a3777</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/CathySChen/restlessBandit2021.git">https://github.com/CathySChen/restlessBandit2021.git</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CS</given-names></name><name><surname>Ebitz</surname><given-names>RB</given-names></name><name><surname>Bindas</surname><given-names>SR</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Grissom</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Divergent Strategies for Learning in Males and Females</article-title><source>Current Biology</source><volume>31</volume><fpage>39</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.09.075</pub-id><pub-id pub-id-type="pmid">33125868</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cinotti</surname><given-names>F</given-names></name><name><surname>Fresno</surname><given-names>V</given-names></name><name><surname>Aklil</surname><given-names>N</given-names></name><name><surname>Coutureau</surname><given-names>E</given-names></name><name><surname>Girard</surname><given-names>B</given-names></name><name><surname>Marchand</surname><given-names>AR</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dopamine blockade impairs the exploration-exploitation trade-off in rats</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>6770</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-43245-z</pub-id><pub-id pub-id-type="pmid">31043685</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id><pub-id pub-id-type="pmid">16778890</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebitz</surname><given-names>R.B</given-names></name><name><surname>Albarran</surname><given-names>E</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Exploration Disrupts Choice-Predictive Signals and Alters Dynamics in Prefrontal Cortex</article-title><source>Neuron</source><volume>97</volume><elocation-id>475</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.011</pub-id><pub-id pub-id-type="pmid">29346756</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebitz</surname><given-names>R.B</given-names></name><name><surname>Sleezer</surname><given-names>BJ</given-names></name><name><surname>Jedema</surname><given-names>HP</given-names></name><name><surname>Bradberry</surname><given-names>CW</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Tonic exploration governs both flexibility and lapses</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007475</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007475</pub-id><pub-id pub-id-type="pmid">31703063</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebitz</surname><given-names>RB</given-names></name><name><surname>Tu</surname><given-names>JC</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rules warp feature encoding in decision-making circuits</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000951</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000951</pub-id><pub-id pub-id-type="pmid">33253163</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Economides</surname><given-names>M</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Lübbert</surname><given-names>A</given-names></name><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Model-Based Reasoning in Humans Becomes Automatic with Training</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004463</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004463</pub-id><pub-id pub-id-type="pmid">26379239</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Filipowicz</surname><given-names>ALS</given-names></name><name><surname>Levine</surname><given-names>J</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Tavoni</surname><given-names>G</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Complexity of Model-Free and Model-Based Learning Strategies</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2019.12.28.879965</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Moustafa</surname><given-names>AA</given-names></name><name><surname>Haughey</surname><given-names>HM</given-names></name><name><surname>Curran</surname><given-names>T</given-names></name><name><surname>Hutchison</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning</article-title><source>PNAS</source><volume>104</volume><fpage>16311</fpage><lpage>16316</lpage><pub-id pub-id-type="doi">10.1073/pnas.0706111104</pub-id><pub-id pub-id-type="pmid">17913879</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Doll</surname><given-names>BB</given-names></name><name><surname>Oas-Terpstra</surname><given-names>J</given-names></name><name><surname>Moreno</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1062</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1038/nn.2342</pub-id><pub-id pub-id-type="pmid">19620978</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Empirical priors for reinforcement learning models</article-title><source>Journal of Mathematical Psychology</source><volume>71</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.01.006</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Flash</surname><given-names>S</given-names></name><name><surname>Reiss</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sex differences in psychiatric disorders: what we can learn from sex chromosome aneuploidies</article-title><source>Neuropsychopharmacology</source><volume>44</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1038/s41386-018-0153-2</pub-id><pub-id pub-id-type="pmid">30127341</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grissom</surname><given-names>NM</given-names></name><name><surname>Reyes</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Let’s call the whole thing off: evaluating gender and sex differences in executive function</article-title><source>Neuropsychopharmacology</source><volume>44</volume><fpage>86</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1038/s41386-018-0179-5</pub-id><pub-id pub-id-type="pmid">30143781</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groman</surname><given-names>SM</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Petrullli</surname><given-names>JR</given-names></name><name><surname>Massi</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Ropchan</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Morris</surname><given-names>ED</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine D3 Receptor Availability Is Associated with Inflexible Decision Making</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>6732</fpage><lpage>6741</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3253-15.2016</pub-id><pub-id pub-id-type="pmid">27335404</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>CD</given-names></name><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Serotonin neurons modulate learning rate through uncertainty</article-title><source>Current Biology</source><elocation-id>S0960-9822(21)01682-1</elocation-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.12.006</pub-id><pub-id pub-id-type="pmid">34936883</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>MD</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Gurney</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dopaminergic Control of the Exploration-Exploitation Trade-Off via the Basal Ganglia</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00009</pub-id><pub-id pub-id-type="pmid">22347155</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishii</surname><given-names>S</given-names></name><name><surname>Yoshida</surname><given-names>W</given-names></name><name><surname>Yoshimoto</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of exploitation-exploration meta-parameter in reinforcement learning</article-title><source>Neural Networks</source><volume>15</volume><fpage>665</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(02)00056-4</pub-id><pub-id pub-id-type="pmid">12371519</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izquierdo</surname><given-names>A</given-names></name><name><surname>Aguirre</surname><given-names>C</given-names></name><name><surname>Hart</surname><given-names>EE</given-names></name><name><surname>Stolyarova</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Rodent Models of Adaptive Value Learning and Decision-Making</article-title><source>Methods in Molecular Biology</source><volume>2011</volume><fpage>105</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-9554-7_7</pub-id><pub-id pub-id-type="pmid">31273696</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenni</surname><given-names>NL</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prefrontal Dopamine D1 and D2 Receptors Regulate Dissociable Aspects of Decision Making via Distinct Ventral Striatal and Amygdalar Circuits</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6200</fpage><lpage>6213</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0030-17.2017</pub-id><pub-id pub-id-type="pmid">28546312</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepma</surname><given-names>M</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupil diameter predicts changes in the exploration-exploitation trade-off: evidence for the adaptive gain theory</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>1587</fpage><lpage>1596</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21548</pub-id><pub-id pub-id-type="pmid">20666595</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katahira</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The statistical structures of reinforcement learning with asymmetric value updates</article-title><source>Journal of Mathematical Psychology</source><volume>87</volume><fpage>31</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2018.09.002</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Rosen</surname><given-names>ZB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decision making and the avoidance of cognitive demand</article-title><source>Journal of Experimental Psychology. General</source><volume>139</volume><fpage>665</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1037/a0020198</pub-id><pub-id pub-id-type="pmid">20853993</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurdi</surname><given-names>B</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Banaji</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Model-free and model-based learning processes in the updating of explicit and implicit evaluations</article-title><source>PNAS</source><volume>116</volume><fpage>6035</fpage><lpage>6044</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820238116</pub-id><pub-id pub-id-type="pmid">30862738</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leao</surname><given-names>D</given-names></name><name><surname>Fragoso</surname><given-names>M</given-names></name><name><surname>Ruffino</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Regular conditional probability, disintegration of probability and radon spaces</article-title><source>Proyecciones</source><volume>23</volume><fpage>15</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.4067/S0716-09172004000100002</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maney</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perils and pitfalls of reporting sex differences</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>371</volume><elocation-id>20150119</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0119</pub-id><pub-id pub-id-type="pmid">26833839</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McLachlan</surname><given-names>GJ</given-names></name><name><surname>Peel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Finite Mixture Models</source><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehlhorn</surname><given-names>K</given-names></name><name><surname>Newell</surname><given-names>BR</given-names></name><name><surname>Todd</surname><given-names>PM</given-names></name><name><surname>Lee</surname><given-names>MD</given-names></name><name><surname>Morgan</surname><given-names>K</given-names></name><name><surname>Braithwaite</surname><given-names>VA</given-names></name><name><surname>Hausmann</surname><given-names>D</given-names></name><name><surname>Fiedler</surname><given-names>K</given-names></name><name><surname>Gonzalez</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unpacking the exploration–exploitation tradeoff: A synthesis of human and animal literatures</article-title><source>Decision</source><volume>2</volume><fpage>191</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1037/dec0000033</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Control of entropy in neural models of environmental state</article-title><source>eLife</source><volume>8</volume><elocation-id>e39404</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.39404</pub-id><pub-id pub-id-type="pmid">30816090</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>JM</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Raghavachari</surname><given-names>S</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neurons in posterior cingulate cortex signal exploratory decisions in a dynamic multioption choice task</article-title><source>Current Biology</source><volume>19</volume><fpage>1532</fpage><lpage>1537</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.07.048</pub-id><pub-id pub-id-type="pmid">19733074</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pisupati</surname><given-names>S</given-names></name><name><surname>Chartarifsky-Lynn</surname><given-names>L</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Lapses in Perceptual Judgments Reflect Exploration</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/613828</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poling</surname><given-names>A</given-names></name><name><surname>Edwards</surname><given-names>TL</given-names></name><name><surname>Weeden</surname><given-names>M</given-names></name><name><surname>Foster</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Matching Law</article-title><source>The Psychological Record</source><volume>61</volume><fpage>313</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1007/BF03395762</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shansky</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Are hormones a “female problem” for animal research?</article-title><source>Science</source><volume>364</volume><fpage>825</fpage><lpage>826</lpage><pub-id pub-id-type="doi">10.1126/science.aaw7570</pub-id><pub-id pub-id-type="pmid">31147505</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Bos</surname><given-names>R</given-names></name><name><surname>Homberg</surname><given-names>J</given-names></name><name><surname>de Visser</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A critical review of sex differences in decision-making tasks: focus on the Iowa Gambling Task</article-title><source>Behavioural Brain Research</source><volume>238</volume><fpage>95</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2012.10.002</pub-id><pub-id pub-id-type="pmid">23078950</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Bonawitz</surname><given-names>E</given-names></name><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Ebitz</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Balancing exploration and exploitation with information and randomization</article-title><source>Current Opinion in Behavioral Sciences</source><volume>38</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.10.001</pub-id><pub-id pub-id-type="pmid">33184605</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A definition of conditional mutual information for arbitrary ensembles</article-title><source>Information and Control</source><volume>38</volume><fpage>51</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/S0019-9958(78)90026-8</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoest</surname><given-names>KE</given-names></name><name><surname>Cummings</surname><given-names>JA</given-names></name><name><surname>Becker</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Estradiol, dopamine and motivation</article-title><source>Central Nervous System Agents in Medicinal Chemistry</source><volume>14</volume><fpage>83</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.2174/1871524914666141226103135</pub-id><pub-id pub-id-type="pmid">25540977</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69748.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2020.12.29.424773" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2020.12.29.424773"/></front-stub><body><p>Following inclusion of new modeling and data presentation, authors have more clearly demonstrated that equivalent performance is seen across males and females in terms of reward rate, yet achieved via different successful strategies. This is an important contribution to the growing literature on sex differences in reinforcement learning.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69748.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewer</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.12.29.424773">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.12.29.424773v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Sex differences in learning from exploration&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Alicia Izquierdo as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Kate Wassum as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Chen et al., trained male and female animals on an explore/exploit (2-armed bandit) task. Despite similar levels of accuracy in these animals, authors report higher levels of exploration in male than in female mice. The patterns of exploration were analyzed in fine-grained detail: males are less likely to stop exploring once exploring is initiated, whereas females stop exploring once they learn. Authors find that both learning rate (α) and noise parameter (β) increase in exploration trials in a hidden Markov model (HMM). When reinforcement learning (RL) models were fitted to animal data, they report females had a higher learning rate and over days of testing, suggesting higher meta-learning in females. They also report that of the RL models they fit, the model incorporating a choice kernel updating rule was found to fit both male and female learning. The results suggest one should pay greater attention to the influence of sex in learning and exploration. Another important takeaway from this study is that similar levels of accuracy do not imply similar strategies. There are 2 categories of essential revisions suggested by Reviewers:</p><p>1) There was a general concern that reframing of conclusions may be warranted due to the major results possibly reflecting learning more than exploration. Female rats may learn the task better than male rats. For more clarity on this issue, reviewers request that authors present more primary behavioral data (p(reward,obtained) vs time (days), reaction times over time, etc.) to justify their conclusions. It was also unclear how reaction times were calculated and how &quot;steady state&quot; was operationalized.</p><p>2) Reviewers also asked for better justification and details for both the hidden Markov model and reinforcement learning parameters. If for example, male rats simply learn the task more poorly and behave more randomly, this would manifest as more exploration in the HMM model. Additional analyses are needed to strengthen authors' claims using the HMM model- the effect of obtained reward on state transitions, and biased exploitations should be further explored as there are presently a number of unjustified assumptions.</p><p>Please address these essential concerns (which are detailed in the reviews below), as well as the reviewers other comments.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Chen et al., trained male and female animals on an explore/exploit (2-armed bandit) task. Despite similar levels of accuracy in these animals, authors report higher levels of exploration in males than in females. The patterns of exploration were analyzed in fine-grained detail: males are less likely to stop exploring once exploring is initiated, whereas female mice stop exploring once they learn. Authors find that both learning rate (α) and noise parameter (β) increase in exploration trials in a hidden Markov model (HMM). When reinforcement learning (RL) models were fitted to animal data, they report females had a higher learning rate and over days of testing, suggesting higher meta-learning in females. They also report that of the RL models they fit, the model incorporating a choice kernel updating rule was found to fit both male and female learning. The results do suggest one should pay greater attention to the influence of sex in learning and exploration. Another important takeaway from this study is that similar levels of accuracy do not imply similar strategies. I have suggestions for clarity in data presentation and interpretation.</p><p>One of the first sections in the Results section dives straight away into the HMM, but in my opinion, authors do not present enough of the primary behavioral data- perhaps I missed this, but can we see p(reward, obtained) over sessions for males and females (more information than Figure 1B)? And the reaction times in Figure 1C, are these reaction times to make a left/right response or reaction times to collect rewards? Can authors show both, as a function over time?</p><p>What is the cited rationale for the different RL models and their parameters? If the RLCK is the best fit for both males and females, does this lend support to the idea that though overall learning many not differ between males and females, the strategies are not well captured by RL? Please clarify.</p><p>Authors should clarify the difference between learning and &quot;steady state.&quot; How was this operationally defined and measured? This was a bit lost in the data presentation.</p><p>The lines 430-432 about rodent behavioral tasks are unclear to me: &quot;However, the vast majority of these tasks were not designed with computational models in mind, and as a result, we are unable to assess whether similar latent cognitive variables are influencing behavior in humans and rodents.&quot; There are several groups that use touchscreen-response methods paired with computational modeling. Do the authors mean they do not have access to similar databases to compare these latent variables across species? Authors may want to clarify how these experiments uniquely identify latent cognitive variables not previously explored with similar methods.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. How is reaction time computed here? Do you remove outliers (extremely long RTs)? Is there a way to separate exploring from guessing in RT (given that behaviorally they are confounded)?</p><p>2. State transitions are not value dependent in the HMM model. Another value independent way of &quot;exploration&quot; is by having a lapse rate in the RL model. I am curious about whether there is a lapse rate difference across sex (and possibly no differences in the temperature term).</p><p>3. There is the inset panel (ROC curve) in all density figures except Figure 3D.</p><p>4. I like your dynamic landscape illustration of the fitted HMM (Figure 1G).</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>(1) A great proportion of reported results and analysis rely on the extracted latent states from the proposed HMM. While HMM is proposed to provide a model free analysis of behavior, certain choices regarding the HMM model need further justification:</p><p>(1.1) Most importantly, authors assume that only most recent state determines the state in the next trial. However, I argue that most recent obtained reward is another determinant of the state in the next trial and should be added to the model. This way instead of using a naïve HMM, and then exploring learning in explore/exploit trials, authors can compare HMM parameters.</p><p>(1.2) Proposed HMM model also assumes that exploit states are uniform across the options. Do authors have any evidence supporting this assumption? Side biases are commonly observed in animals and humans. Extracted RL parameters also confirm this. Please comment.</p><p>(1.3) Moreover, the model assumes that the mice had to pass through exploration in order to start exploiting a new option. Do authors have any evidence supporting this assumption? What will happen to the results if this assumption is lifted? Please comment.</p><p>(2) Page 10, line 252: Please provide a more quantitative comparison of models' choice behavior (and not just RLCK) and the animals' behavior for all sessions. Also, there are no tick-marks on the y-axis.</p><p>(3) How much overlap exists between the extracted latent dynamics from HMM and that of previously proposed models mentioned in the methods (Daw et al., 2006; Jepma and Nieuwenhuis, 2011; Pearson et al., 2009)? It would be helpful to show the extent that results from these different methods deviate/overlap with each other.</p><p>(4) Do authors see any differences between amount of exploration/exploitation at the beginning vs at the end of a session? How about across days? The fact that meta learning is observed, suggests that even in a single session, changes in the strategy of animals might be expected.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69748.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>1) There was a general concern that reframing of conclusions may be warranted due to the major results possibly reflecting learning more than exploration. Female rats may learn the task better than male rats. For more clarity on this issue, reviewers request that authors present more primary behavioral data (p(reward,obtained) vs time (days), reaction times over time, etc.) to justify their conclusions.</p></disp-quote><p>The reviewers were concerned that the apparent sex differences in exploration were actually the result of sex differences in how well the animals learn the task, which we have addressed in two ways. First, we have improved the clarity of the manuscript demonstrating that there were no differences in how “well” the male and female mice performed the task in terms of reward acquisition. The revised manuscript indeed shows that (1) male and female animals had equivalent levels of performance in terms of rewards earned, and (2) males and females both reached equivalent performance levels on the bandit task before comparisons of their strategies began. As we detail below, we have revised the manuscript to make this point clearer, including adding the requested primary behavioral data (Supplemental Figure 1).</p><p>Second, this comment also highlights a deeper point--that learning (technically defined as a process of updating expectations) and exploration (defined as a process of environment sampling) are often conflated and understanding the precise relationship between the two is essential. Further, these measures are often conflated with reward performance (that is, “better reward performance means better learning”), but reward performance is at its core a dependent measure that can reflect different combinations of contributions from the cognitive processes of learning/updating expectations and exploration/sampling the environment. We believe that the insights we provide into the relationship between learning and exploration is a strength of the revised manuscript, not a misinterpretation. We show that trial-by-trial measures of learning are intrinsically linked to exploration, though these links differ between males and females. The reviewer’s comments lead to the interesting question of whether learning differences beget exploration differences, whether the reverse is true, or whether these latent cognitive variables are dissociable from each other. Unfortunately, we cannot yet say whether differences in learning drive differences in exploration or vice versa, but we highlight this issue for future research in the discussion.</p><disp-quote content-type="editor-comment"><p>It was also unclear how reaction times were calculated and how &quot;steady state&quot; was operationalized.</p></disp-quote><p>We apologize for omitting an operational definition of reaction times. The revised manuscript explains that response times were calculated as the time elapsed between choice display onset and the time when the nose poke response was completed, as measured by the touch screen.</p><p>Regarding the term “steady state”, we believe that this comment is related to Reviewer #1’s major comment: “Authors should clarify the difference between learning and &quot;steady state.&quot; How was this operationally defined and measured? This was a bit lost in the data presentation.” We apologize that we could not find a place in the manuscript where we used the term steady-state ourselves, so we present a few ways to hopefully address this point. First, we considered the possibility that the reviewer was referring to asymptotic steady-state behavior (i.e. whether the animals had learned the task before data collection began). If this reading of the comment is correct, then we hope that our other comments and new analyses regarding asymptotic performance levels will address this point. We also considered the alternative possibility that the reviewer may have in mind a different version of a bandit task (e.g.: reversal task, in which steady-state periods in reward contingencies occur prior to reversals). To address this possible concern, we have clarified that there is no steady state component in this task: the reward probabilities of each choice continuously changed over time, so the animals were encouraged to continuously learn about the most rewarding choice(s). These changes are detailed in our response to this comment below.</p><disp-quote content-type="editor-comment"><p>2) Reviewers also asked for better justification and details for both the hidden Markov model and reinforcement learning parameters. If for example, male rats simply learn the task more poorly and behave more randomly, this would manifest as more exploration in the HMM model.</p></disp-quote><p>The reviewers were concerned that the sex difference in HMM-inferred exploration could merely be due to poor learning performance in males. In our brief response to the previous comment and detailed comments below, we explain that male mice did not perform the task more poorly than females and several new analyses show that there were no sex differences in how well the task was performed.</p><p>It is important to clarify that there are a great number of equivalently good strategies in this task, some even involving random choosing (Daw et al., 2006; Sutton and Barto, 1998; Wilson et al., 2021). This happens because this task is so dynamic and the best option so uncertain. From this vantage point, an animal behaving randomly, if possible, could be a valid approach However, this subtle point was a bit lost in the original manuscript, so we have made several revisions to try to bring it to the forefront. We have added new figures and supplemental figures, as well as changed in text in results and discussion. We hope these revisions clarify that the critical observation we make here is not that either sex performed more poorly--instead, males and females received similar amounts of reward through fundamentally different strategies.</p><p>We recognize that the confusion might have come from the result of the reinforcement learning (RL) model, where females had higher learning rates (ɑ) than males, indicating differences in the rate of updating value expectations. In the revised manuscript, we clarified that the learning rate parameter in the RL model is not equivalent to the dependent measure of reward performance, which is better indicated by the number of rewards obtained. We have added a new figure panel to Figure 3 to illustrate two points. First, that higher learning rate is not equivalent to more reward earned. In fact, it is possible to learn so rapidly (that is, make large, sudden adjustments to expectations of value) that performance is actually compromised in a stochastically rewarding environment! Second, this new panel illustrates that different combinations of the RL model parameters learning rate (ɑ) and decision noise (β) could result in similar performance. We have also rewritten our discussion of the significance of the sex difference in learning rate parameter results to clarify our interpretation and ensure that our claims are more precise.</p><disp-quote content-type="editor-comment"><p>Additional analyses are needed to strengthen authors' claims using the HMM model- the effect of obtained reward on state transitions, and biased exploitations should be further explored as there are presently a number of unjustified assumptions.</p></disp-quote><p>The reviews requested detailed justification for the specific HMM we used in the original manuscript. We agree that the HMM was an essential part of the manuscript and the choices we made about its structure deserved a more detailed justification. In the revised manuscript, we include a formal comparison of the simplified model we originally fit with two, more complex versions of the HMM model: an input-output HMM (ioHMM) and an unrestricted HMM, fit without any parameter tying (ntHMM). The input-output HMM model takes into account the effect of reward on choice dynamics. And the unrestricted HMM allows for bias exploitations. However, model comparison revealed that there was no benefit to these more complex models, either in terms of their ability to fit the data, or in terms of their ability to explain variance in choice behavior. The original HMM was the most parsimonious model and as a result it is the best able to explain choice dynamics. These results are included in the revised manuscript, along with additional text that explains why the HMM was structured as it was (details in the comments below).</p><disp-quote content-type="editor-comment"><p>Please address these essential concerns (which are detailed in the reviews below), as well as the reviewers other comments.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>Chen et al., trained male and female animals on an explore/exploit (2-armed bandit) task. Despite similar levels of accuracy in these animals, authors report higher levels of exploration in males than in females. The patterns of exploration were analyzed in fine-grained detail: males are less likely to stop exploring once exploring is initiated, whereas female mice stop exploring once they learn. Authors find that both learning rate (α) and noise parameter (β) increase in exploration trials in a hidden Markov model (HMM). When reinforcement learning (RL) models were fitted to animal data, they report females had a higher learning rate and over days of testing, suggesting higher meta-learning in females. They also report that of the RL models they fit, the model incorporating a choice kernel updating rule was found to fit both male and female learning. The results do suggest one should pay greater attention to the influence of sex in learning and exploration. Another important takeaway from this study is that similar levels of accuracy do not imply similar strategies. I have suggestions for clarity in data presentation and interpretation.</p><p>One of the first sections in the Results section dives straight away into the HMM, but in my opinion, authors do not present enough of the primary behavioral data- perhaps I missed this, but can we see p(reward, obtained) over sessions for males and females (more information than Figure 1B)? And the reaction times in Figure 1C, are these reaction times to make a left/right response or reaction times to collect rewards? Can authors show both, as a function over time?</p></disp-quote><p>Thank you for this important feedback. In the revised manuscript, we have included more primary behavioral data in Supplemental Figure 1, including reward acquisition over sessions, response time over sessions, and reward retrieval time over sessions. The addition of these new figures demonstrates two points more clearly: (1) males and females had identical reward acquisition performance and neither sex was performing poorer than the other; (2) multiple behavioral metrics (reward acquisition, response time, reward retrieval time) converge to show that animals had reached asymptotic performance. We have also clarified those points in the manuscript: that animals have learned how to perform the task before data collection began, whereas we are focused on understanding how animals continued to make flexible decisions in a well-learned, but non-stationary environment.</p><p>Additionally, we have added more text to explain how response time and reward retrieval time was calculated. Response times were calculated as the time elapsed between choice display onset and the time when the nose poke response was completed, as measured by the touch screen. Reward retrieval time was calculated as the time elapsed between the nose-poke response for choice and magazine entry for reward retrieval. We apologize for omitting this definition in the original manuscript.</p><p>New text from the manuscript (page 4, line 93-113):</p><p>“It is worth noting that unlike other versions of bandit tasks such as the reversal learning task, in the restless bandit task, animals were encouraged to continuously learn about the most rewarding choice(s). […] Since both sexes have learned to perform the task prior to data collection, variabilities in task performance are results of how animals learned and adapted their choices in response to the changing reward contingencies.”</p><p>(page 6, line 133-140):</p><p>“Therefore, we examined the response time, which was calculated as time elapsed between choice display onset and nose poke response as recorded by the touchscreen chamber, in both males and females. If males and females had adopted different strategies here, then we might expect response time to systematically differ between males and females, despite the similarities in learning performance. Indeed, females responded significantly faster than did males (Figure 1C, main effect of sex, t(30) = 3.52, p = 0.0014), suggesting that decision making computations may differ across sexes and, if so, that the strategies that tended to be used by females resulted in faster choice response time than those used by males.”</p><disp-quote content-type="editor-comment"><p>What is the cited rationale for the different RL models and their parameters? If the RLCK is the best fit for both males and females, does this lend support to the idea that though overall learning many not differ between males and females, the strategies are not well captured by RL? Please clarify.</p></disp-quote><p>This comment raises an interesting point and an important concern. First, to address the concern, we have added more clarifying text to explain the rationale of fitting different reinforcement learning (RL) models and added relevant citations. This is detailed below. Second, to the interesting point: we too had considered that strategy differences between male and female animals might mean that they have different best-fitting RL models. However, the fact that both were best fit by the same RLCK model does not mean that they did not have different strategies or that RL modeling cannot capture those strategies. It may, however, suggest that strategic differences between the sexes are more a matter of degree (i.e. of differences in the specific pattern of model parameters), rather a matter of categorically distinct computations. This interpretation also makes the most sense in light of the biology of sex differences, which produce few (if any) truly categorically distinct changes in neural function, but rather serve to bias neural systems across sexes in multiple complex ways.</p><p>In terms of why we chose to look at the specific RL models that we did, we have revised the manuscript to explain (1) what different flavors of RL model are commonly used in the decision making literature, and (2) why we chose the specific models we did. We have added 1 additional model, per another review’s request, but the relevant text reads as follows (page 12, line 305-340):</p><p>“The HMM suggested that males and females had different levels of exploration, but it did not provide insight into the latent, cognitive processes behind these differences. […] This interpretation also makes the most sense in light of the biology of sex differences, which produce few (if any) truly categorically distinct changes in neural function, but rather serve to bias neural systems across sexes in multiple complex ways.”</p><disp-quote content-type="editor-comment"><p>Authors should clarify the difference between learning and &quot;steady state.&quot; How was this operationally defined and measured? This was a bit lost in the data presentation.</p></disp-quote><p>We apologize that we could not find a place in the manuscript where we used the term steady-state ourselves, so we present a few ways to hopefully address this point.</p><p>First, we considered the possibility that the reviewer was referring to asymptotic steady-state behavior (i.e. whether the animals had learned the task before data collection began). In the revised manuscript, we first clarified that there are two types of learning that could occur in this task: (1) global learning about the statistics of the reward environment, e.g.: how frequently do reward probabilities change; (2) learning about the most valuable option at each time point. In the revised manuscript, we clarified and included a supplemental figure to demonstrate that the global learning of the task structure has reached asymptotic levels in both sexes (no changes in reward acquisition, response time, reward retrieval time across sessions) and we are examining how animals explored and learned about which option currently provided the best reward probability.</p><p>Second, we considered the alternative possibility that the reviewer may have in mind a different version of a bandit task (e.g.: reversal task, in which there exists steady periods in reward contingencies prior to reversals). In any case, it was important to clarify that there was no steady state component in this task: the reward probabilities of each choice continuously changed over time, so the animals were encouraged to continuously learn about the most rewarding choice(s). We have clarified our descriptions of the task to better explain the dynamic structure of the task and to highlight the differences between this task and classic reversing reward schedules.</p><disp-quote content-type="editor-comment"><p>The lines 430-432 about rodent behavioral tasks are unclear to me: &quot;However, the vast majority of these tasks were not designed with computational models in mind, and as a result, we are unable to assess whether similar latent cognitive variables are influencing behavior in humans and rodents.&quot; There are several groups that use touchscreen-response methods paired with computational modeling. Do the authors mean they do not have access to similar databases to compare these latent variables across species? Authors may want to clarify how these experiments uniquely identify latent cognitive variables not previously explored with similar methods.</p></disp-quote><p>We apologize for the poor choice of phrasing here and hope the revised manuscript clarifies our point: that we are excited to be a part of an emerging and translationally important trend in rodent computational neuroscience, not that we are the only group working in this space. We meant only to distinguish this trend from the historical use of species-specific tasks in translational research. New text from the manuscript (page 19, line 542-550):</p><p>“Rodent operant testing is frequently used to assess cognitive functions. This is critical for translational work in animals needed to link pharmacology, genetics, and other potential biological contributors to behavior (Grissom and Reyes, 2018; Heath et al., 2016). However, many classic rodent cognitive tasks are species-specific: they were not designed to assess the same cognitive processes across species and this limits their translational ability. Currently, there is an emerging exciting trend among rodent researchers with adopting tasks with translational potentials in rodents, such as reversal learning and various versions of bandit task (Izquierdo et al., 2019; Groman et al., 2016; Bari et al., 2019; Grossman et al., 2020), where we can assess the same cognitive processes across species and across animal models of diseases.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. How is reaction time computed here? Do you remove outliers (extremely long RTs)? Is there a way to separate exploring from guessing in RT (given that behaviorally they are confounded)?</p></disp-quote><p>We apologize for omitting the definition of response time. The revised manuscript explains that response times were calculated as the time elapsed between choice display onset and the time when the nose poke response was completed, as measured by the touch screen. Z scores of response times for each animal were calculated and response times with z scores above or below 3 standard deviations from the mean were removed as outliers. As we responded earlier in comment 1B, we are not able to differentiate between exploring and guessing with our models in this task.</p><disp-quote content-type="editor-comment"><p>2. State transitions are not value dependent in the HMM model. Another value independent way of &quot;exploration&quot; is by having a lapse rate in the RL model. I am curious about whether there is a lapse rate difference across sex (and possibly no differences in the temperature term).</p></disp-quote><p>We take the reviewer’s point that there may be other noise/bias outside the softmax value function. In the revised manuscript, we have included a new RL model that incorporates the lapse rate. However, the model comparison (AIC) showed that the new RL lapse rate model did not improve the model fit (Figure 3B). In addition, we also examine the model agreement of this new RL lapse rate model to determine whether including a lapse rate parameter improves the model’s ability to predict the animals’ choice. However, multiple comparisons suggested that including the lapse rate parameter did not improve the model’s ability to predict animals’ choice (Figure 3C, Supplemental Table 2, mean difference &lt; 0.0004, p &gt; 0.99). Based on this result, we believe the RLCK model we used in the original manuscript, which captures both value and value independent choice bias, is still the simplest model that best captured and predicted animals’ actual choice.</p><p>New text from the manuscript (page 12, line 323-324):</p><p>“(4) a three-parameter “RLε” model with a consistent learning rate, and inverse temperature that captures value-based decision noise, and a value-independent noise”</p><p>page 13, line 341-356:</p><p>“To quantify how well each RL model was at predicting animals’ choices, we measured the model agreement for each model, which was calculated as the probability of choice correctly predicted by the optimized model parameters for each model (Figure 3C). Then we conducted a multiple comparison across model agreement of RL models (test statistics reported in Supplemental Table 2). The results suggested that the RL models with parameter(s) that account for choice bias (RLCK, RLCKγ, RLCKη) were significantly better at predicting animals’ actual choices than the models that do not account for choice bias and non-RL models (random, noisy WSLS, RL, RLε). There was no significant difference in model agreement between RLCK, RLCKγ, and RLCKη. Based on the result of model comparison (AIC) and model agreement, we decided that the four-parameter RLCK model is the simplest, best-fit model that best predicted animal’ actual choices. Finally, to visualize how well the RLCK model was at predicting choices of animals with different learning performance, we plotted the simulated choices and actual choices against the matching law (Poling et al., 2011), which dictates that the probability of choice is proportional to the probability of reward. The figure showed that this four-parameter model was able to characterize animals’ choice behaviors regardless of the value-based learning performance (Supplemental Figure 3).”</p><p>page 37, line 927-932:</p><p>“The fourth model incorporates a lapse rate parameter (ε), which reduces the influence of value-independent choices on the estimation of the remaining parameters, capturing any noises outside the softmax value function.</p><p><inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ε</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>[4] “RLε”</p><disp-quote content-type="editor-comment"><p>3. There is the inset panel (ROC curve) in all density figures except Figure 3D.</p></disp-quote><p>The inset ROC curve graph has been added to what’s now Figure 3E. Thank you for pointing it out.</p><disp-quote content-type="editor-comment"><p>4. I like your dynamic landscape illustration of the fitted HMM (Figure 1G).</p></disp-quote><p>Thank you!</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Major concerns:</p><p>1) A great proportion of reported results and analysis rely on the extracted latent states from the proposed HMM. While HMM is proposed to provide a model free analysis of behavior, certain choices regarding the HMM model need further justification:</p></disp-quote><p>We have added additional information on our HMM model design. Briefly, each of these choices simplified the model. In part, we wanted to be mindful of the tradeoff between fitting the data as well as possible and making a simpler, more interpretable model. We erred on the side of the latter here because we had small amounts of data, relatively speaking (which makes fitting more parameters more challenging), and were interested in interpreting specific parameters. In the light of the model comparison results, we decided to stick with the simpler 2-parameter HMM model in the original manuscript. Unfortunately, we do not have enough data to accurately estimate more parameters than we would like to. We believe that in the future experiments oriented towards capturing a large amount of data from our animals, we would be able to fit more sophisticated HMM models that allow for more flexibility to estimate input output relationships, and specific side-based exploration, etc. Please see the detailed responses below.</p><disp-quote content-type="editor-comment"><p>(1.1) Most importantly, authors assume that only most recent state determines the state in the next trial. However, I argue that most recent obtained reward is another determinant of the state in the next trial and should be added to the model. This way instead of using a naïve HMM, and then exploring learning in explore/exploit trials, authors can compare HMM parameters.</p></disp-quote><p>We have extended the model in the ways suggested by the reviewer, but these changes did not improve model fit. We nevertheless share the reviewer’s intuition and suspect that we might be able to fit more complex models if we had more data. However, given that we had ~2k trials per animal, simpler models had an advantage here. The relevant text reads:</p><p>New text from the manuscript (page 8, line 203-211):</p><p>“Since various factors could influence state of the next trial, we considered a simple 2 parameter HMM that models only two states (exploration and exploitation), a 4-parameter input-output HMM (ioHMM) that allows reward outcome to influence the probability of transitioning between states, and a 4-parameter unrestricted HMM with no promoter tying (ntHMM) that allows biased exploitation (see methods). The model comparisons have shown that the 2 parameter HMM was the simplest, most interpretable, and best fit model (AIC: 2 parameter HMM, AIC = 2976.1; ioHMM, AIC = 3117; ntHMM, AIC = 3101.5, see more statistics reported in Methods). Therefore, we selected the simple 2-parameter HMM to infer the likelihood that each choice was part of the exploratory regime, or the exploitative one (see Methods).”</p><p>page 33, line 792-807:</p><p>“To account for the effect of reward on choice dynamics, we extended the 2-parameter HMM model to an input-output HMM model (4-parameter ioHMM), whose framework allows inputs, such as reward outcomes, to influence the probability of transitioning between states (Bengio and Frasconi ; Ebitz et al., 2019). […] While reward outcomes could affect the probability of state transitions, the model comparison suggested that the extra input layer did not explain more variance in choice dynamics, and therefore, we would favor the original, simpler 2-parameter HMM model.”</p><disp-quote content-type="editor-comment"><p>(1.2) Proposed HMM model also assumes that exploit states are uniform across the options. Do authors have any evidence supporting this assumption? Side biases are commonly observed in animals and humans. Extracted RL parameters also confirm this. Please comment.</p></disp-quote><p>We have included a new HMM that allows for bias exploitation as suggested by the reviewer, but this change did not improve model fit. We absolutely agree with the reviewers that exploitation can be biased, as mice do display side bias to some extent. One issue we face with an unrestricted HMM model with no parameter tying is that the unfavored choice would have fewer trials for parameter estimation, especially in animals with strong side bias. Tying the parameters in the HMM model is a common practice for more accurate parameter estimation, but we recognize the limitations. Here, in our original model, tying the parameters allowed us to more accurately estimate the parameters of the transition matrix using information from both the left side and the right side and avoid overfitting. Again, with the amount of data we have, we decide to go with the simpler form of model that captures the processes that we are interested in. However, in the future we are interested in trying out these more sophisticated models that better capture more complex latent processes.</p><p>In the revised manuscript, we have included the two more sophisticated HMM models as described above, along with the results of model comparisons. We also added more justification to the HMM we used and we apologize for the failure on our part to clearly explain our methodology and analytic approaches, so we have revised the manuscript substantially to add clarity.</p><p>New text from the manuscript (page 33, line 808-821):</p><p>“To account for the effect of biased exploitation on the probability of transitioning between states, we also considered an unrestricted HMM model with no parameter tying (4 parameter ntHMM), where we treat exploiting the left side and exploiting the right side as two separate exploit states and allow differential transition probability to each exploit state. However, the ntHMM did not improve model fit with almost identical log-likelihood (2-parameter original HMM: log-likelihood = -1424.0; 4-parameter ntHMM: log-likelihood = -1423.8). Then we compared two models by calculating the AIC/BIC values for both models, which penalized extra parameters in the no parameter-tying HMM. The AIC test favored the original 2-parameter model (AIC: 2-parameter original HMM: AIC = 2976.1; 4-parameter ntHMM: AIC = 3103.5; relative likelihood (AIC weight) of the 4-parameter ntHMM &lt; 10^-28). The BIC test also favored the simpler 2-parameter model (BIC: 2-parameter original HMM: BIC = 3562.8; 4-parameter ntHMM: BIC = 4276.9; relative likelihood (BIC weight) of the 4-parameter ioHMM &lt; 10^-155). In the light of the model comparison results, we decided to fit the simpler 2-parameter HMM model.”</p><disp-quote content-type="editor-comment"><p>(1.3) Moreover, the model assumes that the mice had to pass through exploration in order to start exploiting a new option. Do authors have any evidence supporting this assumption? What will happen to the results if this assumption is lifted? Please comment.</p></disp-quote><p>We apologize for not providing clearer justification for this assumption in the manuscript. We have revised the text to better explain this assumption from a normative point of view and a computational point of view.</p><p>The utility of exploration is to maximize information about the environment, whereas the goal of exploitation is to maximize reward or gains (Mehlhorn et al., 2015). In either of these commonly used definitions of exploration, if an animal switches from a bout of exploiting one option to another option, that very first trial after switching should be exploratory because the outcome or reward contingency of that new option is unknown and that behavior of switching aims to gain information. Therefore, if this assumption is lifted, the states that were inferred from HMM may not align with the normative definition of exploration and exploitation.</p><p>A second reason is that allowing direct transition between exploit states will increase the number of parameters to estimate in the HMM model. As we mentioned above, we do not have enough data to accurately estimate the parameters that account for the probability of direct transition between exploit states (due to the low occurrence of such circumstances).</p><p>New text from the manuscript (page32, line 771-781):</p><p>“The model also assumed that the mice had to pass through exploration in order to start exploiting a new option, even if only for a single trial. This is because the utility of exploration is to maximize information about the environment, as defined in both animal foraging literature and reinforcement learning models (Mehlhorn et al., 2015). If an animal switches from a bout of exploiting one option to another option, that very first trial after switching should be exploratory because the outcome or reward contingency of that new option is unknown and that behavior of switching aims to gain information. Through fixing the emissions model, constraining the structure of the transmission matrix, and tying the parameters, the final HMM had only two free parameters: one corresponding to the probability of exploring, given exploration on the last trial, and one corresponding to the probability of exploiting, given exploitation on the last trial.”</p><disp-quote content-type="editor-comment"><p>2) Page 10, line 252: Please provide a more quantitative comparison of models’ choice behavior (and not just RLCK) and the animals’ behavior for all sessions. Also, there are no tick-marks on the y-axis.</p></disp-quote><p>Thank you for your feedback. To quantitative how well each RL model was predicting animals’ choices, we examined model agreement for each model, which is calculated as the average probability of choice being correctly predicted by the model p (choice|model). Then, we conducted a multiple comparison across model agreement of RL models (Supplemental Table 2, Figure 3C). The results suggested that the RLCK model was the most simple model that best predicted animals’ choices. The tick marks have been added to the figure.</p><p>New text from the manuscript (page 13, line 341-356):</p><p>“To quantify how well each RL model was at predicting animals’ choices, we measured the model agreement for each model, which was calculated as the probability of choice correctly predicted by the optimized model parameters for each model (Figure 3C). Then we conducted a multiple comparison across model agreement of RL models (test statistics reported in Supplemental Table 2). The results suggested that the RL models with parameter(s) that account for choice bias (RLCK, RLCKγ, RLCKη) were significantly better at predicting animals’ actual choices than the models that do not account for choice bias and non-RL models (random, noisy WSLS, RL, RLε). There was no significant difference in model agreement between RLCK, RLCKγ, and RLCKη. Based on the result of model comparison (AIC) and model agreement, we decided that the four-parameter RLCK model is the simplest, best-fit model that best predicted animal’ actual choices. Finally, to visualize how well the RLCK model was at predicting choices of animals with different learning performance, we plotted the simulated choices and actual choices against the matching law (Poling et al., 2011), which dictates that the probability of choice is proportional to the probability of reward. The figure showed that this four-parameter model was able to characterize animals’ choice behaviors regardless of the value-based learning performance (Supplemental Figure 3).”</p><disp-quote content-type="editor-comment"><p>3) How much overlap exists between the extracted latent dynamics from HMM and that of previously proposed models mentioned in the methods (Daw et al., 2006; Jepma and Nieuwenhuis, 2011; Pearson et al., 2009)? It would be helpful to show the extent that results from these different methods deviate/overlap with each other.</p></disp-quote><p>That is a very interesting question, we have included a side-by-side comparison between RL-labeled explore-exploit states and HMM-labeled states. Previous studies have used reinforcement learning related models to estimate the values underlying decision-making (much as we do here with the various versions of the RL models), then identify exploration as the choices that do not match the values predicted by the model. Although differences between the tasks (i.e. probabilistic rewards, rather than reward magnitudes drawn from gaussian distributions) mean we cannot directly apply the exact model Daw et al., 2006 developed for their task, we can examine the relationship between choices that do not maximize reward per our best-fitting RL model and the exploratory choices inferred from the HMM.</p><p>We first examined the correlation between explore-exploit states inferred by the HMM model and the RLCK model and found that states inferred by these two methods are moderately correlated (Supplemental Figure 2B, r<sub>tet</sub> = 0.42). Both HMM-inferred states and RL inferred states showed similar effects on response time – response time was significantly longer during exploration than exploitation (RL label: paired t-test, t(31) = 2.08, p = 0.046; HMM label: t(31) = 3.66, p = 0.0009). However, the effect size of HMM labels on response time was over twice as big as that of RL labels (HMM: R<sup>2</sup> = 0.30; RL: R<sup>2</sup> = 0.12). This analysis highlights the overlaps of two methods (HMM and RL model) in inferring explore-exploit states but the HMM explained more variance in response time. We also calculated the standardized regression coefficients to measure how much of the response time is explained by states odellin by HMM model and RLCK model (Supplemental Figure 2C). Again, the result suggested that the HMM-inferred states explained significantly more variance in response time than the RL-inferred states in explaining response time.</p><p>While both models have demonstrated capacity to differentiate between states (HMM did a better job than the RL model), the RL models label choices that are inconsistent with model prediction as exploratory. However, exploration as a non-reward-maximizing goal should be orthogonal to reward instead of errors. A few recent papers have explained this problem in some depth (Ebitz et al., 2018; Wilson et al., 2021). We have included a Supplemental Figure panel and more text to compare these two state-labelling methods and we believe this change better justified our decision to use the HMM model to infer states and has strengthened our claims.</p><p>New text from the manuscript (page 6, line 147-222):</p><p>“To test this hypothesis, we first need a method to label each choice as an exploratory choice or an exploitative choice. […] Because this approach to infer exploration is agnostic to the generative computations and depends only on the temporal statistics of choices (Ebitz et al., 2018; Wilson et al., 2021; Ebitz et al., 2019; Ebitz et al., 2020), it is particularly ideal for circumstances like this one, where we suspect that the generative computations may differ across groups.”</p><disp-quote content-type="editor-comment"><p>4) Do authors see any differences between amount of exploration/exploitation at the beginning vs at the end of a session? How about across days? The fact that meta learning is observed, suggests that even in a single session, changes in the strategy of animals might be expected.</p></disp-quote><p>We agree with the reviewer that finding differences in exploration over time, either within or between sessions, would add an interesting wrinkle to the story, but unfortunately we did not observe any adaptation in the probability of exploration.</p><p>We have included the analyses in the text of the manuscript as follows (page 10, line 254-257):</p><p>“We calculated the probability of exploration early, mid, and late session. However, we failed to see changes in the amount of exploration within sessions. We also examined the amount of exploration across sessions, and we found that there were no changes in the amount of exploration across sessions in both males and females.”</p><p>It is difficult to know whether this null result means that there is no adaptation. Alternatively, we may just not be able to measure adaptation due to variability in reward schedules over time or across trials. However, we believe this is an important question for future research and appreciate the comment.</p></body></sub-article></article>