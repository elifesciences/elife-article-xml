<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98117</article-id><article-id pub-id-type="doi">10.7554/eLife.98117</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98117.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Human EEG and artificial neural networks reveal disentangled representations and processing timelines of object real-world size and depth in natural images</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Lu</surname><given-names>Zitong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7953-6742</contrib-id><email>zitonglu@mit.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Golomb</surname><given-names>Julie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3489-0702</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rs6vg23</institution-id><institution>Department of Psychology, The Ohio State University</institution></institution-wrap><addr-line><named-content content-type="city">Columbus</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>McGovern Institute for Brain Research, Massachusetts Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>SP</surname><given-names>Arun</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>12</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP98117</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-13"><day>13</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-03-21"><day>21</day><month>03</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.19.553999"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-11"><day>11</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98117.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-10-31"><day>31</day><month>10</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98117.2"/></event></pub-history><permissions><copyright-statement>© 2024, Lu and Golomb</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Lu and Golomb</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98117-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98117-figures-v1.pdf"/><abstract><p>Remarkably, human brains have the ability to accurately perceive and process the real-world size of objects, despite vast differences in distance and perspective. While previous studies have delved into this phenomenon, distinguishing the processing of real-world size from other visual properties, like depth, has been challenging. Using the THINGS EEG2 dataset with human EEG recordings and more ecologically valid naturalistic stimuli, our study combines human EEG and representational similarity analysis to disentangle neural representations of object real-world size from retinal size and perceived depth, leveraging recent datasets and modeling approaches to address challenges not fully resolved in previous work. We report a representational timeline of visual object processing: object real-world depth processed first, then retinal size, and finally, real-world size. Additionally, we input both these naturalistic images and object-only images without natural background into artificial neural networks. Consistent with the human EEG findings, we also successfully disentangled representation of object real-world size from retinal size and real-world depth in all three types of artificial neural networks (visual-only ResNet, visual-language CLIP, and language-only Word2Vec). Moreover, our multi-modal representational comparison framework across human EEG and artificial neural networks reveals real-world size as a stable and higher-level dimension in object space incorporating both visual and semantic information. Our research provides a temporally resolved characterization of how certain key object properties – such as object real-world size, depth, and retinal size – are represented in the brain, which offers further advances and insights into our understanding of object space and the construction of more brain-like visual models.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>real-world size</kwd><kwd>depth perception</kwd><kwd>representational similarity analysis</kwd><kwd>RSA</kwd><kwd>artificial neural networks</kwd><kwd>object recognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY025648</award-id><principal-award-recipient><name><surname>Golomb</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>U.S. National Science Foundation</institution></institution-wrap></funding-source><award-id>1848939</award-id><principal-award-recipient><name><surname>Golomb</surname><given-names>Julie</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural and computational evidence reveals that real-world size is a temporally late, semantically grounded, and hierarchically stable dimension of object representation in both human brains and artificial neural networks.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Imagine you are viewing an apple tree while walking around an orchard: as you change your perspective and distance, the retinal size of the apple you plan to pick varies, but you still perceive the apple as having a constant real-world size. How do our brains extract object real-world size information during object recognition to allow us to understand the complex world? Behavioral studies have demonstrated that perceived real-world size is represented as an object physical property, revealing same-size priming effects (<xref ref-type="bibr" rid="bib57">Setti et al., 2009</xref>), familiar-size stroop effects (<xref ref-type="bibr" rid="bib35">Konkle and Oliva, 2012a</xref>; <xref ref-type="bibr" rid="bib42">Long and Konkle, 2017</xref>), and canonical visual size effects (<xref ref-type="bibr" rid="bib6">Chen et al., 2022</xref>; <xref ref-type="bibr" rid="bib34">Konkle and Oliva, 2011</xref>). Human neuroimaging studies have also found evidence of object real-world size representation (<xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="bib32">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib36">Konkle and Oliva, 2012b</xref>; <xref ref-type="bibr" rid="bib48">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib54">Quek et al., 2023</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2022a</xref>). These findings suggest real-world size is a fundamental dimension of object representation.</p><p>However, previous studies on object real-world size have faced several challenges. Firstly, the perception of an object’s real-world size is closely related to the perception of its real-world distance in depth. For instance, imagine you are looking at photos of an apple and a basketball: if the two photos were zoomed in such that the apple and the basketball filled the same exact retinal (image) size, you could still easily perceive that the apple is the physically smaller real-world object. But you would simultaneously infer that the apple is thus located closer to you (or the camera) than the basketball. In previous neuroimaging studies of perceived real-world size (<xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="bib37">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib36">Konkle and Oliva, 2012b</xref>), researchers presented images of familiar objects zoomed and cropped such that they occupied the same retinal size, finding that neural responses in ventral temporal cortex reflected the perceived real-world size (e.g. an apple smaller than a car). However, while they controlled the retinal size of objects, the intrinsic correlation between real-world size and real-world depth in these images meant that the influence of perceived real-world depth could not be entirely isolated when examining the effects of real-world size. This makes it difficult to ascertain whether their results were driven by neural representations of perceived real-world size and/or perceived real-world depth. MEG and EEG studies focused on temporal processing of object size representations <xref ref-type="bibr" rid="bib32">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2022a</xref> have been similarly susceptible to this limitation. Indeed, one recent study (<xref ref-type="bibr" rid="bib54">Quek et al., 2023</xref>) provided evidence that perceived real-world depth could influence real-world size representations, further illustrating the necessity of investigating pure real-world size representations in the brain. Secondly, the stimuli used in these studies were cropped object stimuli against a plain white or gray background, which are not particularly naturalistic. More and more studies and datasets have highlighted the important role of naturalistic context in object recognition (<xref ref-type="bibr" rid="bib1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="bib18">Gifford et al., 2022</xref>; <xref ref-type="bibr" rid="bib21">Grootswagers et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Hebart et al., 2019</xref>; <xref ref-type="bibr" rid="bib59">Stoinski et al., 2024</xref>). In ecological contexts, inferring the real-world size/distance of an object likely relies on a combination of bottom-up visual information and top-down knowledge about canonical object sizes for familiar objects. Incorporating naturalistic background context in experimental stimuli may produce more accurate assessments of the relative influences of visual shape representations (<xref ref-type="bibr" rid="bib5">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Bracci and Op de Beeck, 2016</xref>; <xref ref-type="bibr" rid="bib53">Proklova et al., 2016</xref>) and higher-level semantic information (<xref ref-type="bibr" rid="bib15">Doerig et al., 2022</xref>; <xref ref-type="bibr" rid="bib27">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="bib62">Wang et al., 2022b</xref>). Furthermore, most previous studies have tended to categorize size rather broadly, such as merely differentiating between big and small objects (<xref ref-type="bibr" rid="bib32">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Konkle and Oliva, 2012b</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2022a</xref>) or dividing object size into seven levels from small to big. To more finely investigate the representation of object size in the brain, it may be necessary to obtain a more continuous measure of size for a more detailed characterization.</p><p>Certainly, a minority of fMRI studies have attempted to utilize natural images and also engaged in more detailed size measurements to more precisely explore the encoding of object real-world size in different brain areas (<xref ref-type="bibr" rid="bib48">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib60">Troiani et al., 2014</xref>). However, no study has yet comprehensively overcome all the challenges and unfolded a clear processing timeline for object retinal size, real-world size, and real-world depth in human visual perception.</p><p>In the current study, we overcome these challenges by combining human EEG recordings, naturalistic stimulus images, artificial neural networks, and computational modeling approaches including representational similarity analysis (RSA) and partial correlation analysis to distinguish the neural representations of object real-world size, retinal size, and real-world depth. We applied our integrated computational approach to an open EEG dataset, THINGS EEG2 (<xref ref-type="bibr" rid="bib18">Gifford et al., 2022</xref>). Firstly, the visual image stimuli used in this dataset are more naturalistic and include objects that vary in real-world size, depth, and retinal size. This allows us to employ a multi-model representational similarity analysis to investigate relatively unconfounded representations of object real-world size, partialing out – and simultaneously exploring – these confounding features. Secondly, we are able to explore the neural dynamics of object feature processing in a more ecological context based on natural images in human object recognition. Thirdly, instead of categorizing object size into discrete levels, we applied a more continuous measure based on detailed behavioral measurements from an online size rating task, allowing us to more finely decode the representation of object size in the brain.</p><p>We first focus on unfolding the neural dynamics of statistically isolated object real-world size representations. The temporal resolution of EEG allows us the opportunity to investigate the representational time course of visual object processing, asking whether processing of perceived object real-world size precedes or follows processing of perceived depth, if these two properties are in fact processed independently.</p><p>We then attempt to further explore the underlying mechanisms of how human brains process object size and depth in natural images by integrating artificial neural networks (ANNs). In the domain of cognitive computational neuroscience, ANNs offer a complementary tool to study visual object recognition, and an increasing number of studies support that ANNs exhibit representations similar to human visual systems (<xref ref-type="bibr" rid="bib9">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib65">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Yamins and DiCarlo, 2016</xref>). Indeed, a recent study found that ANNs also represent real-world size (<xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>); however, their use of a fixed retinal size image dataset with the same cropped objects as described above makes it similarly challenging to ascertain whether the results reflected real-world size and/or depth. Additionally, some recent work indicates that artificial neural networks incorporating semantic embedding and multimodal neural components might more accurately reflect human visual representations within visual areas and even the hippocampus, compared to vision-only networks (<xref ref-type="bibr" rid="bib7">Choksi et al., 2022a</xref>; <xref ref-type="bibr" rid="bib8">Choksi et al., 2022b</xref>; <xref ref-type="bibr" rid="bib12">Conwell et al., 2022</xref>; <xref ref-type="bibr" rid="bib15">Doerig et al., 2022</xref>; <xref ref-type="bibr" rid="bib28">Jozwik et al., 2023</xref>; <xref ref-type="bibr" rid="bib62">Wang et al., 2022b</xref>). Given that perception of real-world size may incorporate both bottom-up visual and top-down semantic knowledge about familiar objects, these models offer yet another novel opportunity to investigate this question. Utilizing both visual and visual-semantic models, as well as different layers within these models, ANNs provide us the approach to extract various image features, low-level visual information from early layers, and higher-level information including both visual and semantic features from late layers.</p><p>The integrated computational approach by cross-modal representational comparisons we take with the current study allows us to compare how representations of perceived real-world size and depth emerge in both human brains and artificial neural networks. Unraveling the internal representations of object size and depth features in both human brains and ANNs enables us to investigate how distinct spatial properties—retinal size, real-world depth, and real-world size—are encoded across systems, and to uncover the representational mechanisms and temporal dynamics through which real-world size emerges as a potentially higher-level, semantically grounded feature.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We conducted a cross-modal representational similarity analysis (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>, see Materials and methods section for details) comparing the patterns of human brain activation while participants viewed naturalistic object images (timepoint-by-timepoint decoding of EEG data), the output of different layers of artificial neural networks and semantic language models fed the same stimuli (ANN and Word2Vec models), and hypothetical patterns of representational similarity based on behavioral and mathematical measurements of different visual image properties (perceived real-world object size, displayed retinal object size, and inferred real-world object depth).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of our analysis pipeline including constructing three types of RDMs and conducting comparisons between them.</title><p>We computed RDMs from three sources: neural data (EEG), hypothesized object features (real-world size, retinal size, and real-world depth), and artificial models (ResNet, CLIP, and Word2Vec). Then we conducted cross-modal representational similarity analyses between: EEG ×HYP (partial correlation, controlling for other two HYP features), ANN (or W2V) ×HYP (partial correlation, controlling for other two HYP features), and EEG ×ANN (correlation).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Four ANN RDMs of ResNet early layer, ResNet late layer, CLIP early layer, and CLIP late layer.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Word2Vec RDM.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig1-figsupp2-v1.tif"/></fig></fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Methods for calculating neural (EEG), hypothesis-based (HYP), and artificial neural network (ANN) &amp; semantic language processing (Word2Vec, W2V) model-based representational dissimilarity matrices (RDMs).</title><p>(<bold>A</bold>) Steps of computing the neural RDMs from EEG data. EEG analyses were performed in a time-resolved manner on 17 channels as features. For each time t, we conducted pairwise cross-validated SVM classification. The classification accuracy values across different image pairs resulted in each 200×200 RDM for each time point. (<bold>B</bold>) Calculating the three hypothesis-based RDMs: Real-World Size RDM, Retinal Size RDM, and Real-World Depth RDM. Real-world size, retinal size, and real-world depth were calculated for the object in each of the 200 stimulus images. The number in the bracket represents the rank (out of 200, in ascending order) based on each feature corresponding to the object in each stimulus image (e.g. ‘ferry’ ranks 197th in real-world size from small to big out of 200 objects). The connection graph to the right of each RDM represents the relative representational distance of three stimuli in the corresponding feature space. (<bold>C</bold>) Steps of computing the ANN and Word2Vec RDMs. For ANNs, the inputs were the resized images, and for Word2Vec, the inputs were the words of object concepts. For clearer visualization, the shown RDMs were separately histogram-equalized (percentile units).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig2-v1.tif"/></fig><sec id="s2-1"><title>Dynamic representations of object size and depth in human brains</title><p>To explore if and when human brains contain distinct representations of perceived real-world size, retinal size, and real-world depth, we constructed timepoint-by-timepoint EEG neural RDMs (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), and compared these to three hypothesis-based RDMs corresponding to different visual image properties (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Firstly, we confirmed that the hypothesis-based RDMs were indeed correlated with each other (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), and without accounting for the confounding variables, Spearman correlations between the EEG and each hypothesis-based RDM revealed overlapping periods of representational similarity (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In particular, representational similarity with real-world size (from 90 to 120ms and from 170 to 240ms) overlapped with the significant time windows of other features, including retinal size from 70 to 210ms, and real-world depth from 60 to 130ms and from 180 to 230ms. But critically, with the partial correlations, we isolated their independent representations. The partial correlation results reveal a relatively unconfounded representation of object real-world size in the human brain from 170 to 240ms after stimulus onset, independent from retinal size and real-world depth, which showed significant representational similarity at different time windows (retinal size from 90 to 200ms, and real-world depth from 60 to 130ms and 270–300ms; <xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cross-modal RSA results.</title><p>(<bold>A</bold>) Similarities (Spearman correlations) between three hypothesis-based RDMs. Asterisks indicate a significant similarity, p&lt;0.05. (<bold>B</bold>) Representational similarity time courses (full Spearman correlations) between EEG neural RDMs and hypothesis-based RDMs. (<bold>C</bold>) Temporal latencies for peak similarity (partial Spearman correlations) between EEG and the three types of object information. Error bars indicate ± SEM. Asterisks indicate significant differences across conditions (p&lt;0.05); (<bold>D</bold>) Representational similarity time courses (partial Spearman correlations) between EEG neural RDMs and hypothesis-based RDMs. (<bold>E</bold>) Representational similarities (partial Spearman correlations) between the four ANN RDMs and hypothesis-based RDMs of real-world depth, retinal size, and real-world size. Asterisks indicate significant partial correlations (bootstrap test, p&lt;0.05). (<bold>F</bold>) Representational similarity time courses (Spearman correlations) between EEG neural RDMs and ANN RDMs. Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Shaded area reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Representational similarities (partial Spearman correlations) between ANN RDMs from multiple layers in ResNet and CLIP and hypothesis-based RDMs of real-world depth, retinal size, and real-world size (as extended results of <xref ref-type="fig" rid="fig3">Figure 3E</xref>).</title><p>Asterisks indicate significant partial correlations (bootstrap test, p&lt;0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Representational similarity time courses (Spearman correlations) between EEG neural RDMs and ANN RDMs from multiple layers (as extended results of <xref ref-type="fig" rid="fig3">Figure 3F</xref>).</title><p>(<bold>A</bold>) Similarity between EEG and ResNet’s multiple layers (from early layer to late layer: ResNet.maxpool layer, ResNet.layer1 layer, ResNet.layer2 layer, ResNet.layer3 layer, ResNet.layer4 layer, ResNet.avgpool). (<bold>B</bold>) Similarity between EEG and CLIP’s multiple layers (from early layer to late layer: CLIP.visual.avgpool layer, CLIP.visual.layer1 layer, CLIP.visual.layer2 layer, CLIP.visual.layer3 layer, CLIP.visual.layer4 layer, CLIP.visual.attnpool). Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Shaded area reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Representational similarity time courses (partial Spearman correlations) between EEG neural RDMs and ANN RDMs controlling for the three hypothesis-based RDMs.</title><p>Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Shaded area reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Noise ceiling of representational similarity analysis based on EEG neural RDMs.</title><p>Shaded area reflects the lower and upper bound.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>RSA results based on CORnet.</title><p>(<bold>A</bold>) Representational similarities (partial Spearman correlations) between the five CORnet RDMs from multiple layers (from early layer to late layer: CORnet.V1, CORnet.V2, CORnet.V4, CORnet.IT, and CORnet.decoder.avgpool) and hypothesis-based RDMs of real-world depth, retinal size, and real-world size. Asterisks indicate significant partial correlations (bootstrap test, p&lt;0.05). (<bold>B</bold>) Representational similarity time courses (Spearman correlations) between EEG neural RDMs and the five CORnet RDMs. Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Shaded area reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig3-figsupp5-v1.tif"/></fig></fig-group><p>Peak latency results showed that neural representations of real-world size, retinal size, and real-world depth reached their peaks at different latencies after stimulus onset (real-world depth: ~87ms, retinal size: ~138ms, real-world size: ~206ms, <xref ref-type="fig" rid="fig3">Figure 3C</xref>). The representation of real-world size had a significantly later peak latency than that of both retinal size, <italic>t</italic>(9)=4.30, p=0.002, and real-world depth, <italic>t</italic>(9)=18.58, p&lt;0.001. And retinal size representation had a significantly later peak latency than real-world depth, <italic>t</italic>(9)=3.72, p=0.005. These varying peak latencies imply an encoding order for distinct visual features, transitioning from real-world depth through retinal size, and then to real-world size.</p></sec><sec id="s2-2"><title>Artificial neural networks also reflect distinct representations of object size and depth</title><p>To test how ANNs process these visual properties, we input the same stimulus images into ANN models and got their latent features from early and late layers (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), and then conducted comparisons between the ANN RDMs and hypothesis-based RDMs. Parallel to our findings of dissociable representations of real-world size, retinal size, and real-world depth in the human brain signal, we also found dissociable representations of these visual features in ANNs (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Our partial correlation RSA analysis showed that early layers of both ResNet and CLIP had significant real-world depth and retinal size representations, whereas the late layers of both ANNs were dominated by real-world size representations, though there was also weaker retinal size representation in the late layer of ResNet and real-world depth representation in the late layer of CLIP (additional results of the extended analysis of multiple layers in ResNet and CLIP are shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The detailed statistical results are shown in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1, table A</xref>.</p><p>Thus, ANNs provide another approach to understand the formation of different visual features, offering convergent results with the EEG representational analysis, where retinal size was reflected most in the early layers of ANNs, while object real-world size representations didn’t emerge until late layers of ANNs, consistent with a potential role of higher-level visual information, such as the semantic information of object concepts.</p></sec><sec id="s2-3"><title>Representational similarity between human EEG and artificial neural networks</title><p>To directly examine the representational similarity between ANNs and human EEG signals, we compared the timepoint-by-timepoint EEG neural RDMs and the ANN RDMs. This analysis allowed us to assess how different stages of visual processing in the human brain align temporally with hierarchical representations in ANNs. As shown in <xref ref-type="fig" rid="fig3">Figure 3F</xref>, the early layer representations of both ResNet and CLIP (ResNet.maxpool layer and CLIP.visual.avgpool) showed significant correlations with early EEG time windows (early layer of ResNet: 40–280ms, early layer of CLIP: 50–130ms and 160–260ms), while the late layers (ResNet.avgpool layer and CLIP.visual.attnpool layer) showed correlations extending into later time windows (late layer of ResNet: 80–300ms, late layer of CLIP: 70–300ms). Although there is substantial temporal overlap between early and late model layers, the overall pattern suggests a rough correspondence between model hierarchy and neural processing stages.</p><p>We further extended this analysis across intermediate layers of both ResNet and CLIP models (from early to late, ResNet: ResNet.maxpool, ResNet.layer1, ResNet.layer2, ResNet.layer3, ResNet.layer4, ResNet.avgpool; from early to late, CLIP: CLIP.visual.avgpool, CLIP.visual.layer1, CLIP.visual.layer2, CLIP.visual.layer3, CLIP.visual.layer4, CLIP.visual.attnpool). The results, now included in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, show a consistent trend: early layers exhibit higher similarity to early EEG time points, and deeper layers show increased similarity to later EEG stages. This pattern of early-to-late correspondence aligns with previous findings that convolutional neural networks exhibit similar hierarchical representations to those in the brain visual cortex (<xref ref-type="bibr" rid="bib9">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib33">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Yamins and DiCarlo, 2016</xref>): that both the early stage of brain processing and the early layer of the ANN encode lower-level visual information, while the late stage of the brain and the late layer of the ANN encode higher-level visual information. Notably, early brain responses showed stronger similarity to early ResNet layers than to CLIP layers, consistent with prior work suggesting that early visual processing is more closely aligned with purely visual models (<xref ref-type="bibr" rid="bib20">Greene and Hansen, 2020</xref>). In contrast, at later time windows, brain activity more closely resembled late CLIP layers, possibly reflecting the integration of visual and semantic information. However, it is also possible that these differences between ResNet and CLIP reflect differences in training data scale and domain.</p><p>To contextualize how much of the shared variance between EEG and ANN representations is driven by the specific visual object features we tested above, we conducted a partial correlation analysis between EEG RDMs and ANN RDMs controlling for the three hypothesis-based RDMs (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). The EEG ×ANN similarity results remained largely unchanged, suggesting that ANN representations capture much more additional rich representational structure beyond these features. Similarly, a supplemental EEG noise ceiling analysis (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>) shows that the observed EEG–HYP similarity values are substantially below a theoretical upper bound of explainable variance. This outcome is fully expected: Each of our HYP RDMs captures only a specific aspect of the neural representational structure, rather than attempting to account for the totality of the EEG or ANN signal. Our goal is not to optimize model performance or maximize fit, but to probe if these different components of object information are reflected – and dissociated – in the temporal dynamics and processing stages of the brain’s responses. In parallel, these supplemental findings help to situate our hypothesis-driven analysis within the broader representational capacity of the brain and the models.</p></sec><sec id="s2-4"><title>Real-world size as a stable and higher-level dimension in object space</title><p>An important aspect of the current study is the use of naturalistic visual images as stimuli, in which objects were presented in their natural contexts, as opposed to cropped images of objects without backgrounds. In natural images, background can play an important role in object perception. How dependent are the above results on the presence of naturalistic background context? To investigate how image context influences object size and depth representations, we next applied a reverse engineering method, feeding the ANNs with modified versions of the stimulus images containing cropped objects without background, and evaluating the ensuing ANN representations compared to the same original hypothesis-based RDMs. If the background significantly contributes to the formation of certain feature representations, we may see some encoding patterns in ANNs disappear when the input only includes the pure object but no background.</p><p>Compared to results based on images with background, the ANNs based on cropped-object modified images showed weaker overall representational similarity for all features (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In the early layers of both ANNs, we now only observed significantly preserved retinal size representations (which is a nice validity check, since retinal size measurements were based purely on the physical object dimensions in the image, independent of the background). Real-world depth representations were almost totally eliminated, with only a small effect in the late layer of ResNet. However, we still observed a preserved pattern of real-world size representations, with significant representational similarity in the late layers of both ResNet and CLIP, and not in the early layers. The detailed statistical results are shown in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1, table B</xref>. Even though the magnitude of representational similarity for object real-world size decreased when we removed the background, this high-level representation was not entirely eliminated. This finding suggests that background information does indeed influence object processing, but the representation of real-world size seems to be a relatively stable higher-level feature. On the other hand, representational formats of real-world depth changed when the input lacked background information. The deficiency of real-world depth representations in early layers, compared to when using full-background images, might suggest that the human brain typically uses background information to estimate object depth, though the significant effect in the late layer of ResNet in background-absent condition might also suggest that the brain (or at least ANN) has additional ability to integrate size information to infer depth when there is no background. These results show that real-world size emerges in the later layers of ANNs regardless of image background information, and – based on our prior EEG results – although we could not test object-only images in the EEG data, we hypothesize that a similar temporal profile would be observed in the brain, even for object-only images.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Contribution of image backgrounds to object size and depth representations.</title><p>Representational similarity results (partial Spearman correlations) between ANNs fed inputs of cropped object images without backgrounds and the hypothesis-based RDMs. Stars above bars indicate significant partial correlations (bootstrap test, p&lt;0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Four ANN RDMs with inputs of cropped object images without background of ResNet early layer, ResNet late layer, CLIP early layer, and CLIP late layer.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig4-figsupp1-v1.tif"/></fig></fig-group><p>The relative robustness of real-world size representations – even in the absence of background – raises an important question: might real-world size be encoded as a more abstract, conceptual-level dimension in the brain’s object representation space? If so, we might expect it to be driven not only by higher-level visual information, but also potentially by purely semantic information about familiar objects. To test this, we extracted object names from each image and input the object names into a Word2Vec model to obtain a Word2Vec RDM (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), and then conducted a partial correlation RSA comparing the Word2Vec representations with the hypothesis-based RDMs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The results showed a significant real-world size representation (<italic>r</italic>=0.1871, p&lt;0.001) but no representation of retinal size (<italic>r</italic>=−0.0064, p=0.8148) or real-world depth (<italic>r</italic>=−0.0040, p=0.7151) from Word2Vec. Also, the significant time window (90–300ms) of similarity between Word2Vec RDM and EEG RDMs (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) contained the significant time window of EEG x real-world size representational similarity (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Representation similarity with a non-visual semantic language processing model (Word2Vec) fed word inputs corresponding to the images’ object concepts.</title><p>(<bold>A</bold>) Representational similarity results (partial Spearman correlations) between Word2Vec RDM and hypothesis-based RDMs. Stars above bars indicate significant partial correlations (bootstrap test, p&lt;0.05). (<bold>B</bold>) Representational similarity time course (Spearman correlations) between EEG RDMs (neural activity while viewing images) and Word2Vec RDM (fed corresponding word inputs). Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Line width reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Representational similarity time courses (partial Spearman correlations) between EEG neural RDMs and the Word2Vec RDM controlling for four ANN RDMs (ResNet early/late and CLIP early/late layers).</title><p>Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Shaded area reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Unique and shared variance of real-world size and semantic information (from Word2Vec) in EEG neural representations using variance partitioning analysis while controlling retinal size and real-world depth.</title><p>Color-coded small dots at the top indicate significant timepoints (cluster-based permutation test, p&lt;0.05). Shaded area reflects ± SEM across the 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98117-fig5-figsupp2-v1.tif"/></fig></fig-group><p>To further probe the relationship between real-world size and semantic information, and to examine whether Word2Vec captures variances in EEG signals beyond that explained by visual models, we conducted two additional analyses. First, we performed a partial correlation between EEG RDMs and the Word2Vec RDM, while regressing out four ANN RDMs (early and late layers of both ResNet and CLIP) (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We found that semantic similarity remained significantly correlated with EEG signals across sustained time windows (100–190ms and 250–300ms), indicating that Word2Vec captures neural variance not fully explained by visual or visual-language models. Second, we conducted a variance partitioning analysis, in which we decomposed the variance in EEG RDMs explained by three hypothesis-based RDMs and the semantic RDM (Word2Vec RDM), and we still found that real-world size explained unique variance in EEG even after accounting for semantic similarity (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). And we also observed a substantial shared variance jointly explained by real-world size and semantic similarity and a unique variance of semantic information. These results suggest that real-world size is indeed partially semantic in nature, but also has independent neural representation not fully explained by general semantic similarity.</p><p>Both the reverse engineering manipulation and Word2Vec findings corroborate that object real-world size representation, unlike retinal size and real-world depth, emerges in both image- and semantic-level in object space.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our study applied computational methods to distinguish the representations of objects’ perceived real-world size, retinal size, and inferred real-world depth features in both human brains and ANNs. Consistent with prior studies reporting real-world size representations (<xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="bib32">Khaligh-Razavi et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib36">Konkle and Oliva, 2012b</xref>; <xref ref-type="bibr" rid="bib48">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib54">Quek et al., 2023</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2022a</xref>), we found that both human brains and ANNs contain significant information about real-world size. Critically, our study goes beyond the contributions of prior studies in several key ways, offering both theoretical and methodological advances: (a) we eliminated the confounding impact of perceived real-world depth (in addition to retinal size) on the real-world size representation; (b) we conducted analyses based on more ecologically valid naturalistic images; (c) we obtained precise feature values for each object in every image, instead of simply dividing objects into two or seven coarse categories; and (d) we utilized a multi-modal, partial correlation RSA that combines EEG, hypothesis-based models, and ANNs. Our approach allowed us to investigate representational time courses and reverse engineering manipulations in unparalleled detail. By integrating EEG data with hypothesis-based models and ANNs, this method offers a powerful tool for dissecting the neural underpinnings of object size and depth perception in more ecological contexts, which enriches our comprehension of the brain’s representational mechanisms.</p><p>Using EEG, we uncovered a representational timeline for visual object processing, with object real-world depth information represented first, followed by retinal size, and finally real-world size. While size and depth are highly correlated to each other, our results suggest that the human brain indeed has dissociated time courses and mechanisms to process them. The later representation time window for object real-world size may suggest that the brain requires more sophisticated, higher-level information to form this representation, perhaps incorporating semantic and/or memory information about familiar objects, which was corroborated by our ANN and Word2Vec analyses. These findings also align with a recent fMRI study (<xref ref-type="bibr" rid="bib48">Luo et al., 2023</xref>) using natural images to explore the neural selectivity for real-world size, finding that low-level visual information could hardly account for neural size preferences, although that study did not consider covariables like retinal size and real-world depth.</p><p> In contrast to the later-emerging real-world size representations, it makes sense that retinal size representations could be processed more quickly based on more fundamental, lower-level information such as shape and edge discrimination. Interestingly, real-world depth representations emerged even earlier than retinal size, suggesting that depth feature may precede real-world size processing. This early emergence might reflect the role of depth cues in facilitating object segmentation, as proposed in classical theories of vision (<xref ref-type="bibr" rid="bib49">Marr, 1982</xref>), where depth helps delineate object boundaries from complex backgrounds. Additionally, there was a secondary, albeit substantially later, significant depth representation time window, which might indicate that our brains also have the ability to integrate object retinal size and higher-level real-size information to form the final representation of real-world depth. Our comparisons between human brains and artificial models and explorations on ANNs and Word2Vec offer further insights and suggest that although real-world object size and depth are closely related, object real-world size appears to be a more stable and higher-level dimension.</p><p>The concept of ‘object space’ in cognitive neuroscience research is crucial for understanding how various visual features of objects are represented. Historically, various visual features have been considered important dimensions in constructing object space, including animate-inanimate (<xref ref-type="bibr" rid="bib38">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Naselaris et al., 2012</xref>), spikiness (<xref ref-type="bibr" rid="bib3">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib11">Coggan and Tong, 2023</xref>), and physical appearance (<xref ref-type="bibr" rid="bib17">Edelman et al., 1998</xref>). In this study, we focus on one particular dimension, real-world size (<xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="bib37">Konkle and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib36">Konkle and Oliva, 2012b</xref>). How we generate neural distinctions of different object real-world size and where this ability comes from remain uncertain. Some previous studies found that object shape rather than texture information could trigger neural size representations (<xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Long et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Long et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Wang et al., 2022a</xref>). Our results attempt to further advance their findings that object real-world size is a stable and higher-level dimension substantially driven by object semantics in object space.</p><p>Increasingly, research has begun to use ANNs to study the mechanisms of object recognition (<xref ref-type="bibr" rid="bib2">Ayzenberg et al., 2023</xref>; <xref ref-type="bibr" rid="bib10">Cichy and Kaiser, 2019</xref>; <xref ref-type="bibr" rid="bib16">Doerig et al., 2023</xref>; <xref ref-type="bibr" rid="bib29">Kanwisher et al., 2023</xref>). We can explore how the human brain processes information at different levels by comparing brain activity with models (<xref ref-type="bibr" rid="bib9">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib40">Kuzovkin et al., 2018</xref>; <xref ref-type="bibr" rid="bib63">Xie et al., 2020</xref>), and we can also analyze the representation patterns of the models with some specific manipulations and infer potential processing mechanisms in the brain (<xref ref-type="bibr" rid="bib19">Golan et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="bib45">Lu and Ku, 2023</xref>; <xref ref-type="bibr" rid="bib64">Xu et al., 2021</xref>). In the current study, our comparisons between EEG signals and different ANNs showed that the visual model’s early layer had a higher similarity to the brain in the early stage, while the visual-semantic model’s late layer had a higher similarity to the brain in the late stage. In addition, to assess whether this EEG-ANN similarity pattern generalizes to more biologically inspired architectures, we conducted the same analyses and found that CORnet (<xref ref-type="bibr" rid="bib39">Kubilius et al., 2019</xref>) showed similar patterns to those observed for ResNet and CLIP, providing converging evidence across models (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>). However, for the representation of objects, partial correlation results for different ANNs didn’t demonstrate the superiority of the multi-modal model at late layers. This might be due to models like CLIP, which contain semantic information, learning more complex image descriptive information (like the relationship between object and the background in the image). Real-world size might be a semantic dimension of the object itself, and its representation does not require overall semantic descriptive information of the image. In contrast, retinal size and real-world depth could rely on image background information for estimation, thus their representations in the CLIP late layer disappeared when input images had only pure object but no background.</p><p>Although our study does not directly test specific models of visual object processing, the observed temporal dynamics provide important constraints for theoretical interpretations. In particular, we find that real-world size representations emerge significantly later than low-level visual features such as retinal size and depth. This temporal profile is difficult to reconcile with a purely feedforward account of visual processing (e.g. <xref ref-type="bibr" rid="bib14">DiCarlo et al., 2012</xref>), which posits that object properties are rapidly computed in a sequential hierarchy of increasingly complex visual features. Instead, our results are more consistent with frameworks that emphasize recurrent or top-down processing, such as the reverse hierarchy theory (<xref ref-type="bibr" rid="bib25">Hochstein and Ahissar, 2002</xref>), which suggests that high-level conceptual information may emerge later and involve feedback to earlier visual areas. This interpretation is further supported by representational similarities with late-stage artificial neural network layers and with a semantic word embedding model (Word2Vec), both of which reflect learned, abstract knowledge rather than low-level visual features. Taken together, these findings suggest that real-world size is not merely a perceptual attribute, but one that draws on conceptual or semantic-level representations acquired through experience. While our EEG analyses focused on posterior electrodes and thus cannot definitively localize cortical sources, we see this study as a step toward linking low-level visual input with higher-level semantic knowledge. Future work incorporating broader spatial coverage (e.g. anterior sensors), source localization, or complementary modalities such as MEG and fMRI will be critical to adjudicate between alternative models of object representation and to more precisely trace the origin and flow of real-world size information in the brain. Moreover, building on the promising findings of our study, future work may further delve into the detailed processes of object processing and object space. One important problem to solve is how real-world size interacts with other object dimensions in object space. In addition, our approach could be used with future studies investigating other influences on object processing, such as how different task conditions impact and modulate the processing of various visual features.</p><p>Moreover, we must also emphasize that in this study, we were concerned with perceived real-world size and depth reflecting a perceptual estimation of our world, which are slightly different from absolute physical size and depth. The differences in brain encoding between perceived and absolute physical size and depth require more comprehensive measurements of an object’s physical attributes for further exploration. Also, we focused on perceiving depth and size from 2D images in this study, which might have some differences in brain mechanism compared to physically exploring the 3D world. Nevertheless, we believe our study offers a valuable contribution to object recognition, especially the encoding process of object real-world size in natural images. Additionally, we acknowledge that our metric for real-world depth was derived indirectly as the ratio of perceived real-world size to retinal size. While this formulation is grounded in geometric principles of perspective projection and served the purpose of analytically dissociating depth from size in our RSA framework, it remains a proxy rather than a direct measure of perceived egocentric distance. Future work incorporating behavioral or psychophysical depth ratings would be valuable for validating and refining this metric.</p><p>In conclusion, we used computational methods to distinguish the representations of real-world size, retinal size, and real-world depth features of objects in ecologically natural images in both human brains and ANNs. We found an unconfounded representation of object real-world size, which emerged at later time windows in the human EEG signal and at later layers of artificial neural networks compared to real-world depth, and which also appeared to be preserved as a stable dimension in object space. Thus, although size and depth properties are closely correlated, the processing of perceived object size and depth may arise through dissociated time courses and mechanisms. Our research provides a temporally resolved characterization of how certain key object properties – such as object real-world size, depth, and retinal size – are represented in the brain, which advances our understanding of real-world size encoding as a stable and semantically grounded dimension in object space, and contributes to the development of more brain-like visual models.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental design, stimuli images, and EEG data</title><p>We utilized the open dataset from THINGS EEG2 (<xref ref-type="bibr" rid="bib18">Gifford et al., 2022</xref>), which includes EEG data from 10 healthy human subjects (age = 28.5 ± 4, 8 female and 2 male) participating in a rapid serial visual presentation (RSVP) paradigm with an orthogonal target detection task to ensure participants paid attention to the visual stimuli. All participants were seated at a fixed distance of 0.6 m from the screen throughout the experiment. For each trial, subjects viewed one image (sized 500×500 pixels) for 100ms. Each subject viewed 16740 images of objects on a natural background for 1854 object concepts from the THINGS dataset (<xref ref-type="bibr" rid="bib24">Hebart et al., 2019</xref>). For the current study, we used the ‘test’ dataset portion, which includes 16,000 trials per subject corresponding to 200 images (200 object concepts, one image per concept) with 80 trials per image, providing high trial counts per condition necessary for reliable EEG decoding. In contrast, images in the training set were only shown four times each. This design choice ensured higher decoding reliability and greater signal quality for RSA. Before inputting the images to the ANNs, we reshaped image sizes to 224x224 pixels and normalized the pixel values of images to ImageNet statistics.</p><p>EEG data were collected using a 64-channel EASYCAP and a BrainVision actiCHamp amplifier. The EEG data were originally sampled at 1000 Hz and online-filtered between 0.1 Hz and 100 Hz during acquisition, with recordings referenced to the Fz electrode. For preprocessing, no additional filtering was applied. Baseline correction was performed by subtracting the mean signal during the 100ms pre-stimulus interval from each trial and channel separately. We already used pre-processed data from 17 channels with labels beginning with ‘O’ or ‘P’ (O1, Oz, O2, PO7, PO3, POz, PO4, PO8, P7, P5, P3, P1, Pz, P2) ensuring full coverage of posterior regions typically involved in visual object processing. The epoched data were then down-sampled to 100 Hz. We re-epoched EEG data ranging from 100ms before stimulus onset to 300ms after onset with a sample frequency of 100 Hz. Thus, the shape of our EEG data matrix for each trial was 17 channels ×40 time points.</p></sec><sec id="s4-2"><title>ANN models</title><p>We applied two pre-trained ANN models: one visual model (ResNet-101 <xref ref-type="bibr" rid="bib23">He et al., 2016</xref> pretrained on ImageNet), and one multi-modal (visual +semantic) model (CLIP with a ResNet-101 backbone (<xref ref-type="bibr" rid="bib55">Radford et al., 2021</xref>) pretrained on YFCC-15M). Our motivation for selecting ResNet-50 and CLIP ResNet-50 was not to make a definitive comparison between model classes, but rather to include two widely used representatives of their respective categories—one trained purely on visual information (ResNet-50 on ImageNet) and one trained with joint visual and linguistic supervision (CLIP ResNet-50 on image–text pairs). These models are both highly influential and commonly used in computational and cognitive neuroscience, allowing for relevant comparisons with existing work (<xref ref-type="bibr" rid="bib7">Choksi et al., 2022a</xref>; <xref ref-type="bibr" rid="bib8">Choksi et al., 2022b</xref>; <xref ref-type="bibr" rid="bib13">Conwell et al., 2024</xref>; <xref ref-type="bibr" rid="bib58">Song et al., 2024</xref>; <xref ref-type="bibr" rid="bib62">Wang et al., 2022b</xref>). We used THINGSvision (<xref ref-type="bibr" rid="bib51">Muttenthaler and Hebart, 2021</xref>) to obtain low- and high-level feature vectors of ANN activations from early and late layers (early layer: second convolutional layer; late layer: last visual layer) for the images.</p></sec><sec id="s4-3"><title>Word2Vec model</title><p>To approximate the non-visual, pure semantic space of objects, we also applied a Word2Vec model, a natural language processing model for word embedding, pretrained on Google News corpus (<xref ref-type="bibr" rid="bib50">Mikolov et al., 2013</xref>), which contains 300-dimensional vectors for 3 million words and phrases. We input the words for each image’s object concept (pre-labeled in THINGS dataset: <xref ref-type="bibr" rid="bib24">Hebart et al., 2019</xref>), instead of the visual images themselves. We used Gensim (<xref ref-type="bibr" rid="bib56">Řehůřek and Sojka, 2010</xref>) to obtain Word2Vec feature vectors for the objects in images.</p></sec><sec id="s4-4"><title>Representational dissimilarity matrices (RDMs)</title><p>To conduct RSA across human EEG, artificial models, and our hypotheses corresponding to different visual features, we first computed representational dissimilarity matrices (RDMs) for different modalities (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The shape of each RDM was 200×200, corresponding to pairwise dissimilarity between the 200 images. We extracted the 19,900 cells from the upper half of the diagonal of each RDM for subsequent analyses.</p><sec id="s4-4-1"><title>Neural RDMs</title><p>From the EEG signal, we constructed timepoint-by-timepoint neural RDMs for each subject with decoding accuracy as the dissimilarity index (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Since EEG has a low SNR and includes rapid transient artifacts, Pearson correlations computed over very short time windows yield unstable dissimilarity estimates (<xref ref-type="bibr" rid="bib30">Kappenman and Luck, 2010</xref>; <xref ref-type="bibr" rid="bib47">Luck, 2014</xref>) and may thus fail to reliably detect differences between images. In contrast, decoding accuracy – by training classifiers to focus on task-relevant features – better mitigates noise and highlights representational differences. We first conducted timepoint-by-timepoint classification-based decoding for each subject and each pair of images (200 images, 19,900 pairs in total). We applied linear Support Vector Machine (SVM) to train and test a two-class classifier, employing a five-time fivefold cross-validation method, to obtain an independent decoding accuracy for each image pair and each timepoint. Therefore, we ultimately acquired 40 (1 per timepoint) EEG RDMs for each subject.</p></sec><sec id="s4-4-2"><title>Hypothesis-based (HYP) RDMs</title><p>We constructed three hypothesis-based RDMs reflecting the different types of visual object properties in the naturalistic images (<xref ref-type="fig" rid="fig2">Figure 2B</xref>): Real-World Size RDM, Retinal Size RDM, and Real-World Depth RDM. We constructed these RDMs as follows:</p><list list-type="order" id="list1"><list-item><p>For Real-World Size RDM, we obtained human behavioral real-world size ratings of each object concept from the THINGS +dataset (<xref ref-type="bibr" rid="bib59">Stoinski et al., 2024</xref>). In the THINGS +dataset, 2010 participants (different from the subjects in THINGS EEG2) did an online size rating task and completed a total of 13,024 trials corresponding to 1854 object concepts using a two-step procedure. In their experiment, first, each object was rated on a 520-unit continuous slider anchored by familiar reference objects (e.g. ‘grain of sand’, ‘microwave oven’, ‘aircraft carrier’) representing a logarithmic size range. Participants were not shown numerical values but used semantic anchors as guides. In the second step, the scale zoomed in around the selected region to allow for finer-grained refinement of the size judgment. Final size values were derived from aggregated behavioral data and rescaled to a range of 0–519 for consistency across objects, with the actual mean ratings across subjects ranging from 100.03 (‘grain of sand’) to 423.09 (‘subway’). We used these ratings as the perceived real-world size measure of the object concept pre-labeled in the THINGS dataset (<xref ref-type="bibr" rid="bib24">Hebart et al., 2019</xref>) for each image. We then constructed the representational dissimilarity matrix by calculating the absolute difference between perceived real-world size ratings for each pair of images.</p></list-item><list-item><p>For Retinal Size RDM, we applied Adobe Photoshop (Adobe Inc, 2019) to crop objects corresponding to object labels from images manually. All cropping and measurement were conducted by one of the authors to ensure consistency across the dataset. The cropped object images have been made publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ZitongLu1996/RWsize">https://github.com/ZitongLu1996/RWsize</ext-link>, copy archived at <xref ref-type="bibr" rid="bib46">Lu, 2024</xref> to facilitate future reuse and reproducibility. For each image, we obtained a rectangular region that precisely contains a single object, then measured the diagonal length of the segmented object in pixels as the retinal size measure (<xref ref-type="bibr" rid="bib34">Konkle and Oliva, 2011</xref>). Due to our calculations being at the object level, if there were more than one same objects in an image, we cropped the most complete one to get more accurate retinal size. We then constructed the RDM by calculating the absolute difference between measured retinal size for each pair of images.</p></list-item><list-item><p>For Real-World Depth RDM, we calculated the perceived depth based on the measured retinal size index and behavioral real-world size ratings, such that real-world depth / visual image depth = real-world size / retinal size. Since visual image depth (viewing distance) is held constant across images in the task, inferred real-world depth is proportional to real-world size / retinal size. We then constructed the RDM by calculating the absolute difference between inferred real-world depth index for each pair of images.</p></list-item></list></sec><sec id="s4-4-3"><title>ANN (and Word2Vec) model RDMs</title><p>We constructed a total of five model-based RDMs (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Our primary analyses used four ANN RDMs, corresponding to the early and late layers for both ResNet and CLIP (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The early layer in ResNet refers to ResNet.maxpool layer, and the late layer in ResNet refers to ResNet.avgpool layer. The early layer in CLIP refers to CLIP.visual.avgpool layer, and the late layer in CLIP refers to CLIP.visual.attnpool layer. We also calculated a single Word2Vec RDM for the pure semantic analysis (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). For each RDM, we got the dissimilarities by calculating 1 – Pearson correlation coefficient between each pair of two vectors of the model features corresponding to two input images.</p></sec></sec><sec id="s4-5"><title>Representational similarity analyses (RSA) and statistical analyses</title><p>We conducted cross-modal representational similarity analyses between the three types of RDMs (<xref ref-type="fig" rid="fig2">Figure 2</xref>). All decoding and RSA analyses were implemented using NeuroRA (<xref ref-type="bibr" rid="bib44">Lu and Ku, 2020</xref>).</p><sec id="s4-5-1"><title>EEG × ANN (or W2V) RSA</title><p>To measure the representational similarity between human brains and ANNs and confirm that ANNs have significantly similar representations to human brains, we calculated the Spearman correlation between the 40 timepoint-by-timepoint EEG neural RDMs and the 4 ANN RDMs corresponding to the representations of ResNet early layer, ResNet late layer, CLIP early layer, CLIP late layer, respectively. We also calculated temporal representational similarity between human brains (EEG RDMs) and the Word2Vec model RDM. Cluster-based permutation tests were conducted to determine the time windows of significant representational similarity. First, we performed one-sample t-tests (one-tailed testing) against zero to get the t-value for each timepoint and extracted significant clusters. We computed the clustering statistic as the sum of t-values in each cluster. Then we conducted 1000 permutations of each subject’s timepoint-by-timepoint similarities to calculate a null distribution of maximum clustering statistics. Finally, we assigned cluster-level p-values to each cluster of the actual representational time course by comparing its cluster statistic with the null distribution. Time windows were determined to be significant if the p-value of the corresponding cluster was &lt;0.05.</p></sec><sec id="s4-5-2"><title>EEG × HYP RSA</title><p>To evaluate how human brains temporally represent different visual features, we calculated the time course of representational similarity between the timepoint-by-timepoint EEG neural RDMs and the three hypothesis-based RDMs. To avoid correlations between hypothesis-based RDMs (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) influencing comparison results, we calculated partial correlations and used a one-tailed test against the null hypothesis that the partial correlation was less than or equal to zero, testing whether the partial correlation was significantly greater than zero. In EEG ×HYP partial correlation (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), we correlated EEG RDMs with one hypothesis-based RDM (e.g. real-world size), while controlling for the other two (e.g. retinal size and real-world depth). Cluster-based permutation tests were performed as described above to determine the time windows of significant representational similarity. In addition, we conducted peak latency analysis to determine the latency of peak representational similarity for each type of visual information with the EEG signal. To assess the stability of peak latency estimates for each subject, we performed a bootstrap procedure across stimulus pairs. At each time point, the EEG RDM was vectorized by extracting the lower triangle (excluding the diagonal), resulting in 19,900 unique pairwise values. For each bootstrap sample, we resampled these 19,900 pairwise entries with replacement to generate a new pseudo-RDM of the same size. We then computed the partial correlation between the EEG pseudo-RDM and a given hypothesis RDM (e.g. real-world size), controlling for other feature RDMs, and obtained a time course of partial correlations. Repeating this procedure 1000 times and extracting the peak latency within the significant time window yielded a distribution of bootstrapped latencies, from which we got the bootstrapped mean latencies per subject. Paired t-tests (two-tailed) were conducted to assess the statistical differences in peak latencies between different visual features.</p></sec><sec id="s4-5-3"><title>ANN (or W2V) × HYP RSA</title><p>To evaluate how different visual information is represented in ANNs, we calculated representational similarity between the ANN RDMs and hypothesis-based RDMs. As in the EEG ×HYP RSA, we calculated partial correlations to avoid correlations between hypothesis-based RDMs. In ANN (or W2V)×HYP partial correlation (<xref ref-type="fig" rid="fig3">Figures 3E</xref> and <xref ref-type="fig" rid="fig5">5A</xref>), we correlated ANN (or W2V) RDMs with one hypothesis-based RDM (e.g. real-world size), while partialling out the other two. We also calculated the partial correlations between hypothesis-based RDMs and the Word2Vec RDM. To determine statistical significance, we conducted a bootstrap test. We shuffled the order of the cells above the diagonal in each ANN (or Word2Vec) RDM 1000 times. For each iteration, we calculated partial correlations corresponding to the three hypothesis-based RDMs. This produced a 1000-sample null distribution for each HYP x ANN (or W2V) RSA. We hypothesized that if the real similarity was higher than the 95% confidence interval of the null distribution, it indicated that ANN (or W2V) features validly encoded the corresponding visual feature.</p><p>Additionally, to explore how the naturalistic background present in the images might influence object real-world size, retinal size, and real-world depth representations, we conducted another version of the analysis by inputting cropped object images without background into ANN models to obtain object-only ANN RDMs (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Then we performed the same ANN x HYP similarity analysis to calculate partial correlations between the hypothesis-based RDMs and object-only ANN RDM. (We didn’t conduct the similarity analysis between timepoint-by-timepoint EEG neural RDMs with subjects viewing natural images and object-only ANN RDMs due to the input differences.)</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Software, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98117-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Statistical results of similarities between ANN and hypothesis-based RDMs.</title></caption><media xlink:href="elife-98117-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>EEG data and images from THINGS EEG2 data are publicly available on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/3JK45">OSF</ext-link>. All Python analysis scripts are publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/ZitongLu1996/RWsize">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib46">Lu, 2024</xref>).</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Dickter</surname><given-names>AH</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Kwok</surname><given-names>WY</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Van Wicklin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>THINGS object concept and object image database</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/JUM2F</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Gifford</surname><given-names>AT</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>A large and rich EEG dataset for modeling human visual object recognition</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/3JK45</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by research grants from the National Institutes of Health (R01-EY025648) and from the National Science Foundation (NSF 1848939) to JDG. The authors declare no competing financial interests.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00962-x</pub-id><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Blauch</surname><given-names>NM</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Using deep neural networks to address the how of object recognition</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/6gjvp</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id><pub-id pub-id-type="pmid">26758835</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Daniels</surname><given-names>N</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task context overrules object- and category-related representational content in the human parietal cortex</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>310</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw419</pub-id><pub-id pub-id-type="pmid">28108492</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>YC</given-names></name><name><surname>Deza</surname><given-names>A</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>How big should this object be? Perceptual influences on viewing-size preferences</article-title><source>Cognition</source><volume>225</volume><elocation-id>105114</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105114</pub-id><pub-id pub-id-type="pmid">35381479</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choksi</surname><given-names>B</given-names></name><name><surname>Mozafari</surname><given-names>M</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Reddy</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Multimodal neural networks better explain multivoxel patterns in the hippocampus</article-title><source>Neural Networks</source><volume>154</volume><fpage>538</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2022.07.033</pub-id><pub-id pub-id-type="pmid">35995019</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Choksi</surname><given-names>B</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Reddy</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Do multimodal neural networks better explain human visual representations than vision-only networks?</article-title><conf-name>Conference on Cognitive Computational Neuroscience</conf-name><pub-id pub-id-type="doi">10.32470/CCN.2022.1183-0</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep neural networks as scientific models</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>305</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.009</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname><given-names>DD</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spikiness and animacy as potential organizing principles of human ventral visual cortex</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>8194</fpage><lpage>8217</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhad108</pub-id><pub-id pub-id-type="pmid">36958809</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Conwell</surname><given-names>C</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Large-scale benchmarking of diverse artificial vision models in prediction of 7T human neuroimaging data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.03.28.485868</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conwell</surname><given-names>C</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A large-scale examination of inductive biases shaping high-level visual representation in brains and machines</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>9383</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-53147-y</pub-id><pub-id pub-id-type="pmid">39477923</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Semantic scene descriptions as an objective of human vision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2209.11737">https://doi.org/10.48550/arXiv.2209.11737</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>RP</given-names></name><name><surname>Seeliger</surname><given-names>K</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Ismael</surname><given-names>J</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The neuroconnectionist research programme</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>431</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00705-w</pub-id><pub-id pub-id-type="pmid">37253949</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>S</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Toward direct visualization of the internal shape representation space by fMRI</article-title><source>Psychobiology</source><volume>26</volume><fpage>309</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.3758/BF03330618</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gifford</surname><given-names>AT</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A large and rich EEG dataset for modeling human visual object recognition</article-title><source>NeuroImage</source><volume>264</volume><elocation-id>119754</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119754</pub-id><pub-id pub-id-type="pmid">36400378</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golan</surname><given-names>T</given-names></name><name><surname>Raju</surname><given-names>PC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Controversial stimuli: Pitting neural networks against each other as models of human cognition</article-title><source>PNAS</source><volume>117</volume><fpage>29330</fpage><lpage>29337</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912334117</pub-id><pub-id pub-id-type="pmid">33229549</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname><given-names>MR</given-names></name><name><surname>Hansen</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Disentangling the independent contributions of visual and conceptual features to the spatiotemporal dynamics of scene categorization</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5283</fpage><lpage>5299</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2088-19.2020</pub-id><pub-id pub-id-type="pmid">32467356</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Zhou</surname><given-names>I</given-names></name><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams</article-title><source>Scientific Data</source><volume>9</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-021-01102-7</pub-id><pub-id pub-id-type="pmid">35013331</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Dickter</surname><given-names>AH</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Kwok</surname><given-names>WY</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Van Wicklin</surname><given-names>C</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0223792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0223792</pub-id><pub-id pub-id-type="pmid">31613926</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochstein</surname><given-names>S</given-names></name><name><surname>Ahissar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>View from the top: hierarchies and reverse hierarchies in the visual system</article-title><source>Neuron</source><volume>36</volume><fpage>791</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01091-7</pub-id><pub-id pub-id-type="pmid">12467584</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>T</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Real-world size of objects serves as an axis of object space</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>749</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-03711-3</pub-id><pub-id pub-id-type="pmid">35896715</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id><pub-id pub-id-type="pmid">23259955</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Deep neural networks and visuo-semantic models explain complementary components of human ventral-stream representational dynamics</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>1731</fpage><lpage>1741</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1424-22.2022</pub-id><pub-id pub-id-type="pmid">36759190</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Khosla</surname><given-names>M</given-names></name><name><surname>Dobs</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Using artificial neural networks to ask “why” questions of minds and brains</article-title><source>Trends in Neurosciences</source><volume>46</volume><fpage>240</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2022.12.008</pub-id><pub-id pub-id-type="pmid">36658072</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappenman</surname><given-names>ES</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The effects of electrode impedance on data quality and statistical significance in ERP recordings</article-title><source>Psychophysiology</source><volume>47</volume><fpage>888</fpage><lpage>904</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2010.01009.x</pub-id><pub-id pub-id-type="pmid">20374541</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tracking the spatiotemporal neural dynamics of real-world object size and animacy in the human brain</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1559</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01290</pub-id><pub-id pub-id-type="pmid">29877767</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id><pub-id pub-id-type="pmid">31591217</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Canonical visual size for real-world objects</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>37</volume><fpage>23</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1037/a0020413</pub-id><pub-id pub-id-type="pmid">20822298</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>A familiar-size Stroop effect: Real-world size is an automatic property of object representation</article-title><source>Journal of Experimental Psychology</source><volume>38</volume><fpage>561</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1037/a0028294</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>A real-world size organization of object responses in occipitotemporal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>1114</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.036</pub-id><pub-id pub-id-type="pmid">22726840</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tripartite organization of the ventral stream by animacy and object size</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>10235</fpage><lpage>10242</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0983-13.2013</pub-id><pub-id pub-id-type="pmid">23785139</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Dicarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain-like object recognition with high-performing shallow recurrent anns</article-title><conf-name>Advances in Neural Information Processing Systems (NeurIPS)</conf-name><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper_files/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html">https://papers.nips.cc/paper_files/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuzovkin</surname><given-names>I</given-names></name><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Petton</surname><given-names>M</given-names></name><name><surname>Lachaux</surname><given-names>JP</given-names></name><name><surname>Baciu</surname><given-names>M</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Rheims</surname><given-names>S</given-names></name><name><surname>Vidal</surname><given-names>JR</given-names></name><name><surname>Aru</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex</article-title><source>Communications Biology</source><volume>1</volume><elocation-id>107</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-018-0110-y</pub-id><pub-id pub-id-type="pmid">30271987</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mid-level perceptual features distinguish objects of different real-world sizes</article-title><source>Journal of Experimental Psychology. General</source><volume>145</volume><fpage>95</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1037/xge0000130</pub-id><pub-id pub-id-type="pmid">26709591</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A familiar-size Stroop effect in the absence of basic-level recognition</article-title><source>Cognition</source><volume>168</volume><fpage>234</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2017.06.025</pub-id><pub-id pub-id-type="pmid">28732302</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>CP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>PNAS</source><volume>115</volume><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id><pub-id pub-id-type="pmid">30171168</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Ku</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>NeuroRA: A python toolbox of representational analysis from multi-modal neural data</article-title><source>Frontiers in Neuroinformatics</source><volume>14</volume><elocation-id>563669</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2020.563669</pub-id><pub-id pub-id-type="pmid">33424573</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Ku</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Bridging the gap between EEG and DCNNs reveals a fatigue mechanism of facial repetition suppression</article-title><source>iScience</source><volume>26</volume><elocation-id>108501</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2023.108501</pub-id><pub-id pub-id-type="pmid">38089588</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>RWsize</data-title><version designator="swh:1:rev:3ec93585322188c99b49f762df9805fc675494bf">swh:1:rev:3ec93585322188c99b49f762df9805fc675494bf</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:1f73002179a5ff4561a729020213da6a249eae6a;origin=https://github.com/ZitongLu1996/RWsize;visit=swh:1:snp:18eabd7729c46f049271d9cecda9c6c9c50ebbba;anchor=swh:1:rev:3ec93585322188c99b49f762df9805fc675494bf">https://archive.softwareheritage.org/swh:1:dir:1f73002179a5ff4561a729020213da6a249eae6a;origin=https://github.com/ZitongLu1996/RWsize;visit=swh:1:snp:18eabd7729c46f049271d9cecda9c6c9c50ebbba;anchor=swh:1:rev:3ec93585322188c99b49f762df9805fc675494bf</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>An Introduction to the Event-Related Potential Technique</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>AF</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Henderson</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural selectivity for real-world object size in natural images</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.17.533179</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Corrado</surname><given-names>G</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Efficient estimation of word representations in vector space</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1301.3781">https://doi.org/10.48550/arXiv.1301.3781</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muttenthaler</surname><given-names>L</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>THINGSvision: A python toolbox for streamlining the extraction of activations from deep neural networks</article-title><source>Frontiers in Neuroinformatics</source><volume>15</volume><elocation-id>679838</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2021.679838</pub-id><pub-id pub-id-type="pmid">34630062</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Stansbury</surname><given-names>DE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical representation of animate and inanimate objects in complex natural scenes</article-title><source>Journal of Physiology, Paris</source><volume>106</volume><fpage>239</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.jphysparis.2012.02.001</pub-id><pub-id pub-id-type="pmid">22472178</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: The animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Quek</surname><given-names>G</given-names></name><name><surname>Theodorou</surname><given-names>A</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Better together: Objects in familiar constellations evoke high-level representations of real-world size</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.05.30.542965</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>JW</given-names></name><name><surname>Hallacy</surname><given-names>C</given-names></name><name><surname>Ramesh</surname><given-names>A</given-names></name><name><surname>Goh</surname><given-names>G</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name><name><surname>Askell</surname><given-names>A</given-names></name><name><surname>Mishkin</surname><given-names>P</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>Krueger</surname><given-names>G</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning transferable visual models from natural language supervision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2103.00020">https://doi.org/10.48550/arXiv.2103.00020</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Řehůřek</surname><given-names>R</given-names></name><name><surname>Sojka</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Software framework for topic modelling with large corpora</article-title><conf-name>Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</conf-name><fpage>45</fpage><lpage>50</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Setti</surname><given-names>A</given-names></name><name><surname>Caramelli</surname><given-names>N</given-names></name><name><surname>Borghi</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Conceptual information about size of objects in nouns</article-title><source>European Journal of Cognitive Psychology</source><volume>21</volume><fpage>1022</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1080/09541440802469499</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Shi</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Decoding natural images from EEG for object recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2308.13234">https://doi.org/10.48550/arXiv.2308.13234</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoinski</surname><given-names>LM</given-names></name><name><surname>Perkuhn</surname><given-names>J</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>THINGSplus: New norms and metadata for the THINGS database of 1854 object concepts and 26,107 natural object images</article-title><source>Behavior Research Methods</source><volume>56</volume><fpage>1583</fpage><lpage>1603</lpage><pub-id pub-id-type="doi">10.3758/s13428-023-02110-8</pub-id><pub-id pub-id-type="pmid">37095326</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troiani</surname><given-names>V</given-names></name><name><surname>Stigliani</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>ME</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multiple object properties drive scene-selective regions</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>883</fpage><lpage>897</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs364</pub-id><pub-id pub-id-type="pmid">23211209</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Janini</surname><given-names>D</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Mid-level feature differences support early animacy and object size distinctions: Evidence from electroencephalography decoding</article-title><source>Journal of Cognitive Neuroscience</source><volume>34</volume><fpage>1670</fpage><lpage>1680</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01883</pub-id><pub-id pub-id-type="pmid">35704550</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>AY</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Incorporating natural language into vision models improves prediction and understanding of higher visual cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.09.27.508760</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Visual imagery and perception share neural representations in the alpha frequency band</article-title><source>Current Biology</source><volume>30</volume><fpage>2621</fpage><lpage>2627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.074</pub-id><pub-id pub-id-type="pmid">32531274</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Zhen</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The face module emerged in a deep convolutional neural network selectively deprived of face experience</article-title><source>Frontiers in Computational Neuroscience</source><volume>15</volume><elocation-id>626259</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2021.626259</pub-id><pub-id pub-id-type="pmid">34093154</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98117.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>SP</surname><given-names>Arun</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study combines EEG, neural networks and multivariate pattern analysis to show that real-world size, retinal size and real-world depth are represented at different latencies. The evidence presented is <bold>convincing</bold> and the work will be of broader interest to the experimental and computational vision community.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98117.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Lu &amp; Golomb combined EEG, artificial neural networks, and multivariate pattern analyses to examine how different visual variables are processed in the brain. The conclusions of the paper are mostly well supported.</p><p>The authors find that not only real-world size is represented in the brain (which was known), but both retinal size and real-world depth is represented, at different time points or latencies, which may reflect different stages of processing. Prior work has not been able to answer the question of real-world depth due to stimuli used. The authors made this possible by assess real-world depth and testing it with appropriate methodology, accounting for retinal and real-world size. The methodological approach combining behavior, RSA, and ANNs is creative and well thought out to appropriately assess the research questions, and the findings may be very compelling if backed up with some clarifications and further analyses.</p><p>The work will be of interest to experimental and computational vision scientists, as well as the broader computational cognitive neuroscience community as the methodology is of interest and the code is or will be made available. The work is important as it is currently not clear what the correspondence between many deep neural network models are and the brain are, and this work pushes our knowledge forward on this front. Furthermore, the availability of methods and data will be useful for the scientific community.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98117.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors used an open EEG dataset of observers viewing real-world objects. Each object had a real-world size value (from human rankings), a retinal size value (measured from each image), and a scene depth value (inferred from the above). The authors combined the EEG and object measurements with extant, pre-trained models (a deep convolutional neural network, a multimodal ANN, and Word2vec) to assess the time course of processing object size (retinal and real-world) and depth. They found that depth was processed first, followed by retinal size, and then real-world size. The depth time course roughly corresponded to the visual ANNs, while the real-world size time course roughly corresponded to the more semantic models.</p><p>The time course result for the three object attributes is very clear and a novel contribution to the literature. The authors have revised the ANN motivations to increase clarity. Additionally, the authors have appropriately toned down some of the language about novelty, and the addition of a noise ceiling has helped the robustness of the work.</p><p>While I appreciate the addition of Cornet in the Supplement, I am less compelled by the authors' argument for Word2Vec over LLMs for &quot;pure&quot; semantic embeddings. While I'm not digging in on this point, this choice may prematurely age this work.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98117.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Zitong</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Golomb</surname><given-names>Julie</given-names></name><role specific-use="author">Author</role><aff><institution>Ohio State University</institution><addr-line><named-content content-type="city">Columbus</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Lu &amp; Golomb combined EEG, artificial neural networks, and multivariate pattern analyses to examine how different visual variables are processed in the brain. The conclusions of the paper are mostly well supported, but some aspects of methods and data analysis would benefit from clarification and potential extensions.</p><p>The authors find that not only real-world size is represented in the brain (which was known), but both retinal size and real-world depth are represented, at different time points or latencies, which may reflect different stages of processing. Prior work has not been able to answer the question of real-world depth due to the stimuli used. The authors made this possible by assessing real-world depth and testing it with appropriate methodology, accounting for retinal and real-world size. The methodological approach combining behavior, RSA, and ANNs is creative and well thought out to appropriately assess the research questions, and the findings may be very compelling if backed up with some clarifications and further analyses.</p><p>The work will be of interest to experimental and computational vision scientists, as well as the broader computational cognitive neuroscience community as the methodology is of interest and the code is or will be made available. The work is important as it is currently not clear what the correspondence between many deep neural network models and the brain is, and this work pushes our knowledge forward on this front. Furthermore, the availability of methods and data will be useful for the scientific community.</p><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This paper aims to test if neural representations of images of objects in the human brain contain a 'pure' dimension of real-world size that is independent of retinal size or perceived depth. To this end, they apply representational similarity analysis on EEG responses in 10 human subjects to a set of 200 images from a publicly available database (THINGS-EEG2), correlating pairwise distinctions in evoked activity between images with pairwise differences in human ratings of real-world size (from THINGS+). By partialling out correlations with metrics of retinal size and perceived depth from the resulting EEG correlation time courses, the paper claims to identify an independent representation of real-world size starting at 170 ms in the EEG signal. Further comparisons with artificial neural networks and language embeddings lead the authors to claim this correlation reflects a relatively 'high-level' and 'stable' neural representation.</p><p>Strengths:</p><p>The paper features insightful figures/illustrations and clear figures.</p><p>The limitations of prior work motivating the current study are clearly explained and seem reasonable (although the rationale for why using 'ecological' stimuli with backgrounds matters when studying real-world size could be made clearer; one could also argue the opposite, that to get a 'pure' representation of the real-world size of an 'object concept', one should actually show objects in isolation).</p><p>The partial correlation analysis convincingly demonstrates how correlations between feature spaces can affect their correlations with EEG responses (and how taking into account these correlations can disentangle them better).</p><p>The RSA analysis and associated statistical methods appear solid.</p><p>Weaknesses:</p><p>The claim of methodological novelty is overblown. Comparing image metrics, behavioral measurements, and ANN activations against EEG using RSA is a commonly used approach to study neural object representations. The dataset size (200 test images from THINGS) is not particularly large, and neither is comparing pre-trained DNNs and language models, or using partial correlations.</p></disp-quote><p>Thanks for your feedback. We agree that the methods used in our study – such as RSA, partial correlations, and the use of pretrained ANN and language models – are indeed well-established in the literature. We therefore revised the manuscript to more carefully frame our contribution: rather than emphasizing methodological novelty in isolation, we now highlight the combination of techniques, the application to human EEG data with naturalistic images, and the explicit dissociation of real-world size, retinal size, and depth representations as the primary strengths of our approach. Corresponding language in the Abstract, Introduction, and Discussion has been adjusted to reflect this more precise positioning:</p><p>(Abstract, line 34 to 37) “our study combines human EEG and representational similarity analysis to disentangle neural representations of object real-world size from retinal size and perceived depth, leveraging recent datasets and modeling approaches to address challenges not fully resolved in previous work.”</p><p>(Introduction, line 104 to 106) “we overcome these challenges by combining human EEG recordings, naturalistic stimulus images, artificial neural networks, and computational modeling approaches including representational similarity analysis (RSA) and partial correlation analysis …”</p><p>(Introduction, line 108) “We applied our integrated computational approach to an open EEG dataset…”</p><p>(Introduction, line 142 to 143) “The integrated computational approach by cross-modal representational comparisons we take with the current study…”</p><p>(Discussion, line 550 to 552) “our study goes beyond the contributions of prior studies in several key ways, offering both theoretical and methodological advances: …”</p><disp-quote content-type="editor-comment"><p>The claims also seem too broad given the fairly small set of RDMs that are used here (3 size metrics, 4 ANN layers, 1 Word2Vec RDM): there are many aspects of object processing not studied here, so it's not correct to say this study provides a 'detailed and clear characterization of the object processing process'.</p></disp-quote><p>Thanks for pointing this out. We softened language in our manuscript to reflect that our findings provide a temporally resolved characterization of selected object features, rather than a comprehensive account of object processing:</p><p>(line 34 to 37) “our study combines human EEG and representational similarity analysis to disentangle neural representations of object real-world size from retinal size and perceived depth, leveraging recent datasets and modeling approaches to address challenges not fully resolved in previous work.”</p><p>(line 46 to 48) “Our research provides a temporally resolved characterization of how certain key object properties – such as object real-world size, depth, and retinal size – are represented in the brain, …”</p><disp-quote content-type="editor-comment"><p>The paper lacks an analysis demonstrating the validity of the real-world depth measure, which is here computed from the other two metrics by simply dividing them. The rationale and logic of this metric is not clearly explained. Is it intended to reflect the hypothesized egocentric distance to the object in the image if the person had in fact been 'inside' the image? How do we know this is valid? It would be helpful if the authors provided a validation of this metric.</p></disp-quote><p>We appreciate the comment regarding the real-world depth metric. Specifically, this metric was computed as the ratio of real-world size (obtained via behavioral ratings) to measured retinal size. The rationale behind this computation is grounded in the basic principles of perspective projection: for two objects subtending the same retinal size, the physically larger object is presumed to be farther away. This ratio thus serves as a proxy for perceived egocentric depth under the simplifying assumption of consistent viewing geometry across images.</p><p>We acknowledge that this is a derived estimate and not a direct measurement of perceived depth. While it provides a useful approximation that allows us to analytically dissociate the contributions of real-world size and depth in our RSA framework, we agree that future work would benefit from independent perceptual depth ratings to validate or refine this metric. We added more discussions about this to our revised manuscript:</p><p>(line 652 to 657) “Additionally, we acknowledge that our metric for real-world depth was derived indirectly as the ratio of perceived real-world size to retinal size. While this formulation is grounded in geometric principles of perspective projection and served the purpose of analytically dissociating depth from size in our RSA framework, it remains a proxy rather than a direct measure of perceived egocentric distance. Future work incorporating behavioral or psychophysical depth ratings would be valuable for validating and refining this metric.”</p><disp-quote content-type="editor-comment"><p>Given that there is only 1 image/concept here, the factor of real-world size may be confounded with other things, such as semantic category (e.g. buildings vs. tools). While the comparison of the real-world size metric appears to be effectively disentangled from retinal size and (the author's metric of) depth here, there are still many other object properties that are likely correlated with real-world size and therefore will confound identifying a 'pure' representation of real-world size in EEG. This could be addressed by adding more hypothesis RDMs reflecting different aspects of the images that may correlate with real-world size.</p></disp-quote><p>We thank the reviewer for this thoughtful and important point. We agree that semantic category and real-world size may be correlated, and that semantic structure is one of the plausible sources of variance contributing to real-world size representations. However, we would like to clarify that our original goal was to isolate real-world size from two key physical image features — retinal size and inferred real-world depth — which have been major confounds in prior work on this topic. We acknowledge that although our analysis disentangled real-world size from depth and retinal size, this does not imply a fully “pure” representation; therefore, we now refer to the real-world size representations as “partially disentangled” throughout the manuscript to reflect this nuance.</p><p>Interestingly, after controlling for these physical features, we still found a robust and statistically isolated representation of real-world size in the EEG signal. This motivated the idea that realworld size may be more than a purely perceptual or image-based property — it may be at least partially semantic. Supporting this interpretation, both the late layers of ANN models and the non-visual semantic model (Word2Vec) also captured real-world size structure. Rather than treating semantic information as an unwanted confound, we propose that semantic structure may be an inherent component of how the brain encodes real-world size.</p><p>To directly address the your concern, we conducted an additional variance partitioning analysis, in which we decomposed the variance in EEG RDMs explained by four RDMs: real-world depth, retinal size, real-world size, and semantic information (from Word2Vec). Specifically, for each EEG timepoint, we quantified (1) the unique variance of real-world size, after controlling for semantic similarity, depth, and retinal size; (2) the unique variance of semantic information, after controlling for real-world size, depth, and retinal size; (3) the shared variance jointly explained by real-world size and semantic similarity, controlling for depth and retinal size. This analysis revealed that real-world size explained unique variance in EEG even after accounting for semantic similarity. And there was also a substantial shared variance, indicating partial overlap between semantic structure and size. Semantic information also contributed unique explanatory power, as expected. These results suggest that real-world size is indeed partially semantic in nature, but also has independent neural representation not fully explained by general semantic similarity. This strengthens our conclusion that real-world size functions as a meaningful, higher-level dimension in object representation space.</p><p>We now include this new analysis and a corresponding figure (Figure S8) in the revised manuscript:</p><p>(line 532 to 539) “Second, we conducted a variance partitioning analysis, in which we decomposed the variance in EEG RDMs explained by three hypothesis-based RDMs and the semantic RDM (Word2Vec RDM), and we still found that real-world size explained unique variance in EEG even after accounting for semantic similarity (Figure S9). And we also observed a substantial shared variance jointly explained by real-world size and semantic similarity and a unique variance of semantic information. These results suggest that real-world size is indeed partially semantic in nature, but also has independent neural representation not fully explained by general semantic similarity.”</p><disp-quote content-type="editor-comment"><p>The choice of ANNs lacks a clear motivation. Why these two particular networks? Why pick only 2 somewhat arbitrary layers? If the goal is to identify more semantic representations using CLIP, the comparison between CLIP and vision-only ResNet should be done with models trained on the same training datasets (to exclude the effect of training dataset size &amp; quality; cf Wang et al., 2023). This is necessary to substantiate the claims on page 19 which attributed the differences between models in terms of their EEG correlations to one of them being a 'visual model' vs. 'visual-semantic model'.</p></disp-quote><p>We argee that the choice and comparison of models should be better contextualized.</p><p>First, our motivation for selecting ResNet-50 and CLIP ResNet-50 was not to make a definitive comparison between model classes, but rather to include two widely used representatives of their respective categories—one trained purely on visual information (ResNet-50 on ImageNet) and one trained with joint visual and linguistic supervision (CLIP ResNet-50 on image–text pairs). These models are both highly influential and commonly used in computational and cognitive neuroscience, allowing for relevant comparisons with existing work (line 181-187).</p><p>Second, we recognize that limiting the EEG × ANN correlation analyses to only early and late layers may be viewed as insufficiently comprehensive. To address this point, we have computed the EEG correlations with multiple layers in both ResNet and CLIP models (ResNet: ResNet.maxpool, ResNet.layer1, ResNet.layer2, ResNet.layer3, ResNet.layer4, ResNet.avgpool; CLIP: CLIP.visual.avgpool, CLIP.visual.layer1, CLIP.visual.layer2, CLIP.visual.layer3, CLIP.visual.layer4, CLIP.visual.attnpool). The results, now included in Figure S4, show a consistent trend: early layers exhibit higher similarity to early EEG time points, and deeper layers show increased similarity to later EEG stages. We chose to highlight early and late layers in the main text to simplify interpretation.</p><p>Third, we appreciate the reviewer’s point that differences in training datasets (ImageNet vs. CLIP's dataset) may confound any attribution of differences in brain alignment to the models' architectural or learning differences. We agree that the comparisons between models trained on matched datasets (e.g., vision-only vs. multimodal models trained on the same image–text corpus) would allow for more rigorous conclusions. Thus, we explicitly acknowledged this limitation in the text:</p><p>(line 443 to 445) “However, it is also possible that these differences between ResNet and CLIP reflect differences in training data scale and domain.”</p><disp-quote content-type="editor-comment"><p>The first part of the claim on page 22 based on Figure 4 'The above results reveal that realworld size emerges with later peak neural latencies and in the later layers of ANNs, regardless of image background information' is not valid since no EEG results for images without backgrounds are shown (only ANNs).</p></disp-quote><p>We revised the sentence to clarify that this is a hypothesis based on the ANN results, not an empirical EEG finding:</p><p>(line 491 to 495) “These results show that real-world size emerges in the later layers of ANNs regardless of image background information, and – based on our prior EEG results – although we could not test object-only images in the EEG data, we hypothesize that a similar temporal profile would be observed in the brain, even for object-only images.”</p><p>While we only had the EEG data of human subjects viewing naturalistic images, the ANN results suggest that real-world size representations may still emerge at later processing stages even in the absence of background, consistent with what we observed in EEG under with-background conditions.</p><disp-quote content-type="editor-comment"><p>The paper is likely to impact the field by showcasing how using partial correlations in RSA is useful, rather than providing conclusive evidence regarding neural representations of objects and their sizes.</p><p>Additional context important to consider when interpreting this work:</p><p>Page 20, the authors point out similarities of peak correlations between models 'Interestingly, the peaks of significant time windows for the EEG × HYP RSA also correspond with the peaks of the EEG × ANN RSA timecourse (Figure 3D,F)'. Although not explicitly stated, this seems to imply that they infer from this that the ANN-EEG correlation might be driven by their representation of the hypothesized feature spaces. However this does not follow: in EEG-image metric model comparisons it is very typical to see multiple peaks, for any type of model, this simply reflects specific time points in EEG at which visual inputs (images) yield distinctive EEG amplitudes (perhaps due to stereotypical waves of neural processing?), but one cannot infer the information being processed is the same. To investigate this, one could for example conduct variance partitioning or commonality analysis to see if there is variance at these specific timepoints that is shared by a specific combination of the hypothesis and ANN feature spaces.</p></disp-quote><p>Thanks for your thoughtful observation! Upon reflection, we agree that the sentence – &quot;Interestingly, the peaks of significant time windows for the EEG × HYP RSA also correspond with the peaks of the EEG × ANN RSA timecourse&quot; – was speculative and risked implying a causal link that our data do not warrant. As you rightly points out, observing coincident peak latencies across different models does not necessarily imply shared representational content, given the stereotypical dynamics of evoked EEG responses. And we think even variance partitioning analysis would still not suffice to infer that ANN-EEG correlations are driven specifically by hypothesized feature spaces. Accordingly, we have removed this sentence from the manuscript to avoid overinterpretation.</p><disp-quote content-type="editor-comment"><p>Page 22 mentions 'The significant time-window (90-300ms) of similarity between Word2Vec RDM and EEG RDMs (Figure 5B) contained the significant time-window of EEG x real-world size representational similarity (Figure 3B)'. This is not particularly meaningful given that the Word2Vec correlation is significant for the entire EEG epoch (from the time-point of the signal 'arriving' in visual cortex around ~90 ms) and is thus much less temporally specific than the realworld size EEG correlation. Again a stronger test of whether Word2Vec indeed captures neural representations of real-world size could be to identify EEG time-points at which there are unique Word2Vec correlations that are not explained by either ResNet or CLIP, and see if those timepoints share variance with the real-world size hypothesized RDM.</p></disp-quote><p>We appreciate your insightful comment. Upon reflection, we agree that the sentence – &quot;'The significant time-window (90-300ms) of similarity between Word2Vec RDM and EEG RDMs (Figure 5B) contained the significant time-window of EEG x real-world size representational similarity (Figure 3B)&quot; – was speculative. And we have removed this sentence from the manuscript to avoid overinterpretation.</p><p>Additionally, we conducted two analyses as you suggested in the supplement. First, we calculated the partial correlation between EEG RDMs and the Word2Vec RDM while controlling for four ANN RDMs (ResNet early/late and CLIP early/late) (Figure S8). Even after regressing out these ANN-derived features, we observed significant correlations between Word2Vec and EEG RDMs in the 100–190 ms and 250–300 ms time windows. This result suggests that</p><p>Word2Vec captures semantic structure in the neural signal that is not accounted for by ResNet or CLIP. Second, we conducted an additional variance partitioning analysis, in which we decomposed the variance in EEG RDMs explained by four RDMs: real-world depth, retinal size, real-world size, and semantic information (from Word2Vec) (Figure S9). And we found significant shared variance between Word2Vec and real-world size at 130–150 ms and 180–250 ms. These results indicate a partially overlapping representational structure between semantic content and real-world size in the brain.</p><p>We also added these in our revised manuscript:</p><p>(line 525 to 539) “To further probe the relationship between real-world size and semantic information, and to examine whether Word2Vec captures variances in EEG signals beyond that explained by visual models, we conducted two additional analyses. First, we performed a partial correlation between EEG RDMs and the Word2Vec RDM, while regressing out four ANN RDMs (early and late layers of both ResNet and CLIP) (Figure S8). We found that semantic similarity remained significantly correlated with EEG signals across sustained time windows (100-190ms and 250-300ms), indicating that Word2Vec captures neural variance not fully explained by visual or visual-language models. Second, we conducted a variance partitioning analysis, in which we decomposed the variance in EEG RDMs explained by three hypothesis-based RDMs and the semantic RDM (Word2Vec RDM), and we still found that real-world size explained unique variance in EEG even after accounting for semantic similarity (Figure S9). And we also observed a substantial shared variance jointly explained by realworld size and semantic similarity and a unique variance of semantic information. These results suggest that real-world size is indeed partially semantic in nature, but also has independent neural representation not fully explained by general semantic similarity.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>The authors used an open EEG dataset of observers viewing real-world objects. Each object had a real-world size value (from human rankings), a retinal size value (measured from each image), and a scene depth value (inferred from the above). The authors combined the EEG and object measurements with extant, pre-trained models (a deep convolutional neural network, a multimodal ANN, and Word2vec) to assess the time course of processing object size (retinal and real-world) and depth. They found that depth was processed first, followed by retinal size, and then real-world size. The depth time course roughly corresponded to the visual ANNs, while the real-world size time course roughly corresponded to the more semantic models.</p><p>The time course result for the three object attributes is very clear and a novel contribution to the literature. However, the motivations for the ANNs could be better developed, the manuscript could better link to existing theories and literature, and the ANN analysis could be modernized. I have some suggestions for improving specific methods.</p><p>(1) Manuscript motivations</p><p>The authors motivate the paper in several places by asking &quot; whether biological and artificial systems represent object real-world size&quot;. This seems odd for a couple of reasons. Firstly, the brain must represent real-world size somehow, given that we can reason about this question. Second, given the large behavioral and fMRI literature on the topic, combined with the growing ANN literature, this seems like a foregone conclusion and undermines the novelty of this contribution.</p></disp-quote><p>Thanks for your helpful comment. We agree that asking whether the brain represents real-world size is not a novel question, given the existing behavioral and neuroimaging evidence supporting this. Our intended focus was not on the existence of real-world size representations per se, but the nature of these representations, particularly the relationship between the temporal dynamics and potential mechanisms of representations of real-world size versus other related perceptual properties (e.g., retinal size and real-world depth). We revised the relevant sentence to better reflect our focue, shifting from a binary framing (“whether or not size is represented”) to a more mechanistic and time-resolved inquiry (“how and when such representations emerge”):</p><p>(line 144 to 149) “Unraveling the internal representations of object size and depth features in both human brains and ANNs enables us to investigate how distinct spatial properties—retinal size, realworld depth, and real-world size—are encoded across systems, and to uncover the representational mechanisms and temporal dynamics through which real-world size emerges as a potentially higherlevel, semantically grounded feature.”</p><disp-quote content-type="editor-comment"><p>While the introduction further promises to &quot;also investigate possible mechanisms of object realworld size representations.&quot;, I was left wishing for more in this department. The authors report correlations between neural activity and object attributes, as well as between neural activity and ANNs. It would be nice to link the results to theories of object processing (e.g., a feedforward sweep, such as DiCarlo and colleagues have suggested, versus a reverse hierarchy, such as suggested by Hochstein, among others). What is semantic about real-world size, and where might this information come from? (Although you may have to expand beyond the posterior electrodes to do this analysis).</p></disp-quote><p>We thank the reviewer for this insightful comment. We agree that understanding the mechanisms underlying real-world size representations is a critical question. While our current study does not directly test specific theoretical frameworks such as the feedforward sweep model or the reverse hierarchy theory, our results do offer several relevant insights: The temporal dynamics revealed by EEG—where real-world size emerges later than retinal size and depth—suggest that such representations likely arise beyond early visual feedforward stages, potentially involving higherlevel semantic processing. This interpretation is further supported by the fact that real-world size is strongly captured by late layers of ANNs and by a purely semantic model (Word2Vec), suggesting its dependence on learned conceptual knowledge.</p><p>While we acknowledge that our analyses were limited to posterior electrodes and thus cannot directly localize the cortical sources of these effects, we view this work as a first step toward bridging low-level perceptual features and higher-level semantic representations. We hope future work combining broader spatial sampling (e.g., anterior EEG sensors or source localization) and multimodal recordings (e.g., MEG, fMRI) can build on these findings to directly test competing models of object processing and representation hierarchy.</p><p>We also added these to the Discussion section:</p><p>(line 619 to 638) “Although our study does not directly test specific models of visual object processing, the observed temporal dynamics provide important constraints for theoretical interpretations. In particular, we find that real-world size representations emerge significantly later than low-level visual features such as retinal size and depth. This temporal profile is difficult to reconcile with a purely feedforward account of visual processing (e.g., DiCarlo et al., 2012), which posits that object properties are rapidly computed in a sequential hierarchy of increasingly complex visual features. Instead, our results are more consistent with frameworks that emphasize recurrent or top-down processing, such as the reverse hierarchy theory (Hochstein &amp; Ahissar, 2002), which suggests that high-level conceptual information may emerge later and involve feedback to earlier visual areas. This interpretation is further supported by representational similarities with late-stage artificial neural network layers and with a semantic word embedding model (Word2Vec), both of which reflect learned, abstract knowledge rather than low-level visual features. Taken together, these findings suggest that real-world size is not merely a perceptual attribute, but one that draws on conceptual or semantic-level representations acquired through experience. While our EEG analyses focused on posterior electrodes and thus cannot definitively localize cortical sources, we see this study as a step toward linking low-level visual input with higher-level semantic knowledge. Future work incorporating broader spatial coverage (e.g., anterior sensors), source localization, or complementary modalities such as MEG and fMRI will be critical to adjudicate between alternative models of object representation and to more precisely trace the origin and flow of real-world size information in the brain.”</p><disp-quote content-type="editor-comment"><p>Finally, several places in the manuscript tout the &quot;novel computational approach&quot;. This seems odd because the computational framework and pipeline have been the most common approach in cognitive computational neuroscience in the past 5-10 years.</p></disp-quote><p>We have revised relevant statements throughout the manuscript to avoid overstating novelty and to better reflect the contribution of our study.</p><disp-quote content-type="editor-comment"><p>(2) Suggestion: modernize the approach</p><p>I was surprised that the computational models used in this manuscript were all 8-10 years old. Specifically, because there are now deep nets that more explicitly model the human brain (e.g., Cornet) as well as more sophisticated models of semantics (e.g., LLMs), I was left hoping that the authors had used more state-of-the-art models in the work. Moreover, the use of a single dCNN, a single multi-modal model, and a single word embedding model makes it difficult to generalize about visual, multimodal, and semantic features in general.</p></disp-quote><p>Thanks for your suggestion. Indeed, our choice of ResNet and CLIP was motivated by their widespread use in the cognitive and computational neuroscience area. These models have served as standard benchmarks in many studies exploring correspondence between ANNs and human brain activity. To address you concern, we have now added additional results from the more biologically inspired model, CORnet, in the supplementary (Figure S10). The results for CORnet show similar patterns to those observed for ResNet and CLIP, providing converging evidence across models.</p><p>Regarding semantic modeling, we intentionally chose Word2Vec rather than large language models (LLMs), because our goal was to examine concept-level, context-free semantic representations. Word2Vec remains the most widely adopted approach for obtaining noncontextualized embeddings that reflect core conceptual similarity, as opposed to the contextdependent embeddings produced by LLMs, which are less directly suited for capturing stable concept-level structure across stimuli.</p><disp-quote content-type="editor-comment"><p>(3) Methodological considerations</p><p>(a) Validity of the real-world size measurement</p><p>I was concerned about a few aspects of the real-world size rankings. First, I am trying to understand why the scale goes from 100-519. This seems very arbitrary; please clarify. Second, are we to assume that this scale is linear? Is this appropriate when real-world object size is best expressed on a log scale? Third, the authors provide &quot;sand&quot; as an example of the smallest realworld object. This is tricky because sand is more &quot;stuff&quot; than &quot;thing&quot;, so I imagine it leaves observers wondering whether the experimenter intends a grain of sand or a sandy scene region. What is the variability in real-world size ratings? Might the variability also provide additional insights in this experiment?</p></disp-quote><p>We now clarify the origin, scaling, and interpretation of the real-world size values obtained from the THINGS+ dataset.</p><p>In their experiment, participants first rated the size of a single object concept (word shown on the screen) by clicking on a continuous slider of 520 units, which was anchored by nine familiar real-world reference objects (e.g., “grain of sand,” “microwave oven,” “aircraft carrier”) that spanned the full expected size range on a logarithmic scale. Importantly, participants were not shown any numerical values on the scale—they were guided purely by the semantic meaning and relative size of the anchor objects. After the initial response, the scale zoomed in around the selected region (covering 160 units of the 520-point scale) and presented finer anchor points between the previous reference objects. Participants then refined their rating by dragging from the lower to upper end of the typical size range for that object. If the object was standardized in size (e.g., “soccer ball”), a single click sufficed. These size judgments were collected across at least 50 participants per object, and final scores were derived from the central tendency of these responses. Although the final size values numerically range from 0 to 519 (after scaling), this range is not known to participants and is only applied post hoc to construct the size RDMs.</p><p>Regarding the term “sand”: the THINGS+ dataset distinguished between object meanings when ambiguity was present. For “sand,” participants were instructed to treat it as “a grain of sand”— consistent with the intended meaning of a discrete, minimal-size reference object.</p><p>Finally, we acknowledge that real-world size ratings may carry some degree of variability across individuals. However, the dataset includes ratings from 2010 participants across 1854 object concepts, with each object receiving at least 50 independent ratings. Given this large and diverse sample, the mean size estimates are expected to be stable and robust across subjects. While we did not include variability metrics in our main analysis, we believe the aggregated ratings provide a reliable estimate of perceived real-world size.</p><p>We added these details in the Materials and Method section:</p><p>(line 219 to 230) “In the THINGS+ dataset, 2010 participants (different from the subjects in THINGS EEG2) did an online size rating task and completed a total of 13024 trials corresponding to 1854 object concepts using a two-step procedure. In their experiment, first, each object was rated on a 520unit continuous slider anchored by familiar reference objects (e.g., “grain of sand,” “microwave oven,” “aircraft carrier”) representing a logarithmic size range. Participants were not shown numerical values but used semantic anchors as guides. In the second step, the scale zoomed in around the selected region to allow for finer-grained refinement of the size judgment. Final size values were derived from aggregated behavioral data and rescaled to a range of 0–519 for consistency across objects, with the actual mean ratings across subjects ranging from 100.03 (‘grain of sand’) to 423.09 (‘subway’).”</p><disp-quote content-type="editor-comment"><p>(b) This work has no noise ceiling to establish how strong the model fits are, relative to the intrinsic noise of the data. I strongly suggest that these are included.</p></disp-quote><p>We have now computed noise ceiling estimates for the EEG RDMs across time. The noise ceiling was calculated by correlating each participant’s EEG RDM with the average EEG RDM across the remaining participants (leave-one-subject-out), at each time point. This provides an upper-bound estimate of the explainable variance, reflecting the maximum similarity that any model—no matter how complex—could potentially achieve, given the intrinsic variability in the EEG data.</p><p>Importantly, the observed EEG–model similarity values are substantially below this upper bound. This outcome is fully expected: Each of our model RDMs (e.g., real-world size, ANN layers) captures only a specific aspect of the neural representational structure, rather than attempting to account for the totality of the EEG signal. Our goal is not to optimize model performance or maximize fit, but to probe which components of object information are reflected in the spatiotemporal dynamics of the brain’s responses.</p><p>For clarity and accessibility of the main findings, we present the noise ceiling time courses separately in the supplementary materials (Figure S7). Including them directly in the EEG × HYP or EEG × ANN plots would conflate distinct interpretive goals: the model RDMs are hypothesis-driven probes of specific representational content, whereas the noise ceiling offers a normative upper bound for total explainable variance. Keeping these separate ensures each visualization remains focused and interpretable.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors)::</bold></p><p>Some analyses are incomplete, which would be improved if the authors showed analyses with other layers of the networks and various additional partial correlation analyses.</p><p>Clarity</p><p>(1) Partial correlations methods incomplete - it is not clear what is being partialled out in each analysis. It is possible to guess sometimes, but it is not entirely clear for each analysis. This is important as it is difficult to assess if the partial correlations are sensible/correct in each case. Also, the Figure 1 caption is short and unclear.</p><p>For example, ANN-EEG partial correlations - &quot;Finally, we directly compared the timepoint-bytimepoint EEG neural RDMs and the ANN RDMs (Figure 3F). The early layer representations of both ResNet and CLIP were significantly correlated with early representations in the human brain&quot; What is being partialled out? Figure 3F says partial correlation</p></disp-quote><p>We apologize for the confusion. We made several key clarifications and corrections in the revised version.</p><p>First, we identified and corrected a labeling error in both Figure 1 and Figure 3F. Specifically, our EEG × ANN analysis used Spearman correlation, not partial correlation as mistakenly indicated in the original figure label and text. We conducted parital correlations for EEG × HYP and ANN × HYP. But for EEG × ANN, we directly calculated the correlation between EEG RDMs and ANN RDM corresponding to different layers respectively. We corrected these errors: (1) In Figure 1, we removed the erroneous “partial” label from the EEG × ANN path and updated the caption to clearly outline which comparisons used partial correlation. (2) In Figure 3F, we corrected the Y-axis label to “(correlation)”.</p><p>Second, to improve clarity, we have now revised the Materials and Methods section to explicitly describe what is partialled out in each parital correlation analysis:</p><p>(line 284 to 286) “In EEG × HYP partial correlation (Figure 3D), we correlated EEG RDMs with one hypothesis-based RDM (e.g., real-world size), while controlling for the other two (retinal size and real-world depth).”</p><p>(line 303 to 305) “In ANN (or W2V) × HYP partial correlation (Figure 3E and Figure 5A), we correlated ANN (or W2V) RDMs with one hypothesis-based RDM (e.g., real-world size), while partialling out the other two.”</p><p>Finally, the caption of Figure 1 has been expanded to clarify the full analysis pipeline and explicitly specify the partial correlation or correlation in each comparison.</p><p>(line 327 to 332) “Figure 1 Overview of our analysis pipeline including constructing three types of RDMs and conducting comparisons between them. We computed RDMs from three sources: neural data (EEG), hypothesized object features (real-world size, retinal size, and real-world depth), and artificial models (ResNet, CLIP, and Word2Vec). Then we conducted cross-modal representational similarity analyses between: EEG × HYP (partial correlation, controlling for other two HYP features), ANN (or W2V) × HYP (partial correlation, controlling for other two HYP features), and EEG × ANN (correlation).”</p><p>We believe these revisions now make all analytic comparisons and correlation types full clear and interpretable.</p><disp-quote content-type="editor-comment"><p>Issues / open questions</p><p>(2) Semantic representations vs hypothesized (hyp) RDMs (real-world size, etc) - are the representations explained by variables in hyp RDMs or are there semantic representations over and above these? E.g., For ANN correlation with the brain, you could partial out hyp RDMs - and assess whether there is still semantic information left over, or is the variance explained by the hyp RDMs?</p></disp-quote><p>Thank for this suggestion. As you suggested, we conducted the partial correlation analysis between EEG RDMs and ANN RDMs, controlling for the three hypothesis-based RDMs. The results (Figure S6) revealed that the EEG×ANN representational similarity remained largely unchanged, indicating that ANN representations capture much more additional representational structure not accounted for by the current hypothesized features. This is also consistent with the observation that EEG×HYP partial correlations were themselves small, but EEG×ANN correlations were much greater.</p><p>We also added this statement to the main text:</p><p>(line 446 to 451) “To contextualize how much of the shared variance between EEG and ANN representations is driven by the specific visual object features we tested above, we conducted a partial correlation analysis between EEG RDMs and ANN RDMs controlling for the three hypothesis-based RDMs (Figure S6). The EEG×ANN similarity results remained largely unchanged, suggesting that ANN representations capture much more additional rich representational structure beyond these features. ”</p><disp-quote content-type="editor-comment"><p>(3) Why only early and late layers? I can see how it's clearer to present the EEG results. However, the many layers in these networks are an opportunity - we can see how simple/complex linear/non-linear the transformation is over layers in these models. It would be very interesting and informative to see if the correlations do in fact linearly increase from early to later layers, or if the story is a bit more complex. If not in the main text, then at least in the supplement.</p></disp-quote><p>Thank you for the thoughtful suggestion. To address this point, we have computed the EEG correlations with multiple layers in both ResNet and CLIP models (ResNet: ResNet.maxpool, ResNet.layer1, ResNet.layer2, ResNet.layer3, ResNet.layer4, ResNet.avgpool; CLIP:CLIP.visual.avgpool, CLIP.visual.layer1, CLIP.visual.layer2, CLIP.visual.layer3, CLIP.visual.layer4, CLIP.visual.attnpool). The results, now included in Figure S4 and S5, show a consistent trend: early layers exhibit higher similarity to early EEG time points, and deeper layers show increased similarity to later EEG stages. We chose to highlight early and late layers in the main text to simplify interpretation, but now provide the full layerwise profile for completeness.</p><disp-quote content-type="editor-comment"><p>(4) Peak latency analysis - Estimating peaks per ppt is presumably noisy, so it seems important to show how reliable this is. One option is to find the bootstrapped mean latencies per subject.</p></disp-quote><p>Thanks for your suggestion. To estimate the robustness of peak latency values, we implemented a bootstrap procedure by resampling the pairwise entries of the EEG RDM with replacement. For each bootstrap sample, we computed a new EEG RDM and recalculated the partial correlation time course with the hypothesis RDMs. We then extracted the peak latency within the predefined significant time window. Repeating this process 1000 times allowed us to get the bootstrapped mean latencies per subject as the more stable peak latency result. Notably, the bootstrapped results showed minimal deviation from the original latency estimates, confirming the robustness of our findings. Accordingly, we updated the Figure 3D and added these in the Materials and Methods section:</p><p>(line 289 to 298) “To assess the stability of peak latency estimates for each subject, we performed a bootstrap procedure across stimulus pairs. At each time point, the EEG RDM was vectorized by extracting the lower triangle (excluding the diagonal), resulting in 19,900 unique pairwise values. For each bootstrap sample, we resampled these 19,900 pairwise entries with replacement to generate a new pseudo-RDM of the same size. We then computed the partial correlation between the EEG pseudo-RDM and a given hypothesis RDM (e.g., real-world size), controlling for other feature RDMs, and obtained a time course of partial correlations. Repeating this procedure 1000 times and extracting the peak latency within the significant time window yielded a distribution of bootstrapped latencies, from which we got the bootstrapped mean latencies per subject.”</p><disp-quote content-type="editor-comment"><p>(5) &quot;Due to our calculations being at the object level, if there were more than one of the same objects in an image, we cropped the most complete one to get a more accurate retinal size. &quot; Did EEG experimenters make sure everyone sat the same distance from the screen? and remain the same distance? This would also affect real-world depth measures.</p></disp-quote><p>Yes, the EEG dataset we used (THINGS EEG2; Gifford et al., 2022) was collected under carefully controlled experimental conditions. We have confirmed that all participants were seated at a fixed distance of 0.6 meters from the screen throughout the experiment. We also added this information in the method (line 156 to 157).</p><disp-quote content-type="editor-comment"><p>Minor issues/questions - note that these are not raised in the Public Review</p><p>(6) Title - less about rigor/quality of the work but I feel like the title could be improved/extended. The work tells us not only about real object size, but also retinal size and depth. In fact, isn't the most novel part of this the real-world depth aspect? Furthermore, it feels like the current title restricts its relevance and impact... Also doesn't touch on the temporal aspect, or processing stages, which is also very interesting. There may be something better, but simply adding something like&quot;...disentangled features of real-world size, depth, and retinal size over time OR processing stages&quot;.</p></disp-quote><p>Thanks for your suggestion! We changed our title – “Human EEG and artificial neural networks reveal disentangled representations and processing timelines of object real-world size and depth in natural images”.</p><disp-quote content-type="editor-comment"><p>(7) &quot;Each subject viewed 16740 images of objects on a natural background for 1854 object concepts from the THINGS dataset (Hebart et al., 2019). For the current study, we used the 'test' dataset portion, which includes 16000 trials per subject corresponding to 200 images.&quot; Why test images? Worth explaining.</p></disp-quote><p>We chose to use the “test set” of the THINGS EEG2 dataset for the following two reasons:</p><p>(1) Higher trial count per condition: In the test set, each of the 200 object images was presented 80 times per subject, whereas in the training set, each image was shown only 4 times. This much higher trial count per condition in the test set allows for substantially higher signal-tonoise ratio in the EEG data.</p><p>(2) Improved decoding reliability: Our analysis relies on constructing EEG RDMs based on pairwise decoding accuracy using linear SVM classifiers. Reliable decoding estimates require a sufficient number of trials per condition. The test set design is thus better suited to support high-fidelity decoding and robust representational similarity analysis.</p><p>We also added these explainations to our revised manuscript (line 161 to 164).</p><disp-quote content-type="editor-comment"><p>(8) &quot;For Real-World Size RDM, we obtained human behavioral real-world size ratings of each object concept from the THINGS+ dataset (Stoinski et al., 2022).... The range of possible size ratings was from 0 to 519 in their online size rating task...&quot; How were the ratings made? What is this scale - do people know the numbers? Was it on a continuous slider?</p></disp-quote><p>We should clarify how the real-world size values were obtained from the THINGS+ dataset.</p><p>In their experiment, participants first rated the size of a single object concept (word shown on the screen) by clicking on a continuous slider of 520 units, which was anchored by nine familiar real-world reference objects (e.g., “grain of sand,” “microwave oven,” “aircraft carrier”) that spanned the full expected size range on a logarithmic scale. Importantly, participants were not shown any numerical values on the scale—they were guided purely by the semantic meaning and relative size of the anchor objects. After the initial response, the scale zoomed in around the selected region (covering 160 units of the 520-point scale) and presented finer anchor points between the previous reference objects. Participants then refined their rating by dragging from the lower to upper end of the typical size range for that object. If the object was standardized in size (e.g., “soccer ball”), a single click sufficed. These size judgments were collected across at least 50 participants per object, and final scores were derived from the central tendency of these responses. Although the final size values numerically range from 0 to 519 (after scaling), this range is not known to participants and is only applied post hoc to construct the size RDMs.</p><p>We added these details in the Materials and Method section:</p><p>(line 219 to 230) “In the THINGS+ dataset, 2010 participants (different from the subjects in THINGS EEG2) did an online size rating task and completed a total of 13024 trials corresponding to 1854 object concepts using a two-step procedure. In their experiment, first, each object was rated on a 520unit continuous slider anchored by familiar reference objects (e.g., “grain of sand,” “microwave oven,” “aircraft carrier”) representing a logarithmic size range. Participants were not shown numerical values but used semantic anchors as guides. In the second step, the scale zoomed in around the selected region to allow for finer-grained refinement of the size judgment. Final size values were derived from aggregated behavioral data and rescaled to a range of 0–519 for consistency across objects, with the actual mean ratings across subjects ranging from 100.03 (‘grain of sand’) to 423.09 (‘subway’).”</p><disp-quote content-type="editor-comment"><p>(9) &quot;For Retinal Size RDM, we applied Adobe Photoshop (Adobe Inc, 2019) to crop objects corresponding to object labels from images manually... &quot; Was this by one person? Worth noting, and worth sharing these values per image if not already for other researchers as it could be a valuable resource (and increase citations).</p></disp-quote><p>Yes, all object cropping were performed consistently by one of the authors to ensure uniformity across images. We agree that this dataset could be a useful resource to the community. We have now made the cropped object images publicly available <ext-link ext-link-type="uri" xlink:href="https://github.com/ZitongLu1996/RWsize">https://github.com/ZitongLu1996/RWsize</ext-link>.</p><p>We also updated the manuscript accordingly to note this (line 236 to 239).</p><disp-quote content-type="editor-comment"><p>(10) &quot;Neural RDMs. From the EEG signal, we constructed timepoint-by-timepoint neural RDMs for each subject with decoding accuracy as the dissimilarity index &quot; Decoding accuracy is presumably a similarity index. Maybe 1-accuracy (proportion correct) for dissimilarity?</p></disp-quote><p>Decoding accuracy is a dissimilarity index instead of a similarity index, as higher decoding accuracy between two conditions indicates that they are more distinguishable – i.e., less similar – in the neural response space. This approach aligns with prior work using classification-based representational dissimilarity measures (Grootswagers et al., 2017; Xie et al., 2020), where better decoding implies greater dissimilarity between conditions. Therefore, there is no need to invert the decoding accuracy values (e.g., using 1 - accuracy).</p><p>Grootswagers, T., Wardle, S. G., &amp; Carlson, T. A. (2017). Decoding dynamic brain patterns from evoked responses: A tutorial on multivariate pattern analysis applied to time series neuroimaging data. Journal of Cognitive Neuroscience, 29(4), 677-697.</p><p>Xie, S., Kaiser, D., &amp; Cichy, R. M. (2020). Visual imagery and perception share neural representations in the alpha frequency band. Current Biology, 30(13), 2621-2627.</p><disp-quote content-type="editor-comment"><p>(11) Figure 1 caption is very short - Could do with a more complete caption. Unclear what the partial correlations are (what is being partialled out in each case), what are the comparisons &quot;between them&quot; - both in the figure and the caption. Details should at least be in the main text.</p></disp-quote><p>Related to your comment (1). We revised the caption and the corresponding text.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>(1) Intro:</p><p>Quek et al., (2023) is referred to as a behavioral study, but it has EEG analyses.</p></disp-quote><p>We corrected this – “…, one recent study (Quek et al., 2023) …”</p><disp-quote content-type="editor-comment"><p>The phrase 'high temporal resolution EEG' is a bit strange - isn't all EEG high temporal resolution? Especially when down-sampling to 100 Hz (40 time points/epoch) this does not qualify as particularly high-res.</p></disp-quote><p>We removed this phrasing in our manuscript.</p><disp-quote content-type="editor-comment"><p>(2) Methods:</p><p>It would be good to provide more details on the EEG preprocessing. Were the data low-pass filtered, for example?</p></disp-quote><p>We added more details to the manuscript:</p><p>(line 167 to 174) “The EEG data were originally sampled at 1000Hz and online-filtered between 0.1 Hz and 100 Hz during acquisition, with recordings referenced to the Fz electrode. For preprocessing, no additional filtering was applied. Baseline correction was performed by subtracting the mean signal during the 100 ms pre-stimulus interval from each trial and channel separately. We used already preprocessed data from 17 channels with labels beginning with “O” or “P” (O1, Oz, O2, PO7, PO3, POz, PO4, PO8, P7, P5, P3, P1, Pz, P2) ensuring full coverage of posterior regions typically involved in visual object processing. The epoched data were then down-sampled to 100 Hz.”</p><disp-quote content-type="editor-comment"><p>It is important to provide more motivation about the specific ANN layers chosen. Were these layers cherry-picked, or did they truly represent a gradual shift over the course of layers?</p></disp-quote><p>We appreciate the reviewer’s concern and fully agree that it is important to ensure transparency in how ANN layers were selected. The early and late layers reported in the main text were not cherry-picked to maximize effects, but rather intended to serve as illustrative examples representing the lower and higher ends of the network hierarchy. To address this point directly, we have computed the EEG correlations with multiple layers in both ResNet and CLIP models (ResNet: ResNet.maxpool, ResNet.layer1, ResNet.layer2, ResNet.layer3, ResNet.layer4, ResNet.avgpool; CLIP: CLIP.visual.avgpool, CLIP.visual.layer1, CLIP.visual.layer2, CLIP.visual.layer3, CLIP.visual.layer4, CLIP.visual.attnpool). The results, now included in Figure S4, show a consistent trend: early layers exhibit higher similarity to early EEG time points, and deeper layers show increased similarity to later EEG stages.</p><disp-quote content-type="editor-comment"><p>It is important to provide more specific information about the specific ANN layers chosen. 'Second convolutional layer': is this block 2, the ReLu layer, the maxpool layer? What is the 'last visual layer'?</p></disp-quote><p>Apologize for the confusing! We added more details about the layer chosen:</p><p>(line 255 to 257) “The early layer in ResNet refers to ResNet.maxpool layer, and the late layer in ResNet refers to ResNet.avgpool layer. The early layer in CLIP refers to CLIP.visual.avgpool layer, and the late layer in CLIP refers to CLIP.visual.attnpool layer.”</p><disp-quote content-type="editor-comment"><p>Again the claim 'novel' is a bit overblown here since the real-world size ratings were also already collected as part of THINGS+, so all data used here is available.</p></disp-quote><p>We removed this phrasing in our manuscript.</p><disp-quote content-type="editor-comment"><p>Real-world size ratings ranged 'from 0 - 519'; it seems unlikely this was the actual scale presented to subjects, I assume it was some sort of slider?</p></disp-quote><p>You are correct. We should clarify how the real-world size values were obtained from the THINGS+ dataset.</p><p>In their experiment, participants first rated the size of a single object concept (word shown on the screen) by clicking on a continuous slider of 520 units, which was anchored by nine familiar real-world reference objects (e.g., “grain of sand,” “microwave oven,” “aircraft carrier”) that spanned the full expected size range on a logarithmic scale. Importantly, participants were not shown any numerical values on the scale—they were guided purely by the semantic meaning and relative size of the anchor objects. After the initial response, the scale zoomed in around the selected region (covering 160 units of the 520-point scale) and presented finer anchor points between the previous reference objects. Participants then refined their rating by dragging from the lower to upper end of the typical size range for that object. If the object was standardized in size (e.g., “soccer ball”), a single click sufficed. These size judgments were collected across at least 50 participants per object, and final scores were derived from the central tendency of these responses. Although the final size values numerically range from 0 to 519 (after scaling), this range is not known to participants and is only applied post hoc to construct the size RDMs.</p><p>We added these details in the Materials and Method section:</p><p>(line 219 to 230) “In the THINGS+ dataset, 2010 participants (different from the subjects in THINGS EEG2) did an online size rating task and completed a total of 13024 trials corresponding to 1854 object concepts using a two-step procedure. In their experiment, first, each object was rated on a 520unit continuous slider anchored by familiar reference objects (e.g., “grain of sand,” “microwave oven,” “aircraft carrier”) representing a logarithmic size range. Participants were not shown numerical values but used semantic anchors as guides. In the second step, the scale zoomed in around the selected region to allow for finer-grained refinement of the size judgment. Final size values were derived from aggregated behavioral data and rescaled to a range of 0–519 for consistency across objects, with the actual mean ratings across subjects ranging from 100.03 (‘grain of sand’) to 423.09 (‘subway’).”</p><disp-quote content-type="editor-comment"><p>Why is conducting a one-tailed (p&lt;0.05) test valid for EEG-ANN comparisons? Shouldn't this be two-tailed?</p></disp-quote><p>Our use of one-tailed tests was based on the directional hypothesis that representational similarity between EEG and ANN RDMs would be positive, as supported by prior literature showing correspondence between hierarchical neural networks and human brain representations (e.g., Cichy et al., 2016; Kuzovkin et al., 2014). This is consistent with a large number of RSA studies which conduct one-tailed tests (i.e., testing the hypothesis that coefficients were greater than zero: e.g., Kuzovkin et al., 2018; Nili et al., 2014; Hebart et al., 2018; Kaiser et al., 2019; Kaiser et al., 2020; Kaiser et al., 2022). Thus, we specifically tested whether the similarity was significantly greater than zero.</p><p>Cichy, R. M., Khosla, A., Pantazis, D., Torralba, A., &amp; Oliva, A. (2016). Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. Scientific reports, 6(1), 27755.</p><p>Kuzovkin, I., Vicente, R., Petton, M., Lachaux, J. P., Baciu, M., Kahane, P., ... &amp; Aru, J. (2018). Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex. Communications biology, 1(1), 107.</p><p>Nili, H., Wingfield, C., Walther, A., Su, L., Marslen-Wilson, W., &amp; Kriegeskorte, N. (2014). A toolbox for representational similarity analysis. PLoS computational biology, 10(4), e1003553.</p><p>Hebart, M. N., Bankson, B. B., Harel, A., Baker, C. I., &amp; Cichy, R. M. (2018). The representational dynamics of task and object processing in humans. Elife, 7, e32816.</p><p>Kaiser, D., Turini, J., &amp; Cichy, R. M. (2019). A neural mechanism for contextualizing fragmented inputs during naturalistic vision. elife, 8, e48182.</p><p>Kaiser, D., Inciuraite, G., &amp; Cichy, R. M. (2020). Rapid contextualization of fragmented scene information in the human visual system. Neuroimage, 219, 117045.</p><p>Kaiser, D., Jacobs, A. M., &amp; Cichy, R. M. (2022). Modelling brain representations of abstract concepts. PLoS Computational Biology, 18(2), e1009837.</p><p>Importantly, we note that using a two-tailed test instead would not change the significance of our results. However, we believe the one-tailed test remains more appropriate given our theoretical prediction of positive similarity between ANN and brain representations.</p><disp-quote content-type="editor-comment"><p>The sentence on the partial correlation description (page 11 'we calculated partial correlations with one-tailed test against the alternative hypothesis that the partial correlation was positive (greater than zero)') didn't make sense to me; are you referring to the null hypothesis here?</p></disp-quote><p>We revised this sentence to clarify that we tested against the null hypothesis that the partial correlation was less than or equal to zero, using a one-tailed test to assess whether the correlation was significantly greater than zero.</p><p>(line 281 to 284) “…, we calculated partial correlations and used a one-tailed test against the null hypothesis that the partial correlation was less than or equal to zero, testing whether the partial correlation was significantly greater than zero.”</p><disp-quote content-type="editor-comment"><p>(3) Results:</p><p>I would prevent the use of the word 'pure', your measurement is one specific operationalization of this concept of real-world size that is not guaranteed to result in unconfounded representations. This is in fact impossible whenever one is using a finite set of natural stimuli and calculating metrics on those - there can always be a factor or metric that was not considered that could explain some of the variance in your measurement. It is overconfident to claim to have achieved some form of Platonic ideal here and to have taken into account all confounds.</p></disp-quote><p>Your point is well taken. Our original use of the term “pure” was intended to reflect statistical control for known confounding factors, but we recognize that this wording may imply a stronger claim than warranted. In response, we revised all relevant language in the manuscript to instead describe the statistically isolated or relatively unconfounded representation of real-world size, clarifying that our findings pertain to the unique contribution of real-world size after accounting for retinal size and real-world depth.</p><disp-quote content-type="editor-comment"><p>Figure 2C: It's not clear why peak latencies are computed on the 'full' correlations rather than the partial ones.</p></disp-quote><p>No. The peak latency results in Figure 2C were computed on the partial correlation results – we mentioned this in the figure caption – “Temporal latencies for peak similarity (partial Spearman correlations) between EEG and the 3 types of object information.”</p><disp-quote content-type="editor-comment"><p>SEM = SEM across the 10 subjects?</p></disp-quote><p>Yes. We added this in the figure caption.</p><disp-quote content-type="editor-comment"><p>Figure 3F y-axis says it's partial correlations but not clear what is partialled out here.</p></disp-quote><p>We identified and corrected a labeling error in both Figure 1 and Figure 3F. Specifically, our EEG × ANN analysis used Spearman correlation, not partial correlation as mistakenly indicated in the original figure label and text. We conducted parital correlations for EEG × HYP and ANN × HYP. But for EEG × ANN, we directly calculated the correlation between EEG RDMs and ANN RDM corresponding to different layers respectively. We corrected these errors: (1) In Figure 1, we removed the erroneous “partial” label from the EEG × ANN path and updated the caption to clearly outline which comparisons used partial correlation. (2) In Figure 3F, we corrected the Y-axis label to “(correlation)”.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>(1) Several methodologies should be clarified:</p><p>(a) It's stated that EEG was sampled at 100 Hz. I assume this was downsampled? From what original frequency?</p></disp-quote><p>Yes. We added more detailed about EEG data:</p><p>(line 167 to 174) “The EEG data were originally sampled at 1000Hz and online-filtered between 0.1 Hz and 100 Hz during acquisition, with recordings referenced to the Fz electrode. For preprocessing, no additional filtering was applied. Baseline correction was performed by subtracting the mean signal during the 100 ms pre-stimulus interval from each trial and channel separately. We used already preprocessed data from 17 channels with labels beginning with “O” or “P” (O1, Oz, O2, PO7, PO3, POz, PO4, PO8, P7, P5, P3, P1, Pz, P2) ensuring full coverage of posterior regions typically involved in visual object processing. The epoched data were then down-sampled to 100 Hz.”</p><disp-quote content-type="editor-comment"><p>(b) Why was decoding accuracy used as the human RDM method rather than the EEG data themselves?</p></disp-quote><p>Thanks for your question! We would like to address why we used decoding accuracy for EEG RDMs rather than correlation. While fMRI RDMs are typically calculated using 1 minus correlation coefficient, decoding accuracy is more commonly used for EEG RDMs (Grootswager et al., 2017; Xie et al., 2020). The primary reason is that EEG signals are more susceptible to noise than fMRI data. Correlation-based methods are particularly sensitive to noise and may not reliably capture the functional differences between EEG patterns for different conditions. Decoding accuracy, by training classifiers to focus on task-relevant features, can effectively mitigate the impact of noisy signals and capture the representational difference between two conditions.</p><p>Grootswagers, T., Wardle, S. G., &amp; Carlson, T. A. (2017). Decoding dynamic brain patterns from evoked responses: A tutorial on multivariate pattern analysis applied to time series neuroimaging data. Journal of Cognitive Neuroscience, 29(4), 677-697.</p><p>Xie, S., Kaiser, D., &amp; Cichy, R. M. (2020). Visual imagery and perception share neural representations in the alpha frequency band. Current Biology, 30(13), 2621-2627.</p><p>We added this explanation to the manuscript:</p><p>(line 204 to 209) “Since EEG has a low SNR and includes rapid transient artifacts, Pearson correlations computed over very short time windows yield unstable dissimilarity estimates (Kappenman &amp; Luck, 2010; Luck, 2014) and may thus fail to reliably detect differences between images. In contrast, decoding accuracy - by training classifiers to focus on task-relevant features - better mitigates noise and highlights representational differences.”</p><disp-quote content-type="editor-comment"><p>(c) How were the specific posterior electrodes selected?</p></disp-quote><p>The 17 posterior electrodes used in our analyses were pre-selected and provided in the THINGS EEG2 dataset, and corresponding to standard occipital and parietal sites based on the 10-10 EEG system. Specifically, we included all 17 electrodes with labels beginning with “O” or “P”, ensuring full coverage of posterior regions typically involved in visual object processing (Page 7).</p><disp-quote content-type="editor-comment"><p>(d) The specific layers should be named rather than the vague (&quot;last visual&quot;)</p></disp-quote><p>Apologize for the confusing! We added more details about the layer information:</p><p>(line 255 to 257) “The early layer in ResNet refers to ResNet.maxpool layer, and the late layer in ResNet refers to ResNet.avgpool layer. The early layer in CLIP refers to CLIP.visual.avgpool layer, and the late layer in CLIP refers to CLIP.visual.attnpool layer.”</p><p>(line 420 to 434) “As shown in Figure 3F, the early layer representations of both ResNet and CLIP (ResNet.maxpool layer and CLIP.visual.avgpool) showed significant correlations with early EEG time windows (early layer of ResNet: 40-280ms, early layer of CLIP: 50-130ms and 160-260ms), while the late layers (ResNet.avgpool layer and CLIP.visual.attnpool layer) showed correlations extending into later time windows (late layer of ResNet: 80-300ms, late layer of CLIP: 70-300ms). Although there is substantial temporal overlap between early and late model layers, the overall pattern suggests a rough correspondence between model hierarchy and neural processing stages.</p><p>We further extended this analysis across intermediate layers of both ResNet and CLIP models (from early to late, ResNet: ResNet.maxpool, ResNet.layer1, ResNet.layer2, ResNet.layer3, ResNet.layer4, ResNet.avgpool; from early to late, CLIP: CLIP.visual.avgpool, CLIP.visual.layer1, CLIP.visual.layer2, CLIP.visual.layer3, CLIP.visual.layer4, CLIP.visual.attnpool).”</p><disp-quote content-type="editor-comment"><p>(e) p19: please change the reporting of t-statistics to standard APA format.</p></disp-quote><p>Thanks for the suggestion. We changed the reporting format accordingly:</p><p>(line 392 to 394) “The representation of real-word size had a significantly later peak latency than that of both retinal size, t(9)=4.30, p=.002, and real-world depth, t(9)=18.58, p&lt;.001. And retinal size representation had a significantly later peak latency than real-world depth, t(9)=3.72, p=.005.”</p><disp-quote content-type="editor-comment"><p>(2) &quot;early layer of CLIP: 50-130ms and 160-260ms, while the late layer representations of twoANNs were significantly correlated with later representations in the human brain (late layer of ResNet: 80-300ms, late layer of CLIP: 70-300ms).&quot;</p><p>This seems a little strong, given the large amount of overlap between these models.</p></disp-quote><p>We agree that our original wording may have overstated the distinction between early and late layers, given the substantial temporal overlap in their EEG correlations. We revised this sentence to soften the language to reflect the graded nature of the correspondence, and now describe the pattern as a general trend rather than a strict dissociation:</p><p>(line 420 to 427) “As shown in Figure 3F, the early layer representations of both ResNet and CLIP (ResNet.maxpool layer and CLIP.visual.avgpool) showed significant correlations with early EEG time windows (early layer of ResNet: 40-280ms, early layer of CLIP: 50-130ms and 160-260ms), while the late layers (ResNet.avgpool layer and CLIP.visual.attnpool layer) showed correlations extending into later time windows (late layer of ResNet: 80-300ms, late layer of CLIP: 70-300ms). Although there is substantial temporal overlap between early and late model layers, the overall pattern suggests a rough correspondence between model hierarchy and neural processing stages.”</p><disp-quote content-type="editor-comment"><p>(3) &quot;Also, human brain representations showed a higher similarity to the early layer representation of the visual model (ResNet) than to the visual-semantic model (CLIP) at an early stage. &quot;</p><p>This has been previously reported by Greene &amp; Hansen, 2020 J Neuro.</p></disp-quote><p>Thanks! We added this reference.</p><disp-quote content-type="editor-comment"><p>(4) &quot;ANN (and Word2Vec) model RDMs&quot;</p><p>Why not just &quot;model RDMs&quot;? Might provide more clarity.</p></disp-quote><p>We chose to use the phrasing “ANN (and Word2Vec) model RDMs” to maintain clarity and avoid ambiguity. In the literature, the term “model RDMs” is sometimes used more broadly to include hypothesis-based feature spaces or conceptual models, and we wanted to clearly distinguish our use of RDMs derived from artificial neural networks and language models. Additionally, explicitly referring to ANN or Word2Vec RDMs improves clarity by specifying the model source of each RDM. We hope this clarification justifies our choice to retain the original phrasing for clarity.</p></body></sub-article></article>