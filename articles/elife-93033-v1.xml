<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93033</article-id><article-id pub-id-type="doi">10.7554/eLife.93033</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93033.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Visual homogeneity computations in the brain enable solving property-based visual tasks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Jacob</surname><given-names>Georgin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8262-0155</contrib-id><email>georginjacob@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5933-7893</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Arun</surname><given-names>SP</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9602-5066</contrib-id><email>sparun@iisc.ac.in</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Centre for Neuroscience &amp; Department of Electrical Communication Engineering, Indian Institute of Science</institution></institution-wrap><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kok</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Massachusetts Institute of Technology, Cambridge, MA, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>18</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP93033</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-02"><day>02</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-11-05"><day>05</day><month>11</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.12.03.518965"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-05"><day>05</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93033.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-14"><day>14</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93033.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-25"><day>25</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93033.3"/></event></pub-history><permissions><copyright-statement>© 2024, Jacob et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Jacob et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93033-v1.pdf"/><abstract><p>Most visual tasks involve looking for specific object features. But we also often perform property-based tasks where we look for specific property in an image, such as finding an odd item, deciding if two items are same, or if an object has symmetry. How do we solve such tasks? These tasks do not fit into standard models of decision making because their underlying feature space and decision process is unclear. Using well-known principles governing multiple object representations, we show that displays with repeating elements can be distinguished from heterogeneous displays using a property we define as visual homogeneity. In behavior, visual homogeneity predicted response times on visual search, same-different and symmetry tasks. Brain imaging during visual search and symmetry tasks revealed that visual homogeneity was localized to a region in the object-selective cortex. Thus, property-based visual tasks are solved in a localized region in the brain by computing visual homogeneity.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual search</kwd><kwd>object categorization</kwd><kwd>same different</kwd><kwd>symmetry</kwd><kwd>object perception</kwd><kwd>high-level vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009053</institution-id><institution>Wellcome Trust DBT India Alliance</institution></institution-wrap></funding-source><award-id>IA/S/17/1/503081</award-id><principal-award-recipient><name><surname>Arun</surname><given-names>SP</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004541</institution-id><institution>MHRD</institution></institution-wrap></funding-source><award-id>Senior Research Fellowship</award-id><principal-award-recipient><name><surname>Jacob</surname><given-names>Georgin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Seemingly disparate property-based tasks (oddball search, same-different and symmetry) are solved by computing a novel image property, visual homogeneity, which is localized to the object selective cortex.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Many visual tasks involve looking for specific objects or features, such as a friend in a crowd or selecting vegetables in the market. In such tasks, which have been studied extensively, we form a template in our brain that helps guide eye movements and locate the target (<xref ref-type="bibr" rid="bib30">Peelen and Kastner, 2014</xref>). This template becomes a decision variable that can be used to solve the task: the degree of match to the template indicates the presence or absence of the desired object (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). However, we also easily perform tasks that do not involve any specific feature template but rather involve finding a property in the image. Examples of such property-based tasks are same-different task, finding the oddball item and judging if an object has symmetry (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). These tasks cannot be solved by looking for any particular feature, and therefore present a major challenge for standard models of decision making since the underlying feature space and decision variable are unknown. Even machine vision algorithms, which are so successful at solving feature-based tasks (<xref ref-type="bibr" rid="bib44">Serre, 2019</xref>), fail at detecting properties like same-different and at other similar visual reasoning challenges (<xref ref-type="bibr" rid="bib12">Fleuret et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Ricci et al., 2021</xref>; <xref ref-type="bibr" rid="bib38">Puebla and Bowers, 2022</xref>). How do we solve such property-based visual tasks? What are the underlying features and what is the decision variable?</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Feature-based and property-based visual tasks.</title><p>(<bold>A</bold>) <italic>Feature-based visual tasks</italic>. Most visual tasks involve making decisions based on looking at specific features. Face recognition is shown here as an example. According to standard theories of decision making, such tasks are solved in the brain by setting up a decision variable in a multidimensional feature space (<italic>arrow</italic>), and making decisions based on whether the value of the decision variable is larger or smaller than a decision boundary (<italic>dashed line</italic>). (<bold>B</bold>) <italic>Property-based visual tasks</italic>. By contrast, some tasks involve detecting properties in the image, such as a same-different task (illustrated using faces; <italic>top row</italic>), detecting an oddball item (illustrated using <italic>middle row</italic>) or judging if an object is symmetric (<italic>bottom row</italic>). These tasks cannot be solved by looking for any specific feature. As a result, such tasks do not fit into standard models of decision making since the underlying feature space and decision variable are unknown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-fig1-v1.tif"/></fig><p>To start with, these tasks appear completely different, at least in the way we describe them verbally. Even theoretical studies have considered visual search (<xref ref-type="bibr" rid="bib56">Verghese, 2001</xref>; <xref ref-type="bibr" rid="bib60">Wolfe and Horowitz, 2017</xref>), same-different judgments (<xref ref-type="bibr" rid="bib28">Nickerson, 1969</xref>; <xref ref-type="bibr" rid="bib32">Petrov, 2009</xref>) and symmetry detection (<xref ref-type="bibr" rid="bib57">Wagemans, 1997</xref>; <xref ref-type="bibr" rid="bib6">Bertamini and Makin, 2014</xref>) as conceptually distinct tasks. However, we note that at a deeper level, these tasks are similar because they all involve discriminating between items with repeating features from those without repeating features. We reasoned that if images with repeating features are somehow represented differently in the brain, this difference could be used to solve all such tasks without requiring separate computations for each task.</p><sec id="s1-1"><title>Key predictions</title><p>Our key predictions are summarized in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Consider an oddball search task where participants have to indicate if a display contains an oddball target (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) or not (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). According to the well-known principle of divisive normalization in high-level visual cortex (<xref ref-type="bibr" rid="bib63">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="bib1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="bib18">Katti and Arun, 2022</xref>), the neural response to multiple objects is the average of the single object responses. Accordingly, the response to an array of identical items will be the same as the response to the single item. Similarly, the response to an array containing a target among distractors, being a mix of images, would lie along the line joining the target and distractor in underlying representational space. These possibilities are shown for all possible arrays made from three objects in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. It can be seen that the homogeneous (target-absent) arrays naturally stand apart since they contain repeating items, whereas the heterogeneous (target-present) arrays come closer to each other since they contain a mixture of items. Since displays with repeating items are further away from the center of this space, this distance can be used to discriminate them from heterogeneous displays (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <italic>inset</italic>). While this decision process may not capture all types of visual search, we reasoned that it could potentially guide the initial stages of target selection.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Solving oddball search and symmetry tasks using visual homogeneity.</title><p>(<bold>A</bold>) Example target-present search display, containing a single oddball target (horse) among identical distractors (dog). Participants in such tasks have to indicate whether the display contains an oddball or not, without knowing the features of the target or distractor. This means they have to perform this task by detecting some property of each display rather than some feature contained in it. (<bold>B</bold>) Example target-absent search display containing no oddball target. (<bold>C</bold>) Hypothesized neural computation for target present/absent judgements. According to multiple object normalization, the response to multiple items is an average of the responses to the individual items. Thus, the response to a target-absent array will be identical to the individual items, whereas the response to a target-present array will lie along the line joining the corresponding target-absent arrays. This causes the target-absent arrays to stay apart (<italic>red lines</italic>), and the target-present arrays to come closer due to mixing (<italic>blue lines</italic>). If we calculate the distance (VH, for visual homogeneity) for each display, then target-absent arrays will have a larger distance to the center (VH<sub>a</sub>) compared to target-present arrays (VH<sub>p</sub>), and this distance can be used to distinguish between them. <italic>Inset:</italic> Schematic distance from center for target-absent arrays (red) and target-present arrays (blue). Note that this approach might only reflect the initial target selection process involved in oddball visual search and may not capture all forms of visual search. Nonetheless it is a quantitative and falsifiable model. (<bold>D</bold>) Example asymmetric object in a symmetry detection task. Here too, participants have to indicate if the display contains a symmetric object or not, without knowing the features of the object itself. This means they have to perform this task by detecting some property in the display. (<bold>E</bold>) Example symmetric object in a symmetry detection task. (<bold>F</bold>) Hypothesized neural computations for symmetry detection. Following multiple object normalization, the response to an object containing repeated parts is equal the response to the individual part, whereas the response to an object containing two different parts will lie along the line joining the objects with the two parts repeating. This causes symmetric objects to stand apart (red lines) and asymmetric objects to come closer due to mixing (<italic>blue lines</italic>). Thus, the visual homogeneity for symmetric objects (VH<sub>s</sub>) will be larger than for asymmetric objects (VH<sub>a</sub>). <italic>Inset:</italic> Schematic distance from center for symmetric objects (red) and asymmetric objects (blue). (<bold>G</bold>) <italic>Behavioral predictions for VH</italic>. If visual homogeneity (VH) is a decision variable in visual search and symmetry detection tasks, then response times (RT) must be largest for displays with VH close to the decision boundary. This predicts opposite correlations between response time and VH for the present/absent or symmetry/asymmetry judgements. It also predicts zero overall correlation between VH and RT. (<bold>H</bold>) <italic>Neural predictions for VH. Left:</italic> Correlation between brain activations and VH for two hypothetical brain regions. In the VH-encoding region, brain activations should be positively correlated with VH. In any region that encodes task difficulty as indexed by response time, brain activity should show no correlation since VH itself is uncorrelated with RT (see Panel <bold>G</bold>). <italic>Right:</italic> Correlation between brain activations and RT. Since VH is uncorrelated with RT overall, the region VH should show little or no correlation, whereas the regions encoding task difficulty would show a positive correlation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-fig2-v1.tif"/></fig><p>We reasoned similarly for symmetry detection: here, participants have to decide if an object is asymmetric (<xref ref-type="fig" rid="fig2">Figure 2D</xref>) or symmetric (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). According to multiple object normalization, objects with two different parts would lie along the line joining objects containing the two repeated parts (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Indeed, both symmetric and asymmetric objects show part summation in their neural responses (<xref ref-type="bibr" rid="bib35">Pramod and Arun, 2018</xref>). Consequently, symmetric objects will be further away from the centre of this space compared to asymmetric objects, and this can be the basis for distinguishing them (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, <italic>inset</italic>).</p><p>We define this distance from the center for each image as its <italic>visual homogeneity</italic> (VH). We made two key experimental predictions to test in behavior and brain imaging. First, if VH is being used to solve visual search and symmetry detection tasks, then responses should be slowest for displays with VH close to the decision boundary and faster for displays with VH far away (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). This predicts opposite correlations between response time and VH: for target-present arrays and asymmetric objects, the response time should be positively correlated with VH. By contrast, for target-absent arrays and symmetric objects, response time should be negatively correlated with VH. Importantly, because response times of the two choices are positively and negatively correlated with VH, the net correlation between response time and VH will be close to zero.</p><p>Second, if VH is encoded by a dedicated brain region, then brain activity in that region will be positively correlated with VH (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). Such a positive correlation cannot be explained easily by cognitive processes linked to response time such as attention or task difficulty, since response times will have zero correlation with the mean activity of this region.</p></sec><sec id="s1-2"><title>Overview of this study</title><p>The above predictions are based on computing distances in an underlying neural representation that is the basis for a variety of visual tasks. We drew upon the principle that object representations in high-level visual cortex match strongly with perceived dissimilarity measured in visual search (<xref ref-type="bibr" rid="bib45">Sripati and Olson, 2010</xref>; <xref ref-type="bibr" rid="bib62">Zhivago and Arun, 2014</xref>; <xref ref-type="bibr" rid="bib1">Agrawal et al., 2020</xref>). We therefore asked whether distance-to-center or visual homogeneity computations, performed on object representations estimated empirically from dissimilarity measurements, could explain the behavior and brain imaging predictions laid out in the preceding section (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>We performed two sets of experiments to test our key predictions. In the first set, we measured perceptual dissimilarities on a set of grayscale natural images using visual search (Experiment 1) and then asked whether visual homogeneity computations on this estimated representation could explain response times as well as brain activations during a oddball target detection task (Experiment 2). In the second set of experiments, we measured perceptual dissimilarities between a set of silhouette images (Experiment 3) and asked whether visual homogeneity computations on this estimated representation could explain response times and brain activations during a symmetry detection task on these images (Experiment 4).</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><p>In Experiments 1–2, we investigated whether visual homogeneity computations could explain decisions about targets being present or absent in an array. Since visual homogeneity requires measuring distance in perceptual space, we set out to first characterize the underlying representation of a set of natural objects using measurements of perceptual dissimilarity.</p><sec id="s2-1"><title>Measuring perceptual space for natural objects</title><p>In Experiment 1, 16 human participants viewed arrays made from a set of 32 grayscale natural objects, with an oddball on the left or right (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), and had to indicate the side on which the oddball appeared using a key press. Participants were highly accurate and consistent in their responses during this task (accuracy, mean ± sd: 98.8 ± 0.9%; correlation between mean response times of even- and odd-numbered participants: <italic>r</italic>=0.91, p&lt;0.0001 across all <sup>32</sup>C<sub>2</sub>=496 object pairs). The reciprocal of response time is a measure of perceptual distance (or dissimilarity) between the two images (<xref ref-type="bibr" rid="bib2">Arun, 2012</xref>). To visualize the underlying object representation, we performed a multidimensional scaling analysis, which embeds objects in a multidimensional space such that their pairwise dissimilarities match the experimentally observed dissimilarities (see Materials and methods). The resulting two-dimensional embedding of all objects is shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. In the resulting plot, nearby objects correspond to hard searches, and far away objects correspond to easy searches. Such representations reconstructed from behavioural data closely match population neural responses in high-level visual areas (<xref ref-type="bibr" rid="bib29">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Sripati and Olson, 2010</xref>). To capture the object representation accurately, we took the multidimensional embedding of all objects and treated the values along each dimension as the responses of an individual artificial neuron. We selected the number of dimensions in the multidimensional embedding so that the correlation between the observed and embedding dissimilarities matches the noise ceiling in the data. Subsequently, we averaged these single object responses to obtain responses to larger visual search arrays, as detailed below.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Visual homogeneity predicts target present/absent responses.</title><p>(<bold>A</bold>) Example search array in an oddball search task (Experiment 1). Participants viewed an array containing identical items except for an oddball present either on the left or right side, and had to indicate using a key press which side the oddball appeared. The reciprocal of average search time was taken as the perceptual distance between the target and distractor items. We measured all possible pairwise distances for 32 grayscale natural objects in this manner. (<bold>B</bold>) Perceptual space reconstructed using multidimensional scaling performed on the pairwise perceptual dissimilarities. In the resulting plot, nearby objects represent hard searches, and far away objects represent easy searches. Some images are shown at a small size due to space constraints; in the actual experiment, all objects were equated to have the same longer dimension. The correlation on the top right indicates the match between the distances in the 2D plot with the observed pairwise distances (**** is p&lt;0.00005). (<bold>C</bold>) Example display from Experiment 2. Participants performed this task inside the scanner. On each trial, they had to indicate whether an oddball target is present or absent using a key press. (<bold>D</bold>) Predicted response to target-present and target-absent arrays, using the principle that the neural response to multiple items is the average of the individual item responses. This predicts that target-present arrays become similar due to mixing of responses, whereas target-absent arrays stand apart. Consequently, these two types of displays can be distinguished using their distance to a central point in this space. We define this distance as visual homogeneity, and it is obtained by finding the optimum center that maximizes the difference in correlations with response times (see Methods). (<bold>E</bold>) Mean visual homogeneity relative to the optimum center for target-present and target-absent displays. Error bars represent s.e.m across all displays. Asterisks represent statistical significance (**** is p&lt;0.00005, unpaired rank-sum test comparing visual homogeneity for 32 target-absent and 32 target-present arrays). (<bold>F</bold>) Response time for target-present searches in Experiment 2 plotted against visual homogeneity calculated from Experiment 1. Asterisks represent statistical significance of the correlation (**** is p&lt;0.00005). Note that a single model is fit to find the optimum center in representational space that predicts the response times for both target-present and target-absent searches. (<bold>G</bold>) Response time for target-absent searches in Experiment 2 plotted against visual homogeneity calculated from Experiment 1. Asterisks represent statistical significance of the correlation (**** is p&lt;0.00005).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-fig3-v1.tif"/></fig></sec><sec id="s2-2"><title>Visual homogeneity predicts target present/absent judgments (Experiments 1-2)</title><p>Having characterized the underlying perceptual representation for single objects, we set out to investigate whether target present/absent responses during visual search can be explained using this representation. In Experiment 2, 16 human participants viewed an array of items on each trial, and indicated using a key press whether there was an oddball target present or not (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This task was performed inside an MRI scanner to simultaneously observe both brain activity and behaviour. Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 95 ± 3%; correlation between average response times of even- and odd-numbered participants: <italic>r</italic>=0.86, p&lt;0.0001 across 32 target-present searches, <italic>r</italic>=0.63, p&lt;0.001 across 32 target-absent searches).</p><p>Next we set out to predict the responses to target-present and target-absent search displays containing these objects. We first took the object coordinates returned by multidimensional scaling in Experiment 1 as the neural responses of multiple neurons. We then used the well-known principle of object representations in high-level visual areas: the response to multiple objects is the average of the single object responses (<xref ref-type="bibr" rid="bib63">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="bib1">Agrawal et al., 2020</xref>). Thus, we took the response vector for a target-present array to be the average of the response vectors of the target and distractor (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). Likewise, we took the response vector for a target-absent array to be equal to the response vector of the single item. We then asked if there is any point in this multidimensional representation such that distances from this point to the target-present and target-absent response vectors can accurately predict the target-present and target-absent response times with a positive and negative correlation, respectively (see Materials and methods). Note that this model has only five free parameters, which are the coordinates of this unknown point or center in multidimensional space, and this model can simultaneously predict both target-present and target-absent judgments. We used nonlinear optimization to find the coordinates of the center to best match the data (see Materials and methods).</p><p>We denoted the distance of each display to the optimized center as the visual homogeneity. As expected, the visual homogeneity of target-present arrays was significantly smaller than target-absent arrays (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). The resulting model predictions are shown in <xref ref-type="fig" rid="fig3">Figure 3F–G</xref>. The response times for target-present searches were positively correlated with visual homogeneity (<italic>r</italic>=0.68, p&lt;0.0001; <xref ref-type="fig" rid="fig3">Figure 3F</xref>). By contrast, the response times for target-absent searches were negatively correlated with visual homogeneity (<italic>r</italic>=–0.71, p&lt;0.0001; <xref ref-type="fig" rid="fig3">Figure 3G</xref>). This is exactly as predicted if visual homogeneity is the underlying decision variable (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). We note that the range of visual homogeneity values for target-present and target-absent searches do overlap, suggesting that visual homogeneity contributes but does not fully determine task performance. Rather, we suggest that visual homogeneity provides a useful and initial first guess at the presence or absence of a target, which can be refined further through detailed scrutiny.</p></sec><sec id="s2-3"><title>Confirming the generality of visual homogeneity</title><p>We performed several additional analysis to confirm the generality of our results, and to reject alternate explanations.</p><p>First, it could be argued that our results are circular because they involve taking oddball search times from Experiment 1 and using them to explain visual search response times in Experiment 2. While this might appear so, we are merely using the search dissimilarities from Experiment 1 only as a proxy for the underlying neural representation, based on previous reports that neural dissimilarities closely match oddball search dissimilarities (<xref ref-type="bibr" rid="bib45">Sripati and Olson, 2010</xref>; <xref ref-type="bibr" rid="bib62">Zhivago and Arun, 2014</xref>). Nonetheless, to thoroughly refute this possibility, we reasoned that we would get similar predictions of the target present/absent responses in Experiment 2 using any other brain-like object representation. To confirm this, we replaced the object representations derived from Experiment 1 with object representations derived from deep neural networks pretrained for object categorization, and asked if distance-to-center computations could predict the target present/absent responses in Experiment 2. This was indeed the case (Appendix 1).</p><p>Second, we wondered whether the nonlinear optimization process of finding the best-fitting center could be yielding disparate optimal centres each time. To investigate this, we repeated the optimization procedure with many randomly initialized starting points, and obtained the same best-fitting center each time (see Materials and methods).</p><p>Third, to confirm that the above model fits are not due to overfitting, we performed a leave-one-out cross validation analysis. We left out all target-present and target-absent searches involving a particular image, and then predicted these searches by calculating visual homogeneity estimated from all other images. This too yielded similar positive and negative correlations (<italic>r</italic>=0.63, p&lt;0.0001 for target-present, <italic>r</italic>=–0.63, p&lt;0.001 for target-absent).</p><p>Fourth, if heterogeneous displays indeed elicit similar neural responses due to mixing, then their average distance to other objects must be related to their visual homogeneity. We confirmed that this was indeed the case, suggesting that the average distance of an object from all other objects in visual search can predict its visual homogeneity (Appendix 2).</p><p>Fifth, the above results are based on taking the neural response to oddball arrays to be the average of the target and distractor responses. To confirm that averaging was indeed the optimal choice, we repeated the above analysis by assuming a range of relative summation weights between the target and distractor. The best correlation was obtained for almost equal weights in the lateral occipital (LO) region, consistent with averaging and its role in the underlying perceptual representation (Appendix 2).</p><p>Finally, we performed several additional experiments on a larger set of natural objects as well as on silhouette shapes. In all cases, present/absent responses were explained using visual homogeneity (Appendix 3).</p></sec><sec id="s2-4"><title>Conclusions</title><p>These findings are non-trivial for several reasons. First, we have shown that a specific decision variable, computed over an underlying object representation, can be used to make target present/absent judgements, without necessarily knowing the precise features of the target or distractor. Second, we have identified a specific image property that explains why target-absent response times vary so systematically. If target-distractor dissimilarity were the sole determining factor in visual search, it would predict no systematic variation in target-absent searches since the target-distractor dissimilarity is zero. Our results elucidate this puzzle by showing that visual homogeneity varies systematically across images, and that this explains systematic variation in target-absent response times. To the best of our knowledge, our model provides for the first time a unified explanation to explain how target present/absent judgements might be made in a generic visual search task.</p><p>In sum, we conclude that visual homogeneity can explain oddball target present/absent judgements during visual search.</p></sec><sec id="s2-5"><title>Visual homogeneity predicts same/different responses</title><p>We have proposed that visual homogeneity can be used to solve any task that requires discriminating between homogeneous and heterogeneous displays. In Experiments 1–2, we have shown that visual homogeneity predicts target present/absent responses in visual search. We performed an additional experiment to assess whether visual homogeneity can be used to solve an entirely different task, namely a same-different task. In this task, participants have to indicate whether two items are the same or different. We note that instructions to participants for the same/different task (‘you have to indicate if the two items are same or different’) are quite different from the visual search task (‘you have to indicate whether an oddball target is present or absent’). Yet both tasks involve discriminating between homogeneous and heterogeneous displays. We therefore predicted that ‘same’ responses would be correlated with target-absent judgements and ‘different’ responses would be correlated with target-present judgements. Remarkably, this was indeed the case (Appendix 4), demonstrating that same/different responses can also be predicted using visual homogeneity.</p></sec><sec id="s2-6"><title>Visual homogeneity is independent of experimental context</title><p>In the above analyses, visual homogeneity was calculated for each display as its distance from an optimum center in perceptual space. This raises the possibility that visual homogeneity could be modified depending on experimental context since it could depend on the set of objects relative to which the visual homogeneity is computed. We performed a number of experiments to evaluate this possibility: we found that target-absent response times, which index visual homogeneity, are unaffected by a variety of experimental context manipulations (Appendix 5).</p><p>We therefore propose that visual homogeneity is an image-computable property that remains stable across tasks. Moreover, it can be empirically estimated as the reciprocal of the target-absent response times in a visual search task.</p></sec><sec id="s2-7"><title>A localized brain region encodes visual homogeneity (Experiment 2)</title><p>So far, we have found that target present/absent response times had opposite correlations with visual homogeneity (<xref ref-type="fig" rid="fig3">Figure 3F–G</xref>), suggesting that visual homogeneity is a possible decision variable for this task. Therefore, we reasoned that visual homogeneity may be localized to specific brain regions, such as in the visual or prefrontal cortices. Since the task in Experiment 2 was performed by participants inside an MRI scanner, we set out to investigate this issue by analyzing their brain activations.</p><p>We estimated brain activations in each voxel for individual target-present and target-absent search arrays (see Materials and methods). To identify the brain regions whose activations correlated with visual homogeneity, we performed a whole-brain searchlight analysis. For each voxel, we calculated the mean activity in a 3 x 3 × 3 volume centered on that voxel (averaged across voxels and participants) for each present/absent search display, and calculated its correlation with visual homogeneity predictions derived from behavior (see Materials and methods). The resulting map is shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. Visual homogeneity was encoded in a highly localized region just anterior of the lateral occipital (LO) region, with additional weak activations in the parietal and frontal regions. To compare these trends across key visual regions, we calculated the correlation between mean activation and visual homogeneity for each region. This revealed visual homogeneity to be encoded strongly in this region VH, and only weakly in other visual regions (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>A localized brain region encodes visual homogeneity.</title><p>(<bold>A</bold>) Searchlight map showing the correlation between mean activation in each 3 x 3 × 3 voxel neighborhood and visual homogeneity. (<bold>B</bold>) Searchlight map showing the correlation between neural dissimilarity in each 3 x 3 × 3 voxel neighborhood and perceptual dissimilarity measured in visual search. (<bold>C</bold>) Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, lateral occipital (LO) region. The visual homogeneity (VH) region was identified using the searchlight map in Panel A. (<bold>D</bold>) Correlation between the mean activation and visual homogeneity in key visual regions EVC, V4, LO, and VH. Error bars represent standard deviation of the correlation obtained using a boostrap process, by repeatedly sampling participants with replacement for 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p&lt;0.05, ** is p&lt;0.01, **** is p&lt;0.0001). (<bold>E</bold>) Correlation between neural dissimilarity in key visual regions with perceptual dissimilarity. Error bars represent the standard deviation of correlation obtained using a bootstrap process, by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (** is p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-fig4-v1.tif"/></fig><p>To ensure that the high match between visual homogeneity and neural activations in the VH region is not due to an artefact of voxel selection, we performed subject-level analysis (Appendix 6). We repeated the searchlight analysis for each subject and defined VH region for each subject. We find this VH region consistently anterior to the LO region in each subject. Next, we divided participants into two groups, and repeated the brain-wide searchlight analysis. Importantly, the match between mean activation and visual homogeneity remained significant even when the VH region was defined using one group of participants and the correlation was calculated using the mean activations of the other group (Appendix 6).</p><p>To confirm that neural activations in VH region are not driven by other cognitive processes linked to response time, such as attention, we performed a whole-brain searchlight analysis using response times across both target-present and target-absent searches. Proceeding as before, we calculated the correlation between mean activations to the target-present, target-absent and all displays with the respective response times. The resulting maps show that mean activations in the VH region are uncorrelated with response times overall (Appendix 6). By contrast, activations in EVC and LO are negatively correlated with response times, suggesting that faster responses are driven by higher activation of these areas. Finally, mean activation of parietal and prefrontal regions were strongly correlated with response times, consistent with their role in attentional modulation (Appendix 6).</p></sec><sec id="s2-8"><title>Object representations in LO match with visual search dissimilarities</title><p>To investigate the neural space on which visual homogeneity is being computed, we performed a dissimilarity analysis. Since target-absent displays contain multiple instances of a single item, we took the neural response to target-absent displays as a proxy for the response to single items. For each pair of objects, we took the neural activations in a 3 x 3 × 3 neighborhood centered around a given voxel and calculated the Euclidean distance between the two 27-dimensional response vectors (averaged across participants). In this manner, we calculated the neural dissimilarity for all <sup>32</sup>C<sub>2</sub>=496 pairs of objects used in the experiment, and calculated the correlation between the neural dissimilarity in each local neighborhood and the perceptual dissimilarities for the same objects measured using oddball search in Experiment 1. The resulting map is shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. It can be seen that perceptual dissimilarities from visual search are best correlated in the lateral occipital region, consistent with previous studies (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). To compare these trends across key visual regions, we performed this analysis for early visual cortex (EVC), area V4, LO and for the newly identified region VH (average MNI coordinates (x, y, z): (−48,–59, –6) with 111 voxels in the left hemisphere; (49, -56, -7) with 60 voxels in the right hemisphere). Perceptual dissimilarities matched best with neural dissimilarities in LO compared to the other visual regions (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). We conclude that neural representations in LO match with perceptual space. This is concordant with many previous studies (<xref ref-type="bibr" rid="bib15">Haushofer et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Storrs et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Ayzenberg et al., 2022</xref>).</p></sec><sec id="s2-9"><title>Equal weights for target and distractor in target-present array responses</title><p>In the preceding sections, visual homogeneity was calculated using behavioural experiments assuming a neural representation that gives equal weights to the target and distractor. We tested this assumption experimentally by asking whether neural responses to target-present displays can be predicted using the response to the target and distractor items separately. The resulting maps revealed that target-present arrays were accurately predicted as a linear sum of the constituent items, with roughly equal weights for the target and distractor (Appendix 6).</p></sec><sec id="s2-10"><title>Visual homogeneity predicts symmetry perception (Experiments 3-4)</title><p>The preceding sections show that visual homogeneity predicts target present/absent responses as well same/different responses. We have proposed that visual homogeneity can be used to solve any task that involves discriminating homogeneous and heterogeneous displays. In Experiments 3 and 4, we extend the generality of these findings to an entirely different task, namely symmetry perception. Here, asymmetric objects are akin to heterogeneous displays whereas symmetric objects are like homogeneous displays.</p><p>In Experiment 3, we measured perceptual dissimilarities for a set of 64 objects (32 symmetric, 32 asymmetric objects) made from a common set of parts. On each trial, participants viewed a search array with identical items except for one oddball, and had to indicate the side (left/right) on which the oddball appeared using a key press. An example search array is shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. Participants performed searches involving all possible <sup>64</sup>C<sub>2</sub>=2016 pairs of objects. Participants made highly accurate and consistent responses on this task (accuracy, mean ± sd: 98.5 ± 1.33%; correlation between average response times from even- and odd-numbered subjects: <italic>r</italic>=0.88, p&lt;0.0001 across 2016 searches). As before, we took the perceptual dissimilarity between each pair of objects to be the reciprocal of the average response time for displays with either item as target and the other as distractor. To visualize the underlying object representation, we performed a multidimensional scaling analysis, which embeds objects in a multidimensional space such that their pairwise dissimilarities match the experimentally observed dissimilarities. The resulting plot for two dimensions is shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, where nearby objects correspond to similar searches. It can be seen that symmetric objects are generally more spread apart than asymmetric objects, suggesting that visual homogeneity could discriminate between symmetric and asymmetric objects.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Visual homogeneity predicts symmetry perception.</title><p>(<bold>A</bold>) Example search array in Experiment 3. Participants viewed an array containing identical items except for an oddball present either on the left or right side, and had to indicate using a key press which side the oddball appeared. The reciprocal of average search time was taken as the perceptual distance between the target and distractor items. We measured all possible pairwise distances for 64 objects (32 symmetric, 32 asymmetric) in this manner. (<bold>B</bold>) Perceptual space reconstructed using multidimensional scaling performed on the pairwise perceptual dissimilarities. In the resulting plot, nearby objects represent hard searches, and far away objects represent easy searches. Some images are shown at a small size due to space constraints; in the actual experiment, all objects were equated to have the same longer dimension. The correlation on the <italic>top right</italic> indicates the match between the distances in the 2D plot with the observed pairwise distances (**** is p&lt;0.00005). (<bold>C</bold>) Two example displays from Experiment 4. Participants had to indicate whether the object is symmetric or asymmetric using a key press. (<bold>D</bold>) Using the perceptual representation of symmetric and asymmetric objects from Experiment 3, we reasoned that they can be distinguished using their distance to a center in perceptual space. The coordinates of this center are optimized to maximize the match to the observed symmetry detection times. (<bold>E</bold>) Visual homogeneity relative to the optimum center for asymmetric and symmetric objects. Error bar represents s.e.m. across images. Asterisks represent statistical significance (* is p&lt;0.05, unpaired rank-sum test comparing visual homogeneity for 32 symmetric and 32 asymmetric objects). (<bold>F</bold>) Response time for asymmetric objects in Experiment 4 plotted against visual homogeneity calculated from Experiment 3. Asterisks represent statistical significance of the correlation (** is p&lt;0.001). (<bold>G</bold>) Response time for symmetric objects in Experiment 4 plotted against visual homogeneity calculated from Experiment 3. Asterisks represent statistical significance of the correlation (* is p&lt;0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-fig5-v1.tif"/></fig><p>In Experiment 4, we tested this prediction experimentally using a symmetry detection task that was performed by participants inside an MRI scanner. On each trial, participants viewed a briefly presented object, and had to indicate whether the object was symmetric or asymmetric using a key press (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Participants made accurate and consistent responses in this task (accuracy, mean ± sd: 97.7 ± 1.7%; correlation between response times of odd- and even-numbered participants: <italic>r</italic>=0.47, p&lt;0.0001).</p><p>To investigate whether visual homogeneity can be used to predict symmetry judgements, we took the embedding of all objects from Experiment 3, and asked whether there was a center in this multidimensional space such that the distance of each object to this center would be oppositely correlated with response times for symmetric and asymmetric objects (see Materials and methods). The resulting model predictions are shown in <xref ref-type="fig" rid="fig5">Figure 5E–G</xref>. As predicted, visual homogeneity was significantly larger for symmetric compared to asymmetric objects (visual homogeneity, mean ± sd: 0.60±0.24 s<sup>–1</sup> for asymmetric objects; 0.76±0.29 s<sup>–1</sup> for symmetric objects; p&lt;0.05, rank-sum test; <xref ref-type="fig" rid="fig5">Figure 5E</xref>). For asymmetric objects, symmetry detection response times were positively correlated with visual homogeneity (<italic>r</italic>=0.56, p&lt;0.001; <xref ref-type="fig" rid="fig5">Figure 5F</xref>). By contrast, for symmetric objects, response times were negatively correlated with visual homogeneity (<italic>r</italic>=–0.39, p&lt;0.05; <xref ref-type="fig" rid="fig5">Figure 5G</xref>). These patterns are exactly as expected if visual homogeneity was the underlying decision variable for symmetry detection. However, we note that the range of visual homogeneity values for asymmetric and symmetric objects do overlap, suggesting that visual homogeneity contributes but does not fully determine task performance. We therefore propose that visual homogeneity provides a useful and initial first guess at symmetry in an image, which can be refined further through detailed scrutiny.</p><p>To confirm that the above model fits are not due to overfitting, we performed a leave-one-out cross validation analysis, where we left out one object at a time, and then calculated its visual homogeneity. This too yielded similar correlations (<italic>r</italic>=0.44 for asymmetric, <italic>r</italic>=–0.39 for symmetric objects, p&lt;0.05 in both cases).</p><p>In sum, we conclude that visual homogeneity can predict symmetry perception. Taken together, these experiments demonstrate that the same computation (distance from a center) explains two disparate property-based visual tasks: symmetry perception and visual search.</p></sec><sec id="s2-11"><title>Visual homogeneity is encoded by the VH region during symmetry detection</title><p>If visual homogeneity is a decision variable for symmetry detection, it could be localized to specific regions in the brain. To investigate this issue, we analyzed the brain activations of participants in Experiment 4.</p><p>To investigate the neural substrates of visual homogeneity, we performed a searchlight analysis. For each voxel, we calculated the correlation between mean activations in a 3x3 × 3 voxel neighborhood and visual homogeneity. This revealed a localized region in the visual cortex as well as some parietal regions where this correlation attained a maximum (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). This VH region (average MNI coordinates (x, y, z): (−57,–56, –8) with 93 voxels in the left hemisphere; (58, -50, -8) with 73 voxels in the right hemisphere) overlaps with VH region defined during visual search in Experiment 3 (for a detailed comparison, see Appendix 8).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Brain region encoding visual homogeneity during symmetry detection.</title><p>(<bold>A</bold>) Searchlight map showing the correlation between mean activation in each 3x3 × 3 voxel neighborhood and visual homogeneity. (<bold>B</bold>) Searchlight map showing the correlation between neural dissimilarity in each 3x3 × 3 voxel neighborhood and perceptual dissimilarity measured in visual search. (<bold>C</bold>) Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, Lateral occipital (LO) region. The visual homogeneity (VH) region was identified using searchlight map in Panel A. (<bold>D</bold>) Correlation between the mean activation and visual homogeneity in key visual regions EVC, V4, LO, and VH. Error bars represent standard deviation of the correlation obtained using a boostrap process, by repeatedly sampling participants with replacement for 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p&lt;0.05, ** is p&lt;0.01, **** is p&lt;0.0001). (<bold>E</bold>) Correlation between neural dissimilarity in key visual regions with perceptual dissimilarity. Error bars represent the standard deviation of correlation obtained using a bootstrap process, by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (** is p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-fig6-v1.tif"/></fig><p>We note that it is not straightforward to interpret the overlap between the VH regions identified in Experiments 2 and 4. The lack of overlap could be due to stimulus differences (natural images in Experiment 2 vs silhouettes in Experiment 4), visual field differences (items in the periphery in Experiment 2 vs items at the fovea in Experiment 4) and even due to different participants in the two experiments. There is evidence supporting all these possibilities: stimulus differences (<xref ref-type="bibr" rid="bib61">Yue et al., 2014</xref>), visual field differences (<xref ref-type="bibr" rid="bib21">Kravitz et al., 2013</xref>) as well as individual differences can all change the locus of neural activations in object-selective cortex (<xref ref-type="bibr" rid="bib58">Weiner and Grill-Spector, 2012</xref>; <xref ref-type="bibr" rid="bib13">Glezer and Riesenhuber, 2013</xref>). We speculate that testing the same participants on search and symmetry tasks using similar stimuli and display properties would reveal even larger overlap in the VH regions that drive behavior.</p><p>To confirm that neural activations in VH region are not driven by other cognitive processes linked to response time, such as attention, we performed a whole-brain searchlight analysis using response times across both symmetric and asymmetric objects. This revealed that mean activations in the VH region were poorly correlated with response times overall, whereas other parietal and prefrontal regions strongly correlated with response times, consistent with their role in attentional modulation and executive functions (Appendix 7).</p><p>To investigate the perceptual representation that is being used for visual homogeneity computations, we performed a neural dissimilarity analysis. For each pair of objects, we took the neural activations in a 3 x 3 × 3 neighborhood centered around a given voxel and calculated the Euclidean distance between the two 27-dimensional response vectors. In this manner, we calculated the neural dissimilarity for all <sup>64</sup>C<sub>2</sub>=2016 pairs of objects used in the experiment, and calculated the correlation between the neural dissimilarity in each local neighborhood and the perceptual dissimilarities for the same objects measured using oddball search in Experiment 3. The resulting map is shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. The match between neural and perceptual dissimilarity was maximum in the lateral occipital region (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p>To compare these trends for key visual regions, we repeated this analysis for anatomically defined regions of interest in the visual cortex: early visual cortex (EVC), area V4, the lateral occipital (LO) region, and the VH region defined based on the searchlight map in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. These regions are depicted in <xref ref-type="fig" rid="fig6">Figure 6C</xref>. We then asked how mean activations in each of these regions is correlated with visual homogeneity. This revealed that the match with visual homogeneity is best in the VH region compared to the other regions (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). To further confirm that visual homogeneity is encoded in a localized region in the symmetry task, we repeated the searchlight analysis on two independent subgroups of participants. This revealed highly similar regions in both groups (Appendix 7).</p><p>Finally, we compared neural dissimilarities and perceptual dissimilarities in each region as before. This revealed that perceptual dissimilarities (measured from Experiment 3, during visual search) matched best with the LO region (<xref ref-type="fig" rid="fig6">Figure 6E</xref>), suggesting that object representations in LO are the basis for visual homogeneity computations in the VH region.</p><p>In sum, our results suggest that visual homogeneity is encoded by the VH region, using object representations present in the adjoining LO region.</p></sec><sec id="s2-12"><title>Target-absent responses predict symmetry detection</title><p>So far, we have shown that visual homogeneity predicts target present/absent responses in visual search as well as symmetry detection responses. These results suggest a direct empirical link between these two tasks. Specifically, since target-absent response time is inversely correlated with visual homogeneity, we can take its reciprocal as an estimate of visual homogeneity. This in turn predicts opposite correlations between symmetry detection times and reciprocal of target-absent response time: in other words, we should see a positive correlation for asymmetric objects, and a negative correlation for symmetric objects. We confirmed these predictions using additional experiments (Appendix 9). These results reconfirm that a common decision variable, visual homogeneity, drives both target present/absent and symmetry judgements.</p></sec><sec id="s2-13"><title>Visual homogeneity explains animate categorization</title><p>Since visual homogeneity is always calculated relative to a location in perceptual space, we reasoned that shifting this center towards a particular object category would make it a decision variable for object categorization. To test this prediction, we reanalyzed data from a previous study in which participants had to categorize images as belonging to three hierarchical categories: animals, dogs or Labradors (<xref ref-type="bibr" rid="bib25">Mohan and Arun, 2012</xref>). By adjusting the center of the perceptual space measured using visual search, we were able to predict categorization responses for all three categories (Appendix 10). We further reasoned that, if the optimum center for animal/dog/Labrador categorization is close to the default center in perceptual space that predicts target present/absent judgements, then even the default visual homogeneity, as indexed by the reciprocal of target-absent search time, should predict categorization responses. Interestingly, this was indeed the case (Appendix 10). We conclude that, at least for the categories tested, visual homogeneity computations can serve as a decision variable for object categorization.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we investigated three disparate visual tasks: detecting whether an oddball is present in a search array, deciding if two items are same or different, and judging whether an object is symmetric/asymmetric. Although these tasks are superficially different in the way we describe them, our key insight is that they all involve discriminating between homogeneous and heterogeneous displays. We defined a new image property computable from the underlying perceptual representation, namely visual homogeneity, that can be used to solve these tasks. Visual homogeneity predicted response times in all three tasks. Finally, visual homogeneity estimated from behavior was best correlated with mean activations in a region anterior to the lateral occipital cortex. This finding strongly suggests a specific function to this part of the high-level visual cortex. Below we discuss these findings in relation to the existing literature.</p><sec id="s3-1"><title>Visual homogeneity unifies visual search, same-different and symmetry tasks</title><p>Our main finding, that a single decision variable (visual homogeneity) can be used to solve three disparate visual tasks (visual search, same/different and symmetry detection) is novel to the best of our knowledge. This finding is interesting and important because it establishes a close correspondence between all three tasks, and explains some unresolved puzzles in each of these tasks, as detailed below.</p><p>First, with regard to visual search, theoretical accounts of search are based on signal detection theory (<xref ref-type="bibr" rid="bib56">Verghese, 2001</xref>; <xref ref-type="bibr" rid="bib60">Wolfe and Horowitz, 2017</xref>), but define the signal only for specific target-distractor pairs. By contrast, the task of detecting whether an oddball item is present requires a more general decision rule that has not been identified. Our results suggest that visual homogeneity is the underlying decision variable at least for oddball visual search tasks. Of course, visual search is a far more complex process driven by many other factors such as distractor statistics, attentional guidance and familiarity (<xref ref-type="bibr" rid="bib60">Wolfe and Horowitz, 2017</xref>). Our findings suggest that visual homogeneity could also be an additional driving factor for making decisions during visual search. Our findings also offer additional insights into visual search. Target-absent search times have always been noted to vary systematically, but lack a clear explanation in the literature. The slope of target-absent search times as a function of set size are typically twice the slope of target present searches (<xref ref-type="bibr" rid="bib59">Wolfe, 1998</xref>). However this observation is based on averaging across many target-present searches. Since there is only one unique item in a target-absent search array, any systematic variation in target-absent search must be due to an intrinsic image property. Our results resolve this puzzle by showing that this systematic variation is driven by visual homogeneity. Finally, our findings also help explain why we sometimes know a target is present without knowing its exact location – this is because the underlying decision variable, visual homogeneity, arises in high-level visual areas with relatively coarse spatial information, and its computation does not entail knowledge of the oddball’s location. However, we note that visual homogeneity computations described in this study do not completely explain all the variance observed in oddball search times in our study – rather they offer a quantitative model that could explain the initial phase of target selection. We speculate that this initial phase could be shared by all forms of visual search (e.g. searching among non-identical distractors, memory-guided search, conjunction search), and these would be interesting possibilities for future work.</p><p>Second, with regards to same-different tasks, most theoretical accounts use signal detection theory but usually with reference to specific stimulus pairs (<xref ref-type="bibr" rid="bib28">Nickerson, 1969</xref>; <xref ref-type="bibr" rid="bib32">Petrov, 2009</xref>). It has long been observed that ‘different’ responses become faster with increasing target-distractor dissimilarity but this trend logically predicts that ‘same’ responses, which have zero difference, should be the slowest (<xref ref-type="bibr" rid="bib27">Nickerson, 1967</xref>; <xref ref-type="bibr" rid="bib28">Nickerson, 1969</xref>). But in fact, ‘same’ responses are faster than ‘different’ responses. This puzzle has been resolved by assuming a separate decision rule for ‘same’ judgements, making the overall decision process more complex (<xref ref-type="bibr" rid="bib32">Petrov, 2009</xref>; <xref ref-type="bibr" rid="bib14">Goulet, 2020</xref>). Our findings resolve this puzzle by identifying a novel variable, visual homogeneity, which can be used to implement a simple decision rule for making same/different responses. Our findings also explain why some images elicit faster ‘same’ responses than others: this is due to image-to-image differences in visual homogeneity.</p><p>Third, with regard to symmetry detection, most theoretical accounts assume that symmetry is explicitly detected using symmetry detectors along particular axes (<xref ref-type="bibr" rid="bib57">Wagemans, 1997</xref>; <xref ref-type="bibr" rid="bib6">Bertamini and Makin, 2014</xref>). By contrast, our findings indicate an indirect mechanism for symmetry detection that does not invoke any special symmetry computations. We show that visual homogeneity computations can easily discriminate between symmetric and asymmetric objects. This is because symmetric objects have high visual homogeneity since they have repeated parts, whereas asymmetric objects have low visual homogeneity since they have disparate parts (<xref ref-type="bibr" rid="bib35">Pramod and Arun, 2018</xref>). In a recent study, symmetry detection was explained by the average distance of objects relative to other objects (<xref ref-type="bibr" rid="bib35">Pramod and Arun, 2018</xref>). This finding is consistent with ours since visual homogeneity is correlated with the average distance to other objects (Appendix 1). However, there is an important distinction between these two quantities. Visual homogeneity is an intrinsic image property, whereas the average distance of an object to other objects depends on the set of other objects on which the average is being computed. Indeed, we have confirmed through additional experiments that visual homogeneity is independent of experimental context (Appendix 4). We speculate that visual homogeneity can explain many other aspects of symmetry perception, such as the relative strength of symmetries.</p></sec><sec id="s3-2"><title>Visual homogeneity in other visual tasks</title><p>Our finding that visual homogeneity explains property-based visual tasks has several important implications for visual tasks in general. First, we note that visual homogeneity can be easily extended to explain other property-based tasks such as delayed match-to-sample tasks or n-back tasks, by taking the response to the test stimulus as being averaged with the sample-related information in working memory. In such tasks, visual homogeneity will be larger for sequences with repeated compared to non-repeated stimuli, and can easily be used to solve the task. Testing these possibilities will require comparing systematic variations in response times in these tasks across images, and measurements of perceptual space for calculating visual homogeneity.</p><p>Second, we note that visual homogeneity can also be extended to explain object categorization, if one assumes that the center in perceptual space for calculating visual homogeneity can be temporarily shifted to the center of an object category. In such tasks, visual homogeneity relative to the category center will be small for objects belonging to a category and large for objects outside the category, and can be used as a decision variable to solve categorization tasks. This idea is consistent with prevalent accounts of object categorization (<xref ref-type="bibr" rid="bib46">Stewart and Morin, 2007</xref>; <xref ref-type="bibr" rid="bib3">Ashby and Maddox, 2011</xref>; <xref ref-type="bibr" rid="bib25">Mohan and Arun, 2012</xref>). Indeed, categorization response times can be explained using perceptual distances to category and non-category items (<xref ref-type="bibr" rid="bib25">Mohan and Arun, 2012</xref>). By reanalyzing data from this study, we have found that, at least for the animate categories tested, visual homogeneity can explain categorization responses (Appendix 9). However, this remains to be tested in a more general fashion across multiple object categories.</p></sec><sec id="s3-3"><title>Neural encoding of visual homogeneity</title><p>We have found that visual homogeneity is encoded in a specific region of the brain, which we denote as region VH, in both visual search and symmetry detection tasks (<xref ref-type="fig" rid="fig4">Figures 4D</xref> and <xref ref-type="fig" rid="fig5">5D</xref>). This finding is consistent with observations of norm-based encoding in IT neurons (<xref ref-type="bibr" rid="bib23">Leopold et al., 2006</xref>) and in face recognition (<xref ref-type="bibr" rid="bib54">Valentine, 1991</xref>; <xref ref-type="bibr" rid="bib39">Rhodes and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="bib9">Carlin and Kriegeskorte, 2017</xref>). However, our findings are significant because they reveal a dedicated region in high-level visual cortex for solving property-based visual tasks.</p><p>We have found that the VH region is located just anterior to the lateral occipital (LO) region, where neural dissimilarities match closely with perceptual dissimilarities (<xref ref-type="fig" rid="fig4">Figures 4E</xref> and <xref ref-type="fig" rid="fig5">5E</xref>). Based on this proximity, we speculate that visual homogeneity computations are based on object representations in LO. However, confirming this prediction will require fine-grained recordings of neural activity in VH and LO. An interesting possibility for future studies would be to causally perturb brain activity separately in VH or LO using magnetic or electrical stimulation, if at all possible. A simple prediction would be that perturbing LO would distort the underlying representation, whereas perturbing VH would distort the underlying decision process. We caution however that the results might not be so easily interpretable if visual homogeneity computations in VH are based on object representations in LO.</p><p>Recent observations from neural recordings in monkeys suggest that perceptual dissimilarities and visual homogeneity need not be encoded in separate regions. For instance, the overall magnitude of the population neural response of monkey inferior temporal (IT) cortex neurons was found to correlate with memorability (<xref ref-type="bibr" rid="bib17">Jaegle et al., 2019</xref>). These results are consistent with encoding of visual homogeneity in these regions. However, we note that neural responses in IT cortex also predict perceptual dissimilarities (<xref ref-type="bibr" rid="bib29">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="bib45">Sripati and Olson, 2010</xref>; <xref ref-type="bibr" rid="bib62">Zhivago and Arun, 2014</xref>; <xref ref-type="bibr" rid="bib1">Agrawal et al., 2020</xref>). Taken together, these findings suggest that visual homogeneity computations and the underlying perceptual representation could be interleaved within a single neural population, unlike in humans where we found separate regions. Indeed, in our study, the mean activations of the LO region were also correlated with visual homogeneity for symmetry detection (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), but not for target present/absent search (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We speculate that perhaps visual homogeneity might be intermingled into the object representation in monkeys but separated into a dedicated region in humans. These are interesting possibilities for future work.</p><p>Although many previous studies have reported brain activations in the vicinity of the VH region, we are unaware of any study that has ascribed a specific function to this region. The localized activations in our study match closely with the location of the recently reported ventral stream attention module in both humans and monkeys (<xref ref-type="bibr" rid="bib42">Sani et al., 2021</xref>). Previous studies have observed important differences in brain activations in this region, which can be explained using visual homogeneity, as detailed below.</p><p>First, previous studies have observed larger brain activations for animate compared to inanimate objects in high-level visual areas which have typically included the newly defined VH region reported here (<xref ref-type="bibr" rid="bib7">Bracci and Op de Beeck, 2015</xref>; <xref ref-type="bibr" rid="bib37">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Thorat et al., 2019</xref>). In our study, visual homogeneity, as indexed by the reciprocal of target-absent search time, is smaller for animate objects compared to inanimate objects (Appendix 9). Likewise, brain activations were weaker for animate objects compared to inanimate objects in region VH (average VH activations, mean ± sd across participants: 0.50±0.61 for animate target-absent displays, 0.64±0.59 for inanimate target-absent displays, p&lt;0.05, sign-rank test across participants). These discrepancies could be due to differences in stimuli or task demands. Our results do however suggest that visual homogeneity may be an additional organizing factor in human ventral temporal cortex. Reconciling these observations will require controlling animate/inanimate stimuli not only for shape but also for visual homogeneity.</p><p>Second, previous studies have reported larger brain activations for symmetric objects compared to asymmetric objects in the vicinity of this region (<xref ref-type="bibr" rid="bib43">Sasaki et al., 2005</xref>; <xref ref-type="bibr" rid="bib55">Van Meel et al., 2019</xref>). This can be explained by our finding that symmetric objects have larger visual homogeneity (<xref ref-type="fig" rid="fig5">Figure 5E</xref>), leading to activation of the VH region (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). But the increased activations in previous studies were located in the V4 and LO regions, whereas we have found greater activations more anteriorly in the VH region. This difference could be due to the stimulus-related differences: both previous studies used dot patterns, which could appear more object-like when symmetric, leading to more widespread differences in brain activation due to other visual processes like figure-ground segmentation (<xref ref-type="bibr" rid="bib55">Van Meel et al., 2019</xref>). By contrast, both symmetric and asymmetric objects in our study are equally object-like. Resolving these discrepancies will require measuring visual homogeneity as well as behavioural performance during symmetry detection for dot patterns.</p><p>Finally, our results are consistent with the greater activity observed for objects with shared features observed in ventral temporal cortex during naming tasks (<xref ref-type="bibr" rid="bib49">Taylor et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Tyler et al., 2013</xref>). Our study extends these observations by demonstrating an empirical measure for shared feature (target-absent times in visual search) and encoding of this empirical measure into a localized region in object selective cortex across many tasks. We speculate that visual homogeneity may at least partially explain semantic concepts such as those described in these studies.</p></sec><sec id="s3-4"><title>Relation to image memorability and saliency</title><p>We have defined a novel image property, visual homogeneity, which refers to the distance of a visual image to a central point in the underlying perceptual representation. It can be reliably estimated for each image as the inverse of the target-absent response time in a visual search task (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and seems to be an intrinsic image property that is unaffected by the immediate experimental context (Appendix 4).</p><p>At the outset, the way we have defined visual homogeneity suggests that it could be related to other empirically measured quantities such as image memorability, or saliency. It has long been noted that faces that are rated as being distinctive or unusual are also easier to remember (<xref ref-type="bibr" rid="bib26">Murdock, 1960</xref>; <xref ref-type="bibr" rid="bib52">Valentine and Bruce, 1986a</xref>; <xref ref-type="bibr" rid="bib53">Valentine and Bruce, 1986b</xref>; <xref ref-type="bibr" rid="bib54">Valentine, 1991</xref>). Recent studies have elucidated this observation by showing that there are specific image properties that predict image memorability (<xref ref-type="bibr" rid="bib5">Bainbridge et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Lukavský and Děchtěrenko, 2017</xref>; <xref ref-type="bibr" rid="bib41">Rust and Mehrpour, 2020</xref>). However, image memorability, as elegantly summarized in a recent review (<xref ref-type="bibr" rid="bib41">Rust and Mehrpour, 2020</xref>), could be driven by a number of both intrinsic and extrinsic factors. Likewise, saliency is empirically measured as the relative proportion of fixations towards an image, and could be driven by top-down as well as by bottom-up factors (<xref ref-type="bibr" rid="bib11">Eimer, 2014</xref>; <xref ref-type="bibr" rid="bib30">Peelen and Kastner, 2014</xref>). Since visual homogeneity, image memorability and saliency are all empirically measured in different tasks, it would be interesting to compare how they are related on the same set of images.</p></sec><sec id="s3-5"><title>Conclusions</title><p>Taken together, our results show that many property-based visual tasks can be solved using visual homogeneity as a decision variable, which is localized to a specific region anterior to the lateral occipital cortex. While this does not explain all possible variations of these tasks, our study represents an important first step in terms of demonstrating a quantitative, falsifiable model and a localized neural substrate. We propose further that visual homogeneity computations might contribute to a variety of other visual tasks as well, and these would be interesting possibilities for future work.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All participants had a normal or corrected-to-normal vision and gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6–15092017). Participants provided written informed consent before each experiment and were monetarily compensated.</p><sec id="s4-1"><title>Experiment 1. Oddball detection for perceptual space (natural objects)</title><sec id="s4-1-1"><title>Participants</title><p>A total of 16 participants (8 males, 22±2.8 years) participated in this experiment.</p></sec><sec id="s4-1-2"><title>Stimuli</title><p>The stimulus set comprised a set of 32 grayscale natural objects (16 animate, 16 inanimate) presented against a black background.</p></sec><sec id="s4-1-3"><title>Procedure</title><p>Participants performed an oddball detection task with a block of practice trials involving unrelated stimuli followed by the main task. Each trial began with a red fixation cross (diameter 0.5°) for 500ms, followed by a 4x4 search array measuring 30° x 30° for 5 s or until a response was made. The search array always contained one oddball target and 15 identical distractors, with the target appearing equally often on the left or right. A vertical red line divided the screen equally into two halves to facilitate responses. Participants were asked to respond as quickly and as accurately as possible using a key press to indicate the side of the screen containing the target ('Z' for left, M’ for right). Incorrect trials were repeated later after a random number of other trials. Each participant completed 992 correct trials (<sup>32</sup>C<sub>2</sub> object pairs x 2 repetitions with either image as target). The experiment was created using PsychoPy (<xref ref-type="bibr" rid="bib31">Peirce et al., 2019</xref>) and ported to the online platform Pavlovia for collecting data.</p><p>Since stimulus size could vary with the monitor used by the online participants, we equated the stimulus size across participants using the ScreenScale function (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8FHQK">https://doi.org/10.17605/OSF.IO/8FHQK</ext-link>). Each participant adjusted the size of a rectangle on the screen such that its size matched the physical dimensions of a credit card. All the stimuli presented were then scaled with the estimated scaling function to obtain the desired size in degrees of visual angle, assuming an average distance to screen of 60 cm.</p></sec><sec id="s4-1-4"><title>Data analysis</title><p>Response times faster than 0.3 s or slower than 3 s were removed from the data. This step removed only 1.25% of the data and improved the overall response time consistency, but did not qualitatively alter the results.</p></sec><sec id="s4-1-5"><title>Characterizing perceptual space using multidimensional scaling</title><p>To characterize the perceptual space on which present/absent decisions are made, we took the inverse of the average response times (across trials and participants) for each image pair. This inverse of response time (i.e. 1/RT) represents the dissimilarity between the target and distractor (<xref ref-type="bibr" rid="bib2">Arun, 2012</xref>), indexes the underlying salience signal in visual search (<xref ref-type="bibr" rid="bib48">Sunder and Arun, 2016</xref>) and combines linearly across a variety of factors (<xref ref-type="bibr" rid="bib33">Pramod and Arun, 2014</xref>; <xref ref-type="bibr" rid="bib34">Pramod and Arun, 2016</xref>; <xref ref-type="bibr" rid="bib16">Jacob and Arun, 2020</xref>). Since there were 32 natural objects in the experiment and all possible (<sup>32</sup>C<sub>2</sub>=496) pairwise searches in the experiment, we obtained 496 pairwise dissimilarities overall. To calculate target-present and target-absent array responses, we embedded these objects into a multidimensional space using multidimensional scaling analysis (<italic>mdscale</italic> function; MATLAB 2019). This analysis finds the n-dimensional coordinates for each object such that pairwise distances between objects best matches with the experimentally observed pairwise distances. We then treated the activations of objects along each dimension as the responses of a single artificial neuron, so that the response to target-present arrays could be computed as the average of the target and distractor responses.</p></sec></sec><sec id="s4-2"><title>Experiment 2. Target present-absent search during fMRI</title><sec id="s4-2-1"><title>Participants</title><p>A total of 16 subjects (11 males; age, mean ± sd: 25±2.9 years) participated in this experiment. Participants with history of neurological or psychiatric disorders, or with metal implants or claustrophobia were excluded through screening questionnaires.</p></sec><sec id="s4-2-2"><title>Procedure</title><p>Inside the scanner, participants performed a single run of a one-back task for functional localizers (block design, object vs scrambled objects), eight runs of the present-absent search task (event-related design), and an anatomical scan. The experiment was deployed using custom MATLAB scripts written using Psychophysics Toolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>).</p></sec><sec id="s4-2-3"><title>Functional localizer runs</title><p>Participants had to view a series of images against a black background and press a response button whenever an item was repeated. On each trial, 16 images were presented (0.8 s on, 0.2 s off), containing one repeat of an image that could occur at random. Trials were combined into blocks of 16 trials each containing either only objects or only scrambled objects. A single run of the functional localizers contained 12 such blocks (6 object blocks and 6 scrambled-object blocks). Stimuli in each block were chosen randomly from a larger pool of 80 naturalistic objects with the corresponding phase-scrambled objects (created by taking the 2D Fourier transform of each image, randomly shuffling the Fourier phase, and performing the Fourier inverse transform). This is a widely used method for functional localization of object-selective cortex. In practice, however, we observed no qualitative differences in our results upon using voxels activated during these functional localizer runs to further narrow down the voxels selected using anatomical masks. As a result, we did not use the functional localizer data, and all the analyses presented here are based on anatomical masks only.</p></sec><sec id="s4-2-4"><title>Visual search task</title><p>In the present-absent search task, participants reported the presence or absence of an oddball target by pressing one of two buttons using their right hand. The response buttons were fixed for a given participant and counterbalanced across participants. Each search array had eight items, measuring 1.5° along the longer dimension, arranged in a 3x3 grid, with no central element to avoid fixation biases (as shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>). The entire search array measured 6.5°, with an average inter-item spacing of 2.5°. Item positions were jittered randomly on each trial according to a uniform distribution with range ±0.2°. Each trial lasted 4 s (1 s ON time and 3 s OFF time), and participants had to respond within 4 s. Each run had 64 unique searches (32 present, 32 absent) presented in random order, using the natural objects from Experiment 1. Target-present searches were chosen randomly from all possible searches such that all 32 images appeared equally often. Target-absent searches included all 32 objects. The location of the target in the target-present searches was chosen such that all eight locations were sampled equally often. In this manner, participants performed 8 such runs of 64 trials each.</p></sec><sec id="s4-2-5"><title>Data acquisition</title><p>Participants viewed images projected on a screen through a mirror placed above their eyes. Functional MRI (fMRI) data were acquired using a 32-channel head coil on a 3T Skyra (Siemens, Mumbai, India) at the HealthCare Global Hospital, Bengaluru. Functional scans were performed using a T2*-weighted gradient-echo- planar imaging sequence with the following parameters: repetition time (TR)=2 s, echo time (TE)=28ms, flip angle = 79°, voxel size = 3 × 3×3 mm<sup>3</sup>, field of view = 192 × 192 mm<sup>2</sup>, and 33 axial-oblique slices for whole-brain coverage. Anatomical scans were performed using T1-weighted images with the following parameters: TR = 2.30 s, TE = 1.99ms, flip angle = 9°, voxel size = 1 × 1×1 mm<sup>3</sup>, field of view = 256 × 256×176 mm<sup>3</sup>.</p></sec><sec id="s4-2-6"><title>Data preprocessing</title><p>The raw fMRI data were preprocessed using Statistical Parametric Mapping (SPM) software (Version12; Welcome Center for Human Neuroimaging; <ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/software">https://www.fil.ion.ucl.ac.uk/spm/software</ext-link> /spm12/), running on MATLAB 2019b. Raw images were realigned, slice-time corrected, co-registered to the anatomical image, segmented, and normalized to the Montreal Neurological Institute (MNI) 305 anatomical template. Repeating the key analyses with voxel activations estimated from individual subjects yielded qualitatively similar results. Smoothing was performed only on the functional localizer blocks using a Gaussian kernel with a full-width half-maximum of 5 mm. Default SPM parameters were used, and voxel size after normalization was kept at 3×3 × 3 mm<sup>3</sup>. The data were further processed using GLMdenoise (<xref ref-type="bibr" rid="bib19">Kay et al., 2013</xref>). GLMdenoise improves the signal-to-noise ratio in the data by regressing out the noise estimated from task-unrelated voxels. The denoised time-series data were modeled using generalized linear modeling in SPM after removing low-frequency drift using a high-pass filter with a cutoff of 128 s. In the main experiment, the activity of each voxel was modeled using 79 regressors (64 stimuli +1 fixation +6 motion regressors +8 runs). In the localizer block, each voxel was modeled using 10 regressors (2 stimuli +1 fixation +6 motion regressors +1 run).</p></sec><sec id="s4-2-7"><title>ROI definitions</title><p>The regions of interest (ROI) of Early Visual Cortex (EVC) and Lateral Occipital (LO) regions were defined using anatomical masks from the SPM anatomy toolbox (<xref ref-type="bibr" rid="bib10">Eickhoff et al., 2005</xref>). All brain maps were visualized on the inflated brain using Freesurfer (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/">https://surfer.nmr.mgh.harvard.edu/fswiki/</ext-link>).</p></sec><sec id="s4-2-8"><title>Behavioral data analysis</title><p>Response times faster than 0.3 s or slower than 3 s were removed from the data. This step removed only 0.75% of the data and improved the overall response time consistency, but did not qualitatively alter the results.</p></sec></sec><sec id="s4-3"><title>Model fitting for visual homogeneity</title><p>We took the multi-dimensional embedding returned by the perceptual space experiment (Experiment 1) in 5 dimensions as the responses of 5 artificial neurons to the entire set of objects. For each target-present array, we calculated the neural response as the average of the responses elicited by these 5 neurons to the target and distractor items. Likewise, for target-absent search arrays, the neural response was simply the response elicited by these 5 neurons to the distractor item in the search array. To estimate the visual homogeneity of the target-present and target-absent search arrays, we calculated the distance of each of these arrays from a single point in the multidimensional representation. We then calculated the correlation between the visual homogeneity calculated relative to this point and the response times for the target-present and target-absent search arrays. The 5 coordinates of this center point was adjusted using constrained nonlinear optimization to maximize the difference between correlations with the target-present and target-absent response times, respectively. This optimum center remained stable across many random starting points, and our results were qualitatively similar upon varying the number of embedding dimensions.</p><p>Additionally, we performed a leave-one-out cross-validation analysis to validate the number of dimensions or neurons used for the multidimensional scaling analysis in the visual homogeneity model fits. For each choice of number of dimensions, we estimated the optimal centre for visual homogeneity calculations while leaving out all searches involving a single image. We then calculated the visual homogeneity for all the target-present and target-absent searches involving the left-out image. Compiling these predictions by leaving out all images by turn results in a leave-one-out predicted visual homogeneity, which we correlated with the target-present and target-absent response times. We found that the absolute sum of the correlations between visual homogeneity and present/absent reaction times increased monotonically from 1 to 5 neurons, remained at a steady level from 5 to 9 neurons and decreased beyond 9 neurons. Furthermore, the visual homogeneity using the optimal center is highly correlated for 5–9 neurons. We therefore selected 5 neurons or dimensions for reporting visual homogeneity computations.</p></sec><sec id="s4-4"><title>Searchlight maps for mean activation (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig6">Figure 6A</xref>)</title><p>To characterize the brain regions that encode visual homogeneity, we performed a whole-brain searchlight analysis. For each voxel, we took the voxels in a 3x3 × 3 neighborhood and calculated the mean activations across these voxels across all participants. To avoid overall activation level differences between target-present and target-absent searches, we z-scored the mean activations separately across target-present and target-absent searches. Similarly, we calculated the visual homogeneity model predictions from behaviour, and z-scored the visual homogeneity values for target-present and target-absent searches separately. We then calculated the correlation between the normalized mean activations and the normalized visual homogeneity for each voxel, and displayed this as a colormap on the inflated MNI brain template in <xref ref-type="fig" rid="fig3">Figures 3A</xref> and <xref ref-type="fig" rid="fig5">5A</xref>.</p><p>Note that the z-scoring of mean activations and visual homogeneity removes any artefactual correlation between mean activation and visual homogeneity arising simply due to overall level differences in mean activation or visual homogeneity itself, but does not alter the overall positive correlation between the visual homogeneity and mean activation across individual search displays.</p></sec><sec id="s4-5"><title>Searchlight maps for neural and behavioural dissimilarity (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig6">Figure 6B</xref>)</title><p>To characterize the brain regions that encode perceptual dissimilarity, we performed a whole-brain searchlight analysis. For each voxel, we took the voxel activations in a 3x3 × 3 neighborhood to target-absent displays as a proxy for the neural response to the single image. For each image pair, we calculated the pair-wise Euclidean distance between the 27-dimensional voxel activations evoked by the two images, and averaged this distance across participants to get a single average distance. For 32 target-absent displays in the experiment, taking all possible pairwise distances results in <sup>32</sup>C<sub>2</sub>=496 pairwise distances. Similarly, we obtained the same 496 pairwise perceptual dissimilarities between these items from the oddball detection task (Experiment 1). We then calculated the correlation between the mean neural dissimilarities at each voxel with perceptual dissimilarities, and displayed this as a colormap on the flattened MNI brain template in <xref ref-type="fig" rid="fig3">Figures 3B</xref> and <xref ref-type="fig" rid="fig5">5B</xref>.</p></sec><sec id="s4-6"><title>Experiment 3. Oddball detection for perceptual space (Symmetric/Asymmetric objects)</title><sec id="s4-6-1"><title>Participants</title><p>A total of 15 participants (11 males, 22.8±4.3 years) participated in this experiment.</p></sec><sec id="s4-6-2"><title>Paradigm</title><p>Participants performed an oddball visual search task. Participants completed 4032 correct trials (<sup>64</sup>C<sub>2</sub> shape pairs x 2 repetitions) as two sessions in 2 days. We used a total of 64 baton shapes (32 symmetric and 32 asymmetric), and all shapes were presented against a black background. We created 32 unique parts with the vertical line as part of the contour. We created 32 symmetric by joining the part and its mirror-filled version, and 32 asymmetric objects were created by randomly pairing the left part and mirror flipped version of another left part. All parts were occurring equally likely. All other task details are the same as Experiment 1.</p></sec></sec><sec id="s4-7"><title>Experiment 4. Symmetry judgment task (fMRI and behavior)</title><sec id="s4-7-1"><title>Participants</title><p>A total of 15 subjects participated in this study. Participants had normal or corrected to normal vision. Participants had no history of neurological or psychiatric impairment. We excluded participants with metal implants or claustrophobia from the study.</p></sec><sec id="s4-7-2"><title>Paradigm</title><p>Inside the scanner, participants performed two runs of one-back identity task (functional localizer), eight runs of symmetry judgment task (event-related design), and one anatomical scan. We excluded the data from one participant due to poor accuracy and long response times.</p></sec><sec id="s4-7-3"><title>Symmetry task</title><p>On each trial, participants had to report whether a briefly presented object was symmetric or not using a keypress. Objects measured 4° and were presented against a black background. Response keys were counterbalanced across participants. Each trial lasted 4 s, with the object displayed for 200ms followed by a blank period of 3800ms. Participants could respond at any time following appearance of the object, up to a time out of 4 s after which the next trial began. Each run had 64 unique conditions (32 symmetric and 32 asymmetric).</p></sec><sec id="s4-7-4"><title>1-back task for functional localizers</title><p>Stimuli were presented as blocks, and participants reported repeated stimuli with a keypress. Each run had blocks of either silhouettes (asymmetric/symmetric), dot patterns (asymmetric/symmetric), combination of baton and dot patterns (asymmetric/symmetric) and natural objects (intact/scrambled).</p></sec></sec><sec id="s4-8"><title>Data analysis</title><sec id="s4-8-1"><title>Noise removal in RT</title><p>Very fast (&lt;100ms) reaction times were removed. We also discarded all reaction times to an object if participant’s accuracy was less than 80%. This process removed 3.6% of RT data.</p></sec><sec id="s4-8-2"><title>Model fitting for visual homogeneity</title><p>We proceeded as before by embedding the oddball detection response times into multidimensional space with three dimensions. For each image, the visual homogeneity was defined as its distance from an optimum center. We then calculated the correlation between the visual homogeneity calculated relative to this optimum center and the response times for the target-present and target-absent search arrays separately. This optimum center was estimated using a constrained nonlinear optimization to maximize the difference between the correlations for asymmetric object response times and symmetric object response times. Other details were the same as in Experiment-2.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6-15092017).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93033-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and code required to reproduce the results are publicly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/cvzxt/">https://osf.io/cvzxt/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>G</given-names></name><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Jacob2025 Visual homogeneity</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/CVZXT</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Divya Gulati for help with data collection of Experiments S4 &amp; S5. This research was supported by the DBT/Wellcome Trust India Alliance Senior Fellowship awarded to SPA (Grant# IA/S/17/1/503081). GJ was supported by a Senior Research Fellowship from MHRD, Government of India.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title><source>eLife</source><volume>9</volume><elocation-id>e54846</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54846</pub-id><pub-id pub-id-type="pmid">32369017</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Turning visual search time on its head</article-title><source>Vision Research</source><volume>74</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.04.005</pub-id><pub-id pub-id-type="pmid">22561524</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Human category learning 2.0</article-title><source>Annals of the New York Academy of Sciences</source><volume>1224</volume><fpage>147</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2010.05874.x</pub-id><pub-id pub-id-type="pmid">21182535</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Kamps</surname><given-names>FS</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Skeletal representations of shape in the human visual cortex</article-title><source>Neuropsychologia</source><volume>164</volume><elocation-id>108092</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2021.108092</pub-id><pub-id pub-id-type="pmid">34801519</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bainbridge</surname><given-names>WA</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Memorability: A stimulus-driven perceptual neural signature distinctive from memory</article-title><source>NeuroImage</source><volume>149</volume><fpage>141</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.01.063</pub-id><pub-id pub-id-type="pmid">28132932</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertamini</surname><given-names>M</given-names></name><name><surname>Makin</surname><given-names>ADJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Brain activity in response to visual symmetry</article-title><source>Symmetry</source><volume>6</volume><fpage>975</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.3390/sym6040975</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>1120</elocation-id><pub-id pub-id-type="doi">10.1167/15.12.1120</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlin</surname><given-names>JD</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adjudicating between face-coding models with individual-face fMRI responses</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005604</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005604</pub-id><pub-id pub-id-type="pmid">28746335</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Grefkes</surname><given-names>C</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title><source>NeuroImage</source><volume>25</volume><fpage>1325</fpage><lpage>1335</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.034</pub-id><pub-id pub-id-type="pmid">15850749</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neural basis of attentional control in visual search</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>526</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.05.005</pub-id><pub-id pub-id-type="pmid">24930047</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleuret</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Dubout</surname><given-names>C</given-names></name><name><surname>Wampler</surname><given-names>EK</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name><name><surname>Geman</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Comparing machines and humans on a visual categorization test</article-title><source>PNAS</source><volume>108</volume><fpage>17621</fpage><lpage>17625</lpage><pub-id pub-id-type="doi">10.1073/pnas.1109168108</pub-id><pub-id pub-id-type="pmid">22006295</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glezer</surname><given-names>LS</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Individual variability in location impacts orthographic selectivity in the “visual word form area”</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>11221</fpage><lpage>11226</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5002-12.2013</pub-id><pub-id pub-id-type="pmid">23825425</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Goulet</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Investigation of the cognitive mechanisms of same and different judgments</article-title><ext-link ext-link-type="uri" xlink:href="https://ruor.uottawa.ca/items/4d42ed19-21e7-442e-a2f7-8463871df931">https://ruor.uottawa.ca/items/4d42ed19-21e7-442e-a2f7-8463871df931</ext-link><date-in-citation iso-8601-date="2022-12-01">December 1, 2022</date-in-citation></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haushofer</surname><given-names>J</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multivariate patterns in object-selective cortex dissociate perceptual and physical shape similarity</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e187</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060187</pub-id><pub-id pub-id-type="pmid">18666833</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>G</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How the forest interacts with the trees: Multiscale shape integration explains global and local processing</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.10.20</pub-id><pub-id pub-id-type="pmid">33107916</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaegle</surname><given-names>A</given-names></name><name><surname>Mehrpour</surname><given-names>V</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Rust</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Population response magnitude variation in inferotemporal cortex predicts image memorability</article-title><source>eLife</source><volume>8</volume><elocation-id>e47596</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47596</pub-id><pub-id pub-id-type="pmid">31464687</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katti</surname><given-names>H</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A separable neural code in monkey IT enables perfect CAPTCHA decoding</article-title><source>Journal of Neurophysiology</source><volume>127</volume><fpage>869</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.1152/jn.00160.2021</pub-id><pub-id pub-id-type="pmid">35196158</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Dougherty</surname><given-names>RF</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>GLMdenoise: a fast, automated technique for denoising task-based fMRI data</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>247</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00247</pub-id><pub-id pub-id-type="pmid">24381539</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Ricci</surname><given-names>M</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Not-So-CLEVR: learning same-different relations strains feedforward neural networks</article-title><source>Interface Focus</source><volume>8</volume><elocation-id>20180011</elocation-id><pub-id pub-id-type="doi">10.1098/rsfs.2018.0011</pub-id><pub-id pub-id-type="pmid">29951191</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.011</pub-id><pub-id pub-id-type="pmid">23265839</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Bondar</surname><given-names>IV</given-names></name><name><surname>Giese</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title><source>Nature</source><volume>442</volume><fpage>572</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1038/nature04951</pub-id><pub-id pub-id-type="pmid">16862123</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukavský</surname><given-names>J</given-names></name><name><surname>Děchtěrenko</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual properties and memorising scenes: Effects of image-space sparseness and uniformity</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>79</volume><fpage>2044</fpage><lpage>2054</lpage><pub-id pub-id-type="doi">10.3758/s13414-017-1375-9</pub-id><pub-id pub-id-type="pmid">28707123</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohan</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Similarity relations in visual search predict rapid visual categorization</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1167/12.11.19</pub-id><pub-id pub-id-type="pmid">23092947</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murdock</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>The distinctiveness of stimuli</article-title><source>Psychological Review</source><volume>67</volume><fpage>16</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1037/h0042382</pub-id><pub-id pub-id-type="pmid">14425343</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nickerson</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>“Same”-&quot;different&quot; response times with multi-attribute stimulus differences</article-title><source>Perceptual and Motor Skills</source><volume>24</volume><fpage>543</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.2466/pms.1967.24.2.543</pub-id><pub-id pub-id-type="pmid">6040229</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nickerson</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>‘Same’-‘different’ response times: A model and A preliminary test</article-title><source>Acta Psychologica</source><volume>30</volume><fpage>257</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(69)90054-7</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Inferotemporal neurons represent low-dimensional configurations of parameterized shapes</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>1244</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1038/nn767</pub-id><pub-id pub-id-type="pmid">11713468</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attention in the real world: toward understanding its neural basis</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>242</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.02.004</pub-id><pub-id pub-id-type="pmid">24630872</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrov</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Symmetry-based methodology for decision-rule identification in same--different experiments</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>1011</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.6.1011</pub-id><pub-id pub-id-type="pmid">19966250</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Features in visual search combine linearly</article-title><source>Journal of Vision</source><volume>14</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1167/14.4.6</pub-id><pub-id pub-id-type="pmid">24715328</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Object attributes combine additively in visual search</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/16.5.8</pub-id><pub-id pub-id-type="pmid">26967014</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Symmetric objects become special in perception because of generic computations in neurons</article-title><source>Psychological Science</source><volume>29</volume><fpage>95</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1177/0956797617729808</pub-id><pub-id pub-id-type="pmid">29219748</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Improving machine vision using human perceptual representations: the case of planar reflection symmetry for object classification</article-title><source>IEEE Trans Pattern Anal</source><volume>44</volume><fpage>228</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3008107</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: the animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puebla</surname><given-names>G</given-names></name><name><surname>Bowers</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Can deep convolutional neural networks support relational reasoning in the same-different task?</article-title><source>Journal of Vision</source><volume>22</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1167/jov.22.10.11</pub-id><pub-id pub-id-type="pmid">36094524</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G</given-names></name><name><surname>Jeffery</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Adaptive norm-based coding of facial identity</article-title><source>Vision Research</source><volume>46</volume><fpage>2977</fpage><lpage>2987</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.03.002</pub-id><pub-id pub-id-type="pmid">16647736</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricci</surname><given-names>M</given-names></name><name><surname>Cadène</surname><given-names>R</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Same-different conceptualization: a machine vision perspective</article-title><source>Current Opinion in Behavioral Sciences</source><volume>37</volume><fpage>47</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.08.008</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Mehrpour</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Understanding image memorability</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>557</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.04.001</pub-id><pub-id pub-id-type="pmid">32386889</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sani</surname><given-names>I</given-names></name><name><surname>Stemmann</surname><given-names>H</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Bullock</surname><given-names>D</given-names></name><name><surname>Stemmler</surname><given-names>T</given-names></name><name><surname>Fahle</surname><given-names>M</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The human endogenous attentional control network includes a ventro-temporal cortical node</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>360</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-20583-5</pub-id><pub-id pub-id-type="pmid">33452252</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>Y</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Knutsen</surname><given-names>T</given-names></name><name><surname>Tyler</surname><given-names>C</given-names></name><name><surname>Tootell</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Symmetry activates extrastriate visual cortex in human and nonhuman primates</article-title><source>PNAS</source><volume>102</volume><fpage>3159</fpage><lpage>3163</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500319102</pub-id><pub-id pub-id-type="pmid">15710884</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning: the good, the bad, and the ugly</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>399</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014951</pub-id><pub-id pub-id-type="pmid">31394043</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sripati</surname><given-names>AP</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Global image dissimilarity in macaque inferotemporal cortex predicts human visual search efficiency</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>1258</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1908-09.2010</pub-id><pub-id pub-id-type="pmid">20107054</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>N</given-names></name><name><surname>Morin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dissimilarity is used as evidence of category membership in multidimensional perceptual categorization: a test of the similarity–dissimilarity generalized context model</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>60</volume><fpage>1337</fpage><lpage>1346</lpage><pub-id pub-id-type="doi">10.1080/17470210701480444</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Diverse deep neural networks all predict human inferior temporal cortex well, after training and fitting</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2044</fpage><lpage>2064</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01755</pub-id><pub-id pub-id-type="pmid">34272948</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sunder</surname><given-names>S</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Look before you seek: Preview adds a fixed benefit to all searches</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/16.15.3</pub-id><pub-id pub-id-type="pmid">27919099</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>KI</given-names></name><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Acres</surname><given-names>K</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Contrasting effects of feature-based statistics on the categorisation and basic-level identification of visual objects</article-title><source>Cognition</source><volume>122</volume><fpage>363</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2011.11.001</pub-id><pub-id pub-id-type="pmid">22137770</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e47142</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47142</pub-id><pub-id pub-id-type="pmid">31496518</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Chiu</surname><given-names>S</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Wright</surname><given-names>P</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Objects and categories: feature statistics and object processing in the ventral stream</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1723</fpage><lpage>1735</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00419</pub-id><pub-id pub-id-type="pmid">23662861</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T</given-names></name><name><surname>Bruce</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1986">1986a</year><article-title>Recognizing familiar faces: The role of distinctiveness and familiarity</article-title><source>Canadian Journal of Psychology / Revue Canadienne de Psychologie</source><volume>40</volume><fpage>300</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1037/h0080101</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T</given-names></name><name><surname>Bruce</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1986">1986b</year><article-title>The effects of distinctiveness in recognising and classifying faces</article-title><source>Perception</source><volume>15</volume><fpage>525</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1068/p150525</pub-id><pub-id pub-id-type="pmid">3588212</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A unified account of the effects of distinctiveness, inversion, and race in face recognition</article-title><source>The Quarterly Journal of Experimental Psychology Section A</source><volume>43</volume><fpage>161</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1080/14640749108400966</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Meel</surname><given-names>C</given-names></name><name><surname>Baeck</surname><given-names>A</given-names></name><name><surname>Gillebert</surname><given-names>CR</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The representation of symmetry in multi-voxel response patterns and functional connectivity throughout the ventral visual stream</article-title><source>NeuroImage</source><volume>191</volume><fpage>216</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.030</pub-id><pub-id pub-id-type="pmid">30771448</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verghese</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual search and attention: a signal detection theory approach</article-title><source>Neuron</source><volume>31</volume><fpage>523</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00392-0</pub-id><pub-id pub-id-type="pmid">11545712</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Characteristics and models of human symmetry detection</article-title><source>Trends in Cognitive Sciences</source><volume>1</volume><fpage>346</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(97)01105-4</pub-id><pub-id pub-id-type="pmid">21223945</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The improbable simplicity of the fusiform face area</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>251</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.03.003</pub-id><pub-id pub-id-type="pmid">22481071</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>What can 1 million trials tell us about visual search?</article-title><source>Psychological Science</source><volume>9</volume><fpage>33</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00006</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name><name><surname>Horowitz</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Five factors that guide attention in visual search</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0058</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-017-0058</pub-id><pub-id pub-id-type="pmid">36711068</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Pourladian</surname><given-names>IS</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Curvature-processing network in macaque visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>E3467</fpage><lpage>E3475</lpage><pub-id pub-id-type="doi">10.1073/pnas.1412616111</pub-id><pub-id pub-id-type="pmid">25092328</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhivago</surname><given-names>KA</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Texture discriminability in monkey inferotemporal cortex predicts human texture perception</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>2745</fpage><lpage>2755</lpage><pub-id pub-id-type="doi">10.1152/jn.00532.2014</pub-id><pub-id pub-id-type="pmid">25210165</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiple object response normalization in monkey inferotemporal cortex</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>8150</fpage><lpage>8164</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2058-05.2005</pub-id><pub-id pub-id-type="pmid">16148223</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Visual homogeneity in deep networks explains search</title><sec sec-type="appendix" id="s8-1"><title>Introduction</title><p>In the main text, we reconstructed the underlying object representation using oddball search in Experiment 1, and used this representation to compute visual homogeneity, and predict target present/absent searches in Expeirment 2. At first glance, this result could appear circular since we are using search data from Experiment 1 to predict search data in Experiment 2. While this is highly unlikely since we are using search dissimilarities in Experiment 1 only to reconstruct the underlying object representations, we nonetheless reasoned that this concern can also be addressed by computing visual homogeneity on any brain-like object representation.</p><p>To this end, we repeated the visual homogeneity predictions of Experiment 2 using object representations derived from deep networks whose representations are known to be brain-like.</p></sec><sec sec-type="appendix" id="s8-2"><title>Methods</title><p>We used a ResNet-50 architecture trained for object classification on the ImageNet dataset. First, we identified the layer that best matches the human representation by comparing previously observed perceptual distances (<xref ref-type="bibr" rid="bib36">Pramod and Arun, 2020</xref>) with distances predicted by the ResNet-50 model. We then obtained the ResNet-50 representation of stimuli used in Experiment 1 specifically from layer3.5conv2. This higher dimensional stimulus representation was reduced to lower dimension using PCA. Using this reduced dimensional representation, we fitted the visual homogeneity model as before.</p></sec><sec sec-type="appendix" id="s8-3"><title>Results</title><p>To identify the layer in the ResNet-50 model that best aligns with human perceptual representation, we compared observed perceptual distances from 32 experiments in a previous study with distances predicted by each layer of the ResNet-50 model. The representations from the convolutional layer (layer3.5conv2, layer 134) demonstrated the closest match to human perceptual distances (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). Specifically, this selected layer of the ResNet-50 model predicted the observed perceptual distances for Experiment-1 (<italic>r</italic>=0.66, p&lt;0.0001 across all <sup>32</sup>C<sub>2</sub>=496 object pairs).</p><p>The visual homogeneity, computed from lower-dimensional ResNet-50 model embeddings, of target-present arrays was significantly smaller than that of target-absent arrays (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). The model’s predictions are depicted in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C and D</xref>. Response times for target-present searches were positively correlated with visual homogeneity (<italic>r</italic>=0.42, p&lt;0.05; <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>), while response times for target-absent searches were negatively correlated with visual homogeneity (<italic>r</italic>=0.56, p&lt;0.0001 across all <sup>32</sup>C<sub>2</sub>=496 object pairs). Additionally, The visual homogeneity computed using ResNet-50 is highly correlated with visual homogeneity computed using Experiments 1&amp;2 (<italic>r</italic>=0.76, p&lt;0.0001).</p><p>Hence, we demonstrate that our model can predict target-present/absent search responses based on any technique to estimate the representational space, and it is not dependent on the odd-ball visual search experiment.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Visual homogeneity in deep networks predicts oddball search.</title><p>(<bold>A</bold>) Correlation between perceptual dissimilarities and deep network dissimilarities across 32 oddball search experiments shown for each layer of ResNet-50 (median ±sem calculated across experiments). The layer with highest median correlation (<italic>r</italic>=0.46) is marked with a <italic>blue dashed line</italic>. This layer is taken for further analyses. (<bold>B</bold>) Predicted visual homogeneity (calculated from ResNet-50 layer 134) for target-present and target-absent searches. Error bars represent s.e.m across all displays. Asterisks represent statistical significance (**** is p&lt;0.00005, unpaired rank-sum test comparing visual homogeneity for 32 target-absent and 32 target-present arrays). (<bold>C</bold>) Observed response time for target-present searches in Experiment 2 plotted against visual homogeneity calculated from ResNet-50 layer 134. Asterisks represent statistical significance of the correlation (**** is p&lt;0.00005). Note that a single model is fit to find the optimum center that predicts the response times for both target-present and target-absent searches. (<bold>D</bold>) Same as (<bold>C</bold>) but for target-absent searches.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app1-fig1-v1.tif"/></fig></sec></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Additional analysis for Experiment 1</title><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Additional analysis for Experiment 1.</title><p>(<bold>A</bold>) Reaction times of target-absent searches (Experiment 2) plotted against the average dissimilarity to all other objects (Experiment 1). (<bold>B</bold>) Visual homogeneity for each object plotted against the average distance of each object to all other objects, suggesting that visual homogeneity is closely related to the average distance of an object to all others. (<bold>C</bold>) Correlation between predicted and observed target-present RT as a function of the weight of target relative to distractors in the search array. The analysis in the main text assumes that the responses to target-present arrays are an average of the target and distractor responses. To validate this assumption, we repeated the analysis by assuming taking target-present array response to be <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the response to the target-present array, <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the responses to the target and distractor, and <inline-formula><mml:math id="inf5"><mml:mi>w</mml:mi></mml:math></inline-formula> represents the weight of the target relative to the distractor. If w=0, it means that the target does not contribute to the overall response, and w=1 implies that the target dominates the overall response. In this plot, for each value of w, we optimized the coordinates of the center to best match the data, and plotted the correlation between predicted and observed target-present RT. It can be seen that roughly equal weighting (w=0.52) of the target and distractor yields the best fit to the data. The gray bar represent the range of weights for which the correlation is statistically significant (p&lt;0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app2-fig1-v1.tif"/></fig></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Generalization to other objects</title><sec sec-type="appendix" id="s10-1"><title>Introduction</title><p>In the main text, using Experiments 1 and 2, we found that visual homogeneity predicts target present/absent responses, but these results were based on testing 32 natural objects. Here, we performed four additional experiments to confirm the generality of these findings. In Experiments S1 and S2, we tested 48 natural objects. In Experiments S3 and S4, we tested 49 silhouette shapes.</p></sec><sec sec-type="appendix" id="s10-2"><title>Methods</title><p>All participants had a normal or corrected-to-normal vision and gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6–15092017). Participants provided written informed consent before each experiment and were monetarily compensated.</p></sec></sec><sec sec-type="appendix" id="s11"><title>Experiment S1: Odd ball detection for perceptual space (natural objects)</title><p>This data is Experiment 1 of a previously published study (<xref ref-type="bibr" rid="bib25">Mohan and Arun, 2012</xref>), and the salient details are reproduced here. The analysis of visual homogeneity is novel and unique to this study.</p><sec sec-type="appendix" id="s11-1"><title>Participants</title><p>A total of 12 participants aged 20–30 years participated in this experiment.</p></sec><sec sec-type="appendix" id="s11-2"><title>Stimuli</title><p>The stimulus set comprised a set of 48 grayscale natural objects (24 animate, 24 inanimate) presented against a black background. Images were equated for brightness. The longer dimension was presented at a visual angle of 4.8<sup>0</sup>.</p></sec><sec sec-type="appendix" id="s11-3"><title>Procedure</title><p>Each participant performed a visual search task, in which a 4x4 search array was presented with one oddball. Participants had to indicate whether the oddball was on the right or left of the screen using a key press (‘Z’ for left, ‘M’ for right). Distractors were varied randomly in size to prevent low-level cues from influencing search.</p></sec><sec sec-type="appendix" id="s11-4"><title>Data analysis</title><p>Response times faster than 0.3 seconds or slower than 4 seconds were removed from the data. This step removed only 0.76% of the data and improved the overall response time consistency but did not qualitatively alter the results.</p></sec></sec><sec sec-type="appendix" id="s12"><title>Experiment S2: Target-present absent search with natural objects</title><sec sec-type="appendix" id="s12-1"><title>Participants</title><p>A total of 16 participants (9 males, 23.31±5.24 years) participated in this experiment.</p></sec><sec sec-type="appendix" id="s12-2"><title>Stimuli</title><p>The stimuli were identical to those used in Experiment S1.</p></sec><sec sec-type="appendix" id="s12-3"><title>Procedure</title><p>Participants performed a present-absent visual search task. Each trial began by presenting a red-coloured fixation cross of size 0.5 dva for 500ms, followed by the presentations of a circular search array measuring 13°x13° for 10 s or until a response, whichever was sooner. The search array contained eight elements, of which one could be different from the others. Participants identified and reported the presence of the target with a keypress (‘A’ for target absence and ‘P’ for target presence). Participants were instructed to respond as quickly and as accurately as possible. Incorrect trials were randomly repeated later.</p><p>Each participant completed 384 correct trials (absent trials: 48 shapes x 4 repetitions = 192, present trials: 24 image pairs x 2 (AB/BA) x 4 repetitions = 192). We used a total of 48 natural images from animate and inanimate categories. All images were presented against a black background. The main experiment block explained above was preceded by an identical practice experiment block of 20 trials involving unrelated stimuli. This experiment was hosted at the Pavlovia platform (<ext-link ext-link-type="uri" xlink:href="https://pavlovia.org/">https://pavlovia.org/</ext-link>) using custom programs written in PyschoPy (<xref ref-type="bibr" rid="bib31">Peirce et al., 2019</xref>).</p></sec><sec sec-type="appendix" id="s12-4"><title>Data analysis</title><p>Image Pairs with an overall accuracy of less than 50% were removed. One target-present search was removed based on this criterion from further analysis.</p></sec></sec><sec sec-type="appendix" id="s13"><title>Experiment S3: Odd ball detection for perceptual space (Silhouettes)</title><p>This data is from Experiment 1 of a previously published study (<xref ref-type="bibr" rid="bib34">Pramod and Arun, 2016</xref>), and the salient details are reproduced here. The analysis of visual homogeneity is novel and unique to this study.</p><sec sec-type="appendix" id="s13-1"><title>Participants</title><p>A total of 8 participants (5 females, aged 20–30 years) participated in this experiment.</p></sec><sec sec-type="appendix" id="s13-2"><title>Stimuli</title><p>Each stimulus was created by joining two of the seven possible parts. The full set consisted of 49 objects formed by all possible combinations of seven parts at two locations.</p></sec><sec sec-type="appendix" id="s13-3"><title>Procedure</title><p>Participants were seated approximately 60 cm from the computer monitor that was under the control of a custom program written using Psychtoolbox in Matlab. Each trial began by presenting a fixation cross for 500ms, followed by a 4x4 search array containing one oddball item among multiple identical distractors with a red vertical line down the middle. Items were jittered. Participants were asked to report the side on which the oddball target appeared as quickly and as accurately as possible using a keypress (Z for left, M for right). All images were presented against a black background.</p></sec><sec sec-type="appendix" id="s13-4"><title>Data analysis</title><p>Response times faster than 0.3 s or slower than 4.5 s were removed from the data. This step removed 1.2% of the data and improved the overall response time consistency but did not qualitatively alter the results.</p></sec></sec><sec sec-type="appendix" id="s14"><title>Experiment S4: Target-present absent search with silhouettes</title><sec sec-type="appendix" id="s14-1"><title>Participants</title><p>A total of 11 participants (6 females, aged 20–30 years) participated in this experiment.</p></sec><sec sec-type="appendix" id="s14-2"><title>Stimuli</title><p>Stimuli were identical to Experiment S3.</p></sec><sec sec-type="appendix" id="s14-3"><title>Procedure</title><p>Participants were seated approximately 60 cm from the computer monitor that was under the control of a custom program written using Psychtoolbox in Matlab. Each trial began by presenting a fixation cross for 500ms, followed by a 4x4 search array containing one oddball item among multiple identical distractors. Items were jittered. Participants were asked to report the presence of an oddball target as quickly and as accurately as possible using keypress (Y for the presence of the target, N for the absence of the target). All images were presented against a black background.</p></sec><sec sec-type="appendix" id="s14-4"><title>Data analysis</title><p>Response times faster than 0.3 s or slower than 5 s were removed from the data. This step removed 7% of the data and improved the overall response time consistency but did not qualitatively alter the results. We predicted the response times of target-present and target-absent search display the same approach used for experiment 2 (see main text). We used the MDS embeddings from experiment S3, and the model has 7 free parameters. To avoid overfitting, we used leave-one-out cross-validation.</p></sec><sec sec-type="appendix" id="s14-5"><title>Results</title><p>In Experiment S1, we characterized the perceptual space of 48 natural objects using a left/right oddball detection task as before. Participants were highly consistent in their responses (correlation between mean response times of even- and odd-numbered participants: <italic>r</italic>=0.90, p&lt;0.0001 across all <sup>48</sup>C<sub>2</sub>=1128 object pairs). In Experiment S2, we checked if visual homogeneity predicted target present/absent responses in visual search. Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 96 ± 1.8%; correlation between mean response times of even- and odd-numbered participants: <italic>r</italic>=0.94, p&lt;0.0001 across 48 target-present searches, <italic>r</italic>=0.93, p&lt;0.0001 across 48 target-absent searches). We estimated the visual homogeneity using the RT data from Experiment S1&amp;2 by fitting model with seven free parameters. The model predictions are shown in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1A</xref>. As expected, the response times of present searches were positively correlated with the visual homogeneity (<italic>r</italic>=0.44, p&lt;0.05),and the response times of absent searches were negatively correlated with visual homogeneity (<italic>r</italic>=–0.72, p&lt;0.0001).</p><p>In Experiment S3, we characterized the perceptual space of 49 silhouette objects using a left/right oddball detection task as before. Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 99 ± 0.5%; correlation between mean response times of even- and odd-numbered participants: <italic>r</italic>=0.80, p&lt;0.0001 across all <sup>49</sup>C<sub>2</sub>=1176 object pairs). In Experiment S4, we checked if visual homogenity predicted target present/absent responses in visual search. Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 92.74 ± 4.27%; correlation between mean response times of even- and odd-numbered participants: <italic>r</italic>=0.80, p&lt;0.0001 across 48 target-present searches, <italic>r</italic>=0.67, p&lt;0.0001 across 49 target-absent searches). We estimated the visual homogeneity using the RT data from the Experiment S3 and 4 by fitting a model with seven free parameters. The model predictions are shown in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1B</xref>. As expected, the response times of present searches were positively correlated with the visual homogeneity (<italic>r</italic>=0.70, p&lt;0.0001), and the response times of the absent searches were negatively correlated with visual homogeneity (<italic>r</italic>=–0.81, p&lt;0.0001).</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Generalization to other objects.</title><p>(<bold>A</bold>) Response time for target present/absent responses in Experiment S2 (involving a larger set of natural objects) plotted against visual homogeneity calculated from Experiment S1. (<bold>B</bold>) Response time for target present/absent responses in Experiment S4 (involving silhouettes) plotted against visual homogeneity calculated from Experiment S3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app3-fig1-v1.tif"/></fig></sec></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s15"><title>Visual homogeneity predicts same/different responses</title><sec sec-type="appendix" id="s15-1"><title>Introduction</title><p>In the main text, we have shown that visual homogeneity computations can be used to solve any task that requires discriminating between homogeneous and heterogeneous displays. Having shown in Experiments 1–2 that visual homogeneity predicts target present/absent responses, we wondered whether these results would generalize to other tasks. Here, we investigated the same/different task, in which participants have to view two items and indicate whether they are same or different using a key press.</p><p>Although the exact instructions of the target present/absent visual search task and the same/different task are quite different, we predicted that ‘same’ responses would be related to the target-absent response, while the ‘different’ responses would be related to the target-present response. This correspondence is interesting and non-trivial since it has never been made previously to the best of our knowledge.</p></sec></sec><sec sec-type="appendix" id="s16"><title>Experiment S5. Same/different task</title><p>In Experiment S5, we recruited participants to perform a same/different task involving the same natural objects as in Experiment 1. The ‘same’trials and ‘different’ trials were exactly matched to the target-absent and target-present conditions in the two experiments, to enable direct comparisons.</p><sec sec-type="appendix" id="s16-2"><title>Participants</title><p>A total of 16 participants (8 males aged 23.5±3.5 years) participated in this experiment.</p></sec><sec sec-type="appendix" id="s16-3"><title>Stimuli</title><p>The stimulus set comprised a set of 32 grayscale natural objects (16 animate, 16 inanimate) presented against a black background. Same as in the main experiments 1 and 2.</p></sec><sec sec-type="appendix" id="s16-4"><title>Procedure</title><p>Participants performed a same-different task with a block of practice trials involving unrelated stimuli followed by the main task. Each trial began with a fixation cross (width 0.6°) for 500ms, followed by two images of size 3.5 dva presented for 500ms on a black background. The images were presented on either side of the fixation cross along the x-axis 3 dva away from the centre. A random position jitter of 0.5 dva was added in both directions. Participants were asked to identify if two images were the same or different and respond with a keypress (‘Z’ for same, ‘M’ for different) as quickly and accurately as possible. Participants could give responses up to 3.5 s. Each participant completed 256 same trials (32 objects x 8 repeats) and 256 different trials (32 randomly selected image pairs x 8 repeats). The experiment was created using PsychoPy (<xref ref-type="bibr" rid="bib31">Peirce et al., 2019</xref>) and ported to the online Pavlovia platform for collecting data. Each participant adjusted a credit card image presented on the screen with keypresses to match their credit card size, ensuring identical image presentation across participants.</p></sec><sec sec-type="appendix" id="s16-5"><title>Results</title><p>In Experiment S5, participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 91.4 ± 3.4%; correlation between mean reaction times of even- and odd-numbered participants: <italic>r</italic>=0.50, p&lt;0.005 across 32 same trials, <italic>r</italic>=0.71, p&lt;0.0005 across 32 diff trials). The absent search response times from Experiment 2 predicted the same response times of Experiment S5 (correlation between response times: <italic>r</italic>=0.67, p&lt;0.0005; <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1E</xref>). The present search response time from Experiment 2 predicted the different response times of Experiment S5 (correlation between response times: <italic>r</italic>=0.75, p&lt;0.0005; <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1F</xref>). The response times of one task predict the response times of another on the same set of image pairs is expected when identical computations can solve both tasks.</p><p>We conclude that visual homogeneity computations can explain same-different judgements.</p><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Target absent/present responses predict same/different responses.</title><p>(<bold>A</bold>) Example target-absent trial from the visual search task (Experiment 2). (<bold>B</bold>) Example target-present trial from the visual search task (Experiment 2). (<bold>C</bold>) Example ‘same’ trial from the same-different task (Experiment S5), matched exactly to the target-absent trial in panel A. (<bold>D</bold>) Example ‘different’ trial from the same-different task (Experiment S5), matched exactly to the target-present trial in panel B. (<bold>E</bold>) Response time on ‘same’ trials in Experiment S5 plotted against response time for the corresponding target-absent trials from Experiment 1. (<bold>F</bold>) Response time on ‘different’ trials in Experiment S5 plotted against response time for the corresponding target-present trials from Experiment 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app4-fig1-v1.tif"/></fig></sec></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s17"><title>Visual homogeneity is unaffected by context</title><p>In Appendix 1, we have shown that visual homogeneity for each image is proportional to its distance to all other images, and that it is negatively correlated with target-absent response times. This raises the question of whether target-absent response times are affected by the other objects used in the experiment. We addressed this question using two experiments.</p></sec><sec sec-type="appendix" id="s18"><title>Experiment S6. Visual search across diverse category context</title><p>We first investigated the possibility that target present/absent response times could vary depending on the experimental context of other searches. To this end, participants had to perform target present/absent task in three separate blocks: searches involving animate objects, searches involving silhouette objects and a mixed block containing both animate and silhouette objects.</p><sec sec-type="appendix" id="s18-2"><title>Participants</title><p>A total of 18 participants (12 males, aged 23.5±3.2 years) participated in this experiment.</p></sec><sec sec-type="appendix" id="s18-3"><title>Stimuli</title><p>The stimulus set comprised a set of 16 grayscale natural objects (animates) and 16 silhouettes presented against a black background.</p></sec><sec sec-type="appendix" id="s18-4"><title>Procedure</title><p>This experiment constituted three blocks: Animal block, Silhouettes block, and mixed block. All participants performed trials from all the blocks, and the block order was counterbalanced across participants. Each participant performed 512 correct trials (128 from animate block, 128 from silhouette block and 256 from mixed block). In the animate block, there were 32 uniques conditions (16 present searches and 16 absent searches) repeated 4 times created from 16 animate images. A similar design was repeated for the silhouette block on 16 silhouette images. The mixed block was created by combining the animate blocks and silhouette blocks. The trial presentation order was random within a block. Participants were seated approximately 60 cm from the computer monitor that was under the control of a custom program written using Psychtoolbox in Matlab. Each trial started by presenting a fixation cross for 500ms, followed by a 4x4 search array containing one oddball item among multiple identical distractors. Items were randomly jittered. Participants were asked to report the presence of an oddball target as quickly and as accurately as possible using keypress (Y for the presence of the target, N for the absence of the target). All images were presented against a black background.</p></sec><sec sec-type="appendix" id="s18-5"><title>Results</title><p>Participants were highly accurate and consistent in their responses (accuracy, mean ±bsd: 96.9 ± 1.8% for Animate block, 97.9 ± 1.3% for mixed block, 97.7±1.8 for silhouette block; correlation between mean reaction times of even- and odd-numbered participants: <italic>r</italic>=0.93, p&lt;0.0005 for Animate block, <italic>r</italic>=0.94, p&lt;0.0005 for Mixed block, <italic>r</italic>=0.95, p&lt;0.0005 for Silhouette block). The response time to searches in the animate block shows a striking correlation with the response times to the corresponding searches in the mixed block (correlation between reaction times: <italic>r</italic>&lt;0.96, p&lt;0.0001 for present searches, <italic>r</italic>&lt;0.95, p&lt;0.0001 for absent searches). Similarly, the response time to searches in the silhouette block shows a striking correlation with the response times to the corresponding searches in the mixed block (correlation between reaction times: <italic>r</italic>&lt;0.96, p&lt;0.0001 for present searches, <italic>r</italic>&lt;0.91, p&lt;0.0001 for absent searches). Even when the animal and silhouette images have completely different image statistics, there was no significant difference between reaction times in the interleaved and separate blocks (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1A and B</xref>). Hence, we conclude that context has no effect on visual homogeneity.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Target-absent responses are unaffected by mixing disparate searches.</title><p>(<bold>A</bold>) Response times in the Mixed Block plotted against the corresponding response times in the Animal Block for present searches (<italic>blue</italic>), and absent searches (<italic>red</italic>). (<bold>B</bold>) Response time in Mixed Block plotted against the corresponding response times in the Silhouette-only Block, with conventions as in panel A.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app5-fig1-v1.tif"/></fig></sec></sec><sec sec-type="appendix" id="s19"><title>Experiment S7. Objects with uncorrelated dissimilarity</title><p>Although the results of Experiment S6 show that target-absent searches are unaffected by whether they are surrounded by similar or mixed object categories, it is still possible that the average distance of each object to other objects was unaffected by mixing with other category objects but could be influenced by the experimental context of other objects in the same category.</p><p>To this end, we constructed two sets of 29 shapes (20 unique to each set, 9 common to both sets) (<xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2A</xref>). We chose the two sets so that the predicted visual homogeneity (i.e. average distance to other objects in that set) relative to each set is uncorrelated (<xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2B</xref>). We then performed a present/absent visual search block in which the 9 common objects are viewed in the context of the other objects in Set 1 or Set 2 in separate blocks.</p><p>If visual homogeneity depends on the objects being seen in a given block, we predicted that the target absent response times of the common objects will be uncorrelated. If on the other hand, the visual homogeneity of an object is unaffected by experimental context, we predicted that the absent search response times will be correlated. We tested this prediction using a target present/absent search experiment.</p><sec sec-type="appendix" id="s19-1"><title>Methods</title><p>Participants. A total of 12 participants (7 males, aged 23.5±1.16 years) participated in this experiment.</p><p>Stimuli. This stimulus set comprised of 49 silhouette shapes. Sets 1 and 2 were constructed by selecting 29 of 49 shapes where 9 shapes were common across sets in such a manner that the average dissimilarity of the 9 common shapes relative to the other objects in the two sets were uncorrelated (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1A-B</xref>).</p><p>Procedure. This experiment comprised two blocks. All participants completed both blocks, and the block order was counterbalanced across participants. Each participant performed 464 correct trials (232 from block 1 and 232 from block 2). In each block there were 58 unique conditions (29 absent searches, 29 present searches) and each condition was repeated 4 times. The trial presentation order was random within the block. Other details were same as experiment S6.</p><p>Data Analysis. Response times faster than 0.3 s or slower than 5 s were removed. We used ‘<italic>isoutlier’</italic> Matlab function to remove the outliers in the data. These steps removed 6.6% of the data and improved the overall response time consistency but did not quantitatively alter the results.</p></sec><sec sec-type="appendix" id="s19-2"><title>Results</title><p>Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 94.8 ± 2.6% for block 1, and 95.2 ± 3.1% for block 2; correlation between mean reaction times of even- and odd-numbered participants: <italic>r</italic>=0.94, p&lt;0.0005 for block 1 and <italic>r</italic>=0.95, p&lt;0.0005 for block 2).</p><p>For the common objects, we computed the average dissimilarity to other objects within the set and the average dissimilarity of the common objects were not correlated across sets (<italic>r</italic>=0.21, p=0.6; <xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2B</xref>), which is by design. Interestingly, the target-absent responses were highly correlated across sets (<italic>r</italic>=0.83, p=0.05; <xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2C</xref>).</p><p>We conclude that visual homogeneity is unaffected by the experimental context regardless similar or dissimilar objects.</p><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>Target-absent responses are unaffected by disparate object context.</title><p>(<bold>A</bold>) 2D embedding of the 49 silhouettes based in the pairwise dissimilarities (1/RT) measured using odd-ball visual search experiment (Experiment S3). Shapes are coloured according to the set to which they are grouped: <italic>red</italic> for shapes common to set 1 and set 2, <italic>blue</italic> for shapes only in set 1, and <italic>green</italic> for shapes only in set 2. (<bold>B</bold>) Average dissimilarity of the common items relative to items in Set 2 plotted against the average dissimilarity relative to items in Set 1. If visual homogeneity depends on the average distance to other objects in the immediate experimental context, then target-absent responses for the common objects should be uncorrelated when presented in a block containing Set 1 items compared to a block containing Set 2 items. (<bold>C</bold>) Absent search response times for the common items in Block 2 (containing Set 2 items) against the corresponding search times in Block 1 (containing Set 1 items). The strong and significant correlation indicates that target-absent search times are independent of the immediate experimental context.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app5-fig2-v1.tif"/></fig></sec></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s20"><title>Additional analyses for experiment 2 (fMRI)</title><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>Brain activations for target-present and target-absent searches.</title><p>Whole brain colormap of activation difference between target-present and target-absent searches. The color at each voxel represents the t-statistic computed between the participant-wise mean activations for target-present minus target-absent searches (averaged across searches of each type, and across a 3 x 3 × 3 voxel neighborhood centered around that voxel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app6-fig1-v1.tif"/></fig><fig id="app6fig2" position="float"><label>Appendix 6—figure 2.</label><caption><title>Robustness of VH region in target present/absent search.</title><p>(<bold>A</bold>) Searchlight map showing the correlation between visual homogeneity and mean activation of an example subject. (<bold>B</bold>) Colormap representing the number of subjects for which a particular voxel belonged to the localized VH region. (<bold>C</bold>) Colormap of the correlation between visual homogeneity and mean activation across eight subjects in Group 1. (<bold>D</bold>) VH region obtained by thresholding the searchlight map in panel C. (<bold>E</bold>) Correlation between visual homogeneity and mean activation for participants in Group 1 for the VH region identified from Group 1, and for the VH region identified from Group 2. Asterisks indicate statistical significance of each correlation, obtained by sampling participants with replacement 10,000 times, and calculating the fraction of times the correlation was below zero (**** is p&lt;0.0005). The significant correlation in Group 2 for the region identified using Group 1 participants suggest that the VH region is consistently localized across subjects. (<bold>F</bold>) Searchlight map similar to panel C, but for participants in Group 2. (<bold>G</bold>) VH region obtained by thresholding the searchlight map in panel F. (<bold>H</bold>) Correlation between visual homogeneity and mean activation for participants in Group 2, for the VH regions identified from Group 1 and from Group 2. Asterisks indicate statistical significance of each correlation, obtained by sampling participants with replacement 1000 times, and calculating the fraction of times the correlation was below zero (***is p&lt;0.005). The significant correlation in Group 1 for the region identified using Group 2 participants suggest that the VH region is consistently localized across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app6-fig2-v1.tif"/></fig><fig id="app6fig3" position="float"><label>Appendix 6—figure 3.</label><caption><title>Searchlight maps with response times.</title><p>(<bold>A</bold>) Colormap of correlation between mean activation and response times for target-present search arrays. (<bold>B</bold>) Colormap of correlation between mean activation and response times for target-absent search arrays. (<bold>C</bold>) Correlation between mean activation and response times for both target-present and target-absent search arrays. To prevent image-wise correlations from being confounded by overall activation level differences, we z-scored the mean activations for each voxel within a particular search type (present/absent) and then combined the mean activations. Likewise, for similar reasons, we z-scored the response times for each particular search type. (<bold>D</bold>) Correlation between mean activation and all response times for key visual regions.Error bars represent the standard deviation of correlation obtained using a bootstrap process by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p&lt;0.05, ** is p&lt;0.01, **** is p&lt;0.0001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app6-fig3-v1.tif"/></fig><fig id="app6fig4" position="float"><label>Appendix 6—figure 4.</label><caption><title>Relative weights of target and distractor in target-present arrays.</title><p>(<bold>A</bold>) Colormap of correlation between observed and predicted voxel activity of the linear voxel model, in which target-present search array response is modelled as a linear combination of target and distractor activity (taken from responses to target-absent arrays). (<bold>B</bold>) Region showing good model prediction, obtained by thresholding the colormap in (<bold>A</bold>). (<bold>C</bold>) Target and distractor model coefficients in this region. Each point corresponds to model coefficients derived from a single voxel. Model coefficients for target and distractor are equal in weight (p=0.33, sign-rank test across weights of 222 voxels in this region).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app6-fig4-v1.tif"/></fig></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s21"><title>Additional analysis for experiment 4 (fMRI)</title><fig id="app7fig1" position="float"><label>Appendix 7—figure 1.</label><caption><title>Brain activations for asymmetric and symmetric objects.</title><p>Whole brain colormap of activation difference between asymmetric and symmetric objects during the symmetry task. The color at each voxel represents the t-statistic computed between the participant-wise mean activations for asymmetric minus symmetric objects (averaged across objects of each type, and across a 3 x 3 × 3 voxel neighborhood centered around that voxel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app7-fig1-v1.tif"/></fig><fig id="app7fig2" position="float"><label>Appendix 7—figure 2.</label><caption><title>Robustness of VH region in symmetry detection.</title><p>(<bold>A</bold>) Searchlight map showing the correlation between visual homogeneity and mean activation of an example subject. (<bold>B</bold>) Colormap representing the number of subjects for which a particular voxel belonged to the localized VH region. (<bold>C</bold>) Colormap of the correlation between visual homogeneity and mean activation across eight subjects in Group 1. (<bold>A</bold>) VH region obtained by thresholding the searchlight map in panel C. (<bold>B</bold>) Correlation between visual homogeneity and mean activation for participants in Group 1 for the VH region identified from Group 1, and for the VH region identified from Group 2. Asterisks indicate statistical significance of each correlation, obtained by sampling participants with replacement 1000 times, and calculating the fraction of times the correlation was below zero (*** is p&lt;0.005). The significant correlation in Group 2 for the region identified using Group 1 participants suggest that the VH region is consistently localized across subjects. (<bold>C</bold>) Searchlight map similar to panel C, but for 7 participants in Group 2. (<bold>D</bold>) VH region obtained by thresholding the searchlight map in panel F. (<bold>E</bold>) Correlation between visual homogeneity and mean activation for participants in Group 2, for the VH regions identified from Group 1 and from Group 2. Asterisks indicate statistical significance of each correlation, obtained by sampling participants with replacement 1000 times, and calculating the fraction of times the correlation was below zero (*** is p&lt;0.005). The significant correlation in Group 1 for the region identified using Group 2 participants suggest that the VH region is consistently localized across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app7-fig2-v1.tif"/></fig><fig id="app7fig3" position="float"><label>Appendix 7—figure 3.</label><caption><title>Searchlight maps for response time during symmetry task.</title><p>(<bold>A</bold>) Colormap of correlation between mean activation and response times for asymmetric objects. (<bold>B</bold>) Colormap of correlation between mean activation and response times for symmetric objects. (<bold>C</bold>) Correlation between mean activation and response times across both asymmetric and symmetric objects. To prevent image-wise correlations from being confounded by overall activation level differences, we z-scored the mean activations for each voxel within a particular object type (asymmetric/symmetric) and then combined the mean activations. Likewise, for similar reasons, we z-scored the response times for each particular object type before combining. (<bold>D</bold>) Correlation between mean activation and all response times for key visual regions. Asterisks indicate statistical significance calculated in the same way as <xref ref-type="fig" rid="fig4">Figure 4D</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app7-fig3-v1.tif"/></fig></sec></app><app id="appendix-8"><title>Appendix 8</title><sec sec-type="appendix" id="s22"><title>Encoding of visual homogeneity in brain across experiments</title><p>In Experiments 1–4, we showed that visual homogeneity can be used to solve a variety of visual tasks that require discriminating between homogeneous and heterogeneous displays. We found that the region anterior to Lateral Occipital Cortex is encoding visual homogeneity signal during both present-absent search task and symmetry judgement task. We compared the spatial location of the visual homogenenity region by directly comparing the region masks derived from Experiments 2 and 4. It can be seen that the VH region localized from the present/absent visual search task in Experiment 2 overlaps reasonably well with the VH region localized from the symmetry detection task in Experiment 4 (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1</xref>).</p><fig id="app8fig1" position="float"><label>Appendix 8—figure 1.</label><caption><title>Comparing the VH region from Experiments 2 and 4.</title><p>Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, lateral occipital (LO) region. The VH region from the present/absent search task (Experiment 2, <xref ref-type="fig" rid="fig4">Figure 4C</xref>) is overlaid with the VH region identified from the symmetry task (Experiment 4, <xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app8-fig1-v1.tif"/></fig></sec></app><app id="appendix-9"><title>Appendix 9</title><sec sec-type="appendix" id="s23"><title>Relating target-absent responses and symmetry</title><p>In Experiments 1–4, we showed that visual homogeneity can be used to solve a variety of visual tasks that require discriminating between homogeneous and heterogeneous displays. These findings suggest a direct empirical link between two disparate tasks, namely visual search for a target and symmetry detection.</p><p>In particular, since target-absent response times are inversely related to visual homogeneity, we can take its reciprocal as a direct estimate of visual homogeneity. Since response times for asymmetric and symmetric objects during a symmetry detection task have opposite correlations to visual homogeneity, we predict that they will have opposite correlations with the reciprocal of target-absent response time. In Experiment S7, we tested this prediction by measuring target-absent response times for symmetric and asymmetric objects used in Experiment 4.</p></sec><sec sec-type="appendix" id="s24"><title>Methods</title><sec sec-type="appendix" id="s24-2"><title>Participants</title><p>A total of 17 participants (10 females, aged 22±5.3 years) participated in this experiment.</p></sec><sec sec-type="appendix" id="s24-3"><title>Paradigm</title><p>Participants performed a target-present/absent search task. Participants completed 512 correct trials (64 present conditions x 4 repetitions +64 absent conditions x 4 repetitions). Participants reported the presence or absence of the target using the ‘P’ or ‘A’ key press. We used the same 64 baton shapes used in Experiment 3. Each array had eight items arranged along a circular grid of radius 5 dva from the centre of the screen. Each item measured 2 dva. There was a random position jitter of ±0.6° in both X and Y directions. All other task details are the same as Experiment 1.</p></sec></sec><sec sec-type="appendix" id="s25"><title>Results</title><p>An example search array from Experiment S8 is shown in <xref ref-type="fig" rid="app9fig1">Appendix 9—figure 1A</xref>. Likewise, an example screen from the symmetry detection task is shown in <xref ref-type="fig" rid="app9fig1">Appendix 9—figure 1B</xref>.</p><p>Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 97.19 ± 2%; correlation between mean reaction times of even- and odd-numbered participants: <italic>r</italic>=0.86, p&lt;0.0001). As predicted, response times to asymmetric objects during the symmetry detection task in Experiment 4 were positively correlated with the inverse of the absent-search times, but this correlation was not statistically significant (<xref ref-type="fig" rid="app9fig1">Appendix 9—figure 1C</xref>). Likewise, response times for symmetric objects in Experiment 4 were negatively and significantly correlated with the inverse of absent-search times (<xref ref-type="fig" rid="app9fig1">Appendix 9—figure 1D</xref>). Thus, the reciprocal of absent search times can be taken as a decision variable for the symmetry detection task.</p><p>Thus, there is a direct and non-trivial empirical link between target-absent response times in a visual search task and response times in a symmetry detection task, because both tasks rely on the same decision variable, namely visual homogeneity.</p><fig id="app9fig1" position="float"><label>Appendix 9—figure 1.</label><caption><title>Target-absent search times predict symmetry detection.</title><p>(<bold>A</bold>) Example search array from Experiment S7. (<bold>B</bold>) Example display containing a symmetric object from Experiment 4. (<bold>C</bold>) Response times for asymmetric objects in Experiment 4 plotted against their target-absent response time inverse in Experiment S7. (<bold>D</bold>) Response times for symmetric objects in Experiment 4 plotted against their target-absent response time inverse in Experiment S7.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app9-fig1-v1.tif"/></fig></sec></app><app id="appendix-10"><title>Appendix 10</title><sec sec-type="appendix" id="s26"><title>Visual homogeneity explains animate categorization</title><sec sec-type="appendix" id="s26-1"><title>Introduction</title><p>Since visual homogeneity for any image is computed as its distance to some fixed point in perceptual space, we reasoned that shifting this fixed point to the center of a particular object category would make it automatically a decision variable for this category.</p><p>To test this prediction, we analyzed data from Experiment 1 of a previous study in which participants had to categorize images as belonging to three hierarchical categories: animals, dogs or Labradors, and also performed an oddball detection task using these same images (<xref ref-type="bibr" rid="bib25">Mohan and Arun, 2012</xref>). The oddball detection task data is already described above as Experiment S1. The categorization task is summarized below.</p></sec><sec sec-type="appendix" id="s26-3"><title>Participants</title><p>A total of 12 participants, aged 20–30 years were recruited for the experiment.</p></sec><sec sec-type="appendix" id="s26-4"><title>Stimuli</title><p>The stimuli consisted of 48 grayscale natural objects (24 animate, 24 inanimate) presented against a black background.</p></sec><sec sec-type="appendix" id="s26-5"><title>Procedure</title><p>Participants performed three separate blocks of categorization: an animal task, a dog task and a Labrador task, with block order counterbalanced across participants. Each block began with a preview of all objects to avoid confusion regarding the category, and each trial consisted of a fixation cross displayed for 750ms, followed by a test object for 50ms followed by a noise mask for 250ms. Participants had to indicate whether the object belonged to the category (animal/dog/Labrador) using a keypress (‘Y’ if yes, ‘N’ if no).</p></sec><sec sec-type="appendix" id="s26-6"><title>Model fitting for visual homogeneity</title><p>We started by embedding all 48 objects into an eight-dimensional space using multidimensional scaling. For each image, visual homogeneity is calculated as its distance from a fixed point in this multidimensional space. To calculate visual homogeneity for a particular category task, we adjusted the fixed center for visual homogeneity calculations so as to maximize the difference between the correlation of visual homogeneity with objects outside vs within that category. In this manner, we calculated the correlation between visual homogeneity for all objects in the category and the corresponding categorization times, and likewise for the non-category objects.</p></sec><sec sec-type="appendix" id="s26-7"><title>Results</title><p><xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1A</xref> shows an example screen from the animal categorization task. For this task, we estimated an optimal center for visual homogeneity computations by maximizing the difference in correlation for inanimate and animate objects. The resulting model fits are shown in <xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1B–C</xref>. It can be seen that categorization times for are positively correlated with visual homogeneity for animate objects (<xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1B</xref>) but negatively correlated for inanimate objects (<xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1C</xref>), suggesting that visual homogeneity can be used to solve the animate categorization task. We obtained similar results for the dog task (<xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1E–G</xref>) and for the Labrador task (<xref ref-type="fig" rid="app10fig1">Appendix 10—figure 1I–K</xref>).</p><p>We conclude that visual homogeneity can serve as a decision variable for categorization tasks at least for the categories tested here, and provided the fixed center for the visual homogeneity computations can be shifted to the center of each category.</p></sec></sec><sec sec-type="appendix" id="s27"><title>Can target-absent response times explain categorization?</title><p>Since the inverse of target-absent response times is a measure of visual homogeneity, we wondered whether they would directly predict the visual homogeneity predictions of each task. To this end, we took the absent-search response times from Experiment S2 and asked whether they are correlated with the visual homogeneity optimized for each categorization task. This revealed a significant positive correlation for all three tasks (<xref ref-type="fig" rid="app10fig2">Appendix 10—figure 2</xref>).</p><fig id="app10fig1" position="float"><label>Appendix 10—figure 1.</label><caption><title>Visual homogeneity predicts categorization times.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app10-fig1-v1.tif"/></fig><fig id="app10fig2" position="float"><label>Appendix 10—figure 2.</label><caption><title>Target-absent search predicts visual homogeneity for each category.</title><p>(<bold>A</bold>) Example trial of the animal categorization task. Stimuli was presented for 50ms followed by a noise mask. Subjects responded if the presented image is an animal or not with Y/N key responses. (<bold>B</bold>) Average categorization times for animals plotted against visual homogeneity relative to an optimum center for this task, calculated from oddball detection task data (Experiment S1). (<bold>C</bold>) Same as panel B but for inanimate objects. (<bold>D</bold>) Example trial of the dog categorization task. (<bold>E</bold>) Average categorization times for dogs plotted against visual homogeneity relative to an optimum center for this task, calculated from oddball detection task data (Experiment S1). (<bold>F</bold>) Same as panel E but for non-dogs. (<bold>G</bold>) Example trial of the Labrador categorization task. (<bold>H</bold>) Average categorization times for dogs plotted against visual homogeneity relative to an optimum center for this task, calculated from oddball detection task data (Experiment S1). (<bold>I</bold>) Same as panel H but for non-Labradors. (<bold>A</bold>) Inverse of target-absent search times from Experiment S2 plotted against the optimized visual homogeneity from the animate task. (<bold>B</bold>) Inverse of target-absent search times from Experiment S2 but now plotted against the optimized visual homogeneity from the dog task. (<bold>C</bold>) Inverse of target-absent search times from Experiment S2 but now plotted against the optimized visual homogeneity from the Labrador task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93033-app10-fig2-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93033.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kok</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Incomplete</kwd><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Useful</kwd></kwd-group></front-stub><body><p>This study uses carefully designed experiments to generate a <bold>useful</bold> behavioural and neuroimaging dataset on visual cognition. The results provide <bold>solid</bold> evidence for the involvement of higher-order visual cortex in processing visual oddballs and asymmetry. However, the evidence provided for the very strong claims of homogeneity as a novel concept in vision science, separable from existing concepts such as target saliency, is <bold>incomplete</bold>. The authors and the reviewers do not agree on several points, which are explained in the reviews and author response.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93033.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors define a new metric for visual displays, derived from psychophysical response times, called visual homogeneity (VH). They attempt to show that VH is explanatory of response times across multiple visual tasks. They use fMRI to find visual cortex regions with VH-correlated activity. On this basis, they declare a new visual region in human brain, area VH, whose purpose is to represent VH for the purpose of visual search and symmetry tasks.</p><p>Link to original review: <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/reviewed-preprints/93033v2/reviews#peer-review-0">https://elifesciences.org/reviewed-preprints/93033v2/reviews#peer-review-0</ext-link></p><p>Comments on latest version:</p><p>Authors rebuttal: We agree that visual homogeneity is similar to existing concepts such as target saliency, memorability etc. We have proposed it as a separate concept because visual homogeneity has an independent empirical measure (the reciprocal of target-absent search time in oddball search, or the reciprocal of same response time in a same-different task, etc) that may or may not be the same as other empirical measures such as saliency and memorability. Investigating these possibilities is beyond the scope of our study but would be interesting for future work. We have now clarified this in the revised manuscript (Discussion, p. 42).</p><p>Reviewer response to rebuttal: Neither the original ms nor the comments on that ms pretended that &quot;visual homogeneity&quot; was entirely separate from target saliency etc. So this is a response to a criticism that was never made. What the authors do claim, and what the comments question, is that they have successfully subsumed long-recognized psychophysical concepts like target saliency etc. under a new, uber-concept, &quot;visual homogeneity&quot; that explains psychophysical experimental results in a more unified and satisfying way. This subsumption of several well-established psychophysical concepts under a new, unified category is what reviewers objected to.</p><p>Authors rebuttal: However, we'd like to emphasize that the question of whether visual homogeneity is novel or related to existing concepts misses entirely the key contribution of our study.</p><p>Reviewer response to rebuttal: Sorry, but the claim of a new uber-concept in psychophysics, &quot;visual homogeneity&quot;, is a major claim of the paper. The fact that it is not the only claim made does not absolve the authors from having to prove it satisfactorily.</p><p>&quot;Authors rebuttal: &quot;In addition, the large regions of VH correlations identified in Experiments 1 and 2 vs. Experiments 3 and 4 are barely overlapping. This undermines the claim that VH is a universal quantity, represented in a newly discovered area of visual cortex, that underlies a wide variety of visual tasks and functions.&quot;</p><p>• We respectfully disagree with your assertion. First of all, there is partial overlap between the VH regions, for which there are several other obvious explanations that must be considered first before dismissing VH outright as a flawed construct. We acknowledge these alternatives in the Results (p. 27), and the relevant text is reproduced below.</p><p>&quot;We note that it is not straightforward to interpret the overlap between the VH regions identified in Experiments 2 &amp; 4. The lack of overlap could be due to stimulus differences (natural images in Experiment 2 vs silhouettes in Experiment 4), visual field differences (items in the periphery in Experiment 2 vs items at the fovea in Experiment 4) and even due to different participants in the two experiments. There is evidence supporting all these possibilities: stimulus differences (Yue et al., 2014), visual field differences (Kravitz et al., 2013) as well as individual differences can all change the locus of neural activations in object-selective cortex (Weiner and Grill-Spector, 2012a; Glezer and Riesenhuber, 2013). We speculate that testing the same participants on search and symmetry tasks using similar stimuli and display properties would reveal even larger overlap in the VH regions that drive behavior.&quot;</p><p>Reviewer response to rebuttal: The authors are saying that their results merely look unconvincing (weak overlap between VH regions defined in different experiments) because there were confounding differences between their experiments, in subject population, stimuli, etc. That is possible, but in that case it is up to the authors to show that their definition of a new &quot;area VH&quot; is convincing when the confounding differences are resolved, e.g. by using the same stimuli in the different experiments they attempt to agglomerate here. That would require new experiments, and none are offered in this revision.</p><p>Authors rebuttal: • Thank you for carefully thinking through our logic. We agree that a distance-to-centre calculation is entirely unnecessary as an explanation for target-present visual search. The similarity between target and distractor, so there is nothing new to explain here. However, this is a narrow and selective interpretation of our findings because you are focusing only on our results on target-present searches, which are only half of all our data. The other half is the target-absent responses which previously have had no clear explanation. You are also missing the fact that we are explaining same-different and symmetry tasks as well using the same visual homogeneity computation. We urge you to think more deeply about the problem of how to decide whether an oddball is present or not in the first place. How do we actually solve this task?</p><p>Reviewer response to rebuttal: It is the role of the authors to think deeply about their paper and on that basis present a clear and compelling case that readers can understand quickly and agree with. That is not done here.</p><p>Authors rebuttal: There must be some underlying representation and decision process. Our study shows that a distance-to-centre computation can actually serve as a decision variable to solve disparate property-based visual tasks. These tasks pose a major challenge to standard models of decision-making because the underlying representation and decision variable have been unclear. Our study resolves this challenge by proposing a novel computation that can be used by the brain to solve all these disparate tasks, and bring these tasks into the ambit of standard theories of decision-making.</p><p>Reviewer response to rebuttal: There is only a &quot;challenge&quot; if you accept the authors' a priori assumption that all of these tasks must have a common explanation and rely on a single neural mechanism. I do not accept that assumption, and I don't think the authors provide evidence to support the assumption. There is nothing &quot;unclear&quot; about how search, oddball, etc. have been thoroughly explained, separately, in the psychophysical literature that spans more than a century.</p><p>Authors rebuttal: • You are indeed correct in noting that both Experiment 1 &amp; 2 involve oddball search, and so at the superficial level, it looks circular that the oddball search data of Experiment 1 is being used to explain the oddball search data of Experiment 2.</p><p>However a deeper scrutiny reveals more fundamental differences: Experiment 1 consisted of only oddball search with the target appearing on the left or right, whereas Experiment 2 consisted of oddball search with the target either present or completely absent. In fact, we were merely using the search dissimilarities from Experiment 1 to reconstruct the underlying object representation, because it is well-known that neural dissimilarities are predicted well by search dissimilarities (Sripati &amp; Olson, 2009; Zhivago et al, 2014).</p><p>Reviewer response to rebuttal: Here again the authors cite differences between their multiple experiments as a virtue that supports their conclusions. Instead, the experiments should have been designed for maximum similarity if the authors intended to explain them with the same theory.</p><p>Authors rebuttal: To thoroughly refute any lingering concern about circularity, we reasoned that the model predictions for Experiment 2 could have been obtained by a distance-to-center computation on any brain like object representation. To this end, we used object representations from deep neural networks pretrained on object categorization, whose representations are known to match well with the brain, and asked if a distance-to-centre computation on these representations could predict the search data in Experiment 2. This was indeed the case, and these results are now included an additional section in Supplementary Material (Section S1).</p><p>Reviewer response to rebuttal: The authors' claims are about human performance and how it is based on the human brain. Their claims are not well supported by the human experiments that they performed. It serves no purpose to redo the same experiments in silico, which cannot provide stronger evidence that compensates for what was lacking in the human data.</p><p>Authors rebuttal: &quot;Confirming the generality of visual homogeneity</p><p>We performed several additional analyses to confirm the generality of our results, and to reject alternate explanations.</p><p>First, it could be argued that our results are circular because they involve taking oddball search times from Experiment 1 and using them to explain search response times in Experiment 2. This is a superficial concern since we are using the search dissimilarities from Experiment 1 only as a proxy for the underlying neural representation, based on previous reports that neural dissimilarities closely match oddball search dissimilarities (Sripati and Olson, 2010; Zhivago and Arun, 2014). Nonetheless, to thoroughly refute this possibility, we reasoned that we would get similar predictions of the target present/absent responses in Experiment using any other brain-like object representation. To confirm this, we replaced the object representations derived from Experiment 1 with object representations derived from deep neural networks pretrained for object categorization, and asked if distance-to-center computations could predict the target present/absent responses in Experiment 2. This was indeed the case (Section S1).</p><p>Second, we wondered whether the nonlinear optimization process of finding the best-fitting center could be yielding disparate optimal centres each time. To investigate this, we repeated the optimization procedure with many randomly initialized starting points, and obtained the same best-fitting center each time (see Methods).</p><p>Third, to confirm that the above model fits are not due to overfitting, we performed a leave-one-out cross validation analysis. We left out all target-present and target-absent searches involving a particular image, and then predicted these searches by calculating visual homogeneity estimated from all other images. This too yielded similar positive and negative correlations (r = 0.63, p &lt; 0.0001 for target-present, r = -0.63, p &lt; 0.001 for target-absent).</p><p>Fourth, if heterogeneous displays indeed elicit similar neural responses due to mixing, then their average distance to other objects must be related to their visual homogeneity. We confirmed that this was indeed the case, suggesting that the average distance of an object from all other objects in visual search can predict visual homogeneity (Section S1).</p><p>Fifth, the above results are based on taking the neural response to oddball arrays to be the average of the target and distractor responses. To confirm that averaging was indeed the optimal choice, we repeated the above analysis by assuming a range of relative weights between the target and distractor. The best correlation was obtained for almost equal weights in the lateral occipital (LO) region, consistent with averaging and its role in the underlying perceptual representation (Section S1).</p><p>Finally, we performed several additional experiments on a larger set of natural objects as well as on silhouette shapes. In all cases, present/absent responses were explained using visual homogeneity (Section S2).&quot;</p><p>Reviewer response to rebuttal: The authors can experiment on side questions for as long as they please, but none of the results described above answer the concern about how center-fitting undercuts the evidentiary value of their main results.</p><p>Authors rebuttal: • While it is true that the optimal center needs to be found by fitting to the data, there no particular mystery to the algorithm: we are simply performing a standard gradient-descent to maximize the fit to the data. We have described the algorithm clearly and are making our codes public. We find the algorithm to yield stable optimal centers despite many randomly initialized starting points. We find the optimal center to be able to predict responses to entirely novel images that were excluded during model training. We are making no assumption about the location of centre with respect to individual points. Therefore, we see no cause for concern regarding the center-finding algorithm.</p><p>Reviewer response to rebuttal: The point of the original comment was that center-fitting should not be done in the first place because it introduces unknowable effects.</p><p>•Authors rebuttal: Most visual tasks, such as finding an animal, are thought to involve building a decision boundary on some underlying neural representation. Even visual search has been portrayed as a signal-detection problem where a particular target is to be discriminated from a distractor. However none of these formulations work in the case of property-based visual tasks, where there is no unique feature to look for.</p><p>We are proposing that, when we view a search array, the neural response to the search array can be deduced from the neural responses to the individual elements using well-known rules, and that decisions about an oddball target being present or absent can be made by computing the distance of this neural response from some canonical mean firing rate of a population of neurons. This distance to center computation is what we denote as visual homogeneity. We have revised our manuscript throughout to make this clearer and we hope that this helps you understand the logic better.</p><p>• You are absolutely correct that the stimulus complexity should matter, but there are no good empirically derived measures for stimulus complexity, other than subjective ratings which are complex on their own and could be based on any number of other cognitive and semantic factors. But considering what factors are correlated with target-absent response times is entirely different from asking what decision variable or template is being used by participants to solve the task.</p><p>Reviewer response to rebuttal: If stimulus complexity is what matters, as the authors agree here, then it is incumbent on them to measure stimulus complexity. The difficulty of measuring stimulus complexity does not justify avoiding the problem with an analysis that ignores complexity.</p><p>Authors rebuttal: • We have provided empirical proof for our claims, by showing that target-present response times in a visual search task are correlated with &quot;different&quot; responses in the same-different task, and that target-absent response times in the visual search task are correlated with &quot;same&quot; responses in the same-different task (Section S4).</p><p>Reviewer response to rebuttal: Sorry, but there is still no reason to think that same-different judgments are based on a mythical boundary halfway between the two. If there is a boundary, it will be close to the same end of the continuum, where subjects might conceivably miss some tiny difference between two stimuli. The vast majority of &quot;different&quot; stimuli will be entirely different from the same stimulus, producing no confusability, and certainly not a decision boundary halfway between two extremes.</p><p>Authors rebuttal: • Again, the opposite correlations between target present/absent search times with VH are the crucial empirical validation of our claims that a distance-to-center calculation explain how we perform these property-based tasks. The VH predictions do not fully explain the data. We have explicitly acknowledged this shortcoming, so we are hardly dismissing it as a problem.</p><p>Reviewer response to rebuttal: The authors' acknowledgement of flaws in the ms does not argue in favor of publication, but rather just the opposite.</p><p>Authors rebuttal: • Finding an oddball, deciding if two items are same or different and symmetry tasks are disparate visual tasks that do not fit neatly into standard models of decision-making. The key conceptual advance of our study is that we propose a plausible neural representation and decision variable that allows all three property-based visual tasks to be reconciled with standard models of decision-making.</p><p>Reviewer response to rebuttal: The original comment stands as written. Same/different will have a boundary very close to the &quot;same&quot; end of the continuum. The boundary is only halfway between two choices if the stimulus design forces the boundary to be there, as in the motion and cat/dog experiments.</p><p>Authors rebuttal: &quot;There is no inherent middle point boundary between target present and target absent. Instead, in both types of trial, maximum information is present when target and distractors are most dissimilar, and minimum information is present when target and distractors are most similar. The point of greatest similarity occurs at then limit of any metric for similarity. Correspondingly, there is no middle point dip in information that would produce greater difficulty and higher response times. Instead, task difficulty and response times increase monotonically with similarity between targets and distractors, for both target present and target absent decisions. Thus, in Figs. 2F and 2G, response times appear to be highest for animals, which share the largest numbers of closely similar distractors.&quot;</p><p>• Your alternative explanation rests on vague factors like &quot;maximum information&quot; which cannot be quantified. By contrast we are proposing a concrete, falsifiable model for three property-based tasks - same/different, oddball present/absent and object symmetry. Any argument based solely on item similarity to explain visual search or symmetry responses cannot explain systematic variations observed for target-absent arrays and for symmetric objects, for the reasons explained earlier.</p><p>Reviewer response to rebuttal: There is nothing vague about this comment. The authors use an analysis that assumes a decision boundary at the centerpoint of their arbitrarily defined stimulus space. This assumption is not supported, and it is unlikely, considering that subjects are likely to notice all but the smallest variations between same and different stimuli, putting the boundary nearly at the same end of the continuum, not the very middle.</p><p>Authors rebuttal: &quot;(1) The area VH boundaries from different experiments are nearly completely non-overlapping.</p><p>In line with their theory that VH is a single continuum with a decision boundary somewhere in the middle, the authors use fMRI searchlight to find an area whose responses positively correlate with homogeneity, as calculated across all of their target present and target absent arrays. They report VH-correlated activity in regions anterior to LO. However, the VH defined by symmetry Experiments 3 and 4 (VHsymmetry) is substantially anterior to LO, while the VH defined by target detection Experiments 1 and 2 (VHdetection) is almost immediately adjacent to LO. Fig. S13 shows that VHsymmetry and VHdetection are nearly non-overlapping. This is a fundamental problem with the claim of discovering a new area that represents a new quantity that explains response times across multiple visual tasks. In addition, it is hard to understand why VHsymmetry does not show up in a straightforward subtraction between symmetric and asymmetric objects, which should show a clear difference in homogeneity.&quot;</p><p>• We respectfully disagree. The partial overlap between the VH regions identified in Experiments 1 &amp; 2 can hardly be taken as evidence against the quantity VH itself, because there are several other obvious alternate explanations for this partial overlap, as summarized earlier as well. The VH region does show up in a straightforward subtraction between symmetric and asymmetric objects (Section S7), so we are not sure what the Reviewer is referring to here.</p><p>Reviewer response to rebuttal: In disagreeing with the comment quoted above, the authors are maintaining that a new functional area of cerebral cortex can be declared even if that area changes location on the cortical map from one experiment to another. That position is patently absurd.</p><p>Authors rebuttal: &quot;(3) Definition of the boundaries and purpose of a new visual area in the brain requires circumspection, abundant and convergent evidence, and careful controls.</p><p>Even if the VH metric, as defined and calculated by the authors here, is a meaningful quantity, it is a bold claim that a large cortical area just anterior to LO is devoted to calculating this metric as its major task. Vision involves much more than target detection and symmetry detection. Cortex anterior to LO is bound to perform a much wider range of visual functionalities. If the reported correlations can be clarified and supported, it would be more circumspect to treat them as one byproduct of unknown visual processing in cortex anterior to LO, rather than treating them as the defining purpose for a large area of visual cortex.&quot;</p><p>• We totally agree with you that reporting a new brain region would require careful interpretation and abundant and converging evidence. However, this requires many studies worth of work, and historically category-selective regions like the FFA have achieved consensus only after they were replicated and confirmed across many studies. We believe our proposal for the computation of a quantity like visual homogeneity is conceptually novel, and our study represents a first step that provides some converging evidence (through replicable results across different experiments) for such a region. We have reworked our manuscript to make this point clearer (Discussion, p 32).</p><p>Reviewer response to rebuttal: Indeed, declaring a new brain area depends on much more work than is done here. Thus, the appropriate course here is to wait before claiming to have identified a new cortical area.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93033.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study proposes visual homogeneity as a novel visual property that enables observers perform to several seemingly disparate visual tasks, such as finding an odd item, deciding if two items are same, or judging if an object is symmetric. In Exp 1, the reaction times on several objects were measured in human subjects. In Exp 2, visual homogeneity of each object was calculated based on the reaction time data. The visual homogeneity scores predicted reaction times. This value was also correlated with the BOLD signals in a specific region anterior to LO. Similar methods were used to analyze reaction time and fMRI data in a symmetry detection task. It is concluded that visual homogeneity is an important feature that enables observers to solve these two tasks.</p><p>Strengths:</p><p>(1) The writing is very clear. The presentation of the study is informative.</p><p>(2) This study includes several behavioral and fMRI experiments. I appreciate the scientific rigor of the authors.</p><p>Weaknesses:</p><p>Before addressing the manuscript itself, I would like to comment the review process first. Having read the lasted revised manuscript, I shared many of the concerns raised by the two reviewers in the last two rounds of review. It appears that the authors have disagreed with the majority of comments made by the two reviewers. If so, I strongly recommend that the authors proceed to make this revision as a Version of Record and conclude this review process. According to eLife's policy that the authors have the right to make a Version of Record at any time during the review process, and I fully respect that right. However, I also ask that the authors respect the reviewer's right to retain the comments regarding this paper.</p><p>Beside that, I still have several further questions about this study.</p><p>(1) My main concern with this paper is the way visual homogeneity is computed. On page 10, lines 188-192, it says: &quot;we then asked if there is any point in this multidimensional representation such that distances from this point to the target-present and target-absent response vectors can accurately predict the target-present and target-absent response times with a positive and negative correlation respectively (see Methods)&quot;. This is also true for the symmetry detection task. If I understand correctly, the reference point in this perceptual space was found by deliberating satisfying the negative and positive correlations in response times. And then on page 10, lines 200-205, it shows that the positive and negative correlations actually exist. This logic is confusing. The positive and negative correlations emerge only because this method is optimized to do so. It seems more reasonable to identify the reference point of this perceptual space independently, without using the reaction time data. Otherwise, the inference process sounds circular. A simple way is to just use the mean point of all objects in Exp 1, without any optimization towards reaction time data.</p><p>I raised this question in my initial review. However, the authors did not address whether the positive and negative correlations still hold if the mean point is defined as the reference point without any optimization. The authors also argue that it is similar to a case of fitting a straight line. It is fine that the authors insist on the straight line (e.g., correlation). However, I would not call &quot;straight line correlations&quot; a &quot;quantitative model&quot; as a high-profile journals like eLife. Please remove all related arguments of a novel quantitative model.</p><p>(2) Visual homogeneity (at least given the current form) is an unnecessary term. It is similar to distractor heterogeneity/distractor variability/distractor saliency in literature. However, the authors attempt to claim it as a novel concept. Both R1 and me raised this question in the very first review. However, the authors refused to revise the manuscript. In the last review, I mentioned this and provided some example sentences claiming novelty. The authors only revised the last sentence of the abstract, and even did not bother to revise the last sentence of significance: &quot;we show that these tasks can be solved using a simple property WE DEFINE as visual homogeneity&quot;. Also, lines 851 still shows &quot;we have defined a NOVEL image property, visual homogeneity...&quot;. I am confused about whether the authors agree or disagree that &quot;visual homogeneity is an unnecessary term&quot;. If the authors agree, they should completely remove the related phrase throughout the paper. If not, they should keep all these and state the reasons. I don't think this is a correct approach to revising a manuscript.</p><p>(3) If the authors agree that visual homogeneity is not new, I suggest a complete rewrite of the title, abstract, significance, and introduction. Let me ask a simple question, can we remove &quot;visual homogeneity&quot; and use some more well-established term like &quot;image feature similarity&quot;? If yes, visual homogeneity is unnecessary.</p><p>(4) If I understand it correctly, one of the key findings of this paper is &quot;the response times for target-present searches were positively correlated with visual homogeneity. By contrast, the response times for target-absent searches were negatively correlated with visual homogeneity&quot; (lines 204-207). I think the authors have already acknowledged that this positive correlation is not surprising at all because it reflects the classic target-distractor similarity effect. If this is the case, please completely remove the positive correlation as a novel prediction and finding.</p><p>(5) In my last review, I mentioned the seminal paper by Duncan and Humphreys (1989) has clearly stated that &quot;difficulty increases with increased similarity of targets to nontargets and decreased similarity between nontargets&quot; (the sentence in their abstract). Here, &quot;similarity between nontargets&quot; is the same as the visual homogeneity defined here. Similar effects have been shown in Duncan (1989) and Nagy, Neriani, and Young (2005). See also the inconsistent results in Nagy&amp; Thomas, 2003, Vicent, Baddeley, Troscianko&amp;Gilchrist, 2009. More recently, Wei Ji Ma has systematically investigated the effects of heterogeneous distractors in visual search. I think the introduction part of Wei Ji Ma's paper (2020) provides a nice summary of this line of research.</p><p>Thanks to the authors' revision, I now better understand the negative correlation. The between-distrator similarity mentioned above describes the heterogeneity of distractors WITHIN an image. However, if I understand it correctly, this study aims to address the negative correlation of reaction time and target-absent stimuli ACROSS images. In other words, why do humans show a shorter reaction time to an image of four pigeons than to an image of four dogs (as shown in Figure 2C), simply because the later image is closer to the reference point of the image space. In this sense, this negative correlation is indeed not the same as distractor heterogeneity. However, this is known as the saliency effect or oddball effects. For example, it seems quite natural to me that humans respond faster to a fish image if the image set contains many images of four-leg dogs that look very different from fish. If this is indeed a saliency effect, why should we define a new term &quot;visual homogeneity&quot;?</p><p>(6) The section &quot;key predictions&quot; is quite straightforward. I understand the logic of positive and negative correlations. However, what is the physical meaning of &quot;decision boundary&quot; (Fig. 1G) here? How does the &quot;decision boundary&quot; map on the image space?</p><p>(7) In my opinion, one of the advantages of this study is the fMRI dataset, which is valuable because previous studies did not collect fMRI data. The key contribution may be the novel brain region associated with display heterogeneity. If this is the case, I would suggest using a more parametric way to measure this region. For example, one can use Gabor stimuli and systematically manipulate the variations of multiple Gabor stimuli, the same logic also applies to motion direction. If this study uses static Gabor, random dot motion, object images that span from low-level to high-level visual stimuli, and consistently shows that the stimulus heterogeneity is encoded in one brain region, I would say this finding is valuable. But this sounds another experiment. In other words, it is insufficient to claim a new brain region given the current form of the manuscript.</p><p>References:</p><p>* Duncan, J., &amp; Humphreys, G. W. (1989). Visual search and stimulus similarity. Psychological Review, 96(3), 433-458. doi: 10.1037/0033-295x.96.3.433</p><p>* Duncan, J. (1989). Boundary conditions on parallel processing in human vision. Perception, 18(4), 457-469. doi: 10.1068/p180457</p><p>* Nagy, A. L., Neriani, K. E., &amp; Young, T. L. (2005). Effects of target and distractor heterogeneity on search for a color target. Vision Research, 45(14), 1885-1899. doi: 10.1016/j.visres.2005.01.007</p><p>* Nagy, A. L., &amp; Thomas, G. (2003). Distractor heterogeneity, attention, and color in visual search. Vision Research, 43(14), 1541-1552. doi: 10.1016/s0042-6989(03)00234-7</p><p>* Vincent, B., Baddeley, R., Troscianko, T., &amp; Gilchrist, I. (2009). Optimal feature integration in visual search. Journal of Vision, 9(5), 15-15. doi: 10.1167/9.5.15</p><p>* Singh, A., Mihali, A., Chou, W. C., &amp; Ma, W. J. (2023). A Computational Approach to Search in Visual Working Memory.</p><p>* Mihali, A., &amp; Ma, W. J. (2020). The psychophysics of visual search with heterogeneous distractors. BioRxiv, 2020-08.</p><p>* Calder-Travis, J., &amp; Ma, W. J. (2020). Explaining the effects of distractor statistics in visual search. Journal of Vision, 20(13), 11-11.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93033.4.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary of the review process from the Reviewing Editor:</p><p>The authors and the reviewers did not agree on several important points made in this paper. The reviewers were critical of the operationalisation of the concept of visual homogeneity (VH), and questioned its validity. For instance, they found it unsatisfying that VH was not calculated on the basis of images themselves, but on the basis of reaction times instead. The authors responded by providing further explanation and argumentation for the importance of this novel concept, but the reviewers were not persuaded. The reviewers also pointed out some data features that did not fit the theory (e.g., overlapping VH between present and absent stimuli), which the authors acknowledge as a point that needs further refining. Finally, the reviewers pointed out that the new so-called visual homogeneity brain region does not overlap very much in the two studies, to which the authors have responded that it is remarkable that there is even partial overlap, given the many confounding differences between the two studies. Altogether, the authors have greatly elaborated their case for VH as an important concept, but the reviewers were not persuaded, and we conclude that the current evidence does not yet meet the high bar for declaring that a novel image property, visual homogeneity, is computed in a localised brain region.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93033.4.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jacob</surname><given-names>Georgin</given-names></name><role specific-use="author">Author</role><aff><institution>Indian Institute of Science Bangalore</institution><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff></contrib><contrib contrib-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>SP</surname><given-names>Arun</given-names></name><role specific-use="author">Author</role><aff><institution>Indian Institute of Science Bangalore</institution><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><p>We are grateful to the editors and reviewers for their careful reading and constructive comments. We have now done our best to respond to them fully through additional analyses and text revisions. In the sections below, the original reviewer comments are in black, and our responses are in red.</p><p>To summarize, the major changes in this round of review are as follows:</p><p>(1) We have included a new introductory figure (Figure 1) to explain the distinction between feature-based tasks and property-based tasks.</p><p>(2) We have included a section on “key predictions” and a section on “overview of this study” in the Introduction to clearly delineate our key predictions and provide a overview of our study.</p><p>(3) We have included additional analyses to address the reviewers’ concerns about circularity in Experiments 1 &amp; 2. We show that distance-to-center or visual homogeneity computations performed on object representations obtained from deep networks (instead of the perceptual dissimilarities from Experiment 1) also yields comparable predictions of target-present and target-absent responses in Experiment 2.</p><p>(4) We have extensively reworked the manuscript wherever possible to address the specific concerns raised by the reviewers.</p><p>We hope that the revised manuscript adequately addresses the concerns raised in this round of review, and we look forward to a positive assessment.</p><disp-quote content-type="editor-comment"><p><bold>eLife Assessment</bold></p><p>This study uses carefully designed experiments to generate a useful behavioural and neuroimaging dataset on visual cognition. The results provide solid evidence for the involvement of higher-order visual cortex in processing visual oddballs and asymmetry. However, the evidence provided for the very strong claims of homogeneity as a novel concept in vision science, separable from existing concepts such as target saliency, is inadequate.</p></disp-quote><p>Thank you for your positive assessment. We agree that visual homogeneity is similar to existing concepts such as target saliency, memorability etc. We have proposed it as a separate concept because visual homogeneity has an independent empirical measure (the reciprocal of target-absent search time in oddball search, or the reciprocal of same response time in a same-different task, etc) that may or may not be the same as other empirical measures such as saliency and memorability. Investigating these possibilities is beyond the scope of our study but would be interesting for future work. We have now clarified this in the revised manuscript (Discussion, p. 42).</p><p>However, we’d like to emphasize that the question of whether visual homogeneity is novel or related to existing concepts misses entirely the key contribution of our study.</p><p>Our key contribution is a quantitative, falsifiable model for how the brain could be solving property-based tasks like same-different, oddball or symmetry. Most theories of decision making consider feature-based tasks where there is a well-defined feature space and decision variable. Property-based tasks pose a significant challenge to standard theories since it is not clear how these tasks could be solved. In fact, oddball search, same-different and symmetry tasks have been considered so different that they are rarely even mentioned in the same study. Our study represents a unifying framework showing that all three tasks can be understood as solving the same underlying fundamental problem, and presents evidence in favor of this solution.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The authors define a new metric for visual displays, derived from psychophysical response times, called visual homogeneity (VH). They attempt to show that VH is explanatory of response times across multiple visual tasks. They use fMRI to find visual cortex regions with VH-correlated activity. On this basis, they declare a new visual region in human brain, area VH, whose purpose is to represent VH for the purpose of visual search and symmetry tasks.</p></disp-quote><p>Thank you for your accurate and positive assessment.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>The authors present carefully designed experiments, combining multiple types of visual judgments and multiple types of visual stimuli with concurrent fMRI measurements. This is a rich dataset with many possibilities for analysis and interpretation.</p></disp-quote><p>Thank you for your accurate and positive assessment.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The datasets presented here should provide a rich basis for analysis. However, in this version of the manuscript, I believe that there are major problems with the logic underlying the authors' new theory of visual homogeneity (VH), with the specific methods they used to calculate VH, and with their interpretation of psychophysical results using these methods. These problems with the coherency of VH as a theoretical construct and metric value make it hard to interpret the fMRI results based on searchlight analysis of neural activity correlated with VH.</p></disp-quote><p>We respectfully disagree with your concerns, and have done our best to respond to them fully below.</p><disp-quote content-type="editor-comment"><p>In addition, the large regions of VH correlations identified in Experiments 1 and 2 vs. Experiments 3 and 4 are barely overlapping. This undermines the claim that VH is a universal quantity, represented in a newly discovered area of visual cortex, that underlies a wide variety of visual tasks and functions.</p></disp-quote><p>We respectfully disagree with your assertion. First of all, there is partial overlap between the VH regions, for which there are several other obvious explanations that must be considered first before dismissing VH outright as a flawed construct. We acknowledge these alternatives in the Results (p. 27), and the relevant text is reproduced below.</p><p>“We note that it is not straightforward to interpret the overlap between the VH regions identified in Experiments 2 &amp; 4. The lack of overlap could be due to stimulus differences (natural images in Experiment 2 vs silhouettes in Experiment 4), visual field differences (items in the periphery in Experiment 2 vs items at the fovea in Experiment 4) and even due to different participants in the two experiments. There is evidence supporting all these possibilities: stimulus differences (Yue et al., 2014), visual field differences (Kravitz et al., 2013) as well as individual differences can all change the locus of neural activations in object-selective cortex (Weiner and Grill-Spector, 2012a; Glezer and Riesenhuber, 2013). We speculate that testing the same participants on search and symmetry tasks using similar stimuli and display properties would reveal even larger overlap in the VH regions that drive behavior.”</p><disp-quote content-type="editor-comment"><p>Maybe I have missed something, or there is some flaw in my logic. But, absent that, I think the authors should radically reconsider their theory, analyses, and interpretations, in light of detailed comments below, in order to make the best use of their extensive and valuable datasets combining behavior and fMRI. I think doing so could lead to a much more coherent and convincing paper, albeit possibly supporting less novel conclusions.</p></disp-quote><p>We respectfully disagree with your assessment, and we hope that our detailed responses below will convince you of the merit of our claims.</p><disp-quote content-type="editor-comment"><p>THEORY AND ANALYSIS OF VH</p><p>(1) VH is an unnecessary, complex proxy for response time and target-distractor similarity.</p><p>VH is defined as a novel visual quality, calculable for both arrays of objects (as studied in Experiments 1-3) and individual objects (as studied in Experiment 4). It is derived from a center-to-distance calculation in a perceptual space. That space in turn is derived from multi-dimensional scaling of response times for target-distractor pairs in an oddball detection task (Experiments 1 and 2) or in a same different task (Experiments 3 and 4). Proximity of objects in the space is inversely proportional to response times for arrays in which they were paired. These response times are higher for more similar objects. Hence, proximity is proportional to similarity. This is visible in Fig. 2B as the close clustering of complex, confusable animal shapes.</p><p>VH, i.e. distance-to-center, for target-present arrays is calculated as shown in Fig. 1C, based on a point on the line connecting target and distractors. The authors justify this idea with previous findings that responses to multiple stimuli are an average of responses to the constituent individual stimuli. The distance of the connecting line to the center is inversely proportional to the distance between the two stimuli in the pair, as shown in Fig. 2D. As a result, VH is inversely proportional to distance between the stimuli and thus to stimulus similarity and response times. But this just makes VH a highly derived, unnecessarily complex proxy for target-distractor similarity and response time. The original response times on which the perceptual space is based are far more simple and direct measures of similarity for predicting response times.</p></disp-quote><p>Thank you for carefully thinking through our logic. We agree that a distance-to-centre calculation is entirely unnecessary as an explanation for target-present visual search. The difficulty of target-present search is already known to be directly proportional to the similarity between target and distractor, so there is nothing new to explain here.</p><p>However, this is a narrow and selective interpretation of our findings because you are focusing only on our results on target-present searches, which are only half of all our data. The other half is the target-absent responses which previously have had no clear explanation. You are also missing the fact that we are explaining same-different and symmetry tasks as well using the same visual homogeneity computation.</p><p>We urge you to think more deeply about the problem of how to decide whether an oddball is present or not in the first place. How do we actually solve this task? There must be some underlying representation and decision process. Our study shows that a distance-to-centre computation can actually serve as a decision variable to solve disparate property-based visual tasks. These tasks pose a major challenge to standard models of decision making, because the underlying representation and decision variable have been unclear. Our study resolves this challenge by proposing a novel computation that can be used by the brain to solve all these disparate tasks, and bring these tasks into the ambit of standard theories of decision making.</p><p>Our results also explain several interesting puzzles in the literature. If oddball search was driven only by target-distractor similarity, the time taken to respond when a target is absent should not vary at all, and should actually take longer than all target-present searches. But in fact, systematic variations in target-absent times have been observed always in the literature, but have never been explained using any theoretical models. Our results explain why target-absent times vary systematically – it is due to visual homogeneity.</p><p>Similarly, in same-different tasks, participants are known to take longer to make a “different” response when the two items differ only slightly. By this logic, they should take the longest to make a “same” response, but in fact, paradoxically, participants are actually faster to make “same” responses. This fast-same effect has been noted several times, but never explained using any models. Our results provide an explanation of why “same” responses to an image vary systematically – it is due to visual homogeneity.</p><p>Finally, in symmetry tasks, symmetric objects evoke fast responses, and this has always been taken as evidence for special symmetry computations in the brain. But we show that the same distance-to-center computation can explain both responses to symmetric and asymmetric objects. Thus there is no need for a special symmetry computation in the brain.</p><disp-quote content-type="editor-comment"><p>(2) The use of VH derived from Experiment 1 to predict response times in Experiment 2 is circular and does not validate the VH theory.</p><p>The use of VH, a response time proxy, to predict response times in other, similar tasks, using the same stimuli, is circular. In effect, response times are being used to predict response times across two similar experiments using the same stimuli. Experiment 1 and the target present condition of Experiment 2 involve the same essential task of oddball detection. The results of Experiment 1 are converted into VH values as described above, and these are used to predict response times in experiment 2 (Fig. 2F). Since VH is a derived proxy for response values in Experiment 1, this prediction is circular, and the observed correlation shows only consistency between two oddball detection tasks in two experiments using the same stimuli.</p></disp-quote><p>You are indeed correct in noting that both Experiment 1 &amp; 2 involve oddball search, and so at the superficial level, it looks circular that the oddball search data of Experiment 1 is being used to explain the oddball search data of Experiment 2.</p><p>However a deeper scrutiny reveals more fundamental differences: Experiment 1 consisted of only oddball search with the target appearing on the left or right, whereas Experiment 2 consisted of oddball search with the target either present or completely absent. In fact, we were merely using the search dissimilarities from Experiment 1 to reconstruct the underlying object representation, because it is well known that neural dissimilarities are predicted well by search dissimilarities (Sripati &amp; Olson, 2009; Zhivago et al, 2014).</p><p>To thoroughly refute any lingering concern about circularity, we reasoned that the model predictions for Experiment 2 could have been obtained by a distance-to-center computation on any brain like object representation. To this end, we used object representations from deep neural networks pretrained on object categorization, whose representations are known to match well with the brain, and asked if a distance-to-centre computation on these representations could predict the search data in Experiment 2. This was indeed the case, and these results are now included an additional section in Supplementary Material (Section S1).</p><disp-quote content-type="editor-comment"><p>(3) The negative correlation of target-absent response times with VH as it is defined for target-absent arrays, based on distance of a single stimulus from center, is uninterpretable without understanding the effects of center-fitting. Most likely, center-fitting and the different VH metric for target-absent trials produce an inverse correlation of VH with target-distractor similarity.</p></disp-quote><p>Unfortunately, as we have mentioned above, target-distractor similarity cannot explain how target-absent searches behave, since there is no distractor in such searches.</p><p>We do understand your broader concern about the center-fitting algorithm itself. We performed a number of additional analyses to confirm the generality of our results and reject alternate explanations – these are summarized in a new section titled “Confirming the generality of visual homogeneity” (p. 12), and the section is reproduced below for your convenience.</p><p>“Confirming the generality of visual homogeneity</p><p>We performed several additional analyses to confirm the generality of our results, and to reject alternate explanations.</p><p>First, it could be argued that our results are circular because they involve taking oddball search times from Experiment 1 and using them to explain search response times in Experiment 2. This is a superficial concern since we are using the search dissimilarities from Experiment 1 only as a proxy for the underlying neural representation, based on previous reports that neural dissimilarities closely match oddball search dissimilarities (Sripati and Olson, 2010; Zhivago and Arun, 2014). Nonetheless, to thoroughly refute this possibility, we reasoned that we would get similar predictions of the target present/absent responses in Experiment using any other brain-like object representation. To confirm this, we replaced the object representations derived from Experiment 1 with object representations derived from deep neural networks pretrained for object categorization, and asked if distance-to-center computations could predict the target present/absent responses in Experiment 2. This was indeed the case (Section S1).</p><p>Second, we wondered whether the nonlinear optimization process of finding the best-fitting center could be yielding disparate optimal centres each time. To investigate this, we repeated the optimization procedure with many randomly initialized starting points, and obtained the same best-fitting center each time (see Methods).</p><p>Third, to confirm that the above model fits are not due to overfitting, we performed a leave-one-out cross validation analysis. We left out all target-present and target-absent searches involving a particular image, and then predicted these searches by calculating visual homogeneity estimated from all other images. This too yielded similar positive and negative correlations (r = 0.63, p &lt; 0.0001 for target-present, r = -0.63, p &lt; 0.001 for target-absent).</p><p>Fourth, if heterogeneous displays indeed elicit similar neural responses due to mixing, then their average distance to other objects must be related to their visual homogeneity. We confirmed that this was indeed the case, suggesting that the average distance of an object from all other objects in visual search can predict visual homogeneity (Section S1).</p><p>Fifth, the above results are based on taking the neural response to oddball arrays to be the average of the target and distractor responses. To confirm that averaging was indeed the optimal choice, we repeated the above analysis by assuming a range of relative weights between the target and distractor. The best correlation was obtained for almost equal weights in the lateral occipital (LO) region, consistent with averaging and its role in the underlying perceptual representation (Section S1).</p><p>Finally, we performed several additional experiments on a larger set of natural objects as well as on silhouette shapes. In all cases, present/absent responses were explained using visual homogeneity (Section S2).”</p><disp-quote content-type="editor-comment"><p>The construction of the VH perceptual space also involves fitting a &quot;center&quot; point such that distances to center predict response times as closely as possible. The effect of this fitting process on distance-to-center values for individual objects or clusters of objects is unknowable from what is presented here. These effects would depend on the residual errors after fitting response times with the connecting line distances. The center point location and its effects on distance-to-center of single objects and object clusters are not discussed or reported here.</p></disp-quote><p>While it is true that the optimal center needs to be found by fitting to the data, there no particular mystery to the algorithm: we are simply performing a standard gradient-descent to maximize the fit to the data. We have described the algorithm clearly and are making our codes public. We find the algorithm to yield stable optimal centers despite many randomly initialized starting points. We find the optimal center to be able to predict responses to entirely novel images that were excluded during model training. We are making no assumption about the location of centre with respect to individual points. Therefore, we see no cause for concern regarding the center-finding algorithm.</p><disp-quote content-type="editor-comment"><p>Yet, this uninterpretable distance-to-center of single objects is chosen as the metric for VH of target-absent displays (VHabsent). This is justified by the idea that arrays of a single stimulus will produce an average response equal to one stimulus of the same kind. But it is not logically clear why response strength to a stimulus should be a metric for homogeneity of arrays constructed from that stimulus, or even what homogeneity could mean for a single stimulus from this set. And it is not clear how this VHabsent metric based on single stimuli can be equated to the connecting line VH metric for stimulus pairs, i.e. VHpresent, or how both could be plotted on a single continuum.</p></disp-quote><p>Most visual tasks, such as finding an animal, are thought to involve building a decision boundary on some underlying neural representation. Even visual search has been portrayed as a signal-detection problem where a particular target is to be discriminated from a distractor. However none of these formulations work in the case of property-based visual tasks, where there is no unique feature to look for.</p><p>We are proposing that, when we view a search array, the neural response to the search array can be deduced from the neural responses to the individual elements using well known rules, and that decisions about an oddball target being present or absent can be made by computing the distance of this neural response from some canonical mean firing rate of a population of neurons. This distance to center computation is what we denote as visual homogeneity. We have revised our manuscript throughout to make this clearer and we hope that this helps you understand the logic better.</p><disp-quote content-type="editor-comment"><p>It is clear, however, what *should* be correlated with difficulty and response time in the target-absent trials, and that is the complexity of the stimuli and the numerosity of similar distractors in the overall stimulus set. Complexity of the target, similarity with potential distractors, and number of such similar distractors all make ruling out distractor presence more difficult. The correlation seen in Fig. 2G must reflect these kinds of effects, with higher response times for complex animal shapes with lots of similar distractors and lower response times for simpler round shapes with fewer similar distractors.</p></disp-quote><p>You are absolutely correct that the stimulus complexity should matter, but there are no good empirically derived measures for stimulus complexity, other than subjective ratings which are complex on their own and could be based on any number of other cognitive and semantic factors. But considering what factors are correlated with target-absent response times is entirely different from asking what decision variable or template is being used by participants to solve the task.</p><disp-quote content-type="editor-comment"><p>The example points in Fig. 2G seem to bear this out, with higher response times for the deer stimulus (complex, many close distractors in the Fig. 2B perceptual space) and lower response times for the coffee cup (simple, few close distractors in the perceptual space). While the meaning of the VH scale in Fig. 2G, and its relationship to the scale in Fig. 2F, are unknown, it seems like the Fig. 2G scale has an inverse relationship to stimulus complexity, in contrast to the expected positive relationship for Fig. 2F. This is presumably what creates the observed negative correlation in Fig. 2G.</p><p>Taken together, points 1-3 suggest that VHpresent and VHabsent are complex, unnecessary, and disconnected metrics for understanding target detection response times. The standard, simple explanation should stand. Task difficulty and response time in target detection tasks, in both present and absent trials, are positively correlated with target-distractor similarity.</p></disp-quote><p>We strongly disagree. Your assessment seems to be based on only considering target-present searches, which are of course driven by target-distractor similarity. Your argument is flawed because systematic variations in target-absent trials cannot be linked to any target-distractor similarity since there are no targets in the first place in such trials.</p><p>We have shown that target-absent response times are in fact, independent of experimental context, which means that they index an image property that is independent of any reference target (Results, p. 15; Section S4). This property is what we define as visual homogeneity.</p><disp-quote content-type="editor-comment"><p>I think my interpretations apply to Experiments 3 and 4 as well, although I find the analysis in Fig. 4 especially hard to understand. The VH space in this case is based on Experiment 3 oddball detection in a stimulus set that included both symmetric and asymmetric objects. But the response times for a very different task in Experiment 4, a symmetric/asymmetric judgment, are plotted against the axes derived from Experiment 3 (Fig. 4F and 4G). It is not clear to me why a measure based on oddball detection that requires no use of symmetry information should be predictive of within-stimulus symmetry detection response times. If it is, that requires a theoretical explanation not provided here.</p></disp-quote><p>We were simply using an oddball detection task to construct the underlying object representation, on the basis of observations that search dissimilarities are strongly correlated with neural dissimilarities. In Section S1, we show that similar results could have been obtained using other object representations such as deep networks, as long as the representation is brain-like.</p><disp-quote content-type="editor-comment"><p>(4) Contrary to the VH theory, same/different tasks are unlikely to depend on a decision boundary in the middle of a similarity or homogeneity continuum.</p></disp-quote><p>We have provided empirical proof for our claims, by showing that target-present response times in a visual search task are correlated with “different” responses in the same-different task, and that target-absent response times in the visual search task are correlated with “same” responses in the same-different task (Section S4).</p><disp-quote content-type="editor-comment"><p>The authors interpret the inverse relationship of response times with VHpresent and VHabsent, described above, as evidence for their theory. They hypothesize, in Fig. 1G, that VHpresent and VHabsent occupy a single scale, with maximum VHpresent falling at the same point as minimum VHabsent. This is not borne out by their analysis, since the VHpresent and VHabsent value scales are mainly overlapping, not only in Experiments 1 and 2 but also in Experiments 3 and 4. The authors dismiss this problem by saying that their analyses are a first pass that will require future refinement. Instead, the failure to conform to this basic part of the theory should be a red flag calling for revision of the theory.</p></disp-quote><p>Again, the opposite correlations between target present/absent search times with VH are the crucial empirical validation of our claims that a distance-to-center calculation explain how we perform these property-based tasks. The VH predictions do not fully explain the data. We have explicitly acknowledged this shortcoming, so we are hardly dismissing it as a problem.</p><disp-quote content-type="editor-comment"><p>The reason for this single scale is that the authors think of target detection as a boundary decision task, along a single scale, with a decision boundary somewhere in the middle, separating present and absent. This model makes sense for decision dimensions or spaces where there are two categories (right/left motion; cats vs. dogs), separated by an inherent boundary (equal left/right motion; training-defined cat/dog boundary). In these cases, there is less information near the boundary, leading to reduced speed/accuracy and producing a pattern like that shown in Fig. 1G.</p></disp-quote><p>Finding an oddball, deciding if two items are same or different and symmetry tasks are disparate visual tasks that do not fit neatly into standard models of decision making. The key conceptual advance of our study is that we propose a plausible neural representation and decision variable that allow all three property-based visual tasks to be reconciled with standard models of decision making.</p><disp-quote content-type="editor-comment"><p>This logic does not hold for target detection tasks. There is no inherent middle point boundary between target present and target absent. Instead, in both types of trial, maximum information is present when target and distractors are most dissimilar, and minimum information is present when target and distractors are most similar. The point of greatest similarity occurs at then limit of any metric for similarity. Correspondingly, there is no middle point dip in information that would produce greater difficulty and higher response times. Instead, task difficulty and response times increase monotonically with similarity between targets and distractors, for both target present and target absent decisions. Thus, in Figs. 2F and 2G, response times appear to be highest for animals, which share the largest numbers of closely similar distractors.</p></disp-quote><p>Your alternative explanation rests on vague factors like “maximum information” which cannot be quantified. By contrast we are proposing a concrete, falsifiable model for three property-based tasks – same/different, oddball present/absent and object symmetry. Any argument based solely on item similarity to explain visual search or symmetry responses cannot explain systematic variations observed for target-absent arrays and for symmetric objects, for the reasons explained earlier.</p><disp-quote content-type="editor-comment"><p>DEFINITION OF AREA VH USING fMRI</p><p>(1) The area VH boundaries from different experiments are nearly completely non-overlapping.</p><p>In line with their theory that VH is a single continuum with a decision boundary somewhere in the middle, the authors use fMRI searchlight to find an area whose responses positively correlate with homogeneity, as calculated across all of their target present and target absent arrays. They report VH-correlated activity in regions anterior to LO. However, the VH defined by symmetry Experiments 3 and 4 (VHsymmetry) is substantially anterior to LO, while the VH defined by target detection Experiments 1 and 2 (VHdetection) is almost immediately adjacent to LO. Fig. S13 shows that VHsymmetry and VHdetection are nearly non-overlapping. This is a fundamental problem with the claim of discovering a new area that represents a new quantity that explains response times across multiple visual tasks. In addition, it is hard to understand why VHsymmetry does not show up in a straightforward subtraction between symmetric and asymmetric objects, which should show a clear difference in homogeneity.</p></disp-quote><p>We respectfully disagree. The partial overlap between the VH regions identified in Experiments 1 &amp; 2 can hardly be taken as evidence against the quantity VH itself, because there are several other obvious alternate explanations for this partial overlap, as summarized earlier as well. The VH region does show up in a straightforward subtraction between symmetric and asymmetric objects (Section S7), so we are not sure what the Reviewer is referring to here.</p><disp-quote content-type="editor-comment"><p>(2) It is hard to understand how neural responses can be correlated with both VHpresent and VHabsent.</p><p>The main paper results for VHdetection are based on both target-present and target-absent trials, considered together. It is hard to interpret the observed correlations, since the VHpresent and VHabsent metrics are calculated in such different ways and have opposite correlations with target similarity, task difficulty, and response times (see above). It may be that one or the other dominates the observed correlations. It would be clarifying to analyze correlations for target-present and target-absent trials separately, to see if they are both positive and correlated with each other.</p></disp-quote><p>Thanks for raising this point. We have now confirmed that the positive correlation between VH and neural response holds even when we do the analysis separately for target-present and -absent searches correlation between neural response in VH region and visual homogeneity (n = 32, r = 0.66, p &lt; 0.0005 for target-present searches &amp; n = 32, r = 0.56, p &lt; 0.005 for target-absent searches).</p><disp-quote content-type="editor-comment"><p>(3) Definition of the boundaries and purpose of a new visual area in the brain requires circumspection, abundant and convergent evidence, and careful controls.</p><p>Even if the VH metric, as defined and calculated by the authors here, is a meaningful quantity, it is a bold claim that a large cortical area just anterior to LO is devoted to calculating this metric as its major task. Vision involves much more than target detection and symmetry detection. Cortex anterior to LO is bound to perform a much wider range of visual functionalities. If the reported correlations can be clarified and supported, it would be more circumspect to treat them as one byproduct of unknown visual processing in cortex anterior to LO, rather than treating them as the defining purpose for a large area of visual cortex.</p></disp-quote><p>We totally agree with you that reporting a new brain region would require careful interpretation and abundant and converging evidence. However, this requires many studies worth of work, and historically category-selective regions like the FFA have achieved consensus only after they were replicated and confirmed across many studies. We believe our proposal for the computation of a quantity like visual homogeneity is conceptually novel, and our study represents a first step that provides some converging evidence (through replicable results across different experiments) for such a region. We have reworked our manuscript to make this point clearer (Discussion, p 32).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>This study proposes visual homogeneity as a novel visual property that enables observers perform to several seemingly disparate visual tasks, such as finding an odd item, deciding if two items are same, or judging if an object is symmetric. In Exp 1, the reaction times on several objects were measured in human subjects. In Exp 2, visual homogeneity of each object was calculated based on the reaction time data. The visual homogeneity scores predicted reaction times. This value was also correlated with the BOLD signals in a specific region anterior to LO. Similar methods were used to analyze reaction time and fMRI data in a symmetry detection task. It is concluded that visual homogeneity is an important feature that enables observers to solve these two tasks.</p></disp-quote><p>Thank you for your accurate and positive assessment.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>(1) The writing is very clear. The presentation of the study is informative.</p><p>(2) This study includes several behavioral and fMRI experiments. I appreciate the scientific rigor of the authors.</p></disp-quote><p>We are grateful to you for your balanced assessment and constructive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) My main concern with this paper is the way visual homogeneity is computed. On page 10, lines 188-192, it says: &quot;we then asked if there is any point in this multidimensional representation such that distances from this point to the target-present and target-absent response vectors can accurately predict the target-present and target-absent response times with a positive and negative correlation respectively (see Methods)&quot;. This is also true for the symmetry detection task. If I understand correctly, the reference point in this perceptual space was found by deliberating satisfying the negative and positive correlations in response times. And then on page 10, lines 200-205, it shows that the positive and negative correlations actually exist. This logic is confusing. The positive and negative correlations emerge only because this method is optimized to do so. It seems more reasonable to identify the reference point of this perceptual space independently, without using the reaction time data. Otherwise, the inference process sounds circular. A simple way is to just use the mean point of all objects in Exp 1, without any optimization towards reaction time data.</p></disp-quote><p>We disagree with you since the same logic applies to any curve-fitting procedure. When we fit data to a straight line, we are finding the slope and intercept that minimizes the error between the data and the straight line, but we would hardly consider the process circular when a good fit is achieved – in fact we take it as a confirmation that the data can be fit linearly. In the same vein, we would not have observed a good fit to the data, if there did not exist any good reference point relative to which the distances of the target-present and target-absent search arrays predicted these response times.</p><p>In Section S2, we show that the visual homogeneity estimates for each object is strongly correlated with the average distance of each object to all other objects (r = 0.84, p&lt;0.0005, Figure S1).</p><p>We have performed several additional analyses to confirm the generality of our results and to reject alternate explanations (see Results, p. 12, Section titled “Confirming the generality of visual homogeneity”). In particular, to confirm that the results we obtained are not due to overfitting, we performed a cross-validation analysis, where we removed all searches involving a particular image and predicted these response times using visual homogeneity. This too revealed a significant model correlation confirming that our results are not due to overfitting.</p><disp-quote content-type="editor-comment"><p>(2) Visual homogeneity (at least given the current from) is an unnecessary term. It is similar to distractor heterogeneity/distractor variability/distractor statics in literature. However, the authors attempt to claim it as a novel concept. The title is &quot;visual homogeneity computations in the brain enable solving generic visual tasks&quot;. The last sentence of the abstract is &quot;a NOVEL IMAGE PROPERTY, visual homogeneity, is encoded in a localized brain region, to solve generic visual tasks&quot;. In the significance, it is mentioned that &quot;we show that these tasks can be solved using a simple property WE DEFINE as visual homogeneity&quot;. If the authors agree that visual homogeneity is not new, I suggest a complete rewrite of the title, abstract, significance, and introduction.</p></disp-quote><p>We respectfully disagree that visual homogeneity is an unnecessary term. Please see our comments to Reviewer 1 above. Just like saliency and memorability can be measured empirically, we propose that visual homogeneity can be empirically measured as the reciprocal of the target-absent search time in a search task, or as the reciprocal of the “same” response time in a same-different task. Understanding how these three quantities interact will require measuring them empirically for an identical set of images, which is beyond the scope of this study but an interesting possibility for future work.</p><disp-quote content-type="editor-comment"><p>(3) Also, &quot;solving generic tasks&quot; is another overstatement. The oddball search tasks, same-different tasks, and symmetric tasks are only a small subset of many visual tasks. Can this &quot;quantitative model&quot; solve motion direction judgment tasks, visual working memory tasks? Perhaps so, but at least this manuscript provides no such evidence. On line 291, it says &quot;we have proposed that visual homogeneity can be used to solve any task that requires discriminating between homogeneous and heterogeneous displays&quot;. I think this is a good statement. A title that says &quot;XXXX enable solving discrimination tasks with multi-component displays&quot; is more acceptable. The phrase &quot;generic tasks&quot; is certainly an exaggeration.</p></disp-quote><p>Thank you for your suggestion. We have now replaced the term “generic tasks” with the term property-based tasks, which we feel is more appropriate and reflect the fact that oddball search, same-different and symmetry tasks all involve looking for a specific image property.</p><disp-quote content-type="editor-comment"><p>(4) If I understand it correctly, one of the key findings of this paper is &quot;the response times for target-present searches were positively correlated with visual homogeneity. By contrast, the response times for target-absent searches were negatively correlated with visual homogeneity&quot; (lines 204-207). I think the authors have already acknowledged that the positive correlation is not surprising at all because it reflects the classic target-distractor similarity effect. But the authors claim that the negative correlations in target-absent searches is the true novel finding.</p><p>(5) I would like to make it clear that this negative correlation is not new either. The seminal paper by Duncan and Humphreys (1989) has clearly stated that &quot;difficulty increases with increased similarity of targets to nontargets and decreased similarity between nontargets&quot; (the sentence in their abstract). Here, &quot;similarity between nontargets&quot; is the same as the visual homogeneity defined here. Similar effects have been shown in Duncan (1989) and Nagy, Neriani, and Young (2005). See also the inconsistent results in Nagy &amp; Thomas, 2003, Vicent, Baddeley, Troscianko &amp; Gilchrist, 2009. More recently, Wei Ji Ma has systematically investigated the effects of heterogeneous distractors in visual search. I think the introduction part of Wei Ji Ma's paper (2020) provides a nice summary of this line of research. I am surprised that these references are not mentioned at all in this manuscript (except Duncan and Humphreys, 1989).</p></disp-quote><p>You are right in noting that Duncan and Humphreys (1989) propose that searches are more difficult when nontargets are dissimilar. However, since our searches have identical distractors, the similarity between nontargets is always constant across target-absent searches, and therefore this cannot predict any systematic variation in target-absent search that is observed in our data. By contrast, our results explain both target-absent searches and target-present searches.</p><p>Thank you for pointing us to previous work. These studies show that it is not just the average distractor similarity but the statistics of the distractor similarity that drive visual search. However these studies do not explain why target-absent searches should vary systematically.</p><disp-quote content-type="editor-comment"><p>(6) If the key contribution is the quantitative model, the study should be organized in a different way. Although the findings of positive and negative correlations are not novel, it is still good to propose new models to explain classic phenomena. I would like to mention the three studies by Wei Ji Ma (see below). In these studies, Bayesian observer models were established to account for trial-by-trial behavioral responses. These computational models can also account for the set-size effect, behavior in both localization and detection tasks. I see much more scientific rigor in their studies. Going back to the quantitative model in this paper, I am wondering whether the model can provide any qualitative prediction beyond the positive and negative correlations? Can the model make qualitative predictions that differ from those of Wei Ji's model? If not, can the authors show that the model can quantitatively better account for the data than existing Bayesian models? We should evaluate a model either qualitatively or quantitatively.</p></disp-quote><p>Thank you for pointing us to prior work by Wei Ji Ma. These studies systematically examined visual search for a target among heterogeneous distractors using simple parametric stimuli and a Bayesian modeling framework. By contrast, our experiments involve searching for single oddball targets among multiple identical distractors, so it is not clear to us that the Wei Ji Ma models can be easily used to generate predictions about these searches used in our study.</p><p>We are not sure what you mean by offering quantitative predictions beyond positive and negative correlations. We have tried to explain systematic variation in target-present and target-absent response times using a model of how these decisions are being made. Our model explains a lot of systematic variation in the data for both types of decisions.</p><disp-quote content-type="editor-comment"><p>(7) In my opinion, one of the advantages of this study is the fMRI dataset, which is valuable because previous studies did not collect fMRI data. The key contribution may be the novel brain region associated with display heterogeneity. If this is the case, I would suggest using a more parametric way to measure this region. For example, one can use Gabor stimuli and systematically manipulate the variations of multiple Gabor stimuli, the same logic also applies to motion direction. If this study uses static Gabor, random dot motion, object images that span from low-level to high-level visual stimuli, and consistently shows that the stimulus heterogeneity is encoded in one brain region, I would say this finding is valuable. But this sounds like another experiment. In other words, it is insufficient to claim a new brain region given the current form of the manuscript.</p></disp-quote><p>We agree that parametric stimulus manipulations are important for studying early visual areas where stimulus dimensions are known (e.g. orientation, spatial frequency). Using parametric stimulus manipulations for more complex stimuli is fraught with issues because the underlying representation may not be encoding the dimensions being manipulated. This is the reason why we attempted to recover the underlying neural representation using dissimilarities measured using visual search, and then asked whether a decision making process operating on this underlying representation can explain how decisions are made. Therefore we disagree that parametric stimulus manipulations are the only way to obtain insight into such tasks.</p><p>We have proposed a quantitative model that explains how decisions about target present and absent can be made through distance-to-center computations on an underlying object representation. We feel that the behavioural and the brain imaging results strongly point to a novel computation that is being performed in a localized region in the brain. These results represent an important first step in understanding how complex, property-based tasks are performed by the brain. We have revised our manuscript to make this point clearer.</p><p>REFERENCES</p><p>- Duncan, J., &amp; Humphreys, G. W. (1989). Visual search and stimulus similarity. Psychological Review, 96(3), 433-458. doi: 10.1037/0033-295x.96.3.433</p><p>- Duncan, J. (1989). Boundary conditions on parallel processing in human vision. Perception, 18(4), 457-469. doi: 10.1068/p180457</p><p>- Nagy, A. L., Neriani, K. E., &amp; Young, T. L. (2005). Effects of target and distractor heterogeneity on search for a color target. Vision Research, 45(14), 1885-1899. doi: 10.1016/j.visres.2005.01.007</p><p>- Nagy, A. L., &amp; Thomas, G. (2003). Distractor heterogeneity, attention, and color in visual search. Vision Research, 43(14), 1541-1552. doi: 10.1016/s0042-6989(03)00234-7</p><p>- Vincent, B., Baddeley, R., Troscianko, T., &amp; Gilchrist, I. (2009). Optimal feature integration in visual search. Journal of Vision, 9(5), 15-15. doi: 10.1167/9.5.15</p><p>- Singh, A., Mihali, A., Chou, W. C., &amp; Ma, W. J. (2023). A Computational Approach to Search in Visual Working Memory.</p><p>- Mihali, A., &amp; Ma, W. J. (2020). The psychophysics of visual search with heterogeneous distractors. BioRxiv, 2020-08.</p><p>- Calder-Travis, J., &amp; Ma, W. J. (2020). Explaining the effects of distractor statistics in visual search. Journal of Vision, 20(13), 11-11.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>The authors have not made substantive changes to address my major concerns. Instead, they have responded with arguments about why their original manuscript was good as written. I did not find these arguments persuasive. Given that, I've left my public review the same, since it still represents my opinions about the paper. Readers can judge which viewpoints are more persuasive.</p></disp-quote><p>We respectfully disagree: we have tried our best to address your concerns with additional analysis wherever feasible, and by acknowledging any limitations.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>(1) As I mentioned above, please consider rewriting title, abstract, introduction, and significance. Please remove the word &quot;visual homogeneity&quot; and instead use distractor heterogeneity/distractor variability/distractor statistics as often used in literature.</p></disp-quote><p>To clarify, visual homogeneity is NOT the same as distractor homogeneity. Visual homogeneity refers to a distance-to-center computation and represents an image-computable property that can vary systematically even when all distractors are identical. By contrast distractor heterogeneity varies only when distractors are different from each other.</p><disp-quote content-type="editor-comment"><p>(2) Better to remove the phrase &quot;generic tasks&quot;.</p></disp-quote><p>Thanks for your suggestions. We now refer to these tasks as property-based tasks.</p><disp-quote content-type="editor-comment"><p>(3) Better to explicitly specify the predictions made by the quantitative model beyond positive and negative correlations.</p></disp-quote><p>The predictions of the quantitative model are to explain systematic variation in the response times. We are not sure what else is there to predict in the response times.</p><disp-quote content-type="editor-comment"><p>(4) If the quantitative model is the key contribution, better to highlight the details and algorithmic contribution of the model, and show the advantage of this model either qualitatively and quantitatively.</p></disp-quote><p>Please see our responses above. Our quantitative model explains behavior and brain imaging data on three disparate tasks – the same/different, oddball visual search and symmetry tasks.</p><disp-quote content-type="editor-comment"><p>(5) If the new brain region is the key contribution, better to downplay the quantitative model.</p></disp-quote><p>Please see our responses above. Our quantitative model explains behavior and brain imaging data on three disparate tasks – the same/different, oddball visual search and symmetry tasks.</p></body></sub-article></article>