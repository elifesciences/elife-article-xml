<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">61909</article-id><article-id pub-id-type="doi">10.7554/eLife.61909</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-141550"><name><surname>Kane</surname><given-names>Gary A</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7703-5055</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-201764"><name><surname>Lopes</surname><given-names>Gonçalo</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0731-4945</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-217969"><name><surname>Saunders</surname><given-names>Jonny L</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-200232"><name><surname>Mathis</surname><given-names>Alexander</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3777-2202</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-187946"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7368-4456</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">The Rowland Institute at Harvard</institution>, <institution>Harvard University</institution>, <addr-line><named-content content-type="city">Cambridge</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution>NeuroGears</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><institution content-type="dept">Institute of Neuroscience, Department of Psychology</institution>, <institution>University of Oregon</institution>, <addr-line><named-content content-type="city">Eugene</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><institution content-type="dept">Life Sciences</institution>, <institution>EPFL</institution>, <addr-line><named-content content-type="city">Geneva</named-content></addr-line>, <country>Switzerland</country></aff><aff id="aff5"><institution content-type="dept">Brain Mind Institute</institution>, <institution>EPFL</institution>, <addr-line><named-content content-type="city">Genève</named-content></addr-line>, <country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-30091"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing editor</role><aff><institution>Emory University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>mackenzie.mathis@epfl.ch</email> (MM);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>08</day><month>12</month><year>2020</year></pub-date><volume>9</volume><elocation-id>e61909</elocation-id><history><date date-type="received"><day>08</day><month>08</month><year>2020</year></date><date date-type="accepted"><day>06</day><month>12</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Kane et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Kane et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-61909-v2.pdf"/><abstract><p>The ability to control a behavioral task or stimulate neural activity based on animal behavior in real-time is an important tool for experimental neuroscientists. Ideally, such tools are noninvasive, low-latency, and provide interfaces to trigger external hardware based on posture. Recent advances in pose estimation with deep learning allows researchers to train deep neural networks to accurately quantify a wide variety of animal behaviors. Here we provide a new DeepLabCut-Live! package that achieves low-latency real-time pose estimation (within 15 ms, &gt;100 FPS), with an additional forward-prediction module that achieves zero-latency feedback, and a dynamic-cropping mode that allows for higher inference speeds. We also provide three options for using this tool with ease: (1) a stand-alone GUI (called DLC-Live! GUI, and integration into (2) Bonsai and (3) AutoPilot. Lastly, we benchmarked performance on a wide range of systems so that experimentalists can easily decide what hardware is required for their needs.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014989</institution-id><institution>Chan Zuckerberg Initiative</institution></institution-wrap></funding-source><award-id>EOSS</award-id><principal-award-recipient><name><surname>Mathis</surname><given-names>Alexander</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1309047</award-id><principal-award-recipient><name><surname>Sanders</surname><given-names>Jonny L</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>The Rowland Institute at Harvard, Harvard University</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kane</surname><given-names>Gary A</given-names></name><name><surname>Mathis</surname><given-names>Alexander</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All mouse work was carried out under the permission of the IACUC at Harvard University (#17-07-309). Dog videos and feedback was exempt from IACUC approval (with conformation with IACUC).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All models, data, test scripts and software is already released and made freely available on GitHub: https://github.com/DeepLabCut/DeepLabCut-live</p></sec><supplementary-material><ext-link xlink:href="elife-61909-supp-v2.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>