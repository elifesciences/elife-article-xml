<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">101069</article-id><article-id pub-id-type="doi">10.7554/eLife.101069</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101069.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Introducing µGUIDE for quantitative imaging via generalized uncertainty-driven inference using deep learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Jallais</surname><given-names>Maëliss</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5939-388X</contrib-id><email>jallaism@cardiff.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Palombo</surname><given-names>Marco</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4892-7967</contrib-id><email>palombom@cardiff.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03kk7td41</institution-id><institution>Cardiff University Brain Research Imaging Centre (CUBRIC), Cardiff University</institution></institution-wrap><addr-line><named-content content-type="city">Cardiff</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03kk7td41</institution-id><institution>School of Computer Science and Informatics, Cardiff University</institution></institution-wrap><addr-line><named-content content-type="city">Cardiff</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sui</surname><given-names>Jing</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP101069</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-06-30"><day>30</day><month>06</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-06-27"><day>27</day><month>06</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2312.17293"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-22"><day>22</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101069.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-30"><day>30</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101069.2"/></event></pub-history><permissions><copyright-statement>© 2024, Jallais and Palombo</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Jallais and Palombo</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-101069-v1.pdf"/><abstract><p>This work proposes µGUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or signal representation, with exemplar demonstration in diffusion-weighted magnetic resonance imaging. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, µGUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>quantitative MRI</kwd><kwd>Bayesian inference</kwd><kwd>microstructure imaging</kwd><kwd>diffusion MRI</kwd><kwd>simulation-based inference</kwd><kwd>uncertainty quantification</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014013</institution-id><institution>UK Research and Innovation</institution></institution-wrap></funding-source><award-id>Future Leaders Fellowship MR/T020296/2</award-id><principal-award-recipient><name><surname>Palombo</surname><given-names>Marco</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>µGUIDE is a Bayesian framework that leverages simulation-based inference to efficiently estimate posterior distributions of any forward model parameters, allowing for uncertainty quantification and degeneracy detection.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Diffusion-weighted magnetic resonance imaging (dMRI) is a promising technique for characterizing brain microstructure in vivo using a paradigm called microstructure imaging (<xref ref-type="bibr" rid="bib62">Novikov et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Alexander et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Jelescu et al., 2020</xref>). Traditionally, microstructure imaging quantifies histologically meaningful features of brain microstructure by fitting a forward (biophysical) model voxel-wise to the set of signals obtained from images acquired with different sensitivities, yielding model parameter maps (<xref ref-type="bibr" rid="bib4">Alexander et al., 2019</xref>).</p><p>Most commonly used techniques rely on a non-linear curve fitting of the signal and return the optimal solution, that is the best parameters guess of the fitting procedure. However, this may hide model degeneracy, that is all the other possible estimates that could explain the observed signal equally well (<xref ref-type="bibr" rid="bib40">Jelescu et al., 2016</xref>). Another crucial consideration in model fitting is accounting for the uncertainty in parameter estimates. This uncertainty serves various purposes, including assessing result confidence (<xref ref-type="bibr" rid="bib44">Jones, 2003</xref>), quantifying noise effects (<xref ref-type="bibr" rid="bib8">Behrens et al., 2003</xref>), or assisting in experimental design (<xref ref-type="bibr" rid="bib2">Alexander, 2008</xref>).</p><p>Instead of attempting to remove the degeneracies, which has been the focus of a large number of studies (<xref ref-type="bibr" rid="bib65">Palombo et al., 2023</xref>; <xref ref-type="bibr" rid="bib17">de Almeida Martins et al., 2021</xref>; <xref ref-type="bibr" rid="bib77">Slator et al., 2021</xref>; <xref ref-type="bibr" rid="bib43">Jelescu et al., 2022</xref>; <xref ref-type="bibr" rid="bib87">Warner et al., 2023</xref>; <xref ref-type="bibr" rid="bib83">Uhl et al., 2024</xref>; <xref ref-type="bibr" rid="bib60">Mougel et al., 2024</xref>; <xref ref-type="bibr" rid="bib63">Olesen et al., 2022</xref>; <xref ref-type="bibr" rid="bib64">Palombo et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Howard et al., 2022</xref>; <xref ref-type="bibr" rid="bib45">Jones et al., 2018</xref>; <xref ref-type="bibr" rid="bib85">Vincent et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Henriques et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Afzali et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Lampinen et al., 2023</xref>; <xref ref-type="bibr" rid="bib90">Zhang et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Guerreri et al., 2023</xref>; <xref ref-type="bibr" rid="bib32">Gyori et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Novikov et al., 2018</xref>), we propose to highlight them and present all the possible parameter values that could explain an observed signal, providing users with more information to make more confident and explainable use of the inference results.</p><p>Posterior distributions are powerful tools to characterize all the possible parameter estimations that could explain an observed measurement, their uncertainty, and existing model degeneracy (<xref ref-type="bibr" rid="bib11">Box and Tiao, 2011</xref>). Bayesian inference allows for the estimation of these posterior distributions, traditionally approximating them using numerical methods, such as Markov-Chain-Monte-Carlo (MCMC) (<xref ref-type="bibr" rid="bib59">Metropolis et al., 1953</xref>). In quantitative MRI, these methods have been used for example to estimate brain connectivity (<xref ref-type="bibr" rid="bib8">Behrens et al., 2003</xref>), optimize imaging protocols (<xref ref-type="bibr" rid="bib2">Alexander, 2008</xref>), or infer crossing fibres by combining multiple spatial resolutions (<xref ref-type="bibr" rid="bib80">Sotiropoulos et al., 2013</xref>). However, these classical Bayesian inference methods are computationally expensive and time consuming. They also often require adjustments and tuning specific to each biophysical model (<xref ref-type="bibr" rid="bib33">Harms and Roebroeck, 2018</xref>).</p><p>Harnessing a new deep learning architecture for automatic signal feature selection and efficient sampling of the posterior distributions using Simulation-Based Inference (SBI) (<xref ref-type="bibr" rid="bib16">Cranmer et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Papamakarios et al., 2019</xref>), here we propose µGUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model/signal representation. µGUIDE extends and generalizes previous work (<xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>) to any forward model and without acquisition constraints, providing fast estimations of posterior distributions voxel-wise. We demonstrate µGUIDE using numerical simulations on three biophysical models of increasing complexity and degeneracy and compare the obtained estimates with existing methods, including the classical MCMC approach. We then apply the proposed framework to dMRI data acquired from healthy human volunteers and participants with epilepsy. µGUIDE framework is agnostic to the origin of the data and the details of the forward model, so we envision its usage and utility to perform Bayesian inference of model parameters also using data from other MRI modalities (e.g. relaxation MRI) and beyond.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Framework overview</title><p>The full architecture of the proposed Bayesian framework, dubbed µGUIDE, is presented in <xref ref-type="fig" rid="fig1">Figure 1</xref>. µGUIDE allows to efficiently estimate full posterior distributions of tissue parameters. It is comprised of two modules that are optimized together to minimize the Kullback–Leibler divergence between the true posterior distribution and the estimated one for every parameters of a given forward model. The ‘Neural Posterior Estimator’ (NPE) module (<xref ref-type="bibr" rid="bib69">Papamakarios et al., 2017</xref>) uses normalizing flows (<xref ref-type="bibr" rid="bib71">Papamakarios et al., 2021</xref>) to approximate the posterior distribution, while the ‘Multi-Layer Perceptron’ (MLP) module is used to reduce the data dimensionality and ensure fast and robust convergence of the NPE module.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>µGUIDE framework.</title><p>µGUIDE takes as input an observed data vector and relies on the definition of a biophysical or computational model (<xref ref-type="bibr" rid="bib7">Ascoli et al., 2007</xref>; <xref ref-type="bibr" rid="bib12">Callaghan et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Jelescu et al., 2020</xref>). It outputs a posterior distribution of the model parameters. Based on a Simulation-Based Inference (SBI) framework, it combines a Multi-Layer Perceptron (MLP) with three layers and a Neural Posterior Estimator (NPE). The MLP learns a low-dimensional representation of <inline-formula><mml:math id="inf1"><mml:mi>𝒙</mml:mi></mml:math></inline-formula>, based on a small number of features (<inline-formula><mml:math id="inf2"><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula>), that can be either defined a priori or determined empirically during training. The MLP is trained simultaneously with the NPE, leading to the extraction of the optimal features that minimize the bias and uncertainty of <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig1-v1.tif"/></fig><p>The full posterior distribution contains a lot of useful information. To summarize and easily visualize this information, we propose three measures that quantify the best estimates and the associated confidence levels, and a way to highlight degeneracy. The three measures are the Maximum A Posteriori (MAP), which corresponds to the most likely parameter estimate; an uncertainty measure, which quantifies the dispersion of the 50% most probable samples using the interquartile range, relative to the prior range; and an ambiguity measure, which measures the Full Width at Half Maximum (FWHM), in percentage with respect to the prior range. <xref ref-type="fig" rid="fig2">Figure 2</xref> presents those measures on exemplar posterior distributions. We show exemplar applications of µGUIDE to three biophysical models of increasing complexity and degeneracy from the dMRI literature: Ball&amp;Stick (<xref ref-type="bibr" rid="bib8">Behrens et al., 2003</xref>) (Model 1); Standard Model (SM) (<xref ref-type="bibr" rid="bib62">Novikov et al., 2019</xref>) (Model 2); and extended-SANDI (<xref ref-type="bibr" rid="bib64">Palombo et al., 2020</xref>) (Model 3).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>µGUIDE summarizes information contained in the estimated posterior distributions.</title><p>(<bold>A</bold>) Examples of degenerate and non-degenerate posterior distributions. Two Gaussian distributions are fitted to the obtained posterior distribution, where the means and standard deviations are represented by the vertical lines and shaded areas. A voxel is considered as degenerate if the derivative of the fitted Gaussian distributions changes signs more than once (i.e. multiple local maxima), and if the two Gaussian distributions are not overlapping (the distance between the two Gaussian means is inferior to the sum of their standard deviations). (<bold>B</bold>) Presentation of the measures introduced to quantify a posterior distribution on exemplar non-degenerate posterior distributions. Maximum A Posteriori (MAP) is the most likely parameter estimate (dashed vertical lines). Uncertainty measures the dispersion of the 50% most probable samples using the interquartile range, with respect to the prior range. Ambiguity measures the Full Width at Half Maximum (FWHM), in percentage with respect to the prior range.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Evaluation of µGUIDE on simulations</title><sec id="s2-2-1"><title>Comparison with MCMC</title><p>We performed a comparison between the posterior distributions obtained using µGUIDE and MCMC, a classical Bayesian method. <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows posterior distributions on three exemplar simulations with <inline-formula><mml:math id="inf4"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> using the Model 2 (SM), obtained with 15,000 samples. Sharper and less biased posterior estimations are obtained using µGUIDE. <xref ref-type="fig" rid="fig3">Figure 3B</xref> presents histograms for each model parameter of the bias between the ground truth value used to simulate a signal, and the MAP of the posterior distributions obtained with either µGUIDE or MCMC, on 200 simulations. Results indicate that the bias is similar or smaller using µGUIDE. Overall, µGUIDE posterior distributions are more accurate than the ones obtained with MCMC.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Comparison between µGUIDE and Markov-Chain-Monte-Carlo (MCMC).</title><p>(<bold>A</bold>) Posterior distributions obtained using either µGUIDE or MCMC on three exemplar simulations with Model 2 (SM − <inline-formula><mml:math id="inf5"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>). Names of the model parameters are indicated in the titles of the panels. (<bold>B</bold>) Bias between the ground truth values used for simulating the diffusion signals, and the Maximum A Posteriori extracted from the posterior distributions using either µGUIDE or MCMC. Sharper and less biased posterior distributions are obtained using µGUIDE.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig3-v1.tif"/></fig><p>Moreover, it took on average 29.3 s to obtain the posterior distribution using MCMC on a GPU (NVIDIA GeForce GT 710) for one voxel, while it only took 0.02 s for µGUIDE. µGUIDE is about 1500 times faster than MCMC, which makes it more suitable for applying it on large datasets.</p></sec><sec id="s2-2-2"><title>The importance of feature selection</title><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows the MAP extracted from the posterior distributions versus the ground truth parameters used to generate the diffusion signal with µGUIDE and manually defined summary statistics for the three models. Less biased MAPs with lower ambiguities and uncertainties are obtained with µGUIDE, indicating that the MLP allows for the extraction of additional information not contained in the summary statistics, helping to solve the inverse problem with higher accuracy and precision. µGUIDE generalizes the method developed in <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref> to make it applicable to any forward model and any acquisition protocol, while making the estimates more precise and accurate thanks to the automatic feature extraction.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Fitting accuracy comparison between µGUIDE’s Multi-Layer Perceptron (MLP)-extracted features and manually defined summary statistics.</title><p>Maximum A Posterioris (MAPs) extracted from the posterior distributions versus ground truth parameters used for generating the signal for the three models. Orange points correspond to the MAPs obtained using MLP-extracted features (µGUIDE) and the blue ones to the MAPs with the manually defined summary statistics. Only the non-degenerate posterior distributions were kept. The summary statistics used in those three models are the direction-averaged signal for the Ball&amp;Stick model, the LEMONADE system of equations (<xref ref-type="bibr" rid="bib61">Novikov et al., 2018</xref>) for the Standard Model (SM), and the summary statistics defined in <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref> for the extended-SANDI model. Results are shown on 100 exemplar noise-free simulations with random parameter combinations. The optimal features extracted by the MLP allow to reduce the bias and variance of the obtained microstructure posterior distributions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig4-v1.tif"/></fig></sec><sec id="s2-2-3"><title>µGUIDE highlights degeneracies</title><p><xref ref-type="fig" rid="fig5">Figure 5</xref> presents the posterior distributions of microstructure parameters for the three models obtained with µGUIDE on exemplar noise-free simulations. Blue curves correspond to non-degenerate posterior distributions, while the red ones present at least one degeneracy for one of the parameters. As the complexity of the model increases, degeneracy in the model definitions appear. This figure showcases µGUIDE ability to highlight degeneracy in the model parameter estimation.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Exemplar posterior distributions of the microstructure parameters for the Ball&amp;Stick, Standard Model (SM), and extended-SANDI models, obtained using µGUIDE on exemplar noise-free simulations.</title><p>As the complexity of the model increases, degeneracies appear (red posterior distributions). µGUIDE allows to highlight those degeneracies present in the model definition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig5-v1.tif"/></fig><p><xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref> present the number of degenerate cases for each parameter in the three models, on 10,000 simulations. <xref ref-type="table" rid="table1">Table 1</xref> considers noise-free simulations and the training and estimations were performed on CPU. <xref ref-type="table" rid="table2">Table 2</xref> reports results on noisy simulations (Rician noise with <inline-formula><mml:math id="inf6"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>), with training and testing performed on a GPU (NVIDIA GeForce RTX 4090). The time needed for the inference and to estimate the posterior distributions on 10,000 simulations, define if they are degenerate or not, and extract the MAP, uncertainty, and ambiguity are also reported. The more complex the model, the more degeneracies.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Number of degenerate cases per parameter on 10,000 noise-free simulations.</title><p>Training and estimations of the posterior distributions were performed on CPU. Time for training each model and time for estimating posterior distributions of 10,000 noise-free simulations, define if they are degenerate or not, and extract the Maximum A Posteriori (MAP), uncertainty, and ambiguity are also reported.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model (SNR = ∞)</th><th align="left" valign="bottom" rowspan="2">Training time (CPU)</th><th align="left" valign="bottom" rowspan="2">Fitting time (on 10,000 simulations)</th><th align="left" valign="bottom" colspan="8">Number of degeneracies (on 10,000 simulations)</th></tr><tr><th align="left" valign="bottom"><inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom">Model 1: Ball&amp;Stick</td><td align="char" char="." valign="bottom">11 min</td><td align="char" char="." valign="bottom">96 s</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">Model 2: Standard Model</td><td align="char" char="." valign="bottom">2h02</td><td align="char" char="." valign="bottom">135 s</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">34</td><td align="char" char="." valign="bottom">23</td><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">Model 3: extended-SANDI model</td><td align="char" char="." valign="bottom">2h02</td><td align="char" char="." valign="bottom">1412 s</td><td align="char" char="." valign="bottom">205</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">260</td><td align="char" char="." valign="bottom">57</td><td align="left" valign="bottom">-</td><td align="char" char="." valign="bottom">1395</td><td align="char" char="." valign="bottom">2571</td><td align="char" char="." valign="bottom">1011</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Number of degenerate cases per parameter on 10,000 noisy simulations (Rician noise with SNR = 50).</title><p>Training and estimations of the posterior distributions were performed using a GPU. Time for training each model and time for estimating posterior distributions of 10,000 noisy simulations, define if they are degenerate or not, and extract the Maximum A Posteriori (MAP), uncertainty, and ambiguity are also reported.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model (SNR = 50)</th><th align="left" valign="bottom" rowspan="2">Training time (CPU)</th><th align="left" valign="bottom" rowspan="2">Fitting time (on 10,000 simulations)</th><th align="left" valign="bottom" colspan="8">Number of degeneracies (on 10,000 simulations)</th></tr><tr><th align="left" valign="bottom"><inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="bottom">Model 1: Ball&amp;Stick</td><td align="char" char="." valign="bottom">26 min</td><td align="char" char="." valign="bottom">79 s</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">Model 2: Standard Model</td><td align="char" char="." valign="bottom">42 min</td><td align="char" char="." valign="bottom">82 s</td><td align="char" char="." valign="bottom">75</td><td align="char" char="." valign="bottom">71</td><td align="char" char="." valign="bottom">117</td><td align="char" char="." valign="bottom">109</td><td align="char" char="." valign="bottom">29</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">Model 3: Extended-SANDI model</td><td align="char" char="." valign="bottom">50 min</td><td align="char" char="." valign="bottom">238 s</td><td align="char" char="." valign="bottom">47</td><td align="char" char="." valign="bottom">24</td><td align="char" char="." valign="bottom">784</td><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">-</td><td align="char" char="." valign="bottom">828</td><td align="char" char="." valign="bottom">1047</td><td align="char" char="." valign="bottom">56</td></tr></tbody></table></table-wrap></sec></sec><sec id="s2-3"><title>Application of µGUIDE to real data</title><p>After demonstrating that the proposed framework provides good estimates in the controlled case of simulations, we applied µGUIDE to both a healthy volunteer and a participant with epilepsy. The estimation of the posterior distributions is done independently for each voxel. To easily assess the values and the quality of the fitting, we are plotting the MAP, ambiguity, and uncertainty maps, but the full posterior distributions are stored and available for all the voxels. Voxels presenting a degeneracy are highlighted with a red dot.</p><sec id="s2-3-1"><title>Healthy volunteer</title><p>We applied µGUIDE to a healthy volunteer, using the Ball&amp;Stick, SM, and extended-SANDI models. <xref ref-type="fig" rid="fig6">Figure 6</xref> presents the parametric maps of an exemplar set of model parameters for each model, alongside their degeneracy, uncertainty, and ambiguity. The Ball&amp;Stick model presents no degeneracy, the SM presents some degeneracy, mostly in voxels with high likelihood of partial voluming with cerebrospinal fluid and at the white matter–grey matter boundaries. The extended-SANDI model is the model showing the highest number of degenerate cases, mostly localized within the white matter areas characterized by complex microstructure, for example crossing fibres. This result is expected, as the complexity of the models increases, leading to more combinations of tissue parameters that can explain an observed signal. Measures of ambiguity and uncertainty allow to quantify the confidence in the estimates and help interpreting the results.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Parametric maps of the Ball&amp;Stick (top), SM (middle) and extended-SANDI model (bottom), obtained using µGUIDE.</title><p>Maximum A Posteriori (MAP), uncertainty and ambiguity measure maps are reported, overlayed with voxels considered degenerate (red dots).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig6-v1.tif"/></fig></sec><sec id="s2-3-2"><title>Participant with epilepsy</title><p><xref ref-type="fig" rid="fig7">Figure 7</xref> demonstrates µGUIDE application to a participant with epilepsy, using the SM. Noteworthy, the axonal signal fraction estimates within the epileptic lesion show low uncertainty and ambiguity measures hence high confidence, while orientation dispersion index estimates show high uncertainty and ambiguity suggesting low confidence, cautioning the interpretation.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Parametric maps of a participant with epilepsy obtained using µGUIDE with the Standard Model (SM), superimposed with the grey matter (black) and white matter (white) lesions segmentation.</title><p>Mean values of the Maximum A Posterior (MAP), uncertainty, and ambiguity measures are reported in the two regions of interest. Lower MAP values are obtained in the lesions for the axonal signal fraction and the orientation dispersion index compared to healthy tissue. Higher uncertainty and ambiguity ODI values are reported, suggesting less stable estimations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig7-v1.tif"/></fig></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Applicability of µGUIDE to multiple models</title><p>The µGUIDE framework offers the advantage of being easily applicable to various biophysical models and representations, thanks to its data-driven approach for data reduction. The need to manually define specific summary statistics that capture the relevant information for microstructure estimation from the multi-shell diffusion signal is removed. This also eliminates the acquisition constraints that were previously imposed by the summary statistics definition (<xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>). The extracted features contain additional information compared to the summary statistics (see Appendix 1), resulting in a notable reduction in bias (on average 5.2-fold lower), uncertainty (on average 2.6-fold lower), and ambiguity (on average 2.7-fold lower) in the estimated posterior distributions. Consequently, µGUIDE improves parameters estimation over current state-of-the-art methods (e.g. <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>), showing for example reduced bias (on average 5.2-fold lower) and dispersion (on average 6.4-fold lower) on the MAP estimates for each of the three example models investigated (see <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>In this study, we presented applications of µGUIDE to brain microstructure estimation using three well-established biophysical models, with increased complexity: the Ball&amp;Stick model, the SM, and an extended-SANDI model. However, our approach is not limited to brain tissue nor to diffusion-weighted MRI and can be extended to different organs by employing their respective acquisition encoding and forward models, such as NEXI for exchange estimates (<xref ref-type="bibr" rid="bib39">Jallais et al., 2024</xref>), mcDESPOT for myelin water fraction mapping using quantitative MRI relaxation (<xref ref-type="bibr" rid="bib18">Deoni et al., 2008</xref>), VERDICT in prostate imaging (<xref ref-type="bibr" rid="bib67">Panagiotaki et al., 2014</xref>), or even adapted to different imaging modalities (e.g. electroencephalography and magnetoencephalography), where there is a way to link (via modelling or simulation) the observed signal to a set of parameters of interest. This versatility underscores the broad applicability of our proposed approach across various biological systems and imaging techniques.</p><p>It is important to note that µGUIDE is still a model-dependent method, meaning that the training process is based on the specific model being used. Additionally, the number of features extracted by the MLP needs to be predetermined. One way to determine the number of features is by matching it with the number of parameters being estimated. Alternatively, a dimensionality-reduction study using techniques like <italic>t</italic>-distributed stochastic neighbour embedding (<xref ref-type="bibr" rid="bib19">der Maaten and Hinton, 2008</xref>) can be conducted to determine the optimal number of features.</p></sec><sec id="s3-2"><title>µGUIDE: an efficient framework for Bayesian inference</title><p>One notable advantage of µGUIDE is its amortized nature. With this approach, the training process is performed only once, and thereafter, the posterior estimations can be independently obtained for all voxels. This amortization enables efficient estimations of the posterior distributions. µGUIDE outperforms in terms of speed conventional Bayesian inference methods such as MCMC, showing a ∼1500-fold acceleration. The time savings achieved with µGUIDE make it a highly efficient and practical tool for estimating posterior distributions in a timely manner.</p><p>This unlocks the possibility to process with Bayesian inference very large datasets in manageable time (e.g. approximately 6 months to process 10 k dMRI datasets) and to include Bayesian inference in iterative processes that require the repeated computation of the posterior distributions (e.g. dMRI acquisition optimization [<xref ref-type="bibr" rid="bib2">Alexander, 2008</xref>]).</p><p>In the dMRI community, the use of SBI methods to characterize full posterior distributions as well as quantify the uncertainty in parameter estimations was first introduced in <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref> for a grey matter model. An application to crossing fibres has recently been proposed by <xref ref-type="bibr" rid="bib46">Karimi et al., 2024</xref>. Those approaches use different density estimators. This work and <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref> rely on Masked Autoregressive Flows (MAFs [<xref ref-type="bibr" rid="bib69">Papamakarios et al., 2017</xref>]), while the work by <xref ref-type="bibr" rid="bib46">Karimi et al., 2024</xref> is based on Mixture Density Networks (MDNs [<xref ref-type="bibr" rid="bib9">Bishop, 1994</xref>]). MAFs have been found to show superior performance compared to MDNs (<xref ref-type="bibr" rid="bib27">Gonçalves et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Patron et al., 2022</xref>).</p></sec><sec id="s3-3"><title>µGUIDE quantifies confidence to guide interpretation</title><p>Quantifying confidence in an estimate is of crucial importance. As demonstrated by our pathological example, changes in the tissue microstructure parameters can help clinicians decide which parameters are the most reliable and better interpret microstructure changes within diseased tissue. On large population studies, the quantified uncertainty can be taken into account when performing group statistics and to detect outliers.</p><p>Multiple approaches have been used to try and quantify this uncertainty. Gradient descent often provides a measure of confidence for each parameter estimate. Alternative approaches use the shape of the fitted tensor itself as a measure of uncertainty for the fibre direction (<xref ref-type="bibr" rid="bib51">Koch et al., 2002</xref>; <xref ref-type="bibr" rid="bib72">Parker and Alexander, 2003</xref>). Other methods also rely on bootstrapping techniques to estimate uncertainty. Repetition bootstrapping for example depends on repeated measurements of signal for each gradient direction, but imply a long acquisition time and cost, and are prone to motion artifacts (<xref ref-type="bibr" rid="bib54">Lazar and Alexander, 2005</xref>; <xref ref-type="bibr" rid="bib44">Jones, 2003</xref>). In contrast, residual bootstrapping methods resample the residuals of a regression model. Yet, this approach is heavily dependent on the model and can lead to overfitting (<xref ref-type="bibr" rid="bib88">Whitcher et al., 2008</xref>; <xref ref-type="bibr" rid="bib13">Chung et al., 2006</xref>). In general, resampling methods can be problematic for sparse samples, as the bootstrapped samples tend to underestimate the true randomness of the distribution (<xref ref-type="bibr" rid="bib47">Kauermann et al., 2009</xref>). We propose to quantify the confidence by estimating full posterior distributions, which also has the benefit of highlighting degeneracy. Model-fitting methods with different initializations, as done in for example <xref ref-type="bibr" rid="bib40">Jelescu et al., 2016</xref>, also allow to highlight degeneracies. However, they only provide a partial description of the solution landscape, which can be interpreted as a partial posterior distribution. In contrast, Bayesian methods estimate the full posterior distributions, offering a more accurate and precise characterization of degeneracies and uncertainties. Hence, in this work we decided to use MCMC, a traditional Bayesian method, as benchmark.</p><p>Variance observed in the posterior distributions can be attributed to several factors. The presence of noise in the signal contributes to irreducible variance, decreasing the confidence in the estimates as the noise level increases (see Appendix 2). Another source of variance can arise from the choice of acquisition parameters. Different acquisitions may provide varying levels of confidence in the parameter estimates. Under-sampled acquisitions or inadequate b-shells may fail to capture essential information about a tissue microstructure, such as soma or neurite radii, resulting in inaccurate estimates.</p><p>µGUIDE can guide users in determining whether an acquisition is suitable for estimating parameters of a given model and vice versa, the variance and bias of the posterior distributions estimated with µGUIDE can be used to guide the optimization of the data acquisition to maximize accuracy and precision of the model parameters estimates.</p><p>The presence of degeneracy in the solution of the inverse problem is influenced by the complexity of the model being used and the lack of sufficient information in the data. In recent years, researchers have introduced increasingly sophisticated models to better represent the brain tissue, such as SANDI (<xref ref-type="bibr" rid="bib64">Palombo et al., 2020</xref>), NEXI (<xref ref-type="bibr" rid="bib43">Jelescu et al., 2022</xref>), and eSANDIX (<xref ref-type="bibr" rid="bib63">Olesen et al., 2022</xref>), that take into account an increasing number of tissue features. By applying µGUIDE, it becomes possible to gain insights into the degree of degeneracy within a model and to assess the balance between model realism and the ability to accurately invert the problem. We have recently provided an example of such application for NEXI and SANDIX (<xref ref-type="bibr" rid="bib39">Jallais et al., 2024</xref>).</p></sec><sec id="s3-4"><title>Summary</title><p>We propose a general Bayesian framework, dubbed µGUIDE, to efficiently estimate posterior distributions of tissue microstructure parameters. For any given acquisition and signal model/representation, µGUIDE improves parameters estimation and computational time over existing state-of-the-art methods. It allows to highlight degeneracy, and quantify confidence in the estimates, guiding results interpretation towards more confident and explainable diagnosis using modern deep learning. µGUIDE is not inherently limited to dMRI and microstructure imaging. We envision its usage and utility to perform efficient Bayesian inference also using data from any modality where there is a way to link (via modelling or simulation) the observed measurements to a set of parameters of interest.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Solving the inverse problem using Bayesian inference</title><sec id="s4-1-1"><title>The inference problem</title><p>We make the hypothesis that an observed dMRI signal <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula> can be explained (and generated) using a handful of relevant tissue microstructure parameters <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>𝜽</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula>, following the definition of a forward model:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo>=</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The objective is, given this observation <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula>, to estimate the parameters <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>𝜽</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula> that generated it.</p><p>Forward models are designed to mimic at best a given biophysical phenomenon, for some given time and scale (<xref ref-type="bibr" rid="bib3">Alexander, 2009</xref>; <xref ref-type="bibr" rid="bib89">Yablonskiy and Sukstanskii, 2010</xref>; <xref ref-type="bibr" rid="bib41">Jelescu and Budde, 2017</xref>; <xref ref-type="bibr" rid="bib62">Novikov et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Alexander et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Jelescu et al., 2020</xref>). As a consequence, forward models are injection functions (every biologically plausible <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> generates exactly one signal <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula>), but do not always happen to be bijections, meaning that multiple <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> can generate the same signal <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula>. It can be impossible, based on biological considerations, to infer which solution <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> best reflects the probed structure. We refer to these models as ‘degenerate models’.</p><p>Point estimates algorithms, such as minimum least square or maximum likelihood estimation algorithms, allow to estimate one set of microstructure parameters that could explain an observed signal. In the case of degenerate models, the solution space can be multi-modal and those algorithms will hide possible solutions. When considering real-life acquisitions, that is noisy and/or under-sampled acquisitions, one also needs to consider the bias introduced with respect to the forward model, and the resulting variance in the estimates (<xref ref-type="bibr" rid="bib44">Jones, 2003</xref>; <xref ref-type="bibr" rid="bib8">Behrens et al., 2003</xref>).</p><p>We propose a new framework that allows for the estimation of full posterior distributions <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, that is all the probable parameters that could represent the underlying tissue, along with an uncertainty measure and the interdependency of parameters. These posteriors can help interpreting the obtained results and make more informed decisions.</p></sec><sec id="s4-1-2"><title>The Bayesian formalism</title><p>The posterior distribution can be defined using Bayes’ theorem as follows:<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="mediummathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mspace width=".5em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>|</mml:mi><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the likelihood of the observed data point, <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the prior distribution defining our initial knowledge of the parameter values, and <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a normalizing constant, commonly referred to as the evidence of the data.</p><p>The evidence term is usually very hard to estimate, as it corresponds to all the possible realizations of <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula>, that is <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For simplification, methods usually estimate an unnormalized probability density function, that is<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width=".5em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To approximate these posterior distributions, traditional methods rely on the estimation of the likelihood <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mi>|</mml:mi><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of the observed data point <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula> via an analytic expression. This likelihood function corresponds to an integral over all possible trajectories through the latent space, that is <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mi>|</mml:mi><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="false">∫</mml:mo><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo separator="true">,</mml:mo><mml:mi>𝒛</mml:mi><mml:mi>|</mml:mi><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>𝒛</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo separator="true">,</mml:mo><mml:mi>𝒛</mml:mi><mml:mi>|</mml:mi><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the joint probability density of observed data <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub></mml:math></inline-formula> and latent variables <inline-formula><mml:math id="inf43"><mml:mi>𝒛</mml:mi></mml:math></inline-formula>. For forward models with large latent spaces, computing this integral explicitly becomes impractical. The likelihood function is then intractable, rendering these methods unusable (<xref ref-type="bibr" rid="bib16">Cranmer et al., 2020</xref>). Models that do not admit a tractable likelihood are called <italic>implicit models</italic> (<xref ref-type="bibr" rid="bib21">Diggle and Gratton, 1984</xref>).</p><p>To circumvent this issue, some techniques have been proposed to sample numerically from the likelihood function, such as MCMC (<xref ref-type="bibr" rid="bib59">Metropolis et al., 1953</xref>). Another set of approaches proposes to train a conditional density estimator to learn a surrogate of the likelihood distribution (<xref ref-type="bibr" rid="bib70">Papamakarios et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Lueckmann et al., 2019</xref>), the likelihood ratio (<xref ref-type="bibr" rid="bib15">Cranmer et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Gutmann et al., 2018</xref>), or the posterior distribution (<xref ref-type="bibr" rid="bib68">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib55">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Papamakarios et al., 2019</xref>), allowing to greatly reduce computation times. These methods are dubbed likelihood-free inference or SBI methods (<xref ref-type="bibr" rid="bib16">Cranmer et al., 2020</xref>; <xref ref-type="bibr" rid="bib81">Tejero-Cantero et al., 2020</xref>). In particular, there has been a growing interest towards deep generative modelling approaches in the machine learning community (<xref ref-type="bibr" rid="bib57">Lueckmann et al., 2021</xref>). They rely on specially tailored neural network architectures to approximate probability density functions from a set of examples. Normalizing flows (<xref ref-type="bibr" rid="bib71">Papamakarios et al., 2021</xref>) are a particular class of such neural networks that have demonstrated promising results for SBI in different research fields (<xref ref-type="bibr" rid="bib27">Gonçalves et al., 2020</xref>; <xref ref-type="bibr" rid="bib29">Greenberg et al., 2019</xref>).</p><p>While this work focuses on the estimate of the posterior distribution using a conditional density estimator, we show a comparison with MCMC, which are commonly used methods in the community. We will therefore introduce this method in the following paragraph.</p></sec><sec id="s4-1-3"><title>Estimating the likelihood function</title><p>Well-established approaches for estimating the likelihood function are MCMC methods. These methods rely on a noise model to define the likelihood distribution, such as the Rician (<xref ref-type="bibr" rid="bib66">Panagiotaki et al., 2012</xref>) or Offset Gaussian models (<xref ref-type="bibr" rid="bib3">Alexander, 2009</xref>). In this work, we will be using the Microstructure Diffusion Toolbox to perform the MCMC computations (<xref ref-type="bibr" rid="bib33">Harms and Roebroeck, 2018</xref>), which relies on the Offset Gaussian model. The log-likelihood function is then the following:<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝒙</mml:mi><mml:mi>|</mml:mi><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover></mml:mrow><mml:msup><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msqrt><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:msubsup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mspace width="0.5em"/><mml:mo separator="true">,</mml:mo></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the signal obtained using the biophysical model, <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <italic>i</italic>th measurement of the signal, <inline-formula><mml:math id="inf46"><mml:mi>σ</mml:mi></mml:math></inline-formula> is the standard deviation of the Gaussian distributed noise, estimated from the reconstructed magnitude images (<xref ref-type="bibr" rid="bib20">Dietrich et al., 2007</xref>), and <inline-formula><mml:math id="inf47"><mml:mi>m</mml:mi></mml:math></inline-formula> is the number of observations in the dataset.</p><p>MCMC methods allow to obtain posterior distributions using Bayes’ formula (<xref ref-type="disp-formula" rid="equ3">Equation 2</xref>) with the previously defined likelihood function (<xref ref-type="disp-formula" rid="equ4">Equation 3</xref>) and some prior distributions, which are usually uniform distributions defined on biologically plausible ranges. They generate a multi-dimensional chain of samples which is guaranteed to converge towards a stationary distribution, which approximates the posterior distribution (<xref ref-type="bibr" rid="bib59">Metropolis et al., 1953</xref>).</p><p>The need to compute the signal following the forward model at each iteration makes these sampling methods computationally expensive and time consuming. In addition, they require some adjustments specific to each model, such as the choice of burn-in length, thinning, and the number of samples to store. <xref ref-type="bibr" rid="bib33">Harms and Roebroeck, 2018</xref> recommend to use the Adaptive Metropolis-Within-Gibbs (AMWG) algorithm for sampling dMRI models, initialized with a maximum likelihood estimator (MLE) obtained from non-linear optimization, with 100–200 samples for burn-in and no thinning. Authors notably investigated the use of starting from the MLE and thinning. They concluded that starting from the MLE allows to start in the stationary distribution of the Markov Chain, and has the advantage of removing salt- and pepper-like noise from the resulting mean and standard deviation maps. Their findings also indicate that thinning is unnecessary and inefficient, and they recommend using more samples instead. The recommended number of samples is model dependent. Authors recommendations can be found in their paper.</p></sec><sec id="s4-1-4"><title>Bypassing the likelihood function</title><p>An alternative method was proposed to overcome the challenges associated with approximating the likelihood function and the limitations of MCMC sampling algorithms. This approach involves directly approximating the posterior distribution by using a conditional density estimator, that is a family of conditional probability density function approximators denoted as <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. These approximators are parameterized by <inline-formula><mml:math id="inf49"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> and accept both the parameters <inline-formula><mml:math id="inf50"><mml:mi>𝜽</mml:mi></mml:math></inline-formula> and the observation <inline-formula><mml:math id="inf51"><mml:mi>𝒙</mml:mi></mml:math></inline-formula> as input arguments. Our posterior approximation is then obtained by minimizing its average Kullback–Leibler divergence with respect to the conditional density estimator for different choices of <inline-formula><mml:math id="inf52"><mml:mi>𝒙</mml:mi></mml:math></inline-formula>, as per <xref ref-type="bibr" rid="bib68">Papamakarios and Murray, 2016</xref>:<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:munder><mml:mtext>min.</mml:mtext><mml:mi mathvariant="bold-italic">ϕ</mml:mi></mml:munder><mml:mspace width="1em"/><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mtext>with</mml:mtext><mml:mspace width="1em"/><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which can be rewritten as<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="left left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∫</mml:mo></mml:mstyle></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∬</mml:mo></mml:mstyle></mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∬</mml:mo></mml:mstyle></mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf53"><mml:mi>C</mml:mi></mml:math></inline-formula> is a constant that does not depend on <inline-formula><mml:math id="inf54"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. Note that in practice we consider a <inline-formula><mml:math id="inf55"><mml:mi>N</mml:mi></mml:math></inline-formula>-sample Monte-Carlo approximation of the loss function:<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mtable displaystyle="true" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"/><mml:mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:msup><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mi>N</mml:mi></mml:msup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" stretchy="false">−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mo fence="false" symmetric="true" minsize="1.8em" maxsize="1.8em">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo fence="false" symmetric="true" minsize="1.8em" maxsize="1.8em">)</mml:mo><mml:mtext/><mml:mo separator="true">,</mml:mo></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf56"><mml:mi>N</mml:mi></mml:math></inline-formula> data points <inline-formula><mml:math id="inf57"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo separator="true">,</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are sampled from the joint distribution with <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝒙</mml:mi><mml:mi>|</mml:mi><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We can then use stochastic gradient descent to obtain a set of parameters <inline-formula><mml:math id="inf60"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> which minimizes <inline-formula><mml:math id="inf61"><mml:msup><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula>.</p><p>If the class of conditional density estimators is sufficiently expressive, it can be demonstrated that the minimizer of <xref ref-type="disp-formula" rid="equ7">Equation (6)</xref> converges to <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:mi>𝒚</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib29">Greenberg et al., 2019</xref>). It is worth noting that the parametrization <inline-formula><mml:math id="inf64"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, obtained at the end of the optimization procedure, serves as an amortized posterior for various choices of <inline-formula><mml:math id="inf65"><mml:mi>𝒙</mml:mi></mml:math></inline-formula>. Hence, for a particular observation <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>𝒙</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, we can simply use <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as an approximation of <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="s4-2"><title>µGUIDE framework</title><p>The full architecture of the proposed Bayesian framework, dubbed µGUIDE, is presented in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The analysis codes underpinning the results presented here can be found on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/mjallais/uGUIDE">https://github.com/mjallais/uGUIDE</ext-link> (copy archived at <xref ref-type="bibr" rid="bib38">Jallais, 2024</xref>) (both CPU and GPU are supported).</p><p>µGUIDE is comprised of two modules that are optimized together to minimize the Kullback–Leibler divergence between the true posterior distribution and the estimated one for every parameters of a given forward model. The NPE module uses normalizing flows to approximate the posterior distribution, while the MLP module is used to reduce the data dimensionality and ensure fast and robust convergence of the NPE module. The following sections provide more details about our implementation of each module.</p><sec id="s4-2-1"><title>Neural Posterior Estimator</title><p>In this study, the Sequential Neural Posterior Estimation (SNPE-C) algorithm (<xref ref-type="bibr" rid="bib68">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib29">Greenberg et al., 2019</xref>) with a single round is employed to train a neural network that directly approximates the posterior distribution. Thus, sampling from the posterior can be done by sampling from the trained neural network. Neural density estimators have the advantage of providing exact density evaluations, in contrast to Variational Autoencoders (VAEs [<xref ref-type="bibr" rid="bib50">Kingma and Welling, 2019</xref>]) or generative adversarial networks (GANs [<xref ref-type="bibr" rid="bib28">Goodfellow et al., 2014</xref>]), which are better suited for generating synthetic data.</p><p>The conditional probability density function approximators used in this project belong to a class of neural networks called normalizing flows (<xref ref-type="bibr" rid="bib71">Papamakarios et al., 2021</xref>). These flows are invertible functions capable of transforming vectors generated from a simple base distribution (e.g. the standard multivariate Gaussian distribution) into an approximation of the true posterior distribution. An autoregressive architecture for normalizing flows is employed, implemented via the MAF (<xref ref-type="bibr" rid="bib69">Papamakarios et al., 2017</xref>), which is constructed by stacking five Masked Autoencoder for Distribution Estimation (MADE) models (<xref ref-type="bibr" rid="bib25">Germain et al., 2015</xref>). An explanation of how MAF and MADE work is provided in Appendix 3.</p><p>To test that the predicted posteriors for a given model are not incorrect we use posterior predictive checks (PPCs), which is described in more details in Appendix 4.</p></sec><sec id="s4-2-2"><title>Handling the large dimensionality of the data with MLP</title><p>As the dimensionality of the input data <inline-formula><mml:math id="inf69"><mml:mi>𝒙</mml:mi></mml:math></inline-formula> grows, the complexity of the corresponding inverse problem also increases. Accurately characterizing the posterior distributions or estimating the tissue microstructure parameters becomes more challenging. As a consequence, it is often necessary to rely on a set of low-dimensional features (or summary statistics) instead of the raw data for the inference task process (<xref ref-type="bibr" rid="bib10">Blum et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Fearnhead and Prangle, 2012</xref>; <xref ref-type="bibr" rid="bib70">Papamakarios et al., 2019</xref>). These summary statistics are features that capture the essential information within the raw data, allowing to reduce the size of the input vector. Learning a set of sufficient statistics before estimating the posterior distribution makes the inference easier and offers many benefits (see e.g. the Rao–Blackwell theorem).</p><p>A follow-up challenge lies in the choice of suitable summary statistics. For well-understood problems and data, it is possible to manually design these features using deterministic functions that condense the information contained in the raw signal into a set of handful summary statistics. Previous works, such as <xref ref-type="bibr" rid="bib61">Novikov et al., 2018</xref> and <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>, have proposed specific summary statistics for two different biophysical models. However, defining these summary statistics is difficult and often requires prior knowledge of the problem at hand. In the context of dMRI, they also rely on acquisition constraints and are model specific.</p><p>In this work, the proposed framework aims to be applicable to any forward model and be as general as possible. We therefore propose to learn the summary statistics from the high-dimensional input signals <inline-formula><mml:math id="inf70"><mml:mi>𝒙</mml:mi></mml:math></inline-formula> using a neural network. This neural network is referred to as an embedding neural network. The observed signals are fed into the embedding neural network, whose outputs are then passed to the neural density estimator. The parameters of the embedding network are learned together with the parameters of the neural density estimator, leading to the extraction of optimal features that minimize the uncertainty of <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Here, we propose to use an MLP with three layers as a summary statistics extractor. The number of features <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula> extracted by the MLP can be either defined a priori or determined empirically during training.</p></sec><sec id="s4-2-3"><title>Training µGUIDE</title><p>To train µGUIDE we need couples of input vectors <inline-formula><mml:math id="inf73"><mml:mi>𝒙</mml:mi></mml:math></inline-formula> and corresponding ground truth values for the model parameters that we want to estimate, <inline-formula><mml:math id="inf74"><mml:mi>𝜽</mml:mi></mml:math></inline-formula>. The input <inline-formula><mml:math id="inf75"><mml:mi>𝒙</mml:mi></mml:math></inline-formula> can be real or simulated data (e.g. dMRI signals); or a mixture of these two. We train µGUIDE by stochastically minimizing the loss function defined in <xref ref-type="disp-formula" rid="equ7">Equation (6)</xref> using the Adam optimizer (<xref ref-type="bibr" rid="bib49">Kingma and Ba, 2015</xref>) with a learning rate of 10<sup>−3</sup> and a minibatch size of 128. We use 1 million simulations for each model, 5% of which are randomly selected to be used as a validation set. Training is stopped when the validation loss does not decrease for 30 consecutive epochs.</p></sec><sec id="s4-2-4"><title>Quantifying the confidence in the estimates</title><p>The full posterior distribution contains a lot of useful information about a given model parameter best estimates, uncertainty, ambiguity, and degeneracy. To summarize and easily visualize this information, we propose three measures that quantify the best estimates and the associated confidence levels, and a way to highlight degeneracy.</p><p>We start by checking whether a posterior distribution is degenerate, that is if the distribution presents multiple distinct parameter solutions, appearing as multiple local maxima (<xref ref-type="fig" rid="fig2">Figure 2</xref>). To that aim, we fit two Gaussian distributions to the obtained posterior distributions. A voxel is considered as degenerate if the derivative of the fitted Gaussian distributions changes signs more than once (i.e. multiple local maxima), and if the two Gaussian distributions are not overlapping (the distance between the two Gaussian means is inferior to the sum of their standard deviations).</p><p>For non-degenerate posterior distributions, we extract three quantities:</p><list list-type="order"><list-item><p>The MAP, which corresponds to the most likely parameter estimate.</p></list-item><list-item><p>An uncertainty measure, which quantifies the dispersion of the 50% most probable samples using the interquartile range, relative to the prior range.</p></list-item><list-item><p>An ambiguity measure, which measures the FWHM, in percentage with respect to the prior range.</p></list-item></list><p><xref ref-type="fig" rid="fig2">Figure 2</xref> presents those measures on exemplar posterior distributions.</p></sec></sec><sec id="s4-3"><title>Application of µGUIDE to biophysical modelling of dMRI data</title><p>We show exemplar applications of µGUIDE to three biophysical models of increasing complexity and degeneracy from the dMRI literature. For each model, we compare the fitting quality of the posterior distributions obtained using the MLP and manually defined summary statistics.</p><sec id="s4-3-1"><title>Biophysical models of dMRI signal</title><sec id="s4-3-1-1"><title>Model 1: Ball&amp;Stick (<xref ref-type="bibr" rid="bib8">Behrens et al., 2003</xref>)</title><p>This is a two-compartment model (intra- and extra-neurite space) where the dMRI signal from the brain tissue is modelled as a weighted sum, with weight <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, of signals from water diffusing inside the neurites, approximated as sticks (i.e. cylinders of zero radius) with diffusivity <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and water diffusing within the extra-neurite space, approximated as Gaussian diffusion in an isotropic medium with diffusivity <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>. The direction of the stick is randomly sampled on a sphere. This model has the main advantage of being non-degenerate. We define the summary statistics as the direction-averaged signal (six b-shells, see section dMRI data acquisition and processing).</p></sec><sec id="s4-3-1-2"><title>Model 2: SM (<xref ref-type="bibr" rid="bib62">Novikov et al., 2019</xref>)</title><p>Expanding on Model 1, this model represents the dMRI signal from the brain tissue as a weighted sum of the signal from water diffusing within the neurite space, approximated as sticks with symmetric orientation dispersion following a Watson distribution and water diffusing within the extra-neurite space, modelled as anisotropic Gaussian diffusion. The microstructure parameters of this two-compartment model are the neurite signal fraction <inline-formula><mml:math id="inf79"><mml:mi>f</mml:mi></mml:math></inline-formula>, the intra-neurite diffusivity <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>D</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>, the orientation dispersion index <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, and the parallel and perpendicular diffusivities within the extra-neurite space <inline-formula><mml:math id="inf82"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mo lspace="0em" rspace="0em">∥</mml:mo></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msubsup></mml:math></inline-formula>. We use the LEMONADE (<xref ref-type="bibr" rid="bib61">Novikov et al., 2018</xref>) system of equations, which is based on a cumulant decomposition of the signal, to define six summary statistics.</p></sec><sec id="s4-3-1-3"><title>Model 3: extended-SANDI (<xref ref-type="bibr" rid="bib64">Palombo et al., 2020</xref>)</title><p>This is a three-compartment model (intra-neurite, intra-soma, and extra-cellular space) where the dMRI signal from the brain tissue is modelled as a weighted sum of the signal from water diffusing within the neurite space, approximated as sticks with symmetric orientation dispersion following a Watson distribution; water diffusing within cell bodies (namely soma), modelled as restricted diffusion in spheres; and water diffusing within the extra-cellular space, modelled as isotropic Gaussian diffusion. The parameters of interest are the neurite signal fraction <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>, the intra-neurite diffusivity <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>D</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>, the orientation dispersion index <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>O</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, the extra-cellular signal fraction <inline-formula><mml:math id="inf87"><mml:msub><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> and isotropic diffusivity <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>, the soma signal fraction <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, and a proxy of soma radius and diffusivity <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, defined as (<xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>):<disp-formula id="equ8"><mml:math id="m8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msup><mml:mi>δ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover></mml:mrow><mml:mfrac><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo fence="true" form="prefix">(</mml:mo><mml:mn>2</mml:mn><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>δ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix">)</mml:mo></mml:mrow><mml:mtext/><mml:mo separator="true">,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> the soma radius and diffusivity, respectively, and <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> the <italic>m</italic>th root of <inline-formula><mml:math id="inf94"><mml:mrow><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msup><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>J</mml:mi><mml:mfrac><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mfrac><mml:mn>5</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the Bessel functions of the first kind. We use the six summary statistics defined in <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>, which are based on a high and low <italic>b</italic>-value signal expansion. Signal fractions follow the rule <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, leading to six parameters to estimate for this model.</p><p>Prior distributions <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are defined as uniform distributions over biophysically plausible ranges. Signal fractions are defined within the interval <inline-formula><mml:math id="inf98"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mn>1</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, diffusivities between 0.1 and 3 µm<sup>2</sup>/ms, ODI between 0.03 and 0.95, and <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> between 0.15 and 1105 µm<sup>2</sup> (which correspond to <inline-formula><mml:math id="inf100"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">;</mml:mo><mml:mn>15</mml:mn><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> µm and fixed <inline-formula><mml:math id="inf101"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> µm<sup>2</sup>/ms).</p><p>The SM imposes the constraint <inline-formula><mml:math id="inf102"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msubsup><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mo lspace="0em" rspace="0em">∥</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. To generate samples uniformly distributed on the space defined by this condition, we are using two random variables <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, both sampled uniformly between 0 and 1, and then relate them to <inline-formula><mml:math id="inf105"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mo lspace="0em" rspace="0em">∥</mml:mo></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mo lspace="0em" rspace="0em">⟂</mml:mo></mml:msubsup></mml:math></inline-formula> using the following equations:<disp-formula id="equ9"><label>(7)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:mn>3.0</mml:mn><mml:mo>−</mml:mo><mml:mn>0.1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msqrt><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The extended-SANDI model requires for the signal fractions to sum to 1, that is <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. To uniformly cover the simplex <inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we define two new parameters <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf110"><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, uniformly sampled between 0 and 1, and use the following equations to get the corresponding signal fractions:<disp-formula id="equ10"><label>(8)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msqrt></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msqrt><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msqrt></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msqrt><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msqrt></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To ensure comparability of results, we extract the same number of features <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula> using the MLP as the number of summary statistics for each model. We therefore use <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> for the Ball&amp;Stick, the SM, and the extended-SANDI models. Although the number of features predicted by the MLP is fixed to <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> for the three models, the characteristics of these six features can be very different, depending on the chosen forward model and the available data (see Appendix 1). Training the MLP together with the NPE module allows to maximize inference performance in terms of accuracy and precision.</p></sec></sec></sec><sec id="s4-4"><title>Validation in numerical simulations</title><p>We start by validating the proposed method using PPCs and simulated signals from Model 2 (see more details in Appendix 4). Since PPC alone does not guarantee the correctness of the estimated posteriors, we further validated the obtained posterior distributions comparing them with the AMWG MCMC (<xref ref-type="bibr" rid="bib74">Roberts and Rosenthal, 2009</xref>). We generated simulations following the same acquisition protocol as the real data (see section dMRI data acquisition and processing), added Gaussian noise to the real and imaginary parts of the simulated signal with a signal-to-noise ratio (SNR) of 50, and then used the magnitude of this noisy complex signal for our experiments. We then estimated the posterior distributions using both µGUIDE and the MCMC method implemented in the MDT toolbox (<xref ref-type="bibr" rid="bib33">Harms and Roebroeck, 2018</xref>). We initialized the sampling using an MLE. We sampled 15,200 samples from the distribution, the first 200 ones being used as burn-in, and no thinning. Similarly, we sampled 15,000 samples from the estimated posterior distributions using µGUIDE.</p><p>Then, we show that µGUIDE can be applicable to any model. We use Models 1 and 3 as examples of simpler (and non-degenerate) and more complex (and degenerate) models than Model 2, respectively.</p><p>We compared the proposed framework to a state-of-the-art method for posterior estimation (<xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>). This method relies on manually defined summary statistics, while µGUIDE automatically extracts some features using an embedded neural network. µGUIDE was trained directly on noisy simulations. The manually defined summary statistics were extracted from these simulated noisy signals and then used as training dataset for an MAF, similar to <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref>.</p><p>Finally, we used µGUIDE to highlight degeneracy in all the models. While the complexity of the models increases, more degeneracy can be found. The degeneracy is inherent to the model definition, and is not induced by the noise. µGUIDE allows to highlight those degeneracies and quantify the confidence in the obtained estimates.</p><p>The training was performed on <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> numerical simulations for each model, computed using the MISST package (<xref ref-type="bibr" rid="bib36">Ianuş et al., 2017</xref>) and random combinations of the model parameters, each uniformly sampled from the previously defined ranges, with the addition of Rician distributed noise with SNR equivalent to the experimental data, that is 50.</p></sec><sec id="s4-5"><title>dMRI data acquisition and processing</title><p>We applied µGUIDE to dMRI data collected from two participants: a healthy volunteer from the WAND dataset (<xref ref-type="bibr" rid="bib58">McNabb et al., 2024</xref>) and an age-matched participant with epilepsy, acquired with the same protocol used for the MICRA dataset (<xref ref-type="bibr" rid="bib52">Koller et al., 2021</xref>). The MRI data from the healthy volunteer used in this work are part of a previously published dataset, publicly available at <ext-link ext-link-type="uri" xlink:href="https://doi.gin.g-node.org/10.12751/g-node.5mv3bf/">https://doi.gin.g-node.org/10.12751/g-node.5mv3bf/</ext-link>. We do not have the authorization to share the epileptic patient data. Data were acquired on a Connectome 3T scanner using a single-shot spin-echo, echo-planar imaging sequence with <italic>b</italic>-values = [200, 500, 1200, 2400, 4000, 6000] s mm<sup>−2</sup>, [20, 20, 30, 61, 61, 61] uniformly distributed directions, respectively, and 13 non-diffusion-weighted images at 2 mm isotropic resolution. TR was set to 3000 ms, TE to 59 ms, and the diffusion gradient duration and separation to 7 ms and 24 ms, respectively. Short diffusion times and TE were achieved thanks to the Connectom gradients, allowing to enhance the SNR and sensitivity to small water displacements (<xref ref-type="bibr" rid="bib45">Jones et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Setsompop et al., 2013</xref>). We considered the noise as Rician with an SNR of 50 for both subjects.</p><p>Data were preprocessed using a combination of in-house pipelines and tools from the FSL (<xref ref-type="bibr" rid="bib5">Andersson et al., 2003</xref>; <xref ref-type="bibr" rid="bib6">Andersson and Sotiropoulos, 2016</xref>; <xref ref-type="bibr" rid="bib78">Smith, 2002</xref>; <xref ref-type="bibr" rid="bib79">Smith et al., 2004</xref>) and MRTrix3 (<xref ref-type="bibr" rid="bib82">Tournier et al., 2019</xref>) software packages. The preprocessing steps included brain extraction (<xref ref-type="bibr" rid="bib78">Smith, 2002</xref>), denoising (<xref ref-type="bibr" rid="bib14">Cordero-Grande et al., 2019</xref>; <xref ref-type="bibr" rid="bib84">Veraart et al., 2016</xref>), drift correction (<xref ref-type="bibr" rid="bib86">Vos et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Sairanen et al., 2018</xref>), susceptibility-induced distortions (<xref ref-type="bibr" rid="bib5">Andersson et al., 2003</xref>; <xref ref-type="bibr" rid="bib79">Smith et al., 2004</xref>), motion and eddy current correction (<xref ref-type="bibr" rid="bib6">Andersson and Sotiropoulos, 2016</xref>), correction for gradient non-linearity distortions (<xref ref-type="bibr" rid="bib26">Glasser et al., 2013</xref>), and Gibbs ringing artefacts correction (<xref ref-type="bibr" rid="bib48">Kellner et al., 2016</xref>).</p></sec><sec id="s4-6"><title>dMRI data analysis</title><p>Diffusion signals were first normalized by the mean non-diffusion-weighted signals acquired for each voxel. Each voxel was then estimated in parallel using the µGUIDE framework. For each observed signal <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒗</mml:mi></mml:msub></mml:math></inline-formula> (i.e. for each voxel), we drew 50,000 samples via rejection sampling from <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒗</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for each model parameter <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, allowing to retrieve the full posterior distributions. If a posterior distribution was not deemed degenerate, the MAP, uncertainty, and ambiguity measures were extracted from the posterior distributions.</p><p>The manually defined summary statistics of the SM are defined using a cumulant expansion, which is only valid for small <italic>b</italic>-values. We therefore only used the <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>b</mml:mi><mml:mo>≤</mml:mo><mml:mn>2500</mml:mn></mml:mrow></mml:math></inline-formula> s mm<sup>−2</sup> data for this model. In order to obtain comparable results, we restricted the application of µGUIDE to this range of <italic>b</italic>-values as well. An extra b-shell (<italic>b</italic>-value = 5000 s mm<sup>−2</sup>; 61 directions) was interpolated using <italic>mapl</italic> (<xref ref-type="bibr" rid="bib24">Fick et al., 2016</xref>) for the extended-SANDI model when using the method developed by <xref ref-type="bibr" rid="bib37">Jallais et al., 2022</xref> based on summary statistics.</p><p>The training of µGUIDE was performed as described in section Validation in numerical simulations and an example of training dataset and input signal vector is provided in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Example training set and input signals for µGUIDE.</title><p>(<bold>A</bold>) Examples of input synthetic data vectors and corresponding ground truth model parameters used in the training set of Model 1 (Ball&amp;Stick). (<bold>B</bold>) Example of input measured signals from a voxel in a healthy participant, used for inference.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-fig8-v1.tif"/></fig><p>All the computations were performed both on CPU and GPU (NVIDIA GeForce RTX 4090).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-101069-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no new data have been generated for this manuscript. The MRI data used in this work are part of a previously published dataset, publicly available at <ext-link ext-link-type="uri" xlink:href="https://doi.gin.g-node.org/10.12751/g-node.5mv3bf/">https://doi.gin.g-node.org/10.12751/g-node.5mv3bf/</ext-link>. We do not have the authorization to share the epileptic patient data. The analysis codes underpinning the results presented here can be found on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/mjallais/uGUIDE">https://github.com/mjallais/uGUIDE</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib38">Jallais, 2024</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>McNabb</surname><given-names>CB</given-names></name><name><surname>Driver</surname><given-names>ID</given-names></name><name><surname>Hyde</surname><given-names>V</given-names></name><name><surname>Hughes</surname><given-names>G</given-names></name><name><surname>Chandler</surname><given-names>HL</given-names></name><name><surname>Thomas</surname><given-names>H</given-names></name><name><surname>Allen</surname><given-names>C</given-names></name><name><surname>Messaritaki</surname><given-names>E</given-names></name><name><surname>Hodgetts</surname><given-names>CJ</given-names></name><name><surname>Hedge</surname><given-names>C</given-names></name><name><surname>Engel</surname><given-names>M</given-names></name><name><surname>Standen</surname><given-names>SF</given-names></name><name><surname>Morgan</surname><given-names>EL</given-names></name><name><surname>Stylianopoulou</surname><given-names>E</given-names></name><name><surname>Manalova</surname><given-names>S</given-names></name><name><surname>Reed</surname><given-names>L</given-names></name><name><surname>Drakesmith</surname><given-names>M</given-names></name><name><surname>Germuska</surname><given-names>M</given-names></name><name><surname>Shaw</surname><given-names>AD</given-names></name><name><surname>Mueller</surname><given-names>L</given-names></name><name><surname>Rossiter</surname><given-names>H</given-names></name><name><surname>Davies-Jenkins</surname><given-names>CW</given-names></name><name><surname>Lancaster</surname><given-names>T</given-names></name><name><surname>Evans</surname><given-names>CJ</given-names></name><name><surname>Owen</surname><given-names>D</given-names></name><name><surname>Perry</surname><given-names>G</given-names></name><name><surname>Kusmia</surname><given-names>S</given-names></name><name><surname>Lambe</surname><given-names>E</given-names></name><name><surname>Partridge</surname><given-names>AM</given-names></name><name><surname>Cooper</surname><given-names>A</given-names></name><name><surname>Hobden</surname><given-names>P</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Graham</surname><given-names>KS</given-names></name><name><surname>Lawrence</surname><given-names>AD</given-names></name><name><surname>Wise</surname><given-names>RG</given-names></name><name><surname>Walters</surname><given-names>JTR</given-names></name><name><surname>Sumner</surname><given-names>P</given-names></name><name><surname>Singh</surname><given-names>KD</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>The Welsh Advanced Neuroimaging Database (WAND)</data-title><source>G-Node</source><pub-id pub-id-type="doi">10.12751/g-node.5mv3bf</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work, Maëliss Jallais and Marco Palombo are supported by UKRI Future Leaders Fellowship (MR/T020296/2). We are thankful to Dr. Dmitri Sastin and Dr. Khalid Hamandi for sharing their dataset from a participant with epilepsy, and to Dr. Carolyn McNabb, Dr. Eirini Messaritaki, and Dr. Pedro Luque Laguna for preprocessing the data of the healthy participant from the WAND data. The WAND data were acquired at the UK National Facility for In Vivo MR Imaging of Human Tissue Microstructure funded by the EPSRC (grant EP/M029778/1) and The Wolfson Foundation, and supported by a Wellcome Trust Investigator Award (096646/Z/11/Z) and a Wellcome Trust Strategic Award (104943/Z/14/Z). The WAND data are available at <ext-link ext-link-type="uri" xlink:href="https://doi.gin.g-node.org/10.12751/g-node.5mv3bf/">https://doi.gin.g-node.org/10.12751/g-node.5mv3bf/</ext-link>.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afzali</surname><given-names>M</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>SPHERIOUSLY? The challenges of estimating sphere radius non-invasively in the human brain from diffusion MRI</article-title><source>NeuroImage</source><volume>237</volume><elocation-id>118183</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118183</pub-id><pub-id pub-id-type="pmid">34020013</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A general framework for experiment design in diffusion MRI and its application in measuring direct tissue-microstructure features</article-title><source>Magnetic Resonance in Medicine</source><volume>60</volume><fpage>439</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1002/mrm.21646</pub-id><pub-id pub-id-type="pmid">18666109</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Modelling, fitting and sampling in diffusion MRI</chapter-title><person-group person-group-type="editor"><name><surname>Laidlaw</surname><given-names>D</given-names></name><name><surname>Weickert</surname><given-names>J</given-names></name></person-group><source>Visualization and Processing of Tensor Fields</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>3</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-88378-4_1</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Dyrby</surname><given-names>TB</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Imaging brain microstructure with diffusion MRI: practicality and applications</article-title><source>NMR in Biomedicine</source><volume>32</volume><elocation-id>e3841</elocation-id><pub-id pub-id-type="doi">10.1002/nbm.3841</pub-id><pub-id pub-id-type="pmid">29193413</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Skare</surname><given-names>S</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging</article-title><source>NeuroImage</source><volume>125</volume><fpage>1063</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.019</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ascoli</surname><given-names>GA</given-names></name><name><surname>Donohue</surname><given-names>DE</given-names></name><name><surname>Halavi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>NeuroMorpho.Org: A central resource for neuronal morphologies</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>9247</fpage><lpage>9251</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2055-07.2007</pub-id><pub-id pub-id-type="pmid">17728438</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Nunes</surname><given-names>RG</given-names></name><name><surname>Clare</surname><given-names>S</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name><name><surname>Brady</surname><given-names>JM</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Characterization and propagation of uncertainty in diffusion-weighted MR imaging</article-title><source>Magnetic Resonance in Medicine</source><volume>50</volume><fpage>1077</fpage><lpage>1088</lpage><pub-id pub-id-type="doi">10.1002/mrm.10609</pub-id><pub-id pub-id-type="pmid">14587019</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Mixture density networks</source><publisher-name>Technical Report</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>MGB</given-names></name><name><surname>Nunes</surname><given-names>MA</given-names></name><name><surname>Prangle</surname><given-names>D</given-names></name><name><surname>Sisson</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A comparative review of dimension reduction methods in approximate bayesian computation</article-title><source>Statistical Science</source><volume>28</volume><elocation-id>S406</elocation-id><pub-id pub-id-type="doi">10.1214/12-STS406</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Box</surname><given-names>GE</given-names></name><name><surname>Tiao</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Bayesian Inference in Statistical Analysis</source><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callaghan</surname><given-names>R</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ConFiG: Contextual Fibre Growth to generate realistic axonal packing for diffusion MRI simulation</article-title><source>NeuroImage</source><volume>220</volume><elocation-id>117107</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117107</pub-id><pub-id pub-id-type="pmid">32622984</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Henry</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Comparison of bootstrap approaches for estimation of uncertainties of DTI parameters</article-title><source>NeuroImage</source><volume>33</volume><fpage>531</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.07.001</pub-id><pub-id pub-id-type="pmid">16938472</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cordero-Grande</surname><given-names>L</given-names></name><name><surname>Christiaens</surname><given-names>D</given-names></name><name><surname>Hutter</surname><given-names>J</given-names></name><name><surname>Price</surname><given-names>AN</given-names></name><name><surname>Hajnal</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Complex diffusion-weighted image estimation via matrix recovery under general noise models</article-title><source>NeuroImage</source><volume>200</volume><fpage>391</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.039</pub-id><pub-id pub-id-type="pmid">31226495</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cranmer</surname><given-names>K</given-names></name><name><surname>Pavez</surname><given-names>J</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Approximating Likelihood Ratios with Calibrated Discriminative Classifiers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.02169">https://arxiv.org/abs/1506.02169</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cranmer</surname><given-names>K</given-names></name><name><surname>Brehmer</surname><given-names>J</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The frontier of simulation-based inference</article-title><source>PNAS</source><volume>117</volume><fpage>30055</fpage><lpage>30062</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Almeida Martins</surname><given-names>JP</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name><name><surname>Lampinen</surname><given-names>B</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>While</surname><given-names>PT</given-names></name><name><surname>Westin</surname><given-names>CF</given-names></name><name><surname>Szczepankiewicz</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural networks for parameter estimation in microstructural MRI: Application to a diffusion-relaxation model of white matter</article-title><source>NeuroImage</source><volume>244</volume><elocation-id>118601</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118601</pub-id><pub-id pub-id-type="pmid">34562578</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deoni</surname><given-names>SCL</given-names></name><name><surname>Rutt</surname><given-names>BK</given-names></name><name><surname>Arun</surname><given-names>T</given-names></name><name><surname>Pierpaoli</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Gleaning multicomponent T1 and T2 information from steady-state imaging data</article-title><source>Magnetic Resonance in Medicine</source><volume>60</volume><fpage>1372</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1002/mrm.21704</pub-id><pub-id pub-id-type="pmid">19025904</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing Data Using T-SNE</article-title><source>Journal of Machine Learning Research</source><volume>9</volume><elocation-id>11</elocation-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietrich</surname><given-names>O</given-names></name><name><surname>Raya</surname><given-names>JG</given-names></name><name><surname>Reeder</surname><given-names>SB</given-names></name><name><surname>Reiser</surname><given-names>MF</given-names></name><name><surname>Schoenberg</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Measurement of signal-to-noise ratios in MR images: influence of multichannel coils, parallel imaging, and reconstruction filters</article-title><source>Journal of Magnetic Resonance Imaging</source><volume>26</volume><fpage>375</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1002/jmri.20969</pub-id><pub-id pub-id-type="pmid">17622966</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diggle</surname><given-names>PJ</given-names></name><name><surname>Gratton</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Monte carlo methods of inference for implicit statistical models</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>46</volume><fpage>193</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1984.tb01290.x</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Durkan</surname><given-names>C</given-names></name><name><surname>Bekasov</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name><name><surname>Papamakarios</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural spline flows</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fearnhead</surname><given-names>P</given-names></name><name><surname>Prangle</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Constructing summary statistics for approximate bayesian computation: Semi-automatic approximate bayesian computation</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>74</volume><fpage>419</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2011.01010.x</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fick</surname><given-names>RHJ</given-names></name><name><surname>Wassermann</surname><given-names>D</given-names></name><name><surname>Caruyer</surname><given-names>E</given-names></name><name><surname>Deriche</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>MAPL: Tissue microstructure estimation using Laplacian-regularized MAP-MRI and its application to HCP data</article-title><source>NeuroImage</source><volume>134</volume><fpage>365</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.03.046</pub-id><pub-id pub-id-type="pmid">27043358</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Germain</surname><given-names>M</given-names></name><name><surname>Gregor</surname><given-names>K</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Made: masked autoencoder for distribution estimation</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>881</fpage><lpage>889</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Wilson</surname><given-names>JA</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Webster</surname><given-names>M</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title><source>NeuroImage</source><volume>80</volume><fpage>105</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonçalves</surname><given-names>PJ</given-names></name><name><surname>Lueckmann</surname><given-names>J-M</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Nonnenmacher</surname><given-names>M</given-names></name><name><surname>Öcal</surname><given-names>K</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Chintaluri</surname><given-names>C</given-names></name><name><surname>Podlaski</surname><given-names>WF</given-names></name><name><surname>Haddad</surname><given-names>SA</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title><source>eLife</source><volume>9</volume><elocation-id>e56261</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56261</pub-id><pub-id pub-id-type="pmid">32940606</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>IJ</given-names></name><name><surname>Pouget-Abadie</surname><given-names>J</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Warde-Farley</surname><given-names>D</given-names></name><name><surname>Ozair</surname><given-names>S</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generative Adversarial Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Nonnenmacher</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automatic Posterior Transformation for Likelihood-Free Inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.07488">https://arxiv.org/abs/1905.07488</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Guerreri</surname><given-names>M</given-names></name><name><surname>Epstein</surname><given-names>S</given-names></name><name><surname>Azadbakht</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Resolving Quantitative MRI Model Degeneracy with Machine Learning via Training Data Distribution Design</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2303.05464">https://arxiv.org/abs/2303.05464</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutmann</surname><given-names>MU</given-names></name><name><surname>Dutta</surname><given-names>R</given-names></name><name><surname>Kaski</surname><given-names>S</given-names></name><name><surname>Corander</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Likelihood-free inference via classification</article-title><source>Statistics and Computing</source><volume>28</volume><fpage>411</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s11222-017-9738-6</pub-id><pub-id pub-id-type="pmid">31997856</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gyori</surname><given-names>NG</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Clark</surname><given-names>CA</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Training data distribution significantly impacts the estimation of tissue microstructure with machine learning</article-title><source>Magnetic Resonance in Medicine</source><volume>87</volume><fpage>932</fpage><lpage>947</lpage><pub-id pub-id-type="doi">10.1002/mrm.29014</pub-id><pub-id pub-id-type="pmid">34545955</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harms</surname><given-names>RL</given-names></name><name><surname>Roebroeck</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robust and fast markov chain monte carlo sampling of diffusion MRI microstructure models</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>97</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00097</pub-id><pub-id pub-id-type="pmid">30618702</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriques</surname><given-names>RN</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Jespersen</surname><given-names>SN</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name><name><surname>Lundell</surname><given-names>H</given-names></name><name><surname>Ianuş</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Double diffusion encoding and applications for biomedical imaging</article-title><source>Journal of Neuroscience Methods</source><volume>348</volume><elocation-id>108989</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108989</pub-id><pub-id pub-id-type="pmid">33144100</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>AF</given-names></name><name><surname>Cottaar</surname><given-names>M</given-names></name><name><surname>Drakesmith</surname><given-names>M</given-names></name><name><surname>Fan</surname><given-names>Q</given-names></name><name><surname>Huang</surname><given-names>SY</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name><name><surname>Lange</surname><given-names>FJ</given-names></name><name><surname>Mollink</surname><given-names>J</given-names></name><name><surname>Rudrapatna</surname><given-names>SU</given-names></name><name><surname>Tian</surname><given-names>Q</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Estimating axial diffusivity in the NODDI model</article-title><source>NeuroImage</source><volume>262</volume><elocation-id>119535</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119535</pub-id><pub-id pub-id-type="pmid">35931306</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ianuş</surname><given-names>A</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Double oscillating diffusion encoding and sensitivity to microscopic anisotropy</article-title><source>Magnetic Resonance in Medicine</source><volume>78</volume><fpage>550</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1002/mrm.26393</pub-id><pub-id pub-id-type="pmid">27580027</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jallais</surname><given-names>M</given-names></name><name><surname>Rodrigues</surname><given-names>PLC</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Wassermann</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Inverting brain grey matter models with likelihood-free inference: A tool for trustable cytoarchitecture measurements</article-title><source>Machine Learning for Biomedical Imaging</source><volume>1</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.59275/j.melba.2022-a964</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jallais</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>µGUIDE</data-title><version designator="swh:1:rev:fd42ba23c94f6d43e6331ab07069c23e33038b94">swh:1:rev:fd42ba23c94f6d43e6331ab07069c23e33038b94</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:6d35d748a96bec70c832c4d7c224314d5e3a27d7;origin=https://github.com/mjallais/uGUIDE;visit=swh:1:snp:b0d1820b06d6965ae09826dc7c5bc748eef03586;anchor=swh:1:rev:fd42ba23c94f6d43e6331ab07069c23e33038b94">https://archive.softwareheritage.org/swh:1:dir:6d35d748a96bec70c832c4d7c224314d5e3a27d7;origin=https://github.com/mjallais/uGUIDE;visit=swh:1:snp:b0d1820b06d6965ae09826dc7c5bc748eef03586;anchor=swh:1:rev:fd42ba23c94f6d43e6331ab07069c23e33038b94</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jallais</surname><given-names>M</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Jelescu</surname><given-names>I</given-names></name><name><surname>Uhl</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Shining light on degeneracies and uncertainties in the NEXI and SANDIX models with µGUIDE</article-title><conf-name>ISMRM</conf-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jelescu</surname><given-names>IO</given-names></name><name><surname>Veraart</surname><given-names>J</given-names></name><name><surname>Fieremans</surname><given-names>E</given-names></name><name><surname>Novikov</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Degeneracy in model parameter estimation for multi-compartmental diffusion in neuronal tissue</article-title><source>NMR in Biomedicine</source><volume>29</volume><fpage>33</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1002/nbm.3450</pub-id><pub-id pub-id-type="pmid">26615981</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jelescu</surname><given-names>IO</given-names></name><name><surname>Budde</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Design and validation of diffusion MRI models of white matter</article-title><source>Frontiers in Physics</source><volume>28</volume><elocation-id>61</elocation-id><pub-id pub-id-type="doi">10.3389/fphy.2017.00061</pub-id><pub-id pub-id-type="pmid">29755979</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jelescu</surname><given-names>IO</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Bagnato</surname><given-names>F</given-names></name><name><surname>Schilling</surname><given-names>KG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Challenges for biophysical modeling of microstructure</article-title><source>Journal of Neuroscience Methods</source><volume>344</volume><elocation-id>108861</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108861</pub-id><pub-id pub-id-type="pmid">32692999</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jelescu</surname><given-names>IO</given-names></name><name><surname>de Skowronski</surname><given-names>A</given-names></name><name><surname>Geffroy</surname><given-names>F</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Novikov</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neurite Exchange Imaging (NEXI): A minimal model of diffusion in gray matter with inter-compartment water exchange</article-title><source>NeuroImage</source><volume>256</volume><elocation-id>119277</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119277</pub-id><pub-id pub-id-type="pmid">35523369</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Determining and visualizing uncertainty in estimates of fiber orientation from diffusion tensor MRI</article-title><source>Magnetic Resonance in Medicine</source><volume>49</volume><fpage>7</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1002/mrm.10331</pub-id><pub-id pub-id-type="pmid">12509814</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DK</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Bowtell</surname><given-names>R</given-names></name><name><surname>Cercignani</surname><given-names>M</given-names></name><name><surname>Dell’Acqua</surname><given-names>F</given-names></name><name><surname>McHugh</surname><given-names>DJ</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Parker</surname><given-names>GJM</given-names></name><name><surname>Rudrapatna</surname><given-names>US</given-names></name><name><surname>Tax</surname><given-names>CMW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Microstructural imaging of the human brain with a ‘super-scanner’: 10 key advantages of ultra-strong gradients for diffusion MRI</article-title><source>NeuroImage</source><volume>182</volume><fpage>8</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.047</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi</surname><given-names>HS</given-names></name><name><surname>Pal</surname><given-names>A</given-names></name><name><surname>Ning</surname><given-names>L</given-names></name><name><surname>Rathi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Likelihood-free posterior estimation and uncertainty quantification for diffusion MRI models</article-title><source>Imaging Neuroscience</source><volume>2</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1162/imag_a_00088</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kauermann</surname><given-names>G</given-names></name><name><surname>Claeskens</surname><given-names>G</given-names></name><name><surname>Opsomer</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bootstrapping for penalized spline regression</article-title><source>Journal of Computational and Graphical Statistics</source><volume>18</volume><fpage>126</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1198/jcgs.2009.0008</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kellner</surname><given-names>E</given-names></name><name><surname>Dhital</surname><given-names>B</given-names></name><name><surname>Kiselev</surname><given-names>VG</given-names></name><name><surname>Reisert</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Gibbs-ringing artifact removal based on local subvoxel-shifts</article-title><source>Magnetic Resonance in Medicine</source><volume>76</volume><fpage>1574</fpage><lpage>1581</lpage><pub-id pub-id-type="doi">10.1002/mrm.26054</pub-id><pub-id pub-id-type="pmid">26745823</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: a method for stochastic optimization</article-title><conf-name>International Conference on Learning Representations (ICLR</conf-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An introduction to variational autoencoders</article-title><source>Foundations and Trends in Machine Learning</source><volume>12</volume><fpage>307</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1561/2200000056</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>MA</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>Hund-Georgiadis</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>An investigation of functional and anatomical connectivity using magnetic resonance imaging</article-title><source>NeuroImage</source><volume>16</volume><fpage>241</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.1052</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koller</surname><given-names>K</given-names></name><name><surname>Rudrapatna</surname><given-names>U</given-names></name><name><surname>Chamberland</surname><given-names>M</given-names></name><name><surname>Raven</surname><given-names>EP</given-names></name><name><surname>Parker</surname><given-names>GD</given-names></name><name><surname>Tax</surname><given-names>CMW</given-names></name><name><surname>Drakesmith</surname><given-names>M</given-names></name><name><surname>Fasano</surname><given-names>F</given-names></name><name><surname>Owen</surname><given-names>D</given-names></name><name><surname>Hughes</surname><given-names>G</given-names></name><name><surname>Charron</surname><given-names>C</given-names></name><name><surname>Evans</surname><given-names>CJ</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>MICRA: Microstructural image compilation with repeated acquisitions</article-title><source>NeuroImage</source><volume>225</volume><elocation-id>117406</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117406</pub-id><pub-id pub-id-type="pmid">33045335</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lampinen</surname><given-names>B</given-names></name><name><surname>Szczepankiewicz</surname><given-names>F</given-names></name><name><surname>Lätt</surname><given-names>J</given-names></name><name><surname>Knutsson</surname><given-names>L</given-names></name><name><surname>Mårtensson</surname><given-names>J</given-names></name><name><surname>Björkman-Burtscher</surname><given-names>IM</given-names></name><name><surname>van Westen</surname><given-names>D</given-names></name><name><surname>Sundgren</surname><given-names>PC</given-names></name><name><surname>Ståhlberg</surname><given-names>F</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Probing brain tissue microstructure with MRI: principles, challenges, and the role of multidimensional diffusion-relaxation encoding</article-title><source>NeuroImage</source><volume>282</volume><elocation-id>120338</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120338</pub-id><pub-id pub-id-type="pmid">37598814</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazar</surname><given-names>M</given-names></name><name><surname>Alexander</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Bootstrap white matter tractography (BOOT-TRAC)</article-title><source>NeuroImage</source><volume>24</volume><fpage>524</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.08.050</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname><given-names>JM</given-names></name><name><surname>Goncalves</surname><given-names>PJ</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Öcal</surname><given-names>K</given-names></name><name><surname>Nonnenmacher</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Flexible statistical inference for mechanistic models of neural dynamics</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname><given-names>JM</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Karaletsos</surname><given-names>T</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Likelihood-free inference with emulator networks</article-title><conf-name>Proceedings of the 1st Symposium on Advances in Approximate Bayesian Inference</conf-name><fpage>32</fpage><lpage>53</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname><given-names>JM</given-names></name><name><surname>Boelts</surname><given-names>J</given-names></name><name><surname>Greenberg</surname><given-names>D</given-names></name><name><surname>Goncalves</surname><given-names>P</given-names></name><name><surname>Macke</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Benchmarking simulation-based inference</article-title><conf-name>Proceedings of the 24th International Conference on Artificial Intelligence and Statistics</conf-name><fpage>343</fpage><lpage>351</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>McNabb</surname><given-names>C</given-names></name><name><surname>Driver</surname><given-names>I</given-names></name><name><surname>Hyde</surname><given-names>V</given-names></name><name><surname>Hughes</surname><given-names>G</given-names></name><name><surname>Chandler</surname><given-names>H</given-names></name><name><surname>Thomas</surname><given-names>H</given-names></name><name><surname>Allen</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>The Welsh Advanced Neuroimaging Database (WAND)</data-title><source>G-Node</source><pub-id pub-id-type="doi">10.12751/g-node.5mv3bf</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metropolis</surname><given-names>N</given-names></name><name><surname>Rosenbluth</surname><given-names>AW</given-names></name><name><surname>Rosenbluth</surname><given-names>MN</given-names></name><name><surname>Teller</surname><given-names>AH</given-names></name><name><surname>Teller</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Equation of state calculations by fast computing machines</article-title><source>The Journal of Chemical Physics</source><volume>21</volume><fpage>1087</fpage><lpage>1092</lpage><pub-id pub-id-type="doi">10.1063/1.1699114</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mougel</surname><given-names>E</given-names></name><name><surname>Valette</surname><given-names>J</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Investigating exchange, structural disorder, and restriction in gray matter via water and metabolites diffusivity and kurtosis time-dependence</article-title><source>Imaging Neuroscience</source><volume>2</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1162/imag_a_00123</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novikov</surname><given-names>DS</given-names></name><name><surname>Veraart</surname><given-names>J</given-names></name><name><surname>Jelescu</surname><given-names>IO</given-names></name><name><surname>Fieremans</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rotationally-invariant mapping of scalar and orientational metrics of neuronal microstructure with diffusion MRI</article-title><source>NeuroImage</source><volume>174</volume><fpage>518</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.03.006</pub-id><pub-id pub-id-type="pmid">29544816</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novikov</surname><given-names>DS</given-names></name><name><surname>Fieremans</surname><given-names>E</given-names></name><name><surname>Jespersen</surname><given-names>SN</given-names></name><name><surname>Kiselev</surname><given-names>VG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Quantifying brain microstructure with diffusion MRI: Theory and parameter estimation</article-title><source>NMR in Biomedicine</source><volume>32</volume><elocation-id>e3998</elocation-id><pub-id pub-id-type="doi">10.1002/nbm.3998</pub-id><pub-id pub-id-type="pmid">30321478</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olesen</surname><given-names>JL</given-names></name><name><surname>Østergaard</surname><given-names>L</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name><name><surname>Jespersen</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Diffusion time dependence, power-law scaling, and exchange in gray matter</article-title><source>NeuroImage</source><volume>251</volume><elocation-id>118976</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.118976</pub-id><pub-id pub-id-type="pmid">35168088</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Ianus</surname><given-names>A</given-names></name><name><surname>Guerreri</surname><given-names>M</given-names></name><name><surname>Nunes</surname><given-names>D</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SANDI: A compartment-based model for non-invasive apparent soma and neurite imaging by diffusion MRI</article-title><source>NeuroImage</source><volume>215</volume><elocation-id>116835</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116835</pub-id><pub-id pub-id-type="pmid">32289460</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Valindria</surname><given-names>V</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Chiou</surname><given-names>E</given-names></name><name><surname>Giganti</surname><given-names>F</given-names></name><name><surname>Pye</surname><given-names>H</given-names></name><name><surname>Whitaker</surname><given-names>HC</given-names></name><name><surname>Atkinson</surname><given-names>D</given-names></name><name><surname>Punwani</surname><given-names>S</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Panagiotaki</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Joint estimation of relaxation and diffusion tissue parameters for prostate cancer with relaxation-VERDICT MRI</article-title><source>Scientific Reports</source><volume>13</volume><elocation-id>2999</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-30182-1</pub-id><pub-id pub-id-type="pmid">36810476</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panagiotaki</surname><given-names>E</given-names></name><name><surname>Schneider</surname><given-names>T</given-names></name><name><surname>Siow</surname><given-names>B</given-names></name><name><surname>Hall</surname><given-names>MG</given-names></name><name><surname>Lythgoe</surname><given-names>MF</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Compartment models of the diffusion MR signal in brain white matter: a taxonomy and comparison</article-title><source>NeuroImage</source><volume>59</volume><fpage>2241</fpage><lpage>2254</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.081</pub-id><pub-id pub-id-type="pmid">22001791</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panagiotaki</surname><given-names>E</given-names></name><name><surname>Walker-Samuel</surname><given-names>S</given-names></name><name><surname>Siow</surname><given-names>B</given-names></name><name><surname>Johnson</surname><given-names>SP</given-names></name><name><surname>Rajkumar</surname><given-names>V</given-names></name><name><surname>Pedley</surname><given-names>RB</given-names></name><name><surname>Lythgoe</surname><given-names>MF</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noninvasive quantification of solid tumor microstructure using VERDICT MRI</article-title><source>Cancer Research</source><volume>74</volume><fpage>1902</fpage><lpage>1912</lpage><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-13-2511</pub-id><pub-id pub-id-type="pmid">24491802</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast ɛ-free inference of simulation models with bayesian conditional density estimation</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Pavlakou</surname><given-names>T</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Masked autoregressive flow for density estimation</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Sterratt</surname><given-names>D</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sequential neural likelihood: fast likelihood-free inference with autoregressive flows</article-title><conf-name>The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>837</fpage><lpage>848</lpage></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Nalisnick</surname><given-names>E</given-names></name><name><surname>Rezende</surname><given-names>DJ</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>Lakshminarayanan</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Normalizing flows for probabilistic modeling and inference</article-title><source>The Journal of Machine Learning Research</source><volume>22</volume><fpage>2617</fpage><lpage>2680</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>GJM</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2003">2003</year><chapter-title>Probabilistic monte carlo based mapping of cerebral connections utilising whole-brain crossing fibre information</chapter-title><person-group person-group-type="editor"><name><surname>Taylor</surname><given-names>C</given-names></name><name><surname>Noble</surname><given-names>JA</given-names></name></person-group><source>Information Processing in Medical Imaging</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>684</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-45087-0_57</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Patron</surname><given-names>JPM</given-names></name><name><surname>Kypraios</surname><given-names>T</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Amortised inference in diffusion MRI biophysical models using artificial neural networks and simulation-based frameworks</article-title><conf-name>ISMRM</conf-name></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>GO</given-names></name><name><surname>Rosenthal</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Examples of Adaptive MCMC</article-title><source>Journal of Computational and Graphical Statistics</source><volume>18</volume><fpage>349</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1198/jcgs.2009.06134</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sairanen</surname><given-names>V</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Tax</surname><given-names>CMW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fast and accurate Slicewise OutLIer Detection (SOLID) with informed model estimation for diffusion MRI data</article-title><source>NeuroImage</source><volume>181</volume><fpage>331</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.07.003</pub-id><pub-id pub-id-type="pmid">29981481</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Setsompop</surname><given-names>K</given-names></name><name><surname>Kimmlingen</surname><given-names>R</given-names></name><name><surname>Eberlein</surname><given-names>E</given-names></name><name><surname>Witzel</surname><given-names>T</given-names></name><name><surname>Cohen-Adad</surname><given-names>J</given-names></name><name><surname>McNab</surname><given-names>JA</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Tisdall</surname><given-names>MD</given-names></name><name><surname>Hoecht</surname><given-names>P</given-names></name><name><surname>Dietz</surname><given-names>P</given-names></name><name><surname>Cauley</surname><given-names>SF</given-names></name><name><surname>Tountcheva</surname><given-names>V</given-names></name><name><surname>Matschl</surname><given-names>V</given-names></name><name><surname>Lenz</surname><given-names>VH</given-names></name><name><surname>Heberlein</surname><given-names>K</given-names></name><name><surname>Potthast</surname><given-names>A</given-names></name><name><surname>Thein</surname><given-names>H</given-names></name><name><surname>Van Horn</surname><given-names>J</given-names></name><name><surname>Toga</surname><given-names>A</given-names></name><name><surname>Schmitt</surname><given-names>F</given-names></name><name><surname>Lehne</surname><given-names>D</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Wedeen</surname><given-names>V</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Pushing the limits of in vivo diffusion MRI for the Human Connectome Project</article-title><source>NeuroImage</source><volume>80</volume><fpage>220</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.078</pub-id><pub-id pub-id-type="pmid">23707579</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slator</surname><given-names>PJ</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Westin</surname><given-names>CF</given-names></name><name><surname>Laun</surname><given-names>F</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Haldar</surname><given-names>JP</given-names></name><name><surname>Benjamini</surname><given-names>D</given-names></name><name><surname>Lemberskiy</surname><given-names>G</given-names></name><name><surname>de Almeida Martins</surname><given-names>JP</given-names></name><name><surname>Hutter</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Combined diffusion-relaxometry microstructure imaging: Current status and future prospects</article-title><source>Magnetic Resonance in Medicine</source><volume>86</volume><fpage>2987</fpage><lpage>3011</lpage><pub-id pub-id-type="doi">10.1002/mrm.28963</pub-id><pub-id pub-id-type="pmid">34411331</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fast robust automated brain extraction</article-title><source>Human Brain Mapping</source><volume>17</volume><fpage>143</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1002/hbm.10062</pub-id><pub-id pub-id-type="pmid">12391568</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Bannister</surname><given-names>PR</given-names></name><name><surname>De Luca</surname><given-names>M</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name><name><surname>Flitney</surname><given-names>DE</given-names></name><name><surname>Niazy</surname><given-names>RK</given-names></name><name><surname>Saunders</surname><given-names>J</given-names></name><name><surname>Vickers</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>De Stefano</surname><given-names>N</given-names></name><name><surname>Brady</surname><given-names>JM</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>RubiX: combining spatial resolutions for Bayesian inference of crossing fibers in diffusion MRI</article-title><source>IEEE Transactions on Medical Imaging</source><volume>32</volume><fpage>969</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1109/TMI.2012.2231873</pub-id><pub-id pub-id-type="pmid">23362247</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tejero-Cantero</surname><given-names>A</given-names></name><name><surname>Boelts</surname><given-names>J</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Lueckmann</surname><given-names>JM</given-names></name><name><surname>Durkan</surname><given-names>C</given-names></name><name><surname>Gonçalves</surname><given-names>PJ</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>SBI -- A Toolkit for Simulation-Based Inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2007.09114">https://arxiv.org/abs/2007.09114</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tournier</surname><given-names>JD</given-names></name><name><surname>Smith</surname><given-names>R</given-names></name><name><surname>Raffelt</surname><given-names>D</given-names></name><name><surname>Tabbara</surname><given-names>R</given-names></name><name><surname>Dhollander</surname><given-names>T</given-names></name><name><surname>Pietsch</surname><given-names>M</given-names></name><name><surname>Christiaens</surname><given-names>D</given-names></name><name><surname>Jeurissen</surname><given-names>B</given-names></name><name><surname>Yeh</surname><given-names>CH</given-names></name><name><surname>Connelly</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MRtrix3: A fast, flexible and open software framework for medical image processing and visualisation</article-title><source>NeuroImage</source><volume>202</volume><elocation-id>116137</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116137</pub-id><pub-id pub-id-type="pmid">31473352</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhl</surname><given-names>Q</given-names></name><name><surname>Pavan</surname><given-names>T</given-names></name><name><surname>Molendowska</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Jelescu</surname><given-names>IO</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Quantifying human gray matter microstructure using neurite exchange imaging (NEXI) and 300 mT/m gradients</article-title><source>Imaging Neuroscience</source><volume>2</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1162/imag_a_00104</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veraart</surname><given-names>J</given-names></name><name><surname>Novikov</surname><given-names>DS</given-names></name><name><surname>Christiaens</surname><given-names>D</given-names></name><name><surname>Ades-aron</surname><given-names>B</given-names></name><name><surname>Sijbers</surname><given-names>J</given-names></name><name><surname>Fieremans</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Denoising of diffusion MRI using random matrix theory</article-title><source>NeuroImage</source><volume>142</volume><fpage>394</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.016</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>M</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Valette</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revisiting double diffusion encoding MRS in the mouse brain at 11.7T: Which microstructural features are we sensitive to?</article-title><source>NeuroImage</source><volume>207</volume><elocation-id>116399</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116399</pub-id><pub-id pub-id-type="pmid">31778817</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vos</surname><given-names>SB</given-names></name><name><surname>Tax</surname><given-names>CMW</given-names></name><name><surname>Luijten</surname><given-names>PR</given-names></name><name><surname>Ourselin</surname><given-names>S</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Froeling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The importance of correcting for signal drift in diffusion MRI</article-title><source>Magnetic Resonance in Medicine</source><volume>77</volume><fpage>285</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1002/mrm.26124</pub-id><pub-id pub-id-type="pmid">26822700</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warner</surname><given-names>W</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Cruz</surname><given-names>R</given-names></name><name><surname>Callaghan</surname><given-names>R</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name><name><surname>Dell’Acqua</surname><given-names>F</given-names></name><name><surname>Ianus</surname><given-names>A</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Temporal Diffusion Ratio (TDR) for imaging restricted diffusion: Optimisation and pre-clinical demonstration</article-title><source>NeuroImage</source><volume>269</volume><elocation-id>119930</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.119930</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitcher</surname><given-names>B</given-names></name><name><surname>Tuch</surname><given-names>DS</given-names></name><name><surname>Wisco</surname><given-names>JJ</given-names></name><name><surname>Sorensen</surname><given-names>AG</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Using the wild bootstrap to quantify uncertainty in diffusion tensor imaging</article-title><source>Human Brain Mapping</source><volume>29</volume><fpage>346</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1002/hbm.20395</pub-id><pub-id pub-id-type="pmid">17455199</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yablonskiy</surname><given-names>DA</given-names></name><name><surname>Sukstanskii</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Theoretical models of the diffusion weighted MR signal</article-title><source>NMR in Biomedicine</source><volume>23</volume><fpage>661</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1002/nbm.1520</pub-id><pub-id pub-id-type="pmid">20886562</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Schneider</surname><given-names>T</given-names></name><name><surname>Wheeler-Kingshott</surname><given-names>CA</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>NODDI: Practical in vivo neurite orientation dispersion and density imaging of the human brain</article-title><source>NeuroImage</source><volume>61</volume><fpage>1000</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.03.072</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Correlation analysis between features extracted by µGUIDE and manually defined summary statistics</title><p><xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> correlation presents the correlation matrices obtained from the correlation between the MLP-extracted features from µGUIDE and the manually defined summary statistics defined in section Application of µGUIDE to biophysical modelling of dMRI data, considering noisy simulations (SNR = 50). For each model, at least one feature extracted by µGUIDE is not or weakly correlated with the summary statistics. Additional information, not contained in the summary statistics, is extracted by the MLP from the input signal, leading to reduced bias, uncertainty and ambiguity in the parameter estimates (see <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Correlation matrices between features extracted by the Multi-Layer Perceptron (MLP) in µGUIDE and manually defined summary features for the three models.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-app1-fig1-v1.tif"/></fig></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Impact of noise on the posterior distributions</title><p>Noise in the signal impacts the fitting quality of a biophysical model. <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A</xref> shows example posterior distributions for one combination of Model 2 parameters, with varying noise levels (no noise, <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>). <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1B</xref> presents uncertainties values obtained on 1000 simulations with varying SNRs. We observe that, as the SNR reduces (i.e. as the noise increases), uncertainty increases. Noise in the signal contributes to irreducible variance. The confidence in the estimates therefore reduces as the noise level increases.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>SNR uncertainty comparison between signals with different noise levels: no noise, <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> using Model 2.</title><p>(<bold>A</bold>) Posterior distributions obtained on one example parameter combination (vertical black dashed line) with the three noise levels. (<bold>B</bold>) Histogram of the uncertainty obtained for 1000 signals with different noise levels (in %). Similar ground truths are used for each noise level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-app2-fig1-v1.tif"/></fig></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Masked Autoregressive Flows</title><p>The conditional probability density function approximators used in this project belong to a class of neural networks called normalizing flows (NFs [<xref ref-type="bibr" rid="bib71">Papamakarios et al., 2021</xref>]). NFs provide a general way of transforming complex probability distributions over continuous random variables into simple base distributions <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mrow style="font-weight:bold;"><mml:mi>𝒛</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (such as normal distributions) through a chain of invertible and differentiable transformations <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>. By applying the change of variable formula, the target distribution <inline-formula><mml:math id="inf125"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mi>|</mml:mi><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be written as:<disp-formula id="equ11"><label>(9)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>det</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>𝒛</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo separator="true">;</mml:mo><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is invertible and differentiable (i.e. a diffeomorphism), and <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo separator="true">;</mml:mo><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the Jacobian of <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo separator="true">;</mml:mo><mml:mi>𝒙</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The forward direction allows for density evaluation, that is learning the mapping between the target and the base distributions, that is learning the parameters <inline-formula><mml:math id="inf129"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. The inverse direction allows to estimate a density estimator <inline-formula><mml:math id="inf130"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo lspace="0.22em" rspace="0.22em" stretchy="false">|</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> by sampling points <inline-formula><mml:math id="inf131"><mml:mi>𝒛</mml:mi></mml:math></inline-formula> from the base distribution and applying the inverse transform <inline-formula><mml:math id="inf132"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝒛</mml:mi><mml:mo separator="true">;</mml:mo><mml:msub><mml:mi>𝒙</mml:mi><mml:mstyle style="font-style:italic;font-family:Cambria, `Times New Roman`, serif;font-weight:bold;"><mml:mn>0</mml:mn></mml:mstyle></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. A main requirement is that the flow needs to be expressive enough to approximate any arbitrarily highly complex distribution. An interesting property of diffeomorphisms is that they are closed under composition, which means that a composition of <italic>K</italic> diffeomorphisms <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∘</mml:mo><mml:mo>⋯</mml:mo><mml:mo>∘</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is also a diffeomorphism, and the Jacobian determinant is the product of the determinant of each component. Combining multiple transformations allows to increase the expressivity of the general flow. We obtain:<disp-formula id="equ12"><label>(10)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>det</mml:mi><mml:mo>⁡</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>det</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Flows need to be flexible and expressive enough to model any desired distribution but also need to be computationally efficient, that is, computing the associated Jacobian determinants need to be tractable and efficient. Among a number of proposed architectures such as mixture density networks (<xref ref-type="bibr" rid="bib9">Bishop, 1994</xref>) or neural spline flows (<xref ref-type="bibr" rid="bib22">Durkan et al., 2019</xref>), we focused on MAFs (<xref ref-type="bibr" rid="bib69">Papamakarios et al., 2017</xref>), which has shown state-of-the-art performance as well as the ability to estimate multi-modal posterior distributions (<xref ref-type="bibr" rid="bib27">Gonçalves et al., 2020</xref>; <xref ref-type="bibr" rid="bib71">Papamakarios et al., 2021</xref>; <xref ref-type="bibr" rid="bib73">Patron et al., 2022</xref>).</p><p>Autoregressive flows are universal approximators and have the form <inline-formula><mml:math id="inf134"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>τ</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo separator="true">;</mml:mo><mml:msub><mml:mi>𝒉</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf135"><mml:mrow><mml:msub><mml:mi>𝒉</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝒛</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">&lt;</mml:mo><mml:mi>𝒊</mml:mi></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib71">Papamakarios et al., 2021</xref>). <inline-formula><mml:math id="inf136"><mml:mi>τ</mml:mi></mml:math></inline-formula> is termed the transformer and is a stritly monotonic function parametrized by <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>𝒉</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the <italic>i</italic>th conditioner. Each <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>𝒉</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> and therefore each <inline-formula><mml:math id="inf140"><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msubsup></mml:math></inline-formula> can be computed independently in parallel, helping to keep a low computation time. The conditioner constraints each output to depend only on variables with dimension indices less than <inline-formula><mml:math id="inf141"><mml:mi>i</mml:mi></mml:math></inline-formula>, which makes the Jacobian of the flow lower diagonal. Its determinant can then be obtained easily as the product of its diagonal elements. To efficiently implement the conditioner, this method relies on the MADE (<xref ref-type="bibr" rid="bib25">Germain et al., 2015</xref>) architecture. To create a neural network that obey the autoregressive structure of the conditioner, a fully connected feedforward neural network is multiplied to binary masks, which removes some connections by assigning them a weigh of 0. The binary masks can easily be obtained by following a few simple steps (see <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref> for an illustration):</p><list list-type="order"><list-item><p>Label the input and output nodes between 1 and <inline-formula><mml:math id="inf142"><mml:mi>D</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf143"><mml:mi>D</mml:mi></mml:math></inline-formula> being the dimension of the input vector <inline-formula><mml:math id="inf144"><mml:mi>𝒛</mml:mi></mml:math></inline-formula>.</p></list-item><list-item><p>Randomly assign each hidden unit a number between 1 and <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, which indicates the number of inputs it will be connected to.</p></list-item><list-item><p>For each hidden layer, connect the hidden units to units with inferior or equal labels.</p></list-item><list-item><p>Connect output units to units with strictly inferior labels.</p></list-item></list><p>For µGUIDE’s implementation, we use a combination of five MADEs. As a result, the MAF architecture only needs a single forward pass through the flow and, combined with the low-cost computation of the determinant, allows for fast training and evaluation of the posterior distributions.</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Schematic of Masked Autoencoder for Distribution Estimation (MADE) autoregressive network construction.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-app3-fig1-v1.tif"/></fig></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s11"><title>Posterior predictive checks</title><p>PPCs are a common safety check to verify inference is not wrong. The idea is to compare input signals with generated signals from samples drawn from the posterior distributions. If the inference is correct, the generated signals should look similar to the input signal.We performed the following steps:</p><list list-type="bullet"><list-item><p>Sample <inline-formula><mml:math id="inf146"><mml:mi>N</mml:mi></mml:math></inline-formula><inline-formula><mml:math id="inf147"><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> from the prior distribution: <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:mi>𝜽</mml:mi><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Generate the corresponding signals using the forward model: <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Perform the inference and estimate the posterior distributions <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Sample <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> samples <inline-formula><mml:math id="inf152"><mml:msub><mml:mi>𝜽</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Reconstruct the signals from the sampled <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>𝜽</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> using the forward model: <inline-formula><mml:math id="inf155"><mml:mrow><mml:msub><mml:mi>𝒙</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Compare the obtained <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>𝒙</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id="inf157"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula>.</p></list-item></list><p><xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref> presents results on Model 2 (SM), on both noise-free and noisy signals (Rician noise with SNR =50) for <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> random combinations of model parameters, and <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>. As dMRI data have a high dimensionality, we report the direction-average signal. Plain lines show the signals <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula>, and the shaded areas correspond to the area in which the corresponding <inline-formula><mml:math id="inf161"><mml:msub><mml:mi>𝒙</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> fall. <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> lie within the support of <inline-formula><mml:math id="inf163"><mml:msub><mml:mi>𝒙</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, indicating the inference is not wrong. Note that the support of <inline-formula><mml:math id="inf164"><mml:msub><mml:mi>𝒙</mml:mi><mml:mrow><mml:mi>𝒊</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>𝒔</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is bigger for noisy simulations, reflecting the wider posterior distributions obtained from the inference.</p><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Posterior predictive checks.</title><p>Comparison between signals <inline-formula><mml:math id="inf165"><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub></mml:math></inline-formula> generated using random parameter combinations and their reconstructions using samples from <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>p</mml:mi><mml:mo form="prefix" stretchy="false">(</mml:mo><mml:msub><mml:mi>𝜽</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mi>|</mml:mi><mml:msub><mml:mi>𝒙</mml:mi><mml:mi>𝒊</mml:mi></mml:msub><mml:mo form="postfix" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101069-app4-fig1-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101069.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sui</surname><given-names>Jing</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Beijing Normal University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The authors proposed an <bold>important</bold> novel deep-learning framework to estimate posterior distributions of tissue microstructure parameters. The method shows superior performance to conventional Bayesian approaches and there is <bold>convincing</bold> evidence for generalizing the method to use data from different protocol acquisitions and work with models of varying complexity.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101069.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors proposed a framework to estimate the posterior distribution of parameters in biophysical models. The framework has two modules: the first MLP module is used to reduce data dimensionality and the second NPE module is used to approximate the desired posterior distribution. The results show that the MLP module can capture additional information compared to manually defined summary statistics. By using the NPE module, the repetitive evaluation of the forward model is avoided, thus making the framework computationally efficient. The results show the framework has promise in identifying degeneracy. This is an interesting work.</p><p>Comment on revised version:</p><p>The authors have addressed all the raised concerns and made appropriate modifications to the manuscript. The changes have improved the clarity, methodology, and overall quality of the paper. Given these improvements, I believe the paper now meets the standards for publication in this journal.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101069.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors improve the work of Jallais et al. (2022) by including a novel module capable of automatically learning feature selection from different acquisition protocols inside a supervised learning framework. Combining the module above with an estimation framework for estimating the posterior distribution of model parameters, they obtain rich probabilistic information (uncertainty and degeneracy) on the parameters in a reasonable computation time.</p><p>The main contributions of the work are:</p><p>(1) The whole framework allows the user to avoid manually defining summary statistics, which may be slow and tedious and affect the quality of the results.</p><p>(2) The authors tested the proposal by tackling three different biophysical models for brain tissue and using data with characteristics commonly used by the diffusion-MR-microstructure research community.</p><p>(3) The authors validated their method well with the state-of-the-art.</p><p>(4) The methodology allows the quantification of the inherent model's degeneration and how it increases with strong noise.</p><p>The authors showed the utility of their proposal by computing complex parameter descriptors automatically in an achievable time for three different and relevant biophysical models.</p><p>Importantly, this proposal promotes tackling, analyzing, and considering the degenerated nature of the most used models in brain microstructure estimation.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101069.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jallais</surname><given-names>Maëliss</given-names></name><role specific-use="author">Author</role><aff><institution>Cardiff University</institution><addr-line><named-content content-type="city">Cardiff</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Palombo</surname><given-names>Marco</given-names></name><role specific-use="author">Author</role><aff><institution>Cardiff University</institution><addr-line><named-content content-type="city">Cardiff</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>The authors proposed a framework to estimate the posterior distribution of parameters in biophysical models. The framework has two modules: the first MLP module is used to reduce data dimensionality and the second NPE module is used to approximate the desired posterior distribution. The results show that the MLP module can capture additional information compared to manually defined summary statistics. By using the NPE module, the repetitive evaluation of the forward model is avoided, thus making the framework computationally efficient. The results show the framework has promise in identifying degeneracy. This is an interesting work.</p></disp-quote><p>We thank the reviewer for the positive comments made on our manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>I have some minor comments.</p><p>(1) The uGUIDE framework has two modules, MLP and NPE. Why are the two modules trained jointly? The MLP module is used to reduce data dimensionality. Given that the number of features for different models is all fixed to 6, why does one need different MLPs? This module should, in principle, be general-purpose and independent of the model used.</p></disp-quote><p>The MLP must be trained together with the NPE module to maximise inference performance in terms of accuracy and precision. Although the number of features predicted by the MLP was fixed to six, the characteristics of these six features can be very different, depending on the chosen forward model and the available data, as we showed in Appendix 1 Figure 1. Training the MLP independently of the NPE would result in suboptimal performance of µGUIDE, with potentially higher bias and variance of the predicted posterior distributions. We have now added these considerations in the Methods section.</p><disp-quote content-type="editor-comment"><p>(2) The authors mentioned at L463 that all the 3 models use 6 features. From L445 to L447, it seems model 3 has 7 unknown parameters. How can one use 6 features to estimate 7 unknowns?</p></disp-quote><p>Thank you for pointing out the lack of clarity regarding the parameters to estimate in this section. Model 3 is a three-compartment model, whose parameters of interest are the signal fraction and diffusivity from water diffusing in the neurite space (<italic>fn</italic> and <italic>Dn</italic>), the neurites orientation dispersion index (<italic>ODI</italic>), the signal fraction in cell bodies (<italic>fs</italic>), a proxy to soma radius and diffusivity (<italic>Cs</italic>), and the signal fraction and diffusivity in the extracellular space (<italic>fe</italic> and <italic>De</italic>). The signal fractions are constrained by the relationship <italic>fn + fs + fe = 1</italic>, hence _fe i_s calculated from the estimated <italic>fn</italic> and <italic>fs</italic>. This leaves us with 6 parameters to estimate: <italic>fn, Dn, ODI, fs, Cs, De.</italic> We clarified it in the revised version of the paper.</p><disp-quote content-type="editor-comment"><p>(3) L471, Rician noise is not a proper term. Rician distribution is the distribution of pixel intensities observed in the presence of noise. And Rician distribution is the result of magnitude reconstruction. See &quot;Noise in magnitude magnetic resonance images&quot; published in 2008. I assume that real-valued Gaussian noise is added to simulated data.</p></disp-quote><p>We apologize for the confusion. We added Gaussian noise to the real and imaginary parts of the simulated signals and then used the magnitude of this noisy complex signal for our experiments. We rephrased the sentence for more clarity.</p><disp-quote content-type="editor-comment"><p>(4) L475, why thinning is not used in MCMC? In figure 3, the MCMC results are more biased than uGUIDE, is it related to no thinning in MCMC?</p></disp-quote><p>We followed the recommendations by Harms et al. (2018) for the MCMC experiments. They analysed the impact of thinning (among other parameters) on the estimated posterior distributions. Their findings indicate that thinning is unnecessary and inefficient, and they recommend using more samples instead. For further details, we refer the reviewer to their publication, along with the theoretical works they cite. We have now added this note in the Methods section.</p><disp-quote content-type="editor-comment"><p>(5) Did the authors try model-fitting methods with different initializations to get a distribution of the parameters? Like the paper &quot;Degeneracy in model parameter estimation for multi‐compartmental diffusion in neuronal tissue&quot;. For the in vivo data, it is informative to see the model-fitting results.</p></disp-quote><p>No, we did not try model-fitting methods with different initializations because such methods provide only a partial description of the solution landscape, which can be interpreted as a partial posterior distribution. Although this approach can help to highlight the problem of degeneracy, it does not provide a complete description of all potential solutions. In contrast, MCMC estimates the full posterior distribution, offering a more accurate and precise characterization of degeneracies and uncertainties compared to model-fitting methods with varying initializations. Hence, we decided to use MCMC as benchmark. We have now added these considerations to the Discussion section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors improve the work of Jallais et al. (2022) by including a novel module capable of automatically learning feature selection from different acquisition protocols inside a supervised learning framework. Combining the module above with an estimation framework for estimating the posterior distribution of model parameters, they obtain rich probabilistic information (uncertainty and degeneracy) on the parameters in a reasonable computation time.</p><p>The main contributions of the work are:</p><p>(1) The whole framework allows the user to avoid manually defining summary statistics, which may be slow and tedious and affect the quality of the results.</p><p>(2) The authors tested the proposal by tackling three different biophysical models for brain tissue and using data with characteristics commonly used by the diffusion-MRmicrostructure research community.</p><p>(3) The authors validated their method well with the state-of-the-art.</p><p>The main weakness is:</p><p>(1) The methodology was tested only on scenarios with a signal-to-noise ratio (SNR) equal to 50. It is interesting to show results with lower SNR and without noise that the method can detect the model's inherent degenerations and how the degeneration increases when strong noise is present. I suggest expanding the Figure in Appendix 1 to include this information.</p><p>The authors showed the utility of their proposal by computing complex parameter descriptors automatically in an achievable time for three different and relevant biophysical models.</p></disp-quote><p>Importantly, this proposal promotes tackling, analysing, and considering the degenerated nature of the most used models in brain microstructure estimation.</p><p>We thank the reviewer for these positive remarks.</p><p>Concerning the main weakness highlighted by the reviewer: In our submitted work, we presented results both without noise and with a signal-to-noise ratio (SNR) equal to 50 (similar to the SNR in the experimental data analysed). Figure 5 shows exemplar posterior distributions obtained in a noise-free scenario, and Table 1 reports the number of degeneracies for each model on 10000 noise-free simulations. These results highlight that the presence of degeneracies is inherent to the model definition. Figures 3, 6 and 7 present results considering an SNR of 50. We acknowledge that results with lower SNR have not been included in the initial submission. To address this, we added a figure in the appendix illustrating the impact of noise on the posterior distributions. Specifically, Figure 1A of Appendix 2 shows posterior distributions estimated from signals generated using an exemplar set of model parameters with varying noise levels</p><p>(no noise, SNR=50 and SNR=25). Figure 1B presents uncertainties values obtained on 1000 simulations for each noise level. We observe that, as the SNR reduces, uncertainty increases. Noise in the signal contributes to irreducible variance. The confidence in the estimates therefore reduces as the noise level increases.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Some suggestions:</p><p>Panel A of Figure 2 may deserve a better explanation in the Figure's caption.</p></disp-quote><p>We agree that the description of panel A of figure 2 was succinct and added more explanation in the figure’s caption.</p><disp-quote content-type="editor-comment"><p>The caption of Figure 3 should mention that the panel's titles are the parameters of the used biophysical models.</p></disp-quote><p>We added in the caption of figure 3 that the names of the model parameters are indicated in the titles of the panels. We apologise for the confusion it may have created.</p><disp-quote content-type="editor-comment"><p>In equation (3), the authors should indicate the summation index.</p></disp-quote><p>We apologise for not putting the summation index in equation 3. We added it in the revised version.</p><disp-quote content-type="editor-comment"><p>In line 474, the authors should discuss if the systematic use of the maximum likelihood estimator as an initializer for the sampling does not bias the computed results.</p></disp-quote><p>Concerning the MCMC estimations, we followed the recommendations from Harms et al. (2018). They investigated the use of starting from the maximum likelihood estimator (MLE). They concluded that starting from the MLE allows to start in the stationary distribution of the Markov chain, removing the need for some burn-in. Additionally, they showed that initializing the sampling from the MLE has the advantage of removing salt- and pepper-like noise from the resulting mean and standard deviation maps. We have now added this note in the Methods section.</p></body></sub-article></article>