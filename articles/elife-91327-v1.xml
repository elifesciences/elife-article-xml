<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91327</article-id><article-id pub-id-type="doi">10.7554/eLife.91327</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91327.4</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Early parafoveal semantic integration in natural reading</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-328772"><name><surname>Pan</surname><given-names>Yali</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0062-4326</contrib-id><email>Y.Pan.1@bham.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-328773"><name><surname>Frisson</surname><given-names>Steven</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-185716"><name><surname>Federmeier</surname><given-names>Kara D</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-9219"><name><surname>Jensen</surname><given-names>Ole</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8193-8348</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>Centre for Human Brain Health, School of Psychology, University of Birmingham</institution></institution-wrap><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047426m28</institution-id><institution>Department of Psychology, Program in Neuroscience, and the Beckman Institute for Advanced Science and Technology, University of Illinois</institution></institution-wrap><addr-line><named-content content-type="city">Champaign</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ding</surname><given-names>Nai</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Zhejiang University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>05</day><month>07</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP91327</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-03"><day>03</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-04"><day>04</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.26.509511"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-11-28"><day>28</day><month>11</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91327.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-27"><day>27</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91327.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-11"><day>11</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91327.3"/></event></pub-history><permissions><copyright-statement>Â© 2023, Pan et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Pan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91327-v1.pdf"/><abstract><p>Humans can read and comprehend text rapidly, implying that readers might process multiple words per fixation. However, the extent to which parafoveal words are previewed and integrated into the evolving sentence context remains disputed. We investigated parafoveal processing during natural reading by recording brain activity and eye movements using MEG and an eye tracker while participants silently read one-line sentences. The sentences contained an unpredictable target word that was either congruent or incongruent with the sentence context. To measure parafoveal processing, we flickered the target words at 60 Hz and measured the resulting brain responses (i.e. <italic>Rapid Invisible Frequency Tagging, RIFT</italic>) during fixations on the pre-target words. Our results revealed a significantly weaker tagging response for target words that were incongruent with the previous context compared to congruent ones, even within 100ms of fixating the word immediately preceding the target. This reduction in the RIFT response was also found to be predictive of individual reading speed. We conclude that semantic information is not only extracted from the parafovea but can also be integrated with the previous context before the word is fixated. This early and extensive parafoveal processing supports the rapid word processing required for natural reading. Our study suggests that theoretical frameworks of natural reading should incorporate the concept of deep parafoveal processing.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reading</kwd><kwd>semantic processing</kwd><kwd>MEG</kwd><kwd>frequency tagging</kwd><kwd>parafoveal processing</kwd><kwd>language comprehension</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>ECF-2023-626</award-id><principal-award-recipient><name><surname>Pan</surname><given-names>Yali</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/207550</award-id><principal-award-recipient><name><surname>Jensen</surname><given-names>Ole</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/R018723/1</award-id><principal-award-recipient><name><surname>Jensen</surname><given-names>Ole</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>Wolfson Research Merit Award</award-id><principal-award-recipient><name><surname>Jensen</surname><given-names>Ole</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>During natural reading, before we fixate on a word, we can already extract its meaning and even integrate it into the evolving context.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Reading is a remarkable human skill that requires rapid processing of written words. We typically fixate each word for only 225â250ms, but nevertheless manage to encode its visual information, extract its meaning, and integrate it into the larger context, while also doing saccade planning (<xref ref-type="bibr" rid="bib70">Rayner, 2009</xref>). To overcome the tight temporal constraints during reading, we preview the next word in the parafovea before moving our eyes to it (<xref ref-type="bibr" rid="bib34">Jensen et al., 2021</xref>; <xref ref-type="bibr" rid="bib78">Reichle and Reingold, 2013</xref>; <xref ref-type="bibr" rid="bib83">Schotter, 2018</xref>). Substantial evidence suggests that parafoveal information can be extracted at various linguistic levels, including orthography (<xref ref-type="bibr" rid="bib15">Drieghe et al., 2005</xref>; <xref ref-type="bibr" rid="bib33">Inhoff, 1989</xref>; <xref ref-type="bibr" rid="bib35">Johnson et al., 2007</xref>; <xref ref-type="bibr" rid="bib97">White, 2008</xref>; <xref ref-type="bibr" rid="bib98">Williams et al., 2006</xref>), phonology (<xref ref-type="bibr" rid="bib4">Ashby et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Ashby and Rayner, 2004</xref>; <xref ref-type="bibr" rid="bib11">Chace et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Miellet and Sparrow, 2004</xref>; <xref ref-type="bibr" rid="bib65">Pollatsek et al., 1992</xref>; <xref ref-type="bibr" rid="bib68">Rayner et al., 1995</xref>), lexicality (<xref ref-type="bibr" rid="bib36">Kennedy and Pynte, 2005</xref>; <xref ref-type="bibr" rid="bib38">Kliegl et al., 2006</xref>), syntax (<xref ref-type="bibr" rid="bib85">Snell et al., 2017</xref>; <xref ref-type="bibr" rid="bib96">Wen et al., 2019</xref>), and semantics (<xref ref-type="bibr" rid="bib71">Rayner and Schotter, 2014</xref>; <xref ref-type="bibr" rid="bib80">Schotter, 2013</xref>; <xref ref-type="bibr" rid="bib81">Schotter et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Schotter and Jia, 2016</xref>); for a comprehensive review see <xref ref-type="bibr" rid="bib79">Schotter et al., 2012</xref>. However, for semantics in particular, controversy remains about the extent and type of information extracted from parafoveal processing under various conditions. Moreover, it is unknown when and how the previewed semantic information can be used â i.e., integrated into the evolving sentence context â which is an integral component of the ongoing reading process.</p><p>For some time, it was claimed that parafoveal preview was limited to perceptual features of words and did not extend to semantics (<xref ref-type="bibr" rid="bib32">Inhoff, 1982</xref>; <xref ref-type="bibr" rid="bib31">Inhoff and Rayner, 1980</xref>; <xref ref-type="bibr" rid="bib72">Rayner et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Rayner et al., 1986</xref>). However, eye tracking-based evidence for the extraction of parafoveal semantic information began to emerge from studies that used languages other than English, including Chinese (<xref ref-type="bibr" rid="bib89">Tsai et al., 2012</xref>; <xref ref-type="bibr" rid="bib102">Yan et al., 2012</xref>; <xref ref-type="bibr" rid="bib101">Yan et al., 2009</xref>; <xref ref-type="bibr" rid="bib107">Zhou et al., 2013</xref>) and German (<xref ref-type="bibr" rid="bib28">Hohenstein et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Hohenstein and Kliegl, 2014</xref>), and was eventually extended into English (<xref ref-type="bibr" rid="bib71">Rayner and Schotter, 2014</xref>; <xref ref-type="bibr" rid="bib81">Schotter et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Schotter and Jia, 2016</xref>; <xref ref-type="bibr" rid="bib94">Veldre and Andrews, 2018</xref>; <xref ref-type="bibr" rid="bib93">Veldre and Andrews, 2017</xref>; <xref ref-type="bibr" rid="bib91">Veldre and Andrews, 2016a</xref>; <xref ref-type="bibr" rid="bib92">Veldre and Andrews, 2016b</xref>). For example, (<xref ref-type="bibr" rid="bib82">Schotter and Jia, 2016</xref>) showed preview benefits on early gaze measures for plausible compared to implausible words, even for plausible words that were unrelated to the target. These results demonstrate that semantic information can indeed be extracted from parafoveal words. However, due to the limitations of the boundary paradigm, which only assesses effects after target words have been fixated, it is challenging to precisely determine when and how parafoveal semantic processing takes place. Furthermore, it is generally hard to distinguish between the effects of cross-saccade integration (e.g. the mismatch between the preview and the word fixated) and the effects of how differing words fit into the context itself (<xref ref-type="bibr" rid="bib91">Veldre and Andrews, 2016a</xref>; <xref ref-type="bibr" rid="bib92">Veldre and Andrews, 2016b</xref>).</p><p>Complementary evidence showing that semantic information can be extracted parafoveally, even in English, comes from electrophysiological studies. Context-based facilitation of semantic processing can be observed as reductions in the amplitude of the N400 component (<xref ref-type="bibr" rid="bib41">Kutas and Hillyard, 1984</xref>; <xref ref-type="bibr" rid="bib40">Kutas and Hillyard, 1980</xref>), a negative-going event-related potential (ERP) response observed between about 300 and 500ms after stimulus onset, which has been linked to semantic access (<xref ref-type="bibr" rid="bib14">DeLong et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Federmeier, 2022</xref>; <xref ref-type="bibr" rid="bib20">Federmeier et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="bib43">Lau et al., 2008</xref>). Basic effects of contextual congruency on the N400 â smaller responses to words that do versus do not fit a sentence context (e.g. to âbutterâ compared to âsocksâ after âHe spread the warm bread with â¦â) â are also observed for parafoveally-presented words (<xref ref-type="bibr" rid="bib2">AntÃºnez et al., 2022</xref>; <xref ref-type="bibr" rid="bib6">Barber et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Barber et al., 2010</xref>; <xref ref-type="bibr" rid="bib46">LÃ³pez-PerÃ©z et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Meade et al., 2021</xref>) and, even when all words are congruent, N400 responses to words in parafoveal preview, like those to foveated words, are graded by increasing context-based predictability (<xref ref-type="bibr" rid="bib64">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="bib63">Payne and Federmeier, 2017</xref>; <xref ref-type="bibr" rid="bib88">Stites et al., 2017</xref>). Although many of these effects have been measured in the context of unnatural reading paradigms (e.g. the âRSVP flanker paradigmâ), similar effects are obtain during natural reading. Using the stimuli and procedures from <xref ref-type="bibr" rid="bib82">Schotter and Jia, 2016</xref>, <xref ref-type="bibr" rid="bib2">AntÃºnez et al., 2022</xref> showed that N400 responses, measured relative to the fixation before the target words i.e., before the boundary change while the manipulated words were in parafoveal preview, were sensitive to the contextual plausibility of these previewed words. These studies suggest that semantic information is available from words before they are fixated, even if that information does not always have an impact on eye fixation patterns.</p><p>Thus, both eye tracking and electrophysiological studies have provided evidence suggesting that semantic information is extracted from words in parafoveal preview. However, most of these studies have been limited to measuring parafoveal preview from fixations to an immediately adjacent word, raising questions about exactly how far in advance semantic information might become available from parafoveal preview. Moreover, important questions remain about the extent to which parafoveally extracted semantic information can be functionally integrated into the building sentence-level representation. Although some ERP studies have found that the semantic information extracted from parafoveal preview is carried forward, affecting semantic processing when that same word is later fixated (<xref ref-type="bibr" rid="bib5">Barber et al., 2010</xref>; <xref ref-type="bibr" rid="bib64">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="bib88">Stites et al., 2017</xref>), other studies have not observed any downstream impact (<xref ref-type="bibr" rid="bib6">Barber et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Li et al., 2015</xref>). Furthermore, post-N400 ERP components, linked to more attentionally-demanding processes associated with message-building and revision, do not seem to be elicited during parafoveal preview (<xref ref-type="bibr" rid="bib45">Li et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Milligan et al., 2023</xref>; <xref ref-type="bibr" rid="bib64">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="bib84">Schotter et al., 2023</xref>). Therefore, critical questions remain about the time course and mechanisms by which semantic information is extracted and used during reading.</p><p>Answering those questions requires an approach that allows a more continuous and specific assessment of sensitivity to target word semantics during parafoveal processing across multiple fixations, and, in particular, that can speak to how attention is allocated across words during natural reading. We tackle these core issues using a new technique that combines the use of frequency tagging and the measurement of magnetoencephalography (MEG)-based signals.</p><p>Frequency tagging, also known as steady-state visually evoked potentials, involves flickering a visual stimulus at a specific frequency and then measuring the neuronal response associated with processing the stimulus (<xref ref-type="bibr" rid="bib58">Norcia et al., 2015</xref>; <xref ref-type="bibr" rid="bib95">Vialatte et al., 2010</xref>). It has been widely used to investigate visuospatial attention (<xref ref-type="bibr" rid="bib25">Gulbinaite et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Kritzman et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">MÃ¼ller et al., 2003</xref>; <xref ref-type="bibr" rid="bib54">MÃ¼ller et al., 1998</xref>; <xref ref-type="bibr" rid="bib58">Norcia et al., 2015</xref>; <xref ref-type="bibr" rid="bib95">Vialatte et al., 2010</xref>) and has recently been applied to language processing (<xref ref-type="bibr" rid="bib7">Beyersmann et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Montani et al., 2019</xref>; <xref ref-type="bibr" rid="bib100">Wu et al., 2023</xref>). However, the traditional frequency tagging technique flickers visual stimuli at a low-frequency band, usually below 30 Hz, such that the flickering can be visible and may interfere with the ongoing task. To address this limitation, we developed the rapid invisible frequency tagging (RIFT) technique, which involves flickering visual stimuli at a frequency above 60 Hz, making it invisible and non-disruptive to the ongoing task. Responses to RIFT have been shown to increase with the allocation of attention to the stimulus bearing the visual flicker (<xref ref-type="bibr" rid="bib9">Brickwedde et al., 2022</xref>; <xref ref-type="bibr" rid="bib16">Drijvers et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Duecker et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Ferrante et al., 2023</xref>; <xref ref-type="bibr" rid="bib26">Gutteling et al., 2022</xref>; <xref ref-type="bibr" rid="bib105">Zhigalov et al., 2021</xref>; <xref ref-type="bibr" rid="bib103">Zhigalov et al., 2019</xref>; <xref ref-type="bibr" rid="bib106">Zhigalov and Jensen, 2022</xref>; <xref ref-type="bibr" rid="bib104">Zhigalov and Jensen, 2020</xref>). In our previous study, we adapted RIFT to a natural reading task and found temporally-precise evidence for parafoveal processing at the lexical level (<xref ref-type="bibr" rid="bib60">Pan et al., 2021</xref>). The RIFT technique provides a notable advantage by generating a signal â the tagging response signal â specifically yoked to just the tagged word. This ensures a clear separation in processing the tagged word from the ongoing processing of other words, addressing a challenge faced by eye tracking and ERP/FRP approaches. Moreover, RIFT enables us to monitor the entire dynamics of attentional engagement with the tagged word, which may begin a few words before the tagged word is fixated.</p><p>In the current study, RIFT was utilised in a natural reading task to investigate parafoveal semantic integration. We recruited participants (n=34) to silently read one-line sentences while their eye movements and brain activity were recorded simultaneously by an eye-tracker and MEG. The target word in each sentence was always unpredictable (see Behavioural pre-tests in Methods) but was semantically congruent or incongruent with the preceding sentence context (for the characteristics of words, see <xref ref-type="table" rid="table1">Table 1</xref>). The target words were tagged by flickering an underlying patch, whose luminance kept changing in a 60 Hz sinusoid throughout the sentence presentation. The patch was perceived as grey, the same color as the background, making it invisible. To ensure that the flicker remained invisible across saccades, we applied a Gaussian transparent mask to smooth out sharp luminance changes around the edges (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Parafoveal processing of the target word was indexed by the RIFT responses recorded using MEG during fixations of pre-target words.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Characteristics of pre-target, target, and post-target words.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Pre-target</th><th align="left" valign="bottom">Target</th><th align="left" valign="bottom">Post-target</th></tr></thead><tbody><tr><td align="left" valign="bottom">Word frequency</td><td align="left" valign="bottom">124.5 (310.9)</td><td align="left" valign="bottom">62.2 (77.0)</td><td align="left" valign="bottom">3619.8 (6725.2)</td></tr><tr><td align="left" valign="bottom">Word Length</td><td align="left" valign="bottom">5.5 (1.1)</td><td align="left" valign="bottom">5.3 (1.1)</td><td align="left" valign="bottom">5.4 (2.0)</td></tr><tr><td align="left" valign="bottom">Position in the sentence</td><td align="left" valign="bottom">5.4 (3.0)</td><td align="left" valign="bottom">6.4 (1.3)</td><td align="left" valign="bottom">7.4 (1.3)</td></tr></tbody></table><table-wrap-foot><fn><p>Note. Word frequency is reported as the total CELEX frequency per million (<xref ref-type="bibr" rid="bib13">Davis, 2005</xref>). Word length is the number of letters in a given word. Position in the sentence refers to the location in the sequence of words where a given word is presented. The number of words in each sentence is 11.6Â±1.7 (mean Â± SD). All values shown here are mean with standard deviations in the parentheses. Note that the pre-target and post-target were identical for the congruent and incongruent conditions. The target word was counterbalanced over items such that it was congruent for one item and incongruent for another.</p></fn></table-wrap-foot></table-wrap><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The paradigm and the eye movement metrics.</title><p>(<bold>A</bold>) After the presentation of a cross-fixation at the screen centre for 1.2â1.6 s, a gaze-contingent box appeared near the left edge of the screen. Fixing the box for 0.2 s triggered the full sentence presentation. Participants (n=34) read 160 one-line sentences silently while brain activity and eye movements were recorded. Each sentence was embedded with one congruent or incongruent target word (see the dashed rectangle; not shown in the actual experiment). The target words could not be predicted based on the sentence context and word-level properties of congruent and incongruent targets were balanced by swapping them between two sentence frames. The target words were tagged by changing the luminance of the underlying patch (with a Gaussian mask) in a 60 Hz sinusoid throughout the sentence presentation (depicted as a bright blob, not shown in the actual experiment). Additionally, we included a small disk at the bottom right of the screen that displayed the tagging signal and was recorded by a photodiode throughout each trial. After reading, gazing at the bottom box for 0.2 s triggered the sentence offset. Twelve percent of the sentences were followed by a simple yes-or-no comprehension question. (<bold>B</bold>) The first fixation durations on the pre-target and target words when the target words were incongruent (in blue) or congruent (in orange) with the sentence context. Each dot indicates one participant (n=34). ***p&lt;0.001; n.s., not statistically significant; ITI, inter-trial interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91327-fig1-v1.tif"/></fig><p>This paradigm allows us to address three questions. First, we aimed to measure when in the course of reading people begin to direct attention to parafoveal words. Second, we sought to ascertain when semantic information obtained through parafoveal preview is integrated into the sentence context in a manner that affects reading behaviours. Modulations of pre-target RIFT responses by the contextual congruity of target words would serve as evidence that parafoveal semantic information has not only been extracted and integrated into the sentence context but that it is affecting how readers allocate attention across the text. Third, we explored whether these parafoveal semantic attention effects have any relationship to reading speed.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>No evidence for semantic parafoveal processing in the eye movement data</title><p>Like prior work measuring eye fixations during English reading (<xref ref-type="bibr" rid="bib32">Inhoff, 1982</xref>; <xref ref-type="bibr" rid="bib31">Inhoff and Rayner, 1980</xref>; <xref ref-type="bibr" rid="bib72">Rayner et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Rayner et al., 1986</xref>), we found no evidence for parafoveal semantic processing in the eye movement data (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left). A paired t-test comparing first fixation durations on the pre-target word showed no effect of contextual (in)congruity (<italic>t</italic><sub>(33)</sub> = 0.84, p=0.407, <italic>d</italic>=0.14, two-sided). However, first fixation durations on the target word were significantly longer when they were incongruent (versus congruent) with the context (<italic>t</italic><sub>(33)</sub> = 5.99, p=9.83 Ã 10<sup>â7</sup>, <italic>d</italic>=1.03, two-sided pairwise <italic>t</italic>-test; <xref ref-type="fig" rid="fig1">Figure 1B</xref>, right). In addition, we found that the contextual congruity of target words affected later eye movement measures (i.e. total gaze duration and the likelihood of refixation after the first pass reading), with additional processing evident when the target words were incongruent with the context compared with when they were congruent (<xref ref-type="fig" rid="app1fig1">Appendix 1âfigure 1</xref>).</p></sec><sec id="s2-2"><title>Parafoveal processing measured by RIFT</title><p>First, we performed a selection procedure to identify MEG sensors that responded to RIFT. We measured neural responses to the flickering target words by calculating the coherence between the MEG sensors and the tagging signal measured by a photodiode. A MEG sensor was considered a good tagging response sensor if it showed significantly stronger 60 Hz coherence during the pre-target intervals (with flicker) compared to the baseline intervals before the sentence presentation (without flicker). Both pre-target and baseline intervals were 1 s epochs. We then applied a cluster-based permutation test and identified sensor clusters that showed a robust tagging response (<italic>p</italic><sub>cluster</sub> &lt;.01; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Tagging response sensors were found in 29 out of 34 participants, and all subsequent analyses were based on these tagging response sensors (7.9Â±4.5 sensors per participant, M Â± SD). The sources of these responses were localised to the left visual association cortex (Brodmann area 18; <xref ref-type="fig" rid="fig2">Figure 2B</xref>) using Dynamic Imaging Coherent Sources (DICS) (<xref ref-type="bibr" rid="bib24">Gross et al., 2001</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural responses to the rapid invisible frequency tagging (RIFT).</title><p>(<bold>A</bold>) Topography of the RIFT response sensors over all participants (7.9Â±4.5 sensors per participant, M Â± SD). These sensors showed significantly stronger coherence to the tagging signal during the pre-target interval (with target words flickering in the parafovea) compared with the baseline interval (no flicker). Further analyses only included participants who had a RIFT response (n=29). (<bold>B</bold>) The source of the RIFT response sensors was localised to the left visual association cortex (MNI coordinates [-9â97 3] mm, Brodmann area 18). (<bold>C</bold>) The averaged 60 Hz coherence over the RIFT response sensors when participants fixated on words at different positions, where n indicates the target word and n-1 indicates the pre-target word. Error bars indicate the SE over participants (n=29). The shaded area indicates the RIFT responses when previewing the flickering target words. We compared the RIFT response at each word position with the baseline (the dashed line). ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, â p=0.051; n.s., not statistically significant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91327-fig2-v1.tif"/></fig><p>Next, we characterised the temporal dynamics of attentional allocation to the flickering target word by calculating the 60 Hz coherence during fixations on several words surrounding the target word (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The resulting RIFT response curve revealed that significant attention was allocated to the target word as far as three words prior, spanning 15.3Â±2.7 letters (M Â± SD), including the spaces between words. This range is consistent with previous estimations of the perceptual span of 12â15 letters during English reading (<xref ref-type="bibr" rid="bib48">McConkie and Rayner, 1975</xref>; <xref ref-type="bibr" rid="bib70">Rayner, 2009</xref>; <xref ref-type="bibr" rid="bib66">Rayner, 1975</xref>; <xref ref-type="bibr" rid="bib90">Underwood and McConkie, 1985</xref>), as reported in the eye movement literature. Moreover, as RIFT directly measures visual attention, the left-skewed RIFT response curve suggests that more visual attention is allocated towards the flickering target words before fixating on them, aligning with the left-to-right order of reading English. The normal size and left skewness of the perceptual span in our study suggest that RIFT did not influence attention distribution during natural reading. Notably, the strongest RIFT responses were observed during fixations on the pre-target word (i.e. word position N-1, <xref ref-type="fig" rid="fig2">Figure 2C</xref>), highlighting the suitability of RIFT for measuring neuronal activity associated with parafoveal processing during natural reading.</p></sec><sec id="s2-3"><title>Neural evidence for semantic parafoveal integration</title><p>Importantly, evidence for parafoveal semantic integration was found using RIFT (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The pre-target coherence was weaker when the sentence contained a contextually incongruent word, compared to when it was congruent (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We conducted a pairwise t-test and found a significant effect on the averaged pre-target coherence at 60 Hz (<italic>t</italic><sub>(28)</sub> = â2.561, p=0.016, <italic>d</italic>=0.476, two-sided pairwise; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). To avoid any contamination of the parafoveal measure with activity from target fixation, pre-target coherence was averaged over the minimum pre-target fixation duration across both conditions for each participant (97.4Â±14.1 ms, M Â± SD, denoted as a dashed rectangle). Next, we conducted a jackknife-based latency estimation and found that the congruency effect on the 60 Hz pre-target coherence had a significantly later onset when previewing an incongruent (116.0Â±1.9 ms, M Â± SD) compared to a congruent target word (91.4Â±2.1 ms, M Â± SD, denoted as a dashed rectangle; <italic>t</italic><sub>(28)</sub> = â2.172, p=0.039, two-sided; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). Therefore, both the magnitude and onset latency of the pre-target coherence were modulated by the contextual congruency of the target word, providing neural evidence that semantic information is integrated into the context during parafoveal processing, detectable within 100 ms after readers fixate on the pre-target word.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neural evidence for parafoveal semantic integration.</title><p>(<bold>A</bold>) The pre-target coherence spectrum averaged over the rapid invisible frequency tagging (RIFT) response sensors at the group level (n=29) when the subsequent target words were incongruent with the sentence context (top panel), congruent with the sentence context (middle panel), and the difference between the two conditions (bottom panel). The horizontal line indicates the tagging frequency at 60 Hz. The two vertical lines indicate the first fixation onset of the pre-target words and the average fixation offset. (<bold>B</bold>) The averaged 60 Hz coherence during the minimum pre-target intervals for each participant (97.4Â±14.1ms, M Â± SD; denoted as a dashed rectangle) with respect to the incongruent and congruent target words. Each dot indicates one participant, the horizontal lines inside of the violins indicate the mean values. The upright inserted figure shows the pre-target coherence difference over participants with the error bar as SE. (<bold>C</bold>) The onset latency of the pre-target coherence at the group level (n=29). The onset latency refers to the time when the coherence curve reaches its half maximum, denoted by the dotted lines. Zero time-point indicates the first fixation onset of the pre-target words. The shaded area shows SE around the mean value. *p&lt;.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91327-fig3-v1.tif"/></fig><p>We conducted a similar analysis of the coherence measured when participants fixated on the target word and found no significant modulations related to the contextual congruity of that target word, in either the magnitude (<italic>t</italic><sub>(28)</sub> = 0.499, p=0.622, <italic>d</italic>=0.093, two-sided pairwise) or onset latency (<italic>t</italic><sub>(28)</sub> = â0.280, p=0.782); (<xref ref-type="fig" rid="fig4">Figure 4</xref>) of the RIFT response. Thus, the parafoveal semantic integration effect identified during the pre-target intervals cannot be attributed to signal contamination from fixations on the target word induced by the temporal smoothing of filters.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural responses to the rapid invisible frequency tagging (RIFT) during the target interval.</title><p>(<bold>A</bold>) The target coherence spectrum averaged over the RIFT response sensors at the group level (n=29) when the target words were incongruent with the sentence context (top panel), or congruent with the sentence context (middle panel); the bottom panel shows the difference between the two conditions. The horizontal line indicates the tagging frequency at 60 Hz. The two vertical lines indicate the first fixation onset of the target words and the averaged fixation offset. (<bold>B</bold>) We averaged the 60 Hz coherence within the minimum target fixation duration over participants (97.6Â±15.7 ms, M Â± SD; denoted as a dashed rectangle). Each dot indicates one participant, and the horizontal lines inside of the violins indicate the mean values. The upright inserted figure shows the target coherence difference over participants with the error bar as SE. (<bold>C</bold>) A jackknife-based method was used to calculate the onset latency of the average coherence at the group level. The onset latency refers to the time when the coherence curve reaches its half maximum, denoted by the dotted lines. n.s., not statistically significant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91327-fig4-v1.tif"/></fig></sec><sec id="s2-4"><title>Parafoveal semantic integration is related to individual reading speed</title><p>The RIFT effects of congruency observed during the parafoveal preview of the targets showed that readers tend to allocate less attention to upcoming text when an upcoming word is semantically incongruent compared to when all words are congruent. If readers differ in the extent to which their attention is ârepelledâ by incongruent words, then we might expect that the magnitude of the RIFT effect would be related to reading speed. Therefore, we conducted a correlation analysis to investigate this relationship (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Individual reading speed was quantified as the number of words read per second from the congruent sentences in the study. We found a positive correlation between the pre-target coherence difference (incongruent - congruent) and individual reading speed (<italic>r</italic><sub>(27)</sub>=0.503, p=0.006; Spearmanâs correlation). This suggests that readers who show greater shifts in attentional allocation in response to semantic incongruity read more slowly on average.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Individual reading speed is correlated with the magnitude of the rapid invisible frequency tagging (RIFT) congruency effect.</title><p>Reading speed was measured as the number of words read per second in the congruent sentences. The RIFT effect was measured as the coherence difference during the pre-target fixations for sentences containing incongruent and congruent target words. Each dot indicates one participant (n=29, Spearman correlation). The shaded area represents the 95% CI.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91327-fig5-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the current natural reading study, we utilised RIFT to probe for evidence that readers are sensitive to the effect of contextual congruity of an upcoming target word during parafoveal processing. We found no significant modulation of fixation durations of pre-target words based on the contextual congruity of the target word (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). However, we observed a significant difference in the amount of covert attention allocated to the target when previewing congruent and incongruent target words (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Specifically, we found lower RIFT responses for parafoveal words that were incongruent compared to congruent with the previous context. Because the target words were always of low predictability, their semantic congruence could only be appreciated if they had been integrated (to some extent) with the unfolding context. Thus, the RIFT patterns provide compelling neural evidence that semantic information can not only be extracted but also integrated during parafoveal processing.</p><p>More specifically, we observed that pre-target coherence was weaker in magnitude (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and had a later onset latency (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) in response to a contextually incongruent target word compared to a congruent one. Two possible explanations for these findings can be considered. First, the decreased RIFT responses may be due to changes in the pattern of allocation of attention across the text during reading. When reading in English, attention continuously shifts from left to right. If the semantic information previewed in the parafovea cannot be easily integrated into the context, this pattern may be interrupted, leading to delayed and/or reduced allocation of attention to the parafoveal word, possibly because readers shift more attention to the currently fixated word or to previous words to ensure that they have decoded and understood what they have read thus far. On this view, the RIFT finding may reflect a covert âregressionâ of attention, similar to overt eye-movement regressions that sometimes occur when readers encounter semantically incongruous words (<xref ref-type="bibr" rid="bib2">AntÃºnez et al., 2022</xref>; <xref ref-type="bibr" rid="bib8">Braze et al., 2002</xref>; <xref ref-type="bibr" rid="bib56">Ni et al., 1998</xref>; <xref ref-type="bibr" rid="bib69">Rayner et al., 2004</xref>) (also see <xref ref-type="fig" rid="app1fig1">Appendix 1âfigure 1A</xref>). Alternatively, the reduction in RIFT responses could arise if readers shift attentional resources away from the text altogether. Previous work has demonstrated that tagging responses decrease as attention shifts from an external task (e.g. counting visual targets) to an internal task (e.g. counting heartbeats) (<xref ref-type="bibr" rid="bib39">Kritzman et al., 2022</xref>). Similarly, in a reading scenario, visually perceiving the flickering word constitutes an external task, while the internal task involves the semantic integration of previewed information into the context. If more attentional resources are internally directed when faced with the challenge of integrating a contextually incongruent word, fewer attentional resources would remain for processing the flickering word. This may be the kind of shift reflected in the reduction in RIFT responses. On either account, the reduced forward allocation of attention diminishes parafoveal processing, and, in turn, may tend to slow reading speed, as supported by our correlation results (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>Our results also provide information about the time course of semantic integration, as we found evidence that readers appreciated the incongruity â and thus must have begun to integrate the semantics of the parafoveal words with their ongoing message-level representation â by as early as within 100ms after fixating on the pre-target word. The timing of this parafoveal semantic effect appears remarkably early, considering that typical semantic access for a single word occurs no earlier than around 200ms, as demonstrated in the visual word recognition literature (<xref ref-type="bibr" rid="bib10">Carreiras et al., 2014</xref>). For instance, in a Go/NoGo paradigm, the earliest distinguishable brain activity related to category-related semantic information of a word occurs at 160ms (<xref ref-type="bibr" rid="bib1">Amsel et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Hauk et al., 2012</xref>). Therefore, the RIFT results presented here suggest that natural reading involves parallel processing that spans multiple words. The level of (covert) attention allocated to the target word, as indexed by the significant difference in RIFT responses compared to the baseline interval, was observed even three words in advance (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). This initial increase in RIFT coincided with the target entering the perceptual span (<xref ref-type="bibr" rid="bib48">McConkie and Rayner, 1975</xref>; <xref ref-type="bibr" rid="bib66">Rayner, 1975</xref>; <xref ref-type="bibr" rid="bib90">Underwood and McConkie, 1985</xref>), likely aligning with the initial extraction of lower-level perceptual information about the target. The emerging sensitivity of the RIFT signal to target plausibility, detected around 100ms after the fixation on the pre-target word, suggests that readers at that time had accumulated sufficient semantic information about the target words and integrated that information with the evolving context. Therefore, it is plausible that the initial semantic processing of the target word commenced even before the pre-target fixation and was distributed across multiple words. This parallel processing of multiple words facilitates rapid and fluent reading.</p><p>Our findings have significant implications for theories of reading. The occurrence and early onset of semantic integration in parafoveal vision suggest that words are processed in an exceptionally parallel manner, posing a challenge for existing serial processing models (<xref ref-type="bibr" rid="bib77">Reichle et al., 2009</xref>; <xref ref-type="bibr" rid="bib76">Reichle et al., 2006</xref>; <xref ref-type="bibr" rid="bib75">Reichle et al., 2003</xref>; <xref ref-type="bibr" rid="bib74">Reichle et al., 1998</xref>). At the same time, it is important to note that the fact that semantic integration begins in parafoveal vision does not mean that it is necessarily completed before a word is fixated. The fact that we observed semantic congruency effects on the fixation durations of the target words (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) suggests that additional processing is required to fully integrate the semantics with overt attention in foveal vision. This also aligns with previous studies that found some ERP responses to semantic violations, including the LPC (Late Positive Component), are elicited only during foveal processing, but not during parafoveal processing (<xref ref-type="bibr" rid="bib45">Li et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Milligan et al., 2023</xref>; <xref ref-type="bibr" rid="bib64">Payne et al., 2019</xref>; <xref ref-type="bibr" rid="bib84">Schotter et al., 2023</xref>).</p><p>Thus, RIFT measures complement eye tracking (and other) measures, providing unique information revealing multiple mechanisms at work during natural reading. The results of the present study are aligned with the SWIFT model of eye movement control in natural reading (<xref ref-type="bibr" rid="bib19">Engbert et al., 2005</xref>), wherein the activation field linked to a given word is hypothesised to be both temporally and spatially distributed. Indeed, we found that the initial increase in covert attention to the target word occurred as early as three words before, as measured by RIFT responses (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). These covert processes enable the detection of semantic incongruity (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="fig" rid="fig3">Figure 3C</xref>). However, it may occur at the non-labile stage of saccade programming, preventing its manifestation in fixation measures of the currently fixated pre-target word (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Therefore, the RIFT techniqueâs capacity to yoke patterns to a specific word offers a unique opportunity to track the activation field of word processing during natural reading. Additional processes, which do impact overt eye movement patterns, are then brought to bear when the target words are fixated, resulting in increased fixation durations for incongruous words. At that same point (i.e. the target word), however, the RIFT responses showed a null effect of congruency (<xref ref-type="fig" rid="fig4">Figure 4</xref>); it may be that the RIFT technique is better suited to capturing parafoveal compared to foveal attentional processes, in part because there are more motion-sensitive rod cells in the parafoveal than foveal area. Finally, even after readers move away from fixating the word, attention to the target can persist or be reinstated, as evidenced by patterns of regressions (<xref ref-type="fig" rid="app1fig1">Appendix 1âfigure 1A</xref>). Therefore, during natural reading, attention is distributed across multiple words. The highly flexible and distributed allocation of attention allows readers to be parallel processors and thereby read fluently and effectively (<xref ref-type="bibr" rid="bib19">Engbert et al., 2005</xref>; <xref ref-type="bibr" rid="bib18">Engbert et al., 2002</xref>; <xref ref-type="bibr" rid="bib86">Snell et al., 2018</xref>; <xref ref-type="bibr" rid="bib87">Snell and Grainger, 2019</xref>). Our natural reading paradigm, where all words are available on the screen and saccadic eye movements are allowed, makes it possible to capture the extensive parallel processing. Moreover, saccades have been found to coordinate our visual and oculomotor systems, further supporting the parallel processing of multiple words during natural reading (<xref ref-type="bibr" rid="bib62">Pan et al., 2023</xref>).</p><p>Two noteworthy limitations exist in the current study. Firstly, the construction of pretargetâtarget word pairs consistently follows an adjective-noun phrase structure, potentially leading to semantic violations arising from immediate local incongruence rather than a broader incongruence derived from the entire sentential context. While the context preceding target words was deliberately minimised to ensure a pure effect of bottom-up parafoveal processing rather than the confounding impact of top-down prediction, it is essential to recognize that information from both local and global contexts can exert distinct effects on word processing during natural reading (<xref ref-type="bibr" rid="bib99">Wong et al., 2024</xref>). Future investigations should incorporate more information-rich contexts to explore the extent to which the parafoveal semantic integration effect observed in this study can be generalised. Second, the correlation analysis between the pre-target RIFT effect and individual reading speed (<xref ref-type="fig" rid="fig5">Figure 5</xref>) does not establish a causal relationship between parafoveal semantic integration and reading performance. Given that the comprehension questions in the current study were designed primarily to maintain readersâ attention and the behavioural performance reached a ceiling level, employing more intricate comprehension questions in future studies would be ideal to accurately measure reading comprehension and reveal the impact of semantic parafoveal processing on it.</p><p>In summary, our findings show that parafoveal processing is not limited to simply extracting word information, such as lexical features, as demonstrated in our previous study (<xref ref-type="bibr" rid="bib60">Pan et al., 2021</xref>). Instead, the previewed parafoveal information from a given word can begin to be integrated into the unfolding sentence representation well before that word is fixated. Moreover, the impact of that parafoveal integration further interacts with reading comprehension by shaping the time course and distribution of attentional allocation â i.e., by causing readers to move attention away from upcoming words that are semantically incongruous. These results support the idea that words are processed in parallel and suggest that early and deep parafoveal processing may be important for fluent reading.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We recruited 36 native English speakers (24 females, 22.5Â±2.8 years old, mean Â± SD) with normal or corrected-to-normal vision. All participants are right-handed and without any history of neurological problems or a language disorder diagnosis. Two participants were excluded from the analysis due to poor eye tracking or falling asleep during the recordings, which left 34 participants (23 females). This sample size was determined based on our previous study with a similar experimental design (<xref ref-type="bibr" rid="bib60">Pan et al., 2021</xref>). The study was approved by the University of Birmingham Ethics Committee (under the approved Programme ERN_18-0226AP27). The informed consent form was signed by all participants after the nature and possible consequences of the studies were explained. Participants received Â£15 per hour or course credits as compensation for their participation.</p></sec><sec id="s4-2"><title>Stimuli</title><p>In total participants read 277 sentences, of which 117 sentences were fillers from a published paper (<xref ref-type="bibr" rid="bib97">White, 2008</xref>). The filler sentences were all plausible and were included to make sure the incongruent sentences were less than one-third of the sentence set. We constructed the remaining 160 sentences with 80 pairs of target words. In all sentences the context was low constraint; i.e., none of the target words could be predicted by the prior context (see Behavioural pre-tests below for details). The target word in each sentence was either incongruent or congruent with the sentence. To focus on semantic integration and avoid any confounds of word-level properties, we embedded each pair of target words in two different sentence frames. By swapping the target words within a pair of sentences, we created four sentences: two congruent ones and two incongruent ones. These were then counterbalanced over participants. In this way, we counterbalanced across lexical characteristics of the target words and characteristics of the sentence frames within each pair. Each participants read one version of the sentence set (A or B). For example, for the target pair <italic>brother/jacket</italic>, one participant read them in the congruent condition in the sentence set version A; while another participant read them in the incongruent condition in version B (see below, targets are in italic type for illustration, but in normal type in the real experiment).</p><p>A. Last night, my lazy <italic>brother</italic> came to the party 1 min before it was over.</p><p>Lily says this blue <italic>jacket</italic> will be a big fashion trend this fall.</p><p>B. Last night, my lazy <italic>jacket</italic> came to the party 1 min before it was over.</p><p>Lily says this blue <italic>brother</italic> will be a big fashion trend this fall.</p><p>For all sentences, the pre-target words were adjectives, and the target words were nouns (for detailed characteristics of the words please see <xref ref-type="table" rid="table1">Table 1</xref>). The word length of pre-target words was from 4 to 8 letters, and for target words was from 4 to 7 letters. The sentences were no longer than 15 words or 85 letters. The target words were embedded somewhere in the middle of each sentence and were never the first three or the last three words in a sentence. Please see Appendix for the full list of the sentence sets that were used in the current study.</p></sec><sec id="s4-3"><title>Behavioural pre-tests</title><p>We recruited native English speakers for two behavioural pre-tests of the sentence sets. These participants did not participate in the MEG session.</p><sec id="s4-3-1"><title>Predictability of target words</title><p>We carried out a cloze test to estimate the predictability of the target words and the contextual constraint of the sentences. Participants read sentence fragments consisting of the experimental materials up to but not including the target words. Then participants were asked to write down the first word that came to mind that could continue the sentence (no need to complete the whole sentence). Example:</p><p>Last night, my lazy ________________</p><p>Lily says this blue ________________</p><p>The predictability of a word was estimated as the percentage of participants who wrote down exactly this word in the cloze test. A target word with less than 10% predictability was deemed to be not predicted by the sentence context. In addition, sentences for which no word was predicted with 50% or greater probability were a low constraint. Twenty participants (aix males, 24.2Â±2.0 years old, mean Â± SD) took part in the first round of the pre-test. Eight sentences were replaced with new sentences because the target words were too predictable and/or the sentence was too constraining. We then conducted a second round of the predictability test with 21 new participants (seven males, 25.0Â±6.0 years old). None of the target words in this final set were predictable (2.3%Â±4.8%, mean Â± SD), and all the sentence contexts were low constraint (25.2%Â±11.8%).</p></sec><sec id="s4-3-2"><title>Plausibility of sentences</title><p>Two groups of participants were instructed to rate how plausible (or acceptable) each sentence was in the sentence set version A or B separately. Plausibility was rated on a seven-point scale with plausibility increasing from point 1â7. Sentences in the experiment were designed to be either highly implausible (the incongruent condition) or highly plausible (the congruent condition). To occupy the full range of the scale, we constructed 70 filler sentences with middle plausibility (e.g. sentence 1 below). In this example, sentences 2 and 3 were the incongruent and congruent sentences from the experiment.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="4"><bold>Implausible</bold></th><th align="right" valign="bottom" colspan="3"><bold>Plausible</bold></th></tr></thead><tbody><tr><td align="left" valign="bottom">1. Kate said that she saw lots of stars twinkling in the sky at noon.</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">2</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">4</td><td align="center" valign="bottom">5</td><td align="center" valign="bottom">6</td><td align="center" valign="bottom">7</td></tr><tr><td align="left" valign="bottom">2. Lily says this blue brother will be a big fashion trend this fall.</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">2</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">4</td><td align="center" valign="bottom">5</td><td align="center" valign="bottom">6</td><td align="center" valign="bottom">7</td></tr><tr><td align="left" valign="bottom">3. Little Jimmy picked up a box and put some coins inside of it.</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">2</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">4</td><td align="center" valign="bottom">5</td><td align="center" valign="bottom">6</td><td align="center" valign="bottom">7</td></tr><tr><td align="left" valign="bottom">â¦â¦</td><td align="center" valign="bottom">1</td><td align="center" valign="bottom">2</td><td align="center" valign="bottom">3</td><td align="center" valign="bottom">4</td><td align="center" valign="bottom">5</td><td align="center" valign="bottom">6</td><td align="center" valign="bottom">7</td></tr></tbody></table></table-wrap><p>For version A we recruited 27 participants (four males, 22.8Â±6.1 years old, mean Â± SD): The plausibility rating for the incongruent sentences was 2.08Â±0.79 (mean Â± SD); while for the congruent sentences was 6.18Â±0.56. For sentence set version B we recruited 22 participants (four males, 21.1Â±2.3 years old, one invalid dataset due to incomplete responses): The plausibility rating was 1.81Â±0.41 (mean Â± SD) for the sentences in the incongruent condition and 6.15Â±0.47 for the sentences in the congruent condition. These results showed that in both versions of the sentences set, incongruent sentences were viewed as highly implausible and congruent sentences as highly plausible.</p></sec></sec><sec id="s4-4"><title>Experimental procedure</title><p>Participants were seated 145 cm away from the projection screen in a dimly lit magnetically shielded room. The MEG gantry was set at 60 degrees upright and covered the participantâs whole head. We programmed in Psychophysics Toolbox â3 (<xref ref-type="bibr" rid="bib37">Kleiner et al., 2007</xref>) to present the one-line sentences on a middle-grey screen (RGB [128 128 128]). All words were displayed in black (RGB [0 0 0]) with an equal-spaced Courier New font. The font size was 20 and the font type was bold so that each letter and space occupied 0.316 visual degrees. The visual angle of the whole sentence was no longer than 27 visual degrees in the horizontal direction. The sentence set was divided into five blocks, each of which took about 7 min. There was a break of at least 1 min between blocks and participants pressed a button to continue the experiment at any time afterwards. Participants were instructed to read each sentence silently at their own pace and to keep their heads and body as stable as possible during the MEG session. Eye movements were acquired during the whole session. In total, the experiment took no longer than 55 min. While the current study was conducted using MEG, these procedures might also work with EEG. If so, this would make our approach accessible to more laboratories as EEG is less expensive. However, there are currently no studies directly comparing the RIFT response in EEG versus MEG. Therefore, it would be of great interest to investigate if the current findings can be replicated using EEG.</p><p>Within a trial, there was first a fixation cross presented at the centre of a middle-grey screen for 1.2â1.6 s. This was followed by a black square with a radius of 1 degree of visual angle. This square was placed at the vertical center, 2 degrees of visual angle away from the left edge of the screen. Participants had to gaze at this black âstarting squareâ for at least 0.2 s to trigger the onset of the sentence presentation. Afterward, the sentence would start from the location of the square (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The sentence was presented with an âending squareâ 5 degrees of visual angle below the screen centre. The âending squareâ was the same size as the âstarting squareâ but in grey colour (RGB [64 64 64]). A gaze at this âending squareâ for at least 0.1 s would end the presentation of the sentence. Then the trial ended with a blank middle-grey screen that lasted for 0.5 s. Randomly, 12% of the trials were followed by a statement about the content of the sentence that was just presented, and participants needed to answer âTrue or Falseâ by pressing a button. For example, the statement for sentence 2 was âLily has a prediction about the fashion trend in this fall,â and the correct answer was âTrue.â The statement for sentence 3 was âLittle Jimmy didnât have a box,â and the correct answer was âFalse.â All participants read the sentences carefully as shown by the high accuracy of answering (96.3%Â±4.7%, mean Â± SD).</p></sec><sec id="s4-5"><title>RIFT</title><sec id="s4-5-1"><title>Projection of the sentence stimuli</title><p>We projected the sentences from the stimulus computer screen in the experimenter room to the projection screen inside of the MEG room using a PROPixx DLP LED projector (VPixx Technologies Inc, Canada). The refresh rate of the PROPixx projector was up to 1440 Hz, while the refresh rate of the stimulus screen was only 120 Hz (1920Ã1200 pixels resolution). We displayed the sentence repeatedly in four quadrants of the stimulus computer screen. In each quadrant, the words were coded in three colour channels as RGB. The projector then interpreted these 12 colour channels (three channelsÃfour quadrants) as 12 individual greyscale frames, which were projected onto the projection screen in rapid succession. Therefore, the projection screen refreshed at 12 times the rate of the stimulus computer screen.</p></sec><sec id="s4-5-2"><title>Flickering of the target word</title><p>We added a square patch underneath the target word to frequency tag the target word. The side length of the square patch was the width of the target word plus the spaces on both sides (2â3Â° visual angle). We flickered the patch by changing its luminance from black to white at a 60 Hz sinusoid (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To reduce the visibility of the patch edges across saccades, we applied a Gaussian smoothed transparent mask on top of the square patch. The mask was created by a two-dimensional Gaussian function (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>):<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>â</mml:mo><mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where, <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi></mml:math></inline-formula> are the mesh grid coordinates for the flickering patch, and <italic>Ï</italic> is the <inline-formula><mml:math id="inf3"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:mi>y</mml:mi></mml:math></inline-formula> spread of the mask with <italic>Ï</italic>=0.02 degrees.</p><p>On average, the patch was perceived as middle-grey, the same colour as the background screen, which made it invisible to participants. The target word was still black, the same colour as the other words on the screen. To record the tagging signal, we attached a custom-made photodiode (Aalto NeuroImaging Centre, Finland) to the disk at the bottom right corner of the screen. The luminance of the disk varied the same as that of the flickering patch underneath the target word. The photodiode was plugged into the MEG system as an external channel.</p></sec></sec><sec id="s4-6"><title>Data acquisition</title><sec id="s4-6-1"><title>MEG</title><p>Brain data were acquired with a 306-sensor TRIUX Elekta Neuromag system, which consisted of 204 orthogonal planar gradiometers and 102 magnetometers (Elekta, Finland). After participants signed the consent form, we attached four head-position indicator coils (HPI coils) to their heads: two on the left and right mastoid bone, and two on the forehead with at least 3 cm distance in between. Afterward, we used a Polhemus Fastrack electromagnetic digitizer system (Polhemus Inc, USA) to digitize the locations for three bony fiducial points: the nasion, left, and right preauricular points. Then we digitised the four HPI coils. Furthermore, at least 200 extra points were acquired, which were distributed evenly and covered the whole scalp. These points were used later in the source analysis when spatially co-registering the MEG head model with individual structural MRI images. The sampling rate of the MEG system was 1000 Hz. Data were band-pass filtered prior to sampling from 0.1 to 330 Hz to reduce aliasing effects.</p></sec><sec id="s4-6-2"><title>Eye movements</title><p>We used an EyeLink 1000 Plus eye-tracker (long-range mount, SR Research Ltd, Canada) to track eye movements throughout the whole MEG session. The eye tracker was placed on a wooden table in front of the projection screen. The centre of the eye tracker was at the middle line of the projection screen, and the top of the eye tracker reached the bottom edge of the screen. The distance between the eye-tracker camera and the centre of the participantâs eyes was 90 cm. We recorded the horizontal and vertical positions as well as the pupil size from the left eye, at a sampling rate of 1000 Hz. Each session began with a nine-point calibration and validation test. The test was accepted if the eye-tracking error was below 1 visual degree both horizontally and vertically. During the session, we performed a one-point drift-checking test every three trials and after the break between blocks. If the drift checking failed or the sentence presentation was unable to be triggered through gazing, a nine-point calibration and validation test was conducted again.</p></sec><sec id="s4-6-3"><title>MRI</title><p>After MEG data acquisition, participants were asked to come to the laboratory another day to have an MRI image acquired. We acquired the T1-weighted structural MRI image using a 3-Tesla Siemens PRISMA scanner (TR = 2000 ms, TE = 2.01 ms, TI = 880 ms, flip angle = 8 degrees, FOV = 256Ã256Ã208 mm, 1 mm isotropic voxel). For 11 participants who dropped out of the MRI acquisition, the MNI template brain (Montreal, Quebec, Canada) was used instead in the source analysis later.</p></sec></sec><sec id="s4-7"><title>Eye movement data analysis</title><p>We extracted the fixation onset events from the EyeLink output file. The EyeLink parsed fixation events based on the online detection of saccade onset using the following parameters: the motion threshold as 0.1 degrees, the velocity threshold as 30 degrees/s, and the acceleration threshold as 8000 degrees/sec2. These conservative settings were suggested by the EyeLink user manual for reading studies, as they can prevent false saccade reports and reduce the number of micro-saccades, and lengthen fixation durations.</p><p>Only the fixation that first landed on a given word was selected. The first fixation durations were averaged within the incongruent and congruent conditions for pre-target and target words. Pairwise, two-sided <italic>t</italic>-test were conducted on the first fixation durations of pre-target and target words separately (conducted in R <xref ref-type="bibr" rid="bib73">R Development Core Team, 2013</xref>). In addition to the early eye movement measure of the first fixation duration, we also conducted <italic>t</italic>-tests for two later eye movement measures. The likelihood of refixation was measured as the proportion of trials on which there was at least one saccade that regressed back to that word. The total gaze duration was the sum of all fixations on a given word, including those fixations during regression or re-reading.</p></sec><sec id="s4-8"><title>MEG data analyses</title><p>The data analyses were performed in MATLAB R2020a (Mathworks Inc, USA) by using the FieldTrip (<xref ref-type="bibr" rid="bib59">Oostenveld et al., 2011</xref>) toolbox (version 20200220), following the FLUX MEG analysis pipeline (<xref ref-type="bibr" rid="bib22">Ferrante et al., 2022</xref>), and custom-made scripts (see Code availability for the shared link).</p><sec id="s4-8-1"><title>Pre-processing</title><p>We first band-pass filtered the MEG data from 0.5 to 100 Hz using phase-preserving two-pass Butterworth filters. Subsequently, detrending was applied individually to each channel of the continuous raw data to factor out the linear trend. Malfunctioning sensors were removed based on inspecting the data quality during online recording (0â2 sensors per participant). Afterward, the data were decomposed into independent components using an independent component analysis (ICA) (<xref ref-type="bibr" rid="bib30">Ikeda and Toyama, 2000</xref>). The number of components was the same as the number of good MEG sensors in the dataset (306 or less). We only removed bad components that related to eye blinks, eye movements, and heartbeat by visually inspecting the components (3.4Â±0.7 components per participant, M Â± SD, range from 2 to 5 components).</p><p>MEG segments were extracted from â0.5â0.5 s intervals aligned with the first fixation onset of the pre-target and target words, respectively (see Eye movement data analysis, above, for information on how fixation onsets were defined). Segments with fixation durations shorter than 0.08 s or longer than 1 s were discarded. We also extracted 1 s long baseline segments, which were aligned with the cross-fixation onset before the sentence presentation. We manually inspected all segments to further identify and remove segments that were contaminated by muscle or movement artefacts.</p></sec><sec id="s4-8-2"><title>Coherence calculation</title><p>We calculated the coherence between the MEG sensors and the photodiode (i.e. the tagging signal) to quantify the tagging responses. The amplitude of the photodiode channel was normalised across each segment. To estimate the coherence spectrum in the frequency domain over time, we filtered the segments using hamming tapered Butterworth bandpass filters (fourth order, phase preserving, two-pass). The frequency of interest was from 40 to 80 Hz in a step of 2 Hz. For each center frequency point, the spectral smoothing was Â±5 Hz. For example, the filter frequency range for 60 Hz was from 55 to 65 Hz. We performed a Hilbert transform to obtain the analytic signals for each centre frequency point, which then were used to estimate the coherence (<xref ref-type="disp-formula" rid="equ1">Equation 2</xref>):<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>n</italic> is the number of trials. For the time point <italic>t</italic> in the trial <italic>j</italic>, <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> are the time-varying magnitude of the analytic signals from a MEG sensor (<italic>x</italic>) and the photodiode (<italic>y</italic>) respectively, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the phase difference as a function of time (for a detailed description, please see <xref ref-type="bibr" rid="bib12">Cohen, 2014</xref>).</p></sec><sec id="s4-8-3"><title>Selection for the RIFT response sensors</title><p>MEG sensors that showed significantly stronger coherence at 60 Hz during the pre-target segments than the baseline segments were selected as the RIFT response sensors. We used a non-parametric Monte-Carlo method (<xref ref-type="bibr" rid="bib47">Maris et al., 2007</xref>) to estimate the statistical significance. The pre-target segments were constructed by pooling the target contextual congruity conditions together. Several previous RIFT studies from our lab observed robust tagging responses from the visual cortex for flicker above 50 Hz (<xref ref-type="bibr" rid="bib16">Drijvers et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Duecker et al., 2021</xref>; <xref ref-type="bibr" rid="bib103">Zhigalov et al., 2019</xref>; <xref ref-type="bibr" rid="bib104">Zhigalov and Jensen, 2020</xref>). Thus, this sensor selection procedure was confined to the MEG sensors in the visual cortex (52 planar sensors). Here, the pre-target segments and baseline segments were treated as two conditions. For each combination of the MEG sensor and photodiode channel, coherence at 60 Hz was estimated over trials for the pre-target and baseline conditions separately. Then, we calculated the z-statistic value for the coherence difference between pre-target and baseline using the following equation (for details please see <xref ref-type="bibr" rid="bib47">Maris et al., 2007</xref>; <xref ref-type="disp-formula" rid="equ1">Equation 3</xref>):<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:msup><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:msup><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the coherence value for the pre-target and baseline segments, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the term used to correct for the bias from trial numbers of the pre-target (<inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) and baseline condition (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). All trials from the pre-target and baseline conditions were used.</p><p>After obtaining the z statistic value for the empirical coherence difference, we ran a permutation procedure to estimate the statistical significance of this comparison. We randomly shuffled the trial labels between pre-target and baseline conditions 5000 times. During each permutation, coherence was computed for both conditions (with shuffled labels), then entered <xref ref-type="disp-formula" rid="equ1">Equation 3</xref> to obtain a z score for the coherence difference. After all randomizations were performed, the resulting z-values established the null distribution. Since a tagging response sensor was supposed to have stronger coherence during the pre-target segments compared with the baseline segments, the statistical test was right-sided. If the z-value of the empirical coherence difference was larger than 99% of z-values in the null distribution, this sensor was selected as the RIFT response sensor (right-sided, p=0.01). For each participant, the coherence values were averaged over all sensors with significant tagging response to obtain an averaged coherence for further analyses. Please note that the tagging response sensors may vary in number across participants (7.9Â±4.5 sensors per participant, M Â± SD). Additionally, they may have a different but overlapping spatial layout, primarily over the visual cortex. For the topography of all tagging response sensors, please refer to <xref ref-type="fig" rid="fig2">Figure 2A</xref>.</p></sec><sec id="s4-8-4"><title>Coherence response curve</title><p>We first extracted MEG segments for the words N-4, N-3, N-2, N+1, N+2, and N+3 following the same procedure described in the pre-processing when extracted MEG segments for the pre-target (N-1) and target words (N). All segments were 1 s long, aligned with the first fixation onset to the word. Then, we calculated the coherence at 60 Hz during these segments for participants who have RIFT response sensors (n=29). Next for each participant, the 60 Hz coherence was first averaged over the RIFT response sensors, then averaged within a time window of [0 0.2] s (the averaged fixation duration for words). We got an averaged of 60 Hz coherence for the word at each position. We also got the 60 Hz coherence for the baseline interval averaged over [0 0.2] s, aligned with the cross-fixation onset. Then a pairwise <italic>t-</italic>test was performed between the baseline coherence and the coherence at each word position.</p></sec><sec id="s4-8-5"><title>Coherence comparison between conditions</title><p>The coherence comparison analyses were only conducted for the participants who had sensors with a reliable tagging response (n=29). To avoid any bias from trial numbers, an equal number of trials under the different contextual congruity conditions was entered into the coherence analysis per participant. We randomly discarded the redundant trials from the condition that had more trials for both the pre-target and target segments.</p><p>To compare the pre-target coherence amplitude between conditions, the coherence values at 60 Hz were averaged across the minimum fixation duration of all pre-target words. The time window for averaging was defined for each participant so that the coherence signal from the target fixation was not involved. Similarly, we averaged the 60 Hz coherence for the target segments over the minimum target fixation duration. Then, a two-sided pairwise Studentâs <italic>t</italic>-test was performed to estimate the statistical significance of the coherence difference as shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p><p>To assess the coherence onset latency difference between conditions, we used a leave-one-out Jackknife-based method (<xref ref-type="bibr" rid="bib51">Miller et al., 1998</xref>). We extracted the 60 Hz coherence during the 1 s long pre-target segments for each participant. Then, during each iteration of participants, we randomly chose and left out one participant. For the remaining participants, coherences were calculated for the incongruent and congruent target conditions. Then, the coherence was averaged over the remaining participants to estimate the onset latency for both conditions. Here, the onset latency was defined as the time point when the averaged coherence value reached its half-maximum (<inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). We computed the onset latency difference by subtracting the onset latency for the incongruent target condition from the congruent condition. After all iterations, onset latency differences from all these subsamples were pooled together to estimate a standard error (<italic>S<sub>D</sub></italic>) using the following equation (<xref ref-type="disp-formula" rid="equ1">Equation 4</xref>):<disp-formula id="equ5"><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>â</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mover accent="false"><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf15"><mml:mover accent="false"><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:math></inline-formula> is the average onset latency difference over all the subsamples, <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the coherence difference obtained from the subsample when participant <inline-formula><mml:math id="inf17"><mml:mi>i</mml:mi></mml:math></inline-formula> was left out, <inline-formula><mml:math id="inf18"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of participants. We also computed the onset latency difference from the overall sample set (without leaving any participant out) and divided it by the <italic>S<sub>D</sub></italic> to obtain its <italic>t</italic>-value. A standard t table (pairwise, two-tailed) provided the statistical significance for the coherence onset latency difference between the incongruent and congruent target conditions. This procedure was conducted for both the pre-target and target segments as shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="fig" rid="fig4">Figure 4C</xref>.</p></sec><sec id="s4-8-6"><title>Source analysis for RIFT</title><p>We used a beamforming-based approach, DICS (<xref ref-type="bibr" rid="bib24">Gross et al., 2001</xref>), to estimate the neural sources that generated the responses to RIFT. The DICS technique was applied to the pre-target segments (0â0.5 s aligned with fixation onset to the pre-target word) regardless of the target contextual congruity conditions, with a focus of 60 Hz in the frequency domain. In this source analysis, only participants with robust tagging responses were included (n=29).</p><p>First, we constructed a semi-realistic head model, where spherical harmonic functions were used to fit the brain surface (<xref ref-type="bibr" rid="bib57">Nolte, 2003</xref>). We aligned the individual structural MRI image with the head shape that was digitised during the MEG session. This was done by spatially co-registering the three fiducial anatomical markers (nasion, left and right ear canal) and extra points that covered the whole scalp. For participants whose MRI image was unavailable, the MNI template brain was used instead. The aligned MRI image was segmented into a grid, which was used to prepare the single-shell head model.</p><p>Next, we constructed the individual source model by inverse-warping a 5 mm spaced regular grid in the MNI template space to each participantâs segmented MRI image. We got the regular grid from the Fieldtrip template folder, which was constructed before doing the source analysis. In this way, the beamformer spatial filters were constructed on the regular grid that mapped to the MNI template space. Even though after this warping procedure grid points in the individual native space were no longer evenly spaced, the homologous grid points across participants were located at the same location in the normalised template space. Thus, the reconstructed sources can be directly averaged across participants on the group level.</p><p>Next, the Cross-Spectral Density (CSD) matrix was calculated at 60 Hz for both the pre-target and baseline segments. The CSD matrix was constructed for all possible combinations between the MEG sensors and the photodiode channel. No regularisation was performed to the CSD matrices (lambda = 0).</p><p>Finally, a common spatial filter was computed based on the individual single-shell head model, source model, and CSD matrices. This spatial filter was applied to both the pre-target and baseline CSD matrices for calculating the 60 Hz coherence. This was done by normalizing the magnitude of the summed CSD between the MEG sensor and the photodiode channel by their respective power. After the grand average over participants, the relative change for pre-target coherence was estimated as the following formula, <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> .</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Visualization, Methodology, Writing â original draft, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing â original draft, Writing â review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the University of Birmingham Ethics Committee(under the approved Programme ERN_18-0226AP27). The informed consent form was signed by all participants after the nature and possible consequences of the studies were explained.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91327-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>We have deposited the following data in the current study on figshare (<ext-link ext-link-type="uri" xlink:href="https://figshare.com/projects/Semantic/149801">https://figshare.com/projects/Semantic/149801</ext-link>): the epoch data after pre-processing, the raw EyeLink files, the Psychotoolbox data, and the head models after the co-registration of T1 images with the MEG data.The experiment presentation scripts (Psychtoolbox), statistics scripts (R), scripts and data to generate all figures (Matlab) are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/yalipan666/Semantic">https://github.com/yalipan666/Semantic</ext-link>, copy archived at <xref ref-type="bibr" rid="bib61">Pan, 2022</xref>).</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Headmodels</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21618657.v2</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>epoch data after pre-processing</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21206990.v4</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Experiment information</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21618708.v1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset4"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>EyeLink files</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21618645.v1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset5"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Psychtoolbox data</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21618636.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Jonathan L Winter for providing help with the MEG recordings. The computations described in this paper were performed using the University of Birminghamâs BlueBEAR HPC service, which provides a High Performance Computing service to the Universityâs research community. See <ext-link ext-link-type="uri" xlink:href="http://www.birmingham.ac.uk/bear">http://www.birmingham.ac.uk/bear</ext-link> for more details. This study was supported by a Leverhulme Early Career Fellowship awarded to YP (ECF-2023â626) and the following grants to OJ: the James S McDonnell Foundation Understanding Human Cognition Collaborative Award (grant number 220020448), Wellcome Trust Investigator Award in Science (grant number 207550), and the BBSRC grant (BB/R018723/1) as well as the Royal Society Wolfson Research Merit Award. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amsel</surname><given-names>BD</given-names></name><name><surname>Urbach</surname><given-names>TP</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Alive and grasping: stable and rapid semantic access to an object category but not object graspability</article-title><source>NeuroImage</source><volume>77</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.058</pub-id><pub-id pub-id-type="pmid">23567884</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>AntÃºnez</surname><given-names>M</given-names></name><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>HernÃ¡ndez-Cabrera</surname><given-names>JA</given-names></name><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Semantic parafoveal processing in natural reading: Insight from fixation-related potentials &amp; eye movements</article-title><source>Psychophysiology</source><volume>59</volume><elocation-id>e13986</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13986</pub-id><pub-id pub-id-type="pmid">34942021</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>J</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Representing syllable information during silent reading: Evidence from eye movements</article-title><source>Language and Cognitive Processes</source><volume>19</volume><fpage>391</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1080/01690960344000233</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>J</given-names></name><name><surname>Treiman</surname><given-names>R</given-names></name><name><surname>Kessler</surname><given-names>B</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Vowel processing during silent reading: evidence from eye movements</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>32</volume><fpage>416</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.32.2.416</pub-id><pub-id pub-id-type="pmid">16569156</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>DoÃ±amayor</surname><given-names>N</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>MÃ¼nte</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Parafoveal N400 effect during sentence reading</article-title><source>Neuroscience Letters</source><volume>479</volume><fpage>152</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2010.05.053</pub-id><pub-id pub-id-type="pmid">20580772</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>van der Meij</surname><given-names>M</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An electrophysiological analysis of contextual and temporal constraints on parafoveal word processing</article-title><source>Psychophysiology</source><volume>50</volume><fpage>48</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2012.01489.x</pub-id><pub-id pub-id-type="pmid">23153323</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beyersmann</surname><given-names>E</given-names></name><name><surname>Montani</surname><given-names>V</given-names></name><name><surname>Ziegler</surname><given-names>JC</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Stoianov</surname><given-names>IP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The dynamics of reading complex words: evidence from steady-state visual evoked potentials</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>111</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-95292-0</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braze</surname><given-names>D</given-names></name><name><surname>Shankweiler</surname><given-names>D</given-names></name><name><surname>Ni</surname><given-names>W</given-names></name><name><surname>Palumbo</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Readersâ eye movements distinguish anomalies of form and content</article-title><source>Journal of Psycholinguistic Research</source><volume>31</volume><fpage>25</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1023/a:1014324220455</pub-id><pub-id pub-id-type="pmid">11924838</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brickwedde</surname><given-names>M</given-names></name><name><surname>Bezsudnova</surname><given-names>Y</given-names></name><name><surname>Kowalczyk</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Zhigalov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Application of rapid invisible frequency tagging for brain computer interfaces</article-title><source>Journal of Neuroscience Methods</source><volume>382</volume><elocation-id>109726</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2022.109726</pub-id><pub-id pub-id-type="pmid">36228894</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carreiras</surname><given-names>M</given-names></name><name><surname>Armstrong</surname><given-names>BC</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Frost</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The what, when, where, and how of visual word recognition</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>90</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.11.005</pub-id><pub-id pub-id-type="pmid">24373885</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chace</surname><given-names>KH</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Well</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye movements and phonological parafoveal preview: effects of reading skill</article-title><source>Canadian Journal of Experimental Psychology = Revue Canadienne de Psychologie Experimentale</source><volume>59</volume><fpage>209</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1037/h0087476</pub-id><pub-id pub-id-type="pmid">16248500</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MX</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Analyzing Neural Time Series Data: Theory and Practice</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9609.001.0001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>N-Watch: A program for deriving neighborhood size and other psycholinguistic statistics</article-title><source>Behavior Research Methods</source><volume>37</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.3758/BF03206399</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeLong</surname><given-names>KA</given-names></name><name><surname>Quante</surname><given-names>L</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Predictability, plausibility, and two late ERP positivities during written sentence comprehension</article-title><source>Neuropsychologia</source><volume>61</volume><fpage>150</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.06.016</pub-id><pub-id pub-id-type="pmid">24953958</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drieghe</surname><given-names>D</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye movements and word skipping during reading revisited</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>31</volume><fpage>954</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.31.5.954</pub-id><pub-id pub-id-type="pmid">16262491</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drijvers</surname><given-names>L</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rapid invisible frequency tagging reveals nonlinear integration of auditory and visual information</article-title><source>Human Brain Mapping</source><volume>42</volume><fpage>1138</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1002/hbm.25282</pub-id><pub-id pub-id-type="pmid">33206441</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duecker</surname><given-names>K</given-names></name><name><surname>Gutteling</surname><given-names>TP</given-names></name><name><surname>Herrmann</surname><given-names>CS</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>No evidence for entrainment: endogenous gamma oscillations and rhythmic flicker responses coexist in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>6684</fpage><lpage>6698</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3134-20.2021</pub-id><pub-id pub-id-type="pmid">34230106</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A dynamical model of saccade generation in reading based on spatially distributed lexical processing</article-title><source>Vision Research</source><volume>42</volume><fpage>621</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00301-7</pub-id><pub-id pub-id-type="pmid">11853779</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Nuthmann</surname><given-names>A</given-names></name><name><surname>Richter</surname><given-names>EM</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>SWIFT: A dynamical model of saccade generation during reading</article-title><source>Psychological Review</source><volume>112</volume><fpage>777</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.4.777</pub-id><pub-id pub-id-type="pmid">16262468</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Wlotko</surname><given-names>EW</given-names></name><name><surname>De Ochoa-Dewald</surname><given-names>E</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Multiple effects of sentential constraint on word processing</article-title><source>Brain Research</source><volume>1146</volume><fpage>75</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.06.101</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Connecting and considering: Electrophysiology provides insights into comprehension</article-title><source>Psychophysiology</source><volume>59</volume><elocation-id>e13940</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13940</pub-id><pub-id pub-id-type="pmid">34520568</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>O</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Minarik</surname><given-names>T</given-names></name><name><surname>Gorska</surname><given-names>U</given-names></name><name><surname>Ghafari</surname><given-names>T</given-names></name><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>FLUX: A pipeline for MEG analysis</article-title><source>NeuroImage</source><volume>253</volume><elocation-id>119047</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119047</pub-id><pub-id pub-id-type="pmid">35276363</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>O</given-names></name><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Hickey</surname><given-names>C</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Statistical learning of distractor suppression downregulates prestimulus neural excitability in early visual cortex</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>2190</fpage><lpage>2198</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1703-22.2022</pub-id><pub-id pub-id-type="pmid">36801825</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kujala</surname><given-names>J</given-names></name><name><surname>Hamalainen</surname><given-names>M</given-names></name><name><surname>Timmermann</surname><given-names>L</given-names></name><name><surname>Schnitzler</surname><given-names>A</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic imaging of coherent sources: Studying neural interactions in the human brain</article-title><source>PNAS</source><volume>98</volume><fpage>694</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1073/pnas.98.2.694</pub-id><pub-id pub-id-type="pmid">11209067</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulbinaite</surname><given-names>R</given-names></name><name><surname>Roozendaal</surname><given-names>DHM</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Attention differentially modulates the amplitude of resonance frequencies in the visual cortex</article-title><source>NeuroImage</source><volume>203</volume><elocation-id>116146</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116146</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutteling</surname><given-names>TP</given-names></name><name><surname>Sillekens</surname><given-names>L</given-names></name><name><surname>Lavie</surname><given-names>N</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Alpha oscillations reflect suppression of distractors with increased perceptual load</article-title><source>Progress in Neurobiology</source><volume>214</volume><elocation-id>102285</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2022.102285</pub-id><pub-id pub-id-type="pmid">35533812</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Coutout</surname><given-names>C</given-names></name><name><surname>Holden</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The time-course of single-word reading: evidence from fast behavioral and brain responses</article-title><source>NeuroImage</source><volume>60</volume><fpage>1462</fpage><lpage>1477</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.061</pub-id><pub-id pub-id-type="pmid">22281671</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hohenstein</surname><given-names>S</given-names></name><name><surname>Laubrock</surname><given-names>J</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Semantic preview benefit in eye movements during reading: A parafoveal fast-priming study</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>36</volume><fpage>1150</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1037/a0020233</pub-id><pub-id pub-id-type="pmid">20804291</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hohenstein</surname><given-names>Sven</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Semantic preview benefit during reading</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>40</volume><fpage>166</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1037/a0033670</pub-id><pub-id pub-id-type="pmid">23895448</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeda</surname><given-names>S</given-names></name><name><surname>Toyama</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Independent component analysis for noisy data--MEG data analysis</article-title><source>Neural Networks</source><volume>13</volume><fpage>1063</fpage><lpage>1074</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(00)00071-x</pub-id><pub-id pub-id-type="pmid">11156188</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inhoff</surname><given-names>AW</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Parafoveal word perception: a case against semantic preprocessing</article-title><source>Perception &amp; Psychophysics</source><volume>27</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.3758/bf03204463</pub-id><pub-id pub-id-type="pmid">7383833</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inhoff</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Parafoveal word perception: A further case against semantic preprocessing</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>8</volume><fpage>137</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.8.1.137</pub-id><pub-id pub-id-type="pmid">6460079</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inhoff</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Parafoveal processing of words and saccade computation during eye fixations in reading</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>15</volume><fpage>544</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.15.3.544</pub-id><pub-id pub-id-type="pmid">2527961</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An oscillatory pipelining mechanism supporting previewing during visual exploration and reading</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>1033</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.08.008</pub-id><pub-id pub-id-type="pmid">34544653</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>RL</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Transposed-letter effects in reading: evidence from eye movements and parafoveal preview</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>33</volume><fpage>209</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.33.1.209</pub-id><pub-id pub-id-type="pmid">17311489</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Pynte</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Parafoveal-on-foveal effects in normal reading</article-title><source>Vision Research</source><volume>45</volume><fpage>153</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.07.037</pub-id><pub-id pub-id-type="pmid">15581917</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Whatâs New in Psychtoolbox-3?</source><publisher-name>NYU Scholars</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Nuthmann</surname><given-names>A</given-names></name><name><surname>Engbert</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Tracking the mind during reading: the influence of past, present, and future words on fixation durations</article-title><source>Journal of Experimental Psychology. General</source><volume>135</volume><fpage>12</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.135.1.12</pub-id><pub-id pub-id-type="pmid">16478314</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kritzman</surname><given-names>L</given-names></name><name><surname>Eidelman-Rothman</surname><given-names>M</given-names></name><name><surname>Keil</surname><given-names>A</given-names></name><name><surname>Freche</surname><given-names>D</given-names></name><name><surname>Sheppes</surname><given-names>G</given-names></name><name><surname>Levit-Binnun</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Steady-state visual evoked potentials differentiate between internally and externally directed attention</article-title><source>NeuroImage</source><volume>254</volume><elocation-id>119133</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119133</pub-id><pub-id pub-id-type="pmid">35339684</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Reading senseless sentences: brain potentials reflect semantic incongruity</article-title><source>Science</source><volume>207</volume><fpage>203</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1126/science.7350657</pub-id><pub-id pub-id-type="pmid">7350657</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Brain potentials during reading reflect word expectancy and semantic association</article-title><source>Nature</source><volume>307</volume><fpage>161</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1038/307161a0</pub-id><pub-id pub-id-type="pmid">6690995</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>EF</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A cortical network for semantics: (de)constructing the N400</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>920</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nrn2532</pub-id><pub-id pub-id-type="pmid">19020511</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Niefind</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Sommer</surname><given-names>W</given-names></name><name><surname>Dimigen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Parafoveal processing in reading Chinese sentences: Evidence from event-related brain potentials</article-title><source>Psychophysiology</source><volume>52</volume><fpage>1361</fpage><lpage>1374</lpage><pub-id pub-id-type="doi">10.1111/psyp.12502</pub-id><pub-id pub-id-type="pmid">26289548</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Midgley</surname><given-names>KJ</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>ERPs reveal how semantic and syntactic processing unfold across parafoveal and foveal vision during sentence comprehension</article-title><source>Language, Cognition and Neuroscience</source><volume>38</volume><fpage>88</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1080/23273798.2022.2091150</pub-id><pub-id pub-id-type="pmid">36776698</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LÃ³pez-PerÃ©z</surname><given-names>PJ</given-names></name><name><surname>DampurÃ©</surname><given-names>J</given-names></name><name><surname>HernÃ¡ndez-Cabrera</surname><given-names>JA</given-names></name><name><surname>Barber</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Semantic parafoveal-on-foveal effects and preview benefits in reading: Evidence from Fixation Related Potentials</article-title><source>Brain and Language</source><volume>162</volume><fpage>29</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2016.07.009</pub-id><pub-id pub-id-type="pmid">27513878</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of coherence differences</article-title><source>Journal of Neuroscience Methods</source><volume>163</volume><fpage>161</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.02.011</pub-id><pub-id pub-id-type="pmid">17395267</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McConkie</surname><given-names>GW</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>The span of the effective stimulus during a fixation in reading</article-title><source>Perception &amp; Psychophysics</source><volume>17</volume><fpage>578</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.3758/BF03203972</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meade</surname><given-names>G</given-names></name><name><surname>Declerck</surname><given-names>M</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Parallel semantic processing in the flankers task: Evidence from the N400</article-title><source>Brain and Language</source><volume>219</volume><elocation-id>104965</elocation-id><pub-id pub-id-type="doi">10.1016/j.bandl.2021.104965</pub-id><pub-id pub-id-type="pmid">33975227</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miellet</surname><given-names>S</given-names></name><name><surname>Sparrow</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Phonological codes are assembled before word fixation: evidence from boundary paradigm in sentence reading</article-title><source>Brain and Language</source><volume>90</volume><fpage>299</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1016/S0093-934X(03)00442-5</pub-id><pub-id pub-id-type="pmid">15172547</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>J</given-names></name><name><surname>Patterson</surname><given-names>T</given-names></name><name><surname>Ulrich</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Jackknife-based method for measuring LRP onset latency differences</article-title><source>Psychophysiology</source><volume>35</volume><fpage>99</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1017/S0048577298000857</pub-id><pub-id pub-id-type="pmid">9499711</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Nestor</surname><given-names>B</given-names></name><name><surname>AntÃºnez</surname><given-names>M</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Out of sight, out of mind: Foveal processing is necessary for semantic integration of words into sentence context</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>49</volume><fpage>687</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1037/xhp0001121</pub-id><pub-id pub-id-type="pmid">37261774</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montani</surname><given-names>V</given-names></name><name><surname>Chanoine</surname><given-names>V</given-names></name><name><surname>Stoianov</surname><given-names>IP</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Ziegler</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Steady state visual evoked potentials in reading aloud: Effects of lexicality, frequency and orthographic familiarity</article-title><source>Brain and Language</source><volume>192</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2019.01.004</pub-id><pub-id pub-id-type="pmid">30826643</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MÃ¼ller</surname><given-names>MM</given-names></name><name><surname>Picton</surname><given-names>TW</given-names></name><name><surname>Valdes-Sosa</surname><given-names>P</given-names></name><name><surname>Riera</surname><given-names>J</given-names></name><name><surname>Teder-SÃ¤lejÃ¤rvi</surname><given-names>WA</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Effects of spatial selective attention on the steady-state visual evoked potential in the 20â28 Hz range</article-title><source>Cognitive Brain Research</source><volume>6</volume><fpage>249</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(97)00036-0</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MÃ¼ller</surname><given-names>MM</given-names></name><name><surname>Malinowski</surname><given-names>P</given-names></name><name><surname>Gruber</surname><given-names>T</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Sustained division of the attentional spotlight</article-title><source>Nature</source><volume>424</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1038/nature01812</pub-id><pub-id pub-id-type="pmid">12867981</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname><given-names>W</given-names></name><name><surname>Fodor</surname><given-names>JD</given-names></name><name><surname>Crain</surname><given-names>S</given-names></name><name><surname>Shankweiler</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Anomaly detection: eye movement patterns</article-title><source>Journal of Psycholinguistic Research</source><volume>27</volume><fpage>515</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1023/a:1024996828734</pub-id><pub-id pub-id-type="pmid">9750312</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Physics in Medicine and Biology</source><volume>48</volume><fpage>3637</fpage><lpage>3652</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id><pub-id pub-id-type="pmid">14680264</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Appelbaum</surname><given-names>LG</given-names></name><name><surname>Ales</surname><given-names>JM</given-names></name><name><surname>Cottereau</surname><given-names>BR</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The steady-state visual evoked potential in vision research: A review</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/15.6.4</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural evidence for lexical parafoveal processing</article-title><source>Nature Communications</source><volume>12</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-25571-x</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Semantic</data-title><version designator="swh:1:rev:dc8c9ad136febd081dcdae456035297bf54e6ce8">swh:1:rev:dc8c9ad136febd081dcdae456035297bf54e6ce8</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:4d36a15c8bb61fe2a872a22efe7ec67bf3cefd73;origin=https://github.com/yalipan666/Semantic;visit=swh:1:snp:d8e8f4b521b0ff15014976e05887019e2faed3aa;anchor=swh:1:rev:dc8c9ad136febd081dcdae456035297bf54e6ce8">https://archive.softwareheritage.org/swh:1:dir:4d36a15c8bb61fe2a872a22efe7ec67bf3cefd73;origin=https://github.com/yalipan666/Semantic;visit=swh:1:snp:d8e8f4b521b0ff15014976e05887019e2faed3aa;anchor=swh:1:rev:dc8c9ad136febd081dcdae456035297bf54e6ce8</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Popov</surname><given-names>T</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Saccades are locked to the phase of alpha oscillations during natural reading</article-title><source>PLOS Biology</source><volume>21</volume><elocation-id>e3001968</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001968</pub-id><pub-id pub-id-type="pmid">36649331</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Event-related brain potentials reveal age-related changes in parafoveal-foveal integration during sentence processing</article-title><source>Neuropsychologia</source><volume>106</volume><fpage>358</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.10.002</pub-id><pub-id pub-id-type="pmid">28987909</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Event-related brain potentials reveal how multiple aspects of semantic processing unfold across parafoveal and foveal vision during sentence reading</article-title><source>Psychophysiology</source><volume>56</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1111/psyp.13432</pub-id><pub-id pub-id-type="pmid">31274200</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Lesch</surname><given-names>M</given-names></name><name><surname>Morris</surname><given-names>RK</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Phonological codes are used in integrating information across saccades in word identification and reading</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>18</volume><fpage>148</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.18.1.148</pub-id><pub-id pub-id-type="pmid">1532185</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>The perceptual span and peripheral cues in reading</article-title><source>Cognitive Psychology</source><volume>7</volume><fpage>65</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(75)90005-5</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Balota</surname><given-names>DA</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Against parafoveal semantic preprocessing during eye fixations in reading</article-title><source>Canadian Journal of Psychology / Revue Canadienne de Psychologie</source><volume>40</volume><fpage>473</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1037/h0080111</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Sereno</surname><given-names>SC</given-names></name><name><surname>Lesch</surname><given-names>MF</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Phonological codes are automatically activated during reading: evidence from an eye movement priming paradigm</article-title><source>Psychological Science</source><volume>6</volume><fpage>26</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1995.tb00300.x</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Warren</surname><given-names>T</given-names></name><name><surname>Juhasz</surname><given-names>BJ</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The effect of plausibility on eye movements in reading</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>30</volume><fpage>1290</fpage><lpage>1301</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.30.6.1290</pub-id><pub-id pub-id-type="pmid">15521805</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Eye movements and attention in reading, scene perception, and visual search</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>62</volume><fpage>1457</fpage><lpage>1506</lpage><pub-id pub-id-type="doi">10.1080/17470210902816461</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Semantic preview benefit in reading English: The effect of initial letter capitalization</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>40</volume><fpage>1617</fpage><lpage>1628</lpage><pub-id pub-id-type="doi">10.1037/a0036763</pub-id><pub-id pub-id-type="pmid">24820439</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Drieghe</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Lack of semantic parafoveal preview benefit in reading revisited</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>1067</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.3758/s13423-014-0582-9</pub-id><pub-id pub-id-type="pmid">24496738</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2013">2013</year><data-title>R: A language and environment for statistical computing</data-title><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.r-project.org">https://www.r-project.org</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Fisher</surname><given-names>DL</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Toward a model of eye movement control in reading</article-title><source>Psychological Review</source><volume>105</volume><fpage>125</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.105.1.125</pub-id><pub-id pub-id-type="pmid">9450374</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The E-Z reader model of eye-movement control in reading: comparisons to other models</article-title><source>The Behavioral and Brain Sciences</source><volume>26</volume><fpage>445</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1017/s0140525x03000104</pub-id><pub-id pub-id-type="pmid">15067951</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>EâZ Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading</article-title><source>Cognitive Systems Research</source><volume>7</volume><fpage>4</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.cogsys.2005.07.002</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Warren</surname><given-names>T</given-names></name><name><surname>McConnell</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Using E-Z Reader to model the effects of higher level language processing on eye movements during reading</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.1.1</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Reingold</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neurophysiological constraints on the eye-mind link</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>361</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00361</pub-id><pub-id pub-id-type="pmid">23874281</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Angele</surname><given-names>B</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Parafoveal processing in reading</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>74</volume><fpage>5</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.3758/s13414-011-0219-2</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Synonyms provide semantic preview benefit in English</article-title><source>Journal of Memory and Language</source><volume>69</volume><fpage>619</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2013.09.002</pub-id><pub-id pub-id-type="pmid">24347813</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Reiderman</surname><given-names>M</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The effect of contextual constraint on parafoveal processing in reading</article-title><source>Journal of Memory and Language</source><volume>83</volume><fpage>118</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2015.04.005</pub-id><pub-id pub-id-type="pmid">26257469</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Jia</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Semantic and plausibility preview benefit effects in English: Evidence from eye movements</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>42</volume><fpage>1839</fpage><lpage>1866</lpage><pub-id pub-id-type="doi">10.1037/xlm0000281</pub-id><pub-id pub-id-type="pmid">27123754</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reading ahead by hedging our bets on seeing the future: eye tracking and electrophysiology evidence for parafoveal lexical processing and saccadic control by partial word recognition</article-title><source>Psychol Learn Motiv - Adv Res Theory</source><volume>68</volume><fpage>263</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1016/BS.PLM.2018.08.011</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Estevez</surname><given-names>VM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Event-related potentials show that parafoveal vision is insufficient for semantic integration</article-title><source>Psychophysiology</source><volume>60</volume><elocation-id>e14246</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.14246</pub-id><pub-id pub-id-type="pmid">36811523</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Meeter</surname><given-names>M</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence for simultaneous syntactic processing of multiple words during reading</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0173720</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0173720</pub-id><pub-id pub-id-type="pmid">28278305</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>van Leipsig</surname><given-names>S</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Meeter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>OB1-reader: A model of word recognition and eye movements in text reading</article-title><source>Psychological Review</source><volume>125</volume><fpage>969</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1037/rev0000119</pub-id><pub-id pub-id-type="pmid">30080066</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Readers Are Parallel Processors</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>537</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.04.006</pub-id><pub-id pub-id-type="pmid">31138515</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Getting ahead of yourself: Parafoveal word expectancy modulates the N400 during sentence reading</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><volume>17</volume><fpage>475</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.3758/s13415-016-0492-6</pub-id><pub-id pub-id-type="pmid">28101830</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>JL</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Yan</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Parafoveal semantic information extraction in traditional Chinese reading</article-title><source>Acta Psychologica</source><volume>141</volume><fpage>17</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2012.06.004</pub-id><pub-id pub-id-type="pmid">22820455</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Underwood</surname><given-names>NR</given-names></name><name><surname>McConkie</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Perceptual span for letter distinctions during reading</article-title><source>Reading Research Quarterly</source><volume>20</volume><elocation-id>153</elocation-id><pub-id pub-id-type="doi">10.2307/747752</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Is semantic preview benefit due to relatedness or plausibility?</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>42</volume><fpage>939</fpage><lpage>952</lpage><pub-id pub-id-type="doi">10.1037/xhp0000200</pub-id><pub-id pub-id-type="pmid">26752734</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Semantic preview benefit in English: Individual differences in the extraction and use of parafoveal semantic information</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>42</volume><fpage>837</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1037/xlm0000212</pub-id><pub-id pub-id-type="pmid">26595070</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parafoveal preview benefit in sentence reading: Independent effects of plausibility and orthographic relatedness</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>519</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1120-8</pub-id><pub-id pub-id-type="pmid">27418260</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Parafoveal preview effects depend on both preview plausibility and target predictability</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>71</volume><fpage>64</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1080/17470218.2016.1247894</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vialatte</surname><given-names>FB</given-names></name><name><surname>Maurice</surname><given-names>M</given-names></name><name><surname>Dauwels</surname><given-names>J</given-names></name><name><surname>Cichocki</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Steady-state visually evoked potentials: focus on essential paradigms and future perspectives</article-title><source>Progress in Neurobiology</source><volume>90</volume><fpage>418</fpage><lpage>438</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2009.11.005</pub-id><pub-id pub-id-type="pmid">19963032</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>Y</given-names></name><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Parallel, cascaded, interactive processing of words during sentence reading</article-title><source>Cognition</source><volume>189</volume><fpage>221</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2019.04.013</pub-id><pub-id pub-id-type="pmid">31005638</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Eye movement control during reading: Effects of word frequency and orthographic familiarity</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>34</volume><fpage>205</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.34.1.205</pub-id><pub-id pub-id-type="pmid">18248149</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>CC</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Previewing the neighborhood: the role of orthographic neighbors as parafoveal previews in reading</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>32</volume><fpage>1072</fpage><lpage>1082</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.32.4.1072</pub-id><pub-id pub-id-type="pmid">16846298</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>R</given-names></name><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Are there independent effects of constraint and predictability on eye movements during reading?</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>50</volume><fpage>331</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1037/xlm0001206</pub-id><pub-id pub-id-type="pmid">36521159</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>M</given-names></name><name><surname>Bosker</surname><given-names>HR</given-names></name><name><surname>Riecke</surname><given-names>L</given-names></name><name><surname>Nl</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Sentential contextual facilitation of auditory word processing builds up during sentence tracking</article-title><source>Journal of Cognitive Neuroscience</source><volume>35</volume><fpage>1262</fpage><lpage>1278</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_02007</pub-id><pub-id pub-id-type="pmid">37172122</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>M</given-names></name><name><surname>Richter</surname><given-names>EM</given-names></name><name><surname>Shu</surname><given-names>H</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Readers of Chinese extract semantic information from parafoveal words</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>561</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.3.561</pub-id><pub-id pub-id-type="pmid">19451385</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>W</given-names></name><name><surname>Shu</surname><given-names>H</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Lexical and sublexical semantic preview benefits in Chinese reading</article-title><source>Journal of Experimental Psychology</source><volume>38</volume><fpage>1069</fpage><lpage>1075</lpage><pub-id pub-id-type="doi">10.1037/a0026935</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Herring</surname><given-names>JD</given-names></name><name><surname>Herpers</surname><given-names>J</given-names></name><name><surname>Bergmann</surname><given-names>TO</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Probing cortical excitability using rapid frequency tagging</article-title><source>NeuroImage</source><volume>195</volume><fpage>59</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.056</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Alpha oscillations do not implement gain control in early visual cortex but rather gating in parieto-occipital regions</article-title><source>Human Brain Mapping</source><volume>41</volume><fpage>5176</fpage><lpage>5186</lpage><pub-id pub-id-type="doi">10.1002/hbm.25183</pub-id><pub-id pub-id-type="pmid">32822098</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Duecker</surname><given-names>K</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The visual cortex produces gamma band echo in response to broadband visual flicker</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009046</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009046</pub-id><pub-id pub-id-type="pmid">34061835</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Travelling waves observed in meg data can be explained by two discrete sources</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.09.28.509870</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>W</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Yan</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A validation of parafoveal semantic information extraction in reading Chinese</article-title><source>Journal of Research in Reading</source><volume>36</volume><fpage>S51</fpage><lpage>S63</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9817.2013.01556.x</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><fig id="app1fig1" position="float"><label>Appendix 1âfigure 1.</label><caption><title>The likelihood of refixation and total gaze duration of eye movement data.</title><p>(<bold>A</bold>) The likelihood of refixation into a word was defined as the proportion of trials that have at least one regression from a later part of the sentence back to that word. We found that when the target words were contextually incongruent with the sentence compared with congruent, there was a significantly higher probability of regression into pre-target words (<italic>t</italic><sub>(33)</sub> = 7.83, p=5.04Ã10<sup>â9</sup>, <italic>d</italic>=1.34, two-sided pairwise <italic>t</italic>-test) and target words (<italic>t</italic><sub>(33)</sub> = 9.13, p=1.49Ã10<sup>â10</sup>, <italic>d</italic>=1.57, two-sided pairwise <italic>t</italic>-test). Each dot indicates one participant. (<bold>B</bold>) The total gaze duration was defined as the sum of all fixations on a given word, including those fixations during re-reading. Significantly longer total gaze durations were found for pre-target words (<italic>t</italic><sub>(33)</sub> = 5.78, p=1.86Ã10<sup>â6</sup>, <italic>d</italic>=0.99, two-sided pairwise <italic>t</italic>-test) and target words (<italic>t</italic><sub>(33)</sub> = 10.55, p=4.20Ã10<sup>â12</sup>, <italic>d</italic>=1.81, two-sided pairwise <italic>t</italic>-test) when the target words were incongruent with the context compared with congruent. ***p&lt;.001; n.s., not statistically significant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91327-app1-fig1-v1.tif"/></fig><sec sec-type="appendix" id="s8"><title>Experimental sentence set</title><sec sec-type="appendix" id="s8-1"><title>Here, we share all 160 sentences embedded with congruent target words</title><p>For sentence set version A, we swapped the target words within each pair for sentences 1â80 and made them incongruent, while sentences 81â160 were kept congruent. For sentence set version B, target words in sentences 1â80 were kept the same but target words in sentences 81â160 were swapped within each pair to make them incongruent. The sequence of the sentences was shuffled to make sure that no more than three sentences in a row were in the same condition. For illustration, the target words are shown in italic type here, but they were in normal type in the experiment. For the 117 filler sentences, please see the Appendix in <xref ref-type="bibr" rid="bib97">White, 2008</xref>.</p><list list-type="order"><list-item><p>Last night, my lazy <italic>brother/jacket</italic> came to the party 1 min before it was over.</p></list-item><list-item><p>Lily says this blue <italic>jacket/brother</italic> will be a big fashion trend this fall.</p></list-item><list-item><p>This area has been populated by many <italic>hikers/coins</italic> over the last year.</p></list-item><list-item><p>Little Jimmy picked up a box and put some <italic>coins/hikers</italic> inside of it.</p></list-item><list-item><p>Joey became an avid <italic>student/ring</italic> during his adolescence.</p></list-item><list-item><p>He could only afford a cheap <italic>ring/student</italic> without a diamond for his fiancÃ©e.</p></list-item><list-item><p>This morning the noisy <italic>kids/ideas</italic> played happily in the backyard.</p></list-item><list-item><p>My parents had no firm <italic>ideas/kids</italic> about what I should become.</p></list-item><list-item><p>The unfortunate pupil lost his beloved <italic>pony/crisis</italic> just before his birthday.</p></list-item><list-item><p>Experts say that the severe <italic>crisis/pony</italic> will cause oil prices to triple.</p></list-item><list-item><p>The construction of this ancient <italic>castle/worker</italic> cost a lot of money.</p></list-item><list-item><p>After the meeting, the anxious <italic>worker/castle</italic> sighed in the hallway.</p></list-item><list-item><p>Peterâs love for this sporting <italic>match/collar</italic> inspired all his friends.</p></list-item><list-item><p>We could see from her torn <italic>collar/match</italic> that she had been in a fight.</p></list-item><list-item><p>With the help of his clever <italic>friend/burger</italic> Jack, he made the first pot of gold.</p></list-item><list-item><p>I always like to order a filling <italic>burger/friend</italic> from the local pub.</p></list-item><list-item><p>She looked at the tired <italic>fireman/scan</italic> with a satisfied smile.</p></list-item><list-item><p>He felt relieved after completing the complex <italic>scan/fireman</italic> within an hour.</p></list-item><list-item><p>Scientists found a steep <italic>boulder/cousin</italic> sitting in the middle of the canyon.</p></list-item><list-item><p>Last week his friendly <italic>cousin/boulder</italic> passed out for no apparent reason.</p></list-item><list-item><p>They asked the selfish <italic>maid/roof</italic> where her huge sums of money came from.</p></list-item><list-item><p>It took Tom a month to mend the broken <italic>roof/maid</italic> all by himself.</p></list-item><list-item><p>Under stress, the crafty <italic>boss/rifles</italic> promised customers a full refund.</p></list-item><list-item><p>The cowboys hung the stolen <italic>rifles/boss</italic> high up on the wall.</p></list-item><list-item><p>She submitted the crucial <italic>file/queen</italic> that can prove her innocence.</p></list-item><list-item><p>According to history books, the proud <italic>queen/file</italic> never accepted any criticism.</p></list-item><list-item><p>Ana complained that the tall <italic>herbs/sport</italic> behind the house had dried up.</p></list-item><list-item><p>He failed in his chosen <italic>sport/herbs</italic> with hopes of success fading with each effort.</p></list-item><list-item><p>She said that the corrupt <italic>company/bush</italic> offered high salaries to young graduates.</p></list-item><list-item><p>In the last few years, the thick <italic>bush/company</italic> died back dramatically.</p></list-item><list-item><p>It turned out that the last- minute <italic>trip/tree</italic> lasted for 6 hr.</p></list-item><list-item><p>Linda found that the slender <italic>tree/trip</italic> dead from a pest infestation.</p></list-item><list-item><p>Suddenly, the warm <italic>coffee/flower</italic> stained his brand new shirt.</p></list-item><list-item><p>Plenty of rain will make the vivid <italic>flower/coffee</italic> blossom well.</p></list-item><list-item><p>Jack became a humble <italic>chef/vehicle</italic> specializing in French cuisine.</p></list-item><list-item><p>The young manâs shiny <italic>vehicle/chef</italic> vanished slowly out of sight.</p></list-item><list-item><p>To the north, the steep <italic>hills/colonel</italic> stretched for many miles.</p></list-item><list-item><p>Before sleeping, the nervous <italic>colonel/hills</italic> smoked a cigarette.</p></list-item><list-item><p>Decades ago, that algae- covered <italic>pond/player</italic> was enough to irrigate the crops.</p></list-item><list-item><p>He saw the smart <italic>player/pond</italic> throw the ball, causing chaos among the opposition.</p></list-item><list-item><p>In recent days, the cruel <italic>murder/cream</italic> has scared citizens from going out.</p></list-item><list-item><p>Mary told me that the light <italic>cream/murder</italic> was low in fat but hard to whip.</p></list-item><list-item><p>News said that the painful <italic>disease/ball</italic> would continue to affect many children.</p></list-item><list-item><p>The boy found his lost <italic>ball/disease</italic> under the tree and stopped crying at once.</p></list-item><list-item><p>Politicians hated the brief <italic>report/drone</italic> criticizing the governmentâs incompetence.</p></list-item><list-item><p>My favourite gift is the shiny <italic>drone/report</italic> from my dad last year.</p></list-item><list-item><p>Every year, the sandy <italic>shore/officer</italic> attracts thousands of tourists.</p></list-item><list-item><p>After taking a deep breath, the junior <italic>officer/shore</italic> entered the room.</p></list-item><list-item><p>Last week, the caring <italic>family/plaza</italic> rescued a stray dog and kept it as a pet.</p></list-item><list-item><p>During the air raid, the spacious public <italic>plaza/family</italic> happened to be ruined.</p></list-item><list-item><p>With his sharp criticism, the young <italic>actor/storm</italic> annoyed his agent as usual.</p></list-item><list-item><p>Laura was told that the sudden <italic>storm/actor</italic> delayed the bus for two days.</p></list-item><list-item><p>Lily said that the vacant <italic>cottage/picture</italic> belonged to her grandparents.</p></list-item><list-item><p>In the small house, a comic <italic>picture/cottage</italic> adorned the reception room.</p></list-item><list-item><p>Facing the lion, the brave <italic>hunter/engine</italic> showed no fear.</p></list-item><list-item><p>Out of repair, the rattling <italic>engine/hunter</italic> was about to be scrapped.</p></list-item><list-item><p>Jack had to admit that this planned <italic>visit/aunt</italic> turned out to be embarrassing.</p></list-item><list-item><p>Tom admired the way his devoted <italic>aunt/visit</italic> always volunteers on weekends.</p></list-item><list-item><p>Rob felt that the brief <italic>letter/clerk</italic> from his wife expressed a hint of sadness.</p></list-item><list-item><p>Alone at home, the tired <italic>clerk/letter</italic> cooked a beef patty.</p></list-item><list-item><p>After the surgery, Robâs poor <italic>health/dusk</italic> left him barely able to get out of bed.</p></list-item><list-item><p>Samâs train arrived before <italic>dusk/health</italic> and we were able to give him a ride home.</p></list-item><list-item><p>She gave the dog a quick <italic>bath/joke</italic> after they came back from the outside.</p></list-item><list-item><p>Michael made a mean <italic>joke/bath</italic> about Boris Johnsonâs hair.</p></list-item><list-item><p>They didnât realize the harsh <italic>impact/crown</italic> that their products could have.</p></list-item><list-item><p>In the museum, we saw the golden <italic>crown/impact</italic> that belonged to the first king.</p></list-item><list-item><p>Bill is a superb <italic>partner/night</italic> because he is easy to get along with.</p></list-item><list-item><p>The explorer made his way through the gloomy <italic>night/partner</italic> with a small torch.</p></list-item><list-item><p>The TV show was an obvious <italic>flop/lady</italic> after the actress joined the cast.</p></list-item><list-item><p>On rainy days, the careful <italic>lady/flop</italic> reminded herself to go slowly.</p></list-item><list-item><p>Jane complained that her white <italic>kitten/problem</italic> hadnât come home for two days.</p></list-item><list-item><p>I guess no one can solve the hard <italic>problem/kitten</italic> without outside help.</p></list-item><list-item><p>The new event was such a huge <italic>failure/suspect</italic> that people kept talking about it.</p></list-item><list-item><p>Before committing the crime, the anxious <italic>suspect/failure</italic> drank a lot of alcohol.</p></list-item><list-item><p>Toby kept his money in a small <italic>shed/deer</italic> because he lived on a farm.</p></list-item><list-item><p>They noticed the young <italic>deer/shed</italic> eating acorns in the forest.</p></list-item><list-item><p>Amy wanted some more of the sliced <italic>pear/canal</italic> for afternoon snack.</p></list-item><list-item><p>Laura went down to the narrow <italic>canal/pear</italic> to watch the boats.</p></list-item><list-item><p>I wondered if the noisy <italic>club/pain</italic> would be a good place for the bachelorette party.</p></list-item><list-item><p>Tara always has an acute <italic>pain/club</italic> in her tooth after eating ice cream.</p></list-item><list-item><p>Before the war, the brave <italic>general/potato</italic> assembled an army.</p></list-item><list-item><p>She began to slice up a large <italic>potato/general</italic> for the dinner.</p></list-item><list-item><p>The child had a large <italic>face/mist</italic> with big, expressive eyes.</p></list-item><list-item><p>Last night, there was a dense <italic>mist/face</italic> when they left the cinema.</p></list-item><list-item><p>Marla enjoyed seeing the chubby <italic>cats/court</italic> playing with each other.</p></list-item><list-item><p>Jim entered the giant <italic>court/cats</italic> to try out for the basketball team.</p></list-item><list-item><p>They visited the antique <italic>chapel/fans</italic> before booking their wedding.</p></list-item><list-item><p>After the defeat, the crazy <italic>fans/chapel</italic> kept cursing and crying.</p></list-item><list-item><p>She approached the rusty <italic>gate/toast</italic> before realizing it was locked.</p></list-item><list-item><p>Many people like to eat crispy <italic>toast/gate</italic> with their morning coffee at breakfast.</p></list-item><list-item><p>They stepped into the messy <italic>garage/hawk</italic> that had high wooden shelves.</p></list-item><list-item><p>We watched the large hungry <italic>hawk/garage</italic> swoop down to get the poor chicken.</p></list-item><list-item><p>Alexandra used a short <italic>hammer/museum</italic> when she created the stone statue.</p></list-item><list-item><p>Ruth visited the public <italic>museum/hammer</italic> that she had read about all these years.</p></list-item><list-item><p>Weâd better buy some tasty <italic>chips/speech</italic> before we watch the big game.</p></list-item><list-item><p>Historians believe the rousing <italic>speech/speech</italic> heralded the start of the revolution.</p></list-item><list-item><p>Under the tree, there is a little <italic>hare/moon</italic> running happily.</p></list-item><list-item><p>In the darkness, only the misty <italic>moon/hare</italic> lit up the street.</p></list-item><list-item><p>The prince inherited the supreme <italic>power/sheep</italic> from the late king.</p></list-item><list-item><p>Look over there, a fluffy <italic>sheep/power</italic> seems to be lost.</p></list-item><list-item><p>The man was a young <italic>teacher/opinion</italic> who always worked late into the night.</p></list-item><list-item><p>As for this scandal, Jo has a clear <italic>opinion/teacher</italic> but she wonât say it.</p></list-item><list-item><p>They had no idea that the blue <italic>liquid/justice</italic> shrinks all woollen clothes.</p></list-item><list-item><p>The report was sent to the honest <italic>justice/liquid</italic> three days before the trial.</p></list-item><list-item><p>Every night, this tired <italic>captain/bottle</italic> drank wine before going to sleep.</p></list-item><list-item><p>The shopkeeper said the metal <italic>bottle/captain</italic> would sell well this year.</p></list-item><list-item><p>For the locals, the salt <italic>lake/animal</italic> triggered a political issue.</p></list-item><list-item><p>Near the small brook, a hungry <italic>animal/lake</italic> hunts quietly for hours.</p></list-item><list-item><p>Every night, this deep <italic>secret/surgeon</italic> makes the pianist toss and turn.</p></list-item><list-item><p>In the lab, a young <italic>surgeon/secret</italic> examined the victimâs body.</p></list-item><list-item><p>Sadly, the lonely <italic>poet/flour</italic> died before he could finish his last poem.</p></list-item><list-item><p>Due to the moist weather, the wheat <italic>flour/poet</italic> became mouldy quickly.</p></list-item><list-item><p>Sueâs colleagues say that her warm <italic>heart/screen</italic> makes everyone like her.</p></list-item><list-item><p>On the wall, the small green <italic>screen/heart</italic> shows the room temperature precisely.</p></list-item><list-item><p>To his surprise, the yummy <italic>dish/nanny</italic> was not expensive.</p></list-item><list-item><p>Eventually, the greedy <italic>nanny/dish</italic> disclosed all the details about this affair.</p></list-item><list-item><p>The sight of the cotton <italic>factory/patient</italic> was something to behold.</p></list-item><list-item><p>It was obvious that the weak <italic>patient/factory</italic> was getting weaker day by day.</p></list-item><list-item><p>In the past month alone, the gentle <italic>scholar/pots</italic> published five papers.</p></list-item><list-item><p>The filthy and rusty <italic>pots/scholar</italic> made the food taste terrible.</p></list-item><list-item><p>Just after dawn, an armed <italic>ship/shirt</italic> approached the pretty lagoon slowly.</p></list-item><list-item><p>At last, she found the wool <italic>skirt/ship</italic> hanging in the wardrobe.</p></list-item><list-item><p>Villagers said that the newly built <italic>school/crowd</italic> was well equipped.</p></list-item><list-item><p>In the downtown market, the agitated <italic>crowd/school</italic> began the parade.</p></list-item><list-item><p>Suzy really likes eating <italic>sugar/music</italic> because she wasnât allowed to eat it as a kid.</p></list-item><list-item><p>Ali said he really enjoyed modern <italic>music/sugar</italic> when he was at college.</p></list-item><list-item><p>Tina wants a spacious <italic>yard/chief</italic> because she likes to lie on the grass and read.</p></list-item><list-item><p>In an open field, the violent <italic>chief/yard</italic> executed prisoners with a gun.</p></list-item><list-item><p>After working overtime for a month, the wronged <italic>manager/grass</italic> wanted to jump ship.</p></list-item><list-item><p>In the Stone Age, the spiny <italic>grass/manager</italic> prevailed over the land.</p></list-item><list-item><p>The sparrow was being chased by some fluffy <italic>hens/cups</italic> under the hot sun.</p></list-item><list-item><p>When the ball was scored, they tapped their <italic>cups/hens</italic> to show their joy.</p></list-item><list-item><p>They recorded the details of the stolen <italic>cars/legs</italic> carefully on a spreadsheet.</p></list-item><list-item><p>The poor boy stood in the snow with bruised <italic>legs/cars</italic> and cried sadly.</p></list-item><list-item><p>Little Roy likes to play with the plastic <italic>bricks/lawyer</italic> at the Lego store.</p></list-item><list-item><p>It was said that the honest <italic>lawyer/bricks</italic> convened the committee meeting.</p></list-item><list-item><p>I learned about the muddy <italic>trail/jury</italic> through a friend on the last hike.</p></list-item><list-item><p>Just now, the calm <italic>jury/trail</italic> delivered a guilty verdict in this notorious case.</p></list-item><list-item><p>The holiday was neglected by this busy <italic>parent/story</italic> but her son was used to it.</p></list-item><list-item><p>This widely spread <italic>story/parent</italic> reflected the distortion of human nature.</p></list-item><list-item><p>We are meeting at the newly built <italic>airport/editor</italic> tonight for our trip to Europe.</p></list-item><list-item><p>Every day before leaving work, the tall <italic>editor/airport</italic> cleans her desk.</p></list-item><list-item><p>Nobody knew when the excited <italic>puppy/area</italic> urinated on the floor.</p></list-item><list-item><p>Everyone knows that entire <italic>area/puppy</italic> has restricted access.</p></list-item><list-item><p>The manâs cunning <italic>excuse/truck</italic> relieved him of the fine.</p></list-item><list-item><p>Roy repaired the broken <italic>truck/excuse</italic> over the weekend.</p></list-item><list-item><p>Ana was glad that the gentle <italic>nurse/meeting</italic> said her little boy was out of danger.</p></list-item><list-item><p>In the company, the annual <italic>meeting/nurse</italic> marks the end of a yearâs hard work.</p></list-item><list-item><p>He carefully placed the sharp <italic>sword/desk</italic> down after the fight.</p></list-item><list-item><p>She found an empty <italic>desk/sword</italic> where she could put her computer.</p></list-item><list-item><p>They danced a slow <italic>tango/note</italic> together after dinner.</p></list-item><list-item><p>Steph noticed a torn <italic>note/tango</italic> and looked for the other half.</p></list-item><list-item><p>Mindyâs dog has a strange <italic>smell/tape</italic> and likes to bark a lot.</p></list-item><list-item><p>Patty likes to cut some pink <italic>tape/smell</italic> to decorate her notebooks.</p></list-item><list-item><p>We could hear the angry <italic>priest/card</italic> shouting at the little girl.</p></list-item><list-item><p>David was happy to receive a nice <italic>card/priest</italic> from his daughter at Christmas.</p></list-item><list-item><p>She always meets the same happy <italic>couple/hole</italic> when she walks in the park.</p></list-item><list-item><p>The stray dog lives in a hidden <italic>hole/couple</italic> that protects it from the cold weather.</p></list-item><list-item><p>Jack failed to submit his concise <italic>paper/baker</italic> before the deadline.</p></list-item><list-item><p>I have heard that the young <italic>baker/paper</italic> makes the best baguettes in town.</p></list-item></list></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91327.4.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ding</surname><given-names>Nai</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Zhejiang University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study contributes to the understanding of how parafoveal words are neurally processed during naturalistic sentence reading. <bold>Convincing</bold> evidence is provided that the MEG response to a word can be modulated by the semantic congruency of a parafoveal target word. The study addresses a classic question in reading using a new Rapid Invisible Frequency Tagging (RIFT) technique, which can separately monitor the neural processing of multiple words during sentence reading.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91327.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The study investigates parafoveal processing during natural reading, combining eye-tracking and MEG techniques, building upon the RIFT paradigm previously introduced by Pan et al. (2021).</p><p>The manuscript is well-written with a clear structure, and the data analysis and experimental results are presented in a lucid manner.</p><p>Comments on revised version:</p><p>I am satisfied with the revisions made by the authors. I believe the study introduces a new research paradigm to the field.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91327.4.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pan</surname><given-names>Yali</given-names></name><role specific-use="author">Author</role><aff><institution>University of Birmingham</institution><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Frisson</surname><given-names>Steven</given-names></name><role specific-use="author">Author</role><aff><institution>University of Birmingham</institution><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Federmeier</surname><given-names>Kara D</given-names></name><role specific-use="author">Author</role><aff><institution>University of Illinois</institution><addr-line><named-content content-type="city">Champaign</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Ole</given-names></name><role specific-use="author">Author</role><aff><institution>University of Birmingham</institution><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>The study investigates parafoveal processing during natural reading, combining eye-tracking and MEG techniques, building upon the RIFT paradigm previously introduced by Pan et al. (2021). Overall, the manuscript is well-written with a clear structure, and the data analysis and experimental results are presented in a lucid manner.</p><p>The authors have addressed the issues I raised in the previous round of review to my satisfaction. However, I still have two concerns that require the authors' consideration.</p><p>Firstly, the similarity between the RIFT analysis process in this study and traditional ERP analysis could lead readers to equate RIFT with components like N400, potentially influencing their interpretation of the results. Although the author's response has somewhat clarified my queries, I seek confirmation: does RIFT itself signify &quot;visual attention&quot; or the &quot;allocation of attentional resources to the flickering target words&quot; (line 208) in this study? While this may not be pivotal, as it primarily serves as an indicator to evaluate whether contextual congruity can indeed modulate the RIFT response rather than indicating early parafoveal semantic integration, I recommend that the authors explicitly address this point in the manuscript, maybe in the discussion section, to enhance reader comprehension of the article's rationale.</p><p>Secondly, regarding the study's conclusions, there appears to be an overemphasis in stating that &quot;semantic information ... can also be integrated with the sentence context ...&quot; (line 21-22). As raised by Reviewer 2 (Major Point 1) and acknowledged by the authors in the limitations of the revised manuscript (lines 403-412), the RIFT effect observed likely stems from local congruency. Therefore, adjusting the conclusion to &quot;integrated with previous context&quot; may offer a more precise reflection of the findings.</p></disp-quote><p>We appreciate the positive comments from the Reviewer.</p><p>In response to the first concern, we have rephrased the sentence (Line 207-209 in the revised manuscript) to clarify that RIFT measure visual attention : âMoreover, as RIFT directly measures visual attention, the left-skewed RIFT response curve suggests that more visual attention is allocated towards the flickering target words before fixating on them, aligning with the left-to-right order of reading English.â</p><p>Regarding the second concern, we have addressed the issue by modifying âsentence contextâ to âprevious contextâ in both the Abstract (Line 18 and Line 22) and the Discussion section (Line 314 and Line 361) of the revised manuscript.</p></body></sub-article></article>