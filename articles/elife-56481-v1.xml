<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56481</article-id><article-id pub-id-type="doi">10.7554/eLife.56481</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>EEG-based detection of the locus of auditory attention with convolutional neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-177129"><name><surname>Vandecappelle</surname><given-names>Servaas</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0266-7293</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177420"><name><surname>Deckers</surname><given-names>Lucas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177421"><name><surname>Das</surname><given-names>Neetha</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177419"><name><surname>Ansari</surname><given-names>Amir Hossein</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177422"><name><surname>Bertrand</surname><given-names>Alexander</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4827-8568</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-206098"><name><surname>Francart</surname><given-names>Tom</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Neurosciences</institution>, <institution>Katholieke Universiteit Leuven</institution>, <addr-line><named-content content-type="city">Leuven</named-content></addr-line>, <country>Belgium</country></aff><aff id="aff2"><institution content-type="dept">Department of Electrical Engineering</institution>, <institution>Katholieke Universiteit Leuven</institution>, <addr-line><named-content content-type="city">Leuven</named-content></addr-line>, <country>Belgium</country></aff><aff id="aff3"><institution content-type="dept">Dept. of Neurosciences</institution>, <institution>Katholieke Universiteit Leuven</institution>, <addr-line><named-content content-type="city">Leuven</named-content></addr-line>, <country>Belgium</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-19576"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Reviewing editor</role><aff><institution>Carnegie Mellon University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>servaas.vandecappelle@gmail.com</email> (SV);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>tom.francart@kuleuven.be</email> (TF);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>30</day><month>04</month><year>2021</year></pub-date><volume>10</volume><elocation-id>e56481</elocation-id><history><date date-type="received"><day>28</day><month>02</month><year>2020</year></date><date date-type="accepted"><day>28</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>Â© 2021, Vandecappelle et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Vandecappelle et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56481-v1.pdf"/><abstract><p>In a multi-speaker scenario, the human auditory system is able to attend to one particular speaker of interest and ignore the others. It has been demonstrated that it is possible to use electroencephalography (EEG) signals to infer to which speaker someone is attending by relating the neural activity to the speech signals. However, classifying auditory attention within a short time interval remains the main challenge. We present a convolutional neural network-based approach to extract the locus of auditory attention (left/right) without knowledge of the speech envelopes. Our results show that it is possible to decode the locus of attention within 1 to 2 s, with a median accuracy of around 81%. These results are promising for neuro-steered noise suppression in hearing aids, in particular in scenarios where per-speaker envelopes are unavailable.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>KU Leuven Special Research Fund</institution></institution-wrap></funding-source><award-id>C14/16/057</award-id><principal-award-recipient><name><surname>Francart</surname><given-names>Tom</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>KU Leuven Special Research Fund</institution></institution-wrap></funding-source><award-id>C24/18/099</award-id><principal-award-recipient><name><surname>Bertrand</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>Research Foundation Flanders</institution></institution-wrap></funding-source><award-id>1.5.123.16N</award-id><principal-award-recipient><name><surname>Bertrand</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Research Foundation Flanders</institution></institution-wrap></funding-source><award-id>G0A4918N</award-id><principal-award-recipient><name><surname>Bertrand</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution>European Research Council</institution></institution-wrap></funding-source><award-id>637424</award-id><principal-award-recipient><name><surname>Francart</surname><given-names>Tom</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution>European Research Council</institution></institution-wrap></funding-source><award-id>802895</award-id><principal-award-recipient><name><surname>Bertrand</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experiment was approved by the Ethics Committee Research UZ/KU Leuven (S57102) and every participant signed an informed consent form approved by the same commitee.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Code used for training and evaluating the network has been made available at https://github.com/exporl/locus-of-auditory-attention-cnn. The CNN models used to generate the results shown in the paper are also available at that location. The dataset used in this study had been made available earlier at https://zenodo.org/record/3377911.</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Das</collab><collab>Neetha</collab><collab>Francart</collab><collab>Tom</collab><collab>&amp; Bertrand</collab><collab>Alexander</collab></person-group><year iso-8601-date="2019">2019</year><source>Auditory Attention Detection Dataset KULeuven</source><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/3377911">https://zenodo.org/record/3377911</ext-link><comment>http://doi.org/10.5281/zenodo.3377911</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-56481-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>