<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56603</article-id><article-id pub-id-type="doi">10.7554/eLife.56603</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Recurrent processes support a cascade of hierarchical decisions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-88354"><name><surname>Gwilliams</surname><given-names>Laura</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9213-588X</contrib-id><email>leg5@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-165228"><name><surname>King</surname><given-names>Jean-Remi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2121-170X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>NYU Abu Dhabi Institute</institution><addr-line><named-content content-type="city">Abu Dhabi</named-content></addr-line><country>United Arab Emirates</country></aff><aff id="aff3"><label>3</label><institution>Frankfurt Institute for Advanced Studies</institution><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Laboratoire des Systèmes Perceptifs (CNRS UMR 8248), Département d’Études Cognitives, École Normale Supérieure, PSL University</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>09</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e56603</elocation-id><history><date date-type="received" iso-8601-date="2020-03-03"><day>03</day><month>03</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-08-30"><day>30</day><month>08</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Gwilliams and King</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Gwilliams and King</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56603-v2.pdf"/><abstract><p>Perception depends on a complex interplay between feedforward and recurrent processing. Yet, while the former has been extensively characterized, the computational organization of the latter remains largely unknown. Here, we use magneto-encephalography to localize, track and decode the feedforward and recurrent processes of reading, as elicited by letters and digits whose level of ambiguity was parametrically manipulated. We first confirm that a feedforward response propagates through the ventral and dorsal pathways within the first 200 ms. The subsequent activity is distributed across temporal, parietal and prefrontal cortices, which sequentially generate five levels of representations culminating in action-specific motor signals. Our decoding analyses reveal that both the content and the timing of these brain responses are best explained by a hierarchy of recurrent neural assemblies, which both maintain and broadcast increasingly rich representations. Together, these results show how recurrent processes generate, over extended time periods, a cascade of decisions that ultimately accounts for subjects’ perceptual reports and reaction times.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>perceptual decision making</kwd><kwd>magnetoencephalography</kwd><kwd>recurrence</kwd><kwd>human brain</kwd><kwd>decoding</kwd><kwd>reading</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>William Orr Dingwall Foundation</institution></institution-wrap></funding-source><award-id>Dissertation Fellowship</award-id><principal-award-recipient><name><surname>Gwilliams</surname><given-names>Laura</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Abu Dhabi Institute Grant</institution></institution-wrap></funding-source><award-id>G1001</award-id><principal-award-recipient><name><surname>Gwilliams</surname><given-names>Laura</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>660086</award-id><principal-award-recipient><name><surname>King</surname><given-names>Jean-Remi</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007492</institution-id><institution>Fondation Bettencourt Schueller</institution></institution-wrap></funding-source><award-id>Bettencourt-Schueller Foundation</award-id><principal-award-recipient><name><surname>King</surname><given-names>Jean-Remi</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008236</institution-id><institution>Fondation Roger de Spoelberch</institution></institution-wrap></funding-source><award-id>Fondation Roger de Spoelberch</award-id><principal-award-recipient><name><surname>King</surname><given-names>Jean-Remi</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014269</institution-id><institution>Philippe Foundation</institution></institution-wrap></funding-source><award-id>Philippe Foundation</award-id><principal-award-recipient><name><surname>King</surname><given-names>Jean-Remi</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01DC05660</award-id><principal-award-recipient><name><surname>Gwilliams</surname><given-names>Laura</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The dynamics of neural responses during visual perception are best explained by a joint feedforward and recurrent architecture, which both maintains and broadcasts input features over time.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To process the rich sensory flow emanating from the retina, the brain recruits a hierarchical network originating in the primary visual areas and culminating in the infero-temporal, dorso-parietal and prefrontal cortices (<xref ref-type="bibr" rid="bib16">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib29">Maunsell and van Essen, 1983</xref>; <xref ref-type="bibr" rid="bib37">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib7">DiCarlo et al., 2012</xref>).</p><p>In theory, the feedforward recruitment of this neural hierarchy could suffice to explain our ability to recognize visual objects. For example, recent studies demonstrate that artificial feedforward neural networks trained to categorize objects generate similar activations patterns to those elicited in the infero-temporal cortices (<xref ref-type="bibr" rid="bib50">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Schrimpf et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib2">Cichy et al., 2016</xref>).</p><p>However, feedforward architectures have a fixed number of processing stages and are thus unable to explain a number of neural and perceptual phenomena. For example, the time it takes subjects to recognize objects considerably varies from one trial to the next (<xref ref-type="bibr" rid="bib36">Ratcliff and Smith, 2004</xref>). In addition, the neural responses to visual stimuli generally exceed the 200 ms feedforward recruitment of the visual hierarchy (<xref ref-type="bibr" rid="bib5">Dehaene and Changeux, 2011a</xref>; <xref ref-type="bibr" rid="bib26">Lamme and Roelfsema, 2000</xref>).</p><p>A large body of research shows that recurrent processing accounts for such behavioral and neural dynamics (<xref ref-type="bibr" rid="bib26">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib13">Gray et al., 1989</xref>; <xref ref-type="bibr" rid="bib11">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib41">Shadlen and Newsome, 2001</xref>; <xref ref-type="bibr" rid="bib31">O'Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Spoerer et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Kar et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Spoerer et al., 2019</xref>). In this view, recurrent processing would mainly consist of accumulating sensory evidence until a decision to act is triggered (<xref ref-type="bibr" rid="bib31">O'Connell et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Mohsenzadeh et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Rajaei et al., 2019</xref>).</p><p>However, the precise neuronal and computational organization of recurrent processing remains unclear at the system level. In particular, how distinct recurrent assemblies implement series of hierarchical decisions remains a major unknown.</p><p>To address this issue, we use magneto-encephalography (MEG) and structural magnetic-resonance imaging (MRI) to localize, track and decode, from whole-brain activity, the feedforward (0–200 ms) and recurrent processes (&gt;200 ms) elicited by variably ambiguous characters briefly flashed on a computer screen. We show that the late and sustained neural activity distributed along the visual pathways generates, over extended time periods, a cascade of categorical decisions that ultimately predicts subjects’ perceptual reports.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Subjective reports of stimulus identity are categorical</title><p>To investigate the brain and computational bases of perceptual recognition, we used visual characters as described in <xref ref-type="bibr" rid="bib22">King and Dehaene, 2014a</xref>. These stimuli can be parametrically morphed between specific letters and digits by varying the contrast of their individual edges, hereafter referred to as pixels (<xref ref-type="fig" rid="fig1">Figure 1A–B</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental protocol and behavioral results.</title><p>Experiment 1: eight human subjects provided perceptual judgments on variably ambiguous digits briefly flashed at the center of a computer screen (<bold>A</bold>). Reports were made by clicking on a disk, where (i) the radius and (ii) the angle on the disk indicate (i) subjective visibility and (ii) subjective identity respectively. (<bold>B</bold>) Distribution (areas) and mean response (dots) location for each color-coded stimulus. (<bold>C</bold>) Top plots show the same data as B, broken down for each morph set. The x-axis indicates the expected angle given the stimulus pixels (color-coded), hereafter referred to as evidence. The y-axis indicates the angle of the mean response relative to stimulus evidence. The bottom plot shows the same data, grouped across morphs. (<bold>D</bold>) Experiment 2: seventeen subjects categorized a briefly flashed and parametrically manipulated-morph using a two-alternative forced-choice. Stimulus-response mapping changed on every block. (<bold>E</bold>) Mean reaction times as a function of categorical evidence (the extent to which the stimulus objectively corresponds to a letter). (<bold>F</bold>) Mean probability of reporting a letter as a function of categorical evidence. (<bold>G</bold>) Evoked activity estimated with dSPM and estimated across all trials and all subjects. These data are also displayed in <xref ref-type="video" rid="video1">Video 1</xref>. Error-bars indicate the standard-error-of-the-mean (SEM) across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig1-v2.tif"/></fig><p>To check that these stimuli create categorical percepts, we asked eight human subjects to provide continuous subjective reports by clicking on a disk after each stimulus presentation (Experiment 1. <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The radius and the angle of the response on this disk indicated the subjective visibility and the subjective identity of the stimulus, respectively. We then compared (i) the reported angle with (ii) the stimulus evidence (i.e. the expected angle given the pixels) for each morph separately (e.g. 5–6, 6–8, etc). Subjective reports were categorical: cross-validated sigmoidal models better predicted subjects’ responses (r = 0.49 ± 0.05, p=0.002) than linear models (r = 0.46 ± 0.043, p=0.002, sigmoid &gt; linear: p=0.017 <xref ref-type="fig" rid="fig1">Figure 1B–C</xref>).</p><p>We adapted this experimental paradigm for an MEG experiment by modifying three main aspects (Experiment 2). First, we used stimuli that could be morphed between letters and digits, to trigger macroscopically distinguishable brain responses in the visual word form area (VWFA) and number form area (NFA) (<xref ref-type="bibr" rid="bib6">Dehaene and Cohen, 2011b</xref>; <xref ref-type="bibr" rid="bib42">Shum et al., 2013</xref>). Second, we added two task-irrelevant flankers next to the target stimulus (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) to increase our chances of eliciting recurrent processes via crowding (<xref ref-type="bibr" rid="bib45">Strasburger et al., 2011</xref>; <xref ref-type="bibr" rid="bib33">Pelli et al., 2004</xref>). Note that this assumption is based on previous work; we did not test the effect of crowding explicitly in this study. Third, a new set of 17 subjects reported subjective identity via a two-alternative forced-choice button press. The identity-response mapping was orthogonal to the letter/digit category and changed on every block of 48 trials. There were 1920 trials total, 320 of which were presented passively, were not ambiguous and did not require a response.</p><p>Perceptual reports followed a similar sigmoidal pattern to Experiment 1: performance was worse for more ambiguous trials (65%) as compared to unambiguous trials (92%, p&lt;0.001). In addition, reaction time slightly, and consistently, increased with uncertainty (i.e. how ambiguous the stimulus is). For example, highly ambiguous stimuli were identified within 690 ms, whereas nonambiguous stimuli were identified within 624 ms (z = −21.68, p&lt;0.001) (<xref ref-type="fig" rid="fig1">Figure 1E–F</xref>). Although subjects were asked to respond as quickly as possible, the observed reaction times were overall quite slow, reflecting the difficulty of the task.</p></sec><sec id="s2-2"><title>Neural representations are functionally organized over time and space</title><p>Here, we aimed to decompose the sequence of decisions that allow subjects to transform raw visual input into perceptual reports. To this aim, we localized the MEG signals onto subjects’ structural MRI with dynamic statistical parametric mapping (dSPM, <xref ref-type="bibr" rid="bib4">Dale et al., 2000</xref>), and morphed these source estimates onto a common brain coordinate (<xref ref-type="bibr" rid="bib8">Fischl, 2012</xref>; <xref ref-type="bibr" rid="bib12">Gramfort et al., 2014</xref>). The results confirmed that the stimuli elicited, on average, a sharp response in the primary visual areas around 70 ms, followed by a fast feedforward response along the ventral and dorsal visual pathways within the first 150–200 ms. After 200 ms, the activity appeared sustained and widely distributed across the associative cortices up until 500–600 ms after stimulus onset (<xref ref-type="fig" rid="fig1">Figure 1G</xref> and <xref ref-type="video" rid="video1">Video 1</xref>).</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-56603-video1.mp4"><label>Video 1.</label><caption><title>Source-localized evoked response averaged over all trials and subjects.</title><p>Activity is plot in noise-normalized dSPM units, and shown on an inflated cortical surface (center) as well as a two-dimensional ‘glass brain’ that shows activity averaged over the transverse plane (bottom right).</p></caption></media><p>To separate the processing stages underlying these neural responses, we applied (i) mass-univariate; (ii) temporal decoding and (iii) spatial decoding analyses based on the five orthogonal features varying in our study: (1) the position of the stimulus, (2) its identity, (3) its perceived category, (4) its uncertainty and (5) its corresponding button press.</p><sec id="s2-2-1"><title>Mass-univariate</title><p>First, we modeled the source localized neural responses over time and space as a function of the five predictors of interest (multivariate in feature space, univariate in source space). Regressor beta coefficients were estimated for each subject separately, then submitted to a spatio-temporal cluster test (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, details provided in Materials and methods). For stimulus position two clusters were found. One in the left hemisphere (number of sources = 2562, mean t-value = 2.87, 20–1560 ms, p=0.001); and one in the right (number of sources = 2562, mean t-value = 2.92, 40–1560 ms, p=0.0005). Stimulus identity also elicited two clusters. One in the left hemisphere (number of sources = 1946, mean t-value = 2.54, 100–840 ms, p=0.0005); and one in the right (number of sources = 2118, mean t-value = 2.48, 120–860 ms, p=0.001). No significant clusters were found for decision, but the largest cluster ranged from 210 to 320 ms (mean t-value = 1.79, p=0.21). Uncertainty elicited two clusters. One in the left hemisphere (number of sources = 2485, mean t-value = 2.42, 280–1560, p=0.001); and one in the right (number of sources = 2319, mean t-value = 2.4, 340–1560 ms, p=0.008). Motor side resulted in two clusters. One in the left hemisphere (number of sources = 2523, mean t-value = 2.56, 280–1560, p=0.0005); and one in the right (number of sources = 2525, mean t-value = 2.66, 280–1560 ms, p=0.0005). See <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplements 4</xref>–<xref ref-type="fig" rid="fig2s8">8</xref> for a full display of these results.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Spatio-temporal hierarchy.</title><p>(<bold>A</bold>) Mass-univariate statistics. Each row plots the average-across-subjects beta coefficients obtained from regression between single-trial evoked activity and each of the five features orthogonally varying in this study. These results are displayed in <xref ref-type="video" rid="video2">Video 2</xref>. Colors are thresholded based on t-values that exceed an uncorrected p&lt;0.1. We chose this threshold because the perceptual category did not exceed the significance threshold in the univariate tests. (<bold>B</bold>) Spatial-decoders, consisting of linear models fit across all time sample for each source separately, summarize where each feature can be decoded. Lines indicate significant clusters of decoding scores across subjects cluster-corrected p&lt;0.05. (<bold>C</bold>) Temporal-decoders, consisting of linear models fit across all MEG channels, for each time sample separately, summarize when each feature can be decoded. To highlight the sequential generation of each representation, decoding scores are normalized by their respective peaks. Additional non-normalized decoding timecourses are available in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref>. (<bold>D</bold>) The peak and the start of temporal decoding plotted for each subject (dot) and for each feature (color). (<bold>E</bold>) The peak spatial decoding plotted for each subject (dot) and for each feature (color).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Exhaustive set of results for decoding different features of the stimulus, time-locked to stimulus onset.</title><p>Shaded area indicates significant decoding as confirmed with a one-sample temporal cluster test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Exhaustive set of results for decoding different features of the stimulus, time-locked to motor onset.</title><p>Shaded area indicates significant decoding as confirmed with a one-sample temporal cluster test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Decoding letter/digit contrast for symbols not presented in the active condition, where subjects viewed the symbols passively and did not have to make a motor response.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Univariate significance mask for ‘Stimulus Position’.</title><p>Localized sources that are contained in a significant (p&lt;0.05) cluster and the specified time-points are highlighted in black.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Univariate significance mask for ‘Stimulus Identity’.</title><p>Localized sources that are contained in a significant (p&lt;0.05) cluster and the specified time-points are highlighted in black.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Univariate significance mask for ‘Uncertainty’.</title><p>Localized sources that are contained in a significant (p&lt;0.05) cluster and the specified time-points are highlighted in black.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 7.</label><caption><title>Univariate significance mask for ‘Motor Side’.</title><p>Localized sources that are contained in a significant (p&lt;0.05) cluster and the specified time-points are highlighted in black.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp7-v2.tif"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 8.</label><caption><title>Univariate significance across the four significant features.</title><p>Localized sources that are contained in a significant (p&lt;0.05) cluster. Colormap indicates for how many time-points the vertex was included in a cluster.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp8-v2.tif"/></fig><fig id="fig2s9" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 9.</label><caption><title>Significance of multivariate tests across the five significant features.</title><p>Localized sources that are contained in a significant (p&lt;0.05) cluster in the spatial decoding analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp9-v2.tif"/></fig><fig id="fig2s10" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 10.</label><caption><title>Non-corrected log-transformed (base 10) p-values for the mass univariate tests, plotted for each of the five features.</title><p>Note that each feature is has a different color-map threshold.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp10-v2.tif"/></fig><fig id="fig2s11" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 11.</label><caption><title>Average decoding timecourses for each of the five features.</title><p>Timing is locked to stimulus onset (above) and motor response onset (below). Unlike the analysis of Ambiguity (blue line) in the main test, here we median-split the ambiguity variable to fit a logical regression and thus show the AUC values in comparison to the other features.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig2-figsupp11-v2.tif"/></fig><media id="fig2video1" mime-subtype="gif" mimetype="video" xlink:href="elife-56603-fig2-video1.gif"><label>Figure 2—animation 1.</label><caption><title>Violin plot of decoding accuracy for the five features of interest over time.</title><p>Each dot represents a different subject. The shaded area represents the distribution density of the decoding performance scores across subjects. The dashed black line indicates chance decoding performance for that feature.</p></caption></media></fig-group><p>The perceived category did not yield significant results after correction for multiple comparisons. Thus, the rest of our analyses are based on multivariate techniques (univariate in feature space, multivariate in source space), which provide highly superior statistical sensitivity to our experimental manipulations. The direct comparison between the sensitivity of the mass-univariate versus multivariate approaches are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></sec><sec id="s2-2-2"><title>Temporal and spatial decoding</title><p>To overcome the poor SNR of single-trial MEG responses, we next applied multivariate decoding analyses to identify when and where low-level visual features are represented in brain activity. We estimated, at each time sample separately, the ability of an l2-regularized regression to predict, from all MEG sensors, the five stimulus features of interest. Overall, the multivariate analyses were far more sensitive than the univariate tests.</p><p>Stimulus position (the location of the stimulus on the computer screen: left versus right) was decodable between 41 and 1500 ms and peaked at 120 ms (AUC = 0.94; SEM = 0.007; p&lt;0.001 as estimated with second-level non-parametric temporal cluster test across subjects, <xref ref-type="fig" rid="fig2">Figure 2C</xref>). To summarize where stimulus position was represented in the brain, we implemented ‘spatial decoders’: l2-regularized logistic regressions fit across all time samples (0–1500 ms) for each estimated brain source separately. Spatial decoding peaked in early visual areas and was significant across a large variety of visual and associative cortices as estimated with a second-level non-parametric spatial cluster test across subjects (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Stimulus position was encoded in the timecourse of all sources, in both the left hemisphere (mean t-value = 9.42, p=0.0005) and the right (mean t-value = 9.97, p=0.0005). These signals peaked in the early visual cortex (mean MNI [x = 27.59; y = −74.15; z = −1.07]), and propagated along the ventral and dorsal streams during the first 200 ms (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="video" rid="video2">Video 2</xref>); confirming the retinotopic organization of the visual hierarchy (<xref ref-type="bibr" rid="bib15">Hagler and Sereno, 2006</xref>; <xref ref-type="bibr" rid="bib48">Wandell et al., 2007</xref>).</p><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-56603-video2.mp4"><label>Video 2.</label><caption><title>Temporal decoding results.</title><p>For each regressor of interest, the trajectory of normalized decoding accuracy is plot over time. The beta coefficients from the univariate spatio-temporal analysis are plot on the inflated brains, averaged over subjects. The timing of the beta coefficients corresponds to the timing of the normalized decoding accuracy, as shown in the ms counter at the bottom.</p></caption></media><p>Second, we aimed to isolate more abstract representations related to stimulus identity. Stimulus identity can be analyzed either from an objective referential (what stimulus is objectively presented?) or from a subjective referential (i.e. what stimulus did subjects report having seen?). We first focus on decoding features of the stimulus that are not ambiguous, such that subjective and objective representations are confounded. To this aim, we grouped stimuli along common continua (e.g. The eight stimuli along the 4-H continuum belong to the same morph and are here considered to share a common identity) and fit logistic regression classifiers across morphs (i.e. E-6 versus 4-H). The corresponding stimulus identity was decodable between 120 and 845 ms and peaked at 225 ms (AUC = 0.59; SEM = 0.01; p&lt;0.001). Spatial decoding revealed decodability from all sources (mean t-value = 7.9, p&lt;0.0001). These effects peaked more anteriorly than those of stimulus position (mean MNI: x = 27.75; y = −62.75; z = −1.55; p&lt;0.001).</p><p>Third, we aimed to isolate the neural signatures of subjective perceptual categorization and thus focus on decoding ambiguous pixels. To this aim, we grouped stimuli based on whether the subject reported a digit or a letter category. Temporal decoders weakly but significantly classified perceptual category from 150 to 940 ms after stimulus onset and peaked at 370 ms (AUC = 0.55; SEM = 0.01; p&lt;0.001, <xref ref-type="fig" rid="fig2">Figure 2C</xref>). The corresponding sources also peaked in the inferotemporal cortex but more anteriorly than stimulus identity (x = 30.89; y = −35.64; z = 21.41; p&lt;0.01). As reported above, the mass-univariate effects did not survive correction for multiple comparisons (e.g. 210–320 ms: mean t-value=1.79, p=0.21). Nonetheless, spatial decoders, which mitigate the trade-off between temporal specificity and the necessity to correct statistical estimates for multiple comparisons, showed that perceptual category was reliably decoded from a large set of brain areas (mean t-value=4.82; p&lt;0.001; 594 significant vertices) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>Importantly, when training the classifier on all active trials to distinguish letters (E/H) and digits (4/6), we could significantly (max AUC = 0.55; SEM = 0.011; p&lt;0.01; 200–550 ms) decode this contrast for different unambiguous tokens (A/C versus 9/8) when trials were presented passively (no button press required). The time-course of responses to these passive trials was statistically indistinguishable until 350 ms. Thereafter, active trials lead to significantly higher decoding accuracy than the generalization to passive trials. This suggests that the decoders specifically track the letter/digit representation, independently of pixel arrangements (see <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref> for passive decoding performance).</p><p>Fourth, trial uncertainty (i.e. the objective distance between the presented stimulus and the closest unambiguous character) could be decoded between 270 and 1485 ms and peaked at 590 ms (l2-regularized Ridge regression fit across sensors, R = 0.12; SEM = 0.024; p&lt;0.01). Uncertainty signals were localized more anteriorly than those of stimulus category (x = 12.58; y = −91.44; z = −1.23; p&lt;0.01). While spatial decoding led to significant clusters in the temporal, parietal and prefrontal areas (mean t-value = 2.91, p=0.002) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), the peak location of stimulus uncertainty was highly variable across subjects and included the dorso-parietal cortex, the temporo-parietal junction and the anterior cingulate cortex (<xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p><p>Finally, temporal decoders of subjects’ button press (left versus right index fingers) were significant from 458 ms after stimulus onset and peaked at 604 ms (AUC = 0.85; SEM = 0.011; p&lt;0.001). A significant cluster of motor signals could be detected around sensorimotor cortices between 590 and 840 ms in the univariate analysis (mean t-value=4.98, p&lt;0.001, <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Spatial decoding corroborated this result (mean t-value 4.1, p&lt;0.0001). Response-locked analyses revealed qualitatively similar but stronger results. For example, temporal decoders were significant from 350 ms prior to the response and up to 500 ms after the response reaching an AUC of 0.94 at response time (p&lt;0.001). Response-locked decoding performance is shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</p><p>Overall, the time at which representations became maximally decodable correlated with their peak location along the postero-anterior axis (<xref ref-type="fig" rid="fig2">Figure 2D–E</xref>) (r = 0.57, p&lt;0.001). The specific hierarchical organization of the stimulus features was in some cases surprising. For example, the letter/digit contrast peaked remarkably late (∼400 ms). Furthermore, Uncertainty was one of the latest features to come online, and extended into responses to the subsequent trial. These results thus strengthen the classic notion that perceptual processes are hierarchically organized across space, time and function. Importantly, however, this cascade of representations spreads over more than 600 ms and largely exceeds the time it takes the feedforward response to ignite the ventral and dorsal pathways (<xref ref-type="fig" rid="fig1">Figure 1G</xref> and <xref ref-type="video" rid="video1">Video 1</xref>).</p></sec></sec><sec id="s2-3"><title>A hierarchy of recurrent layers explains the spatio-temporal dynamics of neural representations</title><p>The above results show that the brain sequentially generates, over an extended time period, a hierarchy of representations that ultimately account for perceptual reports.</p><p>To understand how this cascade of representations emanates from brain activity, we formalize four neural architectures compatible with the above results, and which nonetheless predict distinct spatio-temporal dynamics of each representation (<xref ref-type="fig" rid="fig3">Figure 3</xref>). In these models, we assume that each ‘layer’ generates new hierarchical features, in order to account for the organization of spatial decoders (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Furthermore, we only discuss architectures which can code for all representations simultaneously, in order to account for the overlapping temporal decoding scores (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Finally, we only model discrete activations (i.e. a representation is either encoded or not) as any more subtle variation can be trivially accounted for by signal-to-noise ratio considerations.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Source and temporal generalization predictions for various neural architectures.</title><p>(<bold>A</bold>) Four increasingly complex neural architectures compatible with the spatial and temporal decoders of <xref ref-type="fig" rid="fig2">Figure 2</xref>. For each model (rows), the five layers (L1, L2 … L5) generates new representations. The models differ in their ability to (i) propagate low-level representations across the hierarchy, (ii) maintain information with each layer in a stable or dynamic way. (<bold>B</bold>) Activations within each layer plotted at five distinct time samples. Dot slots indicate different neural assemblies within the same layer. Colors indicate which feature is linearly represented. For clarity purposes, only effective connections are plotted between different time samples. (<bold>C</bold>) Summary of the information represented within each layer across time. (<bold>D</bold>) Expected result for of the temporal generalization analyses, based on the processing dynamics of each model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig3-v2.tif"/></fig><p>Each model predicts (1) ‘source’ decoding time courses (i.e. what is decodable within each layer) and (2) ‘temporal generalization’ (TG) maps. TG is used to characterize the dynamics of neural representations and consists in assessing the extent to which a temporal decoder trained at a given time sample generalizes to other time samples (<xref ref-type="bibr" rid="bib23">King and Dehaene, 2014b</xref>; <xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p>Our spatial and temporal decoding results can be accounted for by a feedforward architecture that both (i) generates new representations at each layer and (ii) propagates low-level representations across layers (<xref ref-type="fig" rid="fig3">Figure 3</xref> Model 1: ‘broadcast’). This architecture predicts that representations would not be maintained within brain areas. However, this lack of maintenance is not supported by additional analyses of our data. First, the position of the stimulus was decodable in the early visual cortex between 80 and 320 ms (mean t-value=5.18, p&lt;0.001) and thus longer than the stimulus presentation. Second, most temporal decoders significantly generalized over several hundreds of milliseconds (<xref ref-type="fig" rid="fig4">Figure 4A–B</xref>). For example, the temporal decoder trained to predict stimulus position from t = 100 ms could accurately generalize until ~500 ms as assessed with spatio-temporal cluster tests across subjects (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Similarly, temporal decoders of perceptual category and button-press generalized, on average, for 287 ms (SEM = 12.47; p&lt;0.001) and 689 ms (SEM = 30.94; p&lt;0.001), respectively. Given that the neural activity underlying the decoded representations is partially stable over several hundreds of milliseconds, recurrent connections seem necessary to account our data (<xref ref-type="fig" rid="fig4">Figure 4</xref> Model 2–4).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Temporal generalization results.</title><p>(<bold>A</bold>) Temporal generalization for each of the five features orthogonally varying in our study. Color indicate decoding score (white = chance). Contours indicate significant decoding clusters across subjects. (<bold>B</bold>) Cumulative temporal generalization scores for the temporal decoders trained at 100, 200, 300, 400 and 500 ms, respectively. These decoding scores are normalized by mean decoding peak for clarity purposes. (<bold>C</bold>) Same data as A but overlaid. For clarity purposes, contours highlight the 25th percentile of decoding performance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig4-v2.tif"/></fig><p>Consequently, we then considered a simple hierarchy of recurrent layers, where recurrence only maintains activated units (<xref ref-type="fig" rid="fig3">Figure 3</xref> Model 2: ‘maintain’). This architecture predicts strictly square TG matrices (i.e. temporal decoders would be equivalent to one another in terms of their performance) and is thus at odds with the largely diagonal TG matrices observed empirically (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Specifically, the duration of significant temporal decoding (fitting a new decoder at each time sample) was significantly longer than the generalization of a single decoder to subsequent time samples (e.g. 1239 versus 287 ms for perceptual category (t = −61.39; p&lt;0.001) and 1215 versus 689 ms for button-press (t = −16.26; p&lt;0.001), <xref ref-type="fig" rid="fig4">Figure 4B</xref>). These results thus suggest that the decoded representations depend on dynamically changing activity: that is each feature is linearly coded by partially distinct brain activity patterns at different time samples.</p><p>It is difficult to determine, with MEG alone, whether such dynamic maintenance results from a change of neural activity within or across brain areas. Indeed, Model 1 and Model 3 can equally predict diagonal TG (<xref ref-type="fig" rid="fig3">Figure 3</xref>). However, these two models, and their combination (Model 4) diverge in terms of <italic>where</italic> information should be decodable. Specifically, source analyses revealed that both stimulus position and perceptual category can be decoded across a wide variety of partially-overlapping brain areas (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="video" rid="video2">Video 2</xref>), similarly to Model 4. Nonetheless, our MEG study remains limited in assessing whether within brain regions dynamics also contribute to the diagonal TG, which would suggest a mixture between models 3 and 4.</p><p>Together, source and TG analyses thus suggest that the slow and sequential generation of increasingly abstract representations depends on a hierarchy of recurrent layers that generate, maintain and broadcast representations across the cortex.</p></sec><sec id="s2-4"><title>Hierarchical recurrence induces an accumulation of delays</title><p>Can a hierarchy of recurrent processes account for the variation in single-trial dynamics? To address this issue, we hypothesized that recurrent processes would take variable amounts of time to converge to each intermediary representation. In this view, (i) each feature is predicted to propagate across brain areas at distinct moments, and (ii) the successive rise of decodable representations is thus predicted to incrementally correlate with reaction times (<xref ref-type="fig" rid="fig5">Figure 5A–E</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Correlation between TG peaks and reaction times.</title><p>(<bold>A, B</bold>) Recurrent processing at a given processing stage is hypothesized to take a variable amount of time to generate adequate representations. (<bold>C</bold>) According to this hypothesis, the rise of the corresponding and subsequent representations would correlate with reaction times. (<bold>D</bold>, left) Predictions when delays are only induced by the perceptual stage of processing. (<bold>D</bold>, middle) Predictions when delays are only induced by the motor processing processing stage. (<bold>D</bold>, right) Predictions when delays are induced by all processing stages. (<bold>E</bold>) TG scores aligned to training time, split into trials within the fastest and slowest reaction-time quantile and averaged across reaction times bins. Dark and light lines indicate the average decoding performance for trials with fastest and slowest reaction times respectively. (<bold>F</bold>) Each subject (dot) mean peak decoding time (y-axis) as a function of reaction time (x-axis) color-coded from dark (fastest) to light (slowest). The beta coefficients indicate the average delay estimate. (<bold>G</bold>) The average slope between processing delay and reaction time for each feature. Error-bars indicate the SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Schematic of the processing delay analysis pipeline.</title><p>The top ‘empirical’ figure shows the temporal generalization matrix for decoding ambiguity split into the trials that were responded to the fastest (top 25%) and slowest (bottom 25%). The cartoon below shows the method for estimating latency curves. First we re-align the temporal generalization matrices such that the test-time is shifted relative to the diagonal plane. Then, we average over training times, to yield the latency curve for subsequent analyses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig5-figsupp1-v2.tif"/></fig></fig-group><p>To test this hypothesis, we estimated how the peak of each temporal decoder varied with reaction times. For clarity purposes, we split reaction times into four quantiles, and averaged the time courses of temporal decoders relative to their training time. This method allowed us to assess the extent to which neural processes are ‘sped up’ or ‘slowed down’ relative to the average processing speed, as represented by the diagonal axis (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> summarizes this method). These analyses showed that the latencies of (i) perceptual category (r = 0.35; p=0.006), (ii) stimulus uncertainty (r = 0.37; p=0.004) and (iii) button press (r = 0.66; p&lt;0.001) increasingly correlated with reaction times (<xref ref-type="fig" rid="fig5">Figure 5F–G</xref>).</p><p>Overall, these results show that we can track with MEG, a series of decisions generated by hierarchical recurrent processes. This neural architecture partially accounts for subjects’ variable and relatively slow reaction times.</p></sec><sec id="s2-5"><title>Hierarchical recurrence implements a series of all-or-none decisions</title><p>An architecture based on successive decisions predicts a loss of ambiguous information akin to all-or-none categorization across successive processing stages (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). To test this prediction, we quantified the extent to which the decoding of ‘percept category’ and of ‘motor action’ varied linearly or categorically with (i) categorical evidence and (ii) motor evidence respectively (i.e. the extent to which the stimulus (i) objectively looks like a letter or a digit and (ii) should have led to a left or right button press given its pixels). Note that due to the limitations of our experimental design, we can only assess the effect of stimulus evidence at these two level of representations: stimulus evidence only varied as a function of decision and motor response but was orthogonal to stimulus position and stimulus identity.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Motor and perceptual decisions.</title><p>(<bold>A</bold>) Hypothesis space for when responses become categorical: during sensory, perceptual or motor processing. (<bold>B</bold>, top) Time course of decoding the perceptual decision. (<bold>B</bold>, bottom) Classifier predictions split into different levels of sensory evidence. (<bold>C</bold>) Averaging probabilities in different time-windows shows the linear-categorical shift in how information is represented. (<bold>D</bold>, top) Time course of decoding the motor decision. (<bold>D</bold>, bottom) Splitting classifier predictions into different levels of uncertainty. (<bold>E</bold>) Different windows of classifier predictions, showing the categorical responses throughout processing.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Decoding the distinction between the unambiguous end-points and stimuli at different step distances away.</title><p>The purple line corresponds to decoding the difference between unambiguous end-points and one step. A temporal cluster test revealed no significant above-chance decodability. The blue line corresponds to the unambiguous end-points versus two steps. Decoding was significantly above chance from 750 to 900 ms (p&lt;0.001) and again from 1300 to 1400 ms (p=0.007). The teal line corresponds to the unambiguous end-points versus three steps. Decoding was significant from 270 to 1570 ms (p&lt;0.001).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Decoding performance for the letter/digit contrast in active trials requiring a button press (blue line), and passively presented non-ambiguous trials requiring no response (red line).</title><p>For the active trials, the classifier is fit with a k-fold cross-validation – training on active trials and testing on active trials. For the passive trials, the classifier is trained on all of the active trials and tested on the 320 passive trials. Note that the actively presented letter/digit stimuli (4, 6 vs H, E) were different from the passively presented ones (9, 8 vs. A, C), thus any significant generalization of neural responses demonstrates a true ‘letter/digit’ percept, which is not specific to particular pixel arrangements. Thicker lines in the decoding timecourses show the timing of significant above-chance decoding for passive trials and active trials separately. Shaded grey areas show when the two timecourses significantly diverge. Both statistical tests used a temporal permutation cluster test and clusters are displayed that are significant at p&lt;0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56603-fig6-figsupp2-v2.tif"/></fig></fig-group><p>The probabilistic decoding predictions of percept category correlated linearly with sensory evidence between 210 and 530 ms (r = 0.38 ±0.03, temporal-cluster p&lt;0.001). The spatial decoders fit from 200 to 400 ms clustered around the VWFA (mean t-value=4.6; p=0.02; 224 vertices). These results suggest that this region first represents the stimulus objectively (i.e. in its full ambiguity).</p><p>Between 400 and 810 ms, the predictions of ‘perceptual category’ decoders were better accounted for by sigmoidal (r = 0.77 ±0.03, p&lt;0.001) than by linear trends (r = 0.7 ±0.03, p&lt;0.001). This suggests that later responses track the categorical perception rather than the linearly varying input. Spatial decoding analyses restricted to the 500–700 ms time window was more distributed (mean t-value=4.4; p=0.022; 110 vertices). Finally, ambiguous stimuli (steps 5 and 6 on the continuum) reached maximum decodability 205 ms later than unambiguous stimuli (steps 1 and 8) (p&lt;0.001) (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The interaction between trend (linear or sigmoidal) and window latency was significant across subjects (r = 0.07; SEM = 0.01; p=0.002).</p><p>This progressive categorization of the letter/digit representations contrasts with the all-or-none pattern of motor signals. Specifically, the probabilistic predictions of button-press decoders varied categorically with response evidence from 440 to 1290 ms (sigmoid &gt; linear cluster, mean t-value=3.17; p&lt;0.001). There was also a more transient linear trend from 410 to 580 ms (mean t-value=3.69; p&lt;0.001). This suggests that, unlike perceptual category, motor signals largely derive from categorical inputs.</p><p>Together, delay (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and categorization (<xref ref-type="fig" rid="fig6">Figure 6</xref>) analyses thus show that perceptual representations slowly become categorical and are subsequently followed by all-or-none motor representations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While the role of feedforward processes is becoming increasingly understood, <italic>what</italic> recurrent processes represent and <italic>how</italic> they are orchestrated remains largely unknown. Here, we show with source-localized MEG that recurrent processes sequentially generate, over an extended time period, a hierarchy of representations that ultimately account for the timing and the content of perceptual reports.</p><p>The conclusions of the present study are limited by two main aspects. First, while our sensory-motor and letter/digit representations are largely consistent with previous findings (<xref ref-type="bibr" rid="bib3">Cohen et al., 2000</xref>; <xref ref-type="bibr" rid="bib42">Shum et al., 2013</xref>), MEG source reconstruction remains imperfect. Consequently, identifying (1) the role of subcortical areas and (2) the extent to which representations dynamically change within each brain area will necessitate invasive brain recordings.</p><p>Second, our study focuses on the brain responses to individual characters. This unusual task (<xref ref-type="bibr" rid="bib22">King and Dehaene, 2014a</xref>) thus adds to the long list of arbitrary stimuli used to probe the neural bases of decisions. Indeed, perceptual decisions have been investigated through the manipulation of Gabor patches (e.g. <xref ref-type="bibr" rid="bib49">Wyart et al., 2012</xref>), clouds of moving dots (<xref ref-type="bibr" rid="bib41">Shadlen and Newsome, 2001</xref>) and even bathroom and kitchen images (<xref ref-type="bibr" rid="bib27">Linsley and MacEvoy, 2014</xref>). Our results are consistent with these studies in that perceptual decisions are represented up to the fronto-parietal cortices. In particular, <xref ref-type="bibr" rid="bib9">Freedman et al., 2002</xref> parametrically manipulated images of cats and dogs through 3D morphing and also show that the lateral prefrontal cortex reflects the category of the stimuli independently of their physical similarity. The benefit of our design choice is that it allowed us to (1) orthogonalize and parametrically manipulate five levels of representations and (2) track the interplay between these different levels of representations in both time and space. In the future, it will thus be critical to verify that these findings can be observed across a wide variety of stimuli (e.g. <xref ref-type="bibr" rid="bib18">Kar et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Kietzmann et al., 2019</xref>), and to further investigate whether decisional boundaries can be manipulated online by specific task demands (<xref ref-type="bibr" rid="bib9">Freedman et al., 2002</xref>).</p><p>Overall, our results bridge three important lines of research on the neural and computational bases of visual processing.</p><p>First, core-object recognition research, generally based on ~100 ms-long image presentations has repeatedly shown that the spiking responses of the inferotemporal cortex is better explained by recurrent models than by feedforward ones (<xref ref-type="bibr" rid="bib26">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib18">Kar et al., 2019</xref>). In particular, <xref ref-type="bibr" rid="bib18">Kar et al., 2019</xref> have recently shown that images that are challenging to recognize, lead to delayed content-specific spiking activity in the macaque’s infero-temporal cortex. Similar evidence for recurrent processes was recently found using MEG (<xref ref-type="bibr" rid="bib20">Kietzmann et al., 2019</xref>). Our findings, based on simpler but highly controlled stimuli, are consistent with these results and further highlight that perceptual representations are not confined to the inferotemporal cortices, but also reach a large variety of parietal and prefrontal areas (<xref ref-type="bibr" rid="bib10">Freedman and Miller, 2008</xref>).</p><p>The specific order that perceptual representations were generated was not entirely predictable a priori. In particular, we were surprised to find that Uncertainty was one of the last variables to come online (∼300 ms) and extended into the processing of the subsequent trial. This result may relate to the fact that, here, Uncertainty is confounded with memory and task-engagement effects, rather than solely the processing of the stimulus property per se (<xref ref-type="bibr" rid="bib1">Bate et al., 1998</xref>). However, this result starkly contrasts with recent work showing very early sensitivity to Uncertainty (∼50 ms) in an auditory syllable categorization task (<xref ref-type="bibr" rid="bib14">Gwilliams et al., 2018</xref>), suggesting that the latency of this response may also depend on the sensory modality and familiarity with the visual or auditory object at study.</p><p>Second, the present study makes important contributions to the perceptual decision making literature (<xref ref-type="bibr" rid="bib11">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib31">O'Connell et al., 2012</xref>). With some notable exceptions (e.g. <xref ref-type="bibr" rid="bib34">Philiastides and Sajda, 2007</xref>), this line of research primarily aims to isolate motor and supra-modal decision signals in the presence of sustained visual inputs: that is, neural responses ramping toward a virtual decision threshold, independently of the representation on which this decision is based (<xref ref-type="bibr" rid="bib31">O'Connell et al., 2012</xref>). The present study complements this approach by tracking the representation-specific signals that slowly emerge after a brief stimulus.</p><p>Our results thus open an exciting avenue for querying the gating mechanisms of successive decisions and clarifying the role of the prefrontal areas in the coordination multiple perceptual and supramodal modules (<xref ref-type="bibr" rid="bib38">Sarafyazd and Jazayeri, 2019</xref>).</p><p>Finally, our results constitute an important confirmation of modern theories of perception. In particular, the Global Neuronal Workspace Theory predicts that perceptual representations need to be broadcast to associative cortices via the fronto-parietal areas to lead to subjective reports (<xref ref-type="bibr" rid="bib5">Dehaene and Changeux, 2011a</xref>). Yet, at some notable exceptions (<xref ref-type="bibr" rid="bib46">Tong et al., 1998</xref>; <xref ref-type="bibr" rid="bib21">King et al., 2016</xref>), previous studies often fail to dissociate perceptual contents and perceptual reports (e.g. <xref ref-type="bibr" rid="bib40">Sergent et al., 2005</xref>; <xref ref-type="bibr" rid="bib47">van Vugt et al., 2018</xref>). By contrast, the present experimental design allows an unprecedented dissection of the distinct processing stages that transform sensory input into perceptual representations and, ultimately, actions. The generation of letter and digit representations in the dedicated brain areas (<xref ref-type="bibr" rid="bib3">Cohen et al., 2000</xref>; <xref ref-type="bibr" rid="bib42">Shum et al., 2013</xref>) and their subsequent broadcast to the cortex reinforce the notion that subjective perception relate to the global sharing of content-specific representations across brain areas (<xref ref-type="bibr" rid="bib5">Dehaene and Changeux, 2011a</xref>; <xref ref-type="bibr" rid="bib25">Lamme, 2003</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Target stimuli</title><p>Using the font designed in <xref ref-type="bibr" rid="bib22">King and Dehaene, 2014a</xref>, the stimuli were made from 0, 4, 5, 6, 8, 9, A, C, E, H, O, S, or from a linear combination of two of these characters varying in a single black bar (hereafter ‘pixel’). The corresponding ‘morphs’ were created by adjusting the contrast of the remaining pixel along eight equally spaced steps between 0 (no bar) and 1 (black bar).</p></sec><sec id="s4-2"><title>Experiment 1</title><p>Eight subjects with normal or corrected vision, seated ~60 cm from a 19’ CRT monitor (60 Hz refresh rate, resolution: 1024 × 768), performed a stimulus identification task with continuous judgements across 28 variably ambiguous stimuli generated from digit stimuli. Ten euros were provided in compensation for this 1 hr experiment.</p><p>Subjects performed four blocks of 50 trials, each organized in the following way. After a 200 ms fixation, a target stimulus, randomly selected from one of the 28 stimuli, was flashed for 83 ms on a 50% gray background to the left or to the right of fixation. The orientation of the reporting disk (e.g 5-6-8-9 versus 5-9-8-6) was counterbalanced across subjects. Subjects had then up to 10 s to move a cursor on a large disk to report their percepts. The radius on the disk indicated subjective visibility (center = did not see the stimulus, disk border = max visibility). The angle on the disk indicated subjective identity (e.g. 5, 6, 8, 9 for the top left, top right, bottom right, and bottom left ‘corners’, respectively). Inter-trial interval was 500 ms. To verify that subjects provided meaningful reports, the target stimulus was absent 15% of the trials. Absent trials were rated with a low visibility (defined as radius below 5% of the disk radius) in most cases. Absent trials and trials reported with a low visibility were excluded from subsequent analyses (16% ± 1.4%). The report distribution plotted in <xref ref-type="fig" rid="fig1">Figure 1B</xref> were generated with Seaborn’s bivariate Gaussian kernel density estimate function with default parameters.</p><sec id="s4-2-1"><title>Modeling categorical reports</title><p>To test whether subjective reports of stimulus identity varied linearly or categorically with sensory evidence, we analyzed how reports’ angle (i.e. subjective identity) varied with the expected angle given the stimulus (i.e. sensory evidence).</p><p>For each morph (5–6, 5–8, 9–8 and 6–8) separately, we fit a linear model: <disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>←</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>and a sigmoidal model:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>←</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf1"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is the report angle predicted by the model, <italic>x</italic> is expected angle given the stimulus pixels and <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is a free bias parameter.</p><p>To minimize the effects of noise, behavioral reports were first averaged within each level of evidence, sorted from the stimulus with the least pixels (e.g. 5, in 5–6 morph) to the stimulus with the most pixels (e.g. 6 in the 5–6 morph). The resulting averages were normalized to range between 0 and 1 within each subject. The β parameters were fit with Scipy’s ‘curve_fit’ function (<xref ref-type="bibr" rid="bib17">Jones et al., 2001</xref>) to minimize a mean squared error across trials <italic>i</italic>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>β</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Because the linear and sigmoidal models have distinct numbers of free parameters, we compared them within a five-split cross-validation. Specifically, the two models were repeatedly fit and tested on independent trials. A Pearson correlation coefficient <italic>r</italic> summarized the ability of each model to accurately predict <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> given <inline-formula><mml:math id="inf4"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Finally, a Wilcoxon test was applied across subjects to test whether the two models were consistently above chance (<inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) and consistently different from one another (<inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>).</p></sec></sec><sec id="s4-3"><title>Experiment 2</title><p>This experiment was performed at Neurospin, Gif usr Yvette, thanks to the support of Stanislas Dehaene. Seventeen subjects performed a discrete identification task across 22 variably ambiguous stimuli generated from letters and digits inside an Elekta Neuromag MEG scanner (204 planar gradiometers and 102 magnetometers). Seventy euros were provided in compensation to the 1 hr experiment and 30 min of preparation.</p><p>A sample size of 17 participants was selected based on previous visual studies utilizing the same MEG machine (<xref ref-type="bibr" rid="bib21">King et al., 2016</xref>).</p><p>Participants’ head shape was digitized along with five fiducial points on the forehead and on each aural canal. Five head-position coils were placed on subjects head and localized at the beginning of each block.</p><p>The trial structure was as follows. A black fixation cross was displayed on a 50% gray background for 300 ms followed by a 100ms-long target stimulus presented on the left or on the right of fixation. Two task-irrelevant flankers (e.g. stimulus can be read as an S or a 5) were displayed on the side of this target stimulus to increase our chances of eliciting recurrent processing via crowding (<xref ref-type="bibr" rid="bib45">Strasburger et al., 2011</xref>). Subjects were given two seconds to report the identity of the stimulus. Reports of stimulus identity were given by pressing a button with the left and right index fingers respectively. The identity-button mapping changed on every block to orthogonalize the neural correlates of stimulus identity and the neural correlates of motor actions. For example, in block 1, perceiving an E or a 4 should have been reported with a left button press, whereas in block 2, E and 4 should have been reported with a right button press. The identity-button was explicitly reminded before each block. In addition, a visual feedback was displayed after non-ambiguous trials. Specifically, the fixation turned green for 100 ms or red for 300 ms in correct and incorrect trials respectively. The brain responses to these feedback stimulations are not analyzed in the present study. Inter-trial interval was 1 s. Subjects were provided a short training to ensure they understood the task, and identified non-ambiguous targets at least 80% of the time.</p><p>A total of 1920 trials, grouped into 40 blocks, were performed by each subject, 320 of which were presented passively at the end of each block – subjects were not required to provide a response. The trial structure was generated by (i) permuting all combinations of stimulus features (e.g. position, identity, response mapping, uncertainty), and (ii) shuffling the order of presentation for each subject. The experiment was presented using Psychtoolbox (<xref ref-type="bibr" rid="bib24">Kleiner et al., 2007</xref>).</p><p>All experiments were approved by the local ethics committee. All subjects signed an informed consent form.</p></sec><sec id="s4-4"><title>Structural MRI</title><p>For each subject, an anatomical MRI with a resolution of <inline-formula><mml:math id="inf9"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1.1</mml:mn></mml:mrow></mml:math></inline-formula> mm was acquired after the MEG experiment with a 3T Siemens scanner. Gray and white matter were segmented with Freesurfer ‘recon-all’ pipeline (<xref ref-type="bibr" rid="bib8">Fischl, 2012</xref>) and coregistered with each subject’s digitized head shapes along with fiducial points.</p></sec><sec id="s4-5"><title>Preprocessing</title><p>The continuous MEG recording was noise-reduced using Maxfilter’s SSS correction on the raw data, bandpass-filtered between 0.5 and 40 Hz using MNE-Python’s default parameters with firwin design (<xref ref-type="bibr" rid="bib12">Gramfort et al., 2014</xref>) and downsampled to 250 Hz. Epochs were then segmented between −300 ms and +1500 ms relative to stimulus onsets.</p><p>After coregistering the MEG sensor data with subjects’ structural MRI and the head position coils, we computed the forward model using a 1-layer (inner skull) boundary element model, for each subject separately and fit a minimum-norm inverse model (signal to noise ratio: 3, loose dipole fitting: 0.2, with normal orientation of the dipole relative to the cortical sheet) using the noise covariance across sensors averaged over the pre-stimulus baseline across trials. Finally, the inverse model was applied to single-trial data resulting in a dynamic Statistical Parameter Map (dSPM) (<xref ref-type="bibr" rid="bib4">Dale et al., 2000</xref>) value for each source at each time sample.</p></sec><sec id="s4-6"><title>Modeled features</title><p>We investigated whether single-trial source and sensor evoked responses varied as a function of five features: (1) the position of the stimulus on the computer screen (left versus right of fixation), (2) the morph from which the stimulus is generated (E-6 versus H-4), (3) the category of the stimulus (letter versus digit), (4) the uncertainty of the trial (maximum uncertainty = stimuli with pixel at 50% contrast; minimally uncertain stimuli with pixels at 0% or 100% contrast), and (5) the response button used to report the stimulus (left versus right button). By design, these five features are independent of one another.</p><p>It is challenging to dissociate brain responses that represent objective sensory information from those that represent perceptual decisions as the two are generally collinear. To address this issue, we first fit univariate and multivariate models to predict perceptual category: that is, whether the button press indicated a character that belongs to the digit or to the letter category. This feature is independent of the button press (e.g. the letter E and the digit 4 can be reported with the same button in a given block). Furthermore, this feature is not necessary to perform the task (i.e. knowing whether E and H are letters is unnecessary to discriminate them). We reasoned that if subjects automatically generates letter/digit representations during perceptual categorization, then we should be able to track the generation of this abstract feature from brain activity.</p></sec><sec id="s4-7"><title>Mass univariate statistics</title><p>To estimate whether brain responses correlated with each of these five features, we first fit, within each subject, mass univariate analyses at each source location and for each time sample with a linear regression:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a design matrix of <italic>n</italic> epochs by <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> features and <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the univariate brain response at a given source and at given time. The effect sizes β were then passed to second-level statistics across subjects corrected for multiple comparisons using non-parametric spatio-temporal cluster testing (see below).</p></sec><sec id="s4-8"><title>Decoding</title><p>Decoding analyses consists in predicting each feature from multivariate brain responses. Decoding analyses were performed within a five-split stratified K-Fold cross-validation using l2-regularized linear models. Classifiers consisted of logistic regressions (with scikit-learn <xref ref-type="bibr" rid="bib32">Pedregosa, 2011</xref>’s default parameters: <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>β</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the feature to be decoded at trial <italic>i</italic> and <italic>x</italic><sub><italic>i</italic></sub> is the multivariate brain response.</p><p>Regressors consisted of ridge regression (with scikit-learn <xref ref-type="bibr" rid="bib32">Pedregosa, 2011</xref>’s default parameters: <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>).<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>β</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For each subject independently, decoding performance was summarized across trials, with an area under the curve (AUC) and a Spearman <italic>r</italic> correlation score for classifiers and regressors, respectively.</p><p>All decoders were provided with data normalized by the mean and the standard deviation in the training set.</p><p>Spatial decoding consists in fitting a series of decoders at each brain source independently, across all 1500 time samples relative to stimulus onset. This analysis results in a decoding brain map that indicates where a feature can be linearly decoded in the brain. These decoding maps were then passed to cluster-corrected second-level statistics across subjects.</p><p>Temporal decoding consists in fitting a series of decoders at each time sample independently, across all 306 MEG sensors. This analysis results in a decoding time course that indicates when a feature can be linearly decoded from MEG signals. These decoding time courses were then passed to cluster-corrected second-level statistics across subjects.</p><p>Temporal generalization (TG) consists in testing whether a temporal decoder fit on a training set at time <italic>t</italic> can decode a testing set at time <inline-formula><mml:math id="inf16"><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib23">King and Dehaene, 2014b</xref>). TG can be summarized with a square training time × testing time decoding matrix. To quantify the stability of neural representations, we measured the duration of above-chance generalization of each temporal decoder. To quantify the dynamics of neural representations, we compared the mean duration of above-chance generalization across temporal decoders to the duration of above-chance temporal decoding (i.e. the diagonal of the matrix versus its rows). These two metrics were assessed within each subject and tested with second-level statistics across subjects.</p></sec><sec id="s4-9"><title>Permutation cluster test</title><p>To evaluate the statistical significance of the univariate and multivariate analyses, we used a one-sample permutation cluster test as implemented in MNE-Python (<xref ref-type="bibr" rid="bib12">Gramfort et al., 2014</xref>). We use the default parameters of the ‘spatio temporal cluster one sample test’ from mne version 0.17.1.</p><p>First we center the data around the theoretical chance level (e.g. 0.5 for AUC, 0 for Spearman correlation or beta coefficient). A one-sample t-test is performed at each location in time and space. Then, spatio-temporally adjacent data-points are clustered based on a cluster-forming threshold of p&lt;0.05. The test statistic for each cluster is the sum of the t-values across time and space. Randomized data are generated with random sign flips, and a new set of clusters are formed. The null distribution is created based on the summed t-values that are generated from 5000 random permutations of the data. This analysis follows <xref ref-type="bibr" rid="bib28">Maris and Oostenveld, 2007</xref>.</p></sec><sec id="s4-10"><title>Linear versus categorical</title><p>To test whether neural representations varied as a function of (i) reaction times (RTs, split into four quantiles), (ii) sensory evidence (i.e. the extent to which the stimulus objectively corresponds to a letter) and (iii) motor evidence (i.e. whether the stimulus should have led to the left button press), we analyzed the extent to which decoders’ predictions covaried with each of these three variables <italic>z</italic>: <disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>f</italic> is a linear or a sigmoidal model, <italic>X</italic> is the multivariate brain response and β is the decoder’s coefficient fit with cross-validation.</p></sec><sec id="s4-11"><title>Statistics</title><p>Univariate, decoding and TG models were fit within subjects, and tested across subjects. In case of repeated estimates (e.g. temporal decoding is repeated at each time sample), statistics derived from non-parametric cluster-testing with 10,000 permutations across subjects with MNE-Python’s default parameters (<xref ref-type="bibr" rid="bib12">Gramfort et al., 2014</xref>).</p><sec id="s4-11-1"><title>Simulations</title><p>To formalize how distinct neural architectures lead to distinct spatio-temporal dynamics, we modeled discrete linear dynamical systems forced with a transient input <italic>U</italic>. Specifically:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where X is a multidimensional times series (i.e. neurons x time), A is the architecture, and corresponds to square connectivity matrix (i.e. neurons x neurons), B is an input connectivity matrix (i.e. inputs x neurons), and U is the input vector.</p><p>Distinct architectures differ in the way units are connected with one another. For simplicity purposes, we order units in the <italic>A</italic> matrix such that their row index correspond to their hierarchical levels.</p><p>In this view, the recurrent, feedforward and skip connections of the architecture A were modeled as a binary diagonal matrix <italic>R</italic>, a shift matrix <italic>F</italic> and a matrix <italic>S</italic> with one entries in the last column respectively. These three matrices were modulated by specific weights, as detailed below. The input <italic>U</italic> was only connected to the first 'processing stage’, that is, to the first unit(s) of <italic>A</italic>, via a matrix <italic>B</italic> constant across architectures, and consisted of a transient square-wave input, that mimics the transient flash of the stimulus onto subjects’ retina.</p><p>To model multiple features, we adopted the same procedure with multiple units per layer. Each unit within each layer was forced to encode a specific feature.</p><p>Each architecture shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> was fed an input at t = 1, and simulated for eight time steps. Finally, temporal generalization analyses based on the architectures’ activations were applied for each of the features.</p><p>The same architecture is shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Here we simulate for a total of 108 time-steps, with an arbitrary delay of 100 time-steps between t = 6 and t = 106.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This project received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 660086, the Bettencourt-Schueller Foundation, the Fyssen Foundation, the Philippe Foundation, the Abu Dhabi Institute G1001, NIH R01DC05660 and the Dingwall Foundation. We are infinitely grateful to Stanislas Dehaene, as well as David Poeppel and Alec Marantz, for their support. We thank Michael Landy for his very helpful and generous feedback on a previous version of the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: This study was ethically approved by the comité de protection des personnes (CPP) IDF 7 under the reference CPP 08 021. All subjects gave written informed consent to participate in this study, which was approved by the local Ethics Committee, in accordance with the Declaration of Helsinki. Participants were compensated for their participation.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Summary table showing the timing and significance of the results across the five features across the three statistical analyses (temporal decoding, spatial decoding and mass univariate).</title><p>pfc corresponds to the p-value after Bonferroni correction across the five features. Average t-value corresponds to the average t-value in the cluster.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-56603-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-56603-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Anonymised source data for figures have been uploaded to Dryad: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.70rxwdbtr">https://doi.org/10.5061/dryad.70rxwdbtr</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Recurrent processes support a cascade of hierarchical decisions</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.70rxwdbtr</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bate</surname> <given-names>A</given-names></name><name><surname>Lindquist</surname> <given-names>M</given-names></name><name><surname>Edwards</surname> <given-names>IR</given-names></name><name><surname>Olsson</surname> <given-names>S</given-names></name><name><surname>Orre</surname> <given-names>R</given-names></name><name><surname>Lansner</surname> <given-names>A</given-names></name><name><surname>De Freitas</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A bayesian neural network method for adverse drug reaction signal generation</article-title><source>European Journal of Clinical Pharmacology</source><volume>54</volume><fpage>315</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1007/s002280050466</pub-id><pub-id pub-id-type="pmid">9696956</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Naccache</surname> <given-names>L</given-names></name><name><surname>Lehéricy</surname> <given-names>S</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Hénaff</surname> <given-names>MA</given-names></name><name><surname>Michel</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients</article-title><source>Brain : A Journal of Neurology</source><volume>123 (Pt 2)</volume><fpage>291</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1093/brain/123.2.291</pub-id><pub-id pub-id-type="pmid">10648437</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Liu</surname> <given-names>AK</given-names></name><name><surname>Fischl</surname> <given-names>BR</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Belliveau</surname> <given-names>JW</given-names></name><name><surname>Lewine</surname> <given-names>JD</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title><source>Neuron</source><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81138-1</pub-id><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Changeux</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Experimental and theoretical approaches to conscious processing</article-title><source>Neuron</source><volume>70</volume><fpage>200</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.018</pub-id><pub-id pub-id-type="pmid">21521609</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>The unique role of the visual word form area in reading</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>254</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.04.003</pub-id><pub-id pub-id-type="pmid">21592844</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual categorization and the primate prefrontal cortex: neurophysiology and behavior</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>929</fpage><lpage>941</lpage><pub-id pub-id-type="doi">10.1152/jn.2002.88.2.929</pub-id><pub-id pub-id-type="pmid">12163542</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural mechanisms of visual categorization: insights from neurophysiology</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>32</volume><fpage>311</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2007.07.011</pub-id><pub-id pub-id-type="pmid">17950874</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Luessi</surname> <given-names>M</given-names></name><name><surname>Larson</surname> <given-names>E</given-names></name><name><surname>Engemann</surname> <given-names>DA</given-names></name><name><surname>Strohmeier</surname> <given-names>D</given-names></name><name><surname>Brodbeck</surname> <given-names>C</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><pub-id pub-id-type="pmid">24161808</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname> <given-names>CM</given-names></name><name><surname>König</surname> <given-names>P</given-names></name><name><surname>Engel</surname> <given-names>AK</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties</article-title><source>Nature</source><volume>338</volume><fpage>334</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1038/338334a0</pub-id><pub-id pub-id-type="pmid">2922061</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname> <given-names>L</given-names></name><name><surname>Linzen</surname> <given-names>T</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Marantz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>In spoken word recognition, the future predicts the past</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7585</fpage><lpage>7599</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0065-18.2018</pub-id><pub-id pub-id-type="pmid">30012695</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagler</surname> <given-names>DJ</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spatial maps in frontal and prefrontal cortex</article-title><source>NeuroImage</source><volume>29</volume><fpage>567</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.08.058</pub-id><pub-id pub-id-type="pmid">16289928</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Oliphant</surname> <given-names>T</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>SciPy: Open Source Scientific Tools for Python</source><version designator="3.0.4">3.0.4</version><ext-link ext-link-type="uri" xlink:href="http://www.scipy.org/">http://www.scipy.org/</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname> <given-names>K</given-names></name><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Schmidt</surname> <given-names>K</given-names></name><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id><pub-id pub-id-type="pmid">31036945</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname> <given-names>TC</given-names></name><name><surname>Spoerer</surname> <given-names>CJ</given-names></name><name><surname>Sörensen</surname> <given-names>LKA</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Hauk</surname> <given-names>O</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id><pub-id pub-id-type="pmid">31591217</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Pescetelli</surname> <given-names>N</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain mechanisms underlying the brief maintenance of seen and unseen sensory information</article-title><source>Neuron</source><volume>92</volume><fpage>1122</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.051</pub-id><pub-id pub-id-type="pmid">27930903</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>J-R</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>A model of subjective report and objective discrimination as categorical decisions in a vast representational space</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130204</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0204</pub-id><pub-id pub-id-type="pmid">24639577</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname> <given-names>M</given-names></name><name><surname>Brainard</surname> <given-names>D</given-names></name><name><surname>Pelli</surname> <given-names>D</given-names></name><name><surname>Ingling</surname> <given-names>A</given-names></name><name><surname>Murray</surname> <given-names>R</given-names></name><name><surname>Broussard</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1068/v070821</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Why visual attention and awareness are different</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>12</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)00013-X</pub-id><pub-id pub-id-type="pmid">12517353</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01657-X</pub-id><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linsley</surname> <given-names>D</given-names></name><name><surname>MacEvoy</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Evidence for participation by object-selective visual cortex in scene category judgments</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1167/14.9.19</pub-id><pub-id pub-id-type="pmid">25146577</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname> <given-names>JH</given-names></name><name><surname>van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The connections of the middle temporal visual area (MT) and their relationship to a cortical hierarchy in the macaque monkey</article-title><source>The Journal of Neuroscience</source><volume>3</volume><fpage>2563</fpage><lpage>2586</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.03-12-02563.1983</pub-id><pub-id pub-id-type="pmid">6655500</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenzadeh</surname> <given-names>Y</given-names></name><name><surname>Qin</surname> <given-names>S</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title><source>eLife</source><volume>7</volume><elocation-id>e36329</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id><pub-id pub-id-type="pmid">29927384</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Connell</surname> <given-names>RG</given-names></name><name><surname>Dockree</surname> <given-names>PM</given-names></name><name><surname>Kelly</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1729</fpage><lpage>1735</lpage><pub-id pub-id-type="doi">10.1038/nn.3248</pub-id><pub-id pub-id-type="pmid">23103963</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in Python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name><name><surname>Palomares</surname> <given-names>M</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Crowding is unlike ordinary masking: distinguishing feature integration from detection</article-title><source>Journal of Vision</source><volume>4</volume><fpage>1136</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1167/4.12.12</pub-id><pub-id pub-id-type="pmid">15669917</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philiastides</surname> <given-names>MG</given-names></name><name><surname>Sajda</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>EEG-informed fMRI reveals spatiotemporal characteristics of perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>13082</fpage><lpage>13091</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3540-07.2007</pub-id><pub-id pub-id-type="pmid">18045902</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajaei</surname> <given-names>K</given-names></name><name><surname>Mohsenzadeh</surname> <given-names>Y</given-names></name><name><surname>Ebrahimpour</surname> <given-names>R</given-names></name><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Beyond core object recognition: recurrent processes account for object recognition under occlusion</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007001</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007001</pub-id><pub-id pub-id-type="pmid">31091234</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A comparison of sequential sampling models for two-choice reaction time</article-title><source>Psychological Review</source><volume>111</volume><fpage>333</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.2.333</pub-id><pub-id pub-id-type="pmid">15065913</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarafyazd</surname> <given-names>M</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical reasoning by neural circuits in the frontal cortex</article-title><source>Science</source><volume>364</volume><elocation-id>eaav8911</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav8911</pub-id><pub-id pub-id-type="pmid">31097640</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schrimpf</surname> <given-names>M</given-names></name><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>Rajalingham</surname> <given-names>R</given-names></name><name><surname>Ziemba</surname> <given-names>C</given-names></name><name><surname>Issa</surname> <given-names>EB</given-names></name><name><surname>Kar</surname> <given-names>K</given-names></name><name><surname>Bashivan</surname> <given-names>P</given-names></name><name><surname>Prescott-Roy</surname> <given-names>J</given-names></name><name><surname>Schmidt</surname> <given-names>K</given-names></name><name><surname>Nayebi</surname> <given-names>A</given-names></name><name><surname>Bear</surname> <given-names>D</given-names></name><name><surname>Yamins</surname> <given-names>DLK</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using brain-score to evaluate and build neural networks for brain-like object recognition</article-title><conf-name>Cosyne 19</conf-name><conf-loc>Lisbon, Portugal</conf-loc></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sergent</surname> <given-names>C</given-names></name><name><surname>Baillet</surname> <given-names>S</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Timing of the brain events underlying access to consciousness during the attentional blink</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1391</fpage><lpage>1400</lpage><pub-id pub-id-type="doi">10.1038/nn1549</pub-id><pub-id pub-id-type="pmid">16158062</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shum</surname> <given-names>J</given-names></name><name><surname>Hermes</surname> <given-names>D</given-names></name><name><surname>Foster</surname> <given-names>BL</given-names></name><name><surname>Dastjerdi</surname> <given-names>M</given-names></name><name><surname>Rangarajan</surname> <given-names>V</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Miller</surname> <given-names>KJ</given-names></name><name><surname>Parvizi</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A brain area for visual numerals</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>6709</fpage><lpage>6715</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4558-12.2013</pub-id><pub-id pub-id-type="pmid">23595729</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spoerer</surname> <given-names>CJ</given-names></name><name><surname>McClure</surname> <given-names>P</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Recurrent convolutional neural networks: a better model of biological object recognition</article-title><source>Frontiers in Psychology</source><volume>8</volume><elocation-id>1551</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.01551</pub-id><pub-id pub-id-type="pmid">28955272</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Spoerer</surname> <given-names>CJ</given-names></name><name><surname>Kietzmann</surname> <given-names>TC</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/677237</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strasburger</surname> <given-names>H</given-names></name><name><surname>Rentschler</surname> <given-names>I</given-names></name><name><surname>Jüttner</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Peripheral vision and pattern recognition: a review</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/11.5.13</pub-id><pub-id pub-id-type="pmid">22207654</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname> <given-names>F</given-names></name><name><surname>Nakayama</surname> <given-names>K</given-names></name><name><surname>Vaughan</surname> <given-names>JT</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Binocular rivalry and visual awareness in human extrastriate cortex</article-title><source>Neuron</source><volume>21</volume><fpage>753</fpage><lpage>759</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80592-9</pub-id><pub-id pub-id-type="pmid">9808462</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vugt</surname> <given-names>B</given-names></name><name><surname>Dagnino</surname> <given-names>B</given-names></name><name><surname>Vartak</surname> <given-names>D</given-names></name><name><surname>Safaai</surname> <given-names>H</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The threshold for conscious report: signal loss and response Bias in visual and frontal cortex</article-title><source>Science</source><volume>360</volume><fpage>537</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1126/science.aar7186</pub-id><pub-id pub-id-type="pmid">29567809</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Brewer</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.012</pub-id><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>de Gardelle</surname> <given-names>V</given-names></name><name><surname>Scholl</surname> <given-names>J</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rhythmic fluctuations in evidence accumulation during decision making in the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>847</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.015</pub-id><pub-id pub-id-type="pmid">23177968</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Seibert</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56603.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Serre</surname><given-names>Thomas</given-names></name><role>Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Recurrent processes support a cascade of hierarchical decisions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by Thomas Serre as Reviewing Editor and Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>The authors use a combination of MEG, structural MRI, and computational modeling to measure how the visual cortex accumulates information for discriminating between objects. Using a digit/letter warping dataset, the authors identify difficult exemplars, devise clever analyses to show when representations become categorical, and combine temporal decoding analyses with modeling to describe dynamics and infer computations. This paper has a significant number of results, elegantly presented in beautiful figures. While some (if not most) of the conclusions derived from the work may have been reached independently by prior studies, one major strength of the present manuscript is to examine all these questions within a single dataset. Further qualities are the use of an elegant experimental design, thorough decoding analysis methods, and adequate use of modeling to help disentangle alternative explanations.</p><p>However, as detailed below, the reviewers have identified a number of weaknesses and are requesting that the authors comment on these critiques. One of the main issues raised by the reviewers has to do with some of the effect sizes and underlying statistical tests.</p><p>Essential revisions:</p><p>Statistics</p><p>The reviewers struggled a bit with the effect sizes reported. The authors normalize their scores (Figure 2C, Figure 4), which makes the effects look strikingly similar. But the truth is that some of the effect sizes are so small (AUCs between 0.5-0.6) that it can be hard to accept some of the findings. The reviewers would like to ask the authors to think of additional analyses that they could run that would ease these concerns.</p><p>While the maximum values of the curves in this figure are very different in range, the variations in baselines of blue, green, and orange curves do not show the scaling. Please provide figures without the scaling.</p><p>Is the trial uncertainty decoding time course significant? The effect size is very small compared to other features and it is very distributed over the brain which makes us wonder if this feature is actually readable from the brain activity.</p><p>There are no statistical tests reported in Figure 1G. Please mark significant decoding scores over time by drawing a contour line around the significant clusters. Please describe the statistical tests in Figure 2A and B as the multiple comparison corrections are unclear.</p><p>Figure 2A is thresholded based on t-values that exceed an uncorrected p &lt;.1. The reviewers are hoping this is a typo.</p><p>For the curves in Figure 2C, the authors do not indicate the time points when the scores are above chance. For example, we do not know if the blue curve with a max of 0.08 is even significant.</p><p>In addition to multiple comparison corrections across time, the authors should correct for multiple comparisons across five features.</p><p>The authors have not reported the thresholds they use for cluster definition and cluster size corrections. Please comment. This is especially important because it is not clear if the authors have also corrected for 5 multiple comparisons across the five features.</p><p>Subsection “Hierarchical recurrence implements a series of all-or-none decisions”: Between 400 and 810 ms, the predictions of 'perceptual category' decoders were better accounted for by sigmoidal (r=0.77 +/-0.03, p&lt;0.001) than by linear trends (r=0.77 +/-0.03, p&lt;0.001)? Please comment.</p><p>Subsection “Hierarchical recurrence induces an accumulation of delays”: the authors test the correlation of peak latency of averaged temporal decodings when averaged over training times. Please do this analysis with the temporal decoding time courses in Figure 2C. Because the main temporal dynamics occur along the diagonal of the TG decoding matrix.</p><p>Modeling</p><p>Another issue is with the modeling simulations described in the subsection “Statistics”, which disambiguates between the hypotheses in Figure 3. The reviewers' (maybe incorrect) interpretation was that the authors tested whether or not these stimuli are being processed via hierarchical recurrent computations or not. The reviewers thought this was a strawman argument, as there is no reason to suspect the converse (non-hierarchical/non-recurrent). This modeling work thus only added to their overall feeling that the contributions of the present manuscript were actually quite limited.</p><p>Interpretation</p><p>We would suggest adding clear statements in the Abstract and in the Discussion cautioning that these exact results may be limited to this specific task (difficult digit vs. letter classification), and could differ for other tasks (e.g. simple detection task, natural scene or object categorization).</p><p>Contributions</p><p>There must be a discussion of (Freedman et al., 2002). Those authors parametrically warped dog/cat stimuli to show that a region of the prefrontal cortex (PFC) reflected stimulus discriminability. This is of course closely related to the present work, where the authors use digit/letter stimuli to accomplish the same thing and focus on the visual cortex rather than PFC. The reviewers request some clarification about the contributions of the present study in light of this work and especially discuss the possibility that most of the presented results could be reflecting common input from PFC as suggested by Linsley and MacEvoy, 2014.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your article &quot;Recurrent processes support a cascade of hierarchical decisions&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>The reviewers made a few additional comments.</p><p>1) The most important one deals with the lack of significance in your mass-univariate analyses. We suggest you describe the analyses in the main text and explain that it did not reach significance. Because the results make sense, the reviewers suggest to keep them but to move it to the SI as they should be taken with a grain of salt (and state that).</p><p>2) Related to comment 2 on statistics, the figures in Figure 2—figure supplements 1 and 2 again are scaled; because the y-axis of all plots are scaled to the maximum of each plot.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56603.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Statistics</p><p>The reviewers struggled a bit with the effect sizes reported. The authors normalize their scores (Figure 2C, Figure 4), which makes the effects look strikingly similar. But the truth is that some of the effect sizes are so small (AUCs between 0.5-0.6) that it can be hard to accept some of the findings… The reviewers would like to ask the authors to think of additional analyses that they could run that would ease these concerns.</p></disp-quote><p>Thank you for highlighting this issue. It is true that some of the effect sizes are small. However, the vast majority of these effects are highly reliable and consistent across subjects. In addition, the motivation behind the normalization of the decoding scores is to highlight their temporal differences. We believe that the successive development of the five representations is a more valuable information than the fact that the visual position of the stimulus is much easier to decode than its letter/digit category.</p><p>To strengthen confidence in the robustness of our results we have now added:</p><p>i) Several supplementary figures, including non-scaled decoding time-courses (Figure 2—figure supplements 1 and 2), and a video showing the effects of each individual subject (violin plots) across time (Figure 2—animation 1).</p><p>ii) A number of provisos in the text itself highlighting the small effect sizes where appropriate.</p><p>We believe that these additional results make it clear that although the decoding scores obtained within each subject may be small in effect size, they are sufficiently consistent across subjects to support our conclusions.</p><disp-quote content-type="editor-comment"><p>While the maximum values of the curves in this figure are very different in range (.), the variations in baselines of blue, green, and orange curves do not show the scaling. Please provide figures without the scaling.</p></disp-quote><p>We agree with this remark. We have added stimulus-locked and response-locked non-scaled decoding time-courses (Figure 2—figure supplements 1 and 2).</p><disp-quote content-type="editor-comment"><p>Is the trial uncertainty decoding time course significant? The effect size is very small compared to other features and it is very distributed over the brain which makes us wonder if this feature is actually readable from the brain activity.</p></disp-quote><p>Yes, the uncertainty decoding is significant (Average R = 0.12; SEM = 0.024; p &lt; 0.01) (subsection “Neural representations are functionally organized over time and space”).</p><p>Note that because this is a continuous variable, we used a ridge regression to decode it, and a Spearman correlation to evaluate the performance of this decoder. This analysis is thus in a different metric scale than the categorical variables, which are decoded with a Logistic Regression and summarized with an AUC.</p><p>We have clarified this issue in the text.</p><disp-quote content-type="editor-comment"><p>There are no statistical tests reported in Figure 1G. Please mark significant decoding scores over time by drawing a contour line around the significant clusters. Please describe the statistical tests in Figure 2A and B as the multiple comparison corrections are unclear.</p></disp-quote><p>We apologize for this omission. We have now added analysis details regarding the univariate spatio-temporal cluster test, and the multivariate spatial cluster test in the text. In addition, we now supply supplementary figures showing the masks of significant decoding accuracy for both tests (Figure 2—figure supplements 4-9).</p><p>For the multivariate spatial cluster test, decoding performance is only displayed for sources that are within a significant cluster (p &lt;.05). We have made this clear in the figure legend.</p><p>Overall, these additional analyses confirm and strengthen our original results.</p><disp-quote content-type="editor-comment"><p>Figure 2A is thresholded based on t-values that exceed an uncorrected p &lt;.1. The reviewers are hoping this is a typo.</p></disp-quote><p>This is not a typo, but an explicit choice, which we insufficiently explained in our original manuscript.</p><p>For the mass-univariate analyses in source-space the effect of letter/digit decision did not reach statistical significance after control for multiple comparisons (p = 0.21). The spatio-temporal test was applied across the whole brain (~5,000 sources) and across the entire epoch (0:1500 ms), making the procedure very conservative. Because the letter/digit contrast is highly significant when using temporal (p &lt;.001) and spatial (p &lt;.001) decoders, we reasoned it would nonetheless be informative to visualize where the corresponding univariate peak activity would be. We thus adapted the plotting threshold to display the location of the strongest univariate effects. These results point to the Visual Word Form and Number Form Area (Dehaene and Cohen, 2011), as expected.</p><p>Overall, these results illustrate that multivariate decoding analyses can be much more sensitive to subtle effects, which would have otherwise been missed with standard mass-univariate analyses, should we not know a priori where to look for them. Nonetheless, these results come at the price of a diminished spatial or temporal specificity.</p><p>We have clarified this in the text and the figure legend.</p><disp-quote content-type="editor-comment"><p>For the curves in Figure 2C, the authors do not indicate the time points when the scores are above chance. For example, we do not know if the blue curve with a max of 0.08 is even significant.</p></disp-quote><p>Thank you for the suggestion. We have now added indicators of significance for the temporal cluster test in Figure 2C.</p><disp-quote content-type="editor-comment"><p>In addition to multiple comparison corrections across time, the authors should correct for multiple comparisons across five features.</p></disp-quote><p>As described in the Materials and methods, we use a temporal and/or spatial permutation cluster test which allows us to avoid the issue of corrections for multiple comparisons across time and space.</p><p>We have not corrected our results for the five features of interest in the main text. However, we have added feature-corrected results to the supplementary materials, broken down into temporal decoding, spatial decoding and mass univariate analyses. Note that this additional correction does not influence the interpretation of any of our results.</p><disp-quote content-type="editor-comment"><p>The authors have not reported the thresholds they use for cluster definition and cluster size corrections. Please comment. This is especially important because it is not clear if the authors have also corrected for 5 multiple comparisons across the five features.</p></disp-quote><p>We apologize for this lack of precision. For all permutation cluster tests we use the default parameters as provided in the Python module MNE-Python version 0.17.1. Clusters are formed with an initial p &lt;.05 threshold. No cluster size correction is applied. We have added this information to the Materials and methods section of the manuscript.</p><disp-quote content-type="editor-comment"><p>Subsection “Hierarchical recurrence implements a series of all-or-none decisions”: Between 400 and 810 ms, the predictions of 'perceptual category' decoders were better accounted for by sigmoidal (r=0.77 +/-0.03, p&lt;0.001) than by linear trends (r=0.77 +/-0.03, p&lt;0.001)? Please comment.</p></disp-quote><p>Thank you for pointing out this mistake. The linear trend was r=0.7 +/- 0.03, p&lt;0.001, which we have now added to the manuscript.</p><disp-quote content-type="editor-comment"><p>Subsection “Hierarchical recurrence induces an accumulation of delays”: the authors test the correlation of peak latency of averaged temporal decodings when averaged over training times. Please do this analysis with the temporal decoding time courses in Figure 2C. Because the main temporal dynamics occur along the diagonal of the TG decoding matrix.</p></disp-quote><p>The delay analysis is not based on the diagonal decoding peak latency. We operationalize delays in processing as shifts relative to the diagonal plane. This involves first aligning the test-time axis relative to the diagonal, and then averaging over train time. To aid interpretation we have added a schematic of the method (Figure 5—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>Modeling</p><p>Another issue is with the modeling simulations described in the subsection “Statistics”, which disambiguates between the hypotheses in Figure 3. The reviewers' (maybe incorrect) interpretation was that the authors tested whether or not these stimuli are being processed via hierarchical recurrent computations or not. The reviewers thought this was a strawman argument, as there is no reason to suspect the converse (non-hierarchical/non-recurrent). This modeling work thus only added to their overall feeling that the contributions of the present manuscript were actually quite limited.</p></disp-quote><p>All but one model included both hierarchical <italic>and</italic> recurrent processes. Consequently, the main goal of our modelling was not to test <italic>whether</italic> hierarchy and recurrency exists, but rather <italic>how</italic> they can be characterized and <italic>what</italic> underlying neural architecture their spatio-temporal dynamics imply. To clarify this issue, we added a paragraph to introduce the model comparisons and amended the discussion.</p><disp-quote content-type="editor-comment"><p>Interpretation</p><p>We would suggest adding clear statements in the Abstract and in the Discussion cautioning that these exact results may be limited to this specific task (difficult digit vs. letter classification), and could differ for other tasks (e.g. simple detection task, natural scene or object categorization).</p></disp-quote><p>This is a fair remark. While it applies to the vast majority of perceptual decision-making studies, we amended the Abstract and Discussion to highlight the fact that we specifically focus on the restricted case of reading individual characters.</p><disp-quote content-type="editor-comment"><p>Contributions</p><p>There must be a discussion of (Freedman et al., 2002). Those authors parametrically warped dog/cat stimuli to show that a region of the prefrontal cortex (PFC) reflected stimulus discriminability. This is of course closely related to the present work, where the authors use digit/letter stimuli to accomplish the same thing and focus on the visual cortex rather than PFC. The reviewers request some clarification about the contributions of the present study in light of this work and especially discuss the possibility that most of the presented results could be reflecting common input from PFC as suggested by Linsley and MacEvoy, 2014.</p></disp-quote><p>We agree with the relevance of the study from Freedman and colleagues (Freedman et al., 2002), in which they recorded PFC neurons in monkeys performing a cats/dogs categorization task. Our study supplements this work in that we</p><p>1) estimate the neural responses of a much larger set of brain areas</p><p>2) parametrically manipulate 5 levels of representations</p><p>3) formalize the different recurrent architectures that could have accounted for our decoding scores</p><p>4) record from human subjects</p><p>We have now added a paragraph in the Discussion relating our results to those of Freedman et al., 2002, and made explicit the contribution of the current work. We also mention Linsley and MacEvoy’s paper as one of the papers investigating perceptual decision using a different set of stimuli.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The reviewers made a few additional comments.</p><p>1) The most important one deals with the lack of significance in your mass-univariate analyses. We suggest you describe the analyses in the main text and explain that it did not reach significance. Because the results make sense, the reviewers suggest to keep them but to move it to the SI as they should be taken with a grain of salt (and state that).</p></disp-quote><p>While we appreciate this concern given the weakness of some of the effects, we believe that it is important to keep the univariate figure for a number of reasons.</p><p>First, all but one of the analyses are significant. Removing this figure because <italic>one</italic> of the effects does not reach statistical significance is not legitimate.</p><p>Second, we explicitly warn the reader both in the figure legend and in the main text that one of the univariate effects (letter/digit category) does not reach statistical significance after correction for multiple comparisons:</p><p>“Modelling neural activity as a function of our orthogonal stimulus properties yielded non-significant results for the decision variable of interest. […] The direct comparison between the sensitivity of the mass-univariate versus multivariate approaches are shown in Figure 2.”</p><p>Third, keeping the univariate analyses allows us to highlight the robustness and complementarity of the multivariate analyses. Specifically, MVPA over time clearly shows that letter/digit category can be decoded from ~150 ms after word onset, and MVPA over space clearly shows that this representation peaks around the visual word form area. These significant effects are compatible with the trend that we observed with univariate analyses.</p><p>Fourth, the non-significant univariate trends actually correspond to the region of interest that we expected: i.e. the visual word form area (Dehaene and Cohen, 2011).</p><p>Together, these elements make us believe that the univariate figures should be present in the main text. We have, however, added the non-thresholded maps of p-values in supplementary materials for completeness purposes.</p><disp-quote content-type="editor-comment"><p>2) Related to comment 2 on statistics, the figures Figure 2—figure supplements 1 and 2 again are scaled; because the y-axis of all plots are scaled to the maximum of each plot.</p></disp-quote><p>We have now added a non-scaled version of Figure 2C in the supplementary materials, time locked to stimulus onset and response onset.</p></body></sub-article></article>