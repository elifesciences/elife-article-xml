<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93636</article-id><article-id pub-id-type="doi">10.7554/eLife.93636</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93636.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Space as a scaffold for rotational generalisation of abstract concepts</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-176825"><name><surname>Pesnot Lerousseau</surname><given-names>Jacques</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3799-0602</contrib-id><email>jacques.pesnot@hotmail.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-16084"><name><surname>Summerfield</surname><given-names>Christopher</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Experimental Psychology, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>03</day><month>04</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP93636</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-10-30"><day>30</day><month>10</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-03-27"><day>27</day><month>03</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/zt6e4"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-03"><day>03</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93636.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-21"><day>21</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93636.2"/></event></pub-history><permissions><copyright-statement>© 2024, Pesnot Lerousseau and Summerfield</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Pesnot Lerousseau and Summerfield</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93636-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-93636-figures-v1.pdf"/><abstract><p>Learning invariances allows us to generalise. In the visual modality, invariant representations allow us to recognise objects despite translations or rotations in physical space. However, how we learn the invariances that allow us to generalise abstract patterns of sensory data (‘concepts’) is a longstanding puzzle. Here, we study how humans generalise relational patterns in stimulation sequences that are defined by either transitions on a nonspatial two-dimensional feature manifold, or by transitions in physical space. We measure rotational generalisation, i.e., the ability to recognise concepts even when their corresponding transition vectors are rotated. We find that humans naturally generalise to rotated exemplars when stimuli are defined in physical space, but not when they are defined as positions on a nonspatial feature manifold. However, if participants are first pre-trained to map auditory or visual features to spatial locations, then rotational generalisation becomes possible even in nonspatial domains. These results imply that space acts as a scaffold for learning more abstract conceptual invariances.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>learning</kwd><kwd>invariance</kwd><kwd>space</kwd><kwd>scaffold</kwd><kwd>abstraction</kwd><kwd>concepts</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100019671</institution-id><institution>Fondation Pour l'Audition</institution></institution-wrap></funding-source><award-id>FPA RD-2021-2</award-id><principal-award-recipient><name><surname>Pesnot Lerousseau</surname><given-names>Jacques</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>725937 - CQR01290.CQ001</award-id><principal-award-recipient><name><surname>Summerfield</surname><given-names>Christopher</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Spatial mapping serves as a cognitive scaffold for acquiring abstract conceptual invariances across sensory domains, shedding light on the mechanisms of human learning.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To recognise objects and events in the natural world, humans form mental representations that are invariant to transformation. The existence of invariant representations allows entities to be recognised and categorised despite changes in their surface properties, which is called ‘generalisation’. The formation of invariances has been most extensively studied in the case of visual object recognition. For example, we have no trouble recognising a teapot that is moved to a new location (translated), tipped on its side (rotated) or viewed from afar (rescaled). How we do so has provoked diverse theories based on assembly from geometric primitives (<xref ref-type="bibr" rid="bib5">Biederman, 1987</xref>; <xref ref-type="bibr" rid="bib19">Marr, 1982</xref>), associative learning (<xref ref-type="bibr" rid="bib23">Rock and DiVita, 1987</xref>; <xref ref-type="bibr" rid="bib30">Wallis and Bülthoff, 1999</xref>), and function approximation in deep networks (<xref ref-type="bibr" rid="bib17">Lindsay, 2021</xref>).</p><p>A core problem in cognitive science, however, is how we form invariances over entities that are defined by more abstract relational properties. Here, we use the term ‘concepts’ to refer to objects or events that are defined by shared relations among features that may unfold in space, or time, or both, in any modality (<xref ref-type="bibr" rid="bib27">Tenenbaum et al., 2011</xref>). For example, the concept of a ‘tree’ implies an entity whose structure is defined by a nested hierarchy, whether this is a physical object whose parts are arranged in space (such as an oak tree in a forest) or a more abstract data structure (such as a family tree or taxonomic tree). The concept of a ‘ring’ implies an entity whose features are arranged cyclically, whether a physical ring (worn on the finger), the (circular) temporal pattern of tones in a peal of bells, or the periodicity in the passage of the seasons (<xref ref-type="bibr" rid="bib16">Kemp and Tenenbaum, 2008</xref><ext-link ext-link-type="uri" xlink:href="https://sciwheel.com/work/citation?ids=282930&amp;pre=&amp;suf=&amp;sa=0">)</ext-link>. Despite great changes in the surface properties of oak trees, family trees, and taxonomic trees, humans perceive them as different instances of a more abstract concept defined by the same relational structure. The human ability to readily form invariances over abstract concepts remains a puzzle for both cognitive and neural scientists hoping to understand neural computations, and a challenge for AI researchers wishing to build intelligent agents.</p><p>One prominent theory argues that we learn invariant concepts because of the way the brain represents physical space (<xref ref-type="bibr" rid="bib3">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bellmund et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Summerfield et al., 2020</xref>; <xref ref-type="bibr" rid="bib11">Gärdenfors, 2000</xref>; <xref ref-type="bibr" rid="bib28">Tversky, 2001</xref>). This argument states that neurons coding for positions in either egocentric (viewer-centred) or allocentric (world-centred) space can be recycled to represent locations in more abstract spaces, defined by continuous variation in features (e.g. red to blue, quiet to loud). This theory is backed up by proof-of-concept computational simulations (<xref ref-type="bibr" rid="bib31">Whittington et al., 2020</xref>), and by findings that brain regions thought to be critical for spatial cognition in mammals (such as the hippocampal-entorhinal complex and parietal cortex) exhibit neural codes that are invariant to relational transformations of nonspatial stimuli (<xref ref-type="bibr" rid="bib21">Park et al., 2021</xref>; <xref ref-type="bibr" rid="bib20">Park et al., 2020</xref>; <xref ref-type="bibr" rid="bib18">Mack et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Viganò and Piazza, 2020</xref>). However, whilst promising, this theory lacks direct empirical evidence. Here, we set out to provide a strong test of the idea that learning about physical space scaffolds conceptual generalisation. Our focus is on the ability to generalise knowledge about the relations among items in a sequence as they are translated or rotated through both spatial and nonspatial domains.</p><p>In the four studies described here, participants made category judgments about a sequence of four successive stimuli in either auditory, visual, or spatial modalities. In auditory and visual modalities, the stimulus was drawn from a two-dimensional (2D) feature manifold (e.g. a bivariate ‘space’ defined by colour and shape in the visual modality or pitch and timbre in the auditory modality). In the spatial modality, each stimulus was a position in physical space (e.g. an <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi></mml:math></inline-formula> coordinates). Concepts were defined by a common pattern of transitions through either feature space or physical space. Our research question concerned the conditions under which concepts could be recognised, even if their corresponding transition vectors had been translated or rotated. We studied generalisation of transition vectors both within the same feature space and to new feature spaces in the same modality.</p><p>Our studies measure the tendency to generalise by both translation and rotation. Conceptual translation occurs when feature values are shifted in either dimension, but with no change in their relational pattern. There is already good evidence that nonspatial concepts are represented in a translation-invariant format. For example, in the auditory domain, we can recognise ‘auditory objects’ that are translated in feature space (e.g. pitch and timbre). This occurs when we understand the same sentence from different speakers, or identify the same melody played with different musical instruments (<xref ref-type="bibr" rid="bib32">Winkler et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Griffiths and Warren, 2004</xref>). However, much less is known about the learning of rotational invariances for abstract concepts. In physical space, we readily learn rotation-invariant object representations (allowing us to recognise an upside-down teapot), and the computational mechanisms by which we do so have been a major fulcrum of debate in the vision sciences (<xref ref-type="bibr" rid="bib23">Rock and DiVita, 1987</xref>; <xref ref-type="bibr" rid="bib30">Wallis and Bülthoff, 1999</xref>). But whether participants can learn rotationally invariant concepts in nonspatial domains, i.e., those that are defined by sequences of visual and auditory features (rather than by locations in physical space, defined in Cartesian or polar coordinates) is not known. In the current study, we first test this, and find that naively, they cannot. Next, turning to our main hypothesis, we then ask if first teaching participants to map nonspatial features to spatial locations (providing a spatial scaffold) allows the learning of rotational invariances, even in nonspatial modalities. We find that it does. This shows that a form of generalisation that is not usually possible for humans becomes possible when their understanding of the concept is ‘scaffolded’ by first learning a corresponding spatial representation. This thus supports the theory that abstract concept learning is linked to our understanding of physical space.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>On each trial, participants were presented with a sequence of four auditory, visual, or spatial stimuli (a quadruplet) drawn from 1 of 16 points on a continuously varying 2D (4×4) feature manifold. In the visual (auditory) modality, this manifold was respectively defined by two orthogonal and continuously varying visual (auditory) features. In the spatial domain, the 2D feature manifold was defined by positions in physical space, in either Cartesian or polar coordinates (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplements 1</xref>–<xref ref-type="fig" rid="fig1s2">2</xref>). Each quadruplet was constructed by first sampling a random point on the 2D feature manifold, and then iteratively choosing three further adjacent feature locations to make a sequence of four stimuli. In each experiment, there were three categories (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Each category was initially defined by a canonical set of transition vectors, which specified the three successive steps on the feature manifold (defining the positions from which stimuli in the quadruplet were sampled). Thus, for example, one set of transition vectors might be defined by compass directions {NE, W, SE}. This would mean that after an initial stimulus was sampled, the second stimulus in the sequence would be the one NE in feature space, and the third W of that, and the fourth SE of that. We define <italic>rotational generalisation</italic> as the ability to recognise regularities in the sequence transition vectors that are independent of both translation and rotation. Thus, just as an upside-down teapot can still be recognised by the relative spatial relations among its handle, body, and spout, in Exp. 1 we asked whether concepts can be recognised when their associated transition vectors are rotated (e.g. vector sequence {NE, W, SE} on the feature manifold becomes {NW, S, NE} after 90° rotation). Note that in our study, quadruplets are also randomly translated on the manifold by virtue of the variable initial feature selection between trials. We thus make the basic assumption that rotational generalisation also involves translation invariance.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Paradigm.</title><p>(<bold>A</bold>) Objects can be perceived as similar despite changes in shape, orientation, and features. (<bold>B</bold>) Similarly, can sequences of features be perceived as similar despite great changes in the features that composed them? (<bold>C</bold>) In our experiments, participants had to learn the category associated with quadruplets composed of four stimuli drawn from a 2D feature manifold. To do so, they could use one of two major strategies: tracking changes in a single dimension (<italic>1D</italic>) or two dimensions (<italic>2D</italic>). (<bold>D</bold>) Illustration of the feature manifold and transition vectors in the visual modality. Each transition vector defined transitions between cardinally or diagonally adjacent features on a 2D manifold (here, given by colour and spikiness for training and near transfer in the left and middle panels, and by transparency and squareness in the far transfer condition). The transition vectors for each category are shown in their canonical (0°) rotation as blue, orange, and green arrows superimposed on each feature manifold. The vectors are shown rotated by 90° in the near and far transfer conditions. Next to each feature manifold is a matrix showing the expected mapping (filled squares) from feature vectors to categories for a participant using the <italic>1D</italic> strategies (top and middle) and <italic>2D</italic> strategy (bottom). Note that the <italic>2D</italic> strategy always leads to effective generalisation (rotated exemplars are mapped on their corresponding categories) even in transfer, as indicated by filled squares on the matrix diagonal, whereas a <italic>1D</italic> strategy leads to a different pattern. We use the symbols ∅, ↶, and ★ to denote canonical (0°), 90° rotation, and far transfer conditions, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Examples of 2D feature manifolds used in the study.</title><p>For each modality, four manifolds are displayed. (<bold>A</bold>, <bold>B</bold>) In the auditory modality, the stimuli could vary along four features: fundamental frequency F0 (110, 220, 330, or 440 Hz), frequency modulation (1, 2, 3, or 4 Hz), amplitude modulation (1, 2, 3, or 4 Hz), and number of high harmonics (1, 3, 7, or 10). A spectrogram of each sound is displayed at its location on the manifold. Two example sets of original and far transfer manifolds corresponding to the manifolds of two different participants are shown in <bold>A</bold> and <bold>B</bold>. (<bold>C</bold>, <bold>D</bold>) In the visual modality, the stimuli could vary along four features: colour (<italic>viridis</italic> perceptually uniform colormap, 0, 0.33, 0.66, or 1), transparency level (0.2, 0.46, 0.73, or 1), squareness (squareness parameter of the Fernandez-Guasti squircle, 0.01, 0.8, 0.98, or 1), and spikiness (amplitude of the cosine modulation relative to the squircle radius, 0, 0.06, 0.13, or 0.2). Two example sets of original and far transfer manifolds corresponding to the manifolds of two different participants are shown in <bold>C</bold> and <bold>D</bold>. (<bold>E</bold>, <bold>F</bold>) In the spatial modality, the stimuli could vary along four features: horizontal position (1, 2, 3, or 4), vertical position (1, 2, 3, or 4), radius (1, 2, 3, or 4), and angle (0, 90°, 180°, or 270°). Two example sets of original and far transfer manifolds corresponding to the manifolds of two different participants are shown in <bold>E</bold> and <bold>F</bold>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>The eight transformations of the canonical quadruplets present in the study.</title><p>There are three categories of quadruplets, shown in three different colours. Eight transformations have been used: canonical (∅), 90° rotation (↶), 180° rotation (↻), 270° rotation (↷), far transfer canonical (★), far transfer 90° rotation (★ + ↶), far transfer 180° rotation (★ + ↻), and far transfer 270° rotation (★ + ↷).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Example trials.</title><p>(<bold>A</bold>) Example trial of the main quadruplet categorisation task. Participants were asked to infer the category of quadruplets, e.g., here four visual shapes that vary in colour and spikiness. Participants gave their response by clicking on one of the three buttons that appeared on screen after the presentation of the quadruplets. Participants received trialwise and blockwise feedback on training trials (see Materials and methods for details). (<bold>B</bold>) Example trial of the mapping task. Participants were asked to associate features from different modalities, e.g., here map individual visual images to their corresponding location in physical space. Participants gave their response by clicking on a location on a 4×4 grid that appeared on screen after the presentation of the stimulus. Participants received trialwise and blockwise feedback. (<bold>C</bold>) Example trial of the duration-match filler task used in Exp. 4. Participants were asked to infer the category of sequences, defined by the number of blue stars in a sequence of four stationary stars. Participants gave their response by clicking on one of the three buttons that appeared on screen after the presentation of the sequence (the buttons were different between the main task and the filler task). Participants received trialwise and blockwise feedback.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Model response matrices to the different quadruplets transformations in Experiment 1.</title><p>(<bold>A</bold>) Near transfer. The degree of fading indicates the probability of a response (white to black: 0–1). (<bold>B</bold>) Far transfer. (<bold>C</bold>) This example response matrix reads as follows: a model with this response matrix would respond ‘orange’ when presented with a ‘category 2’ (the ‘z’ shape) quadruplet. In Exp. 1, feedback was given only on canonical (0°, denoted by ∅) trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Model response matrices to the different quadruplets transformations in Experiment 2, 3, and 4.</title><p>(<bold>A</bold>) Near transfer. The degree of fading indicates the probability of a response (white to black: 0–1). (<bold>B</bold>) Far transfer. (<bold>C</bold>) This example response matrix reads as follows: a model with this response matrix would respond 50% of the time ‘orange’ and 50% of the time ‘green’ when presented with a ‘category 2’ (the ‘z’ shape) quadruplet. In Exp. 2, 3, and 4, feedback was given on canonical (0°, denoted by ∅) and 90° (↶) trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-figsupp5-v1.tif"/></fig><fig id="fig1s6" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 6.</label><caption><title>Model recovery analysis.</title><p>Synthetic data were generated using the same trials as the participants for all models. This analysis was repeated 100 times for four level of the temperature parameter (<italic>β</italic>=0.05, 0.2, 0.35, and 0.5). (<bold>A</bold>) Near transfer in Exp. 1. (<bold>B</bold>) Far transfer in Exp. 1. (<bold>C</bold>) Near transfer in Exp. 2, 3, and 4. (<bold>D</bold>) Far transfer in Exp. 2, 3, and 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig1-figsupp6-v1.tif"/></fig></fig-group><p>Our basic procedure was as follows. During training (120 trials), participants first learned to assign canonical (0° rotation) quadruplets to one of three categories using a button press, receiving fully informative feedback after each response (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Then, during test, participants performed a further 210 trials, half of which were identical to training (with feedback) while the other half were transfer trials involving categorisation of quadruplets whose feature transition vectors were rotated by 90°, 180°, or 270°. These novel quadruplets were either sampled from the same 2D feature manifold (e.g. colour and spikiness in the visual case; <italic>near transfer</italic> condition) or a new 2D feature manifold from the same modality (e.g. transparency and squareness; <italic>far transfer</italic> condition; see <xref ref-type="fig" rid="fig1">Figure 1D</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Transfer trials received no feedback, allowing us to infer what knowledge was being generalised between training and transfer. Exp. 1–3 were pre-registered at <ext-link ext-link-type="uri" xlink:href="https://osf.io/z9572/registrations">https://osf.io/z9572/registrations</ext-link>.</p><sec id="s2-1"><title>Concepts defined by spatial locations, but not auditory or visual features, are rotation-invariant</title><p>In Exp. 1, we recruited three cohorts of online participants (<italic>N</italic>=50 each, see <xref ref-type="fig" rid="fig2">Figure 2</xref>) to perform the task in the auditory, visual, and spatial modalities. These conditions differed only in how the feature manifold was defined: e.g., fundamental frequency and modulation frequency for auditory features (<xref ref-type="fig" rid="fig2">Figure 2A</xref>); e.g., spikiness and colour for visual features (<xref ref-type="fig" rid="fig2">Figure 2E</xref>); e.g., horizontal and vertical position for spatial locations (<xref ref-type="fig" rid="fig2">Figure 2I</xref>). Accuracy on training trials for each modality is shown in <xref ref-type="fig" rid="fig2">Figure 2B, F, and J</xref>. Participants learned the task well in all three conditions (but better in the spatial modality: intercept <italic>β</italic>=2.90 ± 0.19, slope associated with the auditory modality <italic>β</italic>=–1.91 ± 0.27, p&lt;0.001, slope associated with the visual modality, <italic>β</italic>=–1.57 ± 0.26, p&lt;0.001, mixed logistic regression on the probability of a correct response with participants as random effect). However, our main question was how participants would generalise learning to novel, rotated exemplars of the same concept.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experiment 1: Quadruplets composed of spatial locations, but not auditory or visual features, are associated with a <italic>2D</italic> strategy.</title><p>(<bold>A</bold>) In the auditory modality (<italic>N</italic>=50), the 2D feature manifold was defined by auditory features. Here, we illustrate with a schematic in which the feature manifold is defined by fundamental frequency and frequency modulation. (<bold>B</bold>) Accuracy on training trials involving the canonical transition vectors (0° rotation, denoted by the symbol ∅). The bold curve and dots represent the group average; lighter curves are individual participants. During training, random models (<italic>R</italic>) are at chance (33% accuracy; lower dashed line), while idealised <italic>1D</italic> and <italic>2D</italic> models are at ceiling (100% accuracy; upper dashed lines). (<bold>C</bold>) Model fits to the near transfer responses. The bar plot shows model frequencies in the population and black dots are Bayesian point estimates of the model frequency. The matrices show cross-validated model predictions in the same format at <xref ref-type="fig" rid="fig1">Figure 1D</xref>, except with the degree of fading (light to dark colour) signalling average participant responses in each case. The matrices read as follows: the top row depicts the average behaviour of participants who were best fit by the <italic>1Di<sub>u</sub></italic> model (using a held-out set of responses). The <italic>1Di<sub>u</sub></italic> model predictions are shown in black for three rotations of the quadruplets (90°, 180°, 270° denoted by the symbols ↶, ↺, and ↷ respectively). The average response matrices of the participants using held-out responses are shown in red. The scatter plot below shows individual likelihoods for the <italic>1D</italic> and <italic>2D</italic> models (normalised by the <italic>R</italic> model). Each dot is an individual participant, coded by whether they are best fit by either <italic>1D</italic> or <italic>2D</italic> models (colour) or the <italic>R</italic> model (grey). Dashed lines distinguish zones of relative likelihood where participants are best fit by <italic>R</italic> (bottom left square), <italic>1D</italic> (rightmost quadrilateral), or <italic>2D</italic> (upper quadrilateral) models. (<bold>D</bold>) Model fits to the far transfer responses, using the same conventions as C. Note that, at transfer, more 1D models are possible because of the differing ways that participants could map between the <italic>i</italic> and <italic>j</italic> axes and the ★<italic>i</italic> and ★<italic>j</italic> axes during training and far transfer. (<bold>E</bold>) In the visual modality (<italic>N</italic>=52), the 2D feature manifold was defined by visual attributes; here we use colour and spikiness as an illustration. (<bold>F–H</bold>) depict data from the visual modality using conventions from <bold>B–D</bold>. (<bold>I</bold>) In the spatial modality (<italic>N</italic>=51), the 2D feature manifold was defined by the spatial location of a star (here a red dot is used for illustration). (<bold>J–L</bold>) depict data from the spatial modality using conventions from <bold>B–D</bold>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig2-v1.tif"/></fig><p>To test this, we fit a family of quantitative models jointly to the training and transfer trials. To understand the logic of this modelling exercise, it is necessary to consider the alternative strategies that participants may have learned during training. Whereas rotation requires participants to represent both dimensions of the feature manifold (a rotation of 90° is only discernible in 2D), a viable alternative strategy during training is to base categorical decisions on a single feature (e.g. either spikiness or colour but not both). Each quadruplet consists of four adjacent feature locations forming a square on the feature manifold (<xref ref-type="fig" rid="fig2">Figure 2D</xref>) and thus the stimulation sequence comprises two features from each dimension. Thus, for example, if a participant attended only to spikiness, the four stimuli in a quadruplet would be represented as a feature pattern over spikiness levels (such as ABAB or ABBA, where A=more spiky, B=less spiky). During training, participants could learn to map these patterns onto categories, either in a signed fashion (e.g. ABAB maps to one category and BABA to another) or an unsigned fashion (ABAB and BABA both map to the same category). These strategies would lead to perfect performance during training, but would prevent the learning of rotational invariances. We built models that implemented these 1D strategies, which we call <italic>1D<sub>s</sub></italic> and <italic>1D<sub>u</sub></italic> respectively, and compared them to models that used both dimensions for categorisation (the <italic>2D</italic> model) or were simply responding randomly (<italic>R</italic> models). Each of these models predicts a unique pattern of generalisation (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplements 4</xref>–<xref ref-type="fig" rid="fig1s5">5</xref>) and only the <italic>2D</italic> model predicts that participants will assign rotated objects to the same category as their unrotated counterparts (rotational generalisation). Thus, the principal metrics we report in this study are the fraction of (non-random) subjects classified as <italic>1D</italic> vs. <italic>2D</italic> on transfer trials, which is a signature of whether the experimental conditions permitted the learning of rotational invariances for quadruplets. We report both fractions of participants (X/X best fit by each model) and Bayes factors (BF) reflecting the relative likelihood of <italic>1D</italic> vs. <italic>2D</italic> models between conditions.</p><p>Consistent with our first pre-registered prediction, Exp. 1 revealed a striking dissociation in rotational generalisation between modalities. For near transfer, all non-random participants in the auditory and visual modality (26/26 and 38/38) learned a <italic>1D<sub>u</sub></italic> strategy, whereas the vast majority in the spatial modality (35/41) were best fit by a <italic>2D</italic> strategy. Bayesian group model comparison confirmed that the frequency of <italic>1D</italic> vs <italic>2D</italic> models among non-random participants was similar between the auditory and visual modalities (BF=0.1, ‘negative’ evidence for a difference) but different between the auditory and spatial modalities (BF&gt;100, ‘decisive’ evidence for a difference) and the visual and spatial modalities (BF&gt;100; see Tables 6–9 for full results). This implies that the use of a 1D strategy (implying no rotational generalisation) was much more likely than a 2D strategy (implying rotational generalisation) when the manifold was defined by visual or auditory features (e.g. colour and shape or pitch and timbre), but the converse was true when the feature manifold was defined by coordinates in physical space (e.g. horizontal and vertical position).</p><p>For far transfer, the results were very similar. In the auditory modality, all non-random participants (26/26) were again best fit by a <italic>1D<sub>u</sub></italic> strategy, and in the visual modality, most (30/35) were fit by a <italic>1D<sub>u</sub></italic> strategy, 5/35 by a <italic>1D<sub>s</sub></italic> strategy, and none by a <italic>2D</italic> strategy (difference between auditory and visual, BF = 0.1). By contrast, in the spatial modality, where far transfer involved remapping from cardinal to polar coordinates or vice versa, almost all non-random participants (29/31) were again best fit by a <italic>2D</italic> strategy (both BF&gt;100 comparing with the auditory and visual modalities, ‘decisive’ evidence for a difference). Behaviour in each modality of Exp. 1 is illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>, where we display category assignments under each rotation for participants allocated to distinct model classes on the basis of held-out data. Together, these data show definitively that, when categories were characterised by temporal patterns in spatial location (e.g. where transitions in physical space were aligned with those on the feature manifold), participants learned to represent the <italic>2D</italic> structure of the concept, and generalised readily to rotated (as well as translated) exemplars. However, when concepts were defined by patterns of nonspatial auditory or visual features, participants learned mappings to each category by relying on a single feature dimension and thus failed to form rotational invariant representations.</p></sec><sec id="s2-2"><title>Spatial pre-training provides a scaffold for rotational generalisation in the auditory and visual modalities</title><p>Exp. 1 shows that rotational generalisation succeeds for spatial concepts but fails for nonspatial concepts. Next, in Exp. 2 we tested our main prediction: that space can be used as a scaffold for rotational generalisation of nonspatial concepts. We recruited three new cohorts of participants (<italic>N</italic>=50 each, see <xref ref-type="fig" rid="fig3">Figure 3</xref>) to perform a multi-phase task that unfolded over 2 successive days. On day 1, participants received 60 pre-training trials in the <italic>pre-training</italic> modality. These trials matched training trials in Exp. 1 for the corresponding modality (spatial or visual) except that they comprised both canonical (0°) and 90° rotated quadruplets, but not those rotated by 180° or 270° (we included examples of rotated quadruplets in the training set to encourage rotational generalisation, but as shown in Exp. 3, results do not depend on this choice). Subsequently, participants performed 288 trials of a multimodal association task, in which they learned the association between each of the 16 stimuli in the pre-training modality and their corresponding stimulus in a different <italic>testing</italic> modality, where the corresponding stimulus occupied an equivalent position on the 2D feature manifold (we call this the ‘mapping task’). The goal of this task was to teach participants’ correspondences between either spatial and visual, spatial and auditory, or visual and auditory feature manifolds. Then on day 2, after some refresher pre-training and mapping trials, participants performed the same task as in Exp. 1 in the testing modality, again with the exception that training trials also included 90° rotated quadruplets.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Experiment 2: Spatial pre-training triggers the use of a <italic>2D</italic> strategy for quadruplets composed of auditory or visual features.</title><p>(<bold>A</bold>) Performance on the spatial pre-training task in Exp. 2a (<italic>N</italic>=51). For this and all plots below, the bold line is the group average and lighter lines are individual participants. Dashed lines show expected average performance under the corresponding models (labelled). (<bold>B</bold>) Performance in the spatial to auditory mapping task. Chance performance is at 1/16 (dashed line). (<bold>C</bold>) Performance on training trials in the auditory modality for canonical (0°) and 90° rotated quadruplets. During training, random models (<italic>R</italic>) are at chance (33% accuracy), almost all <italic>1D</italic> models are at 50% accuracy, and <italic>1Di<sub>s</sub></italic> and <italic>2D</italic> models are at ceiling (100% accuracy). (<bold>D</bold>) Model fits to the near transfer responses, using the same conventions as <xref ref-type="fig" rid="fig2">Figure 2</xref>. Model predictions are shown in black for two rotations of the quadruplets (180° and 270° denoted by the symbols ↺ and ↷, respectively). (<bold>E</bold>) Model fits to the far transfer responses. (<bold>F–J</bold>) read as A–E for Exp. 2b (<italic>N</italic>=51), where participants performed a spatial pre-training and a spatial to visual mapping task prior to performing the visual version of the main task. (<bold>K–O</bold>) read as A–E for Exp. 2c (<italic>N</italic>=50), where participants performed a visual pre-training and a visual to auditory mapping task prior to performing the auditory version of the main task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig3-v1.tif"/></fig><p>Our pre-registered prediction for Exp. 2 was that when (Cartesian) physical space was the pre-training modality, participants would now (in contrast to Exp. 1) learn using a predominantly <italic>2D</italic> strategy in both auditory (Exp. 2a) and visual (Exp. 2b) testing modalities. In other words, by learning the association between auditory or visual features and a corresponding spatial location, concepts composed of exclusively nonspatial features could now be generalised over rotations in a way not exhibited by a single participant in Exp. 1a or Exp. 1b. By way of control, however, we predicted that when the pre-training involved (nonspatial) visual features, no such benefit would occur, and participants would fail to show rotational invariance.</p><p>This is exactly what we found, for both near and far transfer. In the near transfer condition, with spatial pre-training, 29/37 non-random participants were best fit by a <italic>2D</italic> strategy when audition was the testing modality, and 36/40 when vision was the testing modality. By contrast, however, participants who underwent visual (rather than spatial) pre-training failed to show a benefit when audition was the testing modality. In fact, most (37/50) were best fit by the random model (see below), with 6/13 non-random participants favouring a <italic>1D<sub>s</sub></italic> strategy. We once again calculated BF at the group level to assess the reliability of these results. We found that BFs exceeded 100, providing ‘decisive’ evidence that the <italic>2D</italic> model was more favoured among the groups with spatial pre-training than that without. Similarly, in the far transfer condition, spatial pre-training allowed 29/38 participants in the auditory modality and 13/16 participants in the visual modality to successfully generalise via a <italic>2D</italic> strategy. This was not the case for participants who experienced visual pre-training (again, frequency of <italic>1D</italic> vs <italic>2D</italic> models between conditions: BF&gt;100, ‘decisive’). In other words, spatial pre-training provided an effective scaffold that allowed participants to learn auditory and visual objects in a 2D representational format that permitted generalisation to novel rotated exemplars.</p><p>Participants in Exp. 2c performed poorly during training and were more likely to be fit by the random models during transfer than those who performed the same auditory task in Exp. 1a. Indeed, we computed the BF quantifying the relative likelihood of the random (<italic>R</italic> models) vs all other models (<italic>1D</italic> and <italic>2D</italic> models) and found ‘substantial’ evidence in favour of a difference between groups both in near transfer (BF = 7.4) and in far transfer (BF = 2.7). This might seem curious, because Exp. 2 participants had access to more diverse training (on both 0° and 90° quadruplets) as well as the supplementary visual pre-training. Why did Exp. 2c participants struggle with the task? In fact, this phenomenon makes sense, because training with feedback on both 0° and 90° quadruplets effectively invalidates a <italic>1D</italic> strategy, because there no longer exists a unique mapping between categories and features in either of the two feature dimensions (note that training performance in Exp. 2c plateaus close to 50%). This lack of a viable <italic>1D</italic> strategy during training obliges participants to use a <italic>2D</italic> strategy where possible. Because this is only possible with spatial pre-training, in Exp. 2c they revert to random. Whilst this explains what we observed in Exp. 2, it also allows a further prediction: if we remove the 90° rotated quadruplets from pre-training, then participants in the spatial pre-training modality should be somewhat less prone to use a <italic>2D</italic> strategy (because <italic>1D</italic> is available) whereas participants who undergo visual pre-training should show more <italic>1D</italic> behaviour at the expense of the random model. In Exp. 3, we tested and confirmed this prediction.</p><p>Exp. 3 involved three new cohorts (<italic>N</italic>=50 each, see <xref ref-type="fig" rid="fig4">Figure 4</xref>) and was identical to Exp. 2, except that now pre-training trials consisted exclusively of canonical (0°) quadruplets (although 90° quadruplets were still present when the testing modality was trained on day 2). As predicted, non-random participants who enjoyed spatial pre-training were still prone to use a <italic>2D</italic> strategy when audition was the testing modality (16/32 for near transfer and 17/31 for far transfer) as well as when vision was the testing modality (22/30 and 13/19), replicating the findings of Exp. 2. However, compared to Exp. 2, overall more participants relied on <italic>1D</italic> strategies. In the auditory modality in Exp. 3a, 16/32 were best fit by a <italic>1D</italic> model in the near transfer condition (9/32 <italic>1D<sub>u</sub></italic> and 7/32 <italic>1D<sub>s</sub></italic>) and 14/31 in the far transfer condition (10/31 <italic>1D<sub>u</sub></italic> and 4/31 <italic>1D<sub>s</sub></italic>). At the group level, the BF confirmed that participants were more likely to be fit by a <italic>1D</italic> model in Exp. 3a than Exp. 2a in the auditory modality (frequency of <italic>1D</italic> vs <italic>2D</italic> models between Exp. 2a and Exp. 3a, near transfer BF = 3.4 ‘substantial’ evidence, far transfer BF = 1.5 ‘weak’ evidence). Similarly, in the visual modality in Exp. 3b, 8/30 were best fit by a <italic>1D</italic> model in the near transfer condition (5/30 <italic>1D<sub>u</sub></italic> and 3/30 <italic>1D<sub>s</sub></italic>) and 6/19 in the far transfer condition (6/19 <italic>1D<sub>u</sub></italic>) (again, frequency of <italic>1D</italic> vs <italic>2D</italic> models between Exp. 2b and Exp. 3b, near transfer BF = 5.9 ‘substantial’ evidence, far transfer BF = 2.2 ‘weak’ evidence). By contrast, participants who underwent nonspatial (visual) pre-training did not use a <italic>2D</italic> strategy (1/30) but rather preferred <italic>1D</italic> strategies in both near transfer (13/30 <italic>1D<sub>u</sub></italic> and 16/30 <italic>1D<sub>s</sub></italic>) and far transfer conditions (10/28 <italic>1D<sub>u</sub></italic> and 17/28 <italic>1D<sub>s</sub></italic>). Comparing these results with the frequency of <italic>1D</italic> vs <italic>2D</italic> models in conditions with spatial pre-training (Exp. 3a and 3b), we found that all BFs exceeded 100, providing ‘decisive’ evidence that the <italic>2D</italic> model was more favoured among the groups with spatial pre-training than that without.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Experiment 3: Exposure to the canonical events (0°) during spatial pre-training is sufficient to trigger the use of a <italic>2D</italic> strategy.</title><p>All panels use the same conventions as <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>A–E</bold>) Contrary to Exp. 2a, in Exp. 3a (<italic>N</italic>=50), participants were exposed only to canonical events (0°) during the spatial pre-training. (<bold>F–J</bold>) Exp. 3b (<italic>N</italic>=51). (<bold>K–O</bold>) Exp. 3c (<italic>N</italic>=50).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Experiment 4: (<bold>A–E</bold>).</title><p>Contrary to Exp. 3a, in Exp. 4a (<italic>N</italic>=50), participants were exposed to an irrelevant counting task instead of the spatial pre-training. (<bold>F–J</bold>) Exp. 3b (<italic>N</italic>=52). Figure used the same conventions as <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Thus, these results show that training exclusively on canonical (0°) quadruplets facilitates a 1D strategy, which is expressed more readily than in Exp. 2; but that the 2D strategy is still more likely for participants who underwent spatial pre-training. Further, the results show that participants who did not experience spatial pre-training were still engaged in the task, but were not using the same strategy as the participants who experienced spatial pre-training (1D rather than 2D). Thus, the benefit of the spatial pre-training is not simply to increase the cognitive engagement of the participants. Rather, spatial pre-training provides a scaffold to learn rotation-invariant representation of auditory and visual concepts even when rotation is never explicitly shown during pre-training. Furthermore, participants are sensitive to the available strategies during pre-training, and use the <italic>1D</italic> strategy when possible if they have not learned to associate features with space.</p></sec><sec id="s2-3"><title>Spatial mapping performance predicts rotational generalisation for nonspatial modalities</title><p>Next, we used our data from Exp. 2 and Exp. 3 to study how performance on each phase of our task predicted rotational generalisation in the testing phase (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). For each participant, we created an index of rotational generalisation (<italic>2Dness</italic>) as the difference in log-likelihood between the best <italic>1D</italic> model and the <italic>2D</italic> model during near transfer. We found that <italic>2Dness</italic> was powerfully predicted by training accuracy (Pearson correlation between <italic>2Dness</italic> and training accuracy [<italic>r</italic><sub><italic>2Dness,TRAINING</italic></sub>] = 0.80, p&lt;0.001) in both Exp. 2 (<italic>r</italic><sub><italic>2Dness,TRAINING</italic></sub> = 0.83, p&lt;0.001) and Exp. 3 (<italic>r</italic><sub><italic>2Dness,TRAINING</italic></sub> = 0.78, p&lt;0.001). The fact that training performance is highly correlated with <italic>2Dness</italic> implies that participants who solved the training task formed representations that were generalisable in 2D; in other words, very few participants overfit to the training set. Accordingly, participants were poorly captured by an additional model (the <italic>R’</italic> model; 4/152 in Exp. 2, 4/151 in Exp. 3), that has perfect performance during training but responds randomly during transfer. Next, we asked whether accuracy during pre-training and mapping were systematically associated with <italic>2Dness</italic>, and assessed their relative importance using partial correlations. Pre-training did explain unique variance in <italic>2Dness</italic> after accounting for mapping (correlation between pre-training and <italic>2Dness</italic> after partialling out mapping [<italic>r</italic><sub><italic>2Dness,PRETRAINING - MAPPING</italic></sub>]=0.17, p&lt;0.01) and vice versa (<italic>r</italic><sub><italic>2Dness,MAPPING - PRETRAINING</italic></sub> = 0.27, p&lt;0.001). However, <italic>2Dness</italic> was better predicted by mapping than by pre-training in Exp. 2a (<italic>r</italic><sub><italic>2Dness,MAPPING - PRETRAINING</italic></sub> = 0.42, p&lt;0.005 and <italic>r</italic><sub><italic>2Dness,PRETRAINING - MAPPING</italic></sub> = 0.30, p&lt;0.05), Exp. 2b (<italic>r</italic><sub><italic>2Dness,MAPPING - PRETRAINING</italic></sub> = 0.41, p&lt;0.005 and <italic>r</italic><sub><italic>2Dness,PRETRAINING - MAPPING</italic></sub> = 0.04, p=0.81), Exp. 3a (<italic>r</italic><sub><italic>2Dness,MAPPING - PRETRAINING</italic></sub> = 0.32, p&lt;0.05 and <italic>r</italic><sub><italic>2Dness,PRETRAINING - MAPPING</italic></sub> = 0.06, p=0.70) and Exp. 3b (<italic>r</italic><sub><italic>2Dness,MAPPING - PRETRAINING</italic></sub> = 0.46, p&lt;0.001 and <italic>r</italic><sub><italic>2Dness,PRETRAINING - MAPPING</italic></sub> = 0.33, p&lt;0.05). The strong correlations between the mapping task performances and <italic>2Dness</italic> suggest that learning the association between nonspatial and spatial features is the critical step that allows rotational generalisation.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Correlation analysis.</title><p>Scatter plots of <italic>2Dness</italic> (the difference in likelihood between the best <italic>1D</italic> model and the <italic>2D</italic> model in the training modality in near transfer) and (<bold>A</bold>) pre-training accuracy, (<bold>B</bold>) mapping accuracy, and (<bold>C</bold>) training accuracy in the testing modality in Exp. 2. Dots are individual participants in Exp. 2a (red), 2b (blue), and 2c (brown). (<bold>D–F</bold>) read as <bold>A–C</bold> for Exp. 3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-fig5-v1.tif"/></fig><p>We tested and confirmed this prediction in Exp. 4 (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) which repeated Exp. 3 except that spatial pre-training was replaced with a duration-matched filler task (in which the category is defined by the number of stationary blue stars in a sequence). Without spatial pre-training, a sizeable proportion of participants still learned a <italic>2D</italic> strategy in both the auditory (9/30 in near transfer, 9/28 in far transfer) and visual (12/19 and 7/19) modality, although the majority relied on a 1D strategy (auditory modality: 4/30 <italic>1D<sub>u</sub></italic> and 17/30 <italic>1D<sub>s</sub></italic> for near transfer, 4/38 <italic>1D<sub>u</sub></italic> and 15/28 <italic>1D<sub>s</sub></italic> for far transfer; visual modality: 5/19 <italic>1D<sub>u</sub></italic> and 6/19 <italic>1D<sub>s</sub></italic> for near transfer, 5/14 <italic>1D<sub>u</sub></italic> and 2/14 <italic>1D<sub>s</sub></italic> for far transfer). In the auditory modality (Exp. 4a), this can be compared with Exp. 2c, where almost all participants were using a random strategy (frequency of <italic>R</italic> vs <italic>1D</italic>/<italic>2D</italic> models, BF = 34.0, ‘strong’ evidence), and with Exp. 3c where almost no participants were using a <italic>2D</italic> strategy (frequency of <italic>1D</italic> vs <italic>2D</italic> models, BF = 8.6, ‘substantial’ evidence). Thus, for ~20% participants, the mere exposure to the mapping was sufficient to benefit from the spatial scaffolding effect and actually seeing the quadruplets in the spatial modality was not necessary for them.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We studied the conditions under which participants learn rotation- and translation-invariant representations of abstract concepts. We found that participants can generalise conceptual knowledge to novel sequences (quadruplets) defined by rotations of stimulus feature transition vectors, but only if the features were themselves physical spatial locations (e.g. <italic>x</italic>, <italic>y</italic> position; Exp. 1) or if nonspatial attributes had previously been mapped to a physical spatial location in a pre-training task (Exp. 2–4). Thus, an explicit representation of physical space is a ‘scaffold’ that permits objects to be learned in a rotational-invariant fashion, and thus allows rotational generalisation. This supports the idea that neural representations of space form a critical substrate for learning abstractions in nonspatial domains (<xref ref-type="bibr" rid="bib3">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bellmund et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Summerfield et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Gärdenfors, 2014</xref>).</p><p>It is well known that humans learn rotational invariances for visual objects, whose features are organised in physical space. For example, an upside-down teapot can be recognised by the relative position of handle, lid, and spout. This case mimics our spatial modality condition, where each concept was a pattern of locations in physical space. It is thus perhaps unsurprising that rotational generalisation is possible in this condition. However, we found it striking that participants generalised in such different ways when the features in question were drawn from a nonspatial manifold, in either the visual or auditory domain. In these conditions, participants seemed to have no trouble recognising patterns that were consistently translated in feature space. This is consistent with previous studies that have shown that we can understand language in different accents, or name a familiar tune played at an atypical speed or pitch (<xref ref-type="bibr" rid="bib10">Dupoux and Green, 1997</xref>). However, they did so via a representation that focused on just one of the two possible dimensions, and thus did not permit rotational generalisation. There was thus a clear dissociation between human ability to generalise patterns in physical space and a more abstract feature space.</p><p>Next, we showed that spatial pre-training allowed rotational generalisation even for sequences composed of nonspatial features. This implies that the neural representation of space may serve as a ‘scaffold’, allowing people to visualise and manipulate nonspatial concepts. One alternative explanation of this effect could be that the spatial pre-training encourages participants to attend to both dimensions of the nonspatial stimuli. By contrast, pre-training in the visual or auditory domains (where multiple dimensions of a stimulus may be relevant less often naturally) encourages them to attend to a single dimension. However, data from our control experiments, Exp. 2c and Exp. 3c, are incompatible with this explanation. Around ~65% of the participants show a level of performance in the multimodal association task (&gt;50%) which could only be achieved if they were attending to both dimensions (performance attending to a single dimension would yield 25% and chance performance is at 6.25%). This suggests that participants are attending to both dimensions even in the visual and auditory mapping case. Rather, whilst we are not aware of previous studies that have tested spatial scaffolding in the way described here, our findings are consistent with the more general idea that space is represented in an overlapping fashion with nonspatial information, such as time or number (<xref ref-type="bibr" rid="bib9">Dehaene et al., 2022</xref>). For example, sequences with regular spatial geometry are learned more readily than those composed of arbitrary patterns (<xref ref-type="bibr" rid="bib1">Al Roumi et al., 2021</xref>). Our findings also cohere with evidence that visuospatial skills are correlated with a variety of academic competences, especially in STEM subjects such as maths and engineering (<xref ref-type="bibr" rid="bib25">Stieff and Uttal, 2015</xref>), and that spatial training interventions (such as teaching mental rotation) in educational settings can improve nonspatial abilities, such as calculus grades (<xref ref-type="bibr" rid="bib24">Sorby et al., 2013</xref>).</p><p>The idea that spatial representations form a generalised substrate for cognition – including for coding temporal structure – draws on a long tradition in philosophy (<xref ref-type="bibr" rid="bib14">Kant, 2007</xref>), cognitive science (<xref ref-type="bibr" rid="bib11">Gärdenfors, 2000</xref>), and neuroscience (<xref ref-type="bibr" rid="bib3">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Bellmund et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Summerfield et al., 2020</xref>). The precise substrate for this effect is unclear, but it seems likely that neural assemblies activated by physical locations in space (e.g. in parietal or medial temporal lobe areas) are recycled for representing nonspatial patterns in data. We acknowledge that our study does not provide a mechanistic model of the spatial scaffolding effect but rather delineate which aspects of the training are necessary for generalisation to happen. In our study, thus, the mapping task facilitates this recycling by teaching participants a point-to-point mapping between nonspatial feature combinations and locations in physical space. Indeed, our correlation analysis and Exp. 4 suggested that successfully learning mappings between spatial and nonspatial features was the strongest determinant of rotational generalisation. This mapping task was presented in an egocentric frame of reference defined by the <italic>x</italic>, <italic>y</italic> coordinates of the screen. Explicit representations of location in egocentric space in the primate are found in dorsal stream structures such as the posterior parietal cortex (<xref ref-type="bibr" rid="bib6">Colby et al., 1993</xref><ext-link ext-link-type="uri" xlink:href="https://sciwheel.com/work/citation?ids=1374964&amp;pre=&amp;suf=&amp;sa=0">)</ext-link>. Current deep networks – which successfully categorise lone objects in a natural image but often fail on tests of relational reasoning or scene understanding – may be hampered by their failure to represent space explicitly in this way (<xref ref-type="bibr" rid="bib26">Summerfield et al., 2020</xref><ext-link ext-link-type="uri" xlink:href="https://sciwheel.com/work/citation?ids=10703744&amp;pre=&amp;suf=&amp;sa=0">)</ext-link>.</p><p>All the effects observed in our experiments were consistent across near transfer conditions (rotation of patterns within the same feature space), and far transfer conditions (rotation of patterns within a different feature space, where features are drawn from the same modality). This shows the generality of spatial training for conceptual generalisation. This means that an explicit representation of space might be the substrate for strong forms of transfer observed in humans, such as when we understand the shared meaning between ‘red, amber, green’ at a traffic light and ‘ready, set, go’ before a race. We did not test transfer across modalities nor transfer in a more natural setting; we leave this for future studies.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Exp. 1, 2, and 3 and analyses were pre-registered. The pre-registration documents can be found at <ext-link ext-link-type="uri" xlink:href="https://osf.io/z9572/registrations">https://osf.io/z9572/registrations</ext-link>.</p><sec id="s4-1"><title>Stimuli and paradigm</title><sec id="s4-1-1"><title>Participants</title><p>In total, we collected data from 558 participants with the following demographic characteristics (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><p>Participants were recruited on the crowdsourcing platform Prolific (<ext-link ext-link-type="uri" xlink:href="https://app.prolific.co/">https://app.prolific.co/</ext-link>). Inclusion criteria included being between 18 and 40 years of age, reporting no neurological condition, being an English speaker, being located in the USA or the UK, not having participated in another version of the task, having a minimal approval rate of 90% on Prolific, and having a minimum of five previous submissions on Prolific. Participants received on average £10/hr for their time and effort, including a bonus on performance (£8.5/hr with random performances, £10.5/hr with perfect performances). All experiments were approved by the Medical Sciences Research Ethics Committee of the University of Oxford (approval reference R50750/RE005). Before starting the experiment, informed consent was taken through an online form, and participants indicated that they understood the goals of the study, how to raise any questions, how their data would be handled, and that they were free to withdraw from the experiment at any time.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Demographic data.</title><p><italic>µ</italic>: average, <italic>σ</italic>: standard deviation, F: female, M: male.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"><italic>N</italic></th><th align="left" valign="bottom">Age (<italic>µ</italic> ± <italic>σ</italic>)</th><th align="left" valign="bottom">Sex (F/M/other)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Exp. 1</td><td align="left" valign="bottom">153</td><td align="left" valign="bottom">29.4±6.1</td><td align="left" valign="bottom">70/68/15</td></tr><tr><td align="left" valign="bottom">Exp. 2</td><td align="left" valign="bottom">152</td><td align="left" valign="bottom">29.9±5.9</td><td align="left" valign="bottom">88/62/2</td></tr><tr><td align="left" valign="bottom">Exp. 3</td><td align="left" valign="bottom">151</td><td align="left" valign="bottom">29.4±5.4</td><td align="left" valign="bottom">88/63/0</td></tr><tr><td align="left" valign="bottom">Exp. 4</td><td align="left" valign="bottom">102</td><td align="left" valign="bottom">28.5±6.1</td><td align="left" valign="bottom">65/37/0</td></tr><tr><td align="left" valign="bottom">Total</td><td align="left" valign="bottom">558</td><td align="left" valign="bottom">29.4±5.9</td><td align="left" valign="bottom">311/230/17</td></tr></tbody></table></table-wrap><p>The sample size was determined prior to the data collection, as indicated in the pre-registration documents.</p></sec><sec id="s4-1-2"><title>Stimuli</title><p>Across all experiments, we presented sequences of four stimuli (‘quadruplets’). The stimuli occurred in one of three modalities: auditory, visual, or spatial. The quadruplet consisted of four successive auditory, visual, or spatial features, each drawn from 1 of 16 points (arranged in a 4×4 grid) on a 2D feature manifold (<italic>i</italic>, <italic>j</italic>). The dimensions of the manifold differed as a function of the modality, with four stimulus dimensions per modality (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). For each participant, given the relevant modality, two stimulus dimensions were randomly selected to form the dimensions of the original manifold (for training and near transfer; denoted by <italic>i</italic>, <italic>j</italic>) and the two other dimensions were selected to form the dimensions of the far transfer manifold (for far transfer; denoted by ★<italic>i</italic>, ★j). In each experiment, the stimulus dimensions assigned to the <italic>i</italic> and <italic>j</italic> dimension of the original manifold and the ★<italic>i</italic> and ★<italic>j</italic> dimensions of the far transfer manifold were randomised across participants.</p><p>In the auditory modality, stimuli were 500 ms complex modulated tones generated with the <italic>sndlib</italic> module of the <italic>pychoacoustics</italic> Python library (version 0.4.6, <ext-link ext-link-type="uri" xlink:href="https://pychoacoustics.readthedocs.io/">https://pychoacoustics.readthedocs.io/</ext-link>), with the following features:</p><list list-type="bullet"><list-item><p>Fundamental frequency F0 (110, 220, 330, or 440 Hz)</p></list-item><list-item><p>Frequency modulation FM (1, 2, 3, or 4 Hz)</p></list-item><list-item><p>Amplitude modulation AM (1, 2, 3, or 4 Hz)</p></list-item><list-item><p>Number of high harmonics (1, 3, 7, or 10).</p></list-item></list><p>Any combination of two features could be chosen as manifold feature dimensions except the combination FM and AM, because it is perceptively hard to discriminate FM and AM in a single sound.</p><p>In the visual modality, stimuli were Fernandez-Guasti squircle presented on a black background, generated with the <italic>matplotlib</italic> Python library (version 3.6.2, <ext-link ext-link-type="uri" xlink:href="https://matplotlib.org/">https://matplotlib.org/</ext-link>), with the following features:</p><list list-type="bullet"><list-item><p>Colour (viridis perceptually uniform colormap, 0, 0.33, 0.66, or 1)</p></list-item><list-item><p>Transparency level (alpha level, 0.2, 0.46, 0.73, or 1),</p></list-item><list-item><p>Squareness (squareness parameter of the Fernandez-Guasti squircle, 0.01, 0.8, 0.98, or 1)</p></list-item><list-item><p>Spikiness (amplitude of the cosine modulation relative to the squircle radius, 0, 0.06, 0.13, or 0.2).</p></list-item></list><p>Any combination of two features could be chosen as manifold feature dimensions except the combination transparency level and colour, because it is perceptively hard to discriminate the level of transparency and colour in a single image.</p><p>In the spatial modality, stimuli were a red star with different spatial locations presented on a black background, also generated with <italic>matplotlib</italic>, with the following features:</p><list list-type="bullet"><list-item><p>Horizontal position (1, 2, 3, or 4)</p></list-item><list-item><p>Vertical position (1, 2, 3, or 4)</p></list-item><list-item><p>Radius (1, 2, 3, or 4)</p></list-item><list-item><p>Polar angle (0, 90°, 180°, or –90°).</p></list-item></list><p>Horizontal position and vertical position, as well as radius and angle, were systematically associated. This is because the other feature combinations, such as radius and horizontal position, are impossible.</p><p>The precise intensity level of the auditory stimuli and the precise size of the visual stimuli were dependent on the participant’s headphones and screen and are thus unknown.</p></sec><sec id="s4-1-3"><title>Procedure</title><sec id="s4-1-3-1"><title>JavaScript online experiment</title><p>The experiment was written in JavaScript, using <italic>jsPsych</italic> (version 7.3.1, <ext-link ext-link-type="uri" xlink:href="https://www.jspsych.org/7.3/">https://www.jspsych.org/7.3/</ext-link>) (<xref ref-type="bibr" rid="bib8">de Leeuw, 2015</xref>), and hosted on a web server. Scripts are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/z9572">https://osf.io/z9572</ext-link>.</p></sec><sec id="s4-1-3-2"><title>Game design</title><p>The whole experiment was presented to the participants as an ‘interstellar mission’ game. The goal of this ‘interstellar mission’ was to establish contact with aliens on a distant planet. In the main task, participants were asked to ‘identify the aliens on the planet by paying attention to the sequence that they produce’. In the mapping task, participants were asked to ‘associate each alien sound (/image) with a spatial location (/image) on the screen’.</p></sec><sec id="s4-1-3-3"><title>Screening task</title><p>A screening task was performed prior to the experiment to ensure that the auditory conditions under which recruited participants performed the experiment were sufficient to discriminate the sounds, and to verify that participants were able to pay attention to a complex cognitive task. The screening task was an 8 min long, 2-back auditory task. Stimuli were artificially generated impact sounds of wood, metal, and glass (<xref ref-type="bibr" rid="bib2">Aramaki et al., 2006</xref>). All sounds had the same fundamental frequency, loudness, and duration, and differed only in timbre (examples of ‘tuned’ sounds available at <ext-link ext-link-type="uri" xlink:href="http://www.lma.cnrs-mrs.fr/~kronland/Categorization/sounds.html">http://www.lma.cnrs-mrs.fr/~kronland/Categorization/sounds.html</ext-link>). Each sound was 400 ms long, with cosine ramp on and off of 10 ms. Trials consisted of the following events: (1) sound presentation for 400 ms, (2) key press recording for 1000 ms, (3) trialwise feedback for 800 ms, and (4) an inter-trial interval for 1000 ms (in total, 3200 ms per trial). On every trial but the first two, participants had to indicate whether the sound was the same as the sound presented two trials before, by pressing a key on the keyboard (key [S] for ‘same’ and key [D] for ‘different’). Participants received feedback on every trial. 150 trials were presented. Participants reaching 75% accuracy were recruited in the main experiment. This corresponded to ~40% of participants. Batches of 100–250 participants were screened and allocated to one experiment and one condition until the desired sample size was reached for all experiments. All participants in all experiments did the screening task prior to the experiment.</p></sec><sec id="s4-1-3-4"><title>Main task</title><p>In the main task, participants were asked to infer the category of a quadruplet consisting of four successive visual, auditory, or spatial features (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A</xref>). There were three possible categories. Each category was defined by a canonical set of transition vectors, which specified three successive steps in a 2D feature manifold (category 0: {E, N, W}, category 1: {NE, W, SE}, category 2: {N, SE, N}). The quadruplets were further rotated and embedded in either the original manifold or the far transfer manifold, leading to eight transformations: canonical (∅), 90° rotation (↶), 180° rotation (↻), 270° rotation (↷), far transfer canonical (★), far transfer 90° rotation (★ + ↶), far transfer 180° rotation (★ + ↻), and far transfer 270° rotation (★ + ↷) (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Trials consisted of the following events: (1) a black loading screen for 500 ms, (2) quadruplet presentation for 8000 ms (four times 500 ms of stimulus presentation followed by 1500 ms black screen), (3) response recording window until a response was made, and (4) trialwise feedback for 800 ms. For trials without trialwise feedback, a black screen was presented for 800 ms instead of the feedback screen. Response was made by clicking with the mouse on one of three buttons that appeared on screen. The ordering of the buttons was randomised across participants, and kept fixed for the entire experiment. The ordering of the trials was pseudo-randomised such that exemplars from each of the three categories appeared 10 times each every block (30 trials). The starting location for the transition vector on the feature manifold was chosen randomly every trial from among nine possible positions (excluding the outer ring). Participants were instructed that the task was deterministic. (‘<italic>The rules used by the aliens to produce the sequences are 100% deterministic. This means that once you have discovered the rules, you will reach 100% of correct responses</italic>’.) On top of trialwise feedback on training trials, participants received blockwise feedback on their performance in the last block. Trials without trialwise feedback were not used to compute this blockwise feedback. See below for the exact trial numbers and ordering.</p></sec><sec id="s4-1-3-5"><title>Mapping task</title><p>In the mapping task, participants had to learn associations between features from different modalities (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref>). When space (/vision) was the pre-training modality and auditory (/visual) the testing modality, on each trial participants learned to associate one auditory (/visual) feature with its corresponding (spatial/visual) feature. For the spatial domain, this means mapping position on the latent manifold (<italic>i</italic>, <italic>j</italic>) onto its corresponding location in physical space (<italic>x</italic>, <italic>y</italic>). Trials consisted of the following events: (1) a black loading screen for 500 ms, (2) stimulus presentation for 500 ms, (3) a black screen for 600 ms, (4) a response recording window which continued until a response was made, and (5) trialwise feedback for 800 ms. When space was the pre-training modality, the response was made by clicking on 1 of 16 spatial locations on a 4×4 grid. When vision was the pre-training modality, response was made by clicking on 1 of 16 visual shapes arranged on a 4×4 grid. The spatial arrangement of the visual shapes changed randomly every block (48 trials) to deconfound spatial and visual features. The ordering of the trials was pseudo-randomised such that each of the 16 stimuli appeared three times each every block (48 trials). On top of trialwise feedback, participants received blockwise feedback on their performance in the last block. Finally, the mapping task could be restricted to a given dimension while fixing the other dimension, e.g., only change in the <italic>i</italic> dimension while maintaining the <italic>j</italic> dimension at a constant value.</p></sec><sec id="s4-1-3-6"><title>Filler task (Exp. 4 only)</title><p>In Exp. 4, a duration-matched filler task was introduced to replace the pre-training task, ensuring that the number of trials was kept constant and removing any exposure to the categorisation task in the spatial modality (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3C</xref>). As in the main task, participants were asked to infer the category of sequences of four items. There were three possible categories. The sequences were composed of four coloured stars appearing at the same location in space: either red-red-red-blue, red-red-blue-blue, or red-blue-blue-blue. Trials consisted of the following events: (1) a black loading screen for 500 ms, (2) sequence presentation for 8000 ms (four times 500 ms of stimulus presentation followed by 1500 ms black screen), (3) a response recording window which continued until a response was made, and (4) trialwise feedback for 800 ms. Response was made by clicking with the mouse on one of three buttons that appeared on screen. The ordering of the buttons was randomised across participants, and kept fixed for the entire experiment. The buttons were different from those used in the main task. The ordering of the trials was pseudo-randomised such that the three sequence categories appeared 10 times each every block (30 trials). The location of the star was chosen randomly every trial among the 16 possible locations. Participants were instructed on the deterministic nature of the task. (‘<italic>The rules used by the aliens to produce the sequences are 100% deterministic. This means that once you have discovered the rules, you will reach 100% of correct responses</italic>’.) On top of trialwise feedback, participants received blockwise feedback on their performance in the last block.</p></sec><sec id="s4-1-3-7"><title>Multi-day experiments</title><p>Exp. 2, 3, and 4 took place over the course of 2 days. After having completed the ‘day 1’ of the experiment, participants were proposed the ‘day 2’ of the experiment after 24 hr. If no completion of day 2 had been received after 72 hr, participants were considered dropped out.</p></sec><sec id="s4-1-3-8"><title>Complete task schedule</title><p>The ordering of the tasks and their characteristics varied across experiments. The following tables summarise the task schedules for Exp. 1 (see <xref ref-type="table" rid="table2">Table 2</xref>), Exp. 2 (see <xref ref-type="table" rid="table3">Table 3</xref>), Exp. 3 (see <xref ref-type="table" rid="table4">Table 4</xref>), and Exp. 4 (see <xref ref-type="table" rid="table5">Table 5</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Procedure for Experiment 1.</title><p>The table reads as follows: on day 1, participants in the Exp. 1a began with 120 trials of the quadruplet categorisation task in the auditory modality, with canonical quadruplets (0°, denoted by ∅), with feedback on every trials. They subsequently performed 105 trials (with trialwise feedback) and 105 transfer trials including rotated and far transfer quadruplets (without trialwise feedback) which were presented in mixed blocks of 30 trials. Training and transfer trials were randomly interleaved, and no clue indicated whether participants were currently on a training trial or a transfer trial before feedback (or absence of feedback in case of a transfer trial). All participants thus performed a total of 330 trials in a single session. Abbreviations/symbols: Fb.: trialwise feedback. Trans.: transformations of the quadruplets. Transformations are canonical (∅), 90° rotation (↶), 180° rotation (↻), 270° rotation (↷), far transfer canonical (★), far transfer 90° rotation (★ + ↶), far transfer 180° rotation (★ + ↻), and far transfer 270° rotation (★ + ↷).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Day</th><th align="left" valign="bottom">Task</th><th align="left" valign="bottom">Trial #</th><th align="left" valign="bottom">Trans.</th><th align="left" valign="bottom">Fb.</th><th align="left" valign="bottom">Exp. 1a(<italic>N</italic>=50)</th><th align="left" valign="bottom">Exp. 1b(<italic>N</italic>=52)</th><th align="left" valign="bottom">Exp. 1c(<italic>N</italic>=51)</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom" rowspan="3">1</td><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">120</td><td align="left" valign="bottom">∅</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Spatial</td></tr><tr><td align="left" valign="bottom" rowspan="2">Quadruplet categorisation</td><td align="char" char="." valign="bottom">105</td><td align="left" valign="bottom">∅</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Spatial</td></tr><tr><td align="char" char="." valign="bottom">105</td><td align="left" valign="bottom">↶<break/>↻<break/>↷<break/>★<break/>★ + ↶<break/>★ + ↻<break/>★ + ↷</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Spatial</td></tr></tbody></table></table-wrap><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Procedure for Experiment 2.</title><p>Same as <xref ref-type="table" rid="table2">Table 2</xref>. All participants thus performed a total of 396 trials on day 1 and 600 trials on day 2. Sp. → Aud.: spatial to auditory mapping task. Sp. → Vis.: spatial to visual mapping task. Vis. → Aud.: visual to auditory mapping task.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Day</th><th align="left" valign="bottom">Task</th><th align="left" valign="bottom">Trial #</th><th align="left" valign="bottom">Trans.</th><th align="left" valign="bottom">Fb.</th><th align="left" valign="bottom">Exp. 2a(<italic>N</italic>=51)</th><th align="left" valign="bottom">Exp. 2b(<italic>N</italic>=51)</th><th align="left" valign="bottom">Exp. 2c(<italic>N</italic>=50)</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom" rowspan="7">1</td><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud.</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">96</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom" rowspan="11">2</td><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="left" valign="bottom" rowspan="2">Quadruplet categorisation</td><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp.+Aud</td><td align="left" valign="bottom">Sp.+Vis.</td><td align="left" valign="bottom">Vis.+Aud</td></tr><tr><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr><tr><td align="left" valign="bottom">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud</td></tr><tr><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr><tr><td align="left" valign="bottom">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud</td></tr><tr><td align="left" valign="bottom" rowspan="2">Quadruplet categorisation</td><td align="char" char="." valign="bottom">90</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr><tr><td align="char" char="." valign="bottom">90</td><td align="left" valign="bottom">↻<break/>↷<break/>★<break/>★ + ↶<break/>★ + ↻<break/>★ + ↷</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr></tbody></table></table-wrap><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Procedure for Experiment 3.</title><p>Same conventions as <xref ref-type="table" rid="table2">Table 2</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Day</th><th align="left" valign="bottom">Task</th><th align="left" valign="bottom">Trial #</th><th align="left" valign="bottom">Trans.</th><th align="left" valign="bottom">Fb.</th><th align="left" valign="bottom">Exp. 3a(<italic>N</italic>=50)</th><th align="left" valign="bottom">Exp. 3b(<italic>N</italic>=51)</th><th align="left" valign="bottom">Exp. 3c(<italic>N</italic>=50)</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom" rowspan="7">1</td><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud.</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">96</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom" rowspan="11">2</td><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud.</td></tr><tr><td align="left" valign="bottom" rowspan="2">Quadruplet categorisation</td><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">∅</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp.+Aud</td><td align="left" valign="bottom">Sp.+Vis.</td><td align="left" valign="bottom">Vis.+Aud</td></tr><tr><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr><tr><td align="left" valign="bottom">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud</td></tr><tr><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr><tr><td align="left" valign="bottom">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td><td align="left" valign="bottom">Vis. → Aud</td></tr><tr><td align="left" valign="bottom" rowspan="2">Quadruplet categorisation</td><td align="char" char="." valign="bottom">90</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr><tr><td align="char" char="." valign="bottom">90</td><td align="left" valign="bottom">↻<break/>↷<break/>★<break/>★ + ↶<break/>★ + ↻<break/>★ + ↷</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">Auditory</td></tr></tbody></table></table-wrap><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Procedure for Experiment 4.</title><p>Same as <xref ref-type="table" rid="table2">Table 2</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Day</th><th align="left" valign="bottom">Task</th><th align="left" valign="bottom">Trial #</th><th align="left" valign="bottom">Trans.</th><th align="left" valign="bottom">Fb.</th><th align="left" valign="bottom">Exp. 4a(<italic>N</italic>=50)</th><th align="left" valign="bottom">Exp. 4b(<italic>N</italic>=52)</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom" rowspan="7">1</td><td align="left" valign="bottom">Filler</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Spatial</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud.</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom">96</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom" rowspan="10">2</td><td align="left" valign="bottom">Filler</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Spatial</td><td align="left" valign="bottom">Spatial</td></tr><tr><td align="left" valign="bottom" rowspan="3">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>i</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>j</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="left" valign="bottom">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="left" valign="bottom">Quadruplet categorisation</td><td align="char" char="." valign="bottom">60</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="left" valign="bottom">Mapping</td><td align="char" char="." valign="bottom">48</td><td align="left" valign="bottom"><italic>ij</italic></td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Sp. → Aud</td><td align="left" valign="bottom">Sp. → Vis.</td></tr><tr><td align="left" valign="bottom" rowspan="2">Quadruplet categorisation</td><td align="char" char="." valign="bottom">90</td><td align="left" valign="bottom">∅<break/>↶</td><td align="left" valign="bottom">Yes</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td></tr><tr><td align="char" char="." valign="bottom">90</td><td align="left" valign="bottom">↻<break/>↷<break/>★<break/>★ + ↶<break/>★ + ↻<break/>★ + ↷</td><td align="left" valign="bottom">No</td><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">Visual</td></tr></tbody></table></table-wrap></sec></sec></sec><sec id="s4-2"><title>Statistical analysis</title><sec id="s4-2-1"><title>Outliers</title><p>No outliers were removed from the analyses.</p><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Bayes factors quantifying the support in favour of a difference in model frequencies between each pair of conditions in the near transfer condition: <italic>1D</italic> vs <italic>2D</italic>.</title><p>Models were grouped in two families as follows: [<italic>1Di<sub>u</sub></italic>, <italic>1Dj<sub>u</sub></italic>, <italic>1Di<sub>s</sub></italic>, <italic>1Dj<sub>s</sub>]</italic>, [<italic>2D</italic>]. A BF&gt;1 indicates evidence in favour of a difference in model frequencies (green, BF&gt;3, BF&gt;10, and BF&gt;100 corresponds respectively to substantial, strong, and decisive evidence in favour of a difference in model frequencies between groups). A BF&lt;1 indicates evidence in favour of similar model frequencies (red, BF&lt;0.3, BF&lt;0.1, and BF&lt;0.01 corresponds respectively to substantial, strong, and decisive evidence in favour of no difference in model frequencies between groups).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Exp. 1b</th><th align="left" valign="bottom">Exp. 1c</th><th align="left" valign="bottom">Exp. 2a</th><th align="left" valign="bottom">Exp. 2b</th><th align="left" valign="bottom">Exp. 2c</th><th align="left" valign="bottom">Exp. 3a</th><th align="left" valign="bottom">Exp. 3b</th><th align="left" valign="bottom">Exp. 3c</th><th align="left" valign="bottom">Exp. 4a</th><th align="left" valign="bottom">Exp. 4b</th></tr></thead><tbody><tr><td align="left" valign="bottom">Exp. 1a</td><td align="left" valign="bottom">0.1</td><td align="left" valign="bottom">1.1×10<sup>17</sup></td><td align="left" valign="bottom">4.5×10<sup>10</sup></td><td align="left" valign="bottom">1.0×10<sup>15</sup></td><td align="left" valign="bottom">0.9</td><td align="left" valign="bottom">8.1×10<sup>4</sup></td><td align="left" valign="bottom">6.6×10<sup>7</sup></td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">85.8</td><td align="left" valign="bottom">730.1</td></tr><tr><td align="left" valign="bottom">Exp. 1b</td><td align="left" valign="bottom"/><td align="left" valign="bottom">3.8×10<sup>7</sup></td><td align="left" valign="bottom">1.1×10<sup>11</sup></td><td align="left" valign="bottom">3.1×10<sup>15</sup></td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.2×10<sup>5</sup></td><td align="left" valign="bottom">1.3×10<sup>8</sup></td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">106.7</td><td align="left" valign="bottom">979.4</td></tr><tr><td align="left" valign="bottom">Exp. 1c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">2.0</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">6.3×10<sup>12</sup></td><td align="left" valign="bottom">6999.7</td><td align="left" valign="bottom">40.3</td><td align="left" valign="bottom">2.1×10<sup>15</sup></td><td align="left" valign="bottom">9.6×10<sup>7</sup></td><td align="left" valign="bottom">4.2×10<sup>6</sup></td></tr><tr><td align="left" valign="bottom">Exp. 2a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.6</td><td align="left" valign="bottom">9.0×10<sup>6</sup></td><td align="left" valign="bottom">3.4</td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">1.2×10<sup>9</sup></td><td align="left" valign="bottom">1630.6</td><td align="left" valign="bottom">169.8</td></tr><tr><td align="left" valign="bottom">Exp. 2b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">8.3×10<sup>10</sup></td><td align="left" valign="bottom">483.7</td><td align="left" valign="bottom">5.9</td><td align="left" valign="bottom">2.2×10<sup>13</sup></td><td align="left" valign="bottom">2.6×10<sup>6</sup></td><td align="left" valign="bottom">1.4×10<sup>5</sup></td></tr><tr><td align="left" valign="bottom">Exp. 2c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">122.5</td><td align="left" valign="bottom">3.1×10<sup>4</sup></td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">0.9</td><td align="left" valign="bottom">3.6</td></tr><tr><td align="left" valign="bottom">Exp. 3a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.4</td><td align="left" valign="bottom">4220.0</td><td align="left" valign="bottom">0.9</td><td align="left" valign="bottom">0.3</td></tr><tr><td align="left" valign="bottom">Exp. 3b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">2.4×10<sup>6</sup></td><td align="left" valign="bottom">26.6</td><td align="left" valign="bottom">4.8</td></tr><tr><td align="left" valign="bottom">Exp. 3c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">8.6</td><td align="left" valign="bottom">56.0</td></tr><tr><td align="left" valign="bottom">Exp. 4a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td></tr></tbody></table></table-wrap><table-wrap id="table7" position="float"><label>Table 7.</label><caption><title>Bayes factors quantifying the support in favour of a difference in model frequencies between each pair of conditions in the near transfer condition: <italic>R</italic> vs <italic>1D/2D</italic>.</title><p>Models were grouped in two families as follows: [<italic>R</italic>, <italic>R’</italic>], [<italic>1Di<sub>u</sub></italic>, <italic>1Dj<sub>u</sub></italic>, <italic>1Di<sub>s</sub></italic>, <italic>1Dj<sub>s</sub></italic>, <italic>2D</italic>].</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Exp. 1b</th><th align="left" valign="bottom">Exp. 1c</th><th align="left" valign="bottom">Exp. 2a</th><th align="left" valign="bottom">Exp. 2b</th><th align="left" valign="bottom">Exp. 2c</th><th align="left" valign="bottom">Exp. 3a</th><th align="left" valign="bottom">Exp. 3b</th><th align="left" valign="bottom">Exp. 3c</th><th align="left" valign="bottom">Exp. 4a</th><th align="left" valign="bottom">Exp. 4b</th></tr></thead><tbody><tr><td align="left" valign="bottom">Exp. 1a</td><td align="char" char="." valign="bottom">1.7</td><td align="char" char="." valign="bottom">17.7</td><td align="char" char="." valign="bottom">2.7</td><td align="char" char="." valign="bottom">9.7</td><td align="char" char="." valign="bottom">7.4</td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.3</td></tr><tr><td align="left" valign="bottom">Exp. 1b</td><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">2.1×10<sup>5</sup></td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">0.4</td><td align="left" valign="bottom">0.8</td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">20.4</td></tr><tr><td align="left" valign="bottom">Exp. 1c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">1.4×10<sup>6</sup></td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">2.3</td><td align="left" valign="bottom">6.1</td><td align="left" valign="bottom">2.7</td><td align="left" valign="bottom">437.0</td></tr><tr><td align="left" valign="bottom">Exp. 2a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">4.0×10<sup>5</sup></td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">0.6</td><td align="left" valign="bottom">1.2</td><td align="left" valign="bottom">0.7</td><td align="left" valign="bottom">36.0</td></tr><tr><td align="left" valign="bottom">Exp. 2b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">4.6×10<sup>6</sup></td><td align="left" valign="bottom">0.7</td><td align="left" valign="bottom">1.5</td><td align="left" valign="bottom">3.6</td><td align="left" valign="bottom">1.7</td><td align="left" valign="bottom">196.9</td></tr><tr><td align="left" valign="bottom">Exp. 2c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">198.0</td><td align="left" valign="bottom">47.7</td><td align="left" valign="bottom">13.1</td><td align="left" valign="bottom">34.0</td><td align="left" valign="bottom">0.8</td></tr><tr><td align="left" valign="bottom">Exp. 3a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">1.3</td></tr><tr><td align="left" valign="bottom">Exp. 3b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.6</td></tr><tr><td align="left" valign="bottom">Exp. 3c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.3</td></tr><tr><td align="left" valign="bottom">Exp. 4a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.5</td></tr></tbody></table></table-wrap><table-wrap id="table8" position="float"><label>Table 8.</label><caption><title>Bayes factors quantifying the support in favour of a difference in model frequencies between each pair of conditions in the far transfer condition: <italic>1D</italic> vs <italic>2D</italic>.</title><p>Models were grouped in two families as follows: [<italic>1Di<sub>u</sub></italic>, <italic>1Di</italic>j<italic><sub>u</sub></italic>, <italic>1Dj<sub>u</sub>, 1Dji<sub>u</sub> 1Di<sub>s</sub></italic>, <italic>1Di</italic>j<italic><sub>s</sub></italic>, <italic>1Dj<sub>s</sub>, 1Dji<sub>s</sub></italic>], [<italic>2D</italic>].</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Exp. 1b</th><th align="left" valign="bottom">Exp. 1c</th><th align="left" valign="bottom">Exp. 2a</th><th align="left" valign="bottom">Exp. 2b</th><th align="left" valign="bottom">Exp. 2c</th><th align="left" valign="bottom">Exp. 3a</th><th align="left" valign="bottom">Exp. 3b</th><th align="left" valign="bottom">Exp. 3c</th><th align="left" valign="bottom">Exp. 4a</th><th align="left" valign="bottom">Exp. 4b</th></tr></thead><tbody><tr><td align="left" valign="bottom">Exp. 1a</td><td align="char" char="." valign="bottom">0.1</td><td align="char" char="." valign="bottom">2.0×10<sup>12</sup></td><td align="char" char="." valign="bottom">4.0×10<sup>10</sup></td><td align="char" char="." valign="bottom">3.5×10<sup>11</sup></td><td align="char" char="." valign="bottom">1.9</td><td align="left" valign="bottom">3.4×10<sup>5</sup></td><td align="left" valign="bottom">8.5×10<sup>5</sup></td><td align="left" valign="bottom">0.4</td><td align="left" valign="bottom">88.7</td><td align="left" valign="bottom">366.1</td></tr><tr><td align="left" valign="bottom">Exp. 1b</td><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">4.6×10<sup>12</sup></td><td align="char" char="." valign="bottom">8.4×10<sup>10</sup></td><td align="char" char="." valign="bottom">7.6×10<sup>11</sup></td><td align="char" char="." valign="bottom">2.1</td><td align="left" valign="bottom">5.1×10<sup>5</sup></td><td align="left" valign="bottom">1.3×10<sup>6</sup></td><td align="left" valign="bottom">0.4</td><td align="left" valign="bottom">107.2</td><td align="left" valign="bottom">461.3</td></tr><tr><td align="left" valign="bottom">Exp. 1c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">3.6×10<sup>7</sup></td><td align="left" valign="bottom">7.2</td><td align="left" valign="bottom">4.1</td><td align="left" valign="bottom">2.8×10<sup>9</sup></td><td align="left" valign="bottom">2.3×10<sup>4</sup></td><td align="left" valign="bottom">3403.6</td></tr><tr><td align="left" valign="bottom">Exp. 2a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">1.2×10<sup>6</sup></td><td align="left" valign="bottom">1.5</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">7.3×10<sup>7</sup></td><td align="left" valign="bottom">1439.1</td><td align="left" valign="bottom">266.4</td></tr><tr><td align="left" valign="bottom">Exp. 2b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">8.0×10<sup>6</sup></td><td align="left" valign="bottom">3.7</td><td align="left" valign="bottom">2.2</td><td align="left" valign="bottom">5.6×10<sup>8</sup></td><td align="left" valign="bottom">6820.3</td><td align="left" valign="bottom">1126.7</td></tr><tr><td align="left" valign="bottom">Exp. 2c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">101.4</td><td align="left" valign="bottom">204.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Exp. 3a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">1976.8</td><td align="left" valign="bottom">1.6</td><td align="left" valign="bottom">0.7</td></tr><tr><td align="left" valign="bottom">Exp. 3b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">4445.1</td><td align="left" valign="bottom">2.6</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Exp. 3c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">2.4</td><td align="left" valign="bottom">6.8</td></tr><tr><td align="left" valign="bottom">Exp. 4a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td></tr></tbody></table></table-wrap><table-wrap id="table9" position="float"><label>Table 9.</label><caption><title>Bayes factors quantifying the support in favour of a difference in model frequencies between each pair of conditions in the far transfer condition: <italic>R</italic> vs <italic>1D/2D</italic>.</title><p>Models were grouped in two families as follows: [<italic>R</italic>, <italic>R’</italic>], [<italic>1Di<sub>u</sub></italic>, <italic>1Di</italic>j<italic><sub>u</sub></italic>, <italic>1Dj<sub>u</sub>, 1Dji<sub>u</sub>, 1Di<sub>s</sub></italic>, <italic>1Di</italic>j<italic><sub>s</sub></italic>, <italic>1Dj<sub>s</sub>, 1Dji<sub>s</sub>, 2D</italic>].</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Exp. 1b</th><th align="left" valign="bottom">Exp. 1c</th><th align="left" valign="bottom">Exp. 2a</th><th align="left" valign="bottom">Exp. 2b</th><th align="left" valign="bottom">Exp. 2c</th><th align="left" valign="bottom">Exp. 3a</th><th align="left" valign="bottom">Exp. 3b</th><th align="left" valign="bottom">Exp. 3c</th><th align="left" valign="bottom">Exp. 4a</th><th align="left" valign="bottom">Exp. 4b</th></tr></thead><tbody><tr><td align="left" valign="bottom">Exp. 1a</td><td align="char" char="." valign="bottom">0.5</td><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">1.2</td><td align="left" valign="bottom">1.4</td><td align="left" valign="bottom">2.7</td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">0.6</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">9.6</td></tr><tr><td align="left" valign="bottom">Exp. 1b</td><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">0.2</td><td align="left" valign="bottom">131.0</td><td align="left" valign="bottom">340.4</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">22.9</td><td align="left" valign="bottom">0.4</td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">2635.3</td></tr><tr><td align="left" valign="bottom">Exp. 1c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="char" char="." valign="bottom">0.3</td><td align="left" valign="bottom">14.5</td><td align="left" valign="bottom">33.4</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">3.6</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">185.7</td></tr><tr><td align="left" valign="bottom">Exp. 2a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">525.3</td><td align="left" valign="bottom">1412.4</td><td align="left" valign="bottom">0.3</td><td align="left" valign="bottom">81.1</td><td align="left" valign="bottom">0.9</td><td align="left" valign="bottom">0.6</td><td align="left" valign="bottom">1.2×10<sup>4</sup></td></tr><tr><td align="left" valign="bottom">Exp. 2b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">14.1</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">1.5</td><td align="left" valign="bottom">3.0</td><td align="left" valign="bottom">0.2</td></tr><tr><td align="left" valign="bottom">Exp. 2c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">31.8</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">2.8</td><td align="left" valign="bottom">6.2</td><td align="left" valign="bottom">0.2</td></tr><tr><td align="left" valign="bottom">Exp. 3a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">3.6</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">167.7</td></tr><tr><td align="left" valign="bottom">Exp. 3b</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.6</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">0.4</td></tr><tr><td align="left" valign="bottom">Exp. 3c</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">0.2</td><td align="left" valign="bottom">9.9</td></tr><tr><td align="left" valign="bottom">Exp. 4a</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">25.7</td></tr></tbody></table></table-wrap></sec><sec id="s4-2-2"><title>Inference models</title><p>We designed inference models that used different kinds of representation to make an inference about the quadruplet category. These models were fit to each participant’s choices in order to decipher the most likely strategy they were using during training, near transfer, and far transfer.</p><p>There were seven models to fit the near transfer data (see <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>):</p><list list-type="bullet"><list-item><p><italic>R</italic>: a random model that responds randomly to every trial (null model).</p></list-item><list-item><p><italic>R’</italic>: another random model that responds correctly to the training trials but randomly to the transfer trials (‘non-generaliser’ or ‘overfitting’ model).</p></list-item><list-item><p><italic>1Di<sub>u</sub></italic>: a model that responds according to the unsigned transitions in the <italic>i</italic> dimension, such as ‘ABAB’, ‘ABBA’, and ‘AABB’ (where A and B are two feature locations on the <italic>i</italic> dimension). As the model responds in an unsigned manner, ‘ABAB’ maps onto ‘BABA’, ‘ABBA’ onto ‘BAAB’, and ‘AABB’ onto ‘BBAA’. This model achieves 100% accuracy in the training trials in Exp. 1 but 50% accuracy in the training trials in Exp. 2, 3, and 4. This is because when both canonical (0°) and 90° rotated quadruplets are present, the unsigned transitions in either dimension are not fully diagnostic of the category. For example, the pattern ‘ABBA’ in the <italic>j</italic> dimension correspond to both the category 0 with 0° rotation and category 1 with 90° rotation (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p></list-item><list-item><p><italic>1Dj<sub>u</sub></italic>: same as <italic>1Di<sub>u</sub></italic> but in the <italic>i</italic> dimension.</p></list-item><list-item><p><italic>1Di<sub>s</sub></italic>: a model that responds to the signed transitions in the <italic>i</italic> dimension, such as ‘ABAB’, ‘BABA’, ‘ABBA’, ‘BAAB’, ‘AABB’, and ‘BBAA’ (where A and B are two feature locations on the <italic>i</italic> dimension, and A is lower than B). As the model responds in a signed manner, ‘ABAB’ does not map onto ‘BABA’. This model achieves 100% accuracy in the training trials in Exp. 1, 2, 3, and 4.</p></list-item><list-item><p><italic>1Dj<sub>s</sub></italic>: same as <italic>1Di<sub>s</sub></italic> but in the <italic>j</italic> dimension. This model achieves 100% accuracy in the training trials in Exp. 1 but 50% accuracy in Exp. 2, 3, and 4. This is again because when both canonical (0°) and 90° rotated quadruplets are present, the signed transitions in the <italic>j</italic> dimension are not 100% diagnostic of the quadruplet category.</p></list-item><list-item><p><italic>2D</italic>: a model that responds according to the vector transitions in both <italic>i</italic> and <italic>j</italic> dimensions. This model trivially achieves 100% accuracy in the training trials in Exp. 1, 2, 3, and 4.</p></list-item></list><p>Four more models were added when fitting the far transfer data to account for the fact that the participant can map between dimensions in the original manifold and dimensions in the far transfer manifold in a variety of ways (see <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). For example, a participant tracking patterns in the <italic>i</italic> dimension during training could track the same pattern in the ★<italic>j</italic> dimension in far transfer.</p><list list-type="bullet"><list-item><p><italic>1Di</italic>j<italic><sub>u</sub></italic>: a model that tracks the unsigned transitions in the <italic>i</italic> dimension and respond as if ★<italic>j</italic> was the <italic>i</italic> dimension in far transfer.</p></list-item><list-item><p><italic>1Dji<sub>u</sub></italic>.</p></list-item><list-item><p><italic>1Di</italic>j<italic><sub>s</sub></italic>.</p></list-item><list-item><p><italic>1Dji<sub>s</sub></italic>.</p></list-item></list></sec><sec id="s4-2-3"><title>Model likelihood</title><p>All models, except the random model <italic>R</italic>, had one free parameter: the temperature parameter <italic>β</italic> of a softmax when converting inference over category into choice probability. For a single trial, the likelihood was defined as:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>β</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>β</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>C<sub>p,t</sub></italic> is the category chosen by the participant <italic>p</italic> on trial <italic>t</italic> (<italic>C<sub>p,t</sub></italic>=0, 1, or 2), <italic>Q<sub>p,t</sub></italic> the quadruplet presented on this trial, <italic>M</italic> the inference model, <italic>β</italic> the temperature parameter, and p(<italic>c</italic>|<italic>Q<sub>p,t</sub></italic>, <italic>M</italic>) the probability assigned by model <italic>M</italic> to the category <italic>c</italic> for the quadruplet <italic>Q<sub>p,t</sub></italic>.</p><p>Assuming that trials are independent, the likelihood of model <italic>M</italic> for participant <italic>p</italic> over all trials is the product of the likelihood of the individual trials, or equivalently, the log-likelihood is the sum of the log-likelihood of the individual trials:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-2-4"><title>Model fitting</title><p>For models with a temperature parameter <italic>β</italic>, the maximum likelihood was defined as the maximum value of the likelihood function over 200 linearly spaced values of <italic>β</italic> between 0.01 and 0.5.<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For each participant, the best model was chosen as the model with the lowest Bayesian information criterion (BIC). This was done to adjust for model complexity between models without parameters (the random model <italic>R</italic>) and models with one parameter (all the others). For each participant <italic>p</italic> and model <italic>M</italic>, <italic>BIC<sub>p</sub></italic>(<italic>M</italic>) was defined as:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>k</italic> is the number of parameters (<italic>k</italic>=0 for the random model <italic>R</italic>, <italic>k</italic>=1 for all other models) and <italic>T</italic> the number of trials.</p><p>The inference models were fitted to trial-by-trial choice data independently for each participant using training and near transfer trials for near transfer and using training and far transfer trials for far transfer. Using training trials was done to improve the fits, as some models differ in their response during training, e.g., model <italic>1Di<sub>u</sub></italic> and <italic>1Dj<sub>u</sub></italic> in Exp. 2, 3, and 4.</p></sec><sec id="s4-2-5"><title>Model recovery</title><p>A model recovery analysis was performed to ensure that the experimental design was able to differentiate between models. We generated artificial data for each model with the same trials and the same number of trials as our human participants. We simulated 100 models for four values of the temperature parameter (0.05, 0.2, 0.35, and 0.5). Results showed that model recovery was very good for all experiments, even in high noise regimes (temperature of 0.5) (see <xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>).</p></sec><sec id="s4-2-6"><title>Model comparison</title><p>Model frequencies and difference in model frequencies between groups were estimated using Bayesian group comparison as described in <xref ref-type="bibr" rid="bib22">Rigoux et al., 2014</xref>. The marginal likelihood for model <italic>M</italic> and choice data <italic>C<sub>p</sub></italic> of participant <italic>p</italic> was estimated using <italic>BIC</italic> and defined as:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>This estimate was used to compute the posterior probability p(<italic>H</italic><sub>0</sub>|<italic>C</italic>), which quantifies the probability that two groups come from the same distribution, i.e., have similar model frequencies. Under uniform prior over <italic>H</italic><sub>0</sub> and <italic>H</italic><sub>1</sub> (the two groups do not come from the same distribution), this allowed to compute a <italic>BF</italic> as follows:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mi>B</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>In this form, the BF quantifies the support of the data in favour of a difference in model frequencies between groups. We followed <xref ref-type="bibr" rid="bib15">Kass and Raftery, 1995</xref> for the interpretation of its values: BF&gt;3, BF&gt;10, and BF&gt;100 were respectively taken as substantial, strong, and decisive evidence in favour of a difference in model frequencies between groups (BF&lt;0.3, BF&lt;0.1, and BF&lt;0.01 as evidence in favour of no difference in model frequencies).</p></sec><sec id="s4-2-7"><title>Cross-validation visualisation</title><p>Finally, cross-validation was used for visualisation. For this, we first fitted the models using half of the trials (even trial numbers) and selected the model with the lowest BIC for each participant. We then computed the response matrix of each participant using the unobserved half of the trials (odd trial numbers). We finally displayed the averaged left-one-out response matrices and the expected response matrix for models that had been selected as the best model for at least five participants.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the Medical Sciences Research Ethics Committee of the University of Oxford (approval reference R50750/RE005). Before starting the experiment, informed consent was taken through an online form, and participants indicated that they understood the goals of the study, how to raise any questions, how their data would be handled, and that they were free to withdraw from the experiment at any time.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93636-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Anonymized data, code, materials and pre-registration documents are all available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/z9572/">https://osf.io/z9572/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Pesnot Lerousseau</surname><given-names>J</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Space as a Scaffold for Rotational Generalisation of Abstract Concepts</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/Z9572</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Jean Daunizeau for technical help with modelling. Funding Work supported by Fondation Pour l’Audition FPA RD-2021-2 (JPL) and European Research Council Consolidator Grant n° 725937 - CQR01290.CQ001 (CS).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Marti</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mental compression of spatial sequences in human working memory using numerical and geometrical primitives</article-title><source>Neuron</source><volume>109</volume><fpage>2627</fpage><lpage>2639</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.009</pub-id><pub-id pub-id-type="pmid">34228961</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aramaki</surname><given-names>M</given-names></name><name><surname>Kronland-Martinet</surname><given-names>R</given-names></name><name><surname>Voinier</surname><given-names>T</given-names></name><name><surname>Ystad</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A percussive sound synthesizer based on physical and perceptual attributes</article-title><source>Computer Music Journal</source><volume>30</volume><fpage>32</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1162/comj.2006.30.2.32</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Baram</surname><given-names>AB</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</article-title><source>Neuron</source><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.002</pub-id><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellmund</surname><given-names>JLS</given-names></name><name><surname>Gärdenfors</surname><given-names>P</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Navigating cognition: Spatial codes for human thinking</article-title><source>Science</source><volume>362</volume><elocation-id>eaat6766</elocation-id><pub-id pub-id-type="doi">10.1126/science.aat6766</pub-id><pub-id pub-id-type="pmid">30409861</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><pub-id pub-id-type="pmid">3575582</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Ventral intraparietal area of the macaque: anatomic location and visual response properties</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>902</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.3.902</pub-id><pub-id pub-id-type="pmid">8385201</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname><given-names>AO</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title><source>Science</source><volume>352</volume><fpage>1464</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id><pub-id pub-id-type="pmid">27313047</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>jsPsych: a JavaScript library for creating behavioral experiments in a Web browser</article-title><source>Behavior Research Methods</source><volume>47</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3758/s13428-014-0458-y</pub-id><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Symbols and mental programs: a hypothesis about human singularity</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>751</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.06.010</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dupoux</surname><given-names>E</given-names></name><name><surname>Green</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Perceptual adjustment to highly compressed speech: effects of talker and rate changes</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>23</volume><fpage>914</fpage><lpage>927</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.23.3.914</pub-id><pub-id pub-id-type="pmid">9180050</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gärdenfors</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Conceptual Spaces: The Geometry of Thought</source><publisher-name>The MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/2076.001.0001</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gärdenfors</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>The Geometry of Meaning: Semantics Based on Conceptual Spaces</source><publisher-name>The MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9629.001.0001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Warren</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What is an auditory object?</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>887</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1038/nrn1538</pub-id><pub-id pub-id-type="pmid">15496866</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kant</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Critique of Pure Reason H</source><publisher-name>Palgrave Macmillan</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-137-10016-0</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes Factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The discovery of structural form</article-title><source>PNAS</source><volume>105</volume><fpage>10687</fpage><lpage>10692</lpage><pub-id pub-id-type="doi">10.1073/pnas.0802631105</pub-id><pub-id pub-id-type="pmid">18669663</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Convolutional neural networks as a model of the visual system: past, present, and future</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2017</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01544</pub-id><pub-id pub-id-type="pmid">32027584</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Building concepts one episode at a time: The hippocampus and concept formation</article-title><source>Neuroscience Letters</source><volume>680</volume><fpage>31</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2017.07.061</pub-id><pub-id pub-id-type="pmid">28801273</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>DS</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Map making: constructing, combining, and inferring on abstract cognitive maps</article-title><source>Neuron</source><volume>107</volume><fpage>1226</fpage><lpage>1238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.030</pub-id><pub-id pub-id-type="pmid">32702288</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>DS</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inferences on a multidimensional social hierarchy use a grid-like code</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1292</fpage><lpage>1301</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00916-3</pub-id><pub-id pub-id-type="pmid">34465915</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoux</surname><given-names>L</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian model selection for group studies - revisited</article-title><source>NeuroImage</source><volume>84</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id><pub-id pub-id-type="pmid">24018303</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rock</surname><given-names>I</given-names></name><name><surname>DiVita</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>A case of viewer-centered object perception</article-title><source>Cognitive Psychology</source><volume>19</volume><fpage>280</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(87)90013-2</pub-id><pub-id pub-id-type="pmid">3581759</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorby</surname><given-names>S</given-names></name><name><surname>Casey</surname><given-names>B</given-names></name><name><surname>Veurink</surname><given-names>N</given-names></name><name><surname>Dulaney</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The role of spatial training in improving spatial and calculus performance in engineering students</article-title><source>Learning and Individual Differences</source><volume>26</volume><fpage>20</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.lindif.2013.03.010</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stieff</surname><given-names>M</given-names></name><name><surname>Uttal</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>How much can spatial training improve STEM achievement?</article-title><source>Educational Psychology Review</source><volume>27</volume><fpage>607</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1007/s10648-015-9304-8</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Luyckx</surname><given-names>F</given-names></name><name><surname>Sheahan</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structure learning and the posterior parietal cortex</article-title><source>Progress in Neurobiology</source><volume>184</volume><elocation-id>101717</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2019.101717</pub-id><pub-id pub-id-type="pmid">31669186</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How to grow a mind: statistics, structure, and abstraction</article-title><source>Science</source><volume>331</volume><fpage>1279</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1126/science.1192788</pub-id><pub-id pub-id-type="pmid">21393536</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tversky</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Spatial Schemas in Depictions, Spatial Schemas and Abstract Thought</source><publisher-name>Mitpress</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/6392.001.0001</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname><given-names>S</given-names></name><name><surname>Piazza</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distance and direction codes underlie navigation of a novel semantic space in the human brain</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>2727</fpage><lpage>2736</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1849-19.2020</pub-id><pub-id pub-id-type="pmid">32060171</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>G</given-names></name><name><surname>Bülthoff</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Learning to recognize objects</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>22</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(98)01261-3</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation</article-title><source>Cell</source><volume>183</volume><fpage>1249</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.10.024</pub-id><pub-id pub-id-type="pmid">33181068</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>I</given-names></name><name><surname>Denham</surname><given-names>SL</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>532</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.09.003</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93636.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>These ingenious and thoughtful studies present <bold>important</bold> findings concerning how people can represent and generalise abstract patterns of sensory data. The issue of generalization is a core topic in neuroscience and psychology, relevant across a wide range of areas, and the findings will be of interest to researchers across areas in perception, learning and cognitive science. The findings are <bold>convincing</bold> in this setting, but future research must establish their generality and interrogate the precise nature of the underlying mechanism.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93636.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This manuscript reports a series of experiments examining category learning and subsequent generalization of stimulus representations across spatial and nonspatial domains. In Experiment 1, participants were first trained to make category judgments about sequences of stimuli presented either in nonspatial auditory or visual modalities (with feature values drawn from a two-dimensional feature manifold, e.g., pitch vs timbre), or in a spatial modality (with feature values defined by positions in physical space, e.g., Cartesian x and y coordinates). A subsequent test phase assessed category judgments for 'rotated' exemplars of these stimuli: i.e., versions in which the transition vectors are rotated in the same feature space used during training (near transfer) or in a different feature space belonging to the same domain (far transfer). Findings demonstrate clearly that representations developed for the spatial domain allow for representational generalization, whereas this pattern is not observed for the nonspatial domains that are tested. Subsequent experiments demonstrate that if participants are first pre-trained to map nonspatial auditory/visual features to spatial locations, then rotational generalization is facilitated even for these nonspatial domains. It is argued that these findings are consistent with the idea that spatial representations form a generalized substrate for cognition: that space can act as a scaffold for learning abstract nonspatial concepts.</p><p>Strengths:</p><p>I enjoyed reading this manuscript, which is extremely well written and well presented. The writing is clear and concise throughout, and the figures do a great job of highlighting the key concepts. The issue of generalization is a core topic in neuroscience and psychology, relevant across a wide range of areas, and the findings will be of interest to researchers across areas in perception and cognitive science. It's also excellent to see that the hypotheses, methods and analyses were pre-registered.</p><p>The experiments that have been run are ingenious and thoughtful; I particularly liked the use of stimulus structures that allow for disentangling of one-dimensional and two-dimensional response patterns. The studies are also well powered for detecting effects of interest. The model-based statistical analyses are thorough and appropriate throughout (and it's good to see model recovery analysis too). The findings themselves are clear-cut: I have little doubt about the robustness and replicability of these data.</p><p>Weaknesses:</p><p>In my original review I raised a concern related to a potential alternative interpretation of the findings: the idea that participants have substantial experience of representing space in terms of multiple, independent, and separable dimensions, whereas this may not be the case for the visual and auditory stimuli used here. As I noted in that prior review, on this view &quot;the impact of spatial pre-training and (particularly) mapping is simply to highlight to participants that the auditory / visual stimuli comprise two separable (and independent) dimensions.&quot;</p><p>In addressing this point, the authors note that performance in the visual/auditory &quot;mapping&quot; task in Experiments 2c and 3c suggests that most participants were paying attention to both dimensions of auditory and visual stimuli. I agree that seems to have been the case. But there is a difference between making use of information from both dimensions, and realizing that ***the two dimensions are separable and independent*** (which is what is required for rotational generalization in this task).</p><p>As an analogy, suppose I have a task where participants have to map a pillow and a shuttlecock to category A, and a surfboard and a bicycle to category B. A participant could learn to do this just by memorizing the correct response for each item considered as a &quot;whole thing&quot;. Or they could realize that the items contain component information, learning that &quot;things with feathers&quot; belong in category A, and &quot;things that can carry people&quot; go in category B. Performance may be the same in both cases, but the underlying process is quite different.</p><p>The &quot;attention to dimensions&quot; account that I advanced in my previous review was referring to something more like the latter (feathers/vehicle) case: that spatial pre-training helps people to understand that items can be decomposed into separable pieces of information. Above-chance performance in the visual-auditory mapping task does not (necessarily) demonstrate this ability because it could reflect memorization of &quot;whole&quot; stimuli rather than reflecting decomposition into separable component parts. I agree that it does at least show that participants were paying attention to and making use of information from both dimensions when making their mapping decisions; it's just that they may not have *realized* that they were using information from two separable dimensions.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93636.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this manuscript, L&amp;S investigates the important general question of how humans achieve invariant behavior over stimuli belonging to one category given the widely varying input representation of those stimuli and more specifically, how they do that in arbitrary abstract domains. The authors start with the hypothesis that this is achieved by invariance transformations that observers use for interpreting different entries and furthermore, that these transformations in an arbitrary domain emerge with the help of the transformations (e. g. translation, rotation) within the spatial domain by using those as &quot;scaffolding&quot; during transformation learning. To provide the missing evidence for this hypothesis, L&amp;S used behavioral category learning studies within and across the spatial, auditory and visual domains, where rotated and translated 4-element token sequences had to be learned to categorize and then the learned transformation had to applied in new feature dimensions within the given domain. Through single- and multiple-day supervised training and unsupervised tests, L&amp;S demonstrated by standard computational analyses that in such setups, space and spatial transformations can, indeed, help with developing and using appropriate rotational mapping whereas the visual domain cannot fulfill such a scaffolding role.</p><p>Strengths:</p><p>The overall problem definition and the context of spatial mapping-driven solution to the problem is timely. The general design of testing the scaffolding effect across different domains is more advanced than any previous attempts clarifying the relevance of spatial coding to any other type of representational codes. Once the formulation of the general problem in a specific scientific framework is done, the following steps are clearly and logically defined and executed. The obtained results are well interpretable, and they could serve as a good steppingstone for deeper investigations. The analytical tools used for the interpretations are adequate. The paper is relatively clearly written.</p><p>Weaknesses:</p><p>Some additional effort to clarify the exact contribution of the paper, the link between analyses and the claims of the paper and its link to previous proposals would be necessary to better assess the significance of the results and the true nature of the proposed mechanism of abstract generalization.</p><p>(1) Insufficient conceptual setup: The original theoretical proposal (the Tolman-Eichenbaum-Machine, Whittington et al., Cell 2020) that L&amp;S relate their work proposes that just as in the case of memory for spatial navigation, humans and animal create their flexible relational memory system of any abstract representation by a conjunction code that combines on the one hand, sensory representation and on the other hand, a general structural representation or relational transformation. The TEM also suggest that the structural representation could contain any graph-interpretable spatial relations, albeit in their demonstration 2D neighbor relations were used. The goal of L&amp;S's paper is to provide behavioral evidence for this suggestion by showing that humans use representational codes that are invariant to relational transformations of non-spatial abstract stimuli and moreover, that humans obtain these invariances by developing invariance transformers with the help of available spatial transformers. To obtain such evidence, L&amp;S use the rotational transformation. However, the actual procedure they used actually solved an alternative task: instead of interrogating how humans develop generalizations in abstract spaces, they demonstrated that if one defines rotation in an abstract feature space embedded in visual or auditory modality that is similar to the 2D space (i.e. has two independent dimensions that are clearly segregable and continuous), humans cannot learn to apply rotation of 4-piece temporal sequences in those spaces while they can do it in 2D space, and with co-associating a one-to-one mapping between locations in those feature spaces with locations in the 2D space an appropriate shaping mapping training will lead to successful application of rotation in the given task (and in some other feature spaces in the given domain). While this is an interesting and challenging demonstration, it does not shed light on how humans learn and generalize only that humans CAN do learning and generalization in this, highly constrained scenario. This result is a demonstration of how a stepwise learning regiment can make use of one structure for mapping a complex input into a desired output. The results neither clarify how generalizations would develop in abstract spaces nor the question if this generalization uses transformations developed in the abstract space. The specific training procedure ensures success in the presented experiments but the availability and feasibility of an equivalent procedure in natural setting is a crucial part of validating the original claim and that has not been done in the paper.</p><p>(2) Missing controls: The asymptotic performance in Exp 1 after training in the three tasks was quite different in the three tasks (intercepts 2.9, 1.9, 1.6 for spatial, visual and auditory, respectively; p. 5. para. 1, Fig 2BFJ). It seems that the statement &quot;However, or main question was how participants would generalise learning to novel, rotated exemplars of the same concept.&quot; assumes that learning and generalization are independent. Wouldn't it be possible, though, that the level of generalization depends on the level of acquiring a good representation of the &quot;concept&quot; and after obtaining an adequate level of this knowledge, generalization would kick in without scaffolding? If so, a missing control is to equate the levels of asymptotic learning and see whether there is a significant difference in generalization. A related issue is that we have no information what kind of learning in the three different domains were performed, albeit we probably suspect that in space the 2D representation was dominant while in the auditory and visual domains not so much. Thus, a second missing piece of evidence is the model fitting results of the ⦰ condition that would show which way the original sequences were encoded (similar to Fig 2 CGK and DHL). If the reason for lower performance is not individual stimulus difficulty but the natural tendency to encode the given stimulus type by a combo of random + 1D strategy that would clarify that the result of the cross-training is, indeed, transferring the 2D-mapping strategy.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93636.3.sa3</article-id><title-group><article-title>Author Response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pesnot Lerousseau</surname><given-names>Jacques</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Summerfield</surname><given-names>Christopher</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>These ingenious and thoughtful studies present important findings concerning how people represent and generalise abstract patterns of sensory data. The issue of generalisation is a core topic in neuroscience and psychology, relevant across a wide range of areas, and the findings will be of interest to researchers across areas in perception, learning, and cognitive science. The findings have the potential to provide compelling support for the outlined account, but there appear other possible explanations, too, that may affect the scope of the findings but could be considered in a revision.</p></disp-quote><p>Thank you for sending the feedback from the three peer reviewers regarding our paper. Please find below our detailed responses addressing the reviewers' comments. We have incorporated these suggestions into the paper and provided explanations for the modifications made.</p><p>We have specifically addressed the point of uncertainty highlighted in eLife's editorial assessment, which concerned alternative explanations for the reported effect. In response to Reviewer #1, we have clarified how Exp. 2c and Exp. 3c address the potential alternative explanation related to &quot;attention to dimensions.&quot; Further, we present a supplementary analysis to account for differences in asymptotic learning, as noted by Reviewer #2. We have also clarified how our control experiments address effects associated with general cognitive engagement in the task. Lastly, we have further clarified the conceptual foundation of our paper, addressing concerns raised by Reviewers #2 and #3.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>This manuscript reports a series of experiments examining category learning and subsequent generalization of stimulus representations across spatial and nonspatial domains. In Experiment 1, participants were first trained to make category judgments about sequences of stimuli presented either in nonspatial auditory or visual modalities (with feature values drawn from a two-dimensional feature manifold, e.g., pitch vs timbre), or in a spatial modality (with feature values defined by positions in physical space, e.g., Cartesian x and y coordinates). A subsequent test phase assessed category judgments for 'rotated' exemplars of these stimuli: i.e., versions in which the transition vectors are rotated in the same feature space used during training (near transfer) or in a different feature space belonging to the same domain (far transfer). Findings demonstrate clearly that representations developed for the spatial domain allow for representational generalization, whereas this pattern is not observed for the nonspatial domains that are tested. Subsequent experiments demonstrate that if participants are first pre-trained to map nonspatial auditory/visual features to spatial locations, then rotational generalization is facilitated even for these nonspatial domains. It is argued that these findings are consistent with the idea that spatial representations form a generalized substrate for cognition: that space can act as a scaffold for learning abstract nonspatial concepts.</p><p>Strengths:</p><p>I enjoyed reading this manuscript, which is extremely well-written and well-presented. The writing is clear and concise throughout, and the figures do a great job of highlighting the key concepts. The issue of generalization is a core topic in neuroscience and psychology, relevant across a wide range of areas, and the findings will be of interest to researchers across areas in perception and cognitive science. It's also excellent to see that the hypotheses, methods, and analyses were pre-registered.</p><p>The experiments that have been run are ingenious and thoughtful; I particularly liked the use of stimulus structures that allow for disentangling of one-dimensional and two-dimensional response patterns. The studies are also well-powered for detecting the effects of interest. The model-based statistical analyses are thorough and appropriate throughout (and it's good to see model recovery analysis too). The findings themselves are clear-cut: I have little doubt about the robustness and replicability of these data.</p><p>Weaknesses:</p><p>I have only one significant concern regarding this manuscript, which relates to the interpretation of the findings. The findings are taken to suggest that &quot;space may serve as a 'scaffold', allowing people to visualize and manipulate nonspatial concepts&quot; (p13). However, I think the data may be amenable to an alternative possibility. I wonder if it's possible that, for the visual and auditory stimuli, participants naturally tended to attend to one feature dimension and ignore the other - i.e., there may have been a (potentially idiosyncratic) difference in salience between the feature dimensions that led to participants learning the feature sequence in a one-dimensional way (akin to the 'overshadowing' effect in associative learning: e.g., see Mackintosh, 1976, &quot;Overshadowing and stimulus intensity&quot;, Animal Learning and Behaviour). By contrast, we are very used to thinking about space as a multidimensional domain, in particular with regard to two-dimensional vertical and horizontal displacements. As a result, one would naturally expect to see more evidence of two-dimensional representation (allowing for rotational generalization) for spatial than nonspatial domains.</p><p>In this view, the impact of spatial pre-training and (particularly) mapping is simply to highlight to participants that the auditory/visual stimuli comprise two separable (and independent) dimensions. Once they understand this, during subsequent training, they can learn about sequences on both dimensions, which will allow for a 2D representation and hence rotational generalization - as observed in Experiments 2 and 3. This account also anticipates that mapping alone (as in Experiment 4) could be sufficient to promote a 2D strategy for auditory and visual domains.</p><p>This &quot;attention to dimensions&quot; account has some similarities to the &quot;spatial scaffolding&quot; idea put forward in the article, in arguing that experience of how auditory/visual feature manifolds can be translated into a spatial representation helps people to see those domains in a way that allows for rotational generalization. Where it differs is that it does not propose that space provides a <italic>scaffold</italic> for the development of the nonspatial representations, i.e., that people represent/learn the nonspatial information in a spatial format, and this is what allows them to manipulate nonspatial concepts. Instead, the &quot;attention to dimensions&quot; account anticipates that ANY manipulation that highlights to participants the separable-dimension nature of auditory/visual stimuli could facilitate 2D representation and hence rotational generalization. For example, explicit instruction on how the stimuli are constructed may be sufficient, or pre-training of some form with each dimension separately, before they are combined to form the 2D stimuli.</p><p>I'd be interested to hear the authors' thoughts on this account - whether they see it as an alternative to their own interpretation, and whether it can be ruled out on the basis of their existing data.</p></disp-quote><p>We thank the Reviewer for their comments. We agree with the Reviewer that the “attention to dimensions” hypothesis is an interesting alternative explanation. However, we believe that the results of our control experiments Exp. 2c and Exp. 3c are incompatible with this alternative explanation.</p><p>In Exp. 2c, participants are pre-trained in the visual modality and then tested in the auditory modality. In the multimodal association task, participants have to associate the auditory stimuli and the visual stimuli: on each trial, they hear a sound and then have to click on the corresponding visual stimulus. It is thus necessary to pay attention to both auditory dimensions and both visual dimensions to perform the task. To give an example, the task might involve mapping the fundamental frequency and the amplitude modulation of the auditory stimulus to the colour and the shape of the visual stimulus, respectively. If participants pay attention to only one dimension, this would lead to a maximum of 25% accuracy on average (because they would be at chance on the other dimension, with four possible options). We observed that 30/50 participants reached an accuracy &gt; 50% in the multimodal association task in Exp. 2c. This means that we know for sure that at least 60% of the participants paid attention to both dimensions of the stimuli. Nevertheless, there was a clear difference between participants that received a visual pre-training (Exp. 2c) and those who received a spatial pre-training (Exp. 2a) (frequency of 1D vs 2D models between conditions, BF &gt; 100 in near transfer and far transfer). In fact, only 3/50 participants were best fit by a 2D model when vision was the pre-training modality compared to 29/50 when space was the pre-training modality. Thus, the benefit of the spatial pre-training cannot be due solely to a shift in attention toward both dimensions.</p><p>This effect was replicated in Exp. 3c. Similarly, 33/48 participants reached an accuracy &gt; 50% in the multimodal association task in Exp. 3c, meaning that we know for sure that at least 68% of the participants actually paid attention to both dimensions of the stimuli. Again, there was a clear difference between participants who received a visual pre-training (frequency of 1D vs 2D models between conditions, Exp. 3c) and those who received a spatial pre-training (Exp. 3a) (BF &gt; 100 in near transfer and far transfer).</p><p>Thus, we believe that the alternative explanation raised by the Reviewer is not supported by our data. We have added a paragraph in the discussion:</p><p>“One alternative explanation of this effect could be that the spatial pre-training encourages participants to attend to both dimensions of the non-spatial stimuli. By contrast, pretraining in the visual or auditory domains (where multiple dimensions of a stimulus may be relevant less often naturally) encourages them to attend to a single dimension. However, data from our control experiments Exp. 2c and Exp. 3c, are incompatible with this explanation. Around ~65% of the participants show a level of performance in the multimodal association task (&gt;50%) which could only be achieved if they were attending to both dimensions (performance attending to a single dimension would yield 25% and chance performance is at 6.25%). This suggests that participants are attending to both dimensions even in the visual and auditory mapping case.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>In this manuscript, L&amp;S investigates the important general question of how humans achieve invariant behavior over stimuli belonging to one category given the widely varying input representation of those stimuli and more specifically, how they do that in arbitrary abstract domains. The authors start with the hypothesis that this is achieved by invariance transformations that observers use for interpreting different entries and furthermore, that these transformations in an arbitrary domain emerge with the help of the transformations (e.g. translation, rotation) within the spatial domain by using those as &quot;scaffolding&quot; during transformation learning. To provide the missing evidence for this hypothesis, L&amp;S used behavioral category learning studies within and across the spatial, auditory, and visual domains, where rotated and translated 4-element token sequences had to be learned to categorize and then the learned transformation had to be applied in new feature dimensions within the given domain. Through single- and multiple-day supervised training and unsupervised tests, L&amp;S demonstrated by standard computational analyses that in such setups, space and spatial transformations can, indeed, help with developing and using appropriate rotational mapping whereas the visual domain cannot fulfill such a scaffolding role.</p><p>Strengths:</p><p>The overall problem definition and the context of spatial mapping-driven solution to the problem is timely. The general design of testing the scaffolding effect across different domains is more advanced than any previous attempts clarifying the relevance of spatial coding to any other type of representational codes. Once the formulation of the general problem in a specific scientific framework is done, the following steps are clearly and logically defined and executed. The obtained results are well interpretable, and they could serve as a good stepping stone for deeper investigations. The analytical tools used for the interpretations are adequate. The paper is relatively clearly written.</p><p>Weaknesses:</p><p>Some additional effort to clarify the exact contribution of the paper, the link between analyses and the claims of the paper, and its link to previous proposals would be necessary to better assess the significance of the results and the true nature of the proposed mechanism of abstract generalization.</p><p>(1) Insufficient conceptual setup: The original theoretical proposal (the Tolman-Eichenbaum-Machine, Whittington et al., Cell 2020) that L&amp;S relate their work to proposes that just as in the case of memory for spatial navigation, humans and animals create their flexible relational memory system of any abstract representation by a conjunction code that combines on the one hand, sensory representation and on the other hand, a general structural representation or relational transformation. The TEM also suggests that the structural representation could contain any graph-interpretable spatial relations, albeit in their demonstration 2D neighbor relations were used. The goal of L&amp;S's paper is to provide behavioral evidence for this suggestion by showing that humans use representational codes that are invariant to relational transformations of non-spatial abstract stimuli and moreover, that humans obtain these invariances by developing invariance transformers with the help of available spatial transformers. To obtain such evidence, L&amp;S use the rotational transformation. However, the actual procedure they use actually solved an alternative task: instead of interrogating how humans develop generalizations in abstract spaces, they demonstrated that if one defines rotation in an abstract feature space embedded in a visual or auditory modality that is similar to the 2D space (i.e. has two independent dimensions that are clearly segregable and continuous), humans cannot learn to apply rotation of 4-piece temporal sequences in those spaces while they can do it in 2D space, and with co-associating a one-to-one mapping between locations in those feature spaces with locations in the 2D space an appropriate shaping mapping training will lead to the successful application of rotation in the given task (and in some other feature spaces in the given domain). While this is an interesting and challenging demonstration, it does not shed light on how humans learn and generalize, only that humans CAN do learning and generalization in this, highly constrained scenario. This result is a demonstration of how a stepwise learning regiment can make use of one structure for mapping a complex input into a desired output. The results neither clarify how generalizations would develop in abstract spaces nor the question of whether this generalization uses transformations developed in the abstract space. The specific training procedure ensures success in the presented experiments but the availability and feasibility of an equivalent procedure in a natural setting is a crucial part of validating the original claim and that has not been done in the paper.</p></disp-quote><p>We thank the Reviewer for their detailed comments on our manuscript. We reply to the three main points in turn.</p><p>First, concerning the conceptual grounding of our work, we would point out that the TEM model (Whittington et al., 2020), however interesting, is not our theoretical starting point. Rather, as we hope the text and references make clear, we ground our work in theoretical work from the 1990/2000s proposing that space acts as a scaffold for navigating abstract spaces (such as Gärdenfors, 2000). We acknowledge that the TEM model and other experimental work on the implication of the hippocampus, the entorhinal cortex and the parietal cortex in relational transformations of nonspatial stimuli provide evidence for this general theory. However, our work is designed to test a more basic question: whether there is behavioural evidence that space scaffolds learning in the first place. To achieve this, we perform behavioural experiments with causal manipulation (spatial pre-training vs no spatial pre-training) have the potential to provide such direct evidence. This is why we claim that:</p><p>“This theory is backed up by proof-of-concept computational simulations [13], and by findings that brain regions thought to be critical for spatial cognition in mammals (such as the hippocampal-entorhinal complex and parietal cortex) exhibit neural codes that are invariant to relational transformations of nonspatial stimuli. However, whilst promising, this theory lacks direct empirical evidence. Here, we set out to provide a strong test of the idea that learning about physical space scaffolds conceptual generalisation.“</p><p>Second, we agree with the Reviewer that we do not provide an explicit model for how generalisation occurs, and how precisely space acts as a scaffold for building representations and/or applying the relevant transformations to non-spatial stimuli to solve our task. Rather, we investigate in our Exp. 2-4 which aspects of the training are necessary for rotational generalisation to happen (and conclude that a simple training with the multimodal association task is sufficient for ~20% participants). We now acknowledge in the discussion the fact that we do not provide an explicit model and leave that for future work:</p><p>“We acknowledge that our study does not provide a mechanistic model of spatial scaffolding but rather delineate which aspects of the training are necessary for generalisation to happen.”</p><p>Finally, we also agree with the Reviewer that our task is non-naturalistic. As is common in experimental research, one must sacrifice the naturalistic elements of the task in exchange for the control and the absence of prior knowledge of the participants. We have decided to mitigate as possible the prior knowledge of the participants to make sure that our task involved learning a completely new task and that the pre-training was really causing the better learning/generalisation. The effects we report are consistent across the experiments so we feel confident about them but we agree with the Reviewer that an external validation with more naturalistic stimuli/tasks would be a nice addition to this work. We have included a sentence in the discussion:</p><p>“All the effects observed in our experiments were consistent across near transfer conditions (rotation of patterns within the same feature space), and far transfer conditions (rotation of patterns within a different feature space, where features are drawn from the same modality). This shows the generality of spatial training for conceptual generalisation. We did not test transfer across modalities nor transfer in a more natural setting; we leave this for future studies.”</p><disp-quote content-type="editor-comment"><p>(2) Missing controls: The asymptotic performance in experiment 1 after training in the three tasks was quite different in the three tasks (intercepts 2.9, 1.9, 1.6 for spatial, visual, and auditory, respectively; p. 5. para. 1, Fig 2BFJ). It seems that the statement &quot;However, our main question was how participants would generalise learning to novel, rotated exemplars of the same concept.&quot; assumes that learning and generalization are independent. Wouldn't it be possible, though, that the level of generalization depends on the level of acquiring a good representation of the &quot;concept&quot; and after obtaining an adequate level of this knowledge, generalization would kick in without scaffolding? If so, a missing control is to equate the levels of asymptotic learning and see whether there is a significant difference in generalization. A related issue is that we have no information on what kind of learning in the three different domains was performed, albeit we probably suspect that in space the 2D representation was dominant while in the auditory and visual domains not so much. Thus, a second missing piece of evidence is the model-fitting results of the ⦰ condition that would show which way the original sequences were encoded (similar to Fig 2 CGK and DHL). If the reason for lower performance is not individual stimulus difficulty but the natural tendency to encode the given stimulus type by a combo of random + 1D strategy that would clarify that the result of the cross-training is, indeed, transferring the 2D-mapping strategy.</p></disp-quote><p>We agree with the Reviewer that a good further control is to equate performance during training. Thus, we have run a complementary analysis where we select only the participants that reach &gt; 90% accuracy in the last block of training in order to equate asymptotic performance after training in Exp. 1. The results (see Author response image 1) replicates the results that we report in the main text: there is a large difference between groups (relative likelihood of 1D vs. 2D models, all BF &gt; 100 in favour of a difference between the auditory and the spatial modalities, between the visual and the spatial modalities, in both near and far transfer, “decisive” evidence). We prefer not to include this figure in the paper for clarity, and because we believe this result is expected given the fact that 0/50 and 0/50 of the participants in the auditory and visual condition used a 2D strategy – thus, selecting subgroups of these participants cannot change our conclusions.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Results of Exp. 1 when selecting participants that reached &gt; 90% accuracy in the last block of training.</title><p>Captions are the same as Figure 2 of the main text.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-sa3-fig1-v1.tif"/></fig><p>Second, the Reviewer suggested that we run the model fitting analysis only on the ⦰ condition (training) in Exp. 1 to reveal whether participants use a 1D or a 2D strategy already during training. Unfortunately, we cannot provide the model fits only in the ⦰ condition in Exp. 1 because all models make the same predictions for this condition (see Fig S4). However, note that this is done by design: participants were free to apply whatever strategy they want during training; we then used the generalisation phase with the rotated stimuli precisely to reveal this strategy. Further, we do believe that the strategy used by the participants during training and the strategy during transfer are the same, partly because – starting from block #4 – participants have no idea whether the current trial is a training trial or a transfer trial, as both trial types are randomly interleaved with no cue signalling the trial type. We have made this clear in the methods:</p><p>“They subsequently performed 105 trials (with trialwise feedback) and 105 transfer trials including rotated and far transfer quadruplets (without trialwise feedback) which were presented in mixed blocks of 30 trials. Training and transfer trials were randomly interleaved, and no clue indicated whether participants were currently on a training trial or a transfer trial before feedback (or absence of feedback in case of a transfer trial).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>Pesnot Lerousseau and Summerfield aimed to explore how humans generalize abstract patterns of sensory data (concepts), focusing on whether and how spatial representations may facilitate the generalization of abstract concepts (rotational invariance). Specifically, the authors investigated whether people can recognize rotated sequences of stimuli in both spatial and nonspatial domains and whether spatial pre-training and multi-modal mapping aid in this process.</p><p>Strengths:</p><p>The study innovatively examines a relatively underexplored but interesting area of cognitive science, the potential role of spatial scaffolding in generalizing sequences. The experimental design is clever and covers different modalities (auditory, visual, spatial), utilizing a two-dimensional feature manifold. The findings are backed by strong empirical data, good data analysis, and excellent transparency (including preregistration) adding weight to the proposition that spatial cognition can aid abstract concept generalization.</p><p>Weaknesses:</p><p>The examples used to motivate the study (such as &quot;tree&quot; = oak tree, family tree, taxonomic tree) may not effectively represent the phenomena being studied, possibly confusing linguistic labels with abstract concepts. This potential confusion may also extend to doubts about the real-life applicability of the generalizations observed in the study and raises questions about the nature of the underlying mechanism being proposed.</p></disp-quote><p>We thank the Reviewer for their comments. We agree that we could have explained ore clearly enough how these examples motivate our study. The similarity between “oak tree” and “family tree” is not just the verbal label. Rather, it is the arrangement of the parts (nodes and branches) in a nested hierarchy. Oak trees and family trees share the same relational structure. The reason that invariance is relevant here is that the similarity in relational structure is retained under rigid body transformations such as rotation or translation. For example, an upside-down tree can still be recognised as a tree, just as a family tree can be plotted with the oldest ancestors at either top or bottom. Similarly, in our study, the quadruplets are defined by the relations between stimuli: all quadruplets use the same basic stimuli, but the categories are defined by the relations between successive stimuli. In our task, generalising means recognising that relations between stimuli are the same despite changes in the surface properties (for example in far transfer). We have clarify that in the introduction:</p><p>“For example, the concept of a “tree” implies an entity whose structure is defined by a nested hierarchy, whether this is a physical object whose parts are arranged in space (such as an oak tree in a forest) or a more abstract data structure (such as a family tree or taxonomic tree). [...] Despite great changes in the surface properties of oak trees, family trees and taxonomic trees, humans perceive them as different instances of a more abstract concept defined by the same relational structure.”</p><disp-quote content-type="editor-comment"><p>Next, the study does not explore whether scaffolding effects could be observed with other well-learned domains, leaving open the question of whether spatial representations are uniquely effective or simply one instance of a familiar 2D space, again questioning the underlying mechanism.</p></disp-quote><p>We would like to mention that Reviewer #2 had a similar comment. We agree with both Reviewers that our task is non-naturalistic. As is common in experimental research, one must sacrifice the naturalistic elements of the task in exchange for the control and the absence of prior knowledge of the participants. We have decided to mitigate as possible the prior knowledge of the participants to make sure that our task involved learning a completely new task and that the pre-training was really causing the better learning/generalisation. The effects we report are consistent across the experiments so we feel confident about them but we agree with the Reviewer that an external validation with more naturalistic stimuli/tasks would be a nice addition to this work. We have included a sentence in the discussion:</p><p>“All the effects observed in our experiments were consistent across near transfer conditions (rotation of patterns within the same feature space), and far transfer conditions (rotation of patterns within a different feature space, where features are drawn from the same modality). This shows the generality of spatial training for conceptual generalisation. We did not test transfer across modalities nor transfer in a more natural setting; we leave this for future studies.”</p><disp-quote content-type="editor-comment"><p>Further doubt on the underlying mechanism is cast by the possibility that the observed correlation between mapping task performance and the adoption of a 2D strategy may reflect general cognitive engagement rather than the spatial nature of the task. Similarly, the surprising finding that a significant number of participants benefited from spatial scaffolding without seeing spatial modalities may further raise questions about the interpretation of the scaffolding effect, pointing towards potential alternative interpretations, such as shifts in attention during learning induced by pre-training without changing underlying abstract conceptual representations.</p></disp-quote><p>The Reviewer is concerned about the fact that the spatial pre-training could benefit the participants by increasing global cognitive engagement rather than providing a scaffold for learning invariances. It is correct that the participants in the control group in Exp. 2c have poorer performances on average than participants that benefit from the spatial pre-training in Exp. 2a and 2b. The better performances of the participants in Exp. 2a and 2b could be due to either the spatial nature of the pre-training (as we claim) or a difference in general cognitive engagement. .</p><p>However, if we look closely at the results of Exp. 3, we can see that the general cognitive engagement hypothesis is not well supported by the data. Indeed, the participants in the control condition (Exp. 3c) have relatively similar performances than the other groups during training. Rather, the difference is in the strategy they use, as revealed by the transfer condition. The majority of them are using a 1D strategy, contrary to the participants that benefited from a spatial pre-training (Exp 3a and 3b). We have included a sentence in the results:</p><p>“Further, the results show that participants who did not experience spatial pre-training were still engaged in the task, but were not using the same strategy as the participants who experienced spatial pre-training (1D rather than 2D). Thus, the benefit of the spatial pre-training is not simply to increase the cognitive engagement of the participants. Rather, spatial pre-training provides a scaffold to learn rotation-invariant representation of auditory and visual concepts even when rotation is never explicitly shown during pre-training.”</p><p>Finally, Reviewer #1 had a related concern about a potential alternative explanation that involved a shift in attention. We reproduce our response here: we agree with the Reviewer that the “attention to dimensions” hypothesis is an interesting (and potentially concerning) alternative explanation. However, we believe that the results of our control experiments Exp. 2c and Exp. 3c are not compatible with this alternative explanation.</p><p>Indeed, in Exp. 2c, participants are pre-trained in the visual modality and then tested in the auditory modality. In the multimodal association task, participants have to associate the auditory stimuli and the visual stimuli: on each trial, they hear a sound and then have to click on the corresponding visual stimulus. It is necessary to pay attention to both auditory dimensions and both visual dimensions to perform well in the task. To give an example, the task might involve mapping the fundamental frequency and the amplitude modulation of the auditory stimulus to the colour and the shape of the visual stimulus, respectively. If participants pay attention to only one dimension, this would lead to a maximum of 25% accuracy on average (because they would be at chance on the other dimension, with four possible options). We observed that 30/50 participants reached an accuracy &gt; 50% in the multimodal association task in Exp. 2c. This means that we know for sure that at least 60% of the participants actually paid attention to both dimensions of the stimuli. Nevertheless, there was a clear difference between participants that received a visual pre-training (Exp. 2c) and those who received a spatial pre-training (Exp. 2a) (frequency of 1D vs 2D models between conditions, BF &gt; 100 in near transfer and far transfer). In fact, only 3/50 participants were best fit by a 2D model when vision was the pre-training modality compared to 29/50 when space was the pre-training modality. Thus, the benefit of the spatial pre-training cannot be due solely to a shift in attention toward both dimensions.</p><p>This effect was replicated in Exp. 3c. Similarly, 33/48 participants reached an accuracy &gt; 50% in the multimodal association task in Exp. 3c, meaning that we know for sure that at least 68% of the participants actually paid attention to both dimensions of the stimuli. Again, there was a clear difference between participants who received a visual pre-training (frequency of 1D vs 2D models between conditions, Exp. 3c) and those who received a spatial pre-training (Exp. 3a) (BF &gt; 100 in near transfer and far transfer).</p><p>Thus, we believe that the alternative explanation raised by the Reviewer is not supported by our data. We have added a paragraph in the discussion:</p><p>“One alternative explanation of this effect could be that the spatial pre-training encourages participants to attend to both dimensions of the non-spatial stimuli. By contrast, pretraining in the visual or auditory domains (where multiple dimensions of a stimulus may be relevant less often naturally) encourages them to attend to a single dimension. However, data from our control experiments Exp. 2c and Exp. 3c, are incompatible with this explanation. Around ~65% of the participants show a level of performance in the multimodal association task (&gt;50%) which could only be achieved if they were attending to both dimensions (performance attending to a single dimension would yield 25% and chance performance is at 6.25%). This suggests that participants are attending to both dimensions even in the visual and auditory mapping case.”</p><disp-quote content-type="editor-comment"><p>Conclusions:</p><p>The authors successfully demonstrate that spatial training can enhance the ability to generalize in nonspatial domains, particularly in recognizing rotated sequences. The results for the most part support their conclusions, showing that spatial representations can act as a scaffold for learning more abstract conceptual invariances. However, the study leaves room for further investigation into whether the observed effects are unique to spatial cognition or could be replicated with other forms of well-established knowledge, as well as further clarifications of the underlying mechanisms.</p><p>Impact:</p><p>The study's findings are likely to have a valuable impact on cognitive science, particularly in understanding how abstract concepts are learned and generalized. The methods and data can be useful for further research, especially in exploring the relationship between spatial cognition and abstract conceptualization. The insights could also be valuable for AI research, particularly in improving models that involve abstract pattern recognition and conceptual generalization.</p><p>In summary, the paper contributes valuable insights into the role of spatial cognition in learning abstract concepts, though it invites further research to explore the boundaries and specifics of this scaffolding effect.</p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Minor issues / typos:</p><p>P6: I think the example of the &quot;signed&quot; mapping here should be &quot;e.g., ABAB maps to one category and BABA maps to another&quot;, rather than &quot;ABBA maps to another&quot; (since ABBA would always map to another category, whether the mapping is signed or unsigned).</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>P11: &quot;Next, we asked whether pre-training and mapping were systematically associated with 2Dness...&quot;. I'd recommend changing to: &quot;Next, we asked whether accuracy during pre-training and mapping were systematically associated with 2Dness...&quot;, just to clarify what the analyzed variables are.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>P13, paragraph 1: &quot;only if the features were themselves are physical spatial locations&quot; either &quot;were&quot; or &quot;are&quot; should be removed.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>P13, paragraph 1: should be &quot;neural representations of space form a critical substrate&quot; (not &quot;for&quot;).</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>The authors use in multiple places in the manuscript the phrases &quot;learn invariances&quot; (Abstract), &quot;formation of invariances&quot; (p. 2, para. 1), etc. It might be just me, but this feels a bit like 'sloppy' wording: we do not learn or form invariances, rather we learn or form representations or transformations by which we can perform tasks that require invariance over particular features or transformation of the input such as the case of object recognition and size- translation- or lighting-invariance. We do not form size invariance, we have representations of objects and/or size transformations allowing the recognition of objects of different sizes. The authors might change this way of referring to the phenomenon.</p></disp-quote><p>We respectfully disagree with this comment. An invariance occurs when neurons make the same response under different stimulation patterns. The objects or features to which a neuron responds is shaped by its inputs. Those inputs are in turn determined by experience-dependent plasticity. This process is often called “representation learning”. We think that our language here is consistent with this status quo view in the field.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><list list-type="bullet"><list-item><p>I understand that the objective of the present experiment is to study our ability to generalize abstract patterns of sensory data (concepts). In the introduction, the authors present examples like the concept of a &quot;tree&quot; (encompassing a family tree, an oak tree, and a taxonomic tree) and &quot;ring&quot; to illustrate the idea. However, I am sceptical as to whether these examples effectively represent the phenomena being studied. From my perspective, these different instances of &quot;tree&quot; do not seem to relate to the same abstract concept that is translated or rotated but rather appear to share only a linguistic label. For instance, the conceptual substance of a family tree is markedly different from that of an oak tree, lacking significant overlap in meaning or structure. Thus, to me, these examples do not demonstrate invariance to transformations such as rotations.</p></list-item></list><p>To elaborate further, typically, generalization involves recognizing the same object or concept through transformations. In the case of abstract concepts, this would imply a shared abstract representation rather than a mere linguistic category. While I understand the objective of the experiments and acknowledge their potential significance, I find myself wondering about the real-world applicability and relevance of such generalizations in everyday cognitive functioning. This, in turn, casts some doubt on the broader relevance of the study's results. A more fitting example, or an explanation that addresses my concerns about the suitability of the current examples, would be beneficial to further clarify the study's intent and scope.</p></disp-quote><p>Response in the public review.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Relatedly, the manuscript could benefit from greater clarity in defining key concepts and elucidating the proposed mechanism behind the observed effects. Is it plausible that the changes observed are primarily due to shifts in attention induced by the spatial pre-training, rather than a change in the process of learning abstract conceptual invariances (i.e., modifications to the abstract representations themselves)? While the authors conclude that spatial pre-training acts as a scaffold for enhancing the learning of conceptual invariances, it raises the question: does this imply participants simply became more focused on spatial relationships during learning, or might this shift in attention represent a distinct strategy, and an alternative explanation? A more precise definition of these concepts and a clearer explanation of the authors' perspective on the mechanism underlying these effects would reduce any ambiguity in this regard.</p></list-item></list></disp-quote><p>Response in the public review.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>I am wondering whether the effectiveness of spatial representations in generalizing abstract concepts stems from their special nature or simply because they are a familiar 2D space for participants. It is well-established that memory benefits from linking items to familiar locations, a technique used in memory training (method of loci). This raises the question: Are we observing a similar effect here, where spatial dimensions are the only tested familiar 2D spaces, while the other 2 spaces are simply unfamiliar, as also suggested by the lower performance during training (Fig.2)? Would the results be replicable with another well-learned, robustly encoded domain, such as auditory dimensions for professional musicians, or is there something inherently unique about spatial representations that aids in bootstrapping abstract representations?</p></list-item></list><p>On the other side of the same coin, are spatial representations qualitatively different, or simply more efficient because they are learned more quickly and readily? This leads to the consideration that if visual pre-training and visual-to-auditory mapping were continued until a similar proficiency level as in spatial training is achieved, we might observe comparable performance in aiding generalization. Thus, the conclusion that spatial representations are a special scaffold for abstract concepts may not be exclusively due to their inherent spatial nature, but rather to the general characteristic of well-established representations. This hypothesis could be further explored by either identifying alternative 2D representations that are equally well-learned or by extending training in visual or auditory representations before proceeding with the mapping task. At the very least I believe this potential explanation should be explored in the discussion section.</p></disp-quote><p>Response in the public review.</p><disp-quote content-type="editor-comment"><p>I had some difficulty in following an important section of the introduction: &quot;... whether participants can learn rotationally invariant concepts in nonspatial domains, i.e., those that are defined by sequences of visual and auditory features (rather than by locations in physical space, defined in Cartesian or polar coordinates) is not known.&quot; This was initially puzzling to me as the paragraph preceding it mentions: &quot;There is already good evidence that nonspatial concepts are represented in a translation invariant format.&quot; While I now understand that the essential distinction here is between translation and rotation, this was not immediately apparent upon first reading. This crucial distinction, especially in the context of conceptual spaces, was not clearly established before this point in the manuscript. For better clarity, it would be beneficial to explicitly contrast and define translation versus rotation in this particular section and stress that the present study concerns rotations in abstract spaces.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>The multi-modal association is crucial for the study, however to my knowledge, it is not depicted or well explained in the main text or figures (Results section). In my opinion, the details of this task should be explained and illustrated before the details of the associated results are discussed.</p></list-item></list></disp-quote><p>We have included an illustration of a multimodal association trial in Fig. S3B.</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93636-sa3-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>The observed correlation between the mapping task performance and the adoption of a 2D strategy is logical. However, this correlation might not exclusively indicate the proposed underlying mechanism of spatial scaffolding. Could it also be reflective of more general factors like overall performance, attention levels, or the effort exerted by participants? This alternative explanation suggests that the correlation might arise from broader cognitive engagement rather than specifically from the spatial nature of the task. Addressing this possibility could strengthen the argument for the unique role of spatial representations in learning abstract concepts, or at least this alternative interpretation should be mentioned.</p></list-item></list></disp-quote><p>Response in the public review.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>To me, the finding that ~30% of participants benefited from the spatial scaffolding effect for example in the auditory condition merely through exposure to the mapping (Fig 4D), without needing to see the quadruplets in the spatial modality, was somewhat surprising. This is particularly noteworthy considering that only ~60% of participants adopted the 2D strategy with exposure to rotated contingencies in Experiment 3 (Fig 3D). How do the authors interpret this outcome? It would be interesting to understand their perspective on why such a significant effect emerged from mere exposure to the mapping task.</p></list-item></list><list list-type="bullet"><list-item><p>I appreciate the clarity Fig.1 provides in explaining a challenging experimental setup. Is it possible to provide example trials, including an illustration that shows which rotations produce the trail and an intuitive explanation that response maps onto the 1D vs 2D strategies respectively, to aid the reader in better understanding this core manipulation?</p></list-item></list><list list-type="bullet"><list-item><p>I like that the authors provide transparency by depicting individual subject's data points in their results figures (e.g. Figs. 2 B, F, J). However, with an n=~50 per condition, it becomes difficult to intuit the distribution, especially for conditions with higher variance (e.g., Auditory). The figures might be more easily interpretable with alternative methods of displaying variances, such as violin plots per data point, conventional error shading using 95%CIs, etc.</p></list-item></list><list list-type="bullet"><list-item><p>Why are the authors not reporting exact BFs in the results sections at least for the most important contrasts?</p></list-item></list><list list-type="bullet"><list-item><p>While I understand why the authors report the frequencies for the best model fits, this may become difficult to interpret in some sections, given the large number of reported values. Alternatives or additional summary statistics supporting inference could be beneficial.</p></list-item></list></disp-quote><p>As the Reviewer states, there are a large number of figures that we can report in this study. We have chosen to keep this number at a minimum to be as clear as possible. To illustrate the distribution of individual data points, we have opted to display only the group's mean and standard error (the standard errors are included, but the substantial number of participants per condition provides precise estimates, resulting in error bars that can be smaller than the mean point). This decision stems from our concern that including additional details could lead to a cluttered representation with unnecessary complexity. Finally, we report what we believe to be the critical BFs for the comprehension of the reader in the main text, and choose a cutoff of 100 when BFs are high (corresponding to the label “decisive” evidence, some BFs are larger than 1012). All the exact BFs are in the supplementary for the interested readers.</p></body></sub-article></article>