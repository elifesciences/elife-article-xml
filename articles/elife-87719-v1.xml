<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">87719</article-id><article-id pub-id-type="doi">10.7554/eLife.87719</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87719.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A computationally informed comparison between the strategies of rodents and humans in visual object recognition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-196891"><name><surname>Schnell</surname><given-names>Anna Elisabeth</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9278-1817</contrib-id><email>annaelisabeth.schnell@kuleuven.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-311753"><name><surname>Leemans</surname><given-names>Maarten</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-297944"><name><surname>Vinken</surname><given-names>Kasper</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7038-9638</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-311754"><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Brain and Cognition &amp; Leuven Brain Institute</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Neurobiology, Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Arun</surname><given-names>SP</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><addr-line><named-content content-type="city">Bangalore</named-content></addr-line><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>11</day><month>12</month><year>2023</year></pub-date><volume>12</volume><elocation-id>RP87719</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-04-17"><day>17</day><month>04</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-03-15"><day>15</day><month>03</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.15.532720"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-06-27"><day>27</day><month>06</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87719.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-10-27"><day>27</day><month>10</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87719.2"/></event></pub-history><permissions><copyright-statement>Â© 2023, Schnell et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Schnell et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-87719-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-87719-figures-v1.pdf"/><abstract><p>Many species are able to recognize objects, but it has been proven difficult to pinpoint and compare how different species solve this task. Recent research suggested to combine computational and animal modelling in order to obtain a more systematic understanding of task complexity and compare strategies between species. In this study, we created a large multidimensional stimulus set and designed a visual discrimination task partially based upon modelling with a convolutional deep neural network (CNN). Experiments included rats (N = 11; 1115 daily sessions in total for all rats together) and humans (N = 45). Each species was able to master the task and generalize to a variety of new images. Nevertheless, rats and humans showed very little convergence in terms of which object pairs were associated with high and low performance, suggesting the use of different strategies. There was an interaction between species and whether stimulus pairs favoured early or late processing in a CNN. A direct comparison with CNN representations and visual feature analyses revealed that rat performance was best captured by late convolutional layers and partially by visual features such as brightness and pixel-level similarity, while human performance related more to the higher-up fully connected layers. These findings highlight the additional value of using a computational approach for the design of object recognition tasks. Overall, this computationally informed investigation of object recognition behaviour reveals a strong discrepancy in strategies between rodent and human vision.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object recognition</kwd><kwd>deep neural network</kwd><kwd>rat vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Excellence of Science</institution></institution-wrap></funding-source><award-id>G0E8718N</award-id><principal-award-recipient><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven infrastructure grants</institution></institution-wrap></funding-source><award-id>AKUL/13/06</award-id><principal-award-recipient><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven infrastructure grants</institution></institution-wrap></funding-source><award-id>AKUL/19/05</award-id><principal-award-recipient><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven Research Council Project</institution></institution-wrap></funding-source><award-id>C14/16/031</award-id><principal-award-recipient><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven Research Council Project</institution></institution-wrap></funding-source><award-id>C14/21/047</award-id><principal-award-recipient><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Using a computational approach for designing object recognition tasks is beneficial to highlight the different strategies among different species.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans show high proficiency in invariant object recognition, the ability to recognize the same objects from different viewpoints or in different scenes. This ability is supported by the ventral visual stream, the so-called <italic>what stream</italic> (<xref ref-type="bibr" rid="bib17">Logothetis and Sheinberg, 1996</xref>). A question that is repeatedly addressed in vision studies is whether and how we can model this stream by means of animal models or computational models to further examine and quantify the representations along the ventral visual stream. Computationally, researchers have recently modelled this stream by using convolutional deep neural networks (CNNs), as, for example, done by <xref ref-type="bibr" rid="bib2">AvberÅ¡ek et al., 2021</xref>, <xref ref-type="bibr" rid="bib4">Cadieu et al., 2014</xref>, <xref ref-type="bibr" rid="bib9">Duyck et al., 2021</xref>, <xref ref-type="bibr" rid="bib11">GÃ¼Ã§lÃ¼ and van Gerven, 2015</xref>, <xref ref-type="bibr" rid="bib12">Kalfas et al., 2018</xref>, <xref ref-type="bibr" rid="bib13">Kar et al., 2019</xref>, <xref ref-type="bibr" rid="bib16">Kubilius et al., 2016</xref>, <xref ref-type="bibr" rid="bib27">Pospisil et al., 2018</xref>, and <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>. Lately, the rodent model has become an important animal model in vision studies, motivated by the applicability of molecular and genetic tools rather than by the visual capabilities of rodents. Past studies have examined behavioural (<xref ref-type="bibr" rid="bib1">Alemi-Neissi et al., 2013</xref>; <xref ref-type="bibr" rid="bib7">De Keyser et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Djurdjevic et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Schnell et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Tafazoli et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Vermaercke and Op de Beeck, 2012</xref>; <xref ref-type="bibr" rid="bib35">Vinken et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Zoccolan, 2015</xref>; for a review, see <xref ref-type="bibr" rid="bib41">Zoccolan, 2015</xref>) as well as neural (<xref ref-type="bibr" rid="bib18">Matteucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Tafazoli et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Vermaercke et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Vinken et al., 2016</xref>) data of rodents (rats and mice) performing in visual pattern recognition tasks. The behavioural findings suggested that rats are capable of learning complex visual discrimination tasks. Here we plan to integrate computational and animal modelling approaches by using data about information processing in artificial neural networks when designing the animal experiments.</p><p>One aspect that almost all rodent studies have in common is that the exact task and stimuli are chosen based on what we know from human and monkey studies. Earlier research showed that the intuition of researchers about the complexity of visual tasks can be misleading (<xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>). Through computational CNN modelling of the tasks from previous studies, they showed that behavioural strategies that seem complex at first hand might be best modelled through relatively early levels of processing in CNNs. They recommended that future studies could obtain more direct information about the complexity of visual tasks and behavioural strategies by incorporating neural network models in the design phase of the experiment. One way of implementing this is to train rodents in a challenging and multidimensional visual task and use CNNs to select stimulus examples targeting strategies with different levels of complexity.</p><p>In this study, we implemented this approach and created a large stimulus set that can be used for a variety of visual experiments. We decided to create the stimuli in a way that they are adaptable to different types of tasks, such as a âsimpleâ discrimination task or non-linear tasks (e.g. <xref ref-type="bibr" rid="bib3">Bossens and Op de Beeck, 2016</xref>). We then took a subset of these stimuli and performed a visual discrimination experiment in rats (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for the design). The task itself was defined in a stimulus space with two dimensions, here referred to as concavity and alignment. The stimuli consisted of a base shape that varied in concavity, with three spheres attached to it that were either horizontally aligned or misaligned. The task was then further complicated by transforming the stimuli along several dimensions that preserve the identity of the object. We started by training the animals in a base stimulus pair, with the target being the concave object with horizontally aligned spheres. Once the animals were trained in this base stimulus pair, we used the identity-preserving transformations to test for generalization. After a number of transformation phases, we selected a final stimulus set by choosing a combination of transformations based on the outcomes of a trained CNN. Using the neural network as a (basic) model for the different stages of ventral visual stream processing, we chose stimulus pairs that require either higher or lower levels of processing and thus allow us to maximally differentiate between the task strategies used by the animals. As a final part of this study, we performed an online human experiment with the same stimuli and design as the experiment for the rats, providing us with a rich three-way comparison of rat behavioural data with human behavioural data and with CNN data.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The design of the animal study, including the stimuli.</title><p>Animals started with a standardized shaping procedure, followed by three training protocols, as indicated by the dashed outline. In these protocols, animals received real reward, that is, reward for touching the target. The target corresponds to the concave object in all training protocols. The rats received correction trials for incorrect answers, that is, touching the convex object. After the three training protocols, the animals went through a number of testing protocols. The order of the first six protocols (*) and the last two testing protocols (**) was counterbalanced between the animals. During testing protocols, animals received one-third old trials and two-third new trials. In the new trials, they received random reward in 80% of the trials, whereas in the old trials, they received real reward and correction trials if necessary. Again, the target in the testing protocols correspond to the concave objects, whereas the distractors correspond to the convex objects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 1.</label><caption><title>Design of the online human study.</title><p>The design of the human study resembles the design of the animal study, with minor adaptations. We removed the one-third old trials in the test protocols but included two additional dimension learning protocols.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 2.</label><caption><title>Trial and experimental setup information.</title><p>Timeline of a correct trial (<bold>a</bold>) and an incorrect trial (<bold>b</bold>). In case of a correct trial, animals receive a reward in the form of sugar pellets for touching the target. If they answer incorrectly, they do not receive sugar pellets but instead the house light is turned on. For training protocols, a correction trial is added. (<bold>c</bold>) shows images of the experimental setup.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 3.</label><caption><title>Design of the pilot study.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig1-figsupp3-v1.tif"/></fig></fig-group></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this study, we trained and tested 11 rats and 45 humans on a complex two-dimensional discrimination task (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for the design of the rat study and <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref> for the design of the human study). Rats and humans were first trained in a base pair. Next, we tested their ability to generalize across several image transformations. In the last two protocols of the design, we used a computational approach to select stimuli that require different visual strategies.</p><sec id="s2-1"><title>Animal study</title><sec id="s2-1-1"><title>Training</title><p>We first checked the variation in performance across phases and stimulus pairs during training. In the first <italic>training phase</italic>, animals were trained in the base stimulus pair, which were the maximally different target and distractor in a concavity Ã alignment stimulus space where each dimension was varied with four values (4 Ã 4 space). This training was successful for all 12 animals and lasted on average for 8.62 sessions (SD = 1.61). Animals were trained until they reached 80% performance for two consecutive sessions.</p><p>Once the animals were successfully trained, we examined whether they use both dimensions (concavity and alignment) by presenting them with two additional stimuli pairs where the target and distractor differ in only one dimension (see <xref ref-type="fig" rid="fig2">Figure 2</xref>, <italic>dimension learning</italic>). Performance on the old pair was similar to training performance (85.83%). The animals performed well with the stimuli that differ only along the concavity dimension (78.79%), although it was significantly lower than the performance on the base pair (paired <italic>t</italic>-test on rat performance, <italic>t</italic>(11) = 3.77, p=0.003). Performance dropped to 67.83% for the alignment-only pair, yet also significantly higher than chance level (one-sample <italic>t</italic>-test, p&lt;0.0001). Overall, the <italic>dimension learning</italic> protocol provides evidence that the animals have picked up each of the two dimensions. This finding already excludes trivial explanations in terms of simple visual dimensions. For example, while concavity is correlated with horizontal size (distractor wider) and with overall brightness (distractor brighter, thus the opposite relevance as in the shaping phase), these simple dimensions cannot explain above-chance performance on the alignment dimension.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Results of the Dimension learning and Transformations training protocol.</title><p>(<bold>a</bold>) Results of the dimension learning training protocol. The black dashed horizontal line indicates chance-level performance and the red dashed line represents the 80% performance threshold. The blue circles on top of each bar represent individual rat performances. The three bars represent the average performance of all animals on the old pair (Old), the pair that differs only in concavity (Conc), and on the pair that differs only in alignment (Align). (<bold>b</bold>) Results of the transformations training protocol. Each cell of the matrix indicates the average performance per stimulus pair, pooled over all animals. The columns represent the distractors, whereas the rows separate the targets. The colour bar indicates the performance correct. Testing across transformations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig2-v1.tif"/></fig><p>The third training protocol consisted of a number of small transformations, as visualized in <xref ref-type="fig" rid="fig1">Figure 1</xref> (<italic>transformations</italic>). Rats learned these transformations very well, with an average performance of 83.05% (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). The pairwise percentage matrix in <xref ref-type="fig" rid="fig2">Figure 2</xref> shows that the distractor with the size transformation (rightmost column in the matrix) affected the rat performance the most.</p><p>The variation in performance across targets and distractors can be due to a variety of factors. This can include simple dimensions such as brightness. In the base pair, the distractor is brighter than the target. While this is the opposite from the shaping task of detecting a shape versus a black screen, visual inspection of <xref ref-type="fig" rid="fig2">Figure 2</xref> suggests that the animals perform poorer on trials in which the distractor display is not so much brighter (e.g. when it is small). To quantify this effect of brightness, we calculated the correlation between the performances in the matrix and the difference in pixel values (and thus brightness) of the stimulus pairs. This resulted in a (Pearson) correlation of â0.59 (p&lt;0.01), suggesting that there is indeed an effect of brightness. Yet, brightness is at best a partial explanation because all percentages in the matrix are above chance, with the lowest percentage in the matrix being 68.83%, even though in some pairs the difference in pixel values is abolished or even opposite from the base pair.</p><p>Overall, the findings from the training phase and the above-chance performance on a variety of dimensions and transformations suggest that the rats have learned a pattern classification task with a level of complexity that might be competitive with other tasks in the rodent literature.</p><p>The six protocols that test generalization to various transformations with new, untrained images are associated with performances lower than 80% (binomial test, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref> [lower table] for detailed table with results), but significantly higher than chance level (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref> [lower table]). The pairwise percentage matrices of the animals in <xref ref-type="fig" rid="fig3">Figure 3</xref> provide a more detailed view of what is happening in every test, and <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref> shows the individual accuracy for each animal. The distractor has a higher impact on performance than the target in some tests. <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref> shows the marginal means and standard deviation for each target and distractor for these two test protocols. From these means it is clear that there is a higher variation in the performance between distractors in <italic>rotation X</italic> (52â65%) and <italic>rotation Z</italic> (56â73%) than between targets (55â60% resp. 60â66%). The same happens in the size test protocol.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Pairwise percentage matrices of all nine testing protocols for the rat data.</title><p>The colour bar indicates the percentage correct of the pooled responses of all animals together. The redder a cell is, the higher the average performance. Values below 40% accuracy are indicated in the highest intensity of blue. Cells with an âoâ marker indicate a below-chance performance, whereas cells with an *, **, or *** marker indicate a performance that is significantly higher than chance level (p-value &lt;0.05, &lt;0.01, or &lt;0.001 respectively). This was calculated with a binomial test on pooled performance of all animals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Individual rat accuracy for each testing protocol.</title><p>The black circle indicates the average accuracy across all rats together. Each coloured cross represents the accuracy of one rat.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig3-figsupp1-v1.tif"/></fig></fig-group><p>After these first six test protocols, the animals were presented with a schedule where all three rotations are combined (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). On the new stimuli, the animals performed 58.56%, which is rather low, but still significantly different from chance level (binomial test on pooled performance of all animals: p&lt;0.0001; 95% CI [0.57;0.60]).</p></sec><sec id="s2-1-2"><title>Testing computational levels of complexity</title><p>For the final two test protocols, we used a CNN to find image pairs that would contrast strategies based upon a different stage in visual processing, with either early layers having lower performance than high layers (<italic>zero</italic> vs<italic>. high</italic>), or early layers having better performance than high layers (<italic>high</italic> vs<italic>. zero</italic>). Rat performance was particularly low for <italic>zero</italic> vs<italic>. high</italic> (56.47%), yet still significantly different from chance level when averaged across all stimulus pairs (binomial test on pooled performance of all animals; p&lt;0.0001; 95% CI [0.55;0.58]). In contrast, rats were able to solve the <italic>high</italic> vs<italic>. zero</italic> pairs not only better than chance (average: 64.84%; binomial test on pooled performance of all animals; p&lt;0.0001; 95% CI [0.63;0.66]), but also significantly better than <italic>zero</italic> vs<italic>. high</italic> (paired <italic>t</italic>-test on rat performance, <italic>t</italic>(10) = â4.49, p=0.0012). This suggests that rats align with lower levels of processing when we purposely select image pairs that are optimized to contrast different levels of the visual processing hierarchy.</p><p>Next we checked how much individual CNN layers can predict the variation in behavioural performance across image pairs when we take all test protocols together. We calculated the correlation of the generalization across image pairs between the CNN classifier (summarized in Figure 8) and the rat performance of all nine test protocols. This correlation includes a total of 287 image pairs, that is, all image pairs of all nine test protocols together. We did this by concatenating all performances of the animals into one array and all classification scores of the network into another array, and calculating the correlation between these two arrays to retrieve a correlation for each network layer. The results are displayed in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Overall, we see quite low correlations, but several convolutional layers nevertheless show a significant positive correlation (permutation test) with the behavioural pattern of performance at the image pair level.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Correlation of the classification score for single target/distractor pairs between single convolutional deep neural network (CNN) layers and the rat performance for all nine test protocols together.</title><p>The black and grey horizontal lines on the x-axis indicate the layer blocks (block 1 consisting of conv1, norm1, pool1; block 2 consisting of conv2, norm2, pool2; block 3â4 corresponding to conv3-4, respectively; block 5 consisting of conv5, pool5; block 6-7-8 corresponding to fc6-7-8, respectively). The vertical grey dashed line indicates the division between convolutional and fully connected layer blocks. The horizontal dashed line indicates a correlation of 0. The different markers indicate different sorts of layers: circle for convolutional layers, triangle for normalization layers, point for pool layers, and squares for fully connected layers. The asterisks indicate significant correlations according to a permutation test (*&lt;0.05, **&lt;0.01, and ***&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig4-v1.tif"/></fig><p>Even though some of the correlations are significant, they are low. This could indicate that no CNN layer is able to capture what rats do. Alternatively, it could be caused by a very low reliability of the behavioural data. To test the reliability of the variations in behavioural performance between stimulus pairs in all nine test protocols, we calculated the split-half reliability, as previously done in <xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref>, resulting in a correlation of 0.40. By applying the SpearmanâBrown correction, we obtain a full-set reliability correlation of 0.58. This correlation is much higher than the correlations with individual CNN layers.</p><p>It is possible that rat performance would be based upon multiple levels of processing, in which case we would need a combination of layers in order to explain the variation in performance across stimulus pairs. Given the low correlation between neighbouring layers (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1c</xref>), a multiple linear regression was calculated with the classification scores of the 13 layers as 13 regressors, and the rat performances as response vector. The results of this regression indicate a significant effect of the classification scores (<italic>F</italic>(287,273) = 2.22, p=0.00907, <italic>R</italic><sup>2</sup> = 0.10). Further investigating the 13 predictors showed that the later convolutional layers 8â10 of the network were significant predictors in the regression model (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1d</xref> for results of the regression model). The <italic>R</italic><sup>2</sup> = 10 of the full model would correspond to a correlation of around 0.32. This is better than the correlation of single layers, but still clearly smaller than the reliability of the rat data of 0.58. In conclusion, the CNN model provides a partial explanation of how the performance of rats varies across image pairs.</p><p>Given the relevance of convolutional layers, we can expect that relatively basic visual features might partially explain the behavioural strategy of rats. This includes dimensions such as brightness and pixel-based similarity. To get a first indication of the relevance of these features, we calculated the correlation across image pairs between rat performance and brightness and pixel similarity. Here we found a correlation of 0.34 for pixel similarity and 0.39 for brightness, suggesting that these two visual features partially explain our results when compared to the full-set reliability of rat performance (0.58).</p></sec></sec><sec id="s2-2"><title>Human study</title><p>A final part of this study was to include an online human study that follows the same design as the animal part. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the average performance of humans (dark blue) versus rats (light blue) for all nine test protocols, as well as their performance on the old stimuli that were added in (or during) the testing protocols as quality control. Overall, humans performed better on all tests protocols than rats, with an average performance over all tests of 94.34% (humans) and 62.29% (rats). There was already a difference in terms of training performance (humans 92.86% vs. rats 77.84%), but the difference on the test protocols is larger. We subtracted the training performance of humans or rats from the testing performance of humans or rats, respectively, and even with this normalization for training performance there is still a significantly higher test performance in humans compared to rats (<italic>t</italic>(16) = â6.47, p&lt;0.0001). Thus, not surprisingly, the degree of invariance in this object classification task is higher for humans compared to rat.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Average performance of humans versus rats.</title><p>On the x-axis, the nine test protocols in addition to the performance on all old stimuli are presented in the following order: rotation X (RotX), rotation Y (RotY), rotation Z (RotZ), size, position (Pos), light location (LL), combination rotation (CR), zero vs. high (ZVH), high vs. zero (HVZ), and all old. The dashed horizontal line indicates chance level. The error bars indicate standard error over humans/rats (N = 45 for humans, N = 11 for rats).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig5-v1.tif"/></fig><p>The variation in performance across test protocols and across image pairs can give an indication of the strategies that each species follows. Overall, humans and rats show a mild correspondence in terms of which image pairs are more difficult, with a humanârat correlation of 0.18 across all image pairs of the nine test protocols (p&lt;0.001 with permutation test). Albeit significant, this correlation is clearly lower than the maximum value that could be obtained given the reliability of the data. The split-half reliability of the human data was 0.46, corresponding to a full-set reliability of 0.63. We reported above that full-set reliability is 0.58 for the rat data, resulting in a combined reliability of 0.60 (calculated as described in <xref ref-type="bibr" rid="bib23">Op de Beeck et al., 2008</xref>). Thus, after taking data reliability into account there remains a pronounced discrepancy between rats and humans in terms of how performance varies across image pairs.</p><p>The main question of this study is how this discrepancy relates to computationally informed strategies. If we take a closer look specifically at the two CNN-informed test protocols (<italic>zero</italic> vs<italic>. high</italic> and <italic>high</italic> vs<italic>. zero</italic>), we see an opposite behaviour between animals and humans. Humans performed significantly better in the <italic>zero</italic> vs<italic>. high</italic> protocol, that is, where we used stimuli where the earlier layers of the network perform worse than the higher layers, than in the <italic>high</italic> vs<italic>. zero</italic> protocol (paired <italic>t</italic>-test: <italic>t</italic>(44) = 2.85, p=0.0067). Rats, however, show the opposite (see above for statistics). There even is a significant interaction between species and test protocol (unpaired <italic>t</italic>-test: <italic>t</italic>(54) = 2.50, p=0.016). This suggests a different strategy between animals and humans: rats use strategies that are captured in the lower layers of the network, and thus correspond more to low-level visual processing. Humans, however, tend to rely more on strategies captured by the higher layers of the network, and thus we are looking at more high-level visual processing.</p><p>As a next step, we calculated the correlation between the generalization across image pairs between the CNN classifier and the human performance of all nine test protocols in an identical manner as for the rat performance (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The results are displayed in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Overall, we see quite high correlations, especially in the higher layers. This pattern across layers is very different from the pattern in rats where the highest layers showed no correlations, which again suggests that, despite successful generalization, rats rely on decisively lower-level strategies than humans in the same discrimination task.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Correlation of the classification score for single target/distractor pairs between single convolutional deep neural network (CNN) layers and the human performance for all nine test protocols together.</title><p>The naming convention on the x-axis corresponds to the layers of the network, identical as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The black and grey horizontal lines on the x-axis indicate the layer blocks (block 1 consisting of conv1, norm1, pool1; block 2 consisting of conv2, norm2, pool2; block 3â4 corresponding to conv3-4, respectively; block 5 consisting of conv5, pool5; block 6-7-8 corresponding to fc6-7-8, respectively). The vertical grey dashed line indicates the division between convolutional and fully connected layer blocks. The horizontal dashed line indicates a correlation of 0. The different markers indicate different sorts of layers: circle for convolutional layers, triangle for normalization layers, point for pool layers, and squares for fully connected layers. The asterisks indicate significant correlations according to a permutation test (*&lt;0.05, **&lt;0.01, and ***&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig6-v1.tif"/></fig><p>A multiple linear regression was calculated in an identical manner as we did with the rat performance. The results of this regression indicate a significant effect of the classification scores (<italic>F</italic>(287,273) = 6.8, p&lt;0.0001, <italic>R</italic><sup>2</sup> = 0.25). Further investigating the 13 predictors showed that in particular the fully connected layers 11â13 of the network were strong predictors in the regression model (see for results <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1e</xref> of the regression model).</p><p>Given the high correlations with fully connected layers, we would expect to not find much evidence for an influence of basic visual dimensions such as brightness and pixel-based similarity. Indeed, we find small correlations with variation in human behavioural performance across image pairs for pixel similarity (0.12) and brightness (â0.12).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we trained and tested rats and humans in a discrimination task using two-dimensional stimuli, with the two dimensions being concavity and alignment. We tested generalization across a range of viewing conditions. For the last two testing protocols, we used a computational approach to select the stimuli in terms of specifically dissociating low and high stages of processing. Rats were able to learn both dimensions (concavity and alignment) and showed a preference for concavity. Their performance on the testing protocols revealed a wide variety in percentage correct: for some test protocols, they performed just above chance level, for example, <italic>zero</italic> vs<italic>. high</italic>, whereas for others they could easily reach about 70% correct (<italic>position</italic>). Humans, on the other hand, performed better overall, with performances of 80% or higher on the testing protocols. Addressing the question of the complexity of the underlying strategies, rats performed best on the test protocol designed to specifically target lower levels of processing, whereas humans performed best on the high-level processing protocol. Likewise, direct comparisons with artificial neural network layers showed that the variation of rat performance across images was best explained by late convolutional layers, whereas human performance was most associated with representations in fully connected layers.</p><p>All animals started by being trained in three training protocols. The first <italic>training</italic> protocol only included one image pair, the base pair, containing the most different target and distractor without any further transformations. Learning of the individual dimensions of concavity and alignment was investigated through the <italic>dimension learning</italic> protocol. The results from this <italic>dimension learning</italic> protocol indicate that our rats have more difficulties learning the alignment dimension as opposed to the concavity dimension. One possible explanation for the superior performance on the concavity dimension could be that the animals were partially solving the task such that the brighter stimulus, that is, the convex base shape, is the distractor and that their strategy is to pick the stimulus with the lowest brightness. This was confirmed by analyses on the third training protocol (<italic>transformations</italic>) that included small transformations along various dimensions. Nevertheless, the rats still performed above-chance level for trials in which the brightness differences were reversed, indicating that other dimensions are involved and overrule a contribution from brightness. Similar findings have been obtained in human behaviour and neuroscience. For example, despite the clear category selectivity in regions such as the fusiform face area, the selectivity in these regions is also modulated very strongly by various low-level dimensions (<xref ref-type="bibr" rid="bib38">Yue et al., 2011</xref>). With regard to the size and position transformations, it is important to keep in mind that the animals were freely moving in the touchscreen chambers, and so even for the original base pair were already undergoing changes in retinal size and retinal position. What we manipulate is rather the size and position relative to the rest of the set-up (e.g. relative to screen position and size).</p><p>After these three training protocols, the animals were tested for generalization in a variety of testing protocols, each testing a separate transformation on the stimuli. The first six test protocols included rotation along all the three axes, size, position, and light location, followed by a test protocol in which we combined the rotation along the three axes. Overall, we found that the performance of the animals on these test protocols is affected by these transformations, but still significantly above chance in each protocol. Studies in the literature would often stop here or proceed by systematically testing even larger transformations. Stimulus choices are based upon intuitions of what strategy animals might be using and upon theories of how visual perception works. However, in some cases, a further computational modelling of the task and stimuli finds that what intuitively seems like a task of a particular complexity might not be so complex after all. The first tests of invariant object recognition seemed impressive, but were found to be easily solved with earlier layers of processing (<xref ref-type="bibr" rid="bib20">Minini and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>). This was recently also highlighted by relatively simple pixel-based analyses (<xref ref-type="bibr" rid="bib14">Kell et al., 2020</xref>). As another example, <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref> have used a computational approach to further investigate the levels of information processing in rodents by comparing three hallmark studies that provided evidence for higher order visual processing in rodents (<xref ref-type="bibr" rid="bib8">Djurdjevic et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Vinken et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Zoccolan et al., 2009</xref>) with CNNs. They found that for all three studies the low- and mid-level layers captured the rat performances best, providing thus evidence against the previously concluded high-level visual processing in rodents.</p><p>For these reasons, we decided to directly test image pairs through computational modelling with CNNs and select pairs that are particularly suited for dissociating different levels of processing. Stimuli were chosen by a CNN from a very large set of possible stimuli and combinations, such that the higher layers and the lower layers of the network make distinct errors on classifying the stimuli (<italic>zero</italic> vs<italic>. high</italic> and <italic>high</italic> vs<italic>. zero</italic> protocol), and thus are diagnostic of the level of underlying visual strategies. We chose to work with Alexnet as this is a network that has been used as a benchmark in many previous studies (e.g. <xref ref-type="bibr" rid="bib4">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Groen et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Kalfas et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Nayebi et al., 2023</xref>; <xref ref-type="bibr" rid="bib39">Zeman et al., 2020</xref>), including studies that used more complex stimuli than the stimulus space in this study. The stimuli of the <italic>zero</italic> vs<italic>. high</italic> protocol included stimuli where the higher layers of the network performed better than the lower layers, and thus they address higher-level visual processing. The opposite can be said for the <italic>high</italic> vs<italic>. zero</italic> protocol, which includes stimuli that specifically target lower-level visual processing, given that the lower layers of the network perform best on these stimuli. After presenting these stimuli to the animals, we found that our rats performed best in the <italic>high</italic> vs<italic>. zero</italic> protocol, suggesting that they focus on low-level visual cues to solve this discrimination task. We found the opposite CNN pattern for humans, indicating that they use high-level visual processing. These findings provide more direct information about the level of processing that underlies the behavioural strategies compared to overall performance or to effects of image manipulations.</p><p>This is a new promising way to design experiments in a way that is computationally informed rather than based on researcher intuitions or qualitative predictions. It is in line with the literature that a typical deep neural network, AlexNet and also more complex ones, can explain human and animal behaviour to a certain extent but not fully. The explained variance might differ among CNNs, and there might be CNNs that can explain a higher proportion of rat or human behaviour. Most relevant for this study is that CNNs tend to agree in terms of how representations change from lower to higher hierarchical layers because this is the transformation that we have targeted in the zero vs. high and high vs. zero testing protocols. <xref ref-type="bibr" rid="bib26">Pinto et al., 2008</xref> already revealed that a simple V1-like model can sometimes result in surprisingly good object recognition performance. This aspect of our findings is also in line with the observation of <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref> that the performance of rats in many previous tasks might not be indicative of highly complex representations. Nevertheless, there is still a relative difference in complexity between lower and higher levels in the hierarchy. That is what we capitalize upon with the zero vs. high and high vs. zero protocols. Thus, it might be more fruitful to explicitly contrast different levels of processing in a relative way rather than trying to pinpoint behaviour to specific levels of processing.</p><p>Partially thanks to these computationally inspired tests, our total dataset finds a marked dissociation between how humans and rats solve this object recognition task. Even in the sessions where only the old pairs are shown, the animals performed lower than humans. This was most likely due to motivation and/or distractibility. Our analyses show dissociation between humans and rats most convincingly by correlating the variation in performance across image trials with the predictions of CNN layers. There were significant correlations with multiple layers in both species. In humans, the most pronounced correlations were present for the highest, fully connected layers, while in rats correlations were limited to low and middle convolutional layers. This is the most direct evidence available in the literature that rats resolve object recognition tasks through a very different and computationally simpler strategy compared to humans. The CNN approach does not inform us how we can verbalize this simpler strategy, but based upon earlier work (<xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref>; <xref ref-type="bibr" rid="bib33">Vermaercke and Op de Beeck, 2012</xref>) we would hypothesize that rats rely upon visual contrast features (e.g. this area is darker/lighter than that other area). Such contrast features are also used by humans and monkeys, for example, for face detection (<xref ref-type="bibr" rid="bib22">Ohayon et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Sinha, 2002</xref>), but in addition humans have access to more complex strategies that, for example, refer to complex shape features such as aspect ratio and symmetry (<xref ref-type="bibr" rid="bib3">Bossens and Op de Beeck, 2016</xref>). Tests in this study reveal that other features that partially explain rat performance include basic dimensions such as brightness and pixel-based similarity, the latter being a proxy for retinotopically-based computations that are expected to be present in convolutional layers.</p><p>Our analyses of rat behaviour and DNN modelling do not take into account potential trial-to-trial variability in the distance and position of the ratâs head. From earlier work we can derive that rats typically make their decision from about 12 cm from the stimulus display (<xref ref-type="bibr" rid="bib6">Crijns and Op de Beeck, 2019</xref>), but we have no information on trial-to-trial variability. We can hypothesize about the possible effect. If such variability would exist, then it would artificially increase the variation in distance and position during training, and thus help the animals achieve higher levels of invariance during testing. As a consequence, the difference between rat and human performance in terms of inferred level of processing might even increase under more controlled circumstances.</p><p>For future studies, it will be highly valuable to use this computational informed strategy on a wider battery of behavioural tasks, as well as a wider range of species such as tree shrews and marmosets (<xref ref-type="bibr" rid="bib5">Callahan and Petry, 2000</xref>; <xref ref-type="bibr" rid="bib14">Kell et al., 2020</xref>; <xref ref-type="bibr" rid="bib15">Kell et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Meyer et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Petry et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Petry and Bickford, 2019</xref>). One step further, we can use the information from computational modelling together with behaviour and how it differs among stimuli to further select stimuli for neurophysiological investigations of neuronal response properties along the visual information processing hierarchy, in this way following experimental designs that are optimized for highlighting the primary differences between processing stages and between species.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Animal study</title><sec id="s4-1-1"><title>Animals</title><p>A total of 12 male outbred LongâEvans rats (Janvier Labs, Le Genest-Saint-Isle, France) started this behavioural study. Out of these 12 animals, 2 were tested extensively in a first pilot study and were included in the remainder of the study as well. All animals were 11 weeks old at the start of shaping and were housed in groups of four per cage. Each cage was enriched with a plastic toy (Bio-Serv, Flemington, NJ), paper cage enrichment, and wooden blocks. Near the end of the experiment, one animal had to be excluded because of health issues. During training and testing, the animals were food restricted to maintain a body weight between 85 and 90% of their underprived body weight. They received water ad libitum. All experiments and procedures involving living animals were approved by the Ethical Committee of the University of Leuven and were in accordance with the European Commission Directive of September 22, 2010 (2010/63/EU).</p></sec><sec id="s4-1-2"><title>Setup</title><p>The setup is identical to the one used by <xref ref-type="bibr" rid="bib28">Schnell et al., 2019</xref> and <xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref>. A short description will follow here. The animals were trained and tested in four automated touchscreen rat-testing chambers (Campden Instruments, Ltd., Leicester, UK) with ABET II controller software (v2.18, WhiskerServer v4.5.0). The animals performed one session per day and each session lasted for 100 trials or 60 min, whichever came first. A reward tray in which sugar pellets (45 mg sucrose pellets, TestDiet, St. Louis, MO) could be delivered was installed on one side of the chamber. On the other side of the chamber, an infrared touchscreen monitor was installed. This monitor was covered with a black Perspex mask containing two square response windows (10.0 Ã 10.0 cm). A shelf (5.4 cm wide) was installed onto this black mask (16.5 cm above the floor) to force the animals to attend to the stimuli and view the stimuli within their central visual fields. Close proximity to the screen was enough to elicit a response because the screens are infrared. As the position of the rats in the touchscreen setup is not fixed, the actual size and position of the stimuli might vary in retinal coordinates. In a previous study, we manipulated the cycles per degree of stimuli in an orientation discrimination task and estimated that the decision distance of rats in this setup lies around 12.5 cm from the screen (<xref ref-type="bibr" rid="bib6">Crijns and Op de Beeck, 2019</xref>). <xref ref-type="fig" rid="fig1s2">Figure 1âfigure supplement 2</xref> shows the timeline graphic of a correct and incorrect trial as well as images of the experimental setup.</p></sec><sec id="s4-1-3"><title>Stimuli</title><p>Stimuli were created using the Python scripting implementation of the 3D modelling software Blender 3D (version 2.93.3) and measured 100 Ã 100 pixel. In general, the stimuli were objects that consisted of a body (base) with three spheres attached to it. A first step was to alter two dimensions of the object, namely, the concavity of the base and the alignment of the three spheres. The base was made either concave or convex by increasing (convex) or decreasing (concave) the base parameter. The alignment of the spheres was altered by changing the placement of the left and the right spheres. These spheres could either be horizontally aligned or misaligned. In the misaligned case, the spheres were placed diagonally from upper left to lower right. <xref ref-type="fig" rid="fig7">Figure 7a</xref> shows two example stimuli, the ones that later were selected as the so-called âbase pairâ. Next, additional exemplars were created by uniformly tiling the two-dimensional stimulus space between these two example stimuli. We decided to create 11 levels of the concavity dimension and 4 levels of alignment. This already yields 44 stimuli (see <xref ref-type="fig" rid="fig7s1">Figure 7âfigure supplement 1</xref>). We chose these levels of concavity and alignment based on the pixel dissimilarity of the stimuli (see <xref ref-type="fig" rid="fig7s2">Figure 7âfigure supplement 2</xref>). The final goal was to construct a 4 Ã 4 stimulus grid (<xref ref-type="fig" rid="fig7">Figure 7b</xref>) by selecting a subset of the 4 Ã 11 stimulus grid. We chose a large number of concavity levels as this ensures flexibility in the calibration of the two dimensions relative to each other.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Illustration of the base pair and our stimulus grid.</title><p>(<bold>a</bold>) The base pair of the main experiment. (<bold>b</bold>) The chosen 4 Ã 4 stimulus grid. The red diagonal dotted line indicates the ambiguous stimuli that can be seen as target as well as distractor. All stimuli below this line (green triangle) indicate the distractor sub-grid, whereas all stimuli above this line (yellow triangle) highlight the target sub-grid.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7âfigure supplement 1.</label><caption><title>Illustration of the 4 Ã 11 stimulus grid.</title><p>We chose 11 values for concavity, ranging between â0.5 and 0.8, and 4 values for alignment, ranging between 0 and 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7âfigure supplement 2.</label><caption><title>The pixel dissimilarity matrix of the 4 Ã 11 stimulus grid.</title><p>This matrix is calculated in the exact same way as <xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref>. The colour bar indicates pixel dissimilarity values, normalized between 0 and 100. The higher this value, and thus the redder a cell, the more dissimilar two stimuli are. The naming convention of the stimuli is as follows: XX-alYYsmZZZ.png, where XX is an ordering number, YY indicates the alignment level, and ZZZ indicates the concavity (smoothness) level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7âfigure supplement 3.</label><caption><title>Identity-preserving transformations on one of the basic stimuli.</title><p>The first three rows show rotation along the x-axis, y-axis, and z-axis, respectively, always in angles of 30Â°, 60Â°, 90Â°, 120Â°, 150Â°, and 180Â°. The fourth row illustrates the light location transformation. The last row shows some combinations of transformations, with different values for the alignment and concavity parameter.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig7-figsupp3-v1.tif"/></fig></fig-group><p>We added identity-preserving transformations to the stimuli, such as rotation among the x-axis, y-axis, and z-axis in six different angles (0Â°â180Â° in steps of 30Â°), as well as changing the light location (left, under, up, right, front) and finally the size and position. The latter two transformations were implemented using Python (3.7.3). Excluding the size and position transformation, these transformations resulted in a total set of 75,460 stimuli (4 [alignment] * 11 [concavity] * 7 [x-axis rotation] * 7 [y-axis rotation] * 7 [z-axis rotation] * 5 [light location] = 75,460 stimuli). <xref ref-type="fig" rid="fig7s3">Figure 7âfigure supplement 3</xref> shows examples of these transformations, and <xref ref-type="fig" rid="fig1">Figure 1</xref> shows an overview of all image pairs that were used in this study.</p></sec><sec id="s4-1-4"><title>Protocols</title><p>Once the pilot was finished (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for details), we set up the experiment and chose our stimuli. We started by reducing the 4 Ã 11 stimulus grid to a 4 Ã 4 stimulus grid (see <xref ref-type="fig" rid="fig7">Figure 7b</xref>). All stimuli on the diagonal can be seen as ambiguous stimuli (four stimuli in total) as they can be identified as a target as well as a distractor. The six stimuli above this diagonal create the target part of the grid, and the six stimuli below this diagonal resemble the distractor sub-grid.</p><p>The different phases of the experiment are shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, and this figure shows all stimuli that were used. In the main training phase, we trained the animals in the maximally different stimuli that are placed at the very ends of the corners (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). We refer to this as the base pair. After this training phase, the experiment consisted of two further training protocols. In the dimension learning training phase, we pushed the animals to learn both dimensions (concavity and alignment) by presenting them two additional stimuli pairs from <xref ref-type="fig" rid="fig7">Figure 7b</xref> in which the target and distractor differ in only one dimension. A third training protocol (<italic>transformations</italic>) consisted of stimuli with some small transformations, such as 30Â° rotation along the x-axis, 30Â° rotation along the y-axis, 30Â° rotation along the z-axis, light location below, and size reduction of 80%, resulting in a total of 25 possible stimulus pairs (every combination of targetâdistractor with the five transformed stimuli). During these two training protocols, one-third of the trials were so-called âold trialsâ with the base pair. Correction trials were given if an animal answered incorrectly, that is, the same trial was repeated until the animal answered correctly. These correction trials were excluded from the analyses. In all trials, rats received a reward for touching the correct screen, that is, the screen with the target.</p><p>After these three training protocols, the testing part of the experiment included nine test protocols. The crucial defining difference between these test protocols and the prior training protocols is that rats received a reward randomly in 80% of the trials with new stimulus pairs, and no correction trials were given for an incorrect response. This random reward is important to keep the animals motivated during the testing protocols and measure real generalization, and not training behaviour. We have used a similar approach in the past, where we rewarded the animals in every testing trial (<xref ref-type="bibr" rid="bib28">Schnell et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Vinken et al., 2014</xref>). One-third of the trials in all test protocols consisted of old trials with the base pair, and here, the animals received reward for touching the target and correction trials were shown if necessary. Regularly, we inserted a <italic>dimension learning</italic> session in between two test sessions to maintain the performance high enough on training stimuli, especially for the animals in which we saw a drop in performance on the base pair. We excluded any test sessions where the performance on the base pair stimuli dropped to below 65% and the performance on this base pair was not included in the accuracy calculations.</p><p>The first six test protocols included one protocol for each transformation, that is, <italic>rotation X, rotation Y, rotation Z, light location, size,</italic> and <italic>position</italic>. The order in which these first six test protocols were given to the animals was counterbalanced between the animals. The stimuli that were used in these six test protocols can be seen in <xref ref-type="fig" rid="fig1">Figure 1</xref>, and every combination of targetâdistractor per test protocol was presented to the animals. For the rotation protocols, we used rotation degrees in steps of 30Â°, ranging from 30Â° to 180Â°. This resulted in 36 possible stimulus pairs for each of the three rotation protocols. In the <italic>light location</italic> protocol, we used stimuli where the light location was set at four different positions (below, left, right, and up), resulting in 16 possible stimulus pairs for this protocol. In the <italic>size</italic> protocol, we selected targets and distractors that were 80 and 60% reduced in size compared to the original, training pair. This protocol included four possible stimulus pairs. And finally, in the <italic>position</italic> protocol, we changed the position of the 80% reduced in size stimuli and placed the objects in the lower-left corner, lower-right corner, centre, upper-left corner, and upper-right corner. We have a total of 25 possible stimulus pairs for this protocol.</p><p>After these six test protocols, we presented the animals with six targets and six distractors where all three rotations were combined (<italic>combination rotation</italic>), that is, x-, y-, and z-axis were rotated with the same degree (ranging from 30Â° to 180Â°, in steps of 30Â°). This resulted in a total of 36 new stimulus pairs. Again, no correction trials were included after the trials where rotated stimuli were shown and animals received random reward in 80% of the trials. One-third of the trials consisted of the stimulus pair from the first <italic>training</italic> phase (i.e. the base pair), and here, correction trials were given after an incorrect response and real reward was given to the animals.</p><p>In a final set of two test protocols, we created a CNN-informed stimulus set. The details of the computational modelling are explained in the next section. The first protocol (<italic>zero</italic> vs<italic>. high</italic>) included stimuli in which the lower layers of the network performed around chance level (i.e. targetâdistractor difference in classification scores [difference in signed distance to hyperplane] of about 0), whereas the higher layers scored high (see section âComputational modellingâ). The second protocol (<italic>high</italic> vs<italic>. zero</italic>) included stimuli where the network did the opposite. That is, the earlier layers performed well, whereas the higher layers performed around chance level. The order of the two test protocols was counterbalanced between the animals. Each of these test protocols included 7 targets and 7 distractors, giving a total of 49 new stimulus pairs.</p><p>Animals stayed in each session for 60 min or until they reached 100 training trials or 120 testing trials. We used an intertrial interval (ITI) of 20 s and a timeout of 5 s during training sessions. This timeout was only used in incorrect trials. From another pilot study in the lab, we noticed we could decrease the ITI and timeout without affecting the ratsâ performance. Therefore, we decided to use an ITI of 15 s and timeout of 3 s during testing, and to increase the number of trials during a testing session to 120 trials. The stimuli remained on the screen until the animals made a choice and so there was no time limit for the animals.</p><p>Each protocol was run for multiple sessions per animal. Given that we were interested in how performance would vary across stimulus pairs, we completed more sessions for the protocols that included more stimulus pairs. <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1f</xref> indicates the average number of trials per test protocol for all rats together.</p><p>One animal was not placed in the <italic>transformations</italic> phase as it was the slowest animal during training. However, its performance on the test protocols did not significantly differ from the other animals. We tested this by calculating the correlation of the variation of performance across stimulus pairs for each rat with the pooled responses of all other rats. The average correlation for each of the other animals with the pooled response was 0.24 (Â± 0.09), and the correlation of this slowest animal with the others was very similar, 0.23.</p><p>To further examine the visual features that could explain rat performance, we calculated the correlation between the rat performances and image brightness of the transformations. We did this by calculating the difference in brightness of the base pair (brightness base target â brightness base distractor), and subtracting the difference in brightness of every test targetâdistractor pair for each test protocol (brightness test target â brightness test distractor for each test pair). We then correlated these 287 brightness values (one for each test image pair) with the average rat performance for each test image pair. We performed a similar correlation analysis for pixel similarity to investigate the correlation between pixel similarity of the test stimuli in relation to the base stimuli with the average performance of the animals on all nine test protocols. We did this by calculating the pixel similarity between the base target with every other testing distractor (A), the pixel similarity between the base target with every other testing target (B), the pixel similarity between the base distractor with every other testing distractor (C), and the pixel similarity between the base distractor with every other testing target (D). For each test image pair, we then calculated the average of (A) and (D), and subtracted the average of (C) and (B) from it. We correlated these 287 values (one for each image pair) with the average rat performance on all test image pairs.</p></sec></sec><sec id="s4-2"><title>Computational modelling</title><p>One important goal of this study was to create a CNN-informed stimulus set to present to the animals. To do so, we followed the steps of <xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref> and <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref> to train a CNN on the same stimuli on which our animals were trained. The steps of training the network are identical to <xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref>, and a short description will follow here. We used the standard AlexNet CNN architecture that was pre-trained on ImageNet to classify images into 1000 object categories (MATLAB 2021b Deep Learning Toolbox). Following <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>, we applied principal component analysis to calculate the activations in every layer to standardize the values across inputs and reduce the dimensionality. We then trained a linear support vector machine classifier by using the MATLAB function fitclinear, with limited-memory BFGS solver and default regularization. We performed this with the standardized DNN layer activations in the principal component space as inputs, before ReLU, to our 24 training stimuli (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), that is, all stimuli of the <italic>training, dimension</italic> learning, and <italic>transformations</italic> protocols. The layers of AlexNet were divided into 13 sublayers, similar to that in <xref ref-type="bibr" rid="bib29">Schnell et al., 2023</xref> and <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>.</p><p><xref ref-type="fig" rid="fig8">Figure 8</xref> shows the performance of the network for each of the test protocols after training classifiers on the training stimuli using the different CNN layers. We added noise to the inputs of the network such that the average training performance, averaged over 100 iterations, lies around 75%. By adding noise in this way, the performance on the training pairs matches overall with rat performance on those pairs; otherwise, the performance of the network would be at 100% on the training pairs, and this would complicate comparisons with the animal data (see also <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>). Note that the results for the <italic>size</italic> test are unreliable given the low number of stimulus pairs in that test. The performance of the network on the tests (green line in <xref ref-type="fig" rid="fig8">Figure 8</xref>) differs among the tests and across layers, but typically the network had no problems to achieve a training performance of about 85% in all test protocols in at least some layers. The change in performance across layers is variable across test protocols.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The performance of the convolutional deep neural network (CNN) after training on our training stimuli, with noise added to its input.</title><p>The naming convention on the x-axis corresponds to the layers of the network, identical as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The performance (y-axis) illustrates that each layer is challenged by at least part of the test protocols. The purple line indicates the training performance, and the green line indicates the test performance of the neural network. The x-axis on each subplot indicates the block of the layer: layer blocks 1â8 correspond to (convolutional layer 1, normalization layer 1, pool layer 1), (convolutional layer 2, normalization layer 2, pool layer 2), convolutional layer 3, convolutional layer 4 (convolutional layer 5, pool layer 5), fully connected layer 6, fully connected layer 7, and fully connected layer 8, respectively. The black and grey horizontal lines on the x-axis indicate the layer blocks (block 1 consisting of conv1, norm1, pool1; block 2 consisting of conv2, norm2, pool2; block 3â4 corresponding to conv3-4, respectively; block 5 consisting of conv5, pool5; block 6-7-8 corresponding to fc6-7-8, respectively). The vertical grey dashed line indicates the division between convolutional and fully connected layer blocks. The horizontal dashed line indicates chance level. The shaded error bounds correspond to 95% confidence intervals calculated using Jackknife standard error estimates, as done previously in <xref ref-type="bibr" rid="bib37">Vinken and Op de Beeck, 2021</xref>. The different markers indicate different sorts of layers: circle for convolutional layers, triangle for normalization layers, point for pool layers, and squares for fully connected layers.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-fig8-v1.tif"/></fig><p>To examine the performance of the model for specific image pairs during training and testing in more detail than possible with a binary categorization decision, we calculate the distance to the classifierâs hyperplane (decision boundary) of the targets and distractors. We do this by computing the difference in signed distance to the hyperplane between target and distractor (target â distractor). This is referred to as the classification score. For each stimulus pair in the test protocols, we computed this classification score, and we have such a score per layer.</p><p>We used this classification score to select image pairs for a CNN-informed stimulus set. To do so, we randomly chose one target and one distractor from a subset of the pool of all 4 Ã 4 stimuli, including all possible transformations on these stimuli. This resulted in a stimulus pool of 10.290 stimuli (5145 targets, 5145 distractors) to randomly choose two from, and 5145 * 5145 (26 471 025) possible resulting pairs of two stimuli. Once one random target and one random distractor was chosen, the DNN was tested in a similar manner as we did for the six test protocols. We performed a total of 10,000 iterations of randomly choosing a target and distractor pair. For each iteration, we calculated the average classification score of layers 1â3 and 11â13 as we wanted to compare those two levels of processing (earlier layers vs. higher layers). After these 10,000 iterations, we fine-tuned and filtered the results according to the profile of performance across earlier and higher layers (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1g</xref>). This fine-tuning started by calculating the distribution and standard deviation for two profiles of interest, that is, (i) where early layers show an average classification score close to zero but higher layers show high classification scores (zero vs. high), and (ii) where early layers show high classification scores but higher layers show close to zero classification scores (high vs. zero). The performance was expressed relative to the distribution of values across all pairs, summarized by the standard deviation of the average targetâdistractor difference in classification scores of the early layers and the higher layers. We found a total of 48 stimulus pairs for these two criteria and ended up choosing 14 pairs, 7 of each criterion, that we used for the final part of the animal and human study (see lower two rows in <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Afterward we also calculated the binary targetâdistractor CNN decision performance for the image pairs in the zero vs. high and high vs. zero tests, which is shown in <xref ref-type="fig" rid="fig8">Figure 8</xref> (bottom row). The image pairs in the zero vs. high protocol are more difficult than the other protocols, in particular for the first half of the CNN layers. In contrast, the <italic>high</italic> vs. <italic>zero</italic> protocol is the only protocol associated with chance performance in the last three layers. These analyses confirm that the CNN-based image pair selection resulted in protocols that are very different from protocols that zoom in on intuitively chosen transformations and their combinations.</p><p>Comparing the rat performances to the classification scores of the network was done by calculating the correlation across image pairs between these model scores and the rat performances averaged across animals. We concatenated the performance of the animals on all nine test protocols, as well as the distance to hyperplane of the network on all nine test protocols. Correlating these two arrays resulted in the correlations as visualized in <xref ref-type="fig" rid="fig4">Figure 4</xref>. To test whether these correlations are significant, we performed a permutation test. We permutated these arrays 1000 times, resulting in a normal distribution of permutated data per layer. We then calculated, per layer how many of the permutated values are greater than or equal to the correlation that is presented in <xref ref-type="fig" rid="fig4">Figure 4</xref> and divided this by the number of permutations.</p></sec><sec id="s4-3"><title>Human study</title><sec id="s4-3-1"><title>Participants</title><p>Data was collected from 50 participants (average age 33.24 Â± 12.23; 34 females) who participated in return for a gift voucher of 10 euros. Out of these 50 participants, 5 were excluded because of outlying behaviour during the quality check protocols (see section âMethStimuli and protocolsâ). All participants had normal or corrected-to-normal vision. The experiment was approved by the ethical commission of KU Leuven (G-2020-1902R3), and each participant digitally signed an informed consent form before the start of the experiment.</p></sec><sec id="s4-3-2"><title>Setup</title><p>For the human part of this study, we developed an online experiment using PsychoPy3 (v2020.1.3, Python version 3.8.10) and placed it on the online platform Pavlovia. All participants received the link and their individual participant number by e-mail with which they could participate in the experiment on their own computer. It took 30â45 min to complete the online study.</p></sec><sec id="s4-3-3"><title>MethStimuli and protocols</title><p>We used the same stimuli as in the animal study. The human experiment underwent the same phases as depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>, albeit with small changes. We dropped the one-third old trials in the test protocols and included two additional <italic>dimension learning</italic> protocols in between the first counterbalanced tests as quality check (see <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1h</xref> provides an overview of the number of trials during the human experiment for each phase. <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref> shows an overview of all image pairs that were presented in the human study.</p><p>Similar to that in <xref ref-type="bibr" rid="bib3">Bossens and Op de Beeck, 2016</xref>, we presented the targets and distractors briefly to the left and right sides of a white fixation cross on a grey background. Each stimulus was presented for three frames, followed by a mask (a noise image with 1/f frequency spectrum for three frames). We used this fast and eccentric stimulus presentation with a mask to resemble the stimulus perception more closely to that of rats. <xref ref-type="bibr" rid="bib33">Vermaercke and Op de Beeck, 2012</xref> have found that human visual acuity in these fast and eccentric presentations is not significantly better than the reported visual acuity of rats. By using this approach, we avoid that differences in strategies between humans and rats would be explained by such a difference in acuity. Participants could then answer using the âfâ and âjâ keys to indicate which position they thought was the correct position. If they thought the target was on the left side of the fixation cross, they had to press âfâ, and press âjâ if they thought the target was on the right side. Participants received feedback during the shaping and the three training phases. This happened by colouring the fixation cross green if they answered correctly and red if they answered incorrectly. Each trial was followed by an ITI of 0.5 s. During the <italic>shaping</italic> and <italic>training</italic> phase, we kept a running average of the past 20 (<italic>shaping</italic>) and 40 (<italic>training</italic>) trials and participants continued to the next phase when they reached a performance of 80% or higher on the last 20 or 40 trials, similar to that in <xref ref-type="bibr" rid="bib3">Bossens and Op de Beeck, 2016</xref>. There was no time limit for the participants for providing a response. The order of the first six test protocols (<italic>rotation X, rotation Y, rotation Z, size, light location,</italic> and <italic>position</italic>) was counterbalanced between the participants based on the participant number, as well as the order of the last two test protocols (<italic>zero</italic> vs<italic>. high</italic> and <italic>high</italic> vs<italic>. zero</italic>), similar to the approach in the rat study. <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1f</xref> indicates the average number of trials per test protocol for all human participants together.</p><p>In terms of instructions, we explained to participants that they would see two figures appearing at the same time very quickly next to a fixation cross, and they would have to make a decision of which figure is the correct one. We mentioned that during training the fixation cross would turn green if they answered correctly and red if they answered incorrectly. Participants were informed that during testing they would not get feedback (changing colour of the fixation cross) anymore and that they would have to use the knowledge they gained throughout training to make their decision in the testing.</p><p>We performed a similar correlation analysis as with rat performance to investigate the correlation between pixel similarity and brightness with human performance. We followed the exact same steps as we did for rat performance.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experiment was approved by the ethical commission of KU Leuven (G-2020-1902-R3) and each participant digitally signed an informed consent before the start of the experiment.This study followed the latest update of the Declaration of Helsinki (World Medical Association, 2013).</p></fn><fn fn-type="other"><p>All experiments and procedures involving living animals were approved by the Ethical Committee for animal experimentation of the KU Leuven and were in accordance with the European Commission Directive of September 22, 2010 (2010/63/EU). We have reported the study in accordance with the ARRIVE guidelines.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Supplementary tables.</title><p>(<bold>a</bold>) Results of binomial test on the six test protocols with the pooled data of all animals together, on the old trials and the new trials. (<bold>b</bold>) Marginal means and standard deviation of the rotation X and rotation Z protocols. (<bold>c</bold>) Correlation between neighbouring layers of the deep neural network. (<bold>d</bold>) Results of the linear regression model with rat performances. (<bold>e</bold>) Results of the linear regression model with human performances. (<bold>f</bold>) Average number of trials (SD) per test protocol and per stimulus pair (SP). (<bold>g</bold>) The two criteria of choosing a CNN-informed stimulus set. (<bold>h</bold>) Overview of the human experiment. (<bold>i</bold>) An overview of the performance of the animals on the first six test protocols.</p></caption><media xlink:href="elife-87719-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Information about the pilot study.</title></caption><media xlink:href="elife-87719-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-87719-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data has been made publicly available via the Open Science Framework and can be accessed at <ext-link ext-link-type="uri" xlink:href="https://osf.io/9eqyz/">https://osf.io/9eqyz/</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Schnell</surname><given-names>AE</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Leemans</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Multidimensional stimulus set</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/V9WBN</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Schnell</surname><given-names>AE</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Leemans</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>A computationally informed comparison between the strategies of rodents and humans in visual object recognition</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/9EQYZ</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alemi-Neissi</surname><given-names>A</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multifeatural shape processing in rats engaged in invariant visual object recognition</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>5939</fpage><lpage>5956</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3629-12.2013</pub-id><pub-id pub-id-type="pmid">23554476</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>AvberÅ¡ek</surname><given-names>LK</given-names></name><name><surname>Zeman</surname><given-names>A</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Training for object recognition with increasing spatial frequency: A comparison of deep learning with human vision</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.10.14</pub-id><pub-id pub-id-type="pmid">34533580</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Linear and non-linear visual feature learning in rat and humans</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>10</volume><elocation-id>235</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2016.00235</pub-id><pub-id pub-id-type="pmid">28066201</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Pinto</surname><given-names>N</given-names></name><name><surname>Ardila</surname><given-names>D</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callahan</surname><given-names>TL</given-names></name><name><surname>Petry</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Psychophysical measurement of temporal modulation sensitivity in the tree shrew (Tupaia belangeri)</article-title><source>Vision Research</source><volume>40</volume><fpage>455</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(99)00194-7</pub-id><pub-id pub-id-type="pmid">10820625</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crijns</surname><given-names>E</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The visual acuity of rats in touchscreen setups</article-title><source>Vision</source><volume>4</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3390/vision4010004</pub-id><pub-id pub-id-type="pmid">31906140</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Keyser</surname><given-names>R</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cue-invariant shape recognition in rats as tested with second-order contours</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.15.14</pub-id><pub-id pub-id-type="pmid">26605843</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Djurdjevic</surname><given-names>V</given-names></name><name><surname>Ansuini</surname><given-names>A</given-names></name><name><surname>Bertolini</surname><given-names>D</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Accuracy of rats in discriminating visual objects is explained by the complexity of their perceptual strategy</article-title><source>Current Biology</source><volume>28</volume><fpage>1005</fpage><lpage>1015</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.02.037</pub-id><pub-id pub-id-type="pmid">29551414</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duyck</surname><given-names>S</given-names></name><name><surname>Martens</surname><given-names>F</given-names></name><name><surname>Chen</surname><given-names>CY</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How visual expertise changes representational geometry: A behavioral and neural perspective</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2461</fpage><lpage>2476</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01778</pub-id><pub-id pub-id-type="pmid">34748633</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>II</given-names></name><name><surname>Greene</surname><given-names>MR</given-names></name><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Beck</surname><given-names>DM</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior</article-title><source>eLife</source><volume>7</volume><elocation-id>e32962</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32962</pub-id><pub-id pub-id-type="pmid">29513219</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>GÃ¼Ã§lÃ¼</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalfas</surname><given-names>I</given-names></name><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representations of regular and irregular shapes by deep Convolutional Neural Networks, monkey inferotemporal neurons and human judgments</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006557</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006557</pub-id><pub-id pub-id-type="pmid">30365485</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence that recurrent circuits are critical to the ventral streamâs execution of core object recognition behavior</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id><pub-id pub-id-type="pmid">31036945</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Bokor</surname><given-names>SL</given-names></name><name><surname>Jeon</surname><given-names>YN</given-names></name><name><surname>Toosi</surname><given-names>T</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Conserved Core Visual Object Recognition across Simian Primates: Marmoset Image-by-Image Behavior Mirrors That of Humans and Macaques</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.10.19.345561</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Bokor</surname><given-names>SL</given-names></name><name><surname>Jeon</surname><given-names>YN</given-names></name><name><surname>Toosi</surname><given-names>T</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Brain Organization, Not Size Alone, as Key to High-Level Vision: Evidence from Marmoset Monkeys</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.10.19.345561v3</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep neural networks as a computational model for human shape sensitivity</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004896</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id><pub-id pub-id-type="pmid">27124699</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Visual object recognition</article-title><source>Annual Review of Neuroscience</source><volume>19</volume><fpage>577</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.19.030196.003045</pub-id><pub-id pub-id-type="pmid">8833455</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matteucci</surname><given-names>G</given-names></name><name><surname>Bellacosa Marotti</surname><given-names>R</given-names></name><name><surname>Riggi</surname><given-names>M</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nonlinear processing of shape information in rat lateral extrastriate cortex</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>1649</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1938-18.2018</pub-id><pub-id pub-id-type="pmid">30617210</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>EE</given-names></name><name><surname>Ong</surname><given-names>WS</given-names></name><name><surname>Balboa</surname><given-names>M</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Assessing tree shrew high-level visual behavior using conventional and natural paradigms [Poster presentation]</article-title><conf-name>Society for Neuroscience, November 12-16</conf-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minini</surname><given-names>L</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Do rats use shape to solve âshape discriminationsâ?</article-title><source>Learning &amp; Memory</source><volume>13</volume><fpage>287</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1101/lm.84406</pub-id><pub-id pub-id-type="pmid">16705141</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Kong</surname><given-names>NCL</given-names></name><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mouse Visual Cortex as a Limited Resource System That Self-Learns an Ecologically-General Representation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.16.448730</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>What makes a cell face selective</article-title><source>The Importance of Contrast. Neuron</source><volume>74</volume><fpage>567</fpage><lpage>581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.024</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perceived shape similarity among unfamiliar objects and the organization of the human object vision pathway</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>10111</fpage><lpage>10123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2511-08.2008</pub-id><pub-id pub-id-type="pmid">18829969</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petry</surname><given-names>HM</given-names></name><name><surname>Clark</surname><given-names>C</given-names></name><name><surname>Day-Brown</surname><given-names>J</given-names></name><name><surname>Bolin</surname><given-names>RT</given-names></name><name><surname>Bickford</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Behavioral measurement of RDK velocity discrimination thresholds in the tree shrew</article-title><source>Journal of Vision</source><volume>12</volume><elocation-id>1223</elocation-id><pub-id pub-id-type="doi">10.1167/12.9.1223</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petry</surname><given-names>HM</given-names></name><name><surname>Bickford</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The second visual system of the tree shrew</article-title><source>The Journal of Comparative Neurology</source><volume>527</volume><fpage>679</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1002/cne.24413</pub-id><pub-id pub-id-type="pmid">29446088</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>N</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Why is real-world visual object recognition hard?</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e27</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0040027</pub-id><pub-id pub-id-type="pmid">18225950</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pospisil</surname><given-names>DA</given-names></name><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Bair</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>âArtiphysiologyâ reveals V4-like shape tuning in a deep network trained for image classification</article-title><source>eLife</source><volume>7</volume><elocation-id>e38242</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38242</pub-id><pub-id pub-id-type="pmid">30570484</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnell</surname><given-names>AE</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Gijbels</surname><given-names>K</given-names></name><name><surname>Bossens</surname><given-names>C</given-names></name><name><surname>de Beeck</surname><given-names>HO</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Face categorization and behavioral templates in rats</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/19.14.9</pub-id><pub-id pub-id-type="pmid">31826254</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnell</surname><given-names>AE</given-names></name><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>de Beeck</surname><given-names>HO</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The importance of contrast features in rat vision</article-title><source>Scientific Reports</source><volume>13</volume><elocation-id>459</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-27533-3</pub-id><pub-id pub-id-type="pmid">36627335</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sinha</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Qualitative representations for recognition</chapter-title><person-group person-group-type="editor"><name><surname>BÃ¼lthoff</surname><given-names>HH</given-names></name><name><surname>Wallraven</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>SW</given-names></name><name><surname>Poggio</surname><given-names>TA</given-names></name></person-group><source>Biologically Motivated Computer Vision</source><publisher-name>Springer</publisher-name><fpage>249</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1007/3-540-36181-2</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Di Filippo</surname><given-names>A</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Transformation-tolerant object recognition in rats revealed by visual priming</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>21</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3932-11.2012</pub-id><pub-id pub-id-type="pmid">22219267</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Safaai</surname><given-names>H</given-names></name><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Vanzella</surname><given-names>W</given-names></name><name><surname>Riggi</surname><given-names>M</given-names></name><name><surname>Buffolo</surname><given-names>F</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22794</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22794</pub-id><pub-id pub-id-type="pmid">28395730</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A multivariate approach reveals the behavioral templates underlying visual discrimination in rats</article-title><source>Current Biology</source><volume>22</volume><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.041</pub-id><pub-id pub-id-type="pmid">22209530</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Gerich</surname><given-names>FJ</given-names></name><name><surname>Ytebrouck</surname><given-names>E</given-names></name><name><surname>Arckens</surname><given-names>L</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional specialization in rat occipital and temporal visual cortex</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1963</fpage><lpage>1983</lpage><pub-id pub-id-type="doi">10.1152/jn.00737.2013</pub-id><pub-id pub-id-type="pmid">24990566</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual categorization of natural movies by rats</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>10645</fpage><lpage>10658</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3663-13.2014</pub-id><pub-id pub-id-type="pmid">25100598</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural representations of natural and scrambled movies progressively change from rat striate to temporal cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3310</fpage><lpage>3322</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw111</pub-id><pub-id pub-id-type="pmid">27146315</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Using deep neural networks to evaluate object vision tasks in rats</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008714</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008714</pub-id><pub-id pub-id-type="pmid">33651793</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Cassidy</surname><given-names>BS</given-names></name><name><surname>Devaney</surname><given-names>KJ</given-names></name><name><surname>Holt</surname><given-names>DJ</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Lower-level stimulus features strongly influence responses in the fusiform face area</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq050</pub-id><pub-id pub-id-type="pmid">20375074</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeman</surname><given-names>AA</given-names></name><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Orthogonal representations of object shape and category in deep convolutional neural networks and human visual cortex</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>2453</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-59175-0</pub-id><pub-id pub-id-type="pmid">32051467</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Oertelt</surname><given-names>N</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A rodent model for the study of invariant visual object recognition</article-title><source>PNAS</source><volume>106</volume><fpage>8748</fpage><lpage>8753</lpage><pub-id pub-id-type="doi">10.1073/pnas.0811583106</pub-id><pub-id pub-id-type="pmid">19429704</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Invariant visual object recognition and shape processing in rats</article-title><source>Behavioural Brain Research</source><volume>285</volume><fpage>10</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2014.12.053</pub-id><pub-id pub-id-type="pmid">25561421</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87719.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Arun</surname><given-names>SP</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>Schnell et al. report <bold>important</bold> differences between the strategies used by rodents and humans when discriminating different visual objects. The evidence supporting these findings is <bold>convincing</bold>, showing that rat performance was influenced far more by low-level cues compared to humans. It is, however, unclear to what extent these differences can be explained by the lower visual acuity of rats. This work will be of general interest to vision and cognition researchers, particularly those studying object vision.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87719.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Schnell et al. performed two extensive behavioral experiments concerning the processing of objects in rats and humans. To this aim, they designed a set of objects parametrically varying along alignment and concavity and then they used activations from a pretrained deep convolutional neural network to select stimuli that would require one of two different discrimination strategies, i.e. relying on either low- or high-level processing exclusively. The results show that rodents rely more on low-level processing than humans.</p><p>Strengths:</p><p>1. The results are challenging and call for a different interpretation of previous evidence. Indeed, this work shows that common assumptions about task complexity and visual processing are probably biased by our personal intuitions and are not equivalent in rodents, which instead tend to rely more on low-level properties.</p><p>2. This is an innovative (and assumption-free) approach that will prove useful to many visual neuroscientists. Personally, I second the authors' excitement about the proposed approach, and its potential to overcome the limits of experimenters' creativity and intuitions. In general, the claims seem well supported and the effects sufficiently clear.</p><p>3. This work provides an insightful link between rodent and human literature on object processing. Given the increasing number of studies on visual perception involving rodents, these kinds of comparisons are becoming crucial.</p><p>4. The paper raises several novel questions that will prompt more research in this direction.</p><p>Weaknesses:</p><p>1. The choice of alignment and concavity as baseline properties of the stimuli is not properly discussed.</p><p>2. From the low-correlations I got the feeling that AlexNet is not the best baseline model for rat visual processing.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87719.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Schnell and colleagues trained rats on a two-alternative forced choice visual discrimination task. They used object pairs that differed in their concavity and the alignment of features. They found that rats could discriminate objects across various image transformations. Rat performance correlated best with late convolutional layers of an artificial neural network and was partially explained by factors of brightness and pixel-level similarity. In contrast, human performance showed the strongest correlation with higher, fully connected layers, indicating that rats employed simpler strategies to accomplish this task as compared to humans.</p><p>Strengths:</p><p>1. This is a methodologically rigorous study. The authors tested a substantial number of rats across a large variety of stimuli.</p><p>2. The innovative use of neural networks to generate stimuli with varying levels of complexity is a compelling approach that motivates principled experimental design.</p><p>3. The study provides important data points for cross-species comparisons of object discrimination behavior</p><p>4. The data strongly support the authors' conclusion that rats and humans rely on different visual features for discrimination tasks.</p><p>5. This is a valuable study that provides novel, important insights into the visual capabilities of rats.</p><p>Weaknesses:</p><p>1. The impact of rat visual acuity (~1cycle/degree) on the discriminability of stimuli could be more directly modeled and taken into consideration when comparing rat behavior to humans, who possess substantially higher acuity.</p><p>2. The distinction between low- and high-level visual behavior is coarse, and it remains uncertain which specific features rats utilized for discrimination. The correlations with brightness and pixel-level similarity do provide some insight.</p><p>3. The relatively weak correspondence between rat behavior and AlexNet raises the question of which network architecture, whether computational or biological, might better capture rat behavior, particularly to the level of cross-rat consistency.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87719.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schnell</surname><given-names>Anna Elisabeth</given-names></name><role specific-use="author">Author</role><aff><institution>KU Leuven</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Leemans</surname><given-names>Maarten</given-names></name><role specific-use="author">Author</role><aff><institution>KU Leuven</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Vinken</surname><given-names>Kasper</given-names></name><role specific-use="author">Author</role><aff><institution>Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Op de Beeck</surname><given-names>Hans</given-names></name><role specific-use="author">Author</role><aff><institution>KU Leuven</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the current reviews.</p><p>We thank the editor for the eLife assessment and reviewers for their remaining comments. We will address them in this response.</p><p>First, we thank eLife for the positive assessment. Regarding the point of visual acuity that is mentioned in this assessment, we understand that this comment is made. It is not an uncommon comment when rodent vision is discussed. However, we emphasize that we took the lower visual acuity of rats and the higher visual acuity of humans into account when designing the human study, by using a fast and eccentric stimulus presentation for humans. As a result, we do not expect a higher discriminability of stimuli in humans. We have described this in detail in our Methods section when describing the procedure in the human experiment:</p><p>âWe used this fast and eccentric stimulus presentation with a mask to resemble the stimulus perception more closely to that of rats. Vermaercke &amp; Op de Beeck (2012) have found that human visual acuity in these fast and eccentric presentations is not significantly better than the reported visual acuity of rats. By using this approach we avoid that differences in strategies between humans and rats would be explained by such a difference in acuityâ</p><p>Second, regarding the remaining comment of Reviewer #2 about our use of AlexNet:</p><p>While it is indeed relevant to further look into different computational architectures, we chose to not do this within the current study. First, it is a central characteristic of the study procedure that the computational approach and chosen network is chosen early on as it is used to generate the experimental design that animals are tested with. We cannot decide after data collection to use a different network to select the stimuli with which these data were collected. Second, as mentioned in our first response, using AlexNet is not a random choice. It has been used in many previously published vision studies that were relatively positive about the correspondence with biological vision (Cadieu et al., 2014; Groen et al., 2018; Kalfas et al., 2018; Nayebi et al., 2023; Zeman et al., 2020). Third, our aim was not to find a best DNN model for rat vision, but instead examining the visual features that play a role in our complex discrimination task with a model that was hopefully a good enough starting point. The fact that the designs based upon AlexNet resulted in differential and interpretable effects in rats as well as in humans suggests that this computational model was a good start. Comparing the outcomes of different networks would be an interesting next step, and we expect that our approach could work even better when using a network that is more specifically tailored to mimic rat visual processing.</p><p>Finally, regarding the choice to specifically chose alignment and concavity as baseline properties, this choice is probably not crucial for the current study. We have no reason to expect rats to have an explicit notion about how a shape is built up in terms of a part-based structure, where alignment relates to the relative position of the parts and concavity is a property of the main base. For human vision it might be different, but we did not focus on such questions in this study.</p><p>The following is the authorsâ response to the original reviews.</p><p>We would like to thank you for giving us the opportunity to submit a revised draft our manuscript. We appreciate the time and effort that you dedicated to providing insightful feedback on our manuscript and are grateful for the valuable comments and improvements on our paper. It helped us to improve our manuscript. We have carefully considered the comments and tried our best to address every one of them. We have added clarifications in the Discussion concerning the type of neural network that we used, about which visual features might play a role in our results as well as clarified the experimental setup and protocol in the Methods section as these two sections were lacking key information points.</p><p>Below we provide a response to the public comments and concerns of the reviewers.</p><p>Several key points were addressed by at least two reviewers, and we will respond to them first.</p><p>A first point concerns the type of network we used. In our study, we used AlexNet to simulate the ventral visual stream and to further examine rat and human performance. While other, more complex neural networks might lead to other results, we chose to work with AlexNet because it has been used in many other vision studies that are published in high impact journals (Cadieu et al., 2014; Groen et al., 2018; Kalfas et al., 2018; Nayebi et al., 2023; Zeman et al., 2020). We did not try to find a best DNN model for rat vision but instead, we were looking for an explanation of which visual features play a role in our complex discrimination task. We added a consideration to our Discussion addressing why we worked with AlexNet. Since our data will be published on OSF, we encourage to researchers to use our data with other, more complex neural networks and to further investigate this issue.</p><p>A second point that was addressed by multiple reviewers concerns the visual acuity of the animals and its impact on their performance. The position of the rat was not monitored in the setup. In a previous study in our lab (Crijns &amp; Op de Beeck, 2019), we investigated the visual acuity of rats in the touchscreen setups by presenting gratings with different cycles per screen to see how it affects their performance in orientation discrimination. With the results from this study and general knowledge about rat visual acuity, we derived that the decision distance of rats lies around 12.5cm from the screen. We have added this paragraph to the Discussion.</p><p>A third key point that needs to be addressed as a general point involves which visual features could explain rat and human performance. We reported marked differences between rat and human data in how performance varied across image trials, and we concluded through our computationally informed tests and analyses that rat performance was explained better by lower levels of processing. Yet, we did not investigate which exact features might underlie rat performance. As a starter, we have focused on taking a closer look at pixel similarity and brightness and calculating the correlation between rat/human performance and these two visual features.</p><p>We calculated the correlation between the rat performances and image brightness of the transformations. We did this by calculating the difference in brightness of the base pair (brightness base target â brightness base distractor), and subtracting the difference in brightness of every test target-distractor pair for each test protocol (brightness test target â brightness test distractor for each test pair). We then correlated these 287 brightness values (1 for each test image pair) with the average rat performance for each test image pair. This resulted in a correlation of 0.39, suggesting that there is an influence of brightness in the test protocols. If we perform the same correlation with the human performances, we get a correlation of -0.12, suggesting a negative influence of brightness in the human study.</p><p>We calculated the correlation between pixel similarity of the test stimuli in relation to the base stimuli with the average performance of the animals on all nine test protocols. We did this by calculating the pixel similarity between the base target with every other testing distractor (A), the pixel similarity between the base target with every other testing target (B), the pixel similarity between the base distractor with every other testing distractor (C) and the pixel similarity between the base distractor with every other testing target (D). For each test image pair, we then calculated the average of (A) and (D), and subtracted the average of (C) and (B) from it. We correlated these 287 values (one for each image pair) with the average rat performance on all test image pairs, which resulted in a correlation of 0.34, suggesting an influence of pixel similarity in rat behaviour. Performing the same correlation analysis with the human performances results in a correlation of 0.12.</p><p>We have also addressed this in the Discussion of the revised manuscript. Note that the reliability of the rat data was 0.58, clearly higher than the correlations with brightness and pixel similarity, thus these features capture only part of the strategies used by rats.</p><p>We have also responded to all other insightful suggestions and comments of the reviewers, and a point-by-point response to the more major comments will follow now.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1, general comments:</bold></p><p>The authors should also discuss the potential reason for the human-rat differences too, and importantly discuss whether these differences are coming from the rather unusual approach of training used in rats (i.e. to identify one item among a single pair of images), or perhaps due to the visual differences in the stimuli used (what were the image sizes used in rats and humans?). Can they address whether rats trained on more generic visual tasks (e.g. same-different, or category matching tasks) would show similar performance as humans?</p></disp-quote><p>The task that we used is typically referred to as a two-alternative forced choice (2AFC). This is a simple task to learn. A same-different task is cognitively much more demanding, also for artificial neural networks (see e.g. Puebla &amp; Bowers, 2022, J. Vision). A one-stimulus choice task (probably what the reviewer refers to with category matching) is known to be more difficult compared to 2AFC, with a sensitivity that is predicted to be Sqrt(2) lower according to signal detection theory (MacMillan &amp; Creelman, 1991). We confirmed this prediction empirically in our lab (unpublished observations). Thus, we predict that rats perform less good in the suggested alternatives, potentially even (in case of same-different) resulting in a wider performance gap with humans.</p><disp-quote content-type="editor-comment"><p>I also found that a lot of essential information is not conveyed clearly in the manuscript. Perhaps it is there in earlier studies but it is very tedious for a reader to go back to some other studies to understand this one. For instance, the exact number of image pairs used for training and testing for rats and humans was either missing or hard to find out. The task used on rats was also extremely difficult to understand. An image of the experimental setup or a timeline graphic showing the entire trial with screenshots would have helped greatly.</p></disp-quote><p>All the image pairs used for training and testing for rats and humans are depicted in Figure 1 (for rats) and Supplemental Figure 6 (for humans). For the first training protocol (Training), only one image pair was shown, with the target being the concave object with horizontal alignment of the spheres. For the second training protocol (Dimension learning), three image pairs were shown, consisting of the base pair, a pair which differs only in concavity, and a pair which differs only in alignment. For the third training protocol (Transformations) and all testing protocols, all combination of targets and distractors were presented. For example, in the Rotation X protocol, the stimuli consisted of 6 targets and 6 distractors, resulting in a total of 36 image pairs for this protocol.The task used on rats is exactly as shown in Figure 1. A trial started with two blank screens. Once the animal initiated a trial by sticking its head in the reward tray, one stimulus was presented on each screen. There was no time limit and so the stimuli remained on the screen until the animal made a decision. If the animal touched the target, it received a sugar pellet as reward and a ITI of 20s started. If the animal touched the distractor, it did not receive a sugar pellet and a time-out of 5s started in addition to the 20s ITI.</p><p>We have clarified this in the manuscript.</p><disp-quote content-type="editor-comment"><p>The authors state that the rats received random reward on 80% of the trials, but is that on 80% of the correctly responded trials or on 80% of trials regardless of the correctness of the response? If these are free choice experiments, then the task demands are quite different. This needs to be clarified. Similarly, the authors mention that 1/3 of the trials in a given test block contained the old base pair - are these included in the accuracy calculations?</p></disp-quote><p>The animals receive random reward on 80% on all testing trials with new stimuli, regardless of the correctness of the response. This was done to ensure that we can measure true generalization based upon learning in the training phase, and that the animals do not learn/are not trained in these testing stimuli. For the trials with the old stimuli (base pair), the animals always received real reward (reward when correct; no reward in case of error).</p><p>The 1/3rd trials with old stimuli are not included in the accuracy calculations but were used as a quality check/control to investigate which sessions have to be excluded and to assure that the rats were still doing the task properly. We have added this in the manuscript.</p><disp-quote content-type="editor-comment"><p>The authors were injecting noise with stimuli to cDNN to match its accuracy to rat. However, that noise potentially can interacted with the signal in cDNN and further influence the results. That could generate hidden confound in the results. Can they acknowledge/discuss this possibility?</p></disp-quote><p>Yes, adding noise can potentially interact with the signal and further influence the results. Without noise, the average training data of the network would lie around 100% which would be unrealistic, given the performances of the animals. To match the training performance of the neural networks with that of the rats, we added noise 100 times and averaged over these iterations (cfr. (Schnell et al., 2023; Vinken &amp; Op de Beeck, 2021)).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2, weaknesses:</bold></p><p>1. There are a few inconsistencies in the number of subjects reported. Sometimes 45 humans are mentioned and sometimes 50. Probably they are just typos, but it's unclear.</p></disp-quote><p>Thank you for your feedback. We have doublechecked this and changed the number of subjects where necessary. We collected data from 50 human participants, but had to exclude 5 of them due to low performance during the quality check (Dimension learning) protocols. Similarly, we collected data from 12 rats but had to exclude one animal because of health issues. All these data exclusion steps were mentioned in the Methods section of the original version of the manuscript, but the subject numbers were not always properly adjusted in the description in the Results section. This is now corrected.</p><disp-quote content-type="editor-comment"><p>1. A few aspects mentioned in the introduction and results are only defined in the Methods thus making the manuscript a bit hard to follow (e.g. the alignment dimension), thus I had to jump often from the main text to the methods to get a sense of their meaning.</p></disp-quote><p>Thank you for your feedback. We have clarified some aspects in the Introduction, such as the alignment dimension.</p><disp-quote content-type="editor-comment"><p>1. Many important aspects of the task are not fully described in the Methods (e.g. size of the stimuli, reaction times and basic statistics on the responses).</p></disp-quote><p>We have added the size of the stimuli to the Methods section and clarified that the stimuli remained on the screen until the animals made a choice. Reaction time in our task would not be interpretable given that stimuli come on the screen when the animal initiates a trial with its back to the screen. Therefore we do not have this kind of information.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1</bold></p><p>â¢ Can the authors show all the high vs zero and zero vs high stimulus pairs either in the main or supplementary figures? It would be instructive to know if some other simple property covaried between these two sets.</p></disp-quote><p>In Figure 1, all images of all protocols are shown. For the High vs. Zero and Zero vs. High protocols, we used a deep neural network to select a total of 7 targets and 7 distractors. This results in 49 image pairs (every combination of target-distractor).</p><disp-quote content-type="editor-comment"><p>â¢ Are there individual differences across animals? It would be useful for the authors to show individual accuracy for each animal where possible.</p></disp-quote><p>We now added individual rat data for all test protocols â 1 colour per rat, black circle = average. We have added this picture to the Supplementary material (Supplementary Figure 1).</p><disp-quote content-type="editor-comment"><p>â¢ Figure 1 - it was not truly clear to me how many image pairs were used in the actual experiment. Also, it was very confusing to me what was the target for the test trials. Additionally, authors reported their task as a categorisation task, but it is a discrimination task.</p></disp-quote><p>Figure 1 shows all the images that were used in this study. Every combination of every target-distractor in each protocol (except for Dimension learning) was presented to the animals. For example in Rotation X, the test stimuli as shown in Fig. 1 consisted of 6 targets and 6 distractors, resulting in a total of 36 image pairs for this test protocol.</p><p>In each test protocol, the target corresponded to the concave object with horizontally attached spheres, or the object from the pair that in the stimulus space was closed to this object. We have added this clarification in the Introduction: âWe started by training the animals in a base stimulus pair, with the target being the concave object with horizontally aligned spheres. Once the animals were trained in this base stimulus pair, we used the identity-preserving transformations to test for generalization.â as well as in the caption of Figure 1. We have changed the term âcategorisation taskâ to âdiscrimination taskâ throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>â¢ Figure 2 - what are the red and black lines? How many new pairs are being tested here? Panel labels are missing (a/b/c etc)</p></disp-quote><p>We have changed this figure by adding panel labels, and clarifying the missing information in the caption. All images that were shown to the animals are presented on this figure. For Dimension Learning, only three image pairs were shown (base pair, concavity pair, alignment pair) and for the Transformations protocol, every combination of every target and distractor were shown, i.e. 25 image pairs in total.</p><disp-quote content-type="editor-comment"><p>â¢ Figure 3 - last panel: the 1st and 2nd distractor look identical.</p></disp-quote><p>We understand your concern as these two distractors indeed look quite similar. They are different however in terms of how they are rotated along the x, y and z axes (see Author response image 1 for a bigger image of these two distractors). The similarity is due to the existence of near-symmetry in the object shape which causes high self-similarity for some large rotations.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-sa3-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>â¢ Line 542 â authors say they have âconcatenatedâ the performance of the animals, but do they mean they are taking the average across animals?</p></disp-quote><p>It is both. In this specific analysis we calculated the performance of the animals, which was indeed averaged across animals, per test protocol, per stimulus pair. This resulted in 9 arrays (one for each test protocol) of several performances (1 for each stimulus pair). These 9 arrays were concatenated by linking them together in one big array (i.e. placing them one after the other). We did the same concatenation with the distance to hyperplane of the network on all nine test protocols. These two concatenated arrays with 287 values each (one with the animal performance and one with the DNN performance) were correlated.</p><disp-quote content-type="editor-comment"><p>â¢ Line 164 - What are these 287 image pairs - this is not clear.</p></disp-quote><p>The 287 image pairs correspond to all image pairs of all 9 test protocols: 36 (Rotation X) + 36 (Rotation Y) + 36 (Rotation Z) + 4 (Size) + 25 (Position) + 16 (Light location) + 36 (Combination Rotation) + 49 (Zero vs. high) + 49 (High vs. zero) = 287 image pairs in total. We have clarified this in the manuscript.</p><disp-quote content-type="editor-comment"><p>â¢ Line 215 - Human rat correlation (0.18) was comparable to the best cDNN layer correlation. What does this mean?</p></disp-quote><p>The human rat correlation (0.18) was closest to the best cDNN layer - rat correlation (about 0.15). In the manuscript we emphasize that rat performance is not well captured by individual cDNN layers.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2</bold></p><p>Major comments</p><p>â¢ In l.23 (and in the methods) the authors mention 50 humans, but in l.87 they are 45. Also, both in l.95 and in the Methods the authors mention &quot;twelve animals&quot; but they wrote 11 elsewhere (e.g. abstract and first paragraph of the results).</p></disp-quote><p>In our human study design, we introduced several Dimension learning protocols. These were later used as a quality check to indicate which participants were outliers, using outlier detection in R. This resulted in 5 outlying human participants, and thus we ended with a pool of 45 human participants that were included in the analyses. This information was given in the Methods section of the original manuscript, but we did not mention the correct numbers everywhere. We have corrected this in the manuscript. We also changed the number of participants (humans and rats) to the correct one throughout the entire manuscript.</p><disp-quote content-type="editor-comment"><p>â¢ At l.95 when I first met the &quot;4x4 stimulus grid&quot; I had to guess its meaning. It would be really useful to see the stimulus grid as a panel in Figure 1 (in general Figures S1 and S4 could be integrated as panels of Figure 1). Also, even if the description of the stimulus generation in the Methods is probably clear enough, the authors might want to consider adding a simple schematic in Figure 1 as well (e.g. show the base, either concave or convex, and then how the 3 spheres are added to control alignment).</p></disp-quote><p>We have added the 4x4 stimulus grid in the main text.</p><disp-quote content-type="editor-comment"><p>â¢ There is also another important point related to the choice of the network. As I wrote, I find the overall approach very interesting and powerful, but I'm actually worried that AlexNet might not be a good choice. I have experience trying to model neuronal responses from IT in monkeys, and there even the higher layers of AlexNet aren't that helpful. I need to use much deeper networks (e.g. ResNet or GoogleNet) to get decent fits. So I'm afraid that what is deemed as &quot;high&quot; in AlexNet might not be as high as the authors think. It would be helpful, as a sanity check, to see if the authors get the same sort of stimulus categories when using a different, deeper network.</p></disp-quote><p>We added a consideration to the manuscript about which network to use (see the Discussion): âWe chose to work with Alexnet, as this is a network that has been used as a benchmark in many previous studies (e.g. (Cadieu et al., 2014; Groen et al., 2018; Kalfas et al., 2018; Nayebi et al., 2023; Zeman et al., 2020)), including studies that used more complex stimuli than the stimulus space in our current study. [â¦] . It is in line with the literature that a typical deep neural network, AlexNet and also more complex ones, can explain human and animal behaviour to a certain extent but not fully. The explained variance might differ among DNNs, and there might be DNNs that can explain a higher proportion of rat or human behaviour. Most relevant for our current study is that DNNs tend to agree in terms of how representations change from lower to higher hierarchical layers, because this is the transformation that we have targeted in the Zero vs. high and High vs. zero testing protocols. (Pinto et al., 2008) already revealed that a simple V1-like model can sometimes result in surprisingly good object recognition performance. This aspect of our findings is also in line with the observation of Vinken &amp; Op de Beeck (2021) that the performance of rats in many previous tasks might not be indicative of highly complex representations. Nevertheless, there is still a relative difference in complexity between lower and higher levels in the hierarchy. That is what we capitalize upon with the Zero vs. high and High vs. zero protocols. Thus, it might be more fruitful to explicitly contrast different levels of processing in a relative way rather than trying to pinpoint behaviour to specific levels of processing.â</p><disp-quote content-type="editor-comment"><p>â¢ The task description needs way more detail. For how long were the stimuli presented? What was their size? Were the positions of the stimuli randomized? Was it a reaction time task? Was the time-out used as a negative feedback? In case, when (e.g. mistakes or slow responses)? Also, it is important to report some statistics about the basic responses. What was the average response time, what was the performance of individual animals (over days)? Did they show any bias for a particular dimension (either the 2 baseline dimensions or the identity preserving ones) or side of response? Was there a correlation within animals between performance on the baseline task and performance on the more complex tasks?</p></disp-quote><p>Thank you for your feedback. We have added more details to the task description in the manuscript.</p><p>The stimuli were presented on the screens until the animals reacted to one of the two screens. The size of the stimuli was 100 x 100 pixel. The position of the stimuli was always centred/full screen on the touchscreens. It was not a reaction time task and we also did not measure reaction time.</p><disp-quote content-type="editor-comment"><p>â¢ Related to my previous comment, I wonder if the relative size/position of the stimulus with respect to the position of the animal in the setup might have had an impact on the performance, also given the impact of size shown in Figure 2. Was the position of the rat in the setup monitored (e.g. with DeepLabCut)? I guess that on average any effect of the animal position might be averaged away, but was this actually checked and/or controlled for?</p></disp-quote><p>The position of the rat was not monitored in the setup. In a previous study from our lab (Crijns &amp; Op de Beeck, 2019), we investigated the visual acuity of rats in the touchscreen setups by presenting gratings with different cycles per screen to see how it affects their performance in orientation discrimination. With the results from this study and general knowledge about rat visual acuity, we derived that the decision distance of rats lies around 12.5cm from the screen. We have added this to the discussion.</p><disp-quote content-type="editor-comment"><p>Minor comments</p><p>â¢ l.33 The sentence mentions humans, but the references are about monkeys. I believe that this concept is universal enough not to require any citation to support it.</p></disp-quote><p>Thank you for your feedback. We have removed the citations.</p><disp-quote content-type="editor-comment"><p>â¢ This is very minor and totally negligible. The acronymous cDNN is not that common for convents (and it's kind of similar to cuDNN), it might help clarity to stick to a more popular acronymous, e.g. CNN or ANN. Also, given that the &quot;high&quot; layers used for stimulus selection where not convolutional layers after all (if I'm not mistaken).</p></disp-quote><p>Thank you for your feedback. We have changed the acronym to âCNNâ in the entire manuscript.</p><disp-quote content-type="editor-comment"><p>â¢ In l.107-109 the authors identified a few potential biases in their stimuli, and they claim these biases cannot explain the results. However, the explanation is given only in the next pages. It might help to mention that before or to move that paragraph later, as I was just wondering about it until I finally got to the part on the brightness bias.</p></disp-quote><p>We expanded the analysis of these dimensions (e.g. brightness) throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>â¢ It would help a lot the readability to put also a label close to each dimension in Figures 2 and 3. I had to go and look at Figure S4 to figure that out.</p></disp-quote><p>Figures 2 and 3 have been updated, also including changes related to other comments.</p><disp-quote content-type="editor-comment"><p>â¢ In Figure 2A, please specify what the red dashed line means.</p></disp-quote><p>We have edited the caption of Figure 2: âFigure 2 (a) Results of the Dimension learning training protocol. The black dashed horizontal line indicates chance level performance and the red dashed line represents the 80% performance threshold. The blue circles on top of each bar represent individual rat performances. The three bars represent the average performance of all animals on the old pair (Old), the pair that differs only in concavity (Conc) and on the pair that differs only in alignment (Align). (b) Results of the Transformations training protocol. Each cell of the matrix indicates the average performance per stimulus pair, pooled over all animals. The columns represent the distractors, whereas the rows separate the targets. The colour bar indicates the performance correct. â</p><disp-quote content-type="editor-comment"><p>â¢ Related to that, why performing a binomial test on 80%? It sounds arbitrary.</p></disp-quote><p>We performed the binomial test on 80% as 80% is our performance threshold for the animals</p><disp-quote content-type="editor-comment"><p>â¢ The way the cDNN methods are introduced makes it sound like the authors actually fine-tuned the weights of AlexNet, while (if I'm not mistaken), they trained a classifier on the activations of a pre-trained AlexNet with frozen weights. It might be a bit confusing to readers. The rest of the paragraph instead is very clear and easy to follow.</p></disp-quote><p>We think the most confusing sentence was â Figure 7 shows the performance of the network after training the network on our training stimuli for all test protocols. â We changed this sentence to â Figure 8 shows the performance of the network for each of the test protocols after training classifiers on the training stimuli using the different DNN layers.â</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3</bold></p><p>Main recommendations:</p><p>Although it may not fully explain the entire pattern of visual behavior, it is important to discuss rat visual acuity and its impact on the perception of visual features in the stimulus set.</p></disp-quote><p>We have added a paragraph to the Discussion that discusses the visual acuity of rats and its impact on perceiving the visual features of the stimuli.</p><disp-quote content-type="editor-comment"><p>The authors observed a potential influence of image brightness on behavior during the dimension learning protocol. Was there a correlation between image brightness and the subsequent image transformations?</p></disp-quote><p>We have added this to the Discussion: âTo further investigate to which visual features the rat performance and human performance correlates best with, we calculated the correlation between rat performance and pixel similarity of the test image pairs, as well as the correlation between rat performance and brightness in the test image pairs. Here we found a correlation of 0.34 for pixel similarity and 0.39 for brightness, suggesting that these two visual features partly explain our results when compared to the full-set reliability of rat performance (0.58). If we perform the same correlation with the human performances, we get a correlation of 0.12 for pixel similarity and -0.12 for brightness. With the full-set reliability of 0.58 (rats) and 0.63 (humans) in mind, this suggests that even pixel similarity and brightness only partly explain the performances of rats and humans.â</p><disp-quote content-type="editor-comment"><p>Did the rats rely on consistent visual features to perform the tasks? I assume the split-half analysis was on data pooled across rats. What was the average correlation between rats? Were rats more internally consistent (split-half within rat) than consistent with other rats?</p></disp-quote><p>The split-half analysis was indeed performed on data pooled across rats. We checked whether rats are more internally consistent by comparing the split-half within correlations with the split-half between correlations. For the split-half within correlations, we split the data for each rat in two subsets and calculated the performance vectors (performance across all image pairs). We then calculated the correlation between these two vectors for each animal. To get the split-half between correlation, we calculated the correlation between the performance vector of every subset data of every rat with every other subset data from the other rats. Finally, we compared for each animal its split-half within correlation with the split-half between correlations involving that animal. The result of this paired t-test (p = 0.93, 95%CI [-0.09; 0.08]) suggests that rats were not internally more consistent.</p><disp-quote content-type="editor-comment"><p>Discussion of the cDNN performance and its relation to rat behavior could be expanded and clarified in several ways:</p><p>â¢ The paper would benefit from further discussion regarding the low correlations between rat behavior and cDNN layers. Is the main message that cDNNs are not a suitable model for rat vision? Or can we conclude that the peak in mid layers indicates that rat behavior reflects mid-level visual processing? It would be valuable to explore what we currently know about the organization of the rat visual cortex and how applicable these models are to their visual system in terms of architecture and hierarchy.</p></disp-quote><p>We added a consideration to the manuscript about which network to use (see Discussion).</p><disp-quote content-type="editor-comment"><p>â¢ The cDNN exhibited above chance performance in various early layers for several test protocols (e.g., rotations, light location, combination rotation). Does this limit the interpretation of the complexity of visual behavior required to perform these tasks?</p></disp-quote><p>This is not uncommon to find. Pinto et al. (2008) already revealed that a simple V1-like model can sometimes result in surprisingly good object recognition performance. This aspect of our findings is also in line with the observation of Vinken &amp; Op de Beeck (2021) that the performance of rats in many previous tasks might not be indicative of highly complex representations. Nevertheless, there is still a relative difference in complexity between lower and higher levels in the hierarchy. That is what we capitalize upon with the High vs zero and the Zero vs high protocols. Thus, it might be more fruitful to explicitly contrast different levels of processing in a relative way rather than trying to pinpoint behavior to specific levels of processing. This argumentation is added to the Discussion section.</p><disp-quote content-type="editor-comment"><p>â¢ How representative is the correlation profile between cDNN layers and behavior across protocols? Pooling stimuli across protocols may be necessary to obtain stable correlations due to relatively modest sample numbers. However, the authors could address how much each individual protocol influences the overall correlations in leave-one-out analyses. Are there protocols where rat behavior correlates more strongly with higher layers (e.g., when excluding zero vs. high)?</p></disp-quote><p>We prefer to base our conclusions mostly on the pooled analyses rather than individual protocols. As the reviewer also mentions, we can expect that the pooled analyses will provide the most stable results. For information, we included leave-one-out analyses in the supplemental material. Excluding the Zero vs. High protocol did not result in a stronger correlation with the higher layers. It was rare to see correlations with higher layers, and in the one case that we did (when excluding High versus zero) the correlations were still higher in several mid-level layers.</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87719-sa3-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>â¢ The authors hypothesize that the cDNN results indicate that rats rely on visual features such as contrast. Can this link be established more firmly? e.g., what are the receptive fields in the layers that correlate with rat behavior sensitive to?</p></disp-quote><p>This hypothesis was made based on previous in-lab research (Schnell et al., 2023) where we found rats indeed rely on contrast features. In this study, we performed a face categorization task, parameterized on contrast features, and we investigated to what extent rats use contrast features to perform in a face categorization task. Similarly as in the current study, we used a DNN that as trained and tested on the same stimuli as the animals to investigate the representations of the animals. There, we found that the animals use contrast features to some extent and that this correlated best with the lower layers of the network. Hence, we would say that the lower layers correlate best with rat behaviour that is sensitive to contrast. Earlier layers of the network include local filters that simulate V1-like receptive fields. Higher layers of the network, on the other hand, are used for object selectivity.</p><disp-quote content-type="editor-comment"><p>â¢ There seems to be a disconnect between rat behavior and the selection of stimuli for the high (zero) vs. zero (high) protocols. Specifically, rat behavior correlated best with mid layers, whereas the image selection process relied on earlier layers. What is the interpretation when rat behavior correlates with higher layers than those used to select the stimuli?</p></disp-quote><p>We agree that it is difficult to pinpoint a particular level of processing, and it might be better to use relative terms: lower/higher than. This is addressed in the manuscript by the edit in response to three comments back.</p><disp-quote content-type="editor-comment"><p>â¢ To what extent can we attribute the performance below the ceiling for many protocols to sensory/perceptual limitations as opposed to other factors such as task structure, motivation, or distractibility?</p></disp-quote><p>We agree that these factors play a role in the overall performance difference. In Figure 5, the most right bar shows the percentage of all animals (light blue) vs all humans (dark blue) on the old pair that was presented during the testing protocol. Even here, the performance of the animals was lower than humans, and this pattern extended to the testing protocols as well. This was most likely due to motivation and/or distractibility which we know can happen in both humans and rats but affects the rat results more with our methodology.</p><disp-quote content-type="editor-comment"><p>Minor recommendations:</p><p>â¢ What was the trial-to-trial variability in the distance and position of the rat's head relative to the stimuli displayed on the screen? Can this variability be taken into account in the size and position protocols? How meaningful is the cDNN modelling of these protocols considering that the training and testing of the model does not incorporate this trial-to-trial variability?</p></disp-quote><p>We have no information on this trial-to-trial variability. We have information though on what rats typically do overall from an earlier paper that was mentioned in response to an earlier comment (Crijns et al.).</p><p>We have added a disclaimer in the Discussion on our lack of information on trial-to-trial variability.</p><disp-quote content-type="editor-comment"><p>â¢ Several of the protocols varied a visual feature dimension (e.g., concavity &amp; alignment) relative to the base pair. Did rat performance correlate with these manipulations? How did rat behavior relate to pixel dissimilarity, either between target and distractor or in relation to the trained base pair?</p></disp-quote><p>We have added this to the Discussion. See also our general comments in the Public responses.</p><disp-quote content-type="editor-comment"><p>â¢ What could be the underlying factor(s) contributing to the difference in accuracy between the &quot;small transformations&quot; depicted in Figure 2 and some of the transformations displayed in Figure 3? In particular, it seems that the variability of targets and distractors is greater for the &quot;small transformations&quot; in Figure 2 compared to the rotation along the y-axis shown in Figure 3.</p></disp-quote><p>There are several differences between these protocols. Before considering the stimulus properties, we should take into account other factors. The Transformations protocol was a training protocol, meaning that the animals underwent several sessions in this protocol, always receiving real reward during the trials, and only stopping once a high enough performance was reached. For the protocols in Figure 3, the animals were also placed in these protocols for multiple sessions in order to obtain enough trials, however, the difference here is that they did not receive real reward and testing was also stopped if performance was still low.</p><disp-quote content-type="editor-comment"><p>â¢ In Figure 3, it is unclear which pairwise transformation accuracies were above chance. It would be helpful if the authors could indicate significant cells with an asterisk. The scale for percentage correct is cut off at 50%. Were there any instances where the behaviors were below 50%? Specifically, did the rats consistently choose the wrong option for any of the pairs?It would be helpful to add &quot;old pair&quot;, &quot;concavity&quot; and &quot;alignment&quot; to x-axis labels in Fig 2A .</p></disp-quote><p>We have added âoldâ, âconcâ and âalignâ to the x-axis labels in Figure 2A.</p><disp-quote content-type="editor-comment"><p>â¢ Considering the overall performance across protocols, it seems overstated to claim that the rats were able to &quot;master the task.&quot;</p></disp-quote><p>When talking about âmastering the taskâ, we talk about the training protocols where we aimed that the animals would perform at 80% and not significantly less. We checked this throughout the testing protocols as well, where we also presented the old pair as quality control, and their performance was never significantly lower than our 80% performance threshold on this pair, suggesting that they mastered the task in which they were trained. To avoid discussion on semantics, we also rephrased âmaster the taskâ into âlearn the taskâ.</p><disp-quote content-type="editor-comment"><p>â¢ What are the criteria for the claim that the &quot;animal model of choice for vision studies has become the rodent model&quot;? It is likely that researchers in primate vision may hold a different viewpoint, and data such as yearly total publication counts might not align with this claim.</p></disp-quote><p>Primate vision is important for investigating complex visual aspects. With the advancements in experimental techniques for rodent vision, e.g. genetics and imaging techniques as well as behavioural tasks, the rodent model has become an important model as well. It is not necessarily an âeitherâ or âorâ question (primates or rodents), but more a complementary issue: using both primates and rodents to unravel the full picture of vision.</p><p>We have changed this part in the introduction to âLately, the rodent model has become an important model in vision studies, motivated by the applicability of molecular and genetic tools rather than by the visual capabilities of rodentsâ.</p><disp-quote content-type="editor-comment"><p>â¢ The correspondence between the list of layers in Supplementary Tables 8 and 9 and the layers shown in Figures 4 and 6 could be clarified.</p></disp-quote><p>We have clarified this in the caption of Figure 7</p><disp-quote content-type="editor-comment"><p>â¢ The titles in Figures 4 and 6 could be updated from &quot;DNN&quot; to &quot;cDNN&quot; to ensure consistency with the rest of the manuscript.</p></disp-quote><p>Thank you for your feedback. We have changed the titles in Figures 4 and 6 such that they are consistent with the rest of the manuscript.</p></body></sub-article></article>