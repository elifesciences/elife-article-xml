<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">70661</article-id><article-id pub-id-type="doi">10.7554/eLife.70661</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Minian, an open-source miniscope analysis pipeline</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-144544"><name><surname>Dong</surname><given-names>Zhe</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7366-8939</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-209211"><name><surname>Mau</surname><given-names>William</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3233-3243</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund17"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-144162"><name><surname>Feng</surname><given-names>Yu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-239929"><name><surname>Pennington</surname><given-names>Zachary T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-239930"><name><surname>Chen</surname><given-names>Lingxuan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-168818"><name><surname>Zaki</surname><given-names>Yosif</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-63585"><name><surname>Rajan</surname><given-names>Kanaka</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund13"/><xref ref-type="other" rid="fund14"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-144164"><name><surname>Shuman</surname><given-names>Tristan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2310-6142</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund18"/><xref ref-type="other" rid="fund19"/><xref ref-type="other" rid="fund20"/><xref ref-type="other" rid="fund21"/><xref ref-type="other" rid="fund22"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-273117"><name><surname>Aharoni</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4931-8514</contrib-id><email>dbaharoni@gmail.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund15"/><xref ref-type="other" rid="fund16"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-67465"><name><surname>Cai</surname><given-names>Denise J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7729-0523</contrib-id><email>denisecai@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Nash Family Department of Neuroscience, Icahn School of Medicine at Mount Sinai</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>Department of Neurology, David Geffen School of Medicine, University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e70661</elocation-id><history><date date-type="received" iso-8601-date="2021-05-25"><day>25</day><month>05</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-31"><day>31</day><month>05</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-05-04"><day>04</day><month>05</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.05.03.442492"/></event></pub-history><permissions><copyright-statement>© 2022, Dong et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Dong et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-70661-v2.pdf"/><abstract><p>Miniature microscopes have gained considerable traction for in vivo calcium imaging in freely behaving animals. However, extracting calcium signals from raw videos is a computationally complex problem and remains a bottleneck for many researchers utilizing single-photon in vivo calcium imaging. Despite the existence of many powerful analysis packages designed to detect and extract calcium dynamics, most have either key parameters that are hard-coded or insufficient step-by-step guidance and validations to help the users choose the best parameters. This makes it difficult to know whether the output is reliable and meets the assumptions necessary for proper analysis. Moreover, large memory demand is often a constraint for setting up these pipelines since it limits the choice of hardware to specialized computers. Given these difficulties, there is a need for a low memory demand, user-friendly tool offering interactive visualizations of how altering parameters at each step of the analysis affects data output. Our open-source analysis pipeline, Minian (miniscope analysis), facilitates the transparency and accessibility of single-photon calcium imaging analysis, permitting users with little computational experience to extract the location of cells and their corresponding calcium traces and deconvolved neural activities. Minian contains interactive visualization tools for every step of the analysis, as well as detailed documentation and tips on parameter exploration. Furthermore, Minian has relatively small memory demands and can be run on a laptop, making it available to labs that do not have access to specialized computational hardware. Minian has been validated to reliably and robustly extract calcium events across different brain regions and from different cell types. In practice, Minian provides an open-source calcium imaging analysis pipeline with user-friendly interactive visualizations to explore parameters and validate results.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>calcium imaging</kwd><kwd>Miniscope</kwd><kwd>analysis pipeline</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>DP2MH122399-01</award-id><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01MH120162-01A1</award-id><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>One Mind</institution></institution-wrap></funding-source><award-id>Otsuka Rising Star Award</award-id><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005270</institution-id><institution>McKnight Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Klingenstein-Simons Fellowship</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007277</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap></funding-source><award-id>Distinguished Scholar Award</award-id><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000882</institution-id><institution>Brain Research Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>NARSAD Young Investigator Award</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Cai</surname><given-names>Denise J</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution>NIH</institution></institution-wrap></funding-source><award-id>R01EB028166</award-id><principal-award-recipient><name><surname>Rajan</surname><given-names>Kanaka</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><award-id>220020466</award-id><principal-award-recipient><name><surname>Rajan</surname><given-names>Kanaka</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1926800</award-id><principal-award-recipient><name><surname>Rajan</surname><given-names>Kanaka</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>2046583</award-id><principal-award-recipient><name><surname>Rajan</surname><given-names>Kanaka</given-names></name></principal-award-recipient></award-group><award-group id="fund13"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>891834</award-id><principal-award-recipient><name><surname>Rajan</surname><given-names>Kanaka</given-names></name></principal-award-recipient></award-group><award-group id="fund14"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Alfred P. Sloan Foundation</institution></institution-wrap></funding-source><award-id>FG-2019-12027</award-id><principal-award-recipient><name><surname>Rajan</surname><given-names>Kanaka</given-names></name></principal-award-recipient></award-group><award-group id="fund15"><funding-source><institution-wrap><institution>NIH</institution></institution-wrap></funding-source><award-id>U01NS094286-01</award-id><principal-award-recipient><name><surname>Aharoni</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund16"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1700408 Neurotech Hub</award-id><principal-award-recipient><name><surname>Aharoni</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund17"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32AG067640</award-id><principal-award-recipient><name><surname>Mau</surname><given-names>William</given-names></name></principal-award-recipient></award-group><award-group id="fund18"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100002039</institution-id><institution>CURE</institution></institution-wrap></funding-source><award-id>Epilepsy Taking Flight Award</award-id><principal-award-recipient><name><surname>Shuman</surname><given-names>Tristan</given-names></name></principal-award-recipient></award-group><award-group id="fund19"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001454</institution-id><institution>American Epilepsy Society</institution></institution-wrap></funding-source><award-id>Junior investigator Award</award-id><principal-award-recipient><name><surname>Shuman</surname><given-names>Tristan</given-names></name></principal-award-recipient></award-group><award-group id="fund20"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R03 NS111493</award-id><principal-award-recipient><name><surname>Shuman</surname><given-names>Tristan</given-names></name></principal-award-recipient></award-group><award-group id="fund21"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21 DA049568</award-id><principal-award-recipient><name><surname>Shuman</surname><given-names>Tristan</given-names></name></principal-award-recipient></award-group><award-group id="fund22"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 NS116357</award-id><principal-award-recipient><name><surname>Shuman</surname><given-names>Tristan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Minian is an open-source analysis pipeline for calcium imaging data that enhances usability with low memory demands and transparency with user-friendly interactive visualizations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><sec id="s1-1"><title>Overview of related works</title><p>Open-source projects – hardware, software, training curricula – have changed science and enabled significant advances across multiple disciplines. Neuroscience, in particular, has benefited tremendously from the open-source movement. Numerous open-source projects have emerged (<xref ref-type="bibr" rid="bib51">White et al., 2019</xref>; <xref ref-type="bibr" rid="bib12">Freeman, 2015</xref>), including various types of behavioral apparatus facilitating the design of novel experiments (<xref ref-type="bibr" rid="bib7">Buccino et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Frie and Khokhar, 2019</xref>; <xref ref-type="bibr" rid="bib29">Lopes et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Nguyen et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Matikainen-Ankney et al., 2021</xref>), computational tools enabling the analysis of large-scale datasets (<xref ref-type="bibr" rid="bib25">Kabra et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">van den Boom et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Zhou et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Mukamel et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Klibisz et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Sheintuch et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Pennington et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Friedrich et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Giovannucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Jewell and Witten, 2018</xref>), and recording devices allowing access to large populations of neurons in the brain (<xref ref-type="bibr" rid="bib2">Aharoni et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Owen and Kreitzer, 2019</xref>; <xref ref-type="bibr" rid="bib44">Siegle et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Solari et al., 2018</xref>; <xref ref-type="bibr" rid="bib3">Barbera et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Jacob et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Liberti et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">de Groot et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Skocek et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Scott et al., 2018</xref>). Miniature microscopy has been an area of particular importance for the open-source movement in neuroscience. To increase the usability, accessibility, and transparency of this remarkable technology originally developed by Schnitzer and colleagues (<xref ref-type="bibr" rid="bib16">Ghosh et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Ziv et al., 2013</xref>), a number of labs innovated on top of the original versions with open-source versions (<xref ref-type="bibr" rid="bib3">Barbera et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Jacob et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Liberti et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">de Groot et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Skocek et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Scott et al., 2018</xref>). The UCLA Miniscope project, a user-friendly miniature head-mounted microscope for in vivo calcium imaging in freely behaving animals, is one such project that has been accessible to a large number of users (<xref ref-type="bibr" rid="bib2">Aharoni et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Cai et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Shuman et al., 2020</xref>; <xref ref-type="bibr" rid="bib1">Aharoni and Hoogland, 2019</xref>).</p><p>With the increasing popularity of miniature microscopes, there is a growing need for analysis pipelines that can reliably extract neuronal activities from recording data. To address this need, numerous algorithms have been developed and made available to the neuroscience community. The principal component analysis or independent component analysis (PCA-ICA)-based approach (<xref ref-type="bibr" rid="bib33">Mukamel et al., 2009</xref>) and region of interest (ROI)-based approach (<xref ref-type="bibr" rid="bib8">Cai et al., 2016</xref>) were among the earliest algorithms that reliably detected the locations of neurons and extract their overall activities across pixels. However, one of the limitations of these approaches is that activities from cells that are spatially overlapping cannot be demixed. A subsequent constrained non-negative matrix factorization (CNMF) approach was shown to reliably extract neuronal activity from both two-photon and single-photon calcium imaging data (<xref ref-type="bibr" rid="bib39">Pnevmatikakis et al., 2016</xref>), and demix the activities of overlapping cells. The CNMF algorithm models the video as a product of a ‘spatial’ matrix containing detected neuronal footprints (locations of cells) and a ‘temporal’ matrix containing the temporal calcium traces of each detected cell. This approach is particularly effective at addressing crosstalk between neurons, which is of particular concern in single-photon imaging, where the fluorescence from overlapping or nearby cells contaminates each other. Moreover, by deconvolving calcium traces, the CNMF algorithm enables a closer exploration of the underlying activity of interest, action potentials (<xref ref-type="bibr" rid="bib14">Friedrich et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Vogelstein et al., 2010</xref>). Originally developed for two-photon data, the CNMF algorithm did not include an explicit model of the out-of-focus fluorescence that is often present in single-photon miniature microscope recordings. This issue was addressed via the CNMF-E algorithm (<xref ref-type="bibr" rid="bib52">Zhou et al., 2018</xref>), where a ring model is used as a background term to account for out-of-focus fluorescence. Later, an open-source Python pipeline for calcium imaging analysis, CaImAn, was published, which included both the CNMF and CNMF-E algorithms, as well as many other functionalities (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>). The latest development in analysis pipelines for in vivo miniature microscope data is MIN1PIPE (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>), where a morphological operation is used to remove background fluorescence during preprocessing of the data, and a seed-based approach is used for initialization of the CNMF algorithm. Other approaches have also been used to extract signals from calcium imaging data, including an online approach (<xref ref-type="bibr" rid="bib17">Giovannucci et al., 2017</xref>), <inline-formula><mml:math id="inf1"><mml:mi mathvariant="script">l</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula> -penalization approach to infer spikes (<xref ref-type="bibr" rid="bib24">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Jewell and Witten, 2018</xref>), robust modeling of noise (<xref ref-type="bibr" rid="bib21">Inan et al., 2021</xref>), and source detection using neural networks (<xref ref-type="bibr" rid="bib26">Klibisz et al., 2017</xref>).</p><p>The open sharing of the algorithms necessary for the computation of neural activity has been exceptionally important for the field. However, implementation of these tools can be complex as many algorithms have numerous free parameters (those that must be set by the user) that can influence the outcomes, without clear guidance on how these parameters should be set or to what extent they affect results. Moreover, there is a lack of ground-truth data for in vivo miniature microscope imaging, making it hard to validate algorithms and/or parameters. Together, these obstacles make it challenging for neuroscience labs to adopt the analysis pipelines since it is difficult for researchers to adjust parameters to fit their data or to trust the output of the pipeline for downstream analysis. Thus, the next challenge in open-source analysis pipelines for calcium imaging is to make the analysis tools more user-friendly and underlying algorithms more accessible to neuroscience researchers so that they can more easily understand the pipeline and interpret the results.</p></sec><sec id="s1-2"><title>Contributions of Minian</title><p>To increase the accessibility of the mathematical algorithms, transparency into how altering parameters alters the data output, and usability for researchers with limited computational resources and experience, we developed Minian (miniscope analysis), an open-source analysis pipeline for single-photon calcium imaging data inspired by previously published algorithms. We based Minian on the CNMF algorithm (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Pnevmatikakis et al., 2016</xref>), but also leverage methods from other pipelines, including those originally published by <xref ref-type="bibr" rid="bib8">Cai et al., 2016</xref> and MIN1PIPE (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>). To enhance compatibility with different types of hardware, especially laptops or personal desktop computers, we implemented an approach that supports parallel and out-of-core computation (i.e., computation on data that are too large to fit a computer’s memory). We then developed interactive visualizations for every step in Minian and integrated these steps into annotated Jupyter Notebooks as an interface for the pipeline. We have included detailed notes and discussions on how to adjust the parameters from within the notebook and have included all free parameters in the code for additional flexibility. The interactive visualizations will help users to intuitively understand and visually inspect the effect of each parameter, which we hope will facilitate more usability, transparency, and reliability in calcium imaging analysis.</p><p>Minian contributes to three key aspects of calcium image data analysis:</p><list list-type="order"><list-item><p><italic>Visualization</italic>. For each step in the pipeline, Minian provides visualizations of inputs and results. Thus, users can proceed step-by-step with an understanding of how the data are transformed and processed. In addition, all visualizations are interactive and support simultaneous visualization of the results obtained with different parameters. This feature provides users with knowledge about the corresponding outcome for each parameter value and allows the users to choose the outcome that fits best with their expectation. Hence, the visualizations also facilitate parameter exploration for each step, which is especially valuable when analyzing data from heterogeneous origins that may vary by brain region, cell type, species, and the extent of viral transfection.</p></list-item><list-item><p><italic>Memory demand</italic>. One of the most significant barriers in adopting calcium imaging pipelines is the memory demand of algorithms. The recorded imaging data usually take up tens of gigabytes of space when converted to floating-point datatypes and often cannot fit into the RAM of standard computers without spatially and/or temporally downsampling. CaImAn (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>) addresses this issue by splitting the data into overlapping patches of pixels, processing each patch independently, and merging the results together. This enables out-of-core computation since at any given time only subsets of data are needed and loaded into memory. In Minian, we extend this concept further by flexibly splitting the data either spatially (split into patches of pixels) or temporally (split into chunks of frames). In this way, we avoid the need to merge the results based on overlapping parts. The result is a pipeline that supports out-of-core computation at each step, which gives nearly constant memory demand with respect to input data size. Minian can process more than 20 min of recording (approximately 12.6 GB of raw data) with 8 GB of memory, which makes Minian suitable to be deployed on modern personal laptops.</p></list-item><list-item><p><italic>Accessibility</italic>. Minian is an open-source Python package. In addition to the codebase, Minian distributes several Jupyter Notebooks that integrate explanatory text with code and interactive visualizations of results. For each step in the notebook, detailed instructions, as well as intuition about the underlying mathematical formulation, are provided, along with code, which can be directly executed from within the notebook. Upon running a piece of code within the notebook, visualizations appear directly below. In this way, the notebooks serve as a complement to traditional API documentations of each function. In addition, users can easily rearrange and modify the pipeline notebook to suit their needs without diving into the codebase and modifying the underlying functions. The notebooks distributed by Minian can simultaneously function as a user guide, template, and production tool. We believe the inclusion of these notebooks, in combination with Minian’s other unique features, can increase understanding of the underlying functioning of the algorithms and greatly improve the accessibility of miniature microscopy analysis pipelines.</p></list-item></list></sec><sec id="s1-3"><title>Article organization</title><p>This article is organized as follows. Since Minian’s major contribution is usability and accessibility, we first present the detailed steps in the analysis pipeline in ‘Materials and methods’. Following a step-by-step description of the algorithms Minian adapted from existing works, we present novel visualizations of the results, as well as how users can utilize these visualizations. In ‘Results’, we benchmark Minian across two brain regions and show that spatial footprints and the temporal activity of cells can be reliably extracted. We also show that the cells extracted by Minian in hippocampal CA1 exhibit stable spatial firing properties consistent with the existing literature.</p></sec></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><p>Here, we present a detailed description of Minian. We begin with an overview of the Minian pipeline. Then, we provide an explanation of each step, along with the visualizations. Lastly, we provide information regarding hardware and dependencies.</p><sec id="s2-1"><title>Overview of Minian</title><p>Minian comprises five major stages, as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Raw videos are first passed into a preprocessing stage. During preprocessing, the background caused by vignetting (in which the central portion of the field of view is brighter) is corrected by subtracting a minimum projection of the movie across time. Sensor noise, evident as granular specks, is then corrected with a median filter. Finally, background fluorescence is corrected by the morphological process introduced in MIN1PIPE (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>). The preprocessed video is then motion-corrected with a standard template-matching algorithm based on cross-correlation between each frame and a reference frame (<xref ref-type="bibr" rid="bib6">Brunelli, 2009</xref>). The motion-corrected and preprocessed video then serves as the input to initialization and CNMF algorithms. The seed-based initialization procedure looks for local maxima in max projections of different subsets of frames and then generates an over-complete set of seeds, which are candidate pixels for detected neurons. Because this process is likely to produce many false positives, seeds are then further refined based on various metrics, including the amplitude of temporal fluctuations and the signal-to-noise ratio of temporal signals. The seeds are transformed into an initial estimation of cells’ spatial footprints based on the correlation of neighboring pixels with each seed pixel, and the initial temporal traces are in turn estimated based on the weighted temporal signal of spatial footprints. Finally, the processed video, initial spatial matrix, and temporal matrix are fed into the CNMF algorithm. The CNMF algorithm first refines the spatial footprints of the cells (spatial update). The algorithm then denoises the temporal traces of each cell while simultaneously deconvolving the calcium trace into estimated ‘spikes’ (temporal update). CNMF spatial and temporal updates are performed iteratively and can be repeated until a satisfactory result is reached through visual inspection. Typically, this takes two cycles of spatial, followed by temporal, updates. Minian also includes a demo dataset that allows the user to run and test the pipeline comprised of the pre-made Jupyter Notebook immediately after installation.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the analysis pipeline.</title><p>The analysis is divided into five stages: preprocessing, where sensor noise and background fluorescence from scattered light are removed; motion correction, where rigid motion of the brain is corrected; seeds initialization, where the initial spatial and temporal matrices for later steps are generated from a seed-based approach; spatial update, where the spatial footprints of cells are further refined; and temporal update, where the temporal signals of cells are further refined. The last two steps of the pipeline are iterative and can be repeated multiple times until a satisfactory result is reached.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Setting up</title><p>The first section in the pipeline includes house-keeping scripts to import packages and functions, defining parameters, and setting up parallel computation and visualization. Most notably, the distributed cluster that carries out all computations in Minian are set up in this section. By default, the cluster runs locally with multicore CPUs; however, it can be easily scaled up to run on distributed computers. The computation in Minian is optimized such that in most cases the memory demand for each process/core can be as low as 2 GB. However, in some cases depending on the hardware, the state of operating system and data locality, Minian might need more than 2 GB per process to run. If a memory error (KilledWorker) is encountered, it is common for users to increase the memory limit of the distributed cluster to get around the error. Regardless of the exact memory limit per process, the total memory usage of Minian roughly scales linearly with the number of parallel processes. The number of parallel processes and memory usage of Minian is completely limited and managed by the cluster configuration, allowing users to easily change them to suit their needs.</p></sec><sec id="s2-3"><title>Preprocessing</title><sec id="s2-3-1"><title>Loading data and downsampling</title><p>Currently Minian supports .avi movies, the default output from the UCLA Miniscopes, and .tif stacks, the default output from Inscopix miniscopes. This functionality can be easily extended to support more formats if desired. Users are required to organize their data so that each recording session is contained in a single folder. Because Minian can extract relevant metadata from folder nomenclature (e.g., animal name, group, date), we suggest organizing the video folders based upon animal and other experiment-related groupings to facilitate the incorporation of metadata into Minian output files.</p><p>Minian supports downsampling on any of the three video dimensions (height, width, and frames). Two downsampling strategies are currently implemented: either subsetting data on a regular interval or calculating a mean for each interval. At this stage, users are required to specify (1) the path to their data, (2) a pattern of file names to match all the videos to be processed (e.g., all files containing ‘msCam,’ a typical pattern resulting from Miniscope recordings), (3) a Python dictionary specifying whether and how metadata should be pulled from folder names, (4) another Python dictionary specifying whether and on which dimension downsampling should be carried out, and (5) the downsampling strategy, if desired.</p><p>Once specified, the data can be immediately visualized through an interactive viewer, as shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Along with a player to visualize every frame in the video, the viewer also plots summary values such as mean, maximum, or minimum fluorescence values across time. This helps users to check their input data and potentially exclude any artifacts caused by technical faults during experiments (e.g., dropped frames). Users can further subset data to exclude specified frames, if necessary. Finally, restricting the analysis to a certain subregion of the field of view during specific steps could be beneficial. For example, if the video contains anchoring artifacts resulting from dirt on the lenses, it is often better to avoid such regions during motion correction. To facilitate this, the viewer provides a feature where users can draw an arbitrary box within the field of view and have it recorded as a mask. This mask can be passed into later motion correction steps to avoid the biases resulting from the artifacts.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Interactive visualization of raw input video.</title><p>One frame is shown in the central panel of the visualization that can be interactively updated with the player toolbar on the top. A histogram of fluorescence intensity of the current frame is shown on the right and will update in response to zooming in on the central frame. A line plot of summary values across time is shown at the bottom. Here, the maximum, mean, and minimum fluorescence values are plotted. These summaries are useful in checking whether there are unexpected artifacts or gaps in the recording. Finally, the user can draw an arbitrary box in the central frame, and the position of this boxed region can be recorded and used as a mask during later steps. For example, during motion correction a subregion of the data containing a stable landmark might provide better information on the motion.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig2-v2.tif"/></fig></sec><sec id="s2-3-2"><title>Vignetting correction</title><p>Single-photon miniature microscope data often suffer from a vignetting effect in which the central portion of the field of view appears brighter than the periphery. Vignetting is deleterious to subsequent processing steps and should be removed. We find that the effect can be easily extracted by taking the minimum fluorescence value across time for each pixel and subtracting this value from each frame, pixel-wise. One of the additional benefits of subtracting the minimum is that it preserves the raw video’s linear scale.</p><p>The result of this step can be visualized with the same video viewer used in the previous step. In addition to visualizing a single video, the viewer can also show multiple videos side-by-side (e.g., the original video and the processed video), as shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The operation/visualization is carried out ‘on-the-fly’ upon request for each frame, and users do not have to wait for the operation to finish on the whole video to view the results.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>General visualization of preprocessing.</title><p>The same visualization of input video can be used to visualize the whole video before and after specific preprocessing steps side-by-side. The effect of vignetting correction is visualized here. The image and accompanying histogram on the left side show the original data; the data after vignetting correction are shown on the right side. Any frame of the data can be selected with the player toolbar and histograms are responsive to all updates in the image.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig3-v2.tif"/></fig></sec><sec id="s2-3-3"><title>Denoising</title><p>Next, we correct for salt-and-pepper noise on each frame, which usually results from electronic pixel noise. By default, we pass each frame through a median filter, which is generally considered particularly effective at eliminating this type of noise, though other smoothing filters like Gaussian filters and anisotropic filters can also be implemented. The critical parameter here is the window size of the median filter. A window size that is too small will make the filter ineffective at correcting outliers, while a window size that is too large will remove finer gradient and edges that are much smaller than the window size, and can result in a failure to distinguish between adjacent cells.</p><p>The effect of the window size can be checked with an interactive visualization tool used across the preprocessing stage, as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Additionally, here we show an example of the effect of window size on the resulting data in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Users should see significantly reduced amount of salt-and-pepper noise in the images, which should be made more obvious by the contour plots. At the same time, users should keep the window size below the extent where over-smoothing occurs. As a heuristic, the average cell radius in pixel units works well since a window of the same size as an average cell is unlikely to blend different cells together, while still being able to adequately smooth the image.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Visualization of denoising.</title><p>Here, a single frame from the data is passed through the background removal, and both the image and a contour plot are shown for the frame before and after the process. The contour plots show the iso-contour of five intensity levels spaced linearly across the full intensity range of the corresponding image. The plots are interactive and responsive to the slider of the window size on the right, thus the effect of different window sizes for denoising can be visualized.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig4-v2.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Effect of window size on denoising.</title><p>One example frame is chosen from the data, and the resulting images (top row) and contour plots (bottom row) are shown to demonstrate the effect of window size on denoising. Here, a window size of 11 (middle column) is appropriate while both smaller and larger window sizes result in artifacts.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig5-v2.tif"/></fig></sec><sec id="s2-3-4"><title>Morphological background removal</title><p>Next, we remove any remaining background presumably caused by out-of-focus and tissue fluorescence. To accomplish this, we estimate the background using a morphological opening process first introduced for calcium imaging analysis in MIN1PIPE (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>), which acts as a size filter that removes cell bodies. The morphological opening is composed of two stages: erosion followed by dilation. In morphological erosion, the image is passed through a filter where each pixel will be substituted by the minimum value within the filter window. The effect of this process is that any bright ‘feature’ that is smaller than the filter window will be ‘eroded’ away. Then, the dilation process accomplishes the reverse by substituting each pixel with the maximum value in the window, which ‘dilates’ small bright features to the extent of the filter window size. The combined effect of these two stages is that any bright ‘feature’ that is smaller than the filter window is removed from the image. If we choose the window size to match the expected cell diameter, performing a morphological opening will likely remove cells and provide a good estimation of background. Hence, each frame is passed through the morphological opening operation and the resulting image is subtracted from the original frame.</p><p>Although the window size parameter for the morphological opening can be predetermined by the expected cell diameter, it is helpful to visually inspect the effect of morphological background removal. The effect of different window sizes can be visualized with the same tool used in denoising, as shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Additionally, here we show an example of the effect of window size on the resulting data in <xref ref-type="fig" rid="fig7">Figure 7</xref>. In this case, a window size of 20 pixels is considered appropriate because the resulting cells are appropriately sized and sharply defined. In contrast, a smaller window results in limiting both the size and intensity of the cells. On the other hand, residual out-of-focus fluorescence becomes visible when the window size is set too large.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Visualization of background removal.</title><p>Here, a single frame from the data is passed through background removal, and both the image and a contour plot are shown for the frame before and after the process. The plots are interactive and responsive to the slider of the window size on the right, thus the effect of different window sizes for background removal can be visualized.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig6-v2.tif"/></fig><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Effect of window size on background removal.</title><p>One example frame is chosen from the data, and the resulting images (top row) and contour plots (bottom row) are shown to demonstrate the effect of window size on background removal. The contour plots show the iso-contour of five intensity levels spaced linearly across the full intensity range of the corresponding image. Here, a window size of 20 pixels (middle column) is appropriate while both smaller and larger window sizes produce unsatisfactory results: a window size too small (left column) artificially limits the size of cells, and a window size too large (right column) does not remove the background effectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig7-v2.tif"/></fig></sec></sec><sec id="s2-4"><title>Motion correction</title><sec id="s2-4-1"><title>Estimate and apply translational shifts</title><p>We use a standard template-matching algorithm based on cross-correlation to estimate and correct for translational shifts (<xref ref-type="bibr" rid="bib6">Brunelli, 2009</xref>). In practice, we found that this approach is sufficient to correct for motion artifacts that could have a significant impact on the final outcome. Briefly, for a range of possible shifts, a cross-correlation between each frame and a template frame is calculated. The shift producing the largest cross-correlation is estimated to reflect the degree of movement from the template and is corrected by applying a shift to the frame in that direction. We apply this operation to the whole movie in a divide-and-conquer manner. We split the movie into chunks of frames, within which we register both the first and last frames to the middle frame. We then take the max projections of the three frames that have been registered in each chunk and group every three chunks together and register them using the max projections as templates. After the registration, the three chunks that have been registered are treated as a new single chunk and we again take the max projection to use as a template for further registration. In this way, the number of frames registered in each chunk keeps increasing in powers of 3 (3, 9, 27, 81, etc.), and we repeat this process recursively until all the frames are covered in a single chunk and the whole movie is registered. Since the motion correction is usually carried out after background removal, we essentially use cellular activity as landmarks for registration. Sometimes this can be problematic when cellular activity is very sparse and different across two chunks (e.g., when only two different cells fired in two chunks), leading to false estimation of shifts. To overcome this problem, every time shift is estimated using a max projection from two chunks, we also estimate a shift with the two consecutive frames bordering the chunks (i.e., the last frame from the earlier chunk and the first frame from the latter chunk). In most cases, the shifts estimated with these two sets of templates should be close, in which case we use the shifts estimated with the max projection as the final output. However, when the two estimated shifts differ too much from each other, we use the shifts estimated with consecutive frames as the final output. The reason we still favor using max projections in most cases is that registering with consecutive frames can lead to very fast accumulation of error and a slow drifting artifact in the estimated shifts. In practice, we find that such a process can account for almost all motion in the brain, so currently we only implemented estimation of translational shifts. If the user would like to take advantage of anatomical landmarks (such as blood vessels) within the field of view and would like to implement motion correction before all background subtraction steps have been performed, the pipeline can be easily modified to do so. After the estimation of shifts, the shift in each direction is plotted across time and visualization of the data before and after motion correction is displayed in Minian (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, top right).</p></sec></sec><sec id="s2-5"><title>Seed initialization</title><sec id="s2-5-1"><title>Generation of an over-complete set of seeds</title><p>The CNMF algorithm is a powerful approach to extract cells’ spatial structure and corresponding temporal activity. However, the algorithm requires an initial estimate of cell locations/activity, which it then refines. We use a seed-based approach introduced in MIN1PIPE (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>) to initialize spatial and temporal matrices for CNMF. The first step is to generate an over-complete set of seeds, representing the potential centroids of cells. We iteratively select a subset of frames, compute a maximum projection for these frames, and find the local maxima on the projections. This workflow is repeated multiple times, and we take the union of all local maxima across repetitions to obtain an over-complete set of seeds. In this way, we avoid missing cells that only fire in short periods of time that might be masked by taking a maximum projection across the whole video.</p><p>During seed initialization, the first critical parameter is the spatial window for defining local maxima. Intuitively, this should be the expected diameter of cells. The other critical parameter is an intensity threshold for a local maximum to be considered a seed. Since the spatial window for local maxima is small relative to the field of view, a significant number of local maxima are usually false positives and do not actually reflect the location of cells. Thresholding the fluorescence intensity provides a simple way to filter out false local maxima, and usually a very low value is enough to produce satisfactory results. We have found a value of 3 usually works well (recall that the range of fluorescence intensity is usually 0–255 for unsigned 8-bit data). An alternative strategy to thresholding the intensity is to model the distribution of fluorescence fluctuations and keep the seeds with relatively higher fluctuations. This process is described in ‘Seeds refinement with a Gaussian mixture model’ and is accessible if the user prefers explicit modeling over thresholding.</p><p>Finally, the temporal sampling of frames for the maximum projections also impacts the result. We provide two implementations here: either taking a rolling window of frames across time or randomly sampling frames for a user-defined number of iterations. For the rolling window approach, users can specify a temporal window size (the number of successive frames for each subset) and a step size (the interval between the start of subsets). For the random approach, users can specify the number of frames in each subset and the total number of repetitions. We use the rolling window approach as the default.</p><p>The resulting seeds are visualized on top of a maximum projection image (plot not shown). Although the spatial window size of local maxima can be predetermined, the parameters for either the rolling window or random sampling of frames are hard to estimate intuitively. We provide default parameters that generally provide robust results. However, the user is also free to vary these parameters to obtain reasonable seeds. As long as the resulting seeds are not too dense (populating almost every pixel) or too sparse (missing cells that are visible in the max projection), subsequent steps can be performed efficiently and are fairly tolerable to the specific ways the seeds are initialized.</p></sec><sec id="s2-5-2"><title>Refinement with peak-to-noise ratio</title><p>Next, we refine the seeds by looking at what we call the peak-to-noise ratio of the temporal traces and discard seeds with low peak-to-noise ratios. To compute this ratio, we first separate the noise from the presumed real signal. Calcium dynamics are mainly composed of low-frequency fluctuations (from the slow kinetics of the calcium fluctuations) while noise is composed of higher frequency fluctuations. Thus, to separate the noise from the calcium dynamics we pass the fluorescence time trace of each seed through a low-pass and a high-pass filter to obtain the ‘signal’ and ‘noise’ of each seed. We then compute the difference between the maximum and minimum values (or peak-to-peak values) for both ‘signal’ and ‘noise,’ and the ratio between the two difference values defines the peak-to-noise ratio. Finally, we filter out seeds whose peak-to-noise value falls below a user-defined threshold.</p><p>The first critical parameter here is the cutoff frequency that separates ‘signal’ from ‘noise.’ This parameter is also important for subsequent steps when implementing the CNMF algorithm. We provide a visualization tool, shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>, to help users determine cutoff frequency. In the visualization, six seeds are randomly selected, and their corresponding ‘signal’ and ‘noise’ traces are plotted. The user is then able to use a dynamic slider on the right side of the plots to adjust the cutoff frequency and view the results. The goal is to select a frequency that best separates signal from noise. A cutoff frequency that is too low will leave true calcium dynamics absorbed in ‘noise’ (<xref ref-type="fig" rid="fig9">Figure 9</xref>, left panel), while a frequency that is too high will let ‘noise’ bleed into ‘signal’ (<xref ref-type="fig" rid="fig9">Figure 9</xref>, right panel). A suitable frequency is therefore the one where the ‘signal’ captures all of the characteristics of the calcium indicator dynamics (i.e., large, fast rise, and slow decay), while the ‘noise’ trace remains relatively uniform across time (<xref ref-type="fig" rid="fig9">Figure 9</xref>, middle panel). The interactive plots make this easy to visualize. We also provide an example in <xref ref-type="fig" rid="fig9">Figure 9</xref> to show how cutoff frequency influences the separation of ‘signal’ from ‘noise.’ The second parameter is the threshold of peak-to-noise ratio value. In practice, we have found a threshold of 1 works well in most cases. An additional advantage of using 1 is that it reflects the intuitive interpretation that fluctuations in a real ‘signal’ should be larger than fluctuations in ‘noise.’</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Visualization of noise frequency cutoff.</title><p>The cutoff frequency for noise is one of the critical parameters in the pipeline that affects both the seed initialization process and constrained non-negative matrix factorization’s (CNMF’s) temporal update steps. Here, we help the user determine that parameter by plotting temporal traces from six example seeds. In each plot, the raw signal is passed through a high-pass and low-pass filter at the chosen frequency, and the resulting signals are plotted separately as ‘noise’ and ‘signal.’ The plots are responsive to the chosen frequency controlled by the slider on the right. In this way, the user can visually inspect whether the chosen frequency can effectively filter out high-frequency noise without deforming the calcium signal.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig8-v2.tif"/></fig><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Example of filtered traces with different frequency cutoffs.</title><p>Here, the temporal dynamics of three example seeds are chosen, and the low-pass and high-pass filtered traces with different frequency cutoffs are shown. The low-pass filtered trace corresponds to ‘signal,’ while the high-pass filtered trace corresponds to ‘noise.’ Here, a 1 Hz cutoff frequency is considered appropriate since calcium dynamics and random noise are cleanly separated. A cutoff frequency smaller than 1 Hz left the calcium dynamics in the ‘noise’ trace, while a cutoff frequency larger than 1 Hz let random noise bleed into the ‘signal’ trace (i.e., high-frequency fluctuations are presented in periods where the cells seem to be inactive).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig9-v2.tif"/></fig></sec><sec id="s2-5-3"><title>Refinement with Kolmogorov–Smirnov tests</title><p>Finally, we refine the seeds with a Kolmogorov–Smirnov test. The Kolmogorov–Smirnov test assesses the equality of two distributions and can be used to check whether the fluctuation of values for each seed is non-normally distributed. We expect the noisy fluorescence values when a cell is not firing to form a Gaussian distribution with small mean value, and the fluorescence values when a cell is firing should have a much higher mean value and frequency than expected by the null Gaussian distribution. Therefore, seeds corresponding to cells should be non-normally distributed. We use a default significance threshold of 0.05. In some cases, this might be too conservative or too liberal. Users can tweak this threshold or skip this step altogether depending on the resulting seeds.</p></sec><sec id="s2-5-4"><title>Merge seeds</title><p>There will usually be multiple seeds for a single cell, and it is best to merge them whenever possible. We implement two criteria for merging seeds: first, the distance between the seeds must be below a given threshold, and second, the correlation coefficient of the temporal traces between seeds must be higher than a given threshold. To avoid bias in the correlation due to noise, we implement a smoothing operation on the traces before calculating the correlation. The critical parameters are the distance threshold, correlation threshold, and cutoff frequency for the smoothing operation. While the distance threshold is arbitrary and should be explored, often the average radius of cells provides a good starting point. The cutoff frequency should be the same as that used during the peak-to-noise ratio refinement described above, and the correlation should be relatively high (we typically use 0.8, but this can be refined by the user). The resulting merged seeds can be visualized on the max projection. Since the main purpose of this step is to alleviate computation demands for downstream steps, it is fine to have multiple seeds for a single visually distinct cell. However, users should make sure each of the visually distinct cells still has at least one corresponding seed after the merge.</p></sec><sec id="s2-5-5"><title>Initialize spatial and temporal matrices from seeds</title><p>The last step before implementing CNMF is to initialize the spatial and temporal matrices for the CNMF algorithm from the seeds. These matrices are generated with one dimension representing each putative cell and the other representing each pixel or time, respectively. In other words, the spatial matrix represents the spatial footprint for each cell at each pixel location and the temporal matrix represents the temporal fluorescence value of each cell on each frame. We assume each seed is the center of a potential cell, and we first calculate the spatial footprint for each cell by taking the cosine similarity between the temporal trace of a seed and the pixels surrounding that seed. In other words, we generate the weights in the spatial footprint by computing how similar the temporal activities of each seed are to the surrounding pixels. Then, we generate the temporal activities for each potential cell by taking the input video and weighting the contribution of each pixel to the cell’s temporal trace by the spatial footprint of the cell. The final products are a spatial matrix and temporal matrix.</p><p>Besides the two matrices representing neuronal signals, there are two additional terms in the CNMF model that account for background fluorescence modeled as a spatial footprint for the background and a temporal trace of background activity. To estimate these terms, we subtract the matrix product of our spatial and temporal matrices, which represent cellular activities, from the input data. We take the mean projection of this remainder across time as an estimation of the spatial footprint of the background, and we take the mean fluorescence for each frame as the temporal trace of the background.</p><p>Users can tweak two parameters to improve the outcome and performance of this step: a threshold for cosine similarity and a spatial window identifying pixels on which to perform this computation. To keep the resulting spatial matrix sparse and keep irrelevant pixels from influencing the temporal traces of cells, we set a threshold for the cosine similarity of temporal traces compared to the seed, where pixels whose similarity value falls below this threshold will be set to zero in the spatial footprint of the cell. Cosine similarity is, in essence, a correlation (the scale is 0–1) and thresholds of 0.5 and higher work well in practice. Computing many pairwise similarity measurements is computationally expensive, and it is unnecessary to compute the similarities between pixels that are far apart because they are unlikely to have originated from the same cell. We therefore set a window size to limit the number of pixel pairs to be considered. This size should be set large enough so that it does not limit the size of spatial footprints, but not unnecessarily large to the extent where it will impact performance. In practice, a window size equal to the maximum expected cell diameter is reasonable.</p></sec></sec><sec id="s2-6"><title>Constrained non-negative matrix factorization</title><sec id="s2-6-1"><title>Estimate spatial noise</title><p>CNMF requires that we first estimate the spatial noise over time for each pixel in the input video. The spatial noise of each pixel is simply the power of the high-frequency signals in each pixel. The critical parameter here is again the cutoff frequency for ‘noise,’ and users should employ the visualization tools as described above during peak-to-noise ratio refinement to determine this frequency (see ‘Refinement with peak-to-noise ratio).</p></sec><sec id="s2-6-2"><title>Spatial update</title><p>Next, we proceed to the spatial update of the CNMF algorithm. The original paper describing this algorithm (<xref ref-type="bibr" rid="bib39">Pnevmatikakis et al., 2016</xref>) contains a detailed theoretical derivation of the model. Here, we provide only a conceptual overview of the process so that users can understand the effect of each parameter. The CNMF framework models the input video to be the product of the spatial and temporal matrices representing signals contributed by real cells, a background term, and random noise. In equation form, this is <inline-formula><mml:math id="inf2"><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">C</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">E</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf3"><mml:mi mathvariant="bold">Y</mml:mi></mml:math></inline-formula> represents the input video, <inline-formula><mml:math id="inf4"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> represents the spatial matrix containing the spatial footprints for all putative cells, <inline-formula><mml:math id="inf5"><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula> represents the temporal matrix containing the calcium dynamics for all putative cells, <inline-formula><mml:math id="inf6"><mml:mi mathvariant="bold">B</mml:mi></mml:math></inline-formula> represents the spatial-temporal fluctuation of background, and <inline-formula><mml:math id="inf7"><mml:mi mathvariant="bold">E</mml:mi></mml:math></inline-formula> represents error or noise. Since the full problem of finding proper <inline-formula><mml:math id="inf8"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula> matrices is hard (nonconvex), we break down the full process into spatial update and temporal update steps, where iterative updates of <inline-formula><mml:math id="inf10"><mml:mi mathvariant="bold">A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula> are carried out, respectively. Each iteration will improve on previous results and eventually converge on the best estimation.</p><p>During the spatial update, given an estimation of the temporal matrix and the background term, we seek to update the spatial matrix so that it best fits the input data, along with the corresponding temporal traces. To do so, we first subtract the background term from the input data so that the remainder is composed only of signals from cells and noise. Then, for each pixel, the algorithm attempts to find the weights for each cell’s spatial footprint that best reproduces the input data (<inline-formula><mml:math id="inf12"><mml:mi mathvariant="bold">Y</mml:mi></mml:math></inline-formula>) with the constraint that individual pixels should not weigh on too many cells (controlled through what is called a sparseness penalty). To reduce computational demand, we do this for each pixel independently and in parallel to improve performance, while retaining the ‘demixing’ power of the CNMF algorithm by updating the weights for all cells simultaneously. In the optimization process, the function to be minimized contains both a squared error term to assess error, and an <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm term to promote sparsity (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>). The optimization process can be expressed formally as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd/><mml:mtd><mml:munder><mml:mtext>minimize</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow></mml:munder></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mtext>subject to</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf14"><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> denotes the input movie data indexed at <inline-formula><mml:math id="inf15"><mml:mi>p</mml:mi></mml:math></inline-formula>th pixel, <inline-formula><mml:math id="inf16"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> denotes the spatial matrix indexed at <inline-formula><mml:math id="inf17"><mml:mi>p</mml:mi></mml:math></inline-formula>th pixel across all putative cells, and <inline-formula><mml:math id="inf18"><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf19"><mml:mi mathvariant="bold">b</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf20"><mml:mi mathvariant="bold">f</mml:mi></mml:math></inline-formula> denote the temporal matrix, spatial footprint of background term, and temporal fluctuation of background term, respectively. The scalar <inline-formula><mml:math id="inf21"><mml:mi>λ</mml:mi></mml:math></inline-formula> represents the sparse penalty that controls the balance between the error term and sparsity term.</p><p>Lastly, the spatial footprint of the background term is updated in the exact same way, together with other putative cells. However, the background term the temporal activity used in the spatial update is not constrained by the autoregressive model. After the spatial footprint of the background term is updated, we subtract the neural activity (<inline-formula><mml:math id="inf22"><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">C</mml:mi></mml:math></inline-formula>) from the input data to get residual background fluctuations. Then, the temporal activity of background term is calculated as the projection of residual onto the new background spatial footprint, where the raw activities of each pixel are weighted by the spatial footprint.</p><p>In other CNMF implementations, the estimated spatial noise is used to determine the scaling of the <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm term in the target function and control the balance between error and sparsity of the result. However, in practice we find that it does not always give the best result for all types of datasets. For example, sometimes the estimated spatial noise is too large, which results in an overly conservative estimation of spatial footprints. Hence, we have introduced a sparseness penalty on top of the estimated scaling factor for the <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm term. This parameter gives users more control over how sparsity should be weighted in the updating process. The higher the number, the higher the penalty imposed by the <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm, and the more sparse the spatial footprints will become. The effect of this parameter can be visualized with the tool shown in <xref ref-type="fig" rid="fig10">Figure 10</xref>. Users can employ this tool to determine the best sparseness penalty for their data, where the binarized spatial footprint representing nonzero terms should approach the visible part of the spatial footprint as much as possible, without reducing the amplitude of spatial footprints to the extent that cells are discarded in the spatial update. <xref ref-type="fig" rid="fig11">Figure 11</xref> shows an example of the effect of changing the sparseness penalty on the resulting spatial footprints. A sparseness penalty of 0.1 is considered appropriate in this case. When the sparseness penalty is set much lower, many of the additional ‘fragments’ begin to appear in the binarized spatial footprint, even if they are not part of the cell. On the other hand, when the sparseness penalty is set too high, some cells are discarded. In the interactive visualization tool, users can inspect the temporal dynamics of these discarded cells. In general, however, we do not recommend exploiting the sparseness penalty during the spatial update to filter cells since this step does not have an explicit model of the temporal signal and thus has no power to differentiate real cells from noise.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Visualization of spatial updates.</title><p>Here, 10 cells are randomly chosen to pass through spatial update with different parameters. The resulting spatial footprints, as well as binarized footprints, are plotted. In addition, the corresponding temporal traces of cells are plotted. The user can visually inspect the size and shape of the spatial footprints and at the same time easily determine whether the results are sparse enough by looking at the binarized footprints.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig10-v2.tif"/></fig><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Effect of sparseness penalty in spatial update.</title><p>Here, the sum projection of the spatial matrix and binarized spatial matrix is shown for three different sparse penalties. A sparseness penalty of 0.1 is considered appropriate in this case. When the sparseness penalty is set lower, artifacts begin to appear. On the other hand, when the sparseness penalty is set higher, cells are dropped out.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig11-v2.tif"/></fig><p>In addition, a dilation window parameter must be specified by the user. To reduce the amount of computation when calculating how each pixel weighs onto each cell, we only update weights for cells that are close to each pixel. For each cell, an ROI is computed by performing a morphological dilation process on the previous spatial footprints of that cell. If a pixel lies outside of a cell’s region of interest, this cell will not be considered when updating the pixel’s weight. Thus, the dilation window parameter determines the maximum distance a cell is allowed to grow during the update compared to its previous spatial footprints. This parameter should be set large enough so that it does not interfere with the spatial update process, but at the same time not so large as to impact performance. The expected cell diameter in pixels is a good starting point.</p></sec><sec id="s2-6-3"><title>Temporal update</title><p>Next, we proceed to the temporal update of the CNMF algorithm. Please refer to the original paper for the detailed derivation (<xref ref-type="bibr" rid="bib39">Pnevmatikakis et al., 2016</xref>). Here, given the spatial matrix and background terms, we update the temporal matrix so that it best fits the input data (<inline-formula><mml:math id="inf26"><mml:mi mathvariant="bold">Y</mml:mi></mml:math></inline-formula>). First, we subtract the background term from the input data, leaving only the noisy signal from cells. We then project the data onto the spatial footprints of cells, obtaining the temporal activity for each cell. Next, we estimate a contribution of temporal activity from neighboring overlapping cells using the spatial footprints of cells and subtract it from the temporal activity of each cell. This process results in a two-dimensional matrix representing the raw temporal activity of each cell (<xref ref-type="bibr" rid="bib15">Friedrich et al., 2021</xref>).</p><p>The CNMF algorithm models the relationship between the underlying ‘spiking’ and the calcium dynamics of a cell as an autoregressive (AR) process. It should be noted that although the underlying process that drives calcium influx is presumably cell firing, the ‘spiking’ signal is modeled as a continuous variable rather than a binary variable, and strictly speaking, it is only a deconvolved calcium signal. Following convention, we will refer to this variable as ‘spike signal,’ an approximation of the underlying cellular activity that drives calcium influx. It should be understood, however, that the exact relationship between this variable and the actual firing rate of cells is unclear since the absolute amount of fluorescence generated by a single spike, as well as the numerical effect of integrating multiple spikes on the resulting calcium signal, is unknown.</p><p>We first estimate the coefficients for the AR model. The coefficients of the AR model can be conveniently estimated from the autocorrelation of the estimated temporal activity. In addition, noise power for each cell is also estimated directly from the signal. In practice, we find that during the estimation of the AR model parameters it is helpful to first smooth the signal, otherwise the time constant of the AR model tends to be biased by high-frequency noise. Users should again use the peak-to-noise refinement cutoff frequency for both estimation of the noise power and smoothing of the signals. Finally, we update the temporal matrix by minimizing a target function for different cells, similar to what was done with the spatial matrix. Again, the target function contains a squared error term and a <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm term. We also introduce a sparseness penalty parameter to control the balance between the two terms. The squared error term contains the difference between input signal and estimated calcium dynamics, while the <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm term regulates the sparsity of the ‘spiking’ signal. Pre-estimated AR coefficients allow for a determined relationship between the ‘spiking’ signal and calcium dynamics for a given cell. Thus, the problem can be transformed and simplified as minimizing the target function over ‘spiking’ signals of different cells.</p><p>In practice, it is computationally more efficient to break down the minimization problem into smaller pieces and update subsets of cells independently and in parallel. To do so, we first identify nonoverlapping cells using a Jaccard index, which measures the amount of overlap between the spatial footprints of different cells. Once we identify these individual cells, we can update them independently so that an optimization problem and target function are formulated for each cell independently. Here, we set a cutoff Jaccard index where cells above this amount of overlap are updated in parallel. During the updating process, two additional terms are introduced: a baseline term to account for constitutive nonzero activity of cells and an initial calcium concentration to account for a ‘spiking’ that started just prior to recording. The initial calcium concentration term is a scalar that is recursively multiplied by the same AR coefficient estimated for the cell. The resulting time trace, modeling the decay process of a ‘spiking’ event prior to the recording, is added on top of the calcium trace. The baseline activity term is also a scalar that is simply added on top of all the modeled signals. Both terms are often zero, but they are nevertheless saved and visualized. For each cell, the optimization process can be expressed formally as<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd/><mml:mtd><mml:munder><mml:mtext>minimize</mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">c</mml:mi><mml:mn mathvariant="bold">0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mtext>subject to</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf29"><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">a</mml:mi></mml:math></inline-formula> denotes the input movie data projected onto the spatial footprint of the given cell, <inline-formula><mml:math id="inf30"><mml:mi mathvariant="bold">c</mml:mi></mml:math></inline-formula> denotes the estimated calcium dynamic of the given cell, <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> denotes the constant baseline fluorescent activity, <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> denotes the initial calcium concentration, <inline-formula><mml:math id="inf33"><mml:mi>G</mml:mi></mml:math></inline-formula> represents a matrix of AR coefficients such that <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold">G</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:math></inline-formula> is the estimated ‘spike’ signal, and <inline-formula><mml:math id="inf35"><mml:mi mathvariant="bold">d</mml:mi></mml:math></inline-formula> is a vector representing the temporal decay of a single spike based on the estimated AR coefficients, such that the term <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">d</mml:mi></mml:math></inline-formula> represents the contribution of initial calcium concentration. Similar to spatial update, the scalar <inline-formula><mml:math id="inf37"><mml:mi>λ</mml:mi></mml:math></inline-formula> represents the sparse penalty and controls the balance between the error term and sparsity term.</p><p>The <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> -norm in the optimization problem is known to reduce not only the number of nonzero terms (i.e., promotes sparsity), but also the amplitude/value of nonzero terms. This effect is unwanted since in some cases the numerical of the spatial update step in CNMF algorithm and value of the resulting ‘spike’ signal can become too small as a side effect of promoting sparsity, making it hard to interpret and compare the ‘spike’ signal for downstream analysis. To counteract this phenomenon, we introduce a <italic>post-hoc</italic> scaling process. After the temporal update, each cell is assigned a scaling factor to scale all the fitted signals to the appropriate values. The scaling factor is solved by least-square minimizing the error between the fitted calcium signal and the projected raw signal.</p><p>The critical parameters in temporal updates are as follows: (1) the order of the AR model, usually 1 or 2. Users should choose 1 if near-instantaneous rise time is presented in the calcium dynamics of the input data (i.e., from the relatively slow sampling rate) and should choose 2 otherwise. (2) The cutoff frequency for noise used for both noise power estimation and pre-smoothing of the data during AR coefficients estimation. Users should use the values set during peak-to-noise ratio refinement. (3) The threshold for the Jaccard index determining which cells can be updated independently. Users should use a value as low as possible, as long as the speed of this step is acceptable (with large amounts of cells packed closely together, a low threshold may dramatically slow down this step), or visually inspect how sparse the spatial footprints are and determine what amount of overlap between spatial footprints results in significant crosstalk between cells. (4) The sparseness penalty is best set through visualization tools. The effect of any parameter on the temporal update can be visualized through the tool shown in <xref ref-type="fig" rid="fig12">Figure 12</xref>, where the result of the temporal update for 10 randomly selected cells is plotted as traces. There are a total of four traces shown for each cell: calcium signal, deconvolved ‘spiking’ signal, projected raw signal, and ‘fitted signal.’ The ‘fitted signal’ is very similar to the calcium signal and is often indistinguishable from the latter. The difference between them is that the ‘fitted signal’ also includes the baseline term and the initial calcium concentration term. Hence, the ‘fitted signal’ should better follow the projected raw signal, but it may be less interesting for downstream analysis. Toggling between different parameters triggers the dynamic update of the plots, helping the user to determine the best parameters for their data. Additionally, we highlight the effect of the sparseness penalty on resulting fitted calcium signals and spike signals in <xref ref-type="fig" rid="fig13">Figure 13</xref>. The effect is most evident in the ‘fitted spikes’ trace, which corresponds to the spike signal and can arguably be interpreted as a measure of the underlying neural activity per frame scaled by an unknown scalar. Here, a sparseness penalty of 0.008 is considered most appropriate. A lower sparseness penalty will introduce many false-positive signals that do not correspond to real calcium dynamics, as can be seen in the plots. On the other hand, too high a sparseness penalty will produce false negatives where clear rises in the raw signal are not accompanied by spikes.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Visualization of temporal update.</title><p>Here, a subset of cells is randomly chosen to pass through temporal updates with different parameters. Only one cell is visualized at a given time, and the cell can be selected using the slider on the right. The raw signal, fitted signal, fitted calcium traces, and spike signals are overlaid in the same plot. In addition, a simulated pulse response based on the estimated autoregressive parameters is plotted with the same time scale. Furthermore, the corresponding spatial footprint of the cell is plotted for cross-reference. With a given set of parameters, the user can visually inspect whether the pulse response captures the typical calcium dynamics of the cell, and whether the timing and sparsity of the spike signal fit well with the raw data. The data shown here was acquired with a frame rate of 30 fps.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig12-v2.tif"/></fig><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Effect of the sparseness penalty in temporal update.</title><p>Here, three example cells are selected and passed to the temporal update with different sparseness penalties. The ‘Raw Signal’ corresponds to the input video projected onto predetermined spatial footprints. The ‘Fitted Calcium’ and ‘Fitted Spikes’ correspond to the resulting model-fitted calcium dynamics and spike signals. A sparseness penalty of 0.008 (middle column) is considered appropriate in this case. The data shown here was acquired with a frame rate of 30 fps.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig13-v2.tif"/></fig></sec></sec><sec id="s2-7"><title>Merging cells</title><p>The CNMF algorithm can sometimes misclassify a single cell as multiple cells. To counteract this phenomenon, we implement a step to merge cells based on their proximity and temporal activity. All cells with spatial footprints sharing at least one pixel are considered candidates for merging, and the pairwise correlation of their temporal activity is computed. Users can then specify a threshold where cell pairs with activity correlations above the threshold are merged. Merging is done by taking the sum of the respective spatial footprints and the mean of all of the temporal traces for all cells to be merged. Since this is only a simple way to correct for the number of estimated cells and does not fit numerically with what the model CNMF assumes, merging is only done between iterations of CNMF, but not at the end.</p></sec><sec id="s2-8"><title>Manual curation</title><p>Minian provides an interactive visualization to help the users manually inspect the quality of putative cells and potentially merge or drop cells. At any given time, the visualization shows spatial temporal activities (<xref ref-type="fig" rid="fig14">Figure 14</xref>, top row, middle panel) and temporal dynamics of a selected subset of cells (<xref ref-type="fig" rid="fig14">Figure 14</xref>, bottom row ). The spatial temporal activities are shown side-by-side with the spatial footprints of all cells and the preprocessed movie (input to CNMF algorithm) at a given frame (<xref ref-type="fig" rid="fig14">Figure 14</xref>, top row). The field of view is synchronized across the three images on the top, so that the users can easily zoom in and compare the estimated spatial footprints of cells to the input data. The spatial temporal images in the middle show the product of spatial footprints and calcium dynamics, which represent the model estimated image of a subset of cells at a given frame. This spatial temporal product is calculated on-the-fly and synchronized with the frame indicators on the temporal dynamic plots. In this way, users can easily pick times of interest (e.g., when a cell has a calcium event) and validate whether the estimated spatial temporal activities match the input data. Lastly, this interactive visualization allows the user to either drop false-positive cells or merge multiple cells together via drop-down menus. The result of manual curation is saved as an array with a label for each unit indicating whether a cell should be discarded or how several cells should be merged. In this way, only the new label is saved and no data is modified, allowing the user to repeat or correct the manual curation process if needed.</p><fig id="fig14" position="float"><label>Figure 14.</label><caption><title>Interactive visualization of Minian output.</title><p>The three images on the top show the spatial footprints of all the cells (left), spatial temporal activities of selected subset of cells (middle), and preprocessed data. The bottom row shows the display control panel (left), temporal dynamics of selected subset of cells (middle), and manual curation panel (right). The field of view, current frame, and selection of cells are all synced across different plots to help user focus on a specific region and time. The users can use the control panel to select groups of cells, change display options for temporal dynamics and spatial temporal activities, and change the current frame or play the movie. In addition, the users can directly select cells from the spatial footprints plot on the top left. The users can also directly jump to frames by double-clicking on the temporal dynamic plots. These interactive features help the users quickly focus on region and time of interests. The manual curation menu on bottom right can be used to assign unit labels to each cell, which indicate whether a cell should be dropped or merged.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig14-v2.tif"/></fig></sec><sec id="s2-9"><title>Cross-registration</title><p>After completing the analysis of individual recording sessions, users can register cells across sessions. While more complex approaches are proposed in other pipelines (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Sheintuch et al., 2017</xref>), here, our intention is simplicity. To account for shifts in the field of view from one session to the next, we first align the field of view from each session based upon a summary frame. Users can either choose a max projection of each preprocessed and motion-corrected video, or a summed projection of the spatial footprints of all cells. Users can also choose which session should be used as the template for registration, to which every other session should be aligned. We use a standard cross-correlation based on a template-matching algorithm to estimate the translational shifts for each session relative to the template and then correct for this shift. The weighted centroid of each cell’s spatial footprint is then calculated and pairwise centroid distances are used to cross-register cells. A distance threshold (maximum pixel distance) is set. Users should choose this threshold carefully to reflect the maximum expected displacement of cells across sessions after registration. We found that a threshold of five pixels works well. Finally, a pair of cells must be the closest cells to each other in order to be considered the same cell across sessions.</p><p>To extend this method to more than two sessions, we first cross-register all possible session pairs. We then take the union of all these pairwise results and transitively extend the cross-registration across more than two sessions. At the same time, we discard all matches that result in conflicts. For example, if cell A in the first session is matched with cell B in the second session, and cell B is in turn matched with cell C in the third session, but cells A and C are not matched when directly registering the first and third sessions, all of these matches are discarded and all three cells are treated as individual cells. We recognize that this approach might be overly conservative. However, we believe that this strategy provides an easy-to-interpret result that does not require users to make decisions about whether to accept cell pairs that could conflict across sessions.</p><p>To save computation time, we implement a moving window where centroid distances are only calculated for cell pairs within these windows. Users should set the size of windows to be much larger than the expected size of cells.</p></sec><sec id="s2-10"><title>Hardware and dependencies</title><p>Minian has been tested using OSX, Linux, and Windows operating systems. Additionally, although we routinely use Minian on specialized analysis computers, the pipeline works on personal laptops for many common length (~30 min) miniature microscope experiments. Specifications of all of the computers that have been tested can be found in Tested hardware specifications. We anticipate that any computer with at least 16 GB of memory will be capable of processing at least 20 min of recording data, although increased memory and CPU power will speed up processing. Moreover, due to the read-write processes involved in out-of-core computation, we recommend that the videos to be processed are held locally at the time of analysis, preferably on a solid-state drive. The relatively slow speed of transfer via Ethernet cables, Wi-Fi, or USB cables to external drives will severely impair analysis times.</p><p>Minian is built on top of project Jupyter (<xref ref-type="bibr" rid="bib27">Kluyver et al., 2016</xref>) and depends heavily on packages provided by the open-source community, including NumPy (<xref ref-type="bibr" rid="bib19">Harris et al., 2020</xref>), SciPy (<xref ref-type="bibr" rid="bib49">Virtanen et al., 2020</xref>), xarray (<xref ref-type="bibr" rid="bib20">Hoyer and Hamman, 2017</xref>), HoloViews (<xref ref-type="bibr" rid="bib40">Rudiger et al., 2020</xref>), Bokeh (<xref ref-type="bibr" rid="bib4">Bokeh Development Team, 2020</xref>), OpenCV (<xref ref-type="bibr" rid="bib5">Bradski, 2020</xref>), and Dask (<xref ref-type="bibr" rid="bib10">Dask Team, 2016</xref>). A complete list of direct dependencies for Minian can be found in ‘List of dependencies.’ Of note, the provided install instructions handle the installation of all dependencies.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>To validate the accuracy as well as benchmark the performance of Minian, we ran the Minian pipeline on a series of simulated and experimental datasets and compare the output and performance to those obtained with CaImAn, which is one of the most widely adapted calcium imaging analysis pipelines in the field. In addition, we also validated the full workflow of Minian by applying the pipeline to several recordings of animals running on a linear track and looked at the stability of place cells. These results are presented in the following sections.</p><sec id="s3-1"><title>Validation with simulated datasets</title><p>We first validated Minian with simulated datasets. We synthesized different datasets with varying number of cells and signal levels based on existing works (<xref ref-type="bibr" rid="bib52">Zhou et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>). The simulated datasets contain local background fluctuations, noise, and motions similar to experimental datasets (see ‘Generation of simulated datasets’ for details). The field of view contains 512 × 512 pixels and 20,000 frames, corresponding to roughly 10 min of recording at 30 fps. We processed the data with both Minian and CaImAn. For Minian, we utilized the visualization described here to optimize the parameters. For CaImAn, we used the same parameters as Minian whenever the implementations were equivalent. Otherwise, we followed the suggested parameters and tweaked them based on the knowledge of simulated ground truth.</p><p>To compare the results objectively, we first matched the resulting putative cells from the output of Minian or CaImAn to the simulated ground truth (see ‘Matching neurons for validation’ for details). We then calculated three metrics to measure the quality of output: F1 score, spatial footprints correlation, and temporal dynamics correlation. The F1 score is defined as the harmonic mean of precision (proportion of detected neurons that are true) and recall (proportion of ground-truth neurons that has been detected). Hence, the F1 score measures the overall accuracy of neuron detection. For each detected neuron that has been matched to ground truth, we compute Pearson correlation between the estimated and ground-truth spatial footprint, as well as the Pearson correlation between the estimated calcium dynamic and the ground-truth calcium dynamic. We then take the median correlation across all the matched neurons to measure the overall quality of estimated spatial footprints and temporal dynamics.</p><p>As shown in <xref ref-type="fig" rid="fig15">Figure 15</xref>, both Minian and CaImAn achieve similar and near-perfect levels (&gt;0.95) of F1 score across all conditions. Similarly, the spatial footprints remain nearly perfect (&gt;0.95) for both pipelines across all conditions. At the lowest signal level (0.2), both pipelines suffer from decreased correlation of temporal dynamics. This is likely due to noise and background contaminating the true signal. Overall, these results show that the Minian and CaImAn pipelines perform similarly well in terms of output accuracy on simulated datasets.</p><fig id="fig15" position="float"><label>Figure 15.</label><caption><title>Validation of Minian with simulated datasets.</title><p>Simulated datasets with varying signal level and number of cells are processed through Minian and CaImAn. The F1 score (top), median correlation of spatial footprints (middle), and median correlation of temporal dynamics (bottom) are plotted as a function of signal level. Both pipelines achieve near-perfect (&gt;0.95) F1 scores and spatial footprint correlation across all conditions. The correlation of temporal dynamics is lower when the signal level is 0.2, but remains similar across the two pipelines overall.</p><p><supplementary-material id="fig15sdata1"><label>Figure 15—source data 1.</label><caption><title>Raw validation performance with simulated data.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-70661-fig15-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig15-v2.tif"/></fig><p>Additionally, we want to validate the deconvolved signal from Minian output since this is usually the most important output for downstream analysis. Our ground-truth spikes are simulated as binary signals. However, in reality calcium activity often reflects the integration of several spikes, and the deconvolved signals from Minian output are real-valued. Because of this, we downsampled both the ground-truth spikes and deconvolved signals by five times, and then calculated Pearson correlation for all matched cells. The resulting correlation is summarized in <xref ref-type="fig" rid="fig16">Figure 16A</xref>. Our results indicate that the deconvolved output from Minian is highly similar to ground-truth spikes when signal level is high, and the correlation asymptote and approach 1 when signal level is higher than 1. The lower correlation corresponding to low signal level is likely due to the background and noise contamination being stronger than signal. In line with this idea, the detected ‘spikes’ from the deconvolved signals closely match those from ground truth, as shown by the example traces in <xref ref-type="fig" rid="fig16">Figure 16B</xref>. The main difference between the two traces is the amplitude of the deconvolved signals, which is prone to be influenced by local background and noise. Overall, these results suggest that Minian can produce deconvolved signals that are faithful to ground truth and suitable for downstream analysis.</p><fig id="fig16" position="float"><label>Figure 16.</label><caption><title>Validation of deconvolved signal from Minian.</title><p>(<bold>A</bold>) Correlation of deconvolved signals from Minian output with simulated ground truth. The mean correlation across all cells (blue line) and standard deviation (light blue shade) are shown separately for different signal levels and number of cells. The correlation asymptote and approach 1 when signal level is higher than 1. (<bold>B</bold>) Example deconvolved traces from Minian output overlaid with simulated ground truth. One representative cell is drawn from each signal level. The binary-simulated spikes are shown in green, with the real-valued Minian deconvolved output overlaid on top in blue. The deconvolved signals closely match the ground truth, and the main difference between the two signals is in the amplitude of the deconvolved signals, which tend to be influenced by local background.</p><p><supplementary-material id="fig16sdata1"><label>Figure 16—source data 1.</label><caption><title>Raw correlations between Minian deconvolved traces and simulated ground truth.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-70661-fig16-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig16sdata2"><label>Figure 16—source data 2.</label><caption><title>Raw example traces from Minian and simulated ground truth.</title><p>File names indicate signal level and source of trace.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-70661-fig16-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig16-v2.tif"/></fig></sec><sec id="s3-2"><title>Validation with experimental datasets</title><p>We next validated Minian with experimental datasets. The data was collected from hippocampal CA1 regions in animals performing a spatial navigation task. Six animals with different density of cells were included in the validation dataset. The recordings are collected with 608 × 608 pixels at 30 fps and last 20 min (~36,000 frames). Due to difficulties in obtaining ground truth for experimental data, we choose to validate Minian with CaImAn, which has been established as one of the most accurate existing pipelines. To evaluate the results objectively, we matched resulting ROIs from Minian with those from CaImAn using the same approach as in ‘Validation with simulated datasets.’ We then calculated correlation of spatial footprints and temporal activity between matched ROIs from the two pipelines. Across the six datasets, the mean F1 score is 0.73 (sem ± 0.03). The mean spatial footprints correlation is 0.84 (sem ± 0.02), and the mean temporal activity correlation is 0.86 (sem ± 0.02). An example field of view and temporal activity from matched ROIs is shown in <xref ref-type="fig" rid="fig17">Figure 17</xref>. Our results indicate that most of the ROIs detected by Minian and CaImAn correspond to the same population of putative cells, and the resulting spatial footprints and temporal activity are nearly identical. These cells tend to cluster near the center of the field of view, which usually has better signal-to-noise ratio. However, the cells near the edge of the field of view usually have low intensity and spatial consistency due to the optical property of GRIN lens. As a consequence, Minian and CaImAn might detect different populations of cells near the border of field of view due to differences in preprocessing and initialization between the two pipelines. We have chosen to use the same set of parameters across all datasets so that the results are easier to interpret, hence the parameters we used were relatively conservative. In practice, the users can further fine-tune the parameters for each recording so that Minian would be able to capture all the low signal cells in the field of view. Overall, these results suggest that the output of Minian is highly similar to CaImAn when analyzing experimental datasets.</p><fig id="fig17" position="float"><label>Figure 17.</label><caption><title>Example output of Minian and CaImAn with experimental datasets.</title><p>(<bold>A</bold>) An example field of view from one of the experimental datasets. The spatial footprints from Minian and CaImAn are colored as blue and red, respectively, and overlaid on top of each other. Most of the spatial footprints from both pipelines overlap with each other. (<bold>B</bold>) Five example matched temporal activity from Minian and CaImAn overlaid on top of each other. The extracted temporal activity is highly similar across the two pipelines.</p><p><supplementary-material id="fig17sdata1"><label>Figure 17—source data 1.</label><caption><title>Raw spatial footprint values shown in the overlay plot.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-70661-fig17-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig17sdata2"><label>Figure 17—source data 2.</label><caption><title>Raw example traces from Minian and Caiman.</title><p>File names indicate cell id and source of trace.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-70661-fig17-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig17-v2.tif"/></fig></sec><sec id="s3-3"><title>Benchmarking computational performance</title><p>To see how the performance of Minian scales with different input data size, we synthesized datasets with varying number of cells and number of frames (recording length). The field of view contains 512 × 512 pixels (same as those used in validation of accuracy), and the signal level was held constant at 1 to make sure both Minian and CaImAn can detect roughly equal number of neurons during the pipeline. To this end, we tracked two metrics of performance: the total running time of the pipeline and the peak memory usage during running. The running time was obtained by querying operating system time during the pipeline. The memory usage was tracked with an independent process that queries memory usage of the pipeline from the operating system on a 0.5 s interval. Both pipelines were set to utilize four parallel processes during the run across all conditions. All benchmarking are carried out on a custom-built Linux machine (model ‘Carbon’ under Tested hardware specifications).</p><p>As shown in <xref ref-type="fig" rid="fig18">Figure 18</xref>, the run time of both Minian and CaImAn scales linearly as a function of input recording length. The exact running times vary depending on the number of cells as well as whether visualization is included in the processing, but in general the running time is similar across both pipelines. On the other hand, the peak memory usage of CaImAn scales linearly with recording length when the number of parallel processes was set to be constant. At the same time, the peak memory usage of Minian stays mostly constant across increasing number of frames. This is likely due to the flexible chunking implementation of Minian (see ‘Parallel and out-of-core computation with Dask’), where Minian was able to break down computations into chunks in both spatial and temporal dimensions depending on which way is more efficient. In contrast, CaImAn only splits data into different spatial chunks (patches), resulting in a linear scaling of memory usage with recording length for each chunk-wise computation. Additionally, we run Minian and CaImAn with different number of parallel processes on the simulated dataset with 28,000 frames and 500 cells. As expected, with more parallel processes the performance improves and the run time decreases but at the same time the total peak memory usage increases. The tradeoff between run time and peak memory usage is shown in <xref ref-type="fig" rid="fig19">Figure 19</xref>. In conclusion, these results show that in practice Minian is able to perform as fast as CaImAn, while maintaining near constant memory usage regardless of input data size. This allows the users to process much longer recordings with limited RAM resources.</p><fig id="fig18" position="float"><label>Figure 18.</label><caption><title>Benchmarking of computational performance.</title><p>Data with varying number of cells and frames were processed through Minian and CaImAn. The run time (top) and peak memory usage (bottom) were recorded and plotted as a function of frame number. For both pipelines, the run time scales linearly as a function of the number of frames and remains similar across the pipelines. However, the peak memory usage for CaImAn also scales linearly as the number of frames increases, while Minian maintains a relatively constant peak memory usage across different frame numbers and cell numbers.</p><p><supplementary-material id="fig18sdata1"><label>Figure 18—source data 1.</label><caption><title>Raw memory usage and running time with different datasets for both pipelines.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-70661-fig18-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig18-v2.tif"/></fig><fig id="fig19" position="float"><label>Figure 19.</label><caption><title>Tradeoff between run time and memory usage.</title><p>Simulated data with 500 cells and 28,000 frames were processed through Minian and CaImAn with different numbers of parallel processes. We varied the number of parallel processes from 2 to 10, and the resulting memory usage is plotted as a function of run time. For both pipelines, the curve takes a hyperbola shape, showing the tradeoff between run time and memory usage.</p><p><supplementary-material id="fig19sdata1"><label>Figure 19—source data 1.</label><caption><title>Raw memory usage and running time with different parallel processes for both pipelines.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-70661-fig19-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig19-v2.tif"/></fig></sec><sec id="s3-4"><title>Validation with hippocampal CA1 place cells</title><p>In addition to direct validation of the output for single session, we wanted to validate the scientific significance of the spike signal, as well as the quality of the cross-session registration, and ensure that Minian is capable of generating meaningful results consistent with the existing literature. We leveraged the extensively documented properties of place cells in rodent hippocampal CA1 (<xref ref-type="bibr" rid="bib36">O’Keefe and Dostrovsky, 1971</xref>). Place cells have been shown to have consistent place fields across at least 2 days (<xref ref-type="bibr" rid="bib53">Ziv et al., 2013</xref>; <xref ref-type="bibr" rid="bib47">Thompson and Best, 1990</xref>) with only a minority of detected cells undergoing place field remapping. Here, we looked at place field stability across two linear track sessions (<xref ref-type="fig" rid="fig20">Figure 20A</xref>). Briefly, animals were trained to run back and forth on a 2 m linear track while wearing a Miniscope to obtain water rewards available at either end (<xref ref-type="bibr" rid="bib43">Shuman et al., 2020</xref>). The time gap between each session was 2 days. We record calcium activity in dorsal CA1 region with a FOV of 480 × 752 pixels collected at 30 fps. Each recording session lasts 15 min (~27,000 frames). Calcium imaging data were analyzed with Minian, while the location of animals was extracted with an open-source behavioral analysis pipeline ezTrack (<xref ref-type="bibr" rid="bib38">Pennington et al., 2019</xref>). The resulting calcium dynamics and animal behavior were aligned with the timestamps recorded by Miniscope data acquisition software (<ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">http://miniscope.org/index.php/Main_Page</ext-link>). We used the spike signal for our downstream analysis. To calculate average spatial activity rate, we binned the 2-m-long track into 100 spatial bins. In addition, we separated the epochs when the animals are running in opposite directions, resulting in a total of 200 spatial bins. We then smoothed both the binned activity rate and animal’s occupancy with a Gaussian kernel with a standard deviation of 5 cm. We classified place cells based on three criteria: spatial information criterion, stability criterion, and place field size criterion (<xref ref-type="bibr" rid="bib43">Shuman et al., 2020</xref>). (See ‘Classification of place cells’ for more detail.) Finally, we analyzed cells that are cross-registered by Minian and are classified as place cells in both sessions. We then calculated the Pearson correlation for the average spatial firing rate for each cross-registered cell. We found that, on average, place cells have a correlation of ~0.6, which is consistent with the existing literature (<xref ref-type="bibr" rid="bib43">Shuman et al., 2020</xref>).</p><fig id="fig20" position="float"><label>Figure 20.</label><caption><title>Validation of Minian with hippocampal CA1 place cells.</title><p>(<bold>A</bold>) Matching place cells from two recording sessions. The cells are matched from one session to the other using the cross-session registration algorithm and sorted based on place field in the first session. In both sessions, animals run on a 2-m-long linear track with water reward at both ends. The track is divided into 200 spatial bins. The mean ‘firing’ rate calculated from the spike signal for each cell is shown. Cell IDs are assigned by Minian when each session is analyzed independently. (<bold>B</bold>) Averaged correlations of spatial firing rates with different artificial shifts. We artificially shifted the spatial footprints of the second linear track session, then carried out registration and calculated a mean correlation of spatial firing rates for all place cells. The artificial shifts were relative to the aligned spatial footprints and range from –50 to 50 pixels.</p><p><supplementary-material id="fig20sdata1"><label>Figure 20—source data 1.</label><caption><title>Raw correlation of spatial firing pattern with different shifts in field of view.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-70661-fig20-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig20sdata2"><label>Figure 20—source data 2.</label><caption><title>Raw spatial firing activity for the two sessions shown.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-70661-fig20-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-fig20-v2.tif"/></fig><p>Next, we validated the cross-session registration to verify that the correct cells were being matched across days. We translated the spatial footprints of the second session in both directions up to 50 pixels and registered the cells with the shifted spatial footprints. We then carried out the same analysis with the registration results from shifted spatial footprints. We found that the average correlations between spatial firing patterns have higher values when the shifts are close to zero (<xref ref-type="fig" rid="fig20">Figure 20B</xref>).</p><p>In conclusion, Minian can reliably process in vivo calcium imaging data and produce results that are in agreement with the known properties of rodent CA1. Minian can thus help neuroscience labs easily implement and select the best parameters for their calcium analysis pipeline by providing detailed instructions and visualizations.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><sec id="s4-1"><title>Making open science more accessible</title><p>Neuroscience has benefited tremendously from open-source projects, ranging from do-it-yourself hardware (<xref ref-type="bibr" rid="bib51">White et al., 2019</xref>) to sophisticated algorithms (<xref ref-type="bibr" rid="bib12">Freeman, 2015</xref>). Open-source projects are impactful because they make cutting-edge technologies available to neuroscience labs with limited resources, as well as opening the door for innovation on top of previously established methods. We believe that openly sharing knowledge and tools is just the first step. Making knowledge accessible even to nonexperts should be one of the ultimate goals of open-source projects.</p><p>With the increasing popularity of miniaturized microscopes (<xref ref-type="bibr" rid="bib1">Aharoni and Hoogland, 2019</xref>), there has been significant interest in analysis pipelines that can reliably extract neural activities from the data. Numerous algorithms have been developed to solve this problem (<xref ref-type="bibr" rid="bib52">Zhou et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Mukamel et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">Klibisz et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Friedrich et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Giovannucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Pnevmatikakis et al., 2016</xref>), and many of them are implemented as open-source packages that can function as a one-stop pipeline (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>). However, one of the biggest obstacles for neuroscience labs in adopting analysis pipelines is the difficulty in understanding the exact operation of the algorithms, leading to two notable challenges: first, researchers face difficulties adjusting the parameters when the data they have collected are out of the expected scope of the pipeline’s default parameters. Second, even after neural activity data is obtained, it is hard for researchers to be sure that they have chosen the best approaches and parameters for their dataset. Indeed, it has been found that depending on the features of the data and the metric used, more sophisticated algorithms do not always outperform simpler algorithms (<xref ref-type="bibr" rid="bib37">Pachitariu et al., 2018</xref>), making it even harder for researchers to interpret the results obtained from some analysis pipelines. Researchers therefore often have to outsource data analysis to experts with strong computational backgrounds or simply trust the output of the algorithms being used. Minian was created to address these challenges. By providing not only detailed documentation of all functions, but also by providing rich interactive visualizations, Minian helps researchers to develop an intuitive understanding of the operations of algorithms without expertise in mathematics or computer science. These insights help researchers choose the best parameters, as well as to become more confident in their interpretation of results. Furthermore, transparency regarding the underlying algorithms enables researchers to develop in-house modifications of the pipeline, which is a common practice in neuroscience labs. We believe that Minian will contribute to the open science community by making the analysis of calcium imaging data more accessible and understandable to neuroscience labs.</p></sec><sec id="s4-2"><title>Limitations</title><p>Although Minian provides users with insights into the parameter tuning process across different brain regions, these insights are achieved mainly through visual inspection. However, the performance of an analysis pipeline should be measured objectively. While calcium imaging has been validated with electrophysiology under ex vivo settings (<xref ref-type="bibr" rid="bib9">Chen et al., 2013</xref>), ground-truth data for single-photon in vivo calcium imaging are lacking, making objective evaluation of the algorithms difficult. Therefore, here we have provided only indirect validations of the pipeline by recapitulating well-established biological findings.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Software, Validation, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation, Project administration, Resources, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Resources, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Investigation, Resources, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Resources, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Formal analysis, Resources, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Data curation, Resources, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Formal analysis, Investigation, Resources, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Funding acquisition, Project administration, Resources, Supervision, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were performed in accordance with relevant guidelines and regulations approved by the Institutional Animal Care and Use Committee of Icahn School of Medicine at Mount Sinai (Reference #: IACUC-2017-0361, Protocol #: 17-1994).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-70661-transrepform1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Github repo: <ext-link ext-link-type="uri" xlink:href="https://github.com/denisecailab/minian">https://github.com/denisecailab/minian</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e11e29a1ec322ff836c3a0ca0abc3c2b0303fb81;origin=https://github.com/denisecailab/minian;visit=swh:1:snp:695d1c8410ede7ac197b94946b8f59eb122cbbb8;anchor=swh:1:rev:a6d3339932df3c63aa46811fc70fc00aca09218d">swh:1:rev:a6d3339932df3c63aa46811fc70fc00aca09218d</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Eftychios A Pnevmatikakis, Andrea Giovannucci, and Liam Paninski for establishing the theoretical foundation and providing helpful insights for the pipeline. We thank Pat Gunn for helping with benchmarking with CaImAn pipeline. We thank Taylor Francisco and Denisse Morales-Rodriguez for helping with data analysis and revision. We thank MetaCell (Stephen Larson, Giovanni Idili, Zoran Sinnema, Dan Knudsen, and Paolo Bazzigaluppi) for contributing to the documentation and continuous integration of the pipeline. We thank Stellate Communications for assistance with the preparation of this manuscript. We thank Brandon Wei, Mimi La-Vu, and Christopher Lee for contributing to the dataset used in Minian development and testing. The authors acknowledge support from the following funding sources: WM is supported by NIH F32AG067640. KR is supported by NIH BRAIN Initiative (R01EB028166), James S. McDonnell Foundation’s Understanding Human Cognition Scholar Award (220020466), NSF Award (NSF1926800 and NSF2046583), Simons Foundation (891834) and Alfred P. Sloan Foundation (FG-2019-12027). TS is supported by CURE Epilepsy Taking Flight Award, American Epilepsy Society Junior investigator Award, R03 NS111493, R21 DA049568, and R01 NS116357. DA is supported by NIH U01NS094286-01, and NSF Award (NSF1700408 Neurotech Hub). DJC is supported by NIH DP2MH122399, R01 MH120162, One Mind Otsuka Rising Star Award, McKnight Memory and Cognitive Disorders Award, Klingenstein-Simons Fellowship Award in Neuroscience, Mount Sinai Distinguished Scholar Award, Brain Research Foundation Award, and NARSAD Young Investigator Award.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Hoogland</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Circuit Investigations With Open-Source Miniaturized Microscopes: Past, Present and Future</article-title><source>Frontiers in Cellular Neuroscience</source><volume>13</volume><elocation-id>141</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2019.00141</pub-id><pub-id pub-id-type="pmid">31024265</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Khakh</surname><given-names>BS</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>All the light that we can see: a new era in miniaturized microscopy</article-title><source>Nature Methods</source><volume>16</volume><fpage>11</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0266-x</pub-id><pub-id pub-id-type="pmid">30573833</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbera</surname><given-names>G</given-names></name><name><surname>Liang</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A wireless miniScope for deep brain imaging in freely moving mice</article-title><source>Journal of Neuroscience Methods</source><volume>323</volume><fpage>56</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.05.008</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Bokeh Development Team</collab></person-group><year iso-8601-date="2020">2020</year><data-title>Bokeh: Python library for interactive visualization</data-title><source>Bokeh</source><ext-link ext-link-type="uri" xlink:href="https://bokeh.org/">https://bokeh.org/</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>The OpenCV Library</data-title><source>Dr Dobb’s Journal of Software Tools</source></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brunelli</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Template Matching Techniques in Computer Vision</source><publisher-loc>Chichester, UK</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buccino</surname><given-names>AP</given-names></name><name><surname>Lepperød</surname><given-names>ME</given-names></name><name><surname>Dragly</surname><given-names>SA</given-names></name><name><surname>Häfliger</surname><given-names>P</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Open source modules for tracking animal behavior and closed-loop stimulation based on Open Ephys and Bonsai</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>e055002</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aacf45</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>DJ</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Shuman</surname><given-names>T</given-names></name><name><surname>Shobe</surname><given-names>J</given-names></name><name><surname>Biane</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wei</surname><given-names>B</given-names></name><name><surname>Veshkini</surname><given-names>M</given-names></name><name><surname>La-Vu</surname><given-names>M</given-names></name><name><surname>Lou</surname><given-names>J</given-names></name><name><surname>Flores</surname><given-names>SE</given-names></name><name><surname>Kim</surname><given-names>I</given-names></name><name><surname>Sano</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Baumgaertel</surname><given-names>K</given-names></name><name><surname>Lavi</surname><given-names>A</given-names></name><name><surname>Kamata</surname><given-names>M</given-names></name><name><surname>Tuszynski</surname><given-names>M</given-names></name><name><surname>Mayford</surname><given-names>M</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A shared neural ensemble links distinct contextual memories encoded close in time</article-title><source>Nature</source><volume>534</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1038/nature17955</pub-id><pub-id pub-id-type="pmid">27251287</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T-W</given-names></name><name><surname>Wardill</surname><given-names>TJ</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Pulver</surname><given-names>SR</given-names></name><name><surname>Renninger</surname><given-names>SL</given-names></name><name><surname>Baohan</surname><given-names>A</given-names></name><name><surname>Schreiter</surname><given-names>ER</given-names></name><name><surname>Kerr</surname><given-names>RA</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Kim</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Dask Team</collab></person-group><year iso-8601-date="2016">2016</year><data-title>Dask: Library for dynamic task scheduling</data-title><source>Dask</source><ext-link ext-link-type="uri" xlink:href="https://dask.org">https://dask.org</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Groot</surname><given-names>A</given-names></name><name><surname>van den Boom</surname><given-names>BJ</given-names></name><name><surname>van Genderen</surname><given-names>RM</given-names></name><name><surname>Coppens</surname><given-names>J</given-names></name><name><surname>van Veldhuijzen</surname><given-names>J</given-names></name><name><surname>Bos</surname><given-names>J</given-names></name><name><surname>Hoedemaker</surname><given-names>H</given-names></name><name><surname>Negrello</surname><given-names>M</given-names></name><name><surname>Willuhn</surname><given-names>I</given-names></name><name><surname>De Zeeuw</surname><given-names>CI</given-names></name><name><surname>Hoogland</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>NINscope, a versatile miniscope for multi-region circuit investigations</article-title><source>eLife</source><volume>9</volume><elocation-id>e49987</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49987</pub-id><pub-id pub-id-type="pmid">31934857</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Open source tools for large-scale neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.04.002</pub-id><pub-id pub-id-type="pmid">25982977</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frie</surname><given-names>JA</given-names></name><name><surname>Khokhar</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An Open Source Automated Two-Bottle Choice Test Apparatus for Rats</article-title><source>HardwareX</source><volume>5</volume><elocation-id>e00061</elocation-id><pub-id pub-id-type="doi">10.1016/j.ohx.2019.e00061</pub-id><pub-id pub-id-type="pmid">31245655</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fast online deconvolution of calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005423</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005423</pub-id><pub-id pub-id-type="pmid">28291787</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Online analysis of microendoscopic 1-photon calcium imaging data streams</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008565</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008565</pub-id><pub-id pub-id-type="pmid">33507937</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>KK</given-names></name><name><surname>Burns</surname><given-names>LD</given-names></name><name><surname>Cocker</surname><given-names>ED</given-names></name><name><surname>Nimmerjahn</surname><given-names>A</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name><name><surname>Gamal</surname><given-names>AE</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Miniaturized integration of a fluorescence microscope</article-title><source>Nature Methods</source><volume>8</volume><fpage>871</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1694</pub-id><pub-id pub-id-type="pmid">21909102</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Kaufman</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>A</given-names></name><name><surname>Chklovskii</surname><given-names>D</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>OnACID: Online Analysis of Calcium Imaging Data in Real Time*</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/193383</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Gunn</surname><given-names>P</given-names></name><name><surname>Kalfon</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>BL</given-names></name><name><surname>Koay</surname><given-names>SA</given-names></name><name><surname>Taxidis</surname><given-names>J</given-names></name><name><surname>Najafi</surname><given-names>F</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Khakh</surname><given-names>BS</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>CaImAn an open source tool for scalable calcium imaging data analysis</article-title><source>eLife</source><volume>8</volume><elocation-id>e38173</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38173</pub-id><pub-id pub-id-type="pmid">30652683</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>Hamman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>xarray: N-D labeled Arrays and Datasets in Python</article-title><source>Journal of Open Research Software</source><volume>5</volume><elocation-id>e10</elocation-id><pub-id pub-id-type="doi">10.5334/jors.148</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Inan</surname><given-names>H</given-names></name><name><surname>Schmuckermair</surname><given-names>C</given-names></name><name><surname>Tasci</surname><given-names>T</given-names></name><name><surname>Ahanonu</surname><given-names>BO</given-names></name><name><surname>Hernandez</surname><given-names>O</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Dinç</surname><given-names>F</given-names></name><name><surname>Wagner</surname><given-names>MJ</given-names></name><name><surname>Erdogdu</surname><given-names>MA</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast and Statistically Robust Cell Extraction from Large-Scale Neural Calcium Imaging Datasets</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.24.436279</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>AD</given-names></name><name><surname>Ramsaran</surname><given-names>AI</given-names></name><name><surname>Mocle</surname><given-names>AJ</given-names></name><name><surname>Tran</surname><given-names>LM</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Frankland</surname><given-names>PW</given-names></name><name><surname>Josselyn</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Compact Head-Mounted Endoscope for In Vivo Calcium Imaging in Freely Behaving Mice</article-title><source>Current Protocols in Neuroscience</source><volume>84</volume><elocation-id>e51</elocation-id><pub-id pub-id-type="doi">10.1002/cpns.51</pub-id><pub-id pub-id-type="pmid">29944206</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jewell</surname><given-names>S</given-names></name><name><surname>Witten</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Exact spike train inference via $\ell_{0}$ optimization</article-title><source>The Annals of Applied Statistics</source><volume>12</volume><elocation-id>1162</elocation-id><pub-id pub-id-type="doi">10.1214/18-AOAS1162</pub-id><pub-id pub-id-type="pmid">30627301</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>WL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Rivera-Alba</surname><given-names>M</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Klibisz</surname><given-names>A</given-names></name><name><surname>Rose</surname><given-names>D</given-names></name><name><surname>Eicholtz</surname><given-names>M</given-names></name><name><surname>Blundon</surname><given-names>J</given-names></name><name><surname>Zakharenko</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Fast, Simple Calcium Imaging Segmentation with Fully Convolutional Networks</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-67558-9_33</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kluyver</surname><given-names>T</given-names></name><name><surname>Ragan-Kelley</surname><given-names>B</given-names></name><name><surname>Pé</surname><given-names>FR</given-names></name><name><surname>Granger</surname><given-names>B</given-names></name><name><surname>Bussonnier</surname><given-names>M</given-names></name><name><surname>Frederic</surname><given-names>J</given-names></name><name><surname>Kelley</surname><given-names>K</given-names></name><name><surname>Hamrick</surname><given-names>J</given-names></name><name><surname>Grout</surname><given-names>J</given-names></name><collab>Jupyter Development Team</collab></person-group><year iso-8601-date="2016">2016</year><source>Jupyter Notebooks – a Publishing Format for Reproducible Computational Workflows</source><publisher-name>Positioning and Power in Academic Publishing</publisher-name><pub-id pub-id-type="doi">10.3233/978-1-61499-649-1-87</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberti</surname><given-names>WA</given-names></name><name><surname>Perkins</surname><given-names>LN</given-names></name><name><surname>Leman</surname><given-names>DP</given-names></name><name><surname>Gardner</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An open source, wireless capable miniature microscope system</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>045001</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa6806</pub-id><pub-id pub-id-type="pmid">28514229</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazão</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><name><surname>Calcaterra</surname><given-names>L</given-names></name><name><surname>Dreosti</surname><given-names>E</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Singh-Alvarado</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>ZC</given-names></name><name><surname>Fröhlich</surname><given-names>F</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>MIN1PIPE: A Miniscope 1-Photon-Based Calcium Imaging Signal Extraction Pipeline</article-title><source>Cell Reports</source><volume>23</volume><fpage>3673</fpage><lpage>3684</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.05.062</pub-id><pub-id pub-id-type="pmid">29925007</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matikainen-Ankney</surname><given-names>BA</given-names></name><name><surname>Earnest</surname><given-names>T</given-names></name><name><surname>Ali</surname><given-names>M</given-names></name><name><surname>Casey</surname><given-names>E</given-names></name><name><surname>Wang</surname><given-names>JG</given-names></name><name><surname>Sutton</surname><given-names>AK</given-names></name><name><surname>Legaria</surname><given-names>AA</given-names></name><name><surname>Barclay</surname><given-names>KM</given-names></name><name><surname>Murdaugh</surname><given-names>LB</given-names></name><name><surname>Norris</surname><given-names>MR</given-names></name><name><surname>Chang</surname><given-names>YH</given-names></name><name><surname>Nguyen</surname><given-names>KP</given-names></name><name><surname>Lin</surname><given-names>E</given-names></name><name><surname>Reichenbach</surname><given-names>A</given-names></name><name><surname>Clarke</surname><given-names>RE</given-names></name><name><surname>Stark</surname><given-names>R</given-names></name><name><surname>Conway</surname><given-names>SM</given-names></name><name><surname>Carvalho</surname><given-names>F</given-names></name><name><surname>Al-Hasani</surname><given-names>R</given-names></name><name><surname>McCall</surname><given-names>JG</given-names></name><name><surname>Creed</surname><given-names>MC</given-names></name><name><surname>Cazares</surname><given-names>V</given-names></name><name><surname>Buczynski</surname><given-names>MW</given-names></name><name><surname>Krashes</surname><given-names>MJ</given-names></name><name><surname>Andrews</surname><given-names>ZB</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An open-source device for measuring food intake and operant behavior in rodent home-cages</article-title><source>eLife</source><volume>10</volume><elocation-id>e66173</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66173</pub-id><pub-id pub-id-type="pmid">33779547</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukamel</surname><given-names>EA</given-names></name><name><surname>Nimmerjahn</surname><given-names>A</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated analysis of cellular signals from large-scale calcium imaging data</article-title><source>Neuron</source><volume>63</volume><fpage>747</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.009</pub-id><pub-id pub-id-type="pmid">19778505</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>KP</given-names></name><name><surname>O’Neal</surname><given-names>TJ</given-names></name><name><surname>Bolonduro</surname><given-names>OA</given-names></name><name><surname>White</surname><given-names>E</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Feeding Experimentation Device (FED): A flexible open-source device for measuring feeding behavior</article-title><source>Journal of Neuroscience Methods</source><volume>267</volume><fpage>108</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.003</pub-id><pub-id pub-id-type="pmid">27060385</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owen</surname><given-names>SF</given-names></name><name><surname>Kreitzer</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An open-source control system for in vivo fluorescence measurements from deep-brain structures</article-title><source>Journal of Neuroscience Methods</source><volume>311</volume><fpage>170</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2018.10.022</pub-id><pub-id pub-id-type="pmid">30342106</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robustness of Spike Deconvolution for Neuronal Calcium Imaging</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7976</fpage><lpage>7985</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3339-17.2018</pub-id><pub-id pub-id-type="pmid">30082416</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>ZT</given-names></name><name><surname>Dong</surname><given-names>Z</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Vetere</surname><given-names>LM</given-names></name><name><surname>Page-Harley</surname><given-names>L</given-names></name><name><surname>Shuman</surname><given-names>T</given-names></name><name><surname>Cai</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ezTrack: An open-source video analysis pipeline for the investigation of animal behavior</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>19979</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-56408-9</pub-id><pub-id pub-id-type="pmid">31882950</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Soudry</surname><given-names>D</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Merel</surname><given-names>J</given-names></name><name><surname>Pfau</surname><given-names>D</given-names></name><name><surname>Reardon</surname><given-names>T</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Lacefield</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Ahrens</surname><given-names>M</given-names></name><name><surname>Bruno</surname><given-names>R</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Peterka</surname><given-names>DS</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data</article-title><source>Neuron</source><volume>89</volume><fpage>285</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.037</pub-id><pub-id pub-id-type="pmid">26774160</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rudiger</surname><given-names>P</given-names></name><name><surname>Stevens</surname><given-names>JL</given-names></name><name><surname>Bednar</surname><given-names>JA</given-names></name><name><surname>Nijholt</surname><given-names>B</given-names></name><name><surname>Andrew</surname><given-names>CB</given-names></name><name><surname>Randelhoff</surname><given-names>A</given-names></name><name><surname>Mease</surname><given-names>J</given-names></name><name><surname>Tenner</surname><given-names>V</given-names></name><name><surname>Kbowen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>holoviz/holoviews</data-title><version designator="Version 1.13.3">Version 1.13.3</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/3904606/export/json">https://zenodo.org/record/3904606/export/json</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Thiberge</surname><given-names>SY</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Tervo</surname><given-names>DGR</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Karpova</surname><given-names>AY</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging Cortical Dynamics in GCaMP Transgenic Rats with a Head-Mounted Widefield Macroscope</article-title><source>Neuron</source><volume>100</volume><fpage>1045</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.050</pub-id><pub-id pub-id-type="pmid">30482694</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheintuch</surname><given-names>L</given-names></name><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Brande-Eilat</surname><given-names>N</given-names></name><name><surname>Geva</surname><given-names>N</given-names></name><name><surname>Sadeh</surname><given-names>N</given-names></name><name><surname>Pinchasof</surname><given-names>O</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tracking the Same Neurons across Multiple Days in Ca2+ Imaging Data</article-title><source>Cell Reports</source><volume>21</volume><fpage>1102</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.10.013</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuman</surname><given-names>T</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Cai</surname><given-names>DJ</given-names></name><name><surname>Lee</surname><given-names>CR</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Page-Harley</surname><given-names>L</given-names></name><name><surname>Vetere</surname><given-names>LM</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>CY</given-names></name><name><surname>Mollinedo-Gajate</surname><given-names>I</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Pennington</surname><given-names>ZT</given-names></name><name><surname>Taxidis</surname><given-names>J</given-names></name><name><surname>Flores</surname><given-names>SE</given-names></name><name><surname>Cheng</surname><given-names>K</given-names></name><name><surname>Javaherian</surname><given-names>M</given-names></name><name><surname>Kaba</surname><given-names>CC</given-names></name><name><surname>Rao</surname><given-names>N</given-names></name><name><surname>La-Vu</surname><given-names>M</given-names></name><name><surname>Pandi</surname><given-names>I</given-names></name><name><surname>Shtrahman</surname><given-names>M</given-names></name><name><surname>Bakhurin</surname><given-names>KI</given-names></name><name><surname>Masmanidis</surname><given-names>SC</given-names></name><name><surname>Khakh</surname><given-names>BS</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Breakdown of spatial coding and interneuron synchronization in epileptic mice</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>229</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0559-0</pub-id><pub-id pub-id-type="pmid">31907437</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>López</surname><given-names>AC</given-names></name><name><surname>Patel</surname><given-names>YA</given-names></name><name><surname>Abramov</surname><given-names>K</given-names></name><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Voigts</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Open Ephys: an open-source, plugin-based platform for multichannel electrophysiology</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>045003</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa5eea</pub-id><pub-id pub-id-type="pmid">28169219</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skocek</surname><given-names>O</given-names></name><name><surname>Nöbauer</surname><given-names>T</given-names></name><name><surname>Weilguny</surname><given-names>L</given-names></name><name><surname>Martínez Traub</surname><given-names>F</given-names></name><name><surname>Xia</surname><given-names>CN</given-names></name><name><surname>Molodtsov</surname><given-names>MI</given-names></name><name><surname>Grama</surname><given-names>A</given-names></name><name><surname>Yamagata</surname><given-names>M</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name><name><surname>Vaziri</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-speed volumetric imaging of neuronal activity in freely moving rodents</article-title><source>Nature Methods</source><volume>15</volume><fpage>429</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0008-0</pub-id><pub-id pub-id-type="pmid">29736000</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solari</surname><given-names>N</given-names></name><name><surname>Sviatkó</surname><given-names>K</given-names></name><name><surname>Laszlovszky</surname><given-names>T</given-names></name><name><surname>Hegedüs</surname><given-names>P</given-names></name><name><surname>Hangya</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Open Source Tools for Temporally Controlled Rodent Behavior Suitable for Electrophysiology and Optogenetic Manipulations</article-title><source>Frontiers in Systems Neuroscience</source><volume>12</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2018.00018</pub-id><pub-id pub-id-type="pmid">29867383</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>LT</given-names></name><name><surname>Best</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Long-term stability of the place-field activity of single units recorded from the dorsal hippocampus of freely behaving rats</article-title><source>Brain Research</source><volume>509</volume><fpage>299</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(90)90555-p</pub-id><pub-id pub-id-type="pmid">2322825</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Boom</surname><given-names>BJG</given-names></name><name><surname>Pavlidi</surname><given-names>P</given-names></name><name><surname>Wolf</surname><given-names>CJH</given-names></name><name><surname>Mooij</surname><given-names>AH</given-names></name><name><surname>Willuhn</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automated classification of self-grooming in mice using open-source software</article-title><source>Journal of Neuroscience Methods</source><volume>289</volume><fpage>48</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.05.026</pub-id><pub-id pub-id-type="pmid">28648717</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogelstein</surname><given-names>JT</given-names></name><name><surname>Packer</surname><given-names>AM</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Sippy</surname><given-names>T</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Fast nonnegative deconvolution for spike train inference from population calcium imaging</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>3691</fpage><lpage>3704</lpage><pub-id pub-id-type="doi">10.1152/jn.01073.2009</pub-id><pub-id pub-id-type="pmid">20554834</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SR</given-names></name><name><surname>Amarante</surname><given-names>LM</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Future Is Open: Open-Source Tools for Behavioral Neuroscience Research</article-title><source>Eneuro</source><volume>6</volume><elocation-id>ENEURO</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0223-19.2019</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Resendez</surname><given-names>SL</given-names></name><name><surname>Rodriguez-Romaguera</surname><given-names>J</given-names></name><name><surname>Jimenez</surname><given-names>JC</given-names></name><name><surname>Neufeld</surname><given-names>SQ</given-names></name><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name><name><surname>Hen</surname><given-names>R</given-names></name><name><surname>Kheirbek</surname><given-names>MA</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data</article-title><source>eLife</source><volume>7</volume><elocation-id>e28728</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.28728</pub-id><pub-id pub-id-type="pmid">29469809</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziv</surname><given-names>Y</given-names></name><name><surname>Burns</surname><given-names>LD</given-names></name><name><surname>Cocker</surname><given-names>ED</given-names></name><name><surname>Hamel</surname><given-names>EO</given-names></name><name><surname>Ghosh</surname><given-names>KK</given-names></name><name><surname>Kitch</surname><given-names>LJ</given-names></name><name><surname>El Gamal</surname><given-names>A</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Long-term dynamics of CA1 hippocampal place codes</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>264</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nn.3329</pub-id><pub-id pub-id-type="pmid">23396101</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Supplemental information</title><sec id="s8-1" sec-type="appendix"><title>Parallel and out-of-core computation with Dask</title><p>In Minian, we use a modern parallel computing library called Dask to implement parallel and out-of-core computation. Dask divides the data into small chunks along all dimensions, then flexibly merges the data along some dimensions in each step. We leverage the fact that each step in our pipeline can be carried out chunk by chunk independently along either the temporal (frame) dimension or the spatial (height and width) dimensions, thus requiring no interpolation or special handling of borders when merged together, producing results as if no chunking had been done. For example, motion correction and most preprocessing steps that involve frame-wise filtering can be carried out on independent temporal chunks, whereas computation of pixel correlations can be carried out on independent spatial chunks. Similarly, during the core CNMF computation steps, spatial chunking can be used during update of spatial footprints since spatial update is carried out pixel by pixel. Meanwhile, temporal chunking can be used when projecting the input data onto spatial footprints of cells, which is usually the most memory-demanding step. Although the optimization step during the temporal update is computed across all frames and no temporal chunking can be used, we can still chunk across cells, and in practice the memory demand in this step is much smaller compared to other steps involving raw input data. Consequently, our pipeline fully supports out-of-core computation, and memory demand is dramatically reduced. In practice, a modern laptop can easily handle the analysis of a full experiment with a typical recording length of up to 20 min. Dask also enables us to carry out lazy evaluation of many steps where the computation is postponed until the result is needed, for example, when a plot of the result is requested. This enables selective evaluation of operations only on the subset of data that will become part of the visualization and thus helps users to quickly explore a large space of parameters without committing to the full operation each time.</p></sec></sec><sec id="s9" sec-type="appendix"><title>Seeds refinement with a Gaussian mixture model</title><p>As described earlier, an alternative strategy to thresholding fluorescence intensity during seeds initialization is to explicitly model the distribution of fluorescence fluctuations of all candidate seeds and select those with relatively higher fluctuation. Here, we describe this process and the rationale. Since the seeds are generated from local maxima, they include noise from relatively empty regions with no actual cells. The seeds from these regions usually have low fluctuations in fluorescence across time and can be classified as spurious. To identify these cases, we compute a range of fluctuation for each seed (range of min-max across time) and model these ranges with a Gaussian mixture model of two components. The fluctuations from ‘noise’ seeds compose a Gaussian distribution with low fluctuation, while seeds from actual cells assume a higher degree of fluctuation and form another Gaussian distribution with a higher mean. Any seed whose fluctuations belong to the lower Gaussian distribution is discarded in this step. To compute the range of fluctuation for each seed, we compute the difference between the 99.9 and 0.1 percentile of all fluorescence values across time, which is less biased by outliers than the actual maximum and minimum values.</p><p>Normally, this step is parameter-free. In rare cases, there are regions containing noise while other regions are almost completely dark. Thus, seeds from these two regions will form two peaks in the distribution of what the user would consider ‘bad seeds,’ and a Gaussian mixture model with two components will no longer be valid. In such cases, users can tweak the number of components (number of modeled Gaussian distributions), as well as the number of components to be considered as composed of real signal. However, because the two noise distributions are likely to overlap to some degree, using two components will likely suffice. The distribution of fluctuations, Gaussian mixture model fit, and resulting seeds are visualized, enabling the user to judge the appropriateness and accuracy of this step. It should be noted that in practice we have found this process to depend heavily on the relative proportion of the ‘good’ and ‘bad’ seeds and can easily result in a significant amount of false negatives if the proportion of the ‘bad’ seed is too low. This makes the Gaussian mixture model approach less stable and in general less preferable to simple thresholding unless a good threshold of fluorescence intensity cannot be easily determined.</p></sec><sec id="s10" sec-type="appendix"><title>Generation of simulated datasets</title><p>We use a pipeline modified from (<xref ref-type="bibr" rid="bib52">Zhou et al., 2018</xref>) and (<xref ref-type="bibr" rid="bib30">Lu et al., 2018</xref>) to generate simulated data for validation and benchmarking of Minian. Specifically, we generate a 512 × 512 pixels field of view with varying number of frames and neurons. The neurons are simulated as spherical 2-D Gaussian. The center of neurons is drawn uniformly from the whole field of view, and the Gaussian widths <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each neuron are drawn from <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a minimum value of 3. Spikes are simulated from a Bernoulli process with a 0.01 probability of spiking per frame. Calcium dynamics are simulated by convolving the spikes with a temporal kernel <inline-formula><mml:math id="inf42"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, with rise time <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> frames and decay time <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:math></inline-formula> frames. We simulate the spatial footprints of backgrounds as spherical 2-D Gaussian distributed uniformly across field of view. In total, 300 independent background terms are used for all simulations. The Gaussian widths are drawn from <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>900</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mn>50</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The temporal dynamics of backgrounds are simulated from a constrained Gaussian random walk process with steps drawn from <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, then clipped to be non-negative and Gaussian smoothed temporally with a variance of 60 frames. We also simulate motion of the field of view as 2-D translations. The translational shift in each direction is simulated from a constrained Gaussian random walk process with steps drawn from <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>0.2</mml:mn><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf48"><mml:mi>d</mml:mi></mml:math></inline-formula> is the current amount of shift. Lastly, we add a <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> Gaussian noise to the entire simulated data. The activity of neurons is multiplied by a scalar before combining with the background activity and noise. We call this scalar ‘signal level.’</p><p>To validate the accuracy of Minian output, we simulate data with different signal level and number of cells. The signal levels we use are 0.2, 0.4, 0.6, 1.0, 1.4, and 1.8. The number of cells we use is 100, 300, and 500. On the other hand, to benchmark the performance of Minian, we simulate data with different number of frames and cells. The number of frames varies from 4000 to 28,000 with a step size of 8000. The number of cells we use is 100, 300, and 500.</p></sec><sec id="s11" sec-type="appendix"><title>Matching neurons for validation</title><p>To compute different metrics of the accuracy of Minian output, we first need to match the putative neurons from Minian output with neurons from ground truth. To obtain this mapping, we first compute the max projection of spatial footprints across all neurons. We then register the max projection of putative spatial footprints to the max projection of ground-truth spatial footprints by estimating a translational shift between the two max projection images. After correcting for translational shifts, we compute the center of mass for all neurons, from which we obtain an N × M pairwise distance matrix, where N and M are number of neurons detected by Minian and number of ground-truth neurons, respectively. We then calculate an optimal mapping by solving the linear assignment problem of minimizing the total cost (distance) of a particular cell mapping. Lastly, we threshold the resulting mapping by discarding any matched cell that has a distance larger than 15 pixels.</p></sec><sec id="s12" sec-type="appendix"><title>Classification of place cells</title><p>We use the spatially binned averaged ‘firing’ rate calculated from spike signals to classify whether each cell is a place cell. A place cell must simultaneously satisfy three criteria: spatial information criterion, stability criterion, and place field size criterion. To determine whether a cell has significant spatial information or stability, we obtain a null distribution of the measurements (spatial information and stability) with a bootstrap strategy, where we roll the timing of activity by a random amount for each cell 1000 times. The observed spatial information or stability is defined as significant if it exceeds the 95th percentile of its null distribution (p&lt;0.05). For the spatial information criterion, we use the joint information between ‘firing’ rate and an animal’s location measured in bits per ‘spike.’ For the stability criterion, we calculate the Fisher’s z-transformation of the Pearson correlation coefficient between spatial ‘firing’ patterns across different trials within a recording session. A trial is defined as the time that the animal runs from one end of the linear track to the other and returns to the starting location. We calculate the z-transformed correlation between the odd number of trials and the even number of trials, as well as between the first half of the trials and the second half of the trials. We then average these two measures of correlations and use that as the measure of stability for a cell. Lastly, for the place field size criterion, we define the place field of each cell as the longest contiguous spatial bin where the averaged ‘firing’ rate exceeded the 95th percentile of all averaged firing rate bins. A cell must have a place field larger than 4 cm (i.e., two spatial bins) to pass the place field size criterion.</p></sec><sec id="s13" sec-type="appendix"><title>Animals</title><p>Adult male C57/BL6J mice from Jackson Laboratories were used for all testing. Animals were housed in a temperature, humidity, and light-controlled vivarium down the hall from the experimental testing rooms with lights on at 7 am and off at 7 pm. Water was restricted to maintain a body weight of 85–90%. Water deprivation consisted of allotting the animal ~1 ml of water per day, including water obtained during testing. Water not obtained during testing was given after the testing period. Animals were acclimated to handling for 5–7 days prior to training/testing. All experiments were performed in accordance with relevant guidelines and regulations approved by the Institutional Animal Care and Use Committee of Icahn School of Medicine at Mount Sinai (reference # IACUC-2017-0361, protocol # 17-1994).</p></sec><sec id="s14" sec-type="appendix"><title>Tested hardware specifications</title><p>The hardware specifications of computers that have effectively run Minian are summarized in the following table.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>A list of computers tested with Minian with specifications (listed roughly by increasing computation power).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Manufacture</th><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">CPU</th><th align="left" valign="bottom">RAM</th><th align="left" valign="bottom">Storage</th><th align="left" valign="bottom">Operating system</th></tr></thead><tbody><tr><td align="left" valign="bottom">Custom-built</td><td align="left" valign="bottom">Carbon</td><td align="left" valign="bottom">AMD Ryzen Threadripper 2950 × 4.4 GHz × 16</td><td align="justify" valign="bottom">128 GB</td><td align="justify" valign="bottom">2TB SSD</td><td align="left" valign="bottom">Ubuntu 18.04</td></tr><tr><td align="left" valign="bottom">Microsoft</td><td align="left" valign="bottom">Surface Pro 6</td><td align="left" valign="bottom">Intel Core i5-8250U 1.6 GHz × 4</td><td align="justify" valign="bottom">8 GB</td><td align="justify" valign="bottom">256GB SSD</td><td align="left" valign="bottom">Windows 10</td></tr><tr><td align="left" valign="bottom">Dell</td><td align="left" valign="bottom">Precision 5,530</td><td align="left" valign="bottom">Intel Core i5-8400H 2.5 GHz × 4</td><td align="justify" valign="bottom">16 GB</td><td align="justify" valign="bottom">256GB SSD</td><td align="left" valign="bottom">Ubuntu 18.04</td></tr><tr><td align="left" valign="bottom">Apple</td><td align="left" valign="bottom">MacBook Pro 152</td><td align="left" valign="bottom">Intel Core i7-8559U 2.7 GHz × 4</td><td align="justify" valign="bottom">16 GB</td><td align="justify" valign="bottom">1TB SSD</td><td align="left" valign="bottom">macOS 10.14 Mojave</td></tr><tr><td align="left" valign="bottom">Custom-built</td><td align="left" valign="bottom">Amethyst</td><td align="left" valign="bottom">Intel Xeon E5-1650 3.6 GHz × 6</td><td align="justify" valign="bottom">128 GB</td><td align="justify" valign="bottom">6TB HDD</td><td align="left" valign="bottom">Ubuntu 17.1</td></tr></tbody></table></table-wrap><sec id="s14-1" sec-type="appendix"><title>List of dependencies</title><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>A list of open-source packages and the specific versions on which Minian depends.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Package</th><th align="left" valign="bottom">Version</th></tr></thead><tbody><tr><td align="left" valign="bottom">av</td><td align="left" valign="bottom">7.0</td></tr><tr><td align="left" valign="bottom">Bokeh</td><td align="left" valign="bottom">1.4</td></tr><tr><td align="left" valign="bottom">Bottleneck</td><td align="left" valign="bottom">1.3</td></tr><tr><td align="left" valign="bottom">cairo</td><td align="left" valign="bottom">1.16</td></tr><tr><td align="left" valign="bottom">CVXPY</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Dask</td><td align="left" valign="bottom">2.11</td></tr><tr><td align="left" valign="bottom">Datashader</td><td align="left" valign="bottom">0.1</td></tr><tr><td align="left" valign="bottom">distributed</td><td align="left" valign="bottom">2.11</td></tr><tr><td align="left" valign="bottom">ecos</td><td align="left" valign="bottom">2.0</td></tr><tr><td align="left" valign="bottom">FFmpeg</td><td align="left" valign="bottom">4.1</td></tr><tr><td align="left" valign="bottom">FFTW</td><td align="left" valign="bottom">3.3</td></tr><tr><td align="left" valign="bottom">HoloViews</td><td align="left" valign="bottom">1.12</td></tr><tr><td align="left" valign="bottom">IPython</td><td align="left" valign="bottom">7.12</td></tr><tr><td align="left" valign="bottom">ipywidgets</td><td align="left" valign="bottom">7.5</td></tr><tr><td align="left" valign="bottom">Jupyter</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Matplotlib</td><td align="left" valign="bottom">3.1</td></tr><tr><td align="left" valign="bottom">natsort</td><td align="left" valign="bottom">7.0</td></tr><tr><td align="left" valign="bottom">netCDF4</td><td align="left" valign="bottom">1.5</td></tr><tr><td align="left" valign="bottom">NetworkX</td><td align="left" valign="bottom">2.4</td></tr><tr><td align="left" valign="bottom">Node.js</td><td align="left" valign="bottom">13.9</td></tr><tr><td align="left" valign="bottom">Numba</td><td align="left" valign="bottom">0.48</td></tr><tr><td align="left" valign="bottom">NumPy</td><td align="left" valign="bottom">1.18</td></tr><tr><td align="left" valign="bottom">openCV</td><td align="left" valign="bottom">4.2</td></tr><tr><td align="left" valign="bottom">pandas</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Panel</td><td align="left" valign="bottom">0.8</td></tr><tr><td align="left" valign="bottom">Papermill</td><td align="left" valign="bottom">2.0</td></tr><tr><td align="left" valign="bottom">param</td><td align="left" valign="bottom">1.9</td></tr><tr><td align="left" valign="bottom">pip</td><td align="left" valign="bottom">20.0</td></tr><tr><td align="left" valign="bottom">pyFFTW</td><td align="left" valign="bottom">0.12</td></tr><tr><td align="left" valign="bottom">Python</td><td align="left" valign="bottom">3.8</td></tr><tr><td align="left" valign="bottom">SciPy</td><td align="left" valign="bottom">1.4</td></tr><tr><td align="left" valign="bottom">scs</td><td align="left" valign="bottom">2.1</td></tr><tr><td align="left" valign="bottom">statsmodels</td><td align="left" valign="bottom">0.11</td></tr><tr><td align="left" valign="bottom">tifffile</td><td align="left" valign="bottom">2020.2</td></tr><tr><td align="left" valign="bottom">tqdm</td><td align="left" valign="bottom">4.43</td></tr><tr><td align="left" valign="bottom">xarray</td><td align="left" valign="bottom">0.15</td></tr><tr><td align="left" valign="bottom">Zarr</td><td align="left" valign="bottom">2.4</td></tr><tr><td align="left" valign="bottom">MedPy</td><td align="left" valign="bottom">0.4</td></tr><tr><td align="left" valign="bottom">SimpleITK</td><td align="left" valign="bottom">1.2</td></tr></tbody></table></table-wrap></sec><sec id="s14-2" sec-type="appendix"><title>Comparison of algorithms in related pipelines</title><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>List of algorithm implementations in different pipelines.</title><p>For a lot of steps, different algorithm implementation can be chosen by the user based on features of the data. In such cases, we only list the default and most commonly used algorithms here.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Step</th><th align="left" valign="bottom">Minian implementation</th><th align="left" valign="bottom">CaImAn implementation</th><th align="left" valign="bottom">MIN1PIPE implementation</th><th align="left" valign="bottom">Critical parameters</th></tr></thead><tbody><tr><td align="left" valign="bottom">Denoising</td><td align="left" valign="bottom">Median filter</td><td align="left" valign="bottom">None</td><td align="left" valign="bottom">Anisotropic filter</td><td align="left" valign="bottom">Spatial window size of the filter</td></tr><tr><td align="left" valign="bottom">Background removal</td><td align="left" valign="bottom">Morphological top-hat transform</td><td align="left" valign="bottom">None</td><td align="left" valign="bottom">Morphological top-hat transform</td><td align="left" valign="bottom">Spatial window size of the top-hat transform</td></tr><tr><td align="left" valign="bottom">Motion correction</td><td align="left" valign="bottom">FFT-based translational motion correction</td><td align="left" valign="bottom">Nonrigid patch-wise translational motion correction (NoRMCorre)</td><td align="left" valign="bottom">Mix of translational motion correction and Demons diffeomorphic motion correction</td><td align="left" valign="bottom">Different</td></tr><tr><td align="left" valign="bottom">Initialization</td><td align="left" valign="bottom">Seed-based with peak-noise ratio and KS-test refinement</td><td align="left" valign="bottom">Pixel-wise correlation and peak-noise ratio thresholding</td><td align="left" valign="bottom">Seed-based with GMM, peak-noise ratio and KS-test refinement</td><td align="left" valign="bottom">Threshold for correlation and peak-noise ratio</td></tr><tr><td align="left" valign="bottom">Spatial and temporal updates</td><td align="left" valign="bottom">CNMF with CVXPY as deconvolution backend</td><td align="left" valign="bottom">CNMF-E with Oasis as deconvolution backend</td><td align="left" valign="bottom">CNMF with CVX MATLAB package as deconvolution backend</td><td align="left" valign="bottom">Noise cutoff frequency<break/>Expected size of neurons<break/>Sparse penalty</td></tr></tbody></table><table-wrap-foot><fn><p>GMM: Gaussian mixture model; KS: Kolmogorov–Smirnov; CNMF: constrained non-negative matrix factorization.</p></fn></table-wrap-foot></table-wrap></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70661.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>An increasing number of systems neuroscience experiments involve imaging neural activity using head-mounted epifluorescent microscopes, or &quot;miniscopes&quot;. A growing community has been using the Minian software package to process the imaging data into a useful form for subsequent analysis. The Minian team has done an excellent job of exposing the various parameters involved to be easily accessible to users while maintaining a performant robust tool. This work presents Minian and is in many ways an exemplar of how open source software can be presented in journal form.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70661.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your work entitled &quot;Minian: An Open-source Miniscope Analysis Pipeline&quot; for further consideration by <italic>eLife</italic>. Your revised article has been reviewed by three peer reviewers, including Caleb Kemere as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by a Laura Colgin as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers were unanimous in concluding that there was considerable value in presenting Minian to the field. In particular they appreciated (as you have described) the insight into parameter selection that the Minian codebase allows. However, there was concern that the manuscript does not adequately compare Minian with existing tools (e.g., CaImAn) in terms of batch run time (i.e., after the parameters have been selected and one wants to churn through data) and final performance/accuracy (i.e., are the ROIs &quot;as good&quot;?). Given that memory needs are a claimed advantage, a comparison here would also be valuable. Reviewer 3 has used simulated data for the ROI comparison and includes code in their review.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The advent of open source miniscopes has significantly expanded the population of experimenters who are excited to record activity-dependent fluorescence in freely moving animals. Unfortunately, they often find that once they have solved the challenges that arise in acquiring the data, a new challenge awaits – extracting the neural activity for analysis. While there are existing open source tools for extracting activity-dependent fluorescence traces from video data, these packages are not particularly accessible for investigators with significant experience in computational code. For novices, they can be impenetrable. The Minian toolbox aims to solve the problem of data analysis while also bringing along novices, illuminating different parameter choices, and making each step of the process more understandable. We found that, to a great extent, it achieves this aim, and thus we think that it rises to the level of significant interest to a broad audience. We did have some concerns with the current manuscript. In particular, limited evidence is provided to allow comparison of results, performance and efficiency between Minian and other established packages (e.g., how in terms of processing time, or in terms of the results).</p><p>If Minian's major contribution is usability and accessibility (lines 134-135) it is not clear why a new package had to be created rather than extending CaImAn or MIN1PIPE. The former does have out-of-core support, where memory usage can be reduced by limiting the number of parallel processes. A table detailing the methods/algorithms used by each of these packages may be useful for comparison purposes.</p><p>Lines 113-115: The text indicates that data may still be split into patches of pixels, so it is unclear how merging based on overlapping parts can be avoided. Aren't the overlaps necessary because putative neurons may span patch boundaries? Perhaps the authors could also compare performance of temporal splitting and non-splitting data processing. Will temporal splitting lead to comparable performance?</p><p>Lines 117-119. Had to look at documentation website to figure out how to limit memory usage--put this in paper. Include some graphs to show the tradeoff between memory usage and computational time.</p><p>Please comment on motion correction when there is a rotation in the FOV?</p><p>For motion correction, provide additional information about how the template is generated (e.g. how many frames used) and any user-defined parameters.</p><p>Lines 205-206: Will the same mask be applied to all frames before motion correction? Or the mask will be applied to this certain frame and then be applied to all frames after motion correction?</p><p>Line 499: write the function used for optimization.</p><p>Paragraph starting at line 576 would benefit from some equations for clarity's sake.</p><p>Implement manual merging as in original CNMF-E paper?</p><p>Line 699: define preprocessed with respect to Figure 1--this is post motion-correction presumably?</p><p>Line 725, For Figure 14, what is the fps for the data taken? Only frame number was shown in the right side column, if the fps is 20, then 150 frames correspond to about 10 sec of calcium transient, which is on the long side for a healthy neuron.</p><p>Line 826-828: Is dimension here referring to x and y dimensions of the frame? Please elaborate how each slice on one dimension can be processed independently.</p><p>Discuss robustness to long data (where CaImAn seems to fail). Using Minian, was CNMF performed on each msCam video individually? If so, what happens when a calcium event occurs near the video boundaries?</p><p>Line 879: What would be an example number that you would consider as &quot;significantly high (see below) stability&quot;? It is not clear what &quot;see below&quot; was referring to in terms of high stability. Example provided for place field size criterion only.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Dong et al. presents &quot;Minian&quot; (Miniscope Analysis), a novel, open-source analysis pipeline for single photon in vivo calcium imaging studies. Minian aims to allow for users with relatively little computational experience to extract cell location and corresponding calcium traces in order to process and analyze neural activity from imaging experiments. It does so by enhancing the visualization of changes to free parameters during each step of data processing and analysis. Importantly, Minian operates with a relatively small memory demand, making it feasible for use on a laptop. In this manuscript, the authors validate Minian's ability to reliably extract calcium events across different brain regions and from different cell types. The manuscript is well-organized, clear, and easy to follow. Importantly, this manuscript describes a resource which complements previous open-sourced tools built by the authors (e.g. the UCLA Miniscope), as well as other, similar systems. Collectively, these tools comprise an affordable, open-source, easy to use system for single photon calcium imaging and analysis in freely-behaving mice. This project will impact the field by providing open-source, user-friendly, customizable, intuitive software for the analysis of single photon in vivo calcium imaging experiments across a variety of imaging systems.</p><p>1) Page 5, line 100. In regards to visualization of parameter changes and resulting outcomes using Minian, the authors state that &quot;This feature provides users with knowledge about which parameter values should be used to achieve the best outcomes and whether the results appear accurate.&quot; What do the authors mean by &quot;best outcomes&quot; and &quot;appear accurate&quot;? Is there a way the authors can suggest to rigorously test for data accuracy rather than looking to see if data &quot;appear accurate&quot;? Do the authors plan to provide any guidance for standard parameters/variables to use for brain regions and/or cell types? These issues are somewhat highlighted in the limitations section at the end, but I think there could be some more effort put into providing more objectivity to this approach.</p><p>2) Page 9, line 240. As part of their &quot;denoising&quot; step, the authors pass their frames through a median filter. A number of filters can be used for denoising (e.g. Gaussian). The authors should justify why this filter is used and potentially compare it to other filters.</p><p>3) Page 9 describes setting the window size to avoid &quot;salt and pepper&quot; noise on the one end, and &quot;over-smoothing&quot; on the other. Minian allows for the visualization of distinct settings of the window size, however, it is left up to the user to select the &quot;best&quot; window. This allows for user bias in selection of an optimal window. One thing that would be useful is if the window size selected could be compared to the expected cell diameter, which could then be reported alongside final results.</p><p>4) Page 23, line 671. For registering cells across multiple recording sessions, the authors &quot;discard all matches that result in conflict&quot;. Are users notified of number of discarded matches and is there a way to discard one match (eg cell A to B) but not another (Cell B to C)?</p><p>5) Page 26, like 750. The authors use Minian to process and analyze calcium imaging data and ezTrack to extract animal location. The two are time-locked using timestamps and further analyses are performed to correlate imaging and behavioral data. Do the authors synchronize their imaging and behavioral data using additional software? If so, can they provide information onto how best to synchronize and analyze such data?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Context:</p><p>Brain imaging techniques using miniaturized microscopes have enabled deep brain imaging in freely behaving animals. However, very large background fluctuations and high spatial overlaps intrinsic to this recording modality complicate the goal of efficiently extracting accurate estimates of single-neuronal activity. The CNMF-E algorithm uses a refined background model to address this challenge and has been implemented in the CaImAn analysis pipeline. It yields accurate results, but is demanding both in terms of computing and memory requirements. Later an online adaptation of the CNMF-E algorithm, OnACID-E, has been presented which dramatically reduces its memory and computation requirements.</p><p>Strengths:</p><p>Here the authors use a simpler background model that has been suggested by the authors of the MIN1PIPE pipeline. After the background has been (approximately) removed the data can be processed using the standard CNMF algorithm. The latter requires some user specified parameters and the interactive visualizations provided by Minian greatly simplify the setting of these parameters. Thus Minian's learning curve is less steep than that of other pipelines. Using a less refined background model and libraries that support parallel and out-of-core computation, Minian requires less memory than the batch CNMF-E algorithm.</p><p>Weakness:</p><p>While Minian's memory requirements are low compared to the batch CNMF-E algorithm, its online adaptation OnACID-E is even less demanding. The authors base their insights mainly on visual inspection, and fail to thoroughly validate the accuracy of their extracted temporal traces using simulated ground truth data as well as various real datasets as was done in the original CNMF-E (as well as the MIN1PIPE) paper. Own analysis of simulated ground truth data indicates that Minian uses less memory but also performs less well than CaImAn's CNMF-E implementation with regard to computational processing time and accuracy of extracted deconvolved calcium traces.</p><p>Upshot:</p><p>Minian is a user-friendly single-photon calcium imaging analysis pipeline. It's interactive visualizations facilitate accessibility and provide an easy means to set key parameters. In practice, Minian's slower computational processing time compared to other pipelines is likely offset by less user time spent on parameter specification. I expect other pipelines will take up the idea put forward by Minian to use interactive visualizations for the latter.</p><p>You primarily focus on interactive visualizations. I had no trouble installing Minian and thoroughly enjoyed the visualizations afforded by xarray, dask, bokeh and holoviews.</p><p>Basing your pipeline on established algorithms makes one hopeful to get accurate results, but since you are not just providing a GUI for a well validated pipeline but taking inspiration from CaImAn, MIN1PIPE and your own previous work, that's not guaranteed. In light of this, I don't find the Results section convincing.</p><p>Showing only averaged results and comparing to a null distribution leaves the reader with the impression that the pipeline performs better than random, which is not that impressive. Your pipeline actually does quite well and deserves better promotion.</p><p>You need to rework the Results section and follow along the lines of the CNMF-E and MIN1PIPE paper:</p><p>* compare to simulated ground truth data</p><p>- false positives/false negatives</p><p>- most importantly, how accurate are the extracted deconvolved traces that would be used in the downstream analysis?</p><p>* compare to multiple real datasets</p><p>- false positives/ false negatives using manual annotation as (imperfect) 'ground truth'</p><p>- show individual traces</p><p>Because one of your sale pitches is Minian's low memory demand, you need to include a figure illustrating the scaling of processing time and memory with dataset size (FOV, #frames, #neurons), cf. Figure 8 of the CaImAn paper.</p><p>A good starting point would be the simulated data of the CNMF-E paper. I used that data to gain some experience with Minian as a potential future user, and to compare it to CaImAn, with which I already have some familiarity.</p><p>After parameter tweaking both yielded a perfect F1 score of 1. However, the denoised/deconvolved traces obtained with CaImAn had an average correlation of 0.9970/0.9896 with ground truth, whereas Minian only yielded average correlations of 0.9669/0.9304.</p><p>The processing time on my Desktop/MacBookPro was 46.6s/169.1s for CaImAn and 140.9s/359.8s for Minian (using 12/4 workers, 2 threads). The peak memory was 31.698GB/4.508GB for CaImAn and 11.382GB/5.534GB for Minian.</p><p>Thus, at least in my hands, CaImAn outperforms Minian, presumably due to the more refined background model, and I'll likely stick with the former. Though I grant you that parameter selection is easier in Minian. I wished there was a pipeline with the best of both.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Minian An Open-source Miniscope Analysis Pipeline&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Laura Colgin (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Please address the comments of the reviewers below. In particular, the issue of reproducibility (i.e., missing data source files) is important, as is the concern of reproducing Figure 18.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Most concerns were addressed, but a couple of clarifications are still needed. First, with regard to boundary conditions for chunks, based on our understanding of your response, the text should be updated to clarify that neither spatial nor temporal chunking can be used for the core CNMF computation (as it computes across both space and time). How does this limitation constrain performance?</p><p>Second, with regard to chunking, there is a long description of motion correction. It seems that the initial chunk has a minimum size of 3, which will be aligned, then 9 frames will be aligned, and so on in powers of three (27, 243, etc). Is this correct? Perhaps some numerical example would be helpful for this text description.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The revisions adequately addressed all of my concerns.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The authors did a good job addressing the vast majority of my concerns and improving the paper.</p><p>My main concern is lacking reproducibility, contrary to the claims in the &quot;Transparent Reporting File&quot;:</p><p>While the repo for figure 1-14 and figure 20 can be found at</p><p>https://github.com/denisecailab/Minian_paper_data, the data source files that are supposed to be hosted as zip files on a google drive are missing.</p><p>The repo for figure 15-19 https://github.com/denisecailab/Minian-validation does not exist, or is not public.</p><p>I have not been able to reproduce Figure 18.</p><p>I repeated (6x) and concatenated the 2000frames of your demo pipeline, and processed them using 4 parallel processes. (The only parameters in Caiman's demo pipeline adjusted were gSig=(5,5); gSiz=(21,21); min_corr=.75; min_pnr=7; yielding ~280 cells). The results should thus be in the same ballpark as Figure 18, 300 cells, 12k frames.</p><p>However, the runtime was similarly about 25 min whether using Minian or Caiman, and whether running on a Linux Desktop or MacBookPro.</p><p>The reported memory seems about right for my Desktop, however, my MacBookPro processed the data using Caiman and merely 10GB of RAM, the same amount of memory as Minian.</p><p>Your demo pipeline doesn't run on my MacBookPro (with M1 chip), it fails in update_background step, and I had to increase the memory limit for each worker from 2 to 2.5GB.</p><p>I have the following remaining issues:</p><p>line 542: You need to subtract the neural activity AC before projecting, f = b'(Y-AC).</p><p>line 635: Is the 'input movie data' the data after subtracting the contributions of all the other neurons and background bf? Otherwise projecting the input data on the cell doesn't properly demix them if other components overlap. The least squares solution of min_c ||y-Ac|| is (A'A)^{-1}A'y, not A'y.</p><p>Figure 15: Why not put error bars for the correlations like you do in Figure 16? If the error bars are smaller than the symbols, say so in the paper. (E.g. standard error of the median via bootstrap)</p><p>Figure 16: Why does the correlation saturate at ~0.8? Shouldn't it approach -&gt;1 for noise-&gt;0?</p><p>line 851: Is the data described in a previous publication you missed to cite? Frames, FOV, rate?</p><p>line 895: &quot;despite the fact…&quot;. Why despite? This is expected behavior when the number of parallel processes is set to be constant, without putting a limit on memory.</p><p>line 933: Is the data described in [35]? I.e. 320x320@20Hz.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70661.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>If Minian's major contribution is usability and accessibility (lines 134-135) it is not clear why a new package had to be created rather than extending CaImAn or MIN1PIPE. The former does have out-of-core support, where memory usage can be reduced by limiting the number of parallel processes. A table detailing the methods/algorithms used by each of these packages may be useful for comparison purposes.</p></disp-quote><p>While our initial intention was to standardize input/output of CaImAn and visualize how changing some of the parameters affected the output of the pipeline, we realized an increasing need to re-implement functions to improve efficiency and integration with visualizations which evolved into a separate pipeline, Minian. With that said, Minian is greatly inspired by CaImAn and MIN1PIPE as we highlighted in the manuscript.</p><p>As the reviewer noted, the memory usage of CaImAn is related to the number of parallel processes and limiting this number will limit the performance gain from multi-processor CPUs. Moreover, the memory usage by each process still scales linearly with recording length. In contrast, due to the flexible chunking implementation of Minian, the memory usage can be limited to as low as 2GB per process and stays constant across recording length. We have included a detailed comparison of memory usage between the two pipelines in the Results section. We have also added a table in supplemental information detailing the different algorithm implementation across pipelines (Table 3).</p><p>The new comparison of memory usage starts at page 32 line 895 in the manuscript:</p><p>“As shown in Figure 18, the run time of both Minian and CaImAn scales linearly as a function of input recording length, with Minian consistently achieving shorter running times. On the other hand, the peak memory usage of CaImAn scales linearly with recording length, despite the fact that the number of parallel processes was set to be constant. At the same time, the peak memory usage of Minian stays mostly constant across increasing number of frames. This is likely due to the flexible chunking implementation of Minian (See Parallel and out-of-core computation with dask), where Minian was able to break down computations into chunks in both the spatial and the temporal dimensions depending on which way is more efficient. In contrast, CaImAn only splits data into different spatial chunks (patches), resulting in a linear scaling of memory usage with recording length for each chunk-wise computation.”</p><disp-quote content-type="editor-comment"><p>Lines 113-115: The text indicates that data may still be split into patches of pixels, so it is unclear how merging based on overlapping parts can be avoided. Aren't the overlaps necessary because putative neurons may span patch boundaries? Perhaps the authors could also compare performance of temporal splitting and non-splitting data processing. Will temporal splitting lead to comparable performance?</p></disp-quote><p>We have updated the text regarding the chunking algorithm to make it clear that both spatial and temporal splitting happens in the pipeline. In order to avoid the boundary problem posed by the reviewer, each step in the pipeline performs either spatial or temporal splitting independently. For example, if a smoothing filter is applied to each frame of the data, then temporal splitting is performed. In contrast, if pairwise pixel correlation is calculated, then spatial splitting is performed.</p><p>The relevant updated text starts at page 37 line 1012 in the manuscript:</p><p>“In Minian, we use a modern parallel computing library called dask to implement parallel and out-of-core computation. Dask divides the data into small chunks along all dimensions, then flexibly merges the data along some dimensions in each step. We leverage the fact that each step in our pipeline can be carried out chunk by chunk independently along either the temporal (frame) dimension or the spatial (height and width) dimensions, thus requiring no interpolation or special handling of borders when merged together, producing results as if no chunking had been done. For example, motion correction and most pre-processing steps that involve frame-wise filtering can be carried out on independent temporal chunks, whereas the spatial update step in CNMF algorithm and computation of pixel correlations can be carried out on independent spatial chunks. Consequently, our pipeline fully supports out-of-core computation, and memory demand is dramatically reduced. In practice, a modern laptop can easily handle the analysis of a full experiment with a typical recording length of up to 20 minutes. Dask also enables us to carry out lazy evaluation of many steps where the computation is postponed until the result is needed, for example, when a plot of the result is requested. This enables selective evaluation of operations only on the subset of data that will become part of the visualization and thus helps users to quickly explore a large space of parameters without committing to the full operation each time.”</p><disp-quote content-type="editor-comment"><p>Lines 117-119. Had to look at documentation website to figure out how to limit memory usage--put this in paper. Include some graphs to show the tradeoff between memory usage and computational time.</p></disp-quote><p>In order to further clarify how to use and implement Minian, we have now included a “Setting up” section in the main text. This includes the process of configuring the computation cluster, including the number of parallel processes and memory limit. We have also included a new graph of tradeoff between memory usage and computation time as the reviewer suggested (Figure 19).</p><p>The new “Setting up” section starts at page 5 line 187 in the manuscript:</p><p>“The first section in the pipeline includes house-keeping scripts to import packages and functions, defining parameters, and setting up parallel computation and visualization. Most notably, the distributed cluster that carries out all computations in Minian are set up in this section. By default, the cluster runs locally with multi-core CPUs, however it can be easily scaled up to run on distributed computers. The computation in Minian is optimized such that in most cases the memory demand for each process/core can be as low as 2GB. This allows the total memory usage of Minian to roughly scale linearly with the number of parallel processes. The number of parallel processes and memory usage of Minian are completely limited and managed by the cluster configuration allowing users to easily change them to suit their needs.”</p><p>The discussion on computation tradeoff starts at page 32 line 896 in the manuscript:</p><p>“Additionally, we run Minian and CaImAn with different number of parallel processes on the simulated dataset with 28000 frames and 500 cells. As expected, with more parallel processes the performance improves and the run time decreases but at the same time the total peak memory usage increases. The tradeoff between run time and peak memory usage are shown in Figure 19. In conclusion, these results show that in practice, Minian is able to perform as fast as CaImAn, while maintaining near constant memory usage regardless of input data size.”</p><disp-quote content-type="editor-comment"><p>Please comment on motion correction when there is a rotation in the FOV?</p></disp-quote><p>In practice, we have found that the motion of the brain is mostly translational, which is corrected by the motion correction algorithms currently implemented. We have updated the text describing motion correction and made it explicit that currently no rotational correction is implemented.</p><p>The updated description for motion correction starts at page 12 line 324 in the manuscript:</p><p>“We use a standard template-matching algorithm based on cross-correlation to estimate and correct for translational shifts. […] After the estimation of shifts, the shift in each direction is plotted across time and visualization of the data before and after motion correction is displayed in Minian (see Figure ??, top right).”</p><disp-quote content-type="editor-comment"><p>For motion correction, provide additional information about how the template is generated (e.g. how many frames used) and any user-defined parameters.</p></disp-quote><p>We have updated the text describing the divide-and-conquer implementation of motion correction, as well as how templates are generated for each chunk and how they are used to estimate motion (described in the above updated motion correction section).</p><disp-quote content-type="editor-comment"><p>Lines 205-206: Will the same mask be applied to all frames before motion correction? Or the mask will be applied to this certain frame and then be applied to all frames after motion correction?</p></disp-quote><p>The visualization provided by Minian only records an arbitrary mask drawn by the user, and it’s up to the user to decide when and how to use that mask to apply to a subset of the data. With that, the most common usage of this function (which is included in the default pipeline) is to use the mask to sample a part of the FOV and use the smaller FOV for estimation of motion only. The motion correction will then be carried out on the full data without masking.</p><disp-quote content-type="editor-comment"><p>Line 499: write the function used for optimization.</p></disp-quote><p>We have now included a formal definition of the optimization problem in the main text.</p><disp-quote content-type="editor-comment"><p>Paragraph starting at line 576 would benefit from some equations for clarity's sake.</p></disp-quote><p>We have now included a formal definition of the optimization problem in the main text.</p><disp-quote content-type="editor-comment"><p>Implement manual merging as in original CNMF-E paper?</p></disp-quote><p>As suggested, we have detailed an additional visualization at the end of the CNMF steps allowing for manual merging. The new visualization shows the spatial footprints and temporal activities of cells alongside the raw data. This helps the user easily determine the quality of the output cells, and the user can choose to discard or merge cells within the visualization tool.</p><p>The description of the new visualization and manual curation tool starts at page 24 line 710 in the manuscript:</p><p>“Minian provides an interactive visualization to help the users manually inspect the quality of putative cells and potentially merge or drop cells. […] In this way, only the new label is saved and no data is modified, allowing the user to repeat or correct the manual curation process if needed.”</p><disp-quote content-type="editor-comment"><p>Line 699: define preprocessed with respect to Figure 1--this is post motion-correction presumably?</p></disp-quote><p>Yes, the preprocessed video refer to the data after motion-correction. We have now included clarification in the main text.</p><disp-quote content-type="editor-comment"><p>Line 725, For Figure 14, what is the fps for the data taken? Only frame number was shown in the right side column, if the fps is 20, then 150 frames correspond to about 10 sec of calcium transient, which is on the long side for a healthy neuron.</p></disp-quote><p>The data was acquired with 30 fps and we have now clarified that in figure caption.</p><disp-quote content-type="editor-comment"><p>Line 826-828: Is dimension here referring to x and y dimensions of the frame? Please elaborate how each slice on one dimension can be processed independently.</p></disp-quote><p>We have now updated the section describing the flexible chunking implementation and out-of-core computation support, to clarify that the dimension to be sliced would be chosen for each step to make sure chunks on that dimension can be processed independently (this is included in the updated description of the chunking algorithm above).</p><disp-quote content-type="editor-comment"><p>Discuss robustness to long data (where CaImAn seems to fail). Using Minian, was CNMF performed on each msCam video individually? If so, what happens when a calcium event occurs near the video boundaries?</p></disp-quote><p>In Minian, the individual msCam videos are concatenated from the beginning. However, due to the flexible chunking implementation in Minian, we can choose the dimension to split up for each step. As a result, we can avoid boundary problems suggested by the reviewer. For example, we can split in the spatial dimensions when temporal updates are performed which leads to dramatically reduced memory demand and robustness to longer recordings. We have included a new benchmark figure in the Results section demonstrating this (Figure 18).</p><disp-quote content-type="editor-comment"><p>Line 879: What would be an example number that you would consider as &quot;significantly high (see below) stability&quot;? It is not clear what &quot;see below&quot; was referring to in terms of high stability. Example provided for place field size criterion only.</p></disp-quote><p>We have updated the section describing the place cell criteria moving the shuffling process description to the beginning of the paragraph, making it clear that “significantly high” was defined as 95th percentile of the null distribution of the specific metric.</p><p>The updated description starts at page 39 line 1095 in the manuscript:</p><p>“We use the spatially-binned averaged ‘firing’ rate calculated from spike signals to classify whether each cell is a place cell. A place cell must simultaneously satisfy three criteria: a spatial information criterion, a stability criterion, and a place field size criterion. To determine whether a cell has significant spatial information or stability, we obtain a null distribution of the measurements (spatial information and stability) with a bootstrap strategy, where we roll the timing of activity by a random amount for each cell 1000 times. The observed spatial information or stability is defined as significant if it exceeds the 95th percentile of its null distribution (p &lt; 0.05).”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1) Page 5, line 100. In regards to visualization of parameter changes and resulting outcomes using Minian, the authors state that &quot;This feature provides users with knowledge about which parameter values should be used to achieve the best outcomes and whether the results appear accurate.&quot; What do the authors mean by &quot;best outcomes&quot; and &quot;appear accurate&quot;?</p></disp-quote><p>Indeed, the language could be misleading as it is hard to objectively define “best outcomes”. We have updated the manuscript to make it clear that it is up to the user to determine the outcome that fits best with their expectation.</p><disp-quote content-type="editor-comment"><p>Is there a way the authors can suggest to rigorously test for data accuracy rather than looking to see if data &quot;appear accurate&quot;?</p></disp-quote><p>In the revised manuscript we have included extensive validation of Minian with simulated datasets, along with comparisons with the existing pipeline, CaImAn. The accuracy remains high and indistinguishable from CaImAn as long as proper parameters are chosen. This suggests that Minian can perform accurately.</p><p>The relevant text from the new validation section starts at page 27 line 803 in the manuscript:</p><p>&quot;To compare the results objectively, we first matched the resulting putative cells from the output of Minian or CaImAn to the simulated ground truth (See Matching neurons for validation for details). [...] This is likely due to noise and background contaminating the true signal. Overall, these results show that the Minian and CaImAn pipelines perform similarly well in terms of output accuracy on simulated datasets.”</p><disp-quote content-type="editor-comment"><p>Do the authors plan to provide any guidance for standard parameters/variables to use for brain regions and/or cell types?</p></disp-quote><p>Minian provides a pipeline with default parameters that works best on the datasets we have tested, which consist mainly of single-photon hippocampal CA1 recordings. Additionally, we provide detailed explanation and visualization for each step which equips the users with knowledge to explore and adjust the parameters for datasets obtained across different brain regions/cell types. However, since there are many variables that determine the features of the data and the optimal parameters, it is hard to provide a pre-determined set of parameters that works universally on certain brain regions/cell types.</p><disp-quote content-type="editor-comment"><p>These issues are somewhat highlighted in the limitations section at the end, but I think there could be some more effort put into providing more objectivity to this approach.</p></disp-quote><p>In order to provide objectivity to the Minian pipeline, we have included an updated validation section, which includes results from both simulated data and real experimental data. The outputs of Minian are validated with respect to ground truth and existing pipelines. Our results from both simulated and real datasets suggest that Minian performs as well as CaImAn.</p><p>In addition to the validation with simulated data presented above, we also included validation on real datasets with respect to existing pipeline CaImAn, which starts at page 30 line 854 in the manuscript:</p><p>“We next validated Minian with experimental datasets. The data was collected from hippocampal CA1 regions in animals performing a spatial navigation task. 6 animals with different density of cells were included in the validation dataset. […] Overall, these results suggest that the output of Minian is highly similar to CaImAn when analyzing experimental datasets.”</p><disp-quote content-type="editor-comment"><p>2) Page 9, line 240. As part of their &quot;denoising&quot; step, the authors pass their frames through a median filter. A number of filters can be used for denoising (e.g. Gaussian). The authors should justify why this filter is used and potentially compare it to other filters.</p></disp-quote><p>While Minian implements other filters for denoising purpose, including Gaussian, bilateral, and anisotropic filters seen in MIN1PIPE, we found the median filter works best for the “salt and pepper” type of noise introduced by CMOS sensors. Hence we choose median filter as the default for the pipeline and leave the other filters available as options.</p><disp-quote content-type="editor-comment"><p>3) Page 9 describes setting the window size to avoid &quot;salt and pepper&quot; noise on the one end, and &quot;over-smoothing&quot; on the other. Minian allows for the visualization of distinct settings of the window size, however, it is left up to the user to select the &quot;best&quot; window. This allows for user bias in selection of an optimal window. One thing that would be useful is if the window size selected could be compared to the expected cell diameter, which could then be reported alongside final results.</p></disp-quote><p>While Minian allows the user to choose the “best” window, it is indeed more helpful to suggest optimal parameters based on features of the data whenever possible. We have now included the suggestion to set the window size of the median filter to the expected cell diameter.</p><disp-quote content-type="editor-comment"><p>4) Page 23, line 671. For registering cells across multiple recording sessions, the authors &quot;discard all matches that result in conflict&quot;. Are users notified of number of discarded matches and is there a way to discard one match (eg cell A to B) but not another (Cell B to C)?</p></disp-quote><p>Currently there is no way to discard one match over the other, since there is no clear intuitive way to handle the conflicts and allowing such manual correction would likely induce user bias. That said, users can implement other matching criteria to best fit their requirements.</p><disp-quote content-type="editor-comment"><p>5) Page 26, like 750. The authors use Minian to process and analyze calcium imaging data and ezTrack to extract animal location. The two are time-locked using timestamps and further analyses are performed to correlate imaging and behavioral data. Do the authors synchronize their imaging and behavioral data using additional software? If so, can they provide information onto how best to synchronize and analyze such data?</p></disp-quote><p>The open-source Miniscope data acquisition software synchronizes the start and end time of the calcium video and behavioral video. The time stamps from each frame collected from the Miniscope and behavioral cameras are then synched so each calcium imaging frame corresponds to the correct behavioral video frame.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>You primarily focus on interactive visualizations. I had no trouble installing Minian and thoroughly enjoyed the visualizations afforded by xarray, dask, bokeh and holoviews.</p><p>Basing your pipeline on established algorithms makes one hopeful to get accurate results, but since you are not just providing a GUI for a well validated pipeline but taking inspiration from CaImAn, MIN1PIPE and your own previous work, that's not guaranteed. In light of this, I don't find the Results section convincing.</p><p>Showing only averaged results and comparing to a null distribution leaves the reader with the impression that the pipeline performs better than random, which is not that impressive. Your pipeline actually does quite well and deserves better promotion.</p><p>You need to rework the Results section and follow along the lines of the CNMF-E and MIN1PIPE paper:</p><p>* compare to simulated ground truth data</p><p>- false positives/false negatives</p><p>- most importantly, how accurate are the extracted deconvolved traces that would be used in the downstream analysis?</p><p>* compare to multiple real datasets</p><p>- false positives/ false negatives using manual annotation as (imperfect) 'ground truth'</p><p>- show individual traces</p></disp-quote><p>We thank the reviewer for their thoughtful and detailed comments. We have updated our manuscript to include validation with both simulated and real datasets detailed in the Results section.</p><p>The validation with simulated data starts at page 27 line 794 in the manuscript:</p><p>“We first validated Minian with simulated datasets. We synthesized different datasets with varying number of cells and signal levels based on existing works. […] This is likely due to noise and background contaminating the true signal. Overall, these results show that the Minian and CaImAn pipelines perform similarly well in terms of output accuracy on simulated datasets.”</p><p>In particular, the validation of extracted deconvolved traces starts at page 28 line 828 in the manuscript:</p><p>“Additionally, we want to validate the deconvolved signal from Minian output, since this is usually the most important output for downstream analysis. […] Overall, these results suggest that Minian can produce deconvolved signals that are faithful to ground truth and suitable for downstream analysis.”</p><p>The validation with real data starts at page 30 line 854 in the manuscript:</p><p>“We next validated Minian with experimental datasets. The data was collected from hippocampal CA1 regions in animals performing a spatial navigation task. 6 animals with different density of cells were included in the validation dataset. […] Overall, these results suggest that the output of Minian is highly similar to CaImAn when analyzing experimental datasets.”</p><p>Additionally, we have performed validation on real datasets with manual annotations as the reviewer suggested. We collected manual annotations from two independent labelers who were instructed to find circular-shaped neurons with a characteristic calcium decay dynamic. The labelers were provided with a minimally pre-processed video, with the bright vignetting artifact and motion of the brain corrected. The labelers used Fiji (ImageJ) to visualize the pre-processed video and the free-hand drawing tool to produce ROIs. This annotation procedure provides a binary mask for each putative neuron. To generate consensus ground truth, we match the binary ROIs across the two labelers using the same procedure used for matching neurons for validation, and consider only the matched ROIs as ground truth neurons. To obtain the spatial footprints for each ground truth neuron, we take the mean projection across the binary masks from the two labelers. Hence the spatial footprints generated in this way can only take values 0, 0.5 or 1. To obtain the temporal dynamic for each ground truth neuron, we project the pre-processed video onto the spatial footprints by calculating a mean activity of each pixel weighted by each spatial footprints.</p><p>The result is summarized in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>. Our results indicate that both pipelines capture most of the consensus footprints. However, inconsistencies exist across the two pipelines, and even across human labelers, shown by a relatively low F1 score among the two human labelers. We recognize that this is probably due to the complex nature of single-photon recordings, and in a lot of cases it is hard to determine whether an ROI is a real cell even for experienced human labelers. Furthermore, since we are benchmarking with two different pipelines, we are unable to generate imperfect ground truth by seeding the CNMF algorithm with manual labels, since the choice of CNMF implementation will likely bias the resulting correlations. Instead, we have to use more naive method to generate ‘ground truth’, namely taking the mean of manual labeled spatial footprints and projecting the raw data onto spatial footprints to use as temporal ground truth. However, such approach has significant drawback: First, the ‘ground truth’ spatial footprints can only take 3 discrete values, and their correlation with real-valued spatial footprints from analysis pipelines would be hard to interpret. Secondly, the ‘ground truth’ temporal activity contains noise and is contaminated with cross-talk from nearby cells. A perfect correlation with such ‘ground truth’ would actually indicate that the analysis pipeline failed to perform denoise and demixing of calcium activities. Due to these drawbacks, we decided to only present these results as response to reviewer comments but did not include them in the main text, as we believe these results might be hard to interpret and would be potentially misleading to general readers.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Validation of Minian with experimental datasets.</title><p>An example field of view from one of the experimental datasets is shown in panel A, with contours of detected ROIs overlaid on top. ROIs labeled by human labelers are shown in green, where solid lines indicate ROIs labeled by both labelers (labeled as “Manual-Consensus”) and dashed lines indicate ROIs labeled by one of the labelers (labeled as “Manual-Mismatch”). The F1 scores, spatial footprints correlation and temporal dynamics correlation are plotted for the two pipelines in panel B. The F1 scores of the two human labelers are also included in the plot for comparison (labeled as “Manual”). The F1 scores, spatial footprints correlation and temporal dynamics correlation were all not significantly different across the two pipelines (One-way ANOVA, p &gt; 0.05).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-sa2-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Because one of your sale pitches is Minian's low memory demand, you need to include a figure illustrating the scaling of processing time and memory with dataset size (FOV, #frames, #neurons), cf. Figure 8 of the CaImAn paper.</p><p>A good starting point would be the simulated data of the CNMF-E paper. I used that data to gain some experience with Minian as a potential future user, and to compare it to CaImAn, with which I already have some familiarity.</p><p>After parameter tweaking both yielded a perfect F1 score of 1. However, the denoised/deconvolved traces obtained with CaImAn had an average correlation of 0.9970/0.9896 with ground truth, whereas Minian only yielded average correlations of 0.9669/0.9304.</p><p>The processing time on my Desktop/MacBookPro was 46.6s/169.1s for CaImAn and 140.9s/359.8s for Minian (using 12/4 workers, 2 threads). The peak memory was 31.698GB/4.508GB for CaImAn and 11.382GB/5.534GB for Minian.</p><p>Thus, at least in my hands, CaImAn outperforms Minian, presumably due to the more refined background model, and I'll likely stick with the former. Though I grant you that parameter selection is easier in Minian. I wished there was a pipeline with the best of both.</p></disp-quote><p>We followed the suggested path for benchmarking Minian performance and have include a new benchmarking figure in the Results section. We found that processing time for both Minian and CaImAn scales linearly as a function of recording length, but Minian has a near constant peak memory usage across different recording length, while peak memory usage for CaImAn scales linearly to recording length.</p><p>The relevant text in the new benchmarking section starts at page 32 line 885 in the manuscript:</p><p>“To see how the performance of Minian scales with different input data size, we synthesized datasets with varying number of cells and number of frames (recording length). […]This allows the users to process much longer recordings with limited RAM resources.”</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Please address the comments of the reviewers below. In particular, the issue of reproducibility (i.e., missing data source files) is important, as is the concern of reproducing Figure 18.</p></disp-quote><p>We thank the reviewer’s for their in-depth comments and validation. We addressed the issue of reproducibility in this revision. Specifically, we have used google drive as the data storage service in our previous submission, and we were not aware that it requires a google account to access even when we made it public. Hence, we have now switched to a dedicated data sharing service Figshare and made sure the code (github repos) as well as raw data are publicly available without credentials. We have also included convenient scripts on github to automatically download all the necessary data and produce the correct folder structure, making our results more reproducible. Additionally, we were also able to reproduce Reviewer 3’s results, and we addressed them in the specific responses below. In response to reviewer’s comments, we have included additional parameters/conditions in our benchmarking and validation experiments with simulated data (i.e., Figure 15, 16, 18, 19).</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Most concerns were addressed, but a couple of clarifications are still needed. First, with regard to boundary conditions for chunks, based on our understanding of your response, the text should be updated to clarify that neither spatial nor temporal chunking can be used for the core CNMF computation (as it computes across both space and time). How does this limitation constrain performance?</p></disp-quote><p>We apologize for the confusion. Although the core CNMF involves computations across both space and time, the spatial and temporal update steps are carried out as separate steps, and either spatial or temporal chunking can be independently applied to each step. We have updated the description in the text to clarify this point (starting on line 1033):</p><p>“For example, motion correction and most pre-processing steps that involve frame-wise filtering can be carried out on independent temporal chunks, whereas computation of pixel correlations can be carried out on independent spatial chunks. Similarly, during the core CNMF computation steps, spatial chunking can be used during update of spatial footprints, since spatial update is carried out pixel by pixel. Meanwhile, temporal chunking can be used when projecting the input data onto spatial footprints of cells, which is usually the most memory-demanding step. Although the optimization step during the temporal update is computed across all frames and no temporal chunking can be used, we can still chunk across cells, and in practice the memory demand in this step is much smaller comparing to other steps involving raw input data. Consequently, our pipeline fully supports out-of-core computation, and memory demand is dramatically reduced.”</p><disp-quote content-type="editor-comment"><p>Second, with regard to chunking, there is a long description of motion correction. It seems that the initial chunk has a minimum size of 3, which will be aligned, then 9 frames will be aligned, and so on in powers of three (27, 243, etc). Is this correct? Perhaps some numerical example would be helpful for this text description.</p></disp-quote><p>This is correct and we thank the reviewer for the suggestion. We have updated the relevant description to include a numerical example (starting on line 337):</p><p>“After the registration, the 3 chunks that have been registered are treated as a new single chunk and we again take the max projection to use as a template for further registration. In this way, the number of frames registered in each chunk keeps increasing in powers of three (3, 9, 27, 81 etc.), and we repeat this process recursively until all the frames are covered in a single chunk and the whole movie is registered. Since the motion correction is usually carried out after background removal, we essentially use cellular activity as landmarks for registration.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The authors did a good job addressing the vast majority of my concerns and improving the paper.</p><p>My main concern is lacking reproducibility, contrary to the claims in the &quot;Transparent Reporting File&quot;:</p><p>While the repo for figure 1-14 and figure 20 can be found at</p><p>https://github.com/denisecailab/Minian_paper_data, the data source files that are supposed to be hosted as zip files on a google drive are missing.</p><p>The repo for figure 15-19 https://github.com/denisecailab/Minian-validation does not exist, or is not public.</p></disp-quote><p>We apologize for this error. Along with this resubmission, we have made sure both repos are available publicly, and all the raw data can be downloaded online without any login credentials.</p><disp-quote content-type="editor-comment"><p>I have not been able to reproduce Figure 18.</p><p>I repeated (6x) and concatenated the 2000frames of your demo pipeline, and processed them using 4 parallel processes. (The only parameters in Caiman's demo pipeline adjusted were gSig=(5,5); gSiz=(21,21); min_corr=.75; min_pnr=7; yielding ~280 cells). The results should thus be in the same ballpark as Figure 18, 300 cells, 12k frames.</p><p>However, the runtime was similarly about 25 min whether using Minian or Caiman, and whether running on a Linux Desktop or MacBookPro.</p></disp-quote><p>We thank the reviewer for the careful validation of our results. Upon further investigation, we realized that visualization, especially the generation of two validation videos, takes a significant amount of processing time in Minian pipeline. These steps are enabled by default in the demo pipeline, but was not included in our benchmark in previous submission. Once we include the visualization steps we were able to reproduce the running time similar to reviewer’s results (~25min for 12K frames for both pipelines). We have now updated Figure 18 to include results with and without visualizations for both pipelines (There is only one major visualization in CaImAn hence the results are very similar within CaImAn). The relevant parts in the main text are updated as follows (starting on line 907):</p><p>“As shown in Figure <italic>18</italic>, the run time of both Minian and CaImAn scales linearly as a function of input recording length. The exact running times vary depending on number of cells as well as whether visualization is included in the processing, but in general the running time is similar across both pipelines.”</p><disp-quote content-type="editor-comment"><p>The reported memory seems about right for my Desktop, however, my MacBookPro processed the data using Caiman and merely 10GB of RAM, the same amount of memory as Minian.</p></disp-quote><p>Indeed, we are able to reproduce the low memory footprint of CaImAn on a MacBookPro as well. We believe this is due to the fact that OS X system uses all available space on the machine’s boot partition as swap/virtual memory. Hence, if we only observe usage of physical memory, it appears CaImAn (or any program) would only use as little as the amount of free physical memory, while in fact swap memory would be used to account for any additional memory demand. We have modified our benchmarking code to track the usage of swap memory as well and plot them together with physical memory usage as in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>. Our results suggest that even in OS X, although the physical memory usage is similar across pipelines, the swap memory usage for CaImAn scales linearly as the size of input data increases, while the swap memory usage of Minian stays constantly close to 0. Although this result implies that a user can potentially run analysis on very large datasets on a MacBookPro without Minian, the user still needs special configuration of the operating system if analysis is not performed on a Mac hardware. In general, regardless how an operating system handles memory allocation, Minian still has a much smaller memory demand comparing to existing pipelines.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Benchmarking of computational performance in OS X.</title><p>Data with varying number of cells and frames were processed through Minian and CaImAn without visualization. The run time (top) and peak physical memory usage (middle) and peak swap memory usage (bottom) were recorded and plotted as a function of frame number.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70661-sa2-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Your demo pipeline doesn't run on my MacBookPro (with M1 chip), it fails in update_background step, and I had to increase the memory limit for each worker from 2 to 2.5GB.</p></disp-quote><p>While we don’t have a MacBookPro with M1 chip to test, we expect that sometimes the algorithms would require more than 2GB of memory per worker to finish depending on the exact hardware/software configuration, and it is common for Minian users to get around the memory error by increasing the memory limit a bit. We have updated the “Setting up” section in the main text to include this information (starting on line 191):</p><p>“The computation in Minian is optimized such that in most cases the memory demand for each process/core can be as low as 2GB. However, in some cases depending on the hardware, the state of operating system and data locality, Minian might need more than 2GB per process to run. If a memory error (KilledWorker) is encountered, it is common for users to increase the memory limit of the distributed cluster to get around the error. Regardless of the exact memory limit per process, the total memory usage of Minian roughly scales linearly with the number of parallel processes.”</p><disp-quote content-type="editor-comment"><p>I have the following remaining issues:</p><p>line 542: You need to subtract the neural activity AC before projecting, f = b'(Y-AC).</p></disp-quote><p>We have updated the texts with correct information (starting on line 546):</p><p>“After the spatial footprint of the background term is updated, we subtract the neural activity (AC) from the input data to get residual background fluctuations. Then the temporal activity of background term is calculated as the projection of residual onto the new background spatial footprint, where the raw activities of each pixel is weighted by the spatial footprint.”</p><disp-quote content-type="editor-comment"><p>line 635: Is the 'input movie data' the data after subtracting the contributions of all the other neurons and background bf? Otherwise projecting the input data on the cell doesn't properly demix them if other components overlap. The least squares solution of min_c ||y-Ac|| is (A'A)^{-1}A'y, not A'y.</p></disp-quote><p>The reviewer is correct and we did account for the activity from overlapping cells and subtract them from the projection. We agree that in theory we should use the least square solution as the target activity. However, in practice we use an empirical projection following the block-coordinate decent update rule to include sparsity regularization and achieve better computational performance. This implementation can demix activities from overlapping cells and is consistent with CaImAn. The exact equation is described as Eq. 6 in (Friedrich, Giovannucci, and Pnevmatikakis 2021). We have updated the description (starting on line 598):</p><p>“First, we subtract the background term from the input data, leaving only the noisy signal from cells. We then project the data onto the spatial footprints of cells, obtaining the temporal activity for each cell. Next we estimate a contribution of temporal activity from neighboring overlapping cells using the spatial footprints of cells, and subtract it from the temporal activity of each cell.”</p><disp-quote content-type="editor-comment"><p>Figure 15: Why not put error bars for the correlations like you do in Figure 16? If the error bars are smaller than the symbols, say so in the paper. (E.g. standard error of the median via bootstrap)</p></disp-quote><p>For Figure 15 we did not generate multiple datasets for the same condition (signal level and number of cells). The reason is that we don’t expect much variance in any of the metric for the same conditions.</p><disp-quote content-type="editor-comment"><p>Figure 16: Why does the correlation saturate at ~0.8? Shouldn't it approach -&gt;1 for noise-&gt;0?</p></disp-quote><p>We have fine-tuned some parameters and included additional data points with higher signal level. The results now indicate that the correlation asymptotes and approaches 1 when signal level is higher than 1. We have updated the text and figures (starting on line 842):</p><p>“The resulting correlation is summarized in Figure <italic>16</italic> A. Our results indicate that the deconvolved output from Minian is highly similar to ground truth spikes when signal level is high, and the correlation asymptote and approach 1 when signal level is higher than 1. The lower correlation corresponding to low signal level is likely due to the background and noise contamination being stronger than signal.”</p><disp-quote content-type="editor-comment"><p>line 851: Is the data described in a previous publication you missed to cite? Frames, FOV, rate?</p></disp-quote><p>This data has not been previously published. We apologize for the missing details. We have now added description of the data (starting on line 864):</p><p>“The data was collected from hippocampal CA1 regions in animals performing a spatial navigation task. 6 animals with different density of cells were included in the validation dataset. The recordings are collected with 608 x 608 pixels at 30 fps and lasts 20 min (~36000 frames).”</p><disp-quote content-type="editor-comment"><p>line 895: &quot;despite the fact…&quot;. Why despite? This is expected behavior when the number of parallel processes is set to be constant, without putting a limit on memory.</p></disp-quote><p>We agree it can be confusing regarding the expected memory usage. We have now reworded this sentence as the following (starting on line 910):</p><p>“On the other hand, the peak memory usage of CaImAn scales linearly with recording length when the number of parallel processes was set to be constant.”</p><disp-quote content-type="editor-comment"><p>line 933: Is the data described in [35]? I.e. 320x320@20Hz.</p></disp-quote><p>We apologize for the missing information. We have now added description of the data (starting on line 946):</p><p>“Briefly, animals were trained to run back and forth on a 2 m linear track while wearing a Miniscope to obtain water rewards available at either end. The time gap between each session was 2 days. We record calcium activity in dorsal CA1 region with a FOV of 480 x 752 pixels collected at 30 fps. Each recording session lasts 15 min (~27000 frames).”</p></body></sub-article></article>