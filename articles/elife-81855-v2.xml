<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81855</article-id><article-id pub-id-type="doi">10.7554/eLife.81855</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Memory for incidentally learned categories evolves in the post-learning interval</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-285872"><name><surname>Gabay</surname><given-names>Yafit</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7899-3044</contrib-id><email>ygabay@edu.haifa.ac.il</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-55790"><name><surname>Karni</surname><given-names>Avi</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-287851"><name><surname>Holt</surname><given-names>Lori L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8732-4977</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f009v59</institution-id><institution>Department of Special Education and the Edmond J. Safra Brain Research Center for the Study of Learning Disabilities, University of Haifa, Abba Khoushy Ave 199</institution></institution-wrap><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02f009v59</institution-id><institution>Sagol Department of Neurobiology and the Edmond, J. Safra Brain Research Center for the Study of Learning Disabilities, University of Haifa</institution></institution-wrap><addr-line><named-content content-type="city">Haifa</named-content></addr-line><country>Israel</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Department of Psychology and Neuroscience Institute, Carnegie Mellon University</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Chait</surname><given-names>Maria</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>24</day><month>03</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e81855</elocation-id><history><date date-type="received" iso-8601-date="2022-07-13"><day>13</day><month>07</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-03-23"><day>23</day><month>03</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-06-17"><day>17</day><month>06</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.31234/osf.io/a8ksm"/></event></pub-history><permissions><copyright-statement>© 2023, Gabay et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Gabay et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81855-v2.pdf"/><abstract><p>Humans generate categories from complex regularities evolving across even imperfect sensory input. Here, we examined the possibility that incidental experiences can generate lasting category knowledge. Adults practiced a simple visuomotor task not dependent on acoustic input. Novel categories of acoustically complex sounds were not necessary for task success but aligned incidentally with distinct visuomotor responses in the task. Incidental sound category learning emerged robustly when within-category sound exemplar variability was closely yoked to visuomotor task demands and was not apparent in the initial session when this coupling was less robust. Nonetheless, incidentally acquired sound category knowledge was evident in both cases one day later, indicative of offline learning gains and, nine days later, learning in both cases supported explicit category labeling of novel sounds. Thus, a relatively brief incidental experience with multi-dimensional sound patterns aligned with behaviorally relevant actions and events can generate new sound categories, immediately after the learning experience or a day later. These categories undergo consolidation into long-term memory to support robust generalization of learning, rather than simply reflecting recall of specific sound-pattern exemplars previously encountered. Humans thus forage for information to acquire and consolidate new knowledge that may incidentally support behavior, even when learning is not strictly necessary for performance.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory categorization</kwd><kwd>category learning</kwd><kwd>memory consolidation</kwd><kwd>off-line gains</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Binational Scientific Foundation</institution></institution-wrap></funding-source><award-id>2015227</award-id><principal-award-recipient><name><surname>Gabay</surname><given-names>Yafit</given-names></name><name><surname>Holt</surname><given-names>Lori L</given-names></name><name><surname>Karni</surname><given-names>Avi</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>The National Science Foundation-Binational Scientific Foundation</institution></institution-wrap></funding-source><award-id>2016867</award-id><principal-award-recipient><name><surname>Gabay</surname><given-names>Yafit</given-names></name><name><surname>Holt</surname><given-names>Lori L</given-names></name><name><surname>Karni</surname><given-names>Avi</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>The National Science Foundation-Binational Scientific Foundation</institution></institution-wrap></funding-source><award-id>NSF BCS1655126</award-id><principal-award-recipient><name><surname>Gabay</surname><given-names>Yafit</given-names></name><name><surname>Holt</surname><given-names>Lori L</given-names></name><name><surname>Karni</surname><given-names>Avi</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>The Israel Science Foundation</institution></institution-wrap></funding-source><award-id>734/22</award-id><principal-award-recipient><name><surname>Gabay</surname><given-names>Yafit</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Incidental experiences can lead to lasting category knowledge, demonstrating that humans forage for information to acquire and consolidate new knowledge even when learning is not strictly necessary for success on an ongoing task.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The sensory information that conveys everyday objects and events tends to be multidimensional and probabilistic; often no single sensory cue is sufficient for guiding behavior. For example, edible mushrooms may tend to have a typical shape, or smell, or color but none of these cues, on its own, may be entirely diagnostic of a safe meal. To generate categories, imperfect and complex regularities present in the sensory input must be extracted and learned, but this knowledge then needs to be consolidated into long-term memory to be called upon to guide behavior when encountering novel objects and events with similar properties.</p><p>Natural environments add to this challenge because learning tends to proceed across multiple simultaneously present forms of sensory input, typically with no explicit instruction or feedback as a guide to what is relevant or important. Under these conditions, individuals may be unaware that categories exist or that category decisions are called for. Yet, most studies have examined category learning under conditions in which learners overtly direct attention to explicit category decisions and receive feedback about the correctness of their decisions (<xref ref-type="bibr" rid="bib21">Holt and Lotto, 2006</xref>; <xref ref-type="bibr" rid="bib18">Goudbeek et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">Guenther et al., 1999</xref>; <xref ref-type="bibr" rid="bib20">Holt et al., 2004</xref>; <xref ref-type="bibr" rid="bib39">Mirman et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Earle and Myers, 2015</xref>; <xref ref-type="bibr" rid="bib11">Earle et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Ashby and Maddox, 2005</xref>; <xref ref-type="bibr" rid="bib35">Love, 2002</xref>; <xref ref-type="bibr" rid="bib36">Love et al., 2004</xref>).</p><p>However, there is ample evidence that category learning can proceed even under incidental conditions when the regularities underlying sensory input categories align with successful behavior on a primary task ostensibly unrelated to a need for categorization. Adult listeners are capable of generating multidimensional auditory (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Leech et al., 2009</xref>; <xref ref-type="bibr" rid="bib29">Lim and Holt, 2011</xref>; <xref ref-type="bibr" rid="bib40">Roark and Holt, 2018</xref>; <xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>; <xref ref-type="bibr" rid="bib42">Roark et al., 2022</xref>) and phonetic speech (<xref ref-type="bibr" rid="bib29">Lim and Holt, 2011</xref>; <xref ref-type="bibr" rid="bib31">Lim et al., 2015</xref>) categories when auditory categories incidentally align with successful behavior in a visuomotor task (i.e. when the task can be completed without recourse to the auditory input). Moreover, this incidentally acquired auditory category knowledge can generalize to encompass novel category exemplars and has been shown to draw on cortical and cortico-striatal networks associated with speech-related category expertise (<xref ref-type="bibr" rid="bib27">Leech et al., 2009</xref>; <xref ref-type="bibr" rid="bib33">Liu and Holt, 2011</xref>; <xref ref-type="bibr" rid="bib32">Lim et al., 2019</xref>); this supports the notion that incidental auditory category learning draws upon internal feedback associated with success in the primary task (<xref ref-type="bibr" rid="bib32">Lim et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Roark et al., 2020</xref>). Yet, although such <italic>incidental learning</italic> is well-attested (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Lim and Holt, 2011</xref>; <xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>; <xref ref-type="bibr" rid="bib31">Lim et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Vlahou et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Seitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Gabay et al., 2023</xref>), we understand very little about the consolidation, retention, and generalization of category learning under the incidental learning conditions that characterize most natural learning environments.</p><p>Here, we examine whether and how auditory categories that align, incidentally, with visuomotor task demands can lead to long-term auditory category knowledge. We capitalize upon previous studies showing that auditory categories can be generated during the performance of a simple visuomotor target detection task – the Systematic Multimodal Association Reaction Time (SMART) task (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>). In the SMART task participants practice the rapid detection of a visual target in one of four possible screen locations and report its position by pressing a key corresponding to the visual location (<xref ref-type="fig" rid="fig1">Figure 1</xref>). A brief sequence of sounds, ostensibly unrelated to the simple demands of detecting the suprathreshold visual target, precedes each visual target. Unknown to participants, the sounds are drawn from one of four distinct nonspeech sound categories (<xref ref-type="fig" rid="fig1">Figure 1a</xref>; <xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>). Thus, there is a multimodal (auditory category to visual location/response) correspondence that relates the different exemplars of a specific sound category to a consistent visual target location and response – a many-to-one mapping. The sound categories predict the visual target’s location and the action required to complete the visual detection task. Incidentally learning to treat the acoustically variable sounds as functionally equivalent (and predictive of the upcoming location of a visual target) thus may facilitate visual detection. Overt sound categorization decisions are not required and explicit feedback about category membership to direct learning is not provided. The SMART task therefore makes it possible to investigate how participants learn auditory categories incidentally during the practice of a visuomotor task.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of stimuli and paradigm.</title><p>(<bold>A</bold>) Four nonspeech auditory categories are defined across six exemplars (differentiated by the higher frequency component shown as different colors on the same axes, with a common lower-frequency component shown as a dashed grey line). Categories A and B are characterized by a unidimensional acoustic attribute (offset rises or falls) whereas Categories C and D cannot be defined by a single acoustic attribute and instead are multidimensional, with distributional structure in higher-order perception space (see 15). In the Systematic Multimodal Association Reaction Time (SMART) task each auditory category uniquely predicts the upcoming location of a visual target. Participants respond with a keypress to indicate the visual target location. (<bold>B</bold>) Each of three experiments involves three behavioral testing sessions (Day 1, Day 2, Day 10). The blocks labeled ‘train’ involve a consistent mapping from auditory category to visual target location (and visuomotor response), as shown in (<bold>A</bold>). Blocks 7, 10, and 13 destroy this relationship through randomization of sounds to locations to examine the impact on visuomotor response (as a response time cost). Examination of performance on Day 2 and Day 10 informs offline gains (response time facilitation), consolidation of incidental category learning, and its retention. A final overt labeling task on Day 10 measures generalization of incidental category learning to novel category exemplars (not plotted in Panel A) in an overt labeling task. (<bold>C</bold>) Exp 1 examines visuomotor task demands without auditory exemplars preceding the visual target to characterize putative visuomotor learning, consolidation and retention. Exp 2 examines incidental auditory category when, on each trial, a single category exemplar is repeated five times and predicts the upcoming location of the visual target; exemplar variability is experienced across, not within, trials. Exp 3 examines incidental learning when within-category variability is more tightly coupled to visuomotor task demands; five unique exemplars are sampled from a category on each trial and, as in Exp 2, the category identity predicts the location of the upcoming visual target.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81855-fig1-v2.tif"/></fig><p>We first addressed the contributions of the visuomotor task demands (task practice without sounds) to within-session, online gains in task performance and tested for putative offline (post-session) learning gains, the processes that occur in the immediate post-learning interval after training has ended, and their long-term retention. We next addressed the possibility that a post-learning consolidation phase follows incidental auditory category learning (practice that includes incidental experience with the sounds), and that category knowledge can be retained and serve the categorization of novel stimuli. The underlying motivation was to examine the potential for a theoretical bridge for considering incidental category learning in the light of accounts of task-relevant perceptual and motor skill mastery; delayed (offline) gains in task performance and their retention are considered a behavioral expression of memory consolidation processes (<xref ref-type="bibr" rid="bib24">Karni, 1996</xref>) and signature indicators of the establishment of robust and efficient long-term ‘how to’ memory representations (<xref ref-type="bibr" rid="bib24">Karni, 1996</xref>; <xref ref-type="bibr" rid="bib7">Dorfberger et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Karni and Bertini, 1997</xref>; <xref ref-type="bibr" rid="bib8">Dudai et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Seitz and Dinse, 2007</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Offline gains are evident in a simple visuomotor task</title><p>Exp 1 examined performance gains attained with practice with the visuomotor aspects of the SMART task (no acoustic stimuli; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). To this end, participants reported the location of an above-threshold visual target as quickly and accurately as possible. Response time (RT) to detect the target was stable across the first 8 blocks of training on Day-1 [<italic>F</italic> (7, 147)=0.76, p=0.61; <italic>η<sub>p</sub></italic>²=0.03; <xref ref-type="fig" rid="fig2">Figure 2A</xref>], indicating no significant task-driven online learning on Day-1. However, RT to respond to the visual target in the first block of Day-2 (Block 9, <italic>M</italic>=437.8ms, SE = 11.2ms) was significantly faster compared to the final block of Day-1 (Block 8, <italic>M</italic>=481.4ms, SE = 14.2ms), indicative of offline learning gains in visuomotor task performance [<italic>t</italic>(21)=–3.83, p=0.001, Cohen’s <italic>d</italic>=–0.81]. This facilitation in the speed of reporting the visual target did not come at a cost to accuracy (see Appendix 1) and, moreover, was robustly maintained across a 9-day interval [final block of Day-2, <italic>M</italic>=449.29ms, SE = 12.06ms, to the first block of Day-10, <italic>M</italic>=437.15ms, SE = 11.32ms; <italic>t</italic>(1,)=–1.79, p=0.08; Cohen’s <italic>d</italic>=-0.38]. This establishes a significant facilitation of RT arising from practice of the visuomotor task, per se, that must be considered in interpreting how incidental auditory category learning proceeds when the auditory stimuli are introduced in the SMART task.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visuomotor SMART task behavior: Response time (RT).</title><p>Across all panels, the leftmost graph shows the mean and standard error of the response time (RT) to respond to the visual target, with individuals' data plotted as light grey dots across blocks in Day-1, Day-2, and Day-10 sessions. The middle graph plots the RT Cost of the Random block (Blocks 7, 10, 13) as a function of the preceding block. The rightmost graph shows the offline gain from the last block of a preceding session to the first block of the next session (Day-1 to Day-2, Day2 to Day-10). (<bold>A</bold>) Exp 1 (n=22) characterizes putative visuomotor learning, consolidation and retention without sounds preceding visual targets. (<bold>B</bold>) In Exp 2 (n=24), a consistent category-to-location association is conveyed by a single category exemplar, repeated five times on a trial; different exemplars occurred on different trials. (<bold>C</bold>) In Exp 3 (n=22), the consistent category-to-location association was conveyed by five unique category exemplars sampled from the category on each trial.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81855-fig2-v2.tif"/></fig></sec><sec id="s2-2"><title>Incidental auditory-category learning and consolidation gains are dependent on alignment of within-category exemplar variability with visuomotor task demands</title><p>We next examined auditory category learning across two variations of the SMART task. Based on prior research, each was expected to lead to robust incidental category learning in a single SMART training session, although to different degrees (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>). This allowed us to examine whether less robust online learning gains achieved in a single training session may nonetheless be robustly expressed after a period of delay (Day-2, Day-10) permitting offline learning gains and consolidation (See <xref ref-type="bibr" rid="bib24">Karni, 1996</xref>, <xref ref-type="bibr" rid="bib26">Karni et al., 1998</xref>; <xref ref-type="bibr" rid="bib38">Maquet et al., 2003</xref>; <xref ref-type="bibr" rid="bib48">Stickgold, 2009</xref>).</p><p>Exp 2 and Exp 3 differed only in the assignment of auditory category exemplars to visuomotor trials (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). In each experiment, five acoustic stimuli preceded the appearance of the visual target in one of the four locations and participants quickly reported the target location. The auditory category from which these acoustic stimuli were drawn perfectly predicted the location of the upcoming visual target (except in the Random Blocks 7, 10, and 13 that were used to test the cost of disrupting this regularity). In Exp 2, a single auditory category exemplar was repeated five times in advance of the visual target. In Exp 3, five unique auditory exemplars drawn from a single auditory category were presented in random order prior to the visual target. Although the experiments did not differ in the number of times each sound exemplar was encountered, within-category exemplar variability was experienced on a between-trial basis in Exp 2, whereas within-category exemplar variability was experienced in each trial in Exp 3. Prior research examining a single session demonstrated superior incidental learning when exemplar variability was experienced within-trials as in Exp 3, compared across trials in Exp 2 (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>).</p><p>Response time cost (RT Cost) served as the primary measure of incidental auditory category in each session (Day-1, Day-2, Day-10; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The logic of this measure is that the extent to which participants have learned auditory categories and exploit them to guide visuomotor behavior (i.e., as predictors of the upcoming visual target location) will manifest as a cost—a slowing of visuomotor response—when the association between the auditory categories and the target locations is eliminated (Random Blocks 7, 10, and 13; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). RT Cost is a ‘covert’ measure of incidental category learning because participants are unaware that they are being tested for their (implicit) accounting for the auditory categories. There is no requirement for attention to the sounds, category decisions, or labeling. Importantly, because there was no simple sound-to-location mapping in either Exp 2 or Exp 3, RT Cost depends upon incidentally learning to rely upon <italic>auditory categories</italic> to support performance in the visuomotor task.</p><p>We first examined RT Cost on Day-1 by breaking the category-to-location association in Block 7, after participants were afforded 6 blocks of incidental experience with the category-to-location association in the visuomotor task (<xref ref-type="fig" rid="fig2">Figure 2B and C</xref>). There was no significant RT Cost for Block 7 wherein the category-to-location association was disrupted in Exp 2, indicating no robust incidental learning after the 6 practice blocks [<italic>t</italic>(23)=0.43, p=0.66, Cohen’s <italic>d</italic>=0.08, <italic>M</italic><sub>Block7</sub>=466.1ms, <italic>SE</italic><sub>Block7</sub>=13.8ms; <italic>M</italic><sub>Block6</sub>=461.9ms, <italic>SE</italic><sub>Block6</sub>=17.28ms]. In Exp 3, in which participants experienced within-category variability on each visuomotor trial, there was a significant RT Cost indicative of incidental category learning on Day-1 [<italic>t</italic>(21)=2.77, p=0.01, Cohen’s <italic>d</italic>=.59, <italic>M</italic><sub>Block7</sub>=446.06ms, <italic>SE</italic><sub>Block7</sub>=16.28ms; <italic>M</italic><sub>Block6</sub>=409.41ms, <italic>SE</italic><sub>Block6</sub>=19.13ms].</p><p>Incidental auditory category learning in Exp 3 was also reflected as a decrease in RT across Blocks 1–6 conveying the category-to-location association [<italic>F</italic>(5, 105)=2.87, p=0.01, <italic>η<sub>p</sub>²</italic>=.12]. Response time was facilitated in Blocks 4–6 (<italic>M</italic><sub>Blocks4-6</sub> = 407.6 ms, <italic>SE</italic><sub>Blocks4-6</sub> = 20.3 ms) compared to the earlier Blocks 1–3 (<italic>M</italic> <sub>Blocks1-3</sub> = 424.5 ms, <italic>SE</italic><sub>Blocks1-3</sub> = 17.9 ms) [<italic>F</italic> (1, 21)=7.19, p=0.01; <italic>η<sub>p</sub>²</italic>=0.25]. This speeding of RT was not at the cost of accuracy (see Appendix 1). This facilitation of RT is unlikely to have been driven by visuomotor task practice and learning as it was not observed in Exp 1, [<italic>F</italic> (5, 105)=0.99, p=0.42; <italic>η<sub>p</sub>²</italic>=0.04] or in Exp 2 [<italic>F</italic>(5, 115)=0.30, p=0.91; <italic>η<sub>p</sub>²</italic>=0.01], in which incidental category learning was not evident as an RT Cost. Thus, the RT facilitation across Exp 3 Blocks 1–6 can be ascribed, at least in part, to within-session incidental learning of the auditory categories.</p><p>Despite shared category exemplars, equivalent exemplar variability at the experiment level and common visuomotor task demands, Exp 2 and Exp 3 led to different single-session outcomes. Single-session category learning was more robust when multiple exemplars of the same category preceded each trial’s visuomotor target (Exp 3) than when a single exemplar was repeatedly presented before the target and thus exemplar variability was experienced only across trials (Exp 2). This differential pattern of results is consistent with the notion of per-trial many-to-one auditory-to-visuomotor correspondence serving as a ‘representational glue’ to (better) bind together acoustically distinct sound exemplars to enhance incidental category learning compared to cross-trial binding.</p></sec><sec id="s2-3"><title>Incidental category knowledge emerges by day-2 even when not behaviorally evident on day-1</title><p>Yet, examination of visuomotor task performance on Day-2 suggests that it may not be justified to conclude that <italic>no</italic> incidental learning took place on Day-1 of Exp 2 (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). A significant RT Cost emerged by Day-2 of Exp 2, indicative of incidental auditory category learning [<italic>t</italic>(23)=2.78, p=0.01; Cohen’s <italic>d</italic>=0.56]. Disrupting the category-to-location association in Block 10 led to slower visuomotor responses (<italic>M</italic><sub>Block10</sub>=449.13ms, <italic>SE</italic><sub>Block10</sub>=14.79ms) than in the preceding Block 9 in which the association was present (<italic>M</italic><sub>Block9</sub>=426.7ms, <italic>SE</italic><sub>Block9</sub>=17.2ms). Thus, a significant RT Cost to visual target detection evolved and was incurred when the category-to-location association was disrupted – but only after a post-learning interval.</p><p>Similarly, a Day-2 RT Cost was evident in Exp 3 [<italic>t</italic>(21)=2.18, p=0.04; Cohen’s <italic>d</italic>=0.46], with slower visuomotor responses upon disruption of the category-to-location association (<italic>M</italic><sub>Blocks10</sub>=415.01ms, <italic>S.E</italic>.<sub>Blocks10</sub>=12.58ms) compared to the preceding block (<italic>M</italic><sub>Blocks9</sub>=385.35ms, <italic>S.E</italic>.<sub>Blocks9</sub>=18.33ms). Here, the magnitude of the RT Cost was as large on Day-2 as on Day-1 [t(21)=0.64, p=0.52; Cohen’s <italic>d</italic>=0.11; <xref ref-type="fig" rid="fig2">Figure 2C</xref>] indicating maintenance, but not further evolution, of incidental category learning across sessions.</p><p>There also were offline gains in the speed of visuomotor task responses, expressed as a facilitation in RT from the last block of Day-1 to the first block of Day-2, that were apparent in both Exp 2 [<italic>t</italic>(1, 23)=–2.15, p=0.04; Cohen’s <italic>d</italic>=–0.42] and Exp 3 [<italic>t</italic>(1, 21)=–2.35, p=0.028; Cohen’s <italic>d</italic>=–0.5], with faster responses to the visual target on the first block of Day-2 than the last block of Day-1 (mean difference 17.7ms Exp 2, 37.4ms Exp 3; <xref ref-type="fig" rid="fig2">Figure 2</xref>). These delayed gains in speed were not at the cost of accuracy [see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and Appendix 1]. In view of the robust offline gains observed in Exp 1 when no sounds were present, these gains are most likely to be attributable to visuomotor learning, but there is the possibility of concurrent benefits from category learning.</p><p>We conducted an additional analysis to be sure that encountering the random Block 7 on Day-1 did not artificially slow RT in Block 8 (the last block of Day-1). This is important because slower RTs in the last block of Day-1 due to interference from the prior, random block might masquerade as an offline gain in the first block of Day-2. For Exp 2, the RTs of the first and last halves of Block 8 did not differ significantly [<italic>t</italic>(46)=–0.204, p=0.839, Cohen’s <italic>d</italic>=0.005] and there was no significant RT difference between Blocks 6 and 8 [<italic>t</italic>(46)=–0.717, p=0.476, Cohen’s <italic>d</italic>=0.207]. The same held for Exp 3: RTs in the first and second halves of Block 8 did not differ [<italic>t</italic>(44)=0.340, p=0.735, Cohen’s <italic>d</italic>=0.100] nor did they differ between Blocks 6 and 8 [<italic>t</italic>(42)=–0.480, p=0.633, Cohen’s <italic>d</italic>=0.14]. In all, there was no evidence of interference by Block 7, or of a loss of performance from Blocks 6–8 that could explain the observed offline gains.</p><p>We also conducted a control study to examine the possibility that the offline gains in auditory category knowledge observed in Exp 2 may be attributed to the additional practice afforded in the first block on Day-2 (Block 9), rather than attributable to an overnight consolidation process. A new sample of participants performed the SMART task in a single session separated by a 3 hr daytime break between Blocks 1–8 and Blocks 9–11 that did not include sleep. There was no difference in the magnitude of RT Cost in the blocks preceding versus following the break [<italic>t</italic>(20)=–1.10, p=0.28; Cohen’s <italic>d</italic>=0.26]. Thus, we conclude that the offline gains in auditory category knowledge observed in Exp 2 are unlikely to be attributable to practice from Day-2 Block 9, and instead point to offline gains.</p></sec><sec id="s2-4"><title>Incidental category learning is well-retained on day-10</title><p>Incidental category knowledge, as reflected in the RT Cost incurred by Day-2, remained robust across a 9-day interval. There were significant RT Costs on Day-10 in Exp 2 [<italic>t</italic>(23)=2.76, p=0.01; Cohen’s <italic>d</italic>=0.56; <xref ref-type="fig" rid="fig2">Figure 2B</xref>] and Exp 3 [<italic>t</italic>(21)=2.25 p=0.03; Cohen’s <italic>d</italic>=0.47; <xref ref-type="fig" rid="fig2">Figure 2C</xref>] and, moreover, the magnitude of the RT Costs on Day-10 were as large as those attained on Day-2 in both Exp 2 [<italic>t</italic>(23)=–0.62, p=0.54; Cohen’s <italic>d</italic>=0.08] and Exp 3 [<italic>t</italic>(21)=0.20, p=0.83; Cohen’s <italic>d</italic>=0.03]. (See<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for a full comparison of cross-experiment performance).</p></sec><sec id="s2-5"><title>Individual participant’s RT costs across sessions</title><p>We examined whether individual participant’s RT Costs were correlated across sessions (Pearson correlations, Bonferroni-corrected p&lt;0.017 significant). In Exp 2, participants’ RT Costs on Day-1 and Day-2 were significantly correlated (<italic>r</italic>=0.737, p=0.001), as were the RT Costs incurred on Day-2 and Day-10 (<italic>r</italic>=0.778, p=0.001), and Day-1 and Day-10 (<italic>r</italic>=0.571, p=0.004). The same pattern was found in Exp 3, with significant correlations between participants’ RT costs incurred on Day-1 and Day 2- (<italic>r</italic>=0.670, p=0.001), as well as Day-2 and Day-10 RT Costs (<italic>r</italic>=0.730, p=0.001) and Day-1 and Day-10 (<italic>r</italic>=0.614, p=0.002).</p></sec><sec id="s2-6"><title>Incidental category learning generalizes to novel sound exemplars</title><p>An explicit labeling task followed the SMART task on Day-10 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) to examine generalization of category knowledge to novel sounds (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Novel sound exemplars were randomly, but equally, drawn from each of the four auditory categories and each exemplar was repeated five times. Participants indicated the expected visual target location (no target appeared). Category knowledge generalized to support placing novel sounds in the appropriate location associated with its category at above-chance levels in Exp 2 for both category types (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) [unidimensional categories: <italic>t(</italic>23) = 2.89, p=0.008; Cohen’s <italic>d</italic>=0.57; multidimensional: <italic>t</italic>(23) = 3.104, p=0.005; Cohen’s <italic>d</italic>=0.60]. Categorization performance was not dependent on whether the category was defined by a unidimensional acoustic cue (rising vs. falling frequency of one component of the complex sound) or by more complex, multidimensional, acoustic regularities, [<italic>t(</italic>23)=0.73, p=0.47; Cohen’s <italic>d</italic>=0.07] (<xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Retention and generalization of category knowledge.</title><p>(<bold>A</bold>) Participants label novel category exemplars at the end of the Day-10 session at above chance performance for both unidimensional and multidimensional categories in Exp 2 and Exp 3 (minimum p level = .019). (<bold>B</bold>) Generalization of category knowledge in the Day-10 explicit labeling task was positively associated with RT Cost for each session (Day-1, Day-2, and Day-10) for both Exp 2 and Exp 3. (<bold>C</bold>) In contrast, generalization of category knowledge in the Day-10 explicit labeling task was not associated with offline gains in RT (from Day-1 to Day-2 and from Day-2 to Day-10), consistent with observation of offline gains in the Exp 1 visuomotor task with no auditory stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81855-fig3-v2.tif"/></fig><p>Category knowledge also generalized to novel sounds in Exp 3 [unidimensional: <italic>t</italic>(21) = 3.77, p=0.001; Cohen’s <italic>d</italic>=0.78, multidimensional: <italic>t</italic>(21) = 2.52, p=0.01; Cohen’s <italic>d</italic>=0.53] although here performance in the unidimensional categories was more accurate compared to multidimensional categories [<italic>t(</italic>21)=.3.11, p=0.005; Cohen’s <italic>d</italic>=0.37].</p><p>We also compared generalization to novel exemplars across Exp 2 and Exp 3. Overall, generalization to novel exemplars did not differ across experiments, [<italic>t(</italic>44)=0.54, p=0.58; Cohen’s <italic>d</italic>=0.18].</p></sec><sec id="s2-7"><title>Incidental category learning predicts explicit category labeling and generalization on day-10</title><p>We examined the extent to which the covert RT Cost measure of incidental category learning predicted participants’ ability to subsequently generalize category knowledge by the end of Day-10 (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In both Exp 2 and Exp 3, the magnitude of a participant’s RT Cost on Day-1, Day-2, and Day-10 was highly predictive of the accuracy of category generalization in an explicit category labeling task on Day-10 (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This held true also in follow-up analyses based on a median split in explicit labeling accuracy that better accommodates the clustering evident in participants’ performance. Thus, a median split of participants based on their explicit labeling accuracy on Day-10 into two subgroups, high-performing and low-performing showed, in Exp 2, that the high-performing subgroup had a tendency to exhibit greater RT Costs, compared to the low-performing subgroup, on Day-2 [<italic>t</italic>(22)=–1.79, p=0.08, Cohen’s <italic>d</italic>=–0.733] and on Day-10 [<italic>t</italic>(20)=–2.07, p=0.05, Cohen’s <italic>d</italic>=.-0.846] although the subgroups did not differ on Day-1, <italic>t</italic>(22) = –1.30, p=0.21, Cohen’s <italic>d</italic>=–0.533. In Exp 3, the high-performing subgroup exhibited significantly greater RT Costs than the low-performing subgroup on Day-2 [t (20)=–3.82, p=0.001; Cohen’s d=–0.721] and Day-10 [t(20)=–2.68, p=0.014, Cohen’s d=–1.144], although, here too, the subgroups’ RT Costs did not differ on Day-1 [<italic>t</italic>(20)=–1.69, p=0.106; Cohen’s <italic>d</italic>=-0.721].</p><p>However, the overnight, offline gains in SMART performance (the facilitation in RT between sessions) were not predictive of the ability to categorize novel stimuli accurately on Day-10 (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Together with the results of Exp 1 demonstrating robust overnight gains with no sounds, the lack of a relationship between overnight gains and category generalization argues that skill consolidation in the visuomotor task is likely to be the primary driver of overnight RT facilitation. This, in turn, suggests that two contemporaneous learning processes – visuomotor task learning and incidental category learning – are evident in the present results.</p><p>Comparison of Experiment 2 versus Experiment 3 Outcomes. (See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Given the complexity of everyday situations, ostensibly unrelated perceptual regularities that are not necessary for the execution of a given task may align with the task-related input or responses. The current results suggest that when such alignments occur, substantial learning can take place incidentally, rather than intentionally—without overt instruction, explicit call for perceptual decisions, directed attention, or feedback. After a relatively brief experience practicing a simple visuomotor task in which acoustic input is unnecessary for task performance, young adults learn about auditory category structure of the sounds and use it to hone visuomotor response. This learning extends beyond simple auditory-to-visuomotor mappings because it generalizes to support categorization of sounds not previously encountered. Participants thus process repeated information and establish new knowledge to support behavior, even when it is not strictly necessary – as in the SMART, a simple visuomotor task that can be completed with high accuracy without any reliance on the sounds.</p><p>The current results underscore an important outcome of the SMART training experience: category knowledge emerging from incidental experience is further elaborated and consolidated into long-term memory after the termination of the initial session. There was evidence for robust within-session, ‘online’, auditory category learning in Exp 3, however, there was no clear-cut evidence for auditory category learning during the first session of Exp 2. Nevertheless, by the second session of Exp 2 learners’ performance became significantly reliant on the availability of the sound categories, indicating that auditory category learning had occurred and, nine days later, their generalization of category knowledge was as robust as that of learners in Exp 3. Therefore, in addition to performance gains that can be observed as ‘online learning’ within a single training session, gains reflecting the setting up and utilization of the new sound categories occur post-session and can be expressed as ‘offline’ gains after the training experience has ended (<xref ref-type="bibr" rid="bib22">Karni and Sagi, 1993</xref>).</p><p>These latter, delayed gains emerging in the post learning interval are believed to reflect memory consolidation—the process by which memories become less susceptible to interference and are honed to represent new ‘how to’ knowledge (<xref ref-type="bibr" rid="bib24">Karni, 1996</xref>; <xref ref-type="bibr" rid="bib25">Karni and Bertini, 1997</xref>; <xref ref-type="bibr" rid="bib8">Dudai et al., 2015</xref>). Offline gains can be sleep-dependent (<xref ref-type="bibr" rid="bib47">Stickgold, 2005</xref>; <xref ref-type="bibr" rid="bib23">Karni et al., 1994</xref>), but can occur also after a wakefulness period (<xref ref-type="bibr" rid="bib44">Roth et al., 2005</xref>). Since our protocol included a sleep interval, sleep-dependent consolidation may play a role, but future studies using polysomnography to relate sleep parameters with offline gains would be needed to resolve this question. At present, the results indicate the existence of a post-session memory consolidation phase in incidental category learning within which category knowledge is elaborated to a degree that it can be expressed in subsequent performance, even if it was not apparent by the end of the learning session.</p><p>The many-to-one correspondence of category exemplars and visuomotor task demands may have been instrumental in prompting these different learning trajectories. Overall, a change in the way that within-category exemplar variability was experienced (across or within trials) had a profound influence on the course of incidental learning. The distinct course of learning observed in Exp 2 and Exp 3 is especially notable considering that the experiments shared the same task settings and demands, identical acoustic stimuli, equivalent overall exemplar variability (across the experiment) and the same protocol of visuomotor practice. A tighter coupling of exemplar variability to visuomotor task demands (Exp 3) proved advantageous in early learning, but ultimately did not confer a benefit to long-term retention and generalization at the final session. This pattern of results is resonant with the notion that better within-session (online) learning may not necessarily lead to better retention across longer time periods (<xref ref-type="bibr" rid="bib4">Bjork and Bjork, 2011</xref>) and with previous research reporting greater overnight consolidation of auditory regularities among poor online learners (<xref ref-type="bibr" rid="bib2">Ballan et al., 2023</xref>) as well as reports that difficult auditory regularities are more likely to be consolidated if a night of sleep is afforded (<xref ref-type="bibr" rid="bib9">Durrant et al., 2011</xref>). Nonetheless, we observe that participants’ early learning in the first session was predictive of learning evident in subsequent sessions.</p><p>Importantly, what is consolidated and retained in the long run appears to constitute <italic>category</italic> knowledge rather than a lasting mnemonic trace of the specific sound exemplars that were actually encountered, since learners were able to generalize to novel exemplars nine days after the initial learning experience. Prior studies of incidental learning in the SMART task found that the RT Cost was absent when sounds provided no category-to-visual target regularity (<xref ref-type="bibr" rid="bib17">Gabay et al., 2023</xref>) or when sounds deterministically mapped to targets, but not in a manner that preserved underlying category regularities (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>). RT Costs were also absent when novel category-<italic>consistent</italic> exemplars were introduced in the SMART (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>). Collectively, these findings point to the development of category knowledge that extends beyond auditory-visual-motor associations; the findings cannot be explained by a non-specific cost to performance resulting from a change in the auditory environment. Relatedly, prior studies make clear that category learning does not reliably occur across passive observation of the auditory-visual patterns in the SMART task, or when participants make a category-nonspecific, generic motor response to report simply the presence of a visual target. Instead, the relationship of auditory regularities defining a category to distinct visuomotor task demands appears to be the ‘representational glue’ that binds distinct exemplars together to develop category knowledge that generalizes to novel exemplars consistent with the category regularities (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>).</p><p>RT facilitation in the SMART task may capture both the development of auditory category knowledge and more general skills such as the visual stimulus-response mapping. We attempted to dissociate these types of knowledge by comparing RT in a block in which category-to-target regularities were present and a subsequent block in which the regularities were disrupted, yielding a RT Cost. The RT Cost afforded a graded measure of how much a participant’s performance depended on the correspondence of the auditory category and the visuomotor task demands. Importantly, the RT Cost is elicited only with some level of category knowledge. We included a final block re-introducing the category-target regularities to enable participants to return to the trained task conditions. This regularity-random-regularity structure allowed us to assess the development of a specific reliance (in SMART execution) on (emerging) auditory category knowledge across time, independently of the general gains that could be reflected in the random block performance. Despite its advantages, this approach leaves open the possibility that category reactivation across the SMART task may have influenced generalization assessed in the final session. It will be important for future investigations to refine our understanding of a possible contribution of retained category knowledge to generalization, independent of the potential for mnemonic ‘reactivation’ by re-performing the SMART task.</p><p>The nature of the category regularity associated with the visuomotor target had an influence on learning, at least in Exp 3 where post-test categorization was more accurate for unidimensional, compared to multidimensional, categories. This is consistent with prior research (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>) under single-session incidental training conditions. It has been suggested that there is a complex relationship between how categories are defined by a single, or multiple, input dimensions, and whether the distributions defining the categories are deterministic (as they were here) or probabilistic in their sampling on the one hand, and whether training is incidental or driven by overt feedback on the other (<xref ref-type="bibr" rid="bib40">Roark and Holt, 2018</xref>). It will be important for future research to assess whether – as in visual category learning under overt training conditions (<xref ref-type="bibr" rid="bib1">Ashby and Maddox, 2005</xref>) – an advantage for unidimensional category learning in incidental training relates to the affordance of being more easily verbalizable.</p><p>Our data presents a cautionary note for learning research, very generally. The observation of offline learning gains even for the simple visuomotor task without auditory stimuli in Exp 1 makes the case that we must be attentive to how even the simplest task demands can trigger task-specific learning that may masquerade as other forms of learning. Without Exp 1 as a baseline, it would be easy to attribute the robust facilitation of RT across sessions to auditory category learning. Instead, this facilitation appears to be largely driven by robust post-session, consolidation phase changes in the execution of the visuomotor task—visuomotor learning unrelated to the availability of an auditory input. This can explain the finding of no relationship between the RT facilitation across sessions and category generalization in the final session in Exp 2 and 3. Nevertheless, there remains the prospect that visuomotor learning interacts with incidental auditory category learning. In the context of the more difficult category learning challenge afforded in Exp 2, the offline gains evident in RT facilitation between-sessions were smaller than those observed in either the purely visuomotor task of Exp 1 or the less challenging auditory category learning of Exp 3. This leaves open the possibility that incidental auditory category learning and visuomotor learning may interact, so that overall SMART performance is constrained under conditions of more challenging auditory category learning.</p><p>At the broadest level, the present results speak to debates on how sensory experiences – across any modality and, indeed, between modalities—accumulate to convey regularities that ultimately structure knowledge. Understanding how organisms come to treat physically distinct objects that share deeper statistical structure as functionally equivalent is central to understanding cognition. Yet, most of what we know about category learning has come from studies examining learning under explicit training with overt feedback. In these situations, participants typically know of the existence of categories (at least via the number of response options), actively make category decisions via a motor response, and learn from explicit feedback about the correctness of the decisions. We know much less about incidental learning, when active engagement in behavior involves multiple sources of sensory input, some of which may be, unbeknownst to learner or teacher, coherently structured and not explicitly categorized.</p><p>Our results show that incidental learning continues to evolve after the learning experience has ended, i.e., in the interval following the learning-training session. Brief, incidental experience with novel sound categories that align with a very simple visuomotor task led young adult participants to capitalize on the presence of auditory stimuli, despite the lack of a simple stimulus-response association, to support visuomotor task performance. Our results show clearly that the learning process initiated in the session continues in the post-session interval resulting in delayed gains in the ability of the learners to subsequently employ the new auditory knowledge in performing the visuomotor task. In capitalizing on the auditory stimuli learners seem to build lasting category representations that support both the long-term retention of the new auditory category knowledge and its generalization to similar, novel sensory experiences nine days later. Consistent with prior reports of consolidation phase offline gains for category knowledge (<xref ref-type="bibr" rid="bib6">Djonlagic et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Maddox et al., 2009</xref>; <xref ref-type="bibr" rid="bib3">Barsky et al., 2015</xref>), our findings are resonant with research in motor (<xref ref-type="bibr" rid="bib26">Karni et al., 1998</xref>) and perceptual domains (<xref ref-type="bibr" rid="bib22">Karni and Sagi, 1993</xref>; <xref ref-type="bibr" rid="bib9">Durrant et al., 2011</xref>; <xref ref-type="bibr" rid="bib5">de la Chapelle et al., 2022</xref>; <xref ref-type="bibr" rid="bib34">López-Barroso et al., 2016</xref>) indicating the existence of a consolidation phase in the development of skills (<xref ref-type="bibr" rid="bib24">Karni, 1996</xref>; <xref ref-type="bibr" rid="bib8">Dudai et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Stickgold, 2005</xref>; <xref ref-type="bibr" rid="bib43">Robertson et al., 2004</xref>) and extend these findings to incidental category learning. In this regard, the results may be particularly relevant in understanding speech category learning, which proceeds incidentally without explicit feedback. (<xref ref-type="bibr" rid="bib29">Lim and Holt, 2011</xref>; <xref ref-type="bibr" rid="bib30">Lim et al., 2014</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Eighty-seven healthy young adult participants were recruited from the University of Haifa community. All individuals had normal or corrected-to-normal vision, reported normal hearing, and received payment or course credit for participation. The study was approved by the Institutional Review Board of the University of Haifa and was conducted in accordance with the Declaration of Helsinki. Written informed consent was obtained from all participants, who were compensated for their participation in the study (120 new Israeli shekels, approximately $30). Previous research using the same stimuli, paradigm and cross-participant manipulation of exemplar variability revealed large between-subject effect sizes for RT Cost (i.e. Cohen’s d of 0.76). A power analysis (calculated using Gpower software; <xref ref-type="bibr" rid="bib13">Faul et al., 2007</xref>) indicates that a one-tailed between-subject effect requires 44 participants to reach statistical power at a 0.80 level (alpha = 0.05). Therefore, with a total sample of 46 participants (across Exp 2 and Exp 3) the study was adequately powered to detect differences arising from the exemplar variability manipulation (<xref ref-type="bibr" rid="bib14">Faul et al., 2009</xref>).</p><p>Twenty-two subjects (12 females; 27.27±5.02, 19 y to 34 y old), twenty-four subjects (12 females 26.62 y±4.75 y, 20–42 y old) and twenty-two subjects (12 females, 24.81 y±2.78 y, 20 y to 32 y old) participated in Exp 1, 2, and 3, respectively. An additional twenty-one participants (18 females; 27.27±5.02, 20 y to 27 y old) participated in the control experiment.</p></sec><sec id="s4-2"><title>Stimuli</title><p><xref ref-type="fig" rid="fig1">Figure 1A</xref> illustrates four novel nonspeech auditory categories, drawn from prior research (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Leech et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>; <xref ref-type="bibr" rid="bib33">Liu and Holt, 2011</xref>; <xref ref-type="bibr" rid="bib32">Lim et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Gabay et al., 2018</xref>; <xref ref-type="bibr" rid="bib12">Emberson et al., 2013</xref>). The sounds defining these categories possess some of the spectrotemporal complexity of speech, but are unequivocally nonspeech owing to their noise and square wave sources (<xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>). Each category has six exemplars used in training and five exemplars withheld from training to test generalization on Day 10 (not shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Exemplars from each category are defined by a steady-state frequency and a transition in each of two spectral peaks (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; higher frequency solid colored peak varying across exemplars vs. lower frequency dotted grey peak common across exemplars). Exemplars were acoustically similar within and across categories (<xref ref-type="bibr" rid="bib12">Emberson et al., 2013</xref>). Two categories (Category A, Category B in <xref ref-type="fig" rid="fig1">Figure 1A</xref>) are defined by a unidimensional acoustic cue (up- or down-sweep in frequency of the higher-frequency component). The other two categories are defined in a more complex, multidimensional perceptual space such that no one acoustic cue uniquely defines category membership (<xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>; <xref ref-type="bibr" rid="bib12">Emberson et al., 2013</xref>; Category C, Category D in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). This multidimensional structure models structures present in phonetic categories, such as categorizing /d/ across syllables ending with various vowels (<xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>; <xref ref-type="bibr" rid="bib28">Liberman et al., 1954</xref>), thereby capturing a learning challenge of phonetic acquisition. Prior results demonstrate that the dimensions defining these categories are not easily described verbally and are not well-learned via passive exposure alone (<xref ref-type="bibr" rid="bib50">Wade and Holt, 2005</xref>; <xref ref-type="bibr" rid="bib12">Emberson et al., 2013</xref>). Each exemplar was 250ms and exemplars were matched in root-mean-square amplitude.</p></sec><sec id="s4-3"><title>Systematic multimodal association time (SMART) task</title><p>In the SMART task, participants rapidly detect the appearance of a visual target in one of four possible screen locations and report its position by pressing a key corresponding to the visual location (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This simple visuomotor task is practiced across three experimental sessions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) in each of the three experiments. In Exp 2 and Exp 3, but not in Exp 1, a brief sequence of ostensibly task-irrelevant sounds precedes each visual target, presented diotically over headphones (Beyer, DT-150) at a comfortable listening level in a sound-attenuated booth with participants seated directly in front of the computer monitor on which the visual target appears. Unknown to participants, the sounds are drawn from one of four distinct categories (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Thus, there is a multimodal (auditory category to visual location) correspondence that relates the acoustically variable sound category exemplars to a consistent visual target location and response. This mapping is many-to-one, such that multiple, acoustically-variable sound category exemplars are associated with a single visual location (and response). Therefore, since auditory categories perfectly predict the location of the upcoming visual detection target and the corresponding response button to be pressed, learning to treat the acoustically variable sounds as functionally equivalent may facilitate visual detection without requiring overt sound categorization decisions or even awareness of category structure. The SMART task makes it possible to investigate whether participants learn auditory categories <italic>incidentally</italic>, across practice of a visuomotor task that does not involve auditory category decisions, directed attention to the sound exemplars, or feedback.</p><p>Participants first completed 8 practice trials to become familiar with the visuomotor response. For Exp 2 and Exp 3, sounds preceded visual targets in these practice trials, but there was no consistent category-to-location relationship. Immediately thereafter, there were six training blocks (96 trials, 4 sound categories x 6 exemplars x 4 repetitions; <xref ref-type="fig" rid="fig1">Figure 1B</xref>) for which there was a perfect mapping between auditory category and upcoming visual target location. In the seventh block (48 trials), any sound exemplar could precede presentation of the visual target in any position; sound category no longer predicted the position in which the visual target would appear and the five sounds preceding a visual target were selected randomly (see below). An eighth block on Day 1 restored the relationship between sound category and the location of the upcoming visual target. Exp 1 differed only in that no sounds preceded the visual target, providing a control task that involved only visuomotor task practice without the opportunity for auditory category learning. Approximately twenty-four hours later on Day 2, participants completed a 96-trial training block and a shorter (48 trial) random-mapping block and a final 96-trial training block to restore the mapping. On Day 10, participants completed three blocks with a structure identical to Day 2. Response time (RT) was measured from the onset of the visual target to a button press and a RT Cost incurred as a result of eliminating the auditory category to location mapping was defined as the difference in RT for the Random block (Blocks 7, 10, 13) and the training block that preceded it (Blocks 6, 9, 12, respectively).</p><p>The control experiment was run using a protocol identical to the one used in Exp 2, except that the two sessions corresponding to Day-1 (Blocks 1–8) and Day-2 (Blocks 9–11) were run on the same day with just 3 hr, and no sleep interval were afforded between the two sessions. The control experiment was not extended to Day-10.</p></sec><sec id="s4-4"><title>Explicit labeling task</title><p>Subsequent to the SMART task blocks (Blocks 12, 13, 14) on Day 10, (and after Block 11 in the single-session control experiment) there was an explicit labeling task in which novel sound exemplars drawn from one of the four auditory categories, and never experienced in the prior sessions, were presented on each of 96 trials and participants selected the location at which the visual target was expected; no target appeared. Generalization of category knowledge was defined as the proportion of trials for which the location selected matched the category-to-location mapping experienced across the training sessions. There was no explicit labeling task for Exp 1 (since there were no sounds).</p></sec><sec id="s4-5"><title>Experimental design</title><p>Three separate groups of participants engaged in visuomotor practice across the three sessions on Day 1, Day 2, and Day 10 (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Participants in Exp 1 practiced the visuomotor SMART task exclusively; no sounds preceded the visual target. This provided a measure of task-related learning and consolidation induced by the visuomotor task, apart from auditory category learning. Participants in Exp 2 and Exp 3 practiced this same visuomotor SMART task, but on each trial five sound exemplars preceded the appearance of the visual target. Exp 2 and Exp 3 differed in how within-category acoustic variability was organized across trials (while remaining equivalent at the experiment level). In Exp 2, a single category exemplar was chosen and presented five times preceding the visual target such that within-category exemplar variability was experienced <italic>across</italic> but not <italic>within</italic> trials. In Exp 3, five unique exemplars were randomly selected (without replacement) from the six category exemplars and presented in a random order. In each experiment, the sound categories perfectly predicted the upcoming target location and, across trials, the within-category variability experienced by participants was equivalent at the experiment level across Exp 2 and 3. Prior research (<xref ref-type="bibr" rid="bib15">Gabay et al., 2015</xref>) suggested that incidental auditory category learning would be less efficient in a single session of Exp 2 compared to Exp 3 and so this manipulation allowed for examination of patterns of consolidation across weaker (Exp 2) versus more robust (Exp 3) single-session learning.</p></sec><sec id="s4-6"><title>Data analyses</title><p>In computing response time (RT), trials for which there was a visual detection error (2.4% Exp 1; 2% Exp 2; 3% Exp 3, 3% control experiment) or RT longer than 1500ms or shorter than 100ms from all trials (1% Exp 1; 1% trials Exp 2; 3% Exp 3; 1.6% control experiment) were excluded from analyses.</p><p>We assessed learning, consolidation, retention, and generalization with several measures: (1) Offline facilitation of RT served as a learning measure (<xref ref-type="bibr" rid="bib32">Lim et al., 2019</xref>) across all three experiments. Comparison of the last block of Day-1 (or Day-2) and the first block of Day-2 (or Day-10) was accomplished with paired-samples t-tests; (2) Incidental auditory category learning in each session of Exp 2 and Exp 3 was examined as the RT Cost of eliminating the category-to-location correspondence experienced in training blocks (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The difference in RT to respond to the visual target in each Random block (Blocks 7, 10, 13) compared to the RT in the training block immediately preceding it (Blocks 6, 9, 12, respectively; <xref ref-type="fig" rid="fig1">Figure 1B</xref>) was assessed with paired-samples t-test comparisons; (3) Generalization of category knowledge to exemplars not experienced in training was measured as accuracy in reporting location in the explicit labeling task according to the category-to-location mapping experienced in practicing the visuomotor SMART task. Since the auditory categories are novel, and could be acquired only in the context of the experiment, accuracy was assessed relative to chance (25%) with a one-tailed t-test; (4) The relationships of generalization of category knowledge to RT Cost and Offline Gains were assessed using correlation analyses; (5) The possibility of a trade-off in response time and accuracy was examined such that similar analyses conducted on RT were also calculated for accuracy data [See <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and Appendix 1]. Here the difference in accuracy to respond to the visual target in each Random block (Blocks 7, 10, 13) was compared to accuracy in the training block immediately preceding it (Blocks 6, 9, 12, respectively) and was termed as Accuracy Cost.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the Institutional Review Board of the University of Haifa (no. 099/18) and was conducted in accordance with the Declaration of Helsinki. Written informed consent was obtained from all participants, who were compensated for their participation in the study.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Comparison of Experiment 2 versus Experiment 3 Outcomes.</title><p>Supplemental Table 1 - RT Facilitation as a function of Experiment, ANOVA. Supplemental Table 2 - RT Cost as a function of Session and Experiment, ANOVA. Supplemental Table 3 - RT Facilitation across Day-1 to Day-2 as a function of Experiment, ANOVA. Supplemental Table 4 - Retention RT as a function of Experiment, ANOVA. Supplemental Table 5 - Posttest generalization accuracy as a function Category Type and Experiment, ANOVA.</p></caption><media xlink:href="elife-81855-supp1-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-81855-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Anonymized data and code to reproduce the results presented here are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/7y2nx/">https://osf.io/7y2nx/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Gabay</surname><given-names>Y</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Holt</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Memory for Incidentally Learned Categories Evolves in the Post-Learning Interval</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.31234/osf.io/a8ksm</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the Binational Scientific Foundation (2015227), the National Science Foundation-Binational Scientific Foundation (2016867, NSF BCS1655126) grants to authors LLH, AK and YG and by a grant from the Israel Science Foundation (734/22) to author YG.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Human category learning</article-title><source>Annual Review of Psychology</source><volume>56</volume><fpage>149</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.56.091103.070217</pub-id><pub-id pub-id-type="pmid">15709932</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballan</surname><given-names>R</given-names></name><name><surname>Durrant</surname><given-names>SJ</given-names></name><name><surname>Manoach</surname><given-names>DS</given-names></name><name><surname>Gabay</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Failure to consolidate statistical learning in developmental dyslexia</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>30</volume><fpage>160</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.3758/s13423-022-02169-y</pub-id><pub-id pub-id-type="pmid">36221045</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsky</surname><given-names>MM</given-names></name><name><surname>Tucker</surname><given-names>MA</given-names></name><name><surname>Stickgold</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rem sleep enhancement of probabilistic classification learning is sensitive to subsequent interference</article-title><source>Neurobiology of Learning and Memory</source><volume>122</volume><fpage>63</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2015.02.015</pub-id><pub-id pub-id-type="pmid">25769506</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bjork</surname><given-names>EL</given-names></name><name><surname>Bjork</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Making things hard on yourself, but in a good way: creating desirable difficulties to enhance learning</article-title><source>Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society</source><volume>2</volume><fpage>56</fpage><lpage>64</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de la Chapelle</surname><given-names>A</given-names></name><name><surname>Savard</surname><given-names>MA</given-names></name><name><surname>Restani</surname><given-names>R</given-names></name><name><surname>Ghaemmaghami</surname><given-names>P</given-names></name><name><surname>Thillou</surname><given-names>N</given-names></name><name><surname>Zardoui</surname><given-names>K</given-names></name><name><surname>Chandrasekaran</surname><given-names>B</given-names></name><name><surname>Coffey</surname><given-names>EBJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Sleep affects higher-level categorization of speech sounds, but not frequency encoding</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>154</volume><fpage>27</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2022.04.018</pub-id><pub-id pub-id-type="pmid">35732089</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Djonlagic</surname><given-names>I</given-names></name><name><surname>Rosenfeld</surname><given-names>A</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Myers</surname><given-names>C</given-names></name><name><surname>Gluck</surname><given-names>M</given-names></name><name><surname>Stickgold</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sleep enhances category learning</article-title><source>Learning &amp; Memory</source><volume>16</volume><fpage>751</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1101/lm.1634509</pub-id><pub-id pub-id-type="pmid">19926780</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorfberger</surname><given-names>S</given-names></name><name><surname>Adi-Japha</surname><given-names>E</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reduced susceptibility to interference in the consolidation of motor memory before adolescence</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e240</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000240</pub-id><pub-id pub-id-type="pmid">17327907</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudai</surname><given-names>Y</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The consolidation and transformation of memory</article-title><source>Neuron</source><volume>88</volume><fpage>20</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.004</pub-id><pub-id pub-id-type="pmid">26447570</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durrant</surname><given-names>SJ</given-names></name><name><surname>Taylor</surname><given-names>C</given-names></name><name><surname>Cairney</surname><given-names>S</given-names></name><name><surname>Lewis</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sleep-dependent consolidation of statistical learning</article-title><source>Neuropsychologia</source><volume>49</volume><fpage>1322</fpage><lpage>1331</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.02.015</pub-id><pub-id pub-id-type="pmid">21335017</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Earle</surname><given-names>FS</given-names></name><name><surname>Myers</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sleep and native language interference affect non-native speech sound learning</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>41</volume><fpage>1680</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1037/xhp0000113</pub-id><pub-id pub-id-type="pmid">26280264</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Earle</surname><given-names>FS</given-names></name><name><surname>Landi</surname><given-names>N</given-names></name><name><surname>Myers</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sleep duration predicts behavioral and neural differences in adult speech sound learning</article-title><source>Neuroscience Letters</source><volume>636</volume><fpage>77</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2016.10.044</pub-id><pub-id pub-id-type="pmid">27793703</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emberson</surname><given-names>LL</given-names></name><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Zevin</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Is statistical learning constrained by lower level perceptual organization?</article-title><source>Cognition</source><volume>128</volume><fpage>82</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2012.12.006</pub-id><pub-id pub-id-type="pmid">23618755</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Lang</surname><given-names>AG</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>G*Power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behavior Research Methods</source><volume>39</volume><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.3758/bf03193146</pub-id><pub-id pub-id-type="pmid">17695343</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name><name><surname>Lang</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Statistical power analyses using G*power 3.1: tests for correlation and regression analyses</article-title><source>Behavior Research Methods</source><volume>41</volume><fpage>1149</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.3758/BRM.41.4.1149</pub-id><pub-id pub-id-type="pmid">19897823</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabay</surname><given-names>Y</given-names></name><name><surname>Dick</surname><given-names>FK</given-names></name><name><surname>Zevin</surname><given-names>JD</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Incidental auditory category learning</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>41</volume><fpage>1124</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1037/xhp0000073</pub-id><pub-id pub-id-type="pmid">26010588</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gabay</surname><given-names>Y</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Consolidation and retention of auditory categories acquired incidentally in performing a visuomotor task</article-title><conf-name>Proceedings of the 40th Annual Conference of the Cognitive Science Society</conf-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabay</surname><given-names>Y</given-names></name><name><surname>Madlansacay</surname><given-names>M</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Incidental auditory category learning and visuomotor sequence learning do not compete for cognitive resources</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>85</volume><fpage>452</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.3758/s13414-022-02616-x</pub-id><pub-id pub-id-type="pmid">33478807</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goudbeek</surname><given-names>M</given-names></name><name><surname>Swingley</surname><given-names>D</given-names></name><name><surname>Smits</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Supervised and unsupervised learning of multidimensional acoustic categories</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>35</volume><fpage>1913</fpage><lpage>1933</lpage><pub-id pub-id-type="doi">10.1037/a0015781</pub-id><pub-id pub-id-type="pmid">19968443</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guenther</surname><given-names>FH</given-names></name><name><surname>Husain</surname><given-names>FT</given-names></name><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of categorization and discrimination training on auditory perceptual space</article-title><source>The Journal of the Acoustical Society of America</source><volume>106</volume><fpage>2900</fpage><lpage>2912</lpage><pub-id pub-id-type="doi">10.1121/1.428112</pub-id><pub-id pub-id-type="pmid">10573904</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>Lotto</surname><given-names>AJ</given-names></name><name><surname>Diehl</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Auditory discontinuities interact with categorization: implications for speech perception</article-title><source>The Journal of the Acoustical Society of America</source><volume>116</volume><fpage>1763</fpage><lpage>1773</lpage><pub-id pub-id-type="doi">10.1121/1.1778838</pub-id><pub-id pub-id-type="pmid">15478443</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>Lotto</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cue weighting in auditory categorization: implications for first and second language acquisition</article-title><source>The Journal of the Acoustical Society of America</source><volume>119</volume><fpage>3059</fpage><lpage>3071</lpage><pub-id pub-id-type="doi">10.1121/1.2188377</pub-id><pub-id pub-id-type="pmid">16708961</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Sagi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The time course of learning a visual skill</article-title><source>Nature</source><volume>365</volume><fpage>250</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1038/365250a0</pub-id><pub-id pub-id-type="pmid">8371779</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Tanne</surname><given-names>D</given-names></name><name><surname>Rubenstein</surname><given-names>BS</given-names></name><name><surname>Askenasy</surname><given-names>JJ</given-names></name><name><surname>Sagi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Dependence on REM sleep of overnight improvement of a perceptual skill</article-title><source>Science</source><volume>265</volume><fpage>679</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1126/science.8036518</pub-id><pub-id pub-id-type="pmid">8036518</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karni</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The acquisition of perceptual and motor skills: a memory system in the adult human cortex</article-title><source>Brain Research. Cognitive Brain Research</source><volume>5</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(96)00039-0</pub-id><pub-id pub-id-type="pmid">9049069</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Bertini</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Learning perceptual skills: behavioral probes into adult cortical plasticity</article-title><source>Current Opinion in Neurobiology</source><volume>7</volume><fpage>530</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(97)80033-5</pub-id><pub-id pub-id-type="pmid">9287202</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>G</given-names></name><name><surname>Rey-Hipolito</surname><given-names>C</given-names></name><name><surname>Jezzard</surname><given-names>P</given-names></name><name><surname>Adams</surname><given-names>MM</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The acquisition of skilled motor performance: fast and slow experience-driven changes in primary motor cortex</article-title><source>PNAS</source><volume>95</volume><fpage>861</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.861</pub-id><pub-id pub-id-type="pmid">9448252</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leech</surname><given-names>R</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Expertise with artificial nonspeech sounds recruits speech-sensitive cortical regions</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>5234</fpage><lpage>5239</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5758-08.2009</pub-id><pub-id pub-id-type="pmid">19386919</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>AM</given-names></name><name><surname>Delattre</surname><given-names>PC</given-names></name><name><surname>Cooper</surname><given-names>FS</given-names></name><name><surname>Gerstman</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>The role of consonant-vowel transitions in the perception of the stop and nasal consonants</article-title><source>Psychological Monographs</source><volume>68</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1037/h0093673</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>S</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning foreign sounds in an alien world: videogame training improves non-native speech categorization</article-title><source>Cognitive Science</source><volume>35</volume><fpage>1390</fpage><lpage>1405</lpage><pub-id pub-id-type="doi">10.1111/j.1551-6709.2011.01192.x</pub-id><pub-id pub-id-type="pmid">21827533</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>SJ</given-names></name><name><surname>Fiez</surname><given-names>JA</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How may the basal ganglia contribute to auditory categorization and speech perception?</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>230</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00230</pub-id><pub-id pub-id-type="pmid">25136291</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>SJ</given-names></name><name><surname>Lacerda</surname><given-names>F</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Discovering functional units in continuous speech</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>41</volume><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1037/xhp0000067</pub-id><pub-id pub-id-type="pmid">26010592</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>SJ</given-names></name><name><surname>Fiez</surname><given-names>JA</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Role of the striatum in incidental learning of sound categories</article-title><source>PNAS</source><volume>116</volume><fpage>4671</fpage><lpage>4680</lpage><pub-id pub-id-type="doi">10.1073/pnas.1811992116</pub-id><pub-id pub-id-type="pmid">30782817</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural changes associated with nonspeech auditory category learning parallel those of speech category acquisition</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>683</fpage><lpage>698</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21392</pub-id><pub-id pub-id-type="pmid">19929331</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>López-Barroso</surname><given-names>D</given-names></name><name><surname>Cucurell</surname><given-names>D</given-names></name><name><surname>Rodríguez-Fornells</surname><given-names>A</given-names></name><name><surname>Diego-Balaguer</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Attentional effects on rule extraction and consolidation from speech</article-title><source>Cognition</source><volume>152</volume><fpage>61</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2016.03.016</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Love</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Comparing supervised and unsupervised category learning</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>9</volume><fpage>829</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.3758/bf03196342</pub-id><pub-id pub-id-type="pmid">12613690</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Medin</surname><given-names>DL</given-names></name><name><surname>Gureckis</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sustain: a network model of category learning</article-title><source>Psychological Review</source><volume>111</volume><fpage>309</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.2.309</pub-id><pub-id pub-id-type="pmid">15065912</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>WT</given-names></name><name><surname>Glass</surname><given-names>BD</given-names></name><name><surname>Wolosin</surname><given-names>SM</given-names></name><name><surname>Savarie</surname><given-names>ZR</given-names></name><name><surname>Bowen</surname><given-names>C</given-names></name><name><surname>Matthews</surname><given-names>MD</given-names></name><name><surname>Schnyer</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The effects of sleep deprivation on information-integration categorization performance</article-title><source>Sleep</source><volume>32</volume><fpage>1439</fpage><lpage>1448</lpage><pub-id pub-id-type="doi">10.1093/sleep/32.11.1439</pub-id><pub-id pub-id-type="pmid">19928383</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maquet</surname><given-names>P</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name><name><surname>Perrin</surname><given-names>F</given-names></name><name><surname>Ruby</surname><given-names>P</given-names></name><name><surname>Melchior</surname><given-names>G</given-names></name><name><surname>Boly</surname><given-names>M</given-names></name><name><surname>Vu</surname><given-names>TD</given-names></name><name><surname>Desseilles</surname><given-names>M</given-names></name><name><surname>Peigneux</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Festina lente: evidences for fast and slow learning processes and a role for sleep in human motor skill learning</article-title><source>Learning &amp; Memory</source><volume>10</volume><fpage>237</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1101/lm.64303</pub-id><pub-id pub-id-type="pmid">12888539</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirman</surname><given-names>D</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Categorization and discrimination of nonspeech sounds: differences between steady-state and rapidly-changing acoustic cues</article-title><source>The Journal of the Acoustical Society of America</source><volume>116</volume><fpage>1198</fpage><lpage>1207</lpage><pub-id pub-id-type="doi">10.1121/1.1766020</pub-id><pub-id pub-id-type="pmid">15376685</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roark</surname><given-names>CL</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task and distribution sampling affect auditory category learning</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>80</volume><fpage>1804</fpage><lpage>1822</lpage><pub-id pub-id-type="doi">10.3758/s13414-018-1552-5</pub-id><pub-id pub-id-type="pmid">29968085</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Roark</surname><given-names>CL</given-names></name><name><surname>Lehet</surname><given-names>M</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Factors Influencing Incidental Category Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/mhbxc</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roark</surname><given-names>CL</given-names></name><name><surname>Lehet</surname><given-names>MI</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The representational glue for incidental category learning is alignment with task-relevant behavior</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>48</volume><fpage>769</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1037/xlm0001078</pub-id><pub-id pub-id-type="pmid">34570548</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>EM</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Miall</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Current concepts in procedural consolidation</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>576</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1038/nrn1426</pub-id><pub-id pub-id-type="pmid">15208699</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>DAE</given-names></name><name><surname>Kishon-Rabin</surname><given-names>L</given-names></name><name><surname>Hildesheimer</surname><given-names>M</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A latent consolidation phase in auditory identification learning: time in the awake state is sufficient</article-title><source>Learning &amp; Memory</source><volume>12</volume><fpage>159</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1101/87505</pub-id><pub-id pub-id-type="pmid">15805314</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seitz</surname><given-names>AR</given-names></name><name><surname>Dinse</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A common framework for perceptual learning</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>148</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.02.004</pub-id><pub-id pub-id-type="pmid">17317151</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seitz</surname><given-names>AR</given-names></name><name><surname>Protopapas</surname><given-names>A</given-names></name><name><surname>Tsushima</surname><given-names>Y</given-names></name><name><surname>Vlahou</surname><given-names>EL</given-names></name><name><surname>Gori</surname><given-names>S</given-names></name><name><surname>Grossberg</surname><given-names>S</given-names></name><name><surname>Watanabe</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Unattended exposure to components of speech sounds yields same benefits as explicit auditory training</article-title><source>Cognition</source><volume>115</volume><fpage>435</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2010.03.004</pub-id><pub-id pub-id-type="pmid">20346448</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stickgold</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Sleep-dependent memory consolidation</article-title><source>Nature</source><volume>437</volume><fpage>1272</fpage><lpage>1278</lpage><pub-id pub-id-type="doi">10.1038/nature04286</pub-id><pub-id pub-id-type="pmid">16251952</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stickgold</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How do I remember? let me count the ways</article-title><source>Sleep Medicine Reviews</source><volume>13</volume><fpage>305</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.smrv.2009.05.004</pub-id><pub-id pub-id-type="pmid">19699665</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlahou</surname><given-names>EL</given-names></name><name><surname>Protopapas</surname><given-names>A</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Implicit training of nonnative speech stimuli</article-title><source>Journal of Experimental Psychology. General</source><volume>141</volume><fpage>363</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1037/a0025014</pub-id><pub-id pub-id-type="pmid">21910556</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wade</surname><given-names>T</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Incidental categorization of spectrally complex non-invariant auditory stimuli in a computer game task</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>2618</fpage><lpage>2633</lpage><pub-id pub-id-type="doi">10.1121/1.2011156</pub-id><pub-id pub-id-type="pmid">16266182</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Response Accuracy in the SMART Task</title><p>Analyses of visuomotor response accuracy (correctly reporting the location of the suprathreshold visual target) were calculated to exclude the possibility of a RT-accuracy tradeoff.</p></sec><sec sec-type="appendix" id="s9"><title>Offline gains in accuracy in Experiment 1</title><p>Accuracy was stable across the first 8 blocks of training on Day-1,[ <italic>F</italic> (7, 147)=.57, <italic>P</italic>=.77; <italic>η<sub>p</sub></italic>²=.02, <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>]. However, accuracy in reporting visual location was significantly higher (more accurate) in the first block of Day-2 (<italic>M</italic>=.98, <italic>S.E</italic>.=.003) than in the final block of Day-1 (<italic>M</italic>=.97, <italic>S.E</italic>.=.005), <italic>t</italic> (21)=2.65, <italic>P</italic>=.01, Cohen’s <italic>d</italic>=.57. Moreover, accurate responses were robustly maintained across a nine-day interval [final block of Day-2, <italic>M</italic>=.97, <italic>S.E</italic>.=.004, to the 1<sup>st</sup> block of Day-10, <italic>M</italic>=.98, <italic>S.E</italic>.=.004; <italic>t</italic>(21) = 1.08, <italic>P</italic>=.29]. Therefore, the gains in RT (for reporting the target) were not at the cost of accuracy.</p></sec><sec sec-type="appendix" id="s10"><title>No accuracy Cost in Day-1 (Experiments 2, 3)</title><p>(<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B and C</xref>). There was no cost in accuracy levels on Day-1 in Exp 2 [<italic>t</italic>(23)=–1.11, <italic>P</italic>=.27; Cohen’s <italic>d</italic>=.23. <italic>M</italic><sub>Block7</sub>=.97, <italic>S.E</italic>.=.006, <italic>M</italic><sub>Block6</sub>=.97, <italic>S.E</italic>.=.003] nor in Exp 3 in which participants experienced within-category variability on each visuomotor trial, [<italic>t</italic> (21)=–.27, <italic>P</italic>=.78; Cohen’s <italic>d</italic>=.03. <italic>M</italic><sub>Block7</sub>=.97, <italic>S.E</italic>.=.005, <italic>M</italic><sub>Block6</sub>=.97, <italic>S.E</italic>.=.004]. Therefore, RT Cost effects on Day-1 in experiment 2, 3 were not driven by a change (increase) in accuracy.</p></sec><sec sec-type="appendix" id="s11"><title>RT facilitation in Experiment 3</title><p>In Exp 3, accurate responses to the visual target did not change across the 6 blocks preceding the Random block on Day-1, [<italic>F</italic> (5, 105)=.03, <italic>P</italic>=.99, <italic>η<sub>p</sub>²</italic>=.001]. Therefore, gains in speed observed in Exp 3 (RT facilitation) were not at the cost of accuracy.</p></sec><sec sec-type="appendix" id="s12"><title>Overnight offline gains in Experiment 2 no loss in accuracy in Exp 3</title><p>As can be seen in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B and C</xref> responses to the visual target improved overnight; on Day-2 participants were more accurate than on Day-1 in Exp 2, [<italic>t</italic>(23)=2.21, <italic>P</italic>=.03; Cohen’s <italic>d</italic>=.45]. In Exp 3, accuracy on Day 2 did not differ from that attained in Day 1 [<italic>t</italic>(21)=1.25, <italic>P</italic>=.22; Cohen’s <italic>d</italic>=.5]. Again, this suggests that delayed gains in speed were not at the cost of accuracy.</p></sec><sec sec-type="appendix" id="s13"><title>Accuracy Cost in Day-1 vs. Day-2 (Experiments 2–3)</title><p>There was no significant decline in the magnitude of the accuracy cost (random minus repeated blocks) from Day-1 to Day-2 for either Exp 2, [<italic>t</italic> (23)=–.78, <italic>P</italic>=.44; Cohen’s <italic>d</italic>=.19], or Exp 3, [<italic>t</italic> (21)=2.05, <italic>P</italic>=.052; Cohen’s <italic>d</italic>=.53]. Therefore, changes in RT Cost observed in Experiment 2 were not driven by changes (increase) in accuracy levels.</p></sec><sec sec-type="appendix" id="s14"><title>Robust retention (Experiments 2–3)</title><p>There was no significant decline in visuomotor response accuracy from Day-2 to Day-10 in either Exp 2, [<italic>t</italic>(23)=1.25, <italic>P</italic>=.22; Cohen’s <italic>d</italic>=.35], or Exp 3, [<italic>t</italic>(21)=–1.05, <italic>P</italic>=.30; Cohen’s <italic>d</italic>=.44]. Thus, the ability to retain incidentally acquired auditory category knowledge manifested in a consistent RT Cost but not because of a speed accuracy tradeoff in task performance across sessions.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Visuomotor SMART Task Behavior (Accuracy).</title><p>Across all panels, the leftmost graph shows the mean and standard error of accuracy in responding to the visual target, with individual participants’ data plotted as light grey dots across blocks in Day 1, Day 2 and Day 10 sessions. The middle graph plots the Accuracy Cost of the Random block (Blocks 7, 10, 13) as a function of the preceding block. The rightmost graph shows the offline gain from the last block of a preceding session to the first block of the next session (Day 1–2, Day 2–10). (<bold>A</bold>) Exp 1 characterizes putative visuomotor learning, consolidation and retention without sounds preceding visual targets. (<bold>B</bold>) In Exp 2, a consistent category-to-location association is conveyed by a single category exemplar, repeated five times on a trial; different exemplars occurred on different trials. (<bold>C</bold>) In Exp 3, the consistent category-to-location association was conveyed by five unique category exemplars sampled from the category on each trial.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81855-app1-fig1-v2.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81855.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chait</surname><given-names>Maria</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>This paper is an important contribution to our understanding of fundamental learning processes. It will be of interest to psychologists and neuroscientists studying how humans form complex perceptual categories. The authors take advantage of a clever behavioral paradigm they developed in earlier work to provide strong evidence demonstrating how incidental auditory category learning benefits from increased stimulus variability and offline periods containing sleep.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81855.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Chait</surname><given-names>Maria</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Memory for Incidentally Learned Categories Evolves in the Post-Learning Interval&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>We all agreed this was an interesting, well conducted study that makes an important contribution to the literature. As you will see below, the reviewers have highlighted several issues that we hope can be addressed in a revision. The key points are summarized below. Point #1 is the most concerning and might require new data.</p><p>1) In the last block of Day 1 (block 8), participants might be performing slower than one might expect, as they encountered interference in block 7. It is possible they may have performed faster in a further block, which could lead to an elimination of the gain now interpreted as consolidation. One possibility to address this (without collecting additional data) would be to examine the RT slope within block 8 (relative to block 6; If there are enough trials to merit this). Specifically: are participants ending this block close to how they ended block 6, even if they might start out initially slower?</p><p>2) Please add explicit stats comparing effects across experiments.</p><p>3) Please add a sample size justification (including for comparison between experiments). This can be addressed with equivalence tests or Bayesian analyses.</p><p>4) Please clarify the relationship between explicit learning and RT cost.</p><p>5) Does training on the same day drive generalization (see clarified in individual reviews below)?</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Understanding how multidimensional categories are learned is of both theoretical and practical importance. The mechanisms supporting the rapid and seemingly effortless acquisition of complex category systems (e.g., phonetic categories in speech among infants) have been debated in science and philosophy for centuries. From a more pragmatic perspective, the rapid ability to learn complex categories confers clear survival benefits (e.g., integrating information about the size, shape, color, and location of a foraged berry to correctly categorize it as either edible or poisonous).</p><p>This research addresses some fundamental questions about how humans form complex perceptual categories incidentally – i.e., without overt instruction. To address this question, the authors use a clever experimental paradigm: the &quot;Systematic Multimodal Association Reaction Time&quot; (SMART) task, which was developed by a subset of the current authors in an earlier paper (Gabay et al. J Exp Psychol Hum Percept Perform 2015). This paradigm is deceptively simple from the perspective of the participant (pressing one of four designated keys depending on which of four visual boxes is currently highlighted, subsequently referred to as the visuomotor event) but allows researchers to effectively test for incidental auditory category learning. This is done by playing different sound categories prior to the visuomotor event in a manner that predicts which box will be highlighted (e.g., sound category A indicating that the leftmost box will be highlighted) and examining how this predictive cue facilitates participant response times.</p><p>Specifically, the authors assess how acoustic variability influences efficacy of category learning, both in the short-term (in the same experimental session as initial incidental learning) and in the long-term (one day and nine days after the initial incidental learning). The authors find that within-trial variability (i.e., playing five different exemplar sounds from a sound category prior to the visuomotor event) results in robust incidental category learning in both the short- and long-term, whereas between-trial variability (i.e., playing the same exemplar sound five times from a sound category prior to the visuomotor event) only results in robust incidental category learning after an offline period (i.e., not in the initial experimental session). Furthermore, the authors find that incidental learning is correlated with explicit memory of the audiovisual mappings for both participants who experienced within-trial variability and participants who experienced between-trial variability.</p><p>Strengths</p><p>The paper uses an established paradigm for incidental category learning to provide important new insights into (1) the conditions under which incidental learning is optimized, and (2) the time course of such learning. The results are reported clearly, and the findings help bridge several areas of research (sleep and memory consolidation, perceptual variability and generalization, relationships between implicit and explicit measures of learning).</p><p>The paper also makes effective use of control groups. In particular, I found the inclusion of the visual-only control group (i.e., only responding to the visuomotor event without any preceding auditory cues) to be highly informative. This control group showed stable response times within the first session, but significantly faster response times at the onset of the second day. This finding, as the authors suggest, may reflect a kind of visuomotor skill consolidation. However, without the inclusion of this control group, this effect may have been erroneously attributed to offline effects driven by incidental auditory learning (as the incidental auditory learning participants also showed this significant response time facilitation across session).</p><p>Weaknesses</p><p>The present research provides some intriguing findings that will be of interest to learning and memory researchers, but there are some factors that limit the strength of the conclusions that can be made.</p><p>First, there are questions about the generalizability of the findings. The authors use four highly artificial auditory categories that have been used in prior research, which have similar spectro-temporal properties as speech but are described as decidedly nonspeech-like (and are additionally reported by the authors to be difficult to verbalize). It is thus unclear how these findings would generalize to other perceptual categories, including those found outside of experimental contexts (e.g., phonetic categories found in different languages). There is also a question of how well the findings would generalize to a broader, more diverse population. The current participants were mostly young adults (mean {plus minus} standard deviation of 27 {plus minus} 5, 26 {plus minus} 5, 25 {plus minus} 3, and 27 {plus minus} 5 years old, rounding to the nearest whole number, for each reported condition). Thus, the present approach cannot comment on how incidental category learning changes across the lifespan, limiting the extent to which one can make broad claims about incidental category learning.</p><p>Second, the sample size was only justified in relation to a specific, within-participant analysis (calculating the &quot;response time cost&quot; of random versus predictive auditory cues). Although I appreciated the sample size justification based on a previously reported effect size, the present research has notable differences in its design, including critical between-participant analyses. As such, I worry that the current sample is underpowered to detect anything but large effect sizes, particularly when examining differences across participant groups. This is particularly important for situations in which the authors wish to make claims of equivalence (i.e., non-significant differences across groups being treated as comparable performance).</p><p>I really enjoyed this article. Below are some suggestions for improving the manuscript. I will start with specific ways of potentially addressing the perceived weaknesses outlined in the public review.</p><p>1. Generalizability of findings: I do not think the authors need to collect any additional data to address this (e.g., replicating in a broader age range or replicating using different classes of auditory categories). However, I do think the manuscript would be improved by a discussion of this as a potential limitation of the generalizability of the current claims.</p><p>2. Sample size and a prior power analysis: Here, I think the paper would benefit from a reconsideration of the between-group analyses (and how this might change the needed sample size to be adequately powered). At minimum, the authors need to provide more details about how the power analysis was conducted. The authors might also want to consider using Bayes Factors – particularly in situations where they wish to interpret non-significant effects (e.g., comparable performance across experiments).</p><p>Additional Comments and Questions</p><p>In addition to the comments outlined in the public review, I think the manuscript could be strengthened by addressing the following themes:</p><p>A. Addressing the Subjective Perspective of the Participant</p><p>The research does not provide any information about participants' subjective experiences completing the task. Although introspection has limitations, in the present context it could have been quite valuable. This is because the authors emphasize how the auditory categories were learned &quot;without overt instruction, perceptual decisions, directed attention, or feedback&quot; (line 295). However consistently hearing sounds prior to a response establishes an environment conducive to &quot;foraging&quot; (to borrow the authors' wording), and thus I wonder about the extent to which at least some participants developed more explicit ad-hoc categories for associating sounds to visual responses. Looking at performance on the explicit categorization task in Figure 3, performance is non-normally distributed (and, in the case of Experiment 3, appears pseudo-dichotomous, which participants either performing near chance or near ceiling). It would be highly informative to know if category learning is in any way related to insight processes, or general differences in the degree to which participants attempted to explicitly associate sounds with visuomotor events.</p><p>B. Considering More Extreme Manipulations of Variability</p><p>The specific role of acoustic variability in relation to category learning could have been more comprehensively addressed. Although participants who received within-trial acoustic variability showed evidence of learning in the initial session whereas participants who received between-trial acoustic variability did not, both groups showed robust learning in the following days and both groups showed evidence of explicit learning. Overall, then, it seems as though there were more similarities than differences in terms of how these groups performed. This made me wonder how participants who received zero variability (i.e., genuinely experienced a &quot;one-to-one&quot; mapping of a single exemplar sound to a single visuomotor response) would fare in terms of learning (both offline changes and explicit generalization testing).</p><p>C. More Carefully Defining &quot;Offline&quot;</p><p>The authors find intriguing results across sessions, suggesting a role of offline consolidation processes in incidental category learning. However, the authors do not clarify whether they are defining &quot;offline&quot; as simply a period of time in which participants are not completing the task, or rather as a period that necessarily includes sleep (in which active perceptual exploration is &quot;offline&quot;). The inclusion of the three-hour delay control group was important for making stronger claims about the potential role of sleep in learning, but the authors (1) should be explicit in terms of what they mean by &quot;offline&quot; and (2) acknowledge that other factors would also need to be considered (e.g., time-of-day / circadian effects) to make strong arguments about the role of sleep.</p><p>Specific Comments:</p><p>L43: There appears to be a missing word (&quot;necessary for success&quot;)</p><p>L117-122: This is a critical part of the manuscript. Unfortunately, I found this sentence quite long and hard to follow. Consider breaking this into two or more sentences.</p><p>L174-175: Consider replacing the word &quot;simple&quot; here. A five-to-one mapping is still greatly simplified relative to challenges in category learning faced in the real world.</p><p>L185-186: Throughout the reporting of the results, it appears (nominally) that participants in Experiment 3 are overall faster at responding than participants in Experiment 2. Was this included in your models?</p><p>L228: I am not sure why the t-statistic here has two separate degrees of freedom. Please check.</p><p>L259-260: The differentiation of uni/multidimensional categories was unheralded here (although the difference is well explained in the Stimuli subsection). The difference in performance on uni/multidimensional categories in Experiment 3 was interesting, but was not unpacked anywhere in the discussion (that I saw). Could this possibly relate to Point A – i.e., unidimensional categories having better affordance of being able to be explicitly conceptualized or verbalized?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Gabay and colleagues present a study investigating the lasting effects of incidentally acquired auditory category knowledge. Incidental learning of categories is well established, especially in the auditory domain; however, the extent that such empirically observed effects relate to the type of long-lasting naturalistic learning experienced in everyday life (e.g., speech perception) is an open question. This work looks specifically at this gap in knowledge with a behavioural experiment in which participants were incidentally exposed to auditory stimuli that categorically aligned with responses in an unrelated visuomotor task. Relative to a control task in which no auditory stimuli were present, participants showed greater initial learning (indexed by an RT cost when auditory category to visuomotor response was randomized) when auditory stimuli included variability in stimuli within a trial versus repetition of a single auditory stimulus. Interestingly, after a 24 hour and 10 day delay, this difference was eliminated such that both single and multiple auditory cues led to significant RT costs relative to the control no-auditory condition, indicative of offline gains in category learning. Moreover, the degree of RT cost measured on days 1, 2, and 10 in the incidental learning conditions was related to explicit category labeling of novel category exemplars measured at the end of the experiment. Thus, the authors argue that incidental auditory category learning is long lasting, experiences offline gains potentially driven by consolidation, and supports generalization.</p><p>Overall, this is a strong study with clear findings that support the authors' claims.</p><p>Strengths</p><p>– The aptly named SMART paradigm is a clever approach to investigate incidental learning and its use in the current study offers a novel method for characterizing the lasting effects of such learning. Including delays of both 2 and 10 days, rather than a single delay, provides a more powerful test of the last effects. Overall, the experimental paradigm is well thought out and conducted, and targets the authors' hypotheses.</p><p>– The no auditory condition provides a strong baseline for determining the specific contributions of incidental category learning. Such an extended training protocol would necessarily lead to speed ups in the visuomotor task; thus, ruling out these RT benefits with the no auditory condition is a key empirical strength.</p><p>– The RT cost effects provide an indirect and notably clean measure of category knowledge, and the results provide a robust learning effect.</p><p>Weaknesses</p><p>– The relationship between explicit learning and RT cost is muddied by the nature of the explicit labeling data. The data points depicted in Figure 3B/C suggest a cluster of participants with explicit labeling accuracy hovering around chance levels and a second group of participants with above chance performance. A correlation analysis with this sort of clustering may not provide the best characterization of the data and weakens the interpretation of these effects.</p><p>– The authors argue that incidental category knowledge supports generalization to novel stimuli and their explicit labeling findings mesh well with this interpretation. One limitation to this claim is that the contribution of long lasting category knowledge to this generalization effect is tempered by the fact that training did occur on the same day as the explicit learning task. Thus, the possibility remains that training on the same day drives generalization. A strong test would separate training and generalization to different days to pinpoint the contribution of retained category knowledge on generalization.</p><p>– The notable gains in learning after exposure to across-trial variability (exp 2) to match the degree of learning shown with within-trial variability (exp 3) deserves more speculation on the potential mechanism underlying this effect.</p><p>– More generally, the discussion is fairly limited in scope. The findings seemingly speak to fundamental learning processes relevant to domains of category learning beyond audition, episodic memory, and decision making. Rather than expand on the broader implications and connect to relevant cognitive and neural theories in domains, the discussion focuses on auditory category learning which limits its broader appeal.</p><p>– The relationship between RT cost and explicit labeling (Figure 3) may be better characterized by a median split of participants' explicit labeling accuracy (or grouping the participants based on significantly above vs. below chance accuracy). Then comparing these groups' RT costs could provide a similar result but one better suited to the clustering in the data.</p><p>– I would highly recommend that the authors expand the discussion to connect their findings to the broader literature on episodic memory and category learning. There are central debates in the field on how regularities in sensory experiences are built from seemingly incidental exposures (i.e., how episodic memory leads to structured knowledge). The current findings offer an important contribution to this key question.</p><p>– I am curious about how RT costs change for a given participant across days. If one participant shows an RT cost on day 1, do they also show it on days 2 and 10? Is there a correlation across days? This is certainly a supplemental analysis, but one that could help further characterize the nature of the incidental learning effects.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>While there is good evidence for incidental learning of auditory categories, there is limited work focusing on how categories are consolidated. In an elegant series of experiments, the authors focus on what happens to category knowledge following sleep. Specifically, using the SMART task, they examine evidence for gains in knowledge following sleep. In Experiment 1, authors used a visuomotor version of the SMART task (participants had to press a button whose location was mapped to a visual target), and observed that participants became faster and showed offline gains (24 hours later and 9 days later). In Experiment 2, authors introduced a series of 5 identical sounds that preceded the appearance of the visual target. The sounds differed across trials, with targets mapped to auditory categories. Authors observed little evidence for learning on Day 1, but evidence for offline learning on Day 2 and 10. In Experiment 3, participants encountered a series of 5 different sounds from one auditory category that preceded the appearance of the visual target. There was evidence of learning on Day 1, and evidence of consolidation on Day 2 and Day 10. The authors conclude that learning continues even after the initial learning experience, suggesting that category knowledge is elaborated in the consolidation process.</p><p>A strength of this paper is that it is clear participants are learning category knowledge, not just specific information about the exemplars. I found the distinction between visuomotor skill learning and category learning to be nuanced and compelling, and I think this will have important implications for other work in this area.</p><p>The conclusions of this paper are generally supported by the data, but I think a few issues need to be clarified. My chief concern at this stage is that authors are describing a pattern of results across three different experiments, but these differences are not statistically compared. Further, the correlations of RT cost to labelling accuracy assume that RT cost is a reliable measure in individuals. However, no evidence was presented to convince us this is the case.</p><p>I would like differences in RT cost and offline gains statistically compared across experiments. This is particularly important to support claims made in the discussion.</p><p>What is the dropout rate? Can the authors address whether participants who did not learn the categories dropped out in sessions on Day 2 and Day 10?</p><p>I was concerned that in the last block of Day 1 (block 8), participants might be performing slower than one might expect, as they encountered interference in block 7. It is possible they may have performed faster in a further block, which could lead to an elimination of the gain now interpreted as consolidation.</p><p>One condition I think is missing is a version of the SMART task with no auditory category information, but where participants encounter sounds (either randomly, or in a deterministic fashion). This would allow the authors to show that RT cost is truly associated with auditory category learning, rather than some distraction from a change in the auditory environment.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81855.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>We all agreed this was an interesting, well conducted study that makes an important contribution to the literature. As you will see below, the reviewers have highlighted several issues that we hope can be addressed in a revision. The key points are summarized below. Point #1 is the most concerning and might require new data.</p><p>1) In the last block of Day 1 (block 8), participants might be performing slower than one might expect, as they encountered interference in block 7. It is possible they may have performed faster in a further block, which could lead to an elimination of the gain now interpreted as consolidation. One possibility to address this (without collecting additional data) would be to examine the RT slope within block 8 (relative to block 6; If there are enough trials to merit this). Specifically: are participants ending this block close to how they ended block 6, even if they might start out initially slower?</p></disp-quote><p>AR-E-1: Thank you for raising this important comment. To address this concern, we have added an additional analysis (see also end of p. 9 in the revised manuscript):</p><p>For Experiment 2, the first half of Block 8 did not differ significantly from the last half of Block 8, t(46) = -.204, p = .839, Cohen’s d = .005. Importantly, there was no significant difference between Block 6 versus Block 8, t(46) = -.717, p = .476, Cohen’s d = .207. Thus, there was no evidence for learning across Block 7, or of a loss of performance from Blocks 6 to 8.</p><p>The same holds for Experiment 3. The first half of Block 8 did not differ significantly from the last half of Block 8, t(44) = .340, p = .735, Cohen’s d = .100. Again, there was no significant difference between Block 6 versus Block 8, t(42) = -.480, p = .633, Cohens' d = .14.</p><p>To answer the Editor’s question, Block 8 performance did not differ from Block 6 performance in either Experiment 2 or Experiment 3.</p><disp-quote content-type="editor-comment"><p>2) Please add a sample size justification (including for comparison between experiments). This can be addressed with equivalence tests or Bayesian analyses.</p></disp-quote><p>AR-E-2: We have added a sample size justification, including comparison between experiments (see also p. 19 of the revised manuscript):</p><p>Previous research using the same stimuli, paradigm and cross-participant manipulation of exemplar variability revealed large between-subject effect sizes for RT Cost (i.e., Cohen's d = 0.76). A power analysis (calculated using G*Power software; Faul et al., 2007) indicates that a one-tailed between-subject effect requires 44 participants to reach statistical power at a 0.80 level (α = .05). Therefore, with a total sample of 46 participants the study was adequately powered to detect differences in the exemplar variability manipulation examined across participant groups.</p><disp-quote content-type="editor-comment"><p>3) Please clarify the relationship between explicit learning and RT cost.</p></disp-quote><p>AR-E-3: The revised manuscript includes an examination of the relationship between explicit learning (by grouping participants based on a median split of participants' explicit labeling accuracy) and RT Cost (see p.12, second paragraph).</p><p>We performed a median split of Experiment 2 participants based on Day 10 explicit labeling accuracy, generating high-performing and low-performing subgroups. The high-performing subgroup had marginally significant greater RT Costs on Day 2 [t (22) = -1.79, p = .08, Cohen's d = .-.733] as well as on Day 10 [t (22) = -2.07, p = .05, Cohen's d = .-.846], but not on Day 1, [t (22) = -1.30, p = .21, Cohen's d = .-.533].</p><p>Similarly, we performed a median split of Experiment 3 participants based on Day 10 explicit labeling accuracy, generating high-performing and low-performing subgroups. The high-performing subgroup had significantly greater RT cost on Day 2, [t (20) = -3.82, p=.001; Cohen's d=-.721], Day 10, [t (20) = -2.68, p=.014, Cohen's d=-1.144], but not on Day 1, [t (20) = -1.69, p=.106; Cohen's d=-.721].</p><p>These results suggest that better generalization (explicit labeling) on Day 10 is associated with better learning (the increased reliance on the evolving auditory category knowledge reflected in the RT cost measure) on Day 10.</p><disp-quote content-type="editor-comment"><p>4) Does training on the same day drive generalization (see clarified in individual reviews below)?</p></disp-quote><p>AR-E-4: Reviewer 2 (comment 3, R2-3) is fair in pointing to the possibility that our testing approach may involve generalization based on same-day reactivation or learning. This possibility can be addressed only in across-group comparisons. It will be important for future investigations to determine the contribution of actively retrieving long-lasting category knowledge to generalization by testing for generalization without any direct reminder or reactivation experience on the SMART task. This could elucidate a possible contribution of reactivating retained category knowledge to generalization.</p><p>In the revised manuscript, we expand on the motivation for our testing approach (see p. 16, second paragraph).</p><p>RT facilitation measures may capture both the development of auditory category knowledge and more general skills such as the visual stimulus-response mapping in the SMART task. We tried to dissociate these types of knowledge by comparing RT in a block in which auditory-visual regularities were present and a subsequent block in which the regularities were disrupted, yielding a RT Cost. The RT Cost afforded a measure of how much a participant’s performance depended on the correspondence of the auditory category and the visuo-motor task demands. Importantly, the auditory-visual correspondence can afford a RT Cost only if a participant is able to form some level of category knowledge.</p><p>Following the random block, we added a final block re-introducing the auditory-visual regularities to enable participants to return to the trained task conditions after possible interference from the random block. Thus, regularity-random-regularity structure was employed across sessions to assess the development of auditory category knowledge across time, independently of the general gains that could be reflected in the random block performance, as well.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Understanding how multidimensional categories are learned is of both theoretical and practical importance. The mechanisms supporting the rapid and seemingly effortless acquisition of complex category systems (e.g., phonetic categories in speech among infants) have been debated in science and philosophy for centuries. From a more pragmatic perspective, the rapid ability to learn complex categories confers clear survival benefits (e.g., integrating information about the size, shape, color, and location of a foraged berry to correctly categorize it as either edible or poisonous).</p><p>This research addresses some fundamental questions about how humans form complex perceptual categories incidentally – i.e., without overt instruction. To address this question, the authors use a clever experimental paradigm: the &quot;Systematic Multimodal Association Reaction Time&quot; (SMART) task, which was developed by a subset of the current authors in an earlier paper (Gabay et al. J Exp Psychol Hum Percept Perform 2015). This paradigm is deceptively simple from the perspective of the participant (pressing one of four designated keys depending on which of four visual boxes is currently highlighted, subsequently referred to as the visuomotor event) but allows researchers to effectively test for incidental auditory category learning. This is done by playing different sound categories prior to the visuomotor event in a manner that predicts which box will be highlighted (e.g., sound category A indicating that the leftmost box will be highlighted) and examining how this predictive cue facilitates participant response times.</p><p>Specifically, the authors assess how acoustic variability influences efficacy of category learning, both in the short-term (in the same experimental session as initial incidental learning) and in the long-term (one day and nine days after the initial incidental learning). The authors find that within-trial variability (i.e., playing five different exemplar sounds from a sound category prior to the visuomotor event) results in robust incidental category learning in both the short- and long-term, whereas between-trial variability (i.e., playing the same exemplar sound five times from a sound category prior to the visuomotor event) only results in robust incidental category learning after an offline period (i.e., not in the initial experimental session). Furthermore, the authors find that incidental learning is correlated with explicit memory of the audiovisual mappings for both participants who experienced within-trial variability and participants who experienced between-trial variability.</p><p>Strengths</p><p>The paper uses an established paradigm for incidental category learning to provide important new insights into (1) the conditions under which incidental learning is optimized, and (2) the time course of such learning. The results are reported clearly, and the findings help bridge several areas of research (sleep and memory consolidation, perceptual variability and generalization, relationships between implicit and explicit measures of learning).</p><p>The paper also makes effective use of control groups. In particular, I found the inclusion of the visual-only control group (i.e., only responding to the visuomotor event without any preceding auditory cues) to be highly informative. This control group showed stable response times within the first session, but significantly faster response times at the onset of the second day. This finding, as the authors suggest, may reflect a kind of visuomotor skill consolidation. However, without the inclusion of this control group, this effect may have been erroneously attributed to offline effects driven by incidental auditory learning (as the incidental auditory learning participants also showed this significant response time facilitation across session).</p></disp-quote><p>AR-R1-1: Thank you for these positive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses</p><p>The present research provides some intriguing findings that will be of interest to learning and memory researchers, but there are some factors that limit the strength of the conclusions that can be made.</p><p>First, there are questions about the generalizability of the findings. The authors use four highly artificial auditory categories that have been used in prior research, which have similar spectro-temporal properties as speech but are described as decidedly nonspeech-like (and are additionally reported by the authors to be difficult to verbalize). It is thus unclear how these findings would generalize to other perceptual categories, including those found outside of experimental contexts (e.g., phonetic categories found in different languages). There is also a question of how well the findings would generalize to a broader, more diverse population. The current participants were mostly young adults (mean {plus minus} standard deviation of 27 {plus minus} 5, 26 {plus minus} 5, 25 {plus minus} 3, and 27 {plus minus} 5 years old, rounding to the nearest whole number, for each reported condition). Thus, the present approach cannot comment on how incidental category learning changes across the lifespan, limiting the extent to which one can make broad claims about incidental category learning.</p></disp-quote><p>AR-R1-2: Thank you for the constructive feedback. In the revised manuscript we endeavored to better make these points.</p><p>Categories. In the present study we used novel, non-linguistic auditory categories composed of artificial nonspeech sounds that were designed to model the lack of invariance in speech perception (Wade and Holt, 2005). Our use of non-speech categories rather than familiar/nonnative speech categories has the advantage of controlling prior speech experience which is evident even among neonates (DeCasper and Spence, 1986) and for which first-language effects even on unfamiliar nonnative speech categories can be profound.</p><p>We do believe that this learning is robust to different input distributions and acoustic dimensions that define categories. Incidental category learning in the SMART task is evident for other novel auditory categories (Roark and Holt, 2018) and for nonnative speech categories (Liu and Holt, 2014). Even more distant, the SMART task has been used to train novel haptic categories (Martinez et al., 2020).</p><p>Broader, More Diverse Sample. We believe these results generalize to a broader, more diverse sample. In Roark et al., 2021 parallel in-laboratory studies were reported with a sample that looked much like the one reported here and online samples that drew participants from across the globe. With one exception (that departs from the paradigm we employ in the present work), there was remarkable congruence across the WEIRD (Henrich et al., 2010) in-lab university sample and the more diverse online sample.</p><p>We acknowledge that it will be left to future work to determine how incidental learning may play out over time across different developmental periods over the lifespan, especially since developmental changes are apparent in offline learning gains in other domains (Dorfberger et al., 2007; Fischer et al., 2007). Yet, we note that data from young, healthy adults are an important baseline against which to compare other developmental and clinical samples.</p><disp-quote content-type="editor-comment"><p>Second, the sample size was only justified in relation to a specific, within-participant analysis (calculating the &quot;response time cost&quot; of random versus predictive auditory cues). Although I appreciated the sample size justification based on a previously reported effect size, the present research has notable differences in its design, including critical between-participant analyses. As such, I worry that the current sample is underpowered to detect anything but large effect sizes, particularly when examining differences across participant groups. This is particularly important for situations in which the authors wish to make claims of equivalence (i.e., non-significant differences across groups being treated as comparable performance).</p></disp-quote><p>AR-R1-3: The revised manuscript includes this new description (see p. 19):</p><p>Previous research using the same stimuli, paradigm and cross-participant manipulation of exemplar variability revealed large between-subject effect sizes for RT Cost (i.e., Cohen's d = 0.76). A power analysis (calculated using G*Power software; Faul et al., 2007) indicates that a one-tailed between-subject effect requires 44 participants to reach statistical power at a 0.80 level (α = .05). Therefore, with a total sample of 46 participants the study was adequately powered to detect differences in the exemplar variability manipulation examined across participant groups.</p><disp-quote content-type="editor-comment"><p>I really enjoyed this article. Below are some suggestions for improving the manuscript. I will start with specific ways of potentially addressing the perceived weaknesses outlined in the public review. I will then shift to some more specific questions and comments that were too focused for the public review.</p><p>1. Generalizability of findings: I do not think the authors need to collect any additional data to address this (e.g., replicating in a broader age range or replicating using different classes of auditory categories). However, I do think the manuscript would be improved by a discussion of this as a potential limitation of the generalizability of the current claims.</p></disp-quote><p>AR-R1-4: Thank you for this comment. The revised manuscript discusses this in more detail, as described further in AR-R1-2.</p><disp-quote content-type="editor-comment"><p>2. Sample size and a prior power analysis: Here, I think the paper would benefit from a reconsideration of the between-group analyses (and how this might change the needed sample size to be adequately powered). At minimum, the authors need to provide more details about how the power analysis was conducted. The authors might also want to consider using Bayes Factors – particularly in situations where they wish to interpret non-significant effects (e.g., comparable performance across experiments).</p></disp-quote><p>AR-R1-5: The revised manuscript includes this new description (see p. 19):</p><p>Previous research using the same stimuli, paradigm and cross-participant manipulation of exemplar variability revealed large between-subject effect sizes for RT Cost (i.e., Cohen's d = 0.76). A power analysis (calculated using G*Power software; Faul et al., 2007) indicates that a one-tailed between-subject effect requires 44 participants to reach statistical power at a 0.80 level (α = .05). Therefore, with a total sample of 46 participants the study was adequately powered to detect differences in the exemplar variability manipulation examined across participant groups.</p><disp-quote content-type="editor-comment"><p>Additional Comments and Questions</p><p>I think the manuscript could be strengthened by addressing the following themes:</p><p>A. Addressing the Subjective Perspective of the Participant</p><p>The research does not provide any information about participants' subjective experiences completing the task. Although introspection has limitations, in the present context it could have been quite valuable. This is because the authors emphasize how the auditory categories were learned &quot;without overt instruction, perceptual decisions, directed attention, or feedback&quot; (line 295). However consistently hearing sounds prior to a response establishes an environment conducive to &quot;foraging&quot; (to borrow the authors' wording), and thus I wonder about the extent to which at least some participants developed more explicit ad-hoc categories for associating sounds to visual responses. Looking at performance on the explicit categorization task in Figure 3, performance is non-normally distributed (and, in the case of Experiment 3, appears pseudo-dichotomous, which participants either performing near chance or near ceiling). It would be highly informative to know if category learning is in any way related to insight processes, or general differences in the degree to which participants attempted to explicitly associate sounds with visuomotor events.</p></disp-quote><p>AR-R1-6: Thank you for this comment. In the manuscript, we are careful not to describe the learning as ‘implicit’ for these same reasons. We do not have self-reported strategies from this sample to report. However, to aid future research we discuss this point in the revised discussion (see p. 16, last paragraph).</p><disp-quote content-type="editor-comment"><p>B. Considering More Extreme Manipulations of Variability</p><p>The specific role of acoustic variability in relation to category learning could have been more comprehensively addressed. Although participants who received within-trial acoustic variability showed evidence of learning in the initial session whereas participants who received between-trial acoustic variability did not, both groups showed robust learning in the following days and both groups showed evidence of explicit learning. Overall, then, it seems as though there were more similarities than differences in terms of how these groups performed. This made me wonder how participants who received zero variability (i.e., genuinely experienced a &quot;one-to-one&quot; mapping of a single exemplar sound to a single visuomotor response) would fare in terms of learning (both offline changes and explicit generalization testing).</p></disp-quote><p>AR-R1-7: Thanks for this interesting comment. Our interest has been in examining category learning – which necessarily implies discovering the structure underlying variability across exemplars rather than a simple 1:1 stimulus correspondence. But, there are interesting questions to follow up on with regard to simple stimulus-response mappings in a task like SMART.</p><disp-quote content-type="editor-comment"><p>C. More Carefully Defining &quot;Offline&quot;</p><p>The authors find intriguing results across sessions, suggesting a role of offline consolidation processes in incidental category learning. However, the authors do not clarify whether they are defining &quot;offline&quot; as simply a period of time in which participants are not completing the task, or rather as a period that necessarily includes sleep (in which active perceptual exploration is &quot;offline&quot;). The inclusion of the three-hour delay control group was important for making stronger claims about the potential role of sleep in learning, but the authors (1) should be explicit in terms of what they mean by &quot;offline&quot; and (2) acknowledge that other factors would also need to be considered (e.g., time-of-day / circadian effects) to make strong arguments about the role of sleep.</p></disp-quote><p>AR-R1-8: We have revised the manuscript to better define ‘offline gains' as follows (see also p. 14, second paragraph of the revised manuscript):</p><p>&quot;These latter, delayed gains emerging in the post learning interval are believed to reflect memory consolidation -- the process by which memories become less susceptible to interference and are honed to represent new “how to” knowledge (Dudai et al., 2015; Karni, 1996; Karni and Bertini, 1997). Offline gains can be sleep-dependent (Karni et al., 1994; Stickgold, 2005), but can occur also after a wakefulness period (Roth et al., 2005). Since our protocol included a sleep interval, sleep-dependent consolidation may play a role, but future studies using polysomnography to relate sleep parameters with offline gains would be needed to resolve this question. At present, the results indicate the existence of a post-session memory consolidation phase in incidental category learning within which category knowledge is elaborated to a degree that it can be expressed in subsequent performance, even if it was not apparent by the end of the learning session&quot;.</p><disp-quote content-type="editor-comment"><p>Specific Comments:</p><p>L43: There appears to be a missing word (&quot;necessary for success&quot;)</p></disp-quote><p>AR-R1-9: This is now corrected.</p><disp-quote content-type="editor-comment"><p>L117-122: This is a critical part of the manuscript. Unfortunately, I found this sentence quite long and hard to follow. Consider breaking this into two or more sentences.</p></disp-quote><p>AR-R1-10: This is now corrected.</p><disp-quote content-type="editor-comment"><p>L174-175: Consider replacing the word &quot;simple&quot; here. A five-to-one mapping is still greatly simplified relative to challenges in category learning faced in the real world.</p></disp-quote><p>AR-R1-11: This is now corrected.</p><disp-quote content-type="editor-comment"><p>L185-186: Throughout the reporting of the results, it appears (nominally) that participants in Experiment 3 are overall faster at responding than participants in Experiment 2. Was this included in your models?</p></disp-quote><p>AR-R1-12: We examined changes in RT as a function of training blocks (1-6) before the random block was introduced on Day 1. There was no significant difference in RT across Exp 2 and Exp 3 participants, F(1, 44) = 3.612, (p = .063; parietal eta square = .07).</p><disp-quote content-type="editor-comment"><p>L228: I am not sure why the t-statistic here has two separate degrees of freedom. Please check.</p></disp-quote><p>AR-R1-13: This is corrected. Thank you.</p><disp-quote content-type="editor-comment"><p>L259-260: The differentiation of uni/multidimensional categories was unheralded here (although the difference is well explained in the Stimuli subsection). The difference in performance on uni/multidimensional categories in Experiment 3 was interesting, but was not unpacked anywhere in the discussion (that I saw). Could this possibly relate to Point A – i.e., unidimensional categories having better affordance of being able to be explicitly conceptualized or verbalized?</p></disp-quote><p>AR-R1-14: We added the following text to the revised manuscript (see also p. 16, last paragraph):</p><p>&quot;In Experiment 3, post-test categorization was more accurate for unidimensional, compared to multidimensional, categories. This is consistent with prior research (Gabay et al., 2015; Roark et al., 2022) under single-session incidental training conditions. Other studies demonstrate that there is a complex relationship between whether categories are defined by a single, or multiple, input dimensions, whether the distributions defining the categories are deterministic (as they were here) or probabilistic in their sampling, and whether training is incidental or driven by overt feedback (Roark and Holt, 2018). It will be important for future research to assess whether – as in visual category learning under overt training conditions (Ashby and Maddox, 2005) – an advantage for unidimensional category learning in incidental training relates to the affordance of being more easily verbalizable&quot;.</p><p>Thank you very much, R1, for your careful review of our manuscript and for your constructive comments.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Gabay and colleagues present a study investigating the lasting effects of incidentally acquired auditory category knowledge. Incidental learning of categories is well established, especially in the auditory domain; however, the extent that such empirically observed effects relate to the type of long-lasting naturalistic learning experienced in everyday life (e.g., speech perception) is an open question. This work looks specifically at this gap in knowledge with a behavioural experiment in which participants were incidentally exposed to auditory stimuli that categorically aligned with responses in an unrelated visuomotor task. Relative to a control task in which no auditory stimuli were present, participants showed greater initial learning (indexed by an RT cost when auditory category to visuomotor response was randomized) when auditory stimuli included variability in stimuli within a trial versus repetition of a single auditory stimulus. Interestingly, after a 24 hour and 10 day delay, this difference was eliminated such that both single and multiple auditory cues led to significant RT costs relative to the control no-auditory condition, indicative of offline gains in category learning. Moreover, the degree of RT cost measured on days 1, 2, and 10 in the incidental learning conditions was related to explicit category labeling of novel category exemplars measured at the end of the experiment. Thus, the authors argue that incidental auditory category learning is long lasting, experiences offline gains potentially driven by consolidation, and supports generalization.</p><p>Overall, this is a strong study with clear findings that support the authors' claims.</p><p>Strengths</p><p>– The aptly named SMART paradigm is a clever approach to investigate incidental learning and its use in the current study offers a novel method for characterizing the lasting effects of such learning. Including delays of both 2 and 10 days, rather than a single delay, provides a more powerful test of the last effects. Overall, the experimental paradigm is well thought out and conducted, and targets the authors' hypotheses.</p><p>– The no auditory condition provides a strong baseline for determining the specific contributions of incidental category learning. Such an extended training protocol would necessarily lead to speed ups in the visuomotor task; thus, ruling out these RT benefits with the no auditory condition is a key empirical strength.</p><p>– The RT cost effects provide an indirect and notably clean measure of category knowledge, and the results provide a robust learning effect.</p></disp-quote><p>AR-R2-1: Thank you for these positive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses</p><p>– The relationship between explicit learning and RT cost is muddied by the nature of the explicit labeling data. The data points depicted in Figure 3B/C suggest a cluster of participants with explicit labeling accuracy hovering around chance levels and a second group of participants with above chance performance. A correlation analysis with this sort of clustering may not provide the best characterization of the data and weakens the interpretation of these effects.</p></disp-quote><p>AR-R2-2: Thank you for this comment and for the constructive suggestion. Following up on it, we split participants according to a median split of explicit labeling accuracy. See AR-R2-5 for details.</p><disp-quote content-type="editor-comment"><p>– The authors argue that incidental category knowledge supports generalization to novel stimuli and their explicit labeling findings mesh well with this interpretation. One limitation to this claim is that the contribution of long lasting category knowledge to this generalization effect is tempered by the fact that training did occur on the same day as the explicit learning task. Thus, the possibility remains that training on the same day drives generalization. A strong test would separate training and generalization to different days to pinpoint the contribution of retained category knowledge on generalization.</p></disp-quote><p>AR-R2-3: Thank you. It is a fair point that our testing approach may involve generalization based on same-day reactivation or learning. It will be important for future investigations to determine the contribution of actively retrieving long-lasting category knowledge to generalization by testing for generalization without any direct reminder or reactivation experience on the SMART task. This could elucidate a possible contribution of reactivating retained category knowledge to generalization. Yet, we believe our approach also has benefits. In the revised manuscript, we expand on the motivation for our testing approach as follows (see also p. 16, second paragraph):</p><p>&quot;RT facilitation in the SMART task may capture both the development of auditory category knowledge and more general skills such as the visual stimulus-response mapping. We attempted to dissociate these types of knowledge by comparing RT in a block in which category-to-target regularities were present and a subsequent block in which the regularities were disrupted, yielding a RT Cost. The RT Cost afforded a graded measure of how much a participant’s performance depended on the correspondence of the auditory category and the visuomotor task demands. Importantly, the RT Cost is elicited only with some level of category knowledge. We included a final block re-introducing the category-target regularities to enable participants to return to the trained task conditions. This regularity-random-regularity structure allowed us to assess the development of a specific reliance (in SMART execution) on (emerging) auditory category knowledge across time, independently of the general gains that could be reflected in the random block performance. Despite its advantages, this approach leaves open the possibility that category reactivation across the SMART task may have influenced generalization assessed in the final session. It will be important for future investigations to refine our understanding of a possible contribution of retained category knowledge to generalization, independent of the potential for mnemonic ‘reactivation’ by re-performing the SMART task&quot;.</p><disp-quote content-type="editor-comment"><p>– More generally, the discussion is fairly limited in scope. The findings seemingly speak to fundamental learning processes relevant to domains of category learning beyond audition, episodic memory, and decision making. Rather than expand on the broader implications and connect to relevant cognitive and neural theories in domains, the discussion focuses on auditory category learning which limits its broader appeal.</p></disp-quote><p>AR-R2-4: Thank you for this comment. We have broadened Discussion in the revised manuscript (see p. 17).</p><disp-quote content-type="editor-comment"><p>– The relationship between RT cost and explicit labeling (Figure 3) may be better characterized by a median split of participants' explicit labeling accuracy (or grouping the participants based on significantly above vs. below chance accuracy). Then comparing these groups' RT costs could provide a similar result but one better suited to the clustering in the data.</p></disp-quote><p>AR-R2-5: The revised manuscript includes an examination of the relationship between explicit learning (by grouping participants based on a median split of participants' explicit labeling accuracy) and RT Cost (see p. 12, second paragraph).</p><p>We performed a median split of Experiment 2 participants based on Day 10 explicit labeling accuracy, generating high-performing and low-performing subgroups. The high-performing subgroup had marginally significant greater RT Costs on Day 2 [t (22) = -1.79, p = .08, Cohen's d = .-.733] as well as on Day 10 [t (22) = -2.07, p = .05, Cohen's d = .-.846], but not on Day 1, [t (22) = -1.30, p = .21, Cohen's d = .-.533].</p><p>Similarly, we performed a median split of Experiment 3 participants based on Day 10 explicit labeling accuracy, generating high-performing and low-performing subgroups. The high-performing subgroup had significantly greater RT cost on Day 2, [t (20) = -3.82, p=.001; Cohen's d=-.721], Day 10, [t (20) = -2.68, p=.014, Cohen's d=-1.144], but not on Day 1, [t (20) = -1.69, p=.106; Cohen's d=-.721].</p><p>These results suggest that better generalization (explicit labeling) on Day 10 is associated with better learning (the increased reliance on the evolving auditory category knowledge reflected in the RT cost measure) on Day 10.</p><disp-quote content-type="editor-comment"><p>– I would highly recommend that the authors expand the discussion to connect their findings to the broader literature on episodic memory and category learning. There are central debates in the field on how regularities in sensory experiences are built from seemingly incidental exposures (i.e., how episodic memory leads to structured knowledge). The current findings offer an important contribution to this key question.</p></disp-quote><p>AR-R2-6: Thank you for this encouragement. We have expanded the Discussion in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>– I am curious about how RT costs change for a given participant across days. If one participant shows an RT cost on day 1, do they also show it on days 2 and 10? Is there a correlation across days? This is certainly a supplemental analysis, but one that could help further characterize the nature of the incidental learning effects.</p></disp-quote><p>AR-R2-7: Thank you for this suggestion. We observed significant correlations across RT Costs measured in different sessions. We added the following paragraphs to the revised MS (p. 11, first paragraph):</p><p>We examined whether participants exhibiting larger RT Costs, consistent with incidental auditory category learning, were likely to continue to exhibit larger RT Costs in subsequent sessions.</p><p>For Experiment 2, the magnitude of RT Cost incurred on Day 1 was related to Day 2 RT Cost (r = .737, p = .001) and Day 10 RT Cost (r = .571, p = .004). RT Costs on Day 2 also were associated with those of Day 10 (r = .778, p = .001) (with a p value less than the Bonferroni-corrected significant value of.025 (0.05/3) considered to be significant). Likewise, in Experiment 3, there were significant positive relationships of participants’ RT Costs across Day 1 and Day 2 (r = .670, p = .001), Day 1 and Day 10 (r = .614, p = .002), and Day 2 and Day 10 (r = .730, p = .001) (with a p value less than the Bonferroni-corrected significant value of.025 (0.05/3) considered to be significant).</p><p>Thank you, R2. We really appreciate your careful review and helpful comments.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>While there is good evidence for incidental learning of auditory categories, there is limited work focusing on how categories are consolidated. In an elegant series of experiments, the authors focus on what happens to category knowledge following sleep. Specifically, using the SMART task, they examine evidence for gains in knowledge following sleep. In Experiment 1, authors used a visuomotor version of the SMART task (participants had to press a button whose location was mapped to a visual target), and observed that participants became faster and showed offline gains (24 hours later and 9 days later). In Experiment 2, authors introduced a series of 5 identical sounds that preceded the appearance of the visual target. The sounds differed across trials, with targets mapped to auditory categories. Authors observed little evidence for learning on Day 1, but evidence for offline learning on Day 2 and 10. In Experiment 3, participants encountered a series of 5 different sounds from one auditory category that preceded the appearance of the visual target. There was evidence of learning on Day 1, and evidence of consolidation on Day 2 and Day 10. The authors conclude that learning continues even after the initial learning experience, suggesting that category knowledge is elaborated in the consolidation process.</p><p>A strength of this paper is that it is clear participants are learning category knowledge, not just specific information about the exemplars. I found the distinction between visuomotor skill learning and category learning to be nuanced and compelling, and I think this will have important implications for other work in this area.</p></disp-quote><p>AR-R3-1: Thank you for these supportive comments.</p><disp-quote content-type="editor-comment"><p>The conclusions of this paper are generally supported by the data, but I think a few issues need to be clarified. My chief concern at this stage is that authors are describing a pattern of results across three different experiments, but these differences are not statistically compared. Further, the correlations of RT cost to labelling accuracy assume that RT cost is a reliable measure in individuals. However, no evidence was presented to convince us this is the case.</p></disp-quote><p>AR-R3-2: Thank you for this suggestion, which was mirrored by R2-7. We observed significant correlations across RT Costs measured in different sessions. We added the following paragraph to the revised manuscript (see p. 11, first paragraph):</p><p>For Experiment 2, the magnitude of RT Cost incurred on Day 1 was related to Day 2 RT Cost (r = .737, p = .001) and Day 10 RT Cost (r = .571, p = .004). RT Costs on Day 2 also were associated with those of Day 10 (r = .778, p = .001) (with a p value less than the Bonferroni-corrected significant value of.025 (0.05/3) considered to be significant). Likewise, in Experiment 3, there were significant positive relationships of participants’ RT Costs across Day 1 and Day 2 (r = .670, p = .001), Day 1 and Day 10 (r = .614, p = .002), and Day 2 and Day 10 (r = .730, p = .001) (with a p value less than the Bonferroni-corrected significant value of.025 (0.05/3) considered to be significant).</p><disp-quote content-type="editor-comment"><p>I would like differences in RT cost and offline gains statistically compared across experiments. This is particularly important to support claims made in the discussion.</p></disp-quote><p>AR-R3-4: We now added comparisons across the different conditions (see Supplementary File 1). Analyses supported our claims made in the Discussion section.</p><disp-quote content-type="editor-comment"><p>What is the dropout rate? Can the authors address whether participants who did not learn the categories dropped out in sessions on Day 2 and Day 10?</p></disp-quote><p>AR-R3-5: Task performance in the SMART is straight forward (accuracy is almost at ceiling in reporting the location of suprathreshold visual targets) and there is no indication to participants that they are tested on the auditory aspects of the learning session, until the final explicit labeling task on Day 10. One participant was not able to complete the entire experiment due to sickness, so we do not have his final explicit labeling task results.</p><disp-quote content-type="editor-comment"><p>I was concerned that in the last block of Day 1 (block 8), participants might be performing slower than one might expect, as they encountered interference in block 7. It is possible they may have performed faster in a further block, which could lead to an elimination of the gain now interpreted as consolidation.</p></disp-quote><p>AR-R3-5: Thank you for raising this important comment. To address this concern, we have added an additional analysis (see also end of p. 9 in the revised manuscript):</p><p>For Experiment 2, the first half of Block 8 did not differ significantly from the last half of Block 8, t(46) = -.204, p = .839, Cohen’s d = .005. Importantly, there was no significant difference between Block 6 versus Block 8, t(46) = -.717, p = .476, Cohen’s d = .207. Thus, there was no evidence for learning across Block 7, or of a loss of performance from Blocks 6 to 8.</p><p>The same holds for Experiment 3. the first half of Block 8 did not differ significantly from the last half of Block 8, t(44) = .340, p = .735, Cohen’s d = .100. Again, there was no significant difference between Block 6 versus Block 8, t(42) = -.480, p = .633, Cohens' d = .14.</p><p>To answer the reviewer's question, Block 8 performance did not differ from Block 6 performance in either Experiment 2 or Experiment 3.</p><disp-quote content-type="editor-comment"><p>One condition I think is missing is a version of the SMART task with no auditory category information, but where participants encounter sounds (either randomly, or in a deterministic fashion). This would allow the authors to show that RT cost is truly associated with auditory category learning, rather than some distraction from a change in the auditory environment.</p></disp-quote><p>AR-R3-6: Thank you for this suggestion.</p><p>Although we did not include this condition in the present study, our prior research demonstrates that the SMART task RT Cost is absent when there is no category-to-visual target regularity (Gabay et al., 2022; Roark et al., 2022). Further, it is absent when incidental training involves shuffled category exemplars deterministically assigned to visual targets, thus preserving an auditory-visual mapping but destroying category-to-target relationship (Gabay et al., 2015, Exp 2b). It is also absent when novel category-consistent generalization exemplars are introduced in the SMART task (Gabay et al., 2015, Exp 2a). Collectively, these findings point to developing category knowledge that extends beyond auditory-visual-motor associations to support category generalization via incidental learning and cannot be explained by distraction from a shift in the auditory environment.</p><p>Relatedly, category learning does not reliably occur across passive observation of the auditory-visual patterns in SMART, or when participants make a category-nonspecific, generic motor response to report the presence of a visual target (Roark et al., 2022).</p><p>References</p><p>Ashby, F. G., and Maddox, W. T. (2005). Human category learning. Annu. Rev. Psychol., 56, 149-178.</p><p>DeCasper, A. J., and Spence, M. J. (1986). Prenatal maternal speech influences newborns' perception of speech sounds. Infant Behavior and Development, 9(2), 133-150.</p><p>Dorfberger, S., Adi-Japha, E., and Karni, A. (2007). Reduced susceptibility to interference in the consolidation of motor memory before adolescence. PloS one, 2(2), e240.</p><p>Dudai, Y., Karni, A., and Born, J. (2015). The consolidation and transformation of memory. Neuron, 88(1), 20-32.</p><p>Faul, F., Erdfelder, E., Lang, A.-G., and Buchner, A. (2007). G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods, 39(2), 175-191.</p><p>Fischer, S., Wilhelm, I., and Born, J. (2007). Developmental differences in sleep's role for implicit off-line learning: comparing children with adults. Journal of cognitive neuroscience, 19(2), 214-227.</p><p>Gabay, Y., Dick, F. K., Zevin, J. D., and Holt, L. L. (2015). Incidental auditory category learning. Journal of Experimental Psychology: Human Perception and Performance, 41(4), 1124.</p><p>Gabay, Y., Madlansacay, M., and Holt, L. L. (2022). Incidental Auditory Category Learning and Motor Sequence Learning.</p><p>Henrich, J., Heine, S. J., and Norenzayan, A. (2010). Most people are not WEIRD. Nature, 466(7302), 29-29.</p><p>Karni, A. (1996). The acquisition of perceptual and motor skills: a memory system in the adult human cortex. Cognitive Brain Research.</p><p>Karni, A., and Bertini, G. (1997). Learning perceptual skills: behavioral probes into adult cortical plasticity. Current opinion in neurobiology, 7(4), 530-535.</p><p>Karni, A., Tanne, D., Rubenstein, B. S., Askenasy, J. J., and Sagi, D. (1994). Dependence on REM sleep of overnight improvement of a perceptual skill. Science, 265(5172), 679-682.</p><p>Martinez, J. S., Holt, L. L., Reed, C. M., and Tan, H. Z. (2020). Incidental Categorization of Vibrotactile Stimuli. IEEE Transactions on Haptics, 13(1), 73-79.</p><p>Roark, C. L., and Holt, L. L. (2018). Task and distribution sampling affect auditory category learning. Attention, Perception, and Psychophysics, 80(7), 1804-1822.</p><p>Roark, C. L., Lehet, M. I., Dick, F., and Holt, L. L. (2022). The representational glue for incidental category learning is alignment with task-relevant behavior. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(6), 769.</p><p>Roth, D. A.-E., Kishon-Rabin, L., Hildesheimer, M., and Karni, A. (2005). A latent consolidation phase in auditory identification learning: Time in the awake state is sufficient. Learning and memory, 12(2), 159-164.</p><p>Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272-1278.</p></body></sub-article></article>