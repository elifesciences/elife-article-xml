<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78278</article-id><article-id pub-id-type="doi">10.7554/eLife.78278</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Vocalization categorization behavior explained by a feature-based auditory categorization model</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-272729"><name><surname>Kar</surname><given-names>Manaswini</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-272730"><name><surname>Pernia</surname><given-names>Marianny</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9889-3577</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-272731"><name><surname>Williams</surname><given-names>Kayla</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-272733"><name><surname>Parida</surname><given-names>Satyabrata</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2896-2522</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-272732"><name><surname>Schneider</surname><given-names>Nathan Alan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9145-5427</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-272734"><name><surname>McAndrew</surname><given-names>Madelyn</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-272735"><name><surname>Kumbam</surname><given-names>Isha</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-59526"><name><surname>Sadagopan</surname><given-names>Srivatsun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1116-8728</contrib-id><email>vatsun@pitt.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>Center for Neuroscience at the University of Pittsburgh</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jfeg660</institution-id><institution>Center for the Neural Basis of Cognition</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>Department of Neurobiology, University of Pittsburgh</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>Department of Bioengineering, University of Pittsburgh</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an3r305</institution-id><institution>Department of Communication Science and Disorders, University of Pittsburgh</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>13</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e78278</elocation-id><history><date date-type="received" iso-8601-date="2022-03-01"><day>01</day><month>03</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-12"><day>12</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-03-10"><day>10</day><month>03</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.03.09.483596"/></event></pub-history><permissions><copyright-statement>© 2022, Kar et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Kar et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78278-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-78278-figures-v2.pdf"/><abstract><p>Vocal animals produce multiple categories of calls with high between- and within-subject variability, over which listeners must generalize to accomplish call categorization. The behavioral strategies and neural mechanisms that support this ability to generalize are largely unexplored. We previously proposed a theoretical model that accomplished call categorization by detecting features of intermediate complexity that best contrasted each call category from all other categories. We further demonstrated that some neural responses in the primary auditory cortex were consistent with such a model. Here, we asked whether a feature-based model could predict call categorization behavior. We trained both the model and guinea pigs (GPs) on call categorization tasks using natural calls. We then tested categorization by the model and GPs using temporally and spectrally altered calls. Both the model and GPs were surprisingly resilient to temporal manipulations, but sensitive to moderate frequency shifts. Critically, the model predicted about 50% of the variance in GP behavior. By adopting different model training strategies and examining features that contributed to solving specific tasks, we could gain insight into possible strategies used by animals to categorize calls. Our results validate a model that uses the detection of intermediate-complexity contrastive features to accomplish call categorization.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>guinea pig</kwd><kwd>vocalization</kwd><kwd>categorization</kwd><kwd>auditory cortex</kwd><kwd>behavior</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01DC017141</award-id><principal-award-recipient><name><surname>Sadagopan</surname><given-names>Srivatsun</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007921</institution-id><institution>University of Pittsburgh</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Sadagopan</surname><given-names>Srivatsun</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A theoretical model that uses maximally informative spectrotemporal features to classify vocalizations, trained solely on natural guinea pig vocalizations, predicts guinea pig behavioral performance in classifying natural as well as spectrotemporally manipulated vocalizations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Communication sounds, such as human speech or animal vocalizations (calls), are typically produced with tremendous subject-to-subject and trial-to-trial variability. These sounds are also typically encountered in highly variable listening conditions—in the presence of noise, reverberations, and competing sounds. A central function of auditory processing is to extract the underlying meaningful signal being communicated so that appropriate behavioral responses can be produced. A key step in this process is a many-to-one mapping that bins communication sounds, perhaps carrying similar ‘meanings’ or associated with specific behavioral responses, into distinct categories. To accomplish this, the auditory system must generalize over the aforementioned variability in the production and transmission of communication sounds. We previously proposed, based on a model of visual categorization (<xref ref-type="bibr" rid="bib52">Ullman et al., 2002</xref>), a theoretical model that identified distinctive acoustic features that were highly likely to be found across most exemplars of a category and were most contrastive with respect to other categories. Using these ‘most informative features (MIFs)’, the model accomplished auditory categorization with high accuracy (<xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>). In that study, we also showed that features of intermediate lengths, bandwidths, and complexities were typically more informative for auditory categorization. In a second study, we further showed in a guinea pig (GP) animal model that neurons in the superficial layers of the primary auditory cortex (A1) demonstrated call-feature-selective responses and complex receptive fields that were consistent with model-predicted features, providing support for the model at the neurophysiological level (<xref ref-type="bibr" rid="bib30">Montes-Lourido et al., 2021a</xref>). In this study, we investigated whether the feature-based model held true at a behavioral level, by determining whether the model, trained solely using natural GP calls, could predict GP behavioral performance in categorizing both natural calls as well as calls with altered spectral and temporal features.</p><p>Studies in a wide range of species have probed the impact of alterations to spectral and temporal cues on call recognition. For example, in humans, it has been shown that speech recognition relies primarily on temporal envelope cues based on experiments that measured recognition performance when subjects were presented with noise-vocoded speech at different spectral resolutions (<xref ref-type="bibr" rid="bib47">Shannon et al., 1995</xref>; <xref ref-type="bibr" rid="bib48">Smith et al., 2002</xref>). However, recognition is also remarkably resilient when the envelope is altered because of tempo changes—for example, word intelligibility is resilient to a large degree of time-compression of speech (<xref ref-type="bibr" rid="bib22">Janse et al., 2003</xref>). Results from other mammalian species are broadly consistent with findings in humans. In gerbils, it has been shown that firing rate patterns of A1 neurons could be used to reliably classify calls that were composed of only four spectral bands (<xref ref-type="bibr" rid="bib50">Ter-Mikaelian et al., 2013</xref>). In GPs, small neuronal populations have been shown to be resistant to such degradations as well (<xref ref-type="bibr" rid="bib1">Aushana et al., 2018</xref>). Slow amplitude modulation cues have been proposed as a critical cue for the neuronal discriminability of calls (<xref ref-type="bibr" rid="bib49">Souffi et al., 2020</xref>), but behaviorally, call identification can be resilient to large changes in these cues. For example, mice can discriminate between calls that have been doubled or halved in length (<xref ref-type="bibr" rid="bib33">Neilans et al., 2014</xref>). This remarkable tolerance to cue variations might be related to the wide range of variations with which calls are produced in different behavioral contexts. For example, for luring female mice and during direct courtship, male mice modify many call parameters including sequence length and complexity (<xref ref-type="bibr" rid="bib6">Chabout et al., 2015</xref>). In contrast, one study in GPs using naturalistic stimuli (a human footstep sound) demonstrated that while GPs discriminated time-compressed sounds from the natural sound on which they were trained, they did not distinguish between time-expanded sounds and natural sounds (<xref ref-type="bibr" rid="bib35">Ojima and Horikawa, 2015</xref>). Along the spectral dimension, mouse call discrimination can be robust to changes in long-term spectra, including moderate frequency shifts and removal of frequency modulations (<xref ref-type="bibr" rid="bib33">Neilans et al., 2014</xref>). Indeed, it has been suggested that the bandwidth of ultrasonic vocalizations is more important for communication than the precise frequency contours of these calls (<xref ref-type="bibr" rid="bib45">Screven and Dent, 2016</xref>). Again, given that mice also modify the spectral features of their calls in a context-dependent manner (<xref ref-type="bibr" rid="bib6">Chabout et al., 2015</xref>), it stands to reason that their perception of call identity is also robust to alterations of spectral features. In GPs using footstep stimuli, animals reliably discriminated sounds subjected to a band-reject filter from the natural sound (<xref ref-type="bibr" rid="bib35">Ojima and Horikawa, 2015</xref>).</p><p>Overall, these studies suggest that for calls, in particular, information is encoded at multiple levels. Whereas the specific parameters of a given call utterance might carry rich information about the identity (<xref ref-type="bibr" rid="bib5">Boinski and Mitchell, 1997</xref>; <xref ref-type="bibr" rid="bib29">Miller et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Gamba et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Fukushima et al., 2015</xref>) and internal state of the caller as well as social context (<xref ref-type="bibr" rid="bib46">Seyfarth and Cheney, 2003</xref>; <xref ref-type="bibr" rid="bib9">Coye et al., 2016</xref>), call category identity encompasses all these variations. In some behavioral situations, listeners might need to be sensitive to these specific parameter variations—for example, for courtship, female mice have been shown to exhibit a high preference for temporal regularity of male calls (<xref ref-type="bibr" rid="bib37">Perrodin et al., 2020</xref>). But in other situations, animals must and do generalize over this variability to extract call category identity, which is critical for providing an appropriate behavioral response. What mechanisms enable animals to generalize over this tremendous variability with which calls are heard and how they accomplish call categorization, however, is not well understood.</p><p>In this study, based on our earlier modeling and neurophysiological results (<xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Montes-Lourido et al., 2021a</xref>), we hypothesized that animals can generalize over this production variability and achieve call categorization by detecting features of intermediate complexity within these calls. To test this hypothesis, we trained feature-based models and GPs to classify multiple categories of natural, spectrotemporally rich GP calls. We then tested the categorization performance of both the model and GPs with manipulated versions of the calls. We found that the feature-based model of auditory categorization, trained solely using natural GP calls, could explain about 50% of the overall variance of GP behavioral responses to manipulated calls. By comparing different model versions, we could derive further insight into possible behavioral strategies used by GPs to solve these call categorization tasks. Examining the factors contributing to high model performance in different conditions also provided insight into why a feature-based encoding strategy is highly advantageous. Overall, these results provide support at a behavioral level for a feature-based auditory categorization model, further validating our model as a novel and powerful approach to deconstructing complex auditory behaviors.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The acquisition of learned categorization behaviors likely involves two underlying processes—the acquisition of the knowledge of auditory categories, and the expression of this knowledge by learning the association between categories and reward outcomes (<xref ref-type="bibr" rid="bib26">Kuchibhotla et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Moore and Kuchibhotla, 2022</xref>). Many models have been developed to characterize the latter process, that is, the association of a stimulus with reward. Examples include models of classical conditioning and reinforcement learning (<xref ref-type="bibr" rid="bib42">Rescorla and Wagner, 1972</xref>), models that account for the motivational state of subjects during behavior (<xref ref-type="bibr" rid="bib4">Berditchevskaia et al., 2016</xref>), models that account for exploratory drive in subjects (<xref ref-type="bibr" rid="bib39">Pisupati et al., 2021</xref>), or models that account for arousal state (<xref ref-type="bibr" rid="bib10">de Gee et al., 2020</xref>). However, the former process of how knowledge of auditory categories is acquired, especially when categories are ‘non-compact’ or evident only in multiparametric spaces, is far less understood. Thus, while we briefly describe the behavioral acquisition of a vocalization categorization task for completeness (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> ), in this study, our goal was to determine the computational principles underlying the former process, that is, what features animals may use to acquire knowledge about acoustic stimulus category, and in particular, vocalization categories.</p><sec id="s2-1"><title>GPs learn to report call category in a Go/No-go task</title><p>We trained GPs on call categorization tasks using a Go/No-go task structure. Animals initiated trials by moving to the ‘home base’ region of the behavioral arena (<xref ref-type="fig" rid="fig1">Figure 1A, B</xref>). Stimuli were presented from an overhead speaker. On hearing Go stimuli, GPs were trained to move to a reward region, where they received a food pellet reward. The correct response to No-go stimuli was to remain in the home base. We trained two cohorts of GPs to categorize two pairs of call categories—Cohort 1 was trained on chuts (Go) versus purrs (No-go), calls that had similar spectral content (long-term spectral power) but different temporal (overall envelope) structure (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), and Cohort 2 was trained on wheeks (Go) versus whines (No-go), calls that had similar temporal structure but different spectral content (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). GPs were trained on this task over multiple short sessions every day (~6 sessions of ~40 trials each, ~10 min per session; see Materials and methods). On each trial, we presented a randomly chosen exemplar from an initial training set of 8 exemplars per category. We estimated hit rates and false alarm (FA) rates from all trials in a given day and computed a sensitivity index (<italic>d</italic>′). GPs were considered trained when <italic>d′</italic> reliably crossed a threshold of 1.5. On average, GPs acquired this task after ~2–3 weeks of training (~4,000 total trials,~250 trials per exemplar; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). On the last day of training, GPs displayed a mean <italic>d</italic>′ of 1.94±0.26 for the chuts versus purrs task, and a mean <italic>d′</italic> of 1.90±0.57 for the wheeks versus whines task.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Call categorization behavior in GPs.</title><p>(<bold>A</bold>) Behavioral setup, indicating home base region for trial initiation (yellow) and reward area (green). Some naive animals observed expert animals performing the task to speed up task acquisition. (<bold>B</bold>) Video tracking was employed to detect GP position and trigger task events (stimulus presentation, reward delivery, etc.). (<bold>C</bold>) Spectrograms of example chut calls (Go stimuli for Cohort 1) and purr calls (No-go stimuli for Cohort 1). (<bold>D</bold>) Spectrograms of example wheek calls (Go stimuli for Cohort 2) and whine calls (No-go stimuli for Cohort 2). Performance of GPs during the training phase of the call categorization task is shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. GP, guinea pig.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Learning rates of GPs performing a call categorization task.</title><p>(<bold>A</bold>) and (<bold>D</bold>) Probability of hits (green) and false alarms (FAs; red) as a function training day (averaged over ~6 sessions per day) for the chuts versus purrs (<bold>A</bold>) and wheeks versus whines (<bold>D</bold>) tasks. Dark lines are averages of all subjects, faint lines correspond to individual subjects. (<bold>B</bold>) and (<bold>E</bold>) Sensitivity index (<italic>d</italic>′) as a function of training day. Black line is average over four subjects, gray lines are individual subjects. Subjects were considered trained when their performance showed <italic>d</italic>′&gt;1.5 (dashed blue line). (<bold>C</bold>) and (<bold>F</bold>) hits and FAs of animals as a function of intra-day session number, averaged over 4 days after animals acquired the task.</p><p><supplementary-material id="fig1s1sdata1"><label>Figure 1—figure supplement 1—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs over the course of training on vocalization categorization tasks.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig1-figsupp1-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig1-figsupp1-v2.tif"/></fig></fig-group><p>To gain insight into possible behavioral strategies that GPs might adopt to solve the categorization task, we examined trends of behavioral performance over the training period. Initially, GPs exhibited low hit rates as well as low FA rates, suggesting that they did not associate the auditory stimulus with reward (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>). Note that this initial phase was not recorded for the first cohort (chuts vs. purrs task, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). Within 2–3 days, GPs formed a stimulus-reward association and exhibited ‘Go’ responses for all stimuli but did not discriminate between Go and No-go stimulus categories. This resulted in high hit rates as well as FA rates, but low <italic>d′</italic>. For the remainder of the training period, hit rates remained stable whereas FA rates gradually declined, suggesting that the improvements to <italic>d′</italic> resulted from GPs learning to suppress responses to No-go stimuli (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A, B, D, E</xref>).</p><p>While these data were averaged over all sessions daily for further analyses, we noticed within-day trends in performance that might provide insight into the behavioral state of the GPs. We analyzed performance across intra-day sessions, averaged over 4 days after the animals acquired the task (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C, F</xref>). In early sessions, both hit rates and FA rates were high, suggesting that the GPs weighted the food reward highly, risking punishments (air puffs/time outs) in the process. In subsequent sessions, both the hit rate and FA rate declined, suggesting that the GPs shifted to a punishment-avoidance strategy. Despite these possible changes in decision criteria used by the GPs, they maintained consistent performance, as <italic>d′</italic> remained consistent across sessions. Therefore, in all further analyses, we used <italic>d′</italic> values averaged over all sessions as a performance metric. Note that these observations of possible motivational and other fluctuations are not intended to be captured by our model. Rather, our model solely focuses on what spectrotemporal features GPs can use to deduce stimulus categories.</p></sec><sec id="s2-2"><title>A feature-based computational model can be trained to accomplish call categorization</title><p>In parallel, we extended a feature-based model that we previously developed for auditory categorization (<xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>) to accomplish GP call categorization in a Go/No-go framework. Briefly, we implemented a three-layer model consisting of a spectrotemporal representation layer, a feature-detection (FD) layer, and a winner-take-all (WTA) layer. The spectrotemporal layer was a biophysically realistic model of the auditory periphery (<xref ref-type="bibr" rid="bib56">Zilany et al., 2014</xref>). For the FD layer, we used greedy search optimization and information theoretic principles to derive a set of MIFs for each call type that was optimal for the categorization of that call type from all other call types (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>; <xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>). We derived five distinct sets of MIFs for each call type that could accomplish categorization (see Materials and methods). We refer to models using these distinct MIF sets as different instantiations of the model.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Framework of the model trained to perform call categorization tasks.</title><p>(<bold>A</bold>) and (<bold>B</bold>) Example cochleagrams for target (<bold>A</bold>) and distractor (<bold>B</bold>) calls. Cochleagram rows were normalized to set the maximum value as 1 and then smoothed for display. White rectangles denote detected MIFs for that call. For an input call, the target (green) FD stage response is the sum of all detected target MIF weights normalized by the sum of all MIF weights for that call type. The distractor response (red) is similarly computed. (<bold>C</bold>) The output of the winner-take-all (WTA) stage is determined based on the difference between the target and distractor FD stage responses. Dots represent the WTA outputs for all calls used for training the models. Rows represent the five instantiations of the model with different MIF sets. MIF, maximally informative features; det, detected MIFs; all, all MIFs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig2-v2.tif"/></fig><p>Call-specific MIF sets in the FD layer showed near-perfect performance (area under the curve, or AUC&gt;0.97 for all 20 MIF sets [4 call categories×5 instantiations per category], mean=0.994) in categorizing target GP calls from other calls in the training data set. Similar to results from <xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>, the number of MIFs for each instantiation of the model ranged from 8 to 20 (mean=16.5), with MIFs spanning ~3 octaves in bandwidth and ~110 ms in duration on average (<xref ref-type="table" rid="table1">Table 1</xref>). To assess the performance of the WTA layer based on these training data, we estimated <italic>d′</italic> using <xref ref-type="disp-formula" rid="equ1">equation 1</xref> (Materials and methods). The WTA output also showed near-perfect performance for classifying the target from the distractor for both chuts versus purrs (mean <italic>d′</italic>=4.65) and wheeks versus whines (mean <italic>d′</italic>=3.69) tasks (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Properties of MIFs.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Call name</th><th align="left" valign="bottom">Instantiation</th><th align="left" valign="bottom">Number of MIFs</th><th align="left" valign="bottom">MIF duration (ms)(mean±std)</th><th align="left" valign="bottom">MIF bandwidth (octaves)(mean±std)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Chut</td><td align="left" valign="bottom">1, 2, 3, 4, 5</td><td align="left" valign="bottom">20, 20, 20, 20, 20</td><td align="left" valign="bottom">88±63, 106±53, 108±56, 109±64, 133±47</td><td align="left" valign="bottom">4.0±2.0, 4.4±1.2, 3.1±1.9, 3.7±1.9, 2.6±1.8</td></tr><tr><td align="left" valign="bottom">Purr</td><td align="left" valign="bottom">1, 2, 3, 4, 5</td><td align="left" valign="bottom">8, 9, 20, 20, 20</td><td align="left" valign="bottom">91±49, 83±43, 116±49, 116±56, 86±63</td><td align="left" valign="bottom">2.6±1.2, 2.8±1.2, 3.1±1.4, 3.2±1.5, 3.6±1.2</td></tr><tr><td align="left" valign="bottom">Wheek</td><td align="left" valign="bottom">1, 2, 3, 4, 5</td><td align="left" valign="bottom">8, 14, 13, 11, 12</td><td align="left" valign="bottom">144±47, 99±58, 104±68, 116±62, 114±65</td><td align="left" valign="bottom">2.3±1.6, 2.6±1.8, 2.9±2.2, 2.1±1.1, 2.5±1.7</td></tr><tr><td align="left" valign="bottom">Whine</td><td align="left" valign="bottom">1, 2, 3, 4, 5</td><td align="left" valign="bottom">20, 20, 15, 20, 20</td><td align="left" valign="bottom">109±55, 111±68, 133±37, 117±51, 108±70</td><td align="left" valign="bottom">3.5±1.8, 3.4±1.6, 2.6±1.4, 3.2±1.5, 3.9±1.6</td></tr><tr><td align="left" valign="bottom">Summary</td><td align="left" valign="bottom"/><td align="left" valign="bottom">16.5±4.7</td><td align="left" valign="bottom">109±57</td><td align="left" valign="bottom">3.2±1.7</td></tr></tbody></table></table-wrap></sec><sec id="s2-3"><title>Both GPs and the model generalize to new exemplars</title><p>To determine if GPs learned to report call category or if they simply remembered the specific call exemplars on which they were trained, we tested whether their performance generalized to a new set of Go and No-go stimuli (eight exemplars each) that the GPs had not encountered before. On each generalization day, we ran four sessions of ~40 trials each, with the first two sessions containing only training exemplars and the last two sessions containing only new exemplars. All GPs achieved a high-performance level (<italic>d′</italic>&gt;1) to the new exemplars by generalization day 2 (<xref ref-type="fig" rid="fig3">Figure 3</xref>), that is, after being exposed to only a few repetitions of the new exemplars (~5 trials per new exemplar on generalization day 1). As an additional control to ensure that GPs did not rapidly learn reward associations for the new exemplars, for GPs performing the wheeks versus whines task (<italic>n</italic>=3), we also quantified generalization performance when the regular training exemplars and a second new set of exemplars were presented in an interleaved manner (400 trials with an 80/20 mix of training and new exemplars). GPs achieved <italic>d′</italic>&gt;1 for new exemplars in this interleaved set as well, further supporting the notion that GPs were truly reporting call category.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>GP and model performance generalizes to new exemplars.</title><p>(<bold>A</bold>) and (<bold>C</bold>) Hit (green) and false alarm (FA; red) rates of GPs when categorizing new exemplars as a function of generalization day. We presented ~5 trials of each new exemplar per day. Dark lines correspond to average over subjects, faint lines correspond to individual subjects. (<bold>B</bold>) and (<bold>D</bold>) Quantification of generalization performance. Black line corresponds to average <italic>d′</italic>, gray lines are <italic>d′</italic> values of individual subjects. GPs achieved a <italic>d′</italic>&gt;1 by generalization day 2, that is, after exposure to only ~5 trials of each new exemplar on day 1. The feature-based model (orange, n = 5 instantiations, error bars correspond to s.e.m.) also generalized to new exemplars that were not part of the model’s training set of calls. Source data are available in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>. GP, guinea pig.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d′</italic> values of GPs and the feature-based model for generalization to new vocalization exemplars.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig3-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig3-v2.tif"/></fig><p>Similar to GPs, to test model generalization, we quantified model performance for new call exemplars (<xref ref-type="fig" rid="fig3">Figure 3B, D</xref>). Models using different MIF sets, that is, all instantiations of the model for chut, purr, wheek, and whine classification achieved high categorization performance (<italic>d′</italic>&gt;1) for the new exemplars. In summary, GPs as well as the feature-based model could rapidly generalize to novel exemplars.</p></sec><sec id="s2-4"><title>Both GPs and the model exhibit similar categorization-in-noise thresholds</title><p>Real-world communication typically occurs in noisy listening environments. To test how well GPs could maintain categorization in background noise, we assessed their performance when call stimuli were masked by additive white noise at several signal-to-noise ratios (SNRs) for both Go and No-go stimuli. Experiments were conducted in a block design, using a fixed SNR level per session (~40 trials) and testing 5 or 6 SNR levels each day. At the most favorable SNR (&gt;20 dB), GPs exhibited high hit rates and low FA rates, leading to high <italic>d′</italic> (&gt;2) for both call groups (<xref ref-type="fig" rid="fig4">Figure 4</xref>). With increasing noise level (i.e., decreasing SNR), we observed a decrease in hit rate and an increase in FA, as expected, with a concomitant decrease in <italic>d′</italic>. To determine the effect of SNR value on <italic>d′</italic>, we constructed a generalized linear model with a logit link function to predict trial-by-trial behavioral outcomes, with stimulus type (Go or No-go), SNR value, and an interaction term as predictors, and animal ID as a random effect (see <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, Materials and methods). We compared this full model to a null model consisting only of stimulus type as a predictor and animal ID as a random effect (see <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, Materials and methods). This comparison revealed a strong effect of SNR value of <italic>d′</italic> (chuts vs. purrs: χ<sup>2</sup>=124.6, <italic>p</italic>=2.2×10<sup>–16</sup>; wheeks vs. whines: χ<sup>2</sup>=182.5; <italic>p</italic>=2.2×10<sup>–16</sup>). At the most adverse SNR (–18 dB) for both call groups, hit and FA rates were similar, suggesting that the animals were performing at chance level. To estimate the SNR corresponding to the performance threshold (<italic>d′</italic>=1) for call categorization in noise, we fit a psychometric function to the behavioral <italic>d′</italic> data (see Materials and methods). We obtained performance thresholds (SNR at which <italic>d′</italic>=1) for both the chuts versus purrs (–6.8 dB SNR) and wheeks versus whines (–11 dB SNR) tasks that were qualitatively similar to human speech discrimination performance in white noise (<xref ref-type="bibr" rid="bib38">Phatak and Allen, 2007</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Call categorization is robust to degradation by noise.</title><p>(<bold>A</bold>) and (<bold>C</bold>) Hit (green) and false alarm (FA; red) rates of GPs categorizing calls with additive white noise at different SNRs. (<bold>B</bold>) and (<bold>D</bold>) Sensitivity index (<italic>d</italic>′) as a function of SNR. Black symbols correspond to the mean <italic>d′</italic> across animals (<italic>n</italic>=4); error bars correspond to s.e.m. Black line corresponds to a psychometric function fit to the behavioral data. Orange symbols correspond to the mean <italic>d′</italic> across five instantiations of the model, error bars correspond to s.e.m. Orange line corresponds to a psychometric function fit to the model data. Dashed blue line signifies <italic>d′</italic>=1. The statistical significance of SNR value on behavior was evaluated using a generalized linear model (see main text). Source data are available in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>. GP, guinea pig; MAE, mean absolute error; SNR, signal-to-noise ratio.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for vocalization categorization in noise (at different SNRs).</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig4-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig4-v2.tif"/></fig><p>We also tested the performance of the feature-based model (trained only on clean stimuli) on the same set of noisy stimuli as the behavioral paradigm. Model performance trends mirrored behavior, with a higher threshold for the chuts vs. purrs task (–5.4 dB SNR) compared to the wheeks versus whines task (–15 dB SNR). Similar to behavioral data, we confirmed a significant effect of SNR value on model performance using a trialwise GLM analysis (chuts vs. purrs: χ<sup>2</sup>=402.3, p=2.2×10<sup>–16</sup>; wheeks vs. whines: χ<sup>2</sup>=203.7; p=2.2×10<sup>–16</sup>). To compare behavioral data with the model, we fit a line to the <italic>d′</italic> values obtained from the model plotted against the <italic>d′</italic> values obtained from behavior, and computed the <italic>R</italic><sup>2</sup> value. We used the mean absolute error (MAE) to quantify the absolute deviation between model and behavior <italic>d′</italic> values. Although the model over-performed for the wheeks vs. whines task, it could explain a high degree of variance (<italic>R</italic><sup>2</sup>=0.94 for both tasks) of GP call-in-noise categorization behavior.</p></sec><sec id="s2-5"><title>Stimulus information might be available to GPs in short-duration segments of calls</title><p>Several studies across species, including humans (<xref ref-type="bibr" rid="bib28">Marslen-Wilson and Zwitserlood, 1989</xref>; <xref ref-type="bibr" rid="bib44">Salasoo and Pisoni, 1985</xref>), birds (<xref ref-type="bibr" rid="bib25">Knudsen and Gentner, 2010</xref>; <xref ref-type="bibr" rid="bib51">Toarmino et al., 2011</xref>), sea-lions (<xref ref-type="bibr" rid="bib40">Pitcher et al., 2012</xref>), and mice (<xref ref-type="bibr" rid="bib21">Holfoth et al., 2014</xref>), have suggested that the initial parts of calls might be the most critical parts for recognition. We reasoned that if that were the case for GPs as well, and later call segments did not add much information for call categorization, we might observe a plateauing of behavioral performance after a certain length of a call was presented. To test this, we presented call segments of different lengths (50–800 ms) beginning at the call onsets (<xref ref-type="fig" rid="fig5">Figure 5A, D</xref>) to estimate the minimum call duration required for successful categorization by GPs. Trials were presented in a randomized manner in sessions of ~40 trials, that is, each trial could be a Go or No-go stimulus of any segment length. We did not observe systematic changes to <italic>d′</italic> values when comparing the first and second halves of the entire set of trials used for testing, demonstrating that the GPs were not learning the specific manipulated exemplars that we presented. GPs showed <italic>d′</italic> values &gt;1 for as small as 75 ms segments for both tasks, and as expected, the performance stabilized for all longer segment lengths (<xref ref-type="fig" rid="fig5">Figure 5B, C, E and F</xref>). As with the SNR experiment, we used a comparison between a full GLM including stimulus length value and a null GLM to evaluate the statistical significance of the effect of stimulus length on behavioral performance. We confirmed a significant effect of segment length on behavioral performance (chuts vs. purrs: χ<sup>2</sup>=58.7, <italic>p</italic>=1.8×10<sup>–13</sup>; wheeks vs. whines: χ<sup>2</sup>=120.2; <italic>p</italic>=2.2×10<sup>–16</sup>). These data suggest that short-duration segments of calls carry sufficient information for call categorization, at least in the tested one-vs.-one scenarios. The fact that call category can be extracted from the earliest occurrences of such segments suggests two possibilities: (1) a large degree of redundancy is present in calls, or (2) the repeated segments can be used to derive information beyond call category (e.g., caller identity or emotional valence).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>GPs can obtain information for categorization from short-duration segments of calls.</title><p>(<bold>A</bold>) and (<bold>D</bold>) Schematic showing truncation of stimuli at different segment lengths from the onset of calls. (<bold>B</bold>) and (<bold>E</bold>) Average (<italic>n</italic>=3 GPs) hit rate (green) and false alarm rates (red) as a function of stimulus segment length. (<bold>C</bold>) and (<bold>F</bold>) Black symbols correspond to average GP <italic>d′</italic> (<italic>n</italic>=3 GPs), error bars correspond to s.e.m. Orange symbols correspond to average model <italic>d′</italic> (<italic>n</italic>=5 model instantiations), error bars correspond to s.e.m. Dashed blue line denotes <italic>d′</italic>=1. The statistical significance of segment length value on behavior was evaluated using a generalized linear model (see main text). Source data are available in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>. FA, false alarm; GP, guinea pig.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for the segment-length manipulation.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig5-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig5-v2.tif"/></fig><p>Model performance, however, only exceeded a <italic>d′</italic> value of 1 for ~150 ms call segments, and performance only plateaued after a 200 ms duration (<xref ref-type="fig" rid="fig5">Figure 5C, F</xref>). The effect of segment length on model performance was statistically significant (chuts vs. purrs: χ<sup>2</sup>=613.2, <italic>p</italic>=1.8× 0<sup>–13</sup>; wheeks vs. whines: χ<sup>2</sup>=775.4; <italic>p</italic>=2.2×10<sup>–16</sup>). This observation could reflect the fact that the MIFs identified for categorization were on average about 110 ms long. Despite these differences, model performance was in general agreement with behavioral performance for both the chuts versus purrs and wheeks versus whines tasks (<italic>R</italic><sup>2</sup>=0.674 and 0.444, respectively).</p></sec><sec id="s2-6"><title>Temporal manipulations had little effect on model performance and GP behavior</title><p>To investigate the importance of temporal cues for GP call categorization, we introduced several gross temporal manipulations to the calls. We first started by changing the tempo of the calls, that is, stretching/compressing the calls without introducing alterations to the long-term spectra of calls (<xref ref-type="fig" rid="fig6">Figure 6A, D</xref>). This resulted in calls that were ~0.45, 0.5, ~0.56, ~0.63, ~0.77, ~1.43, 2.5, and 5 times the original lengths of the calls. As earlier, we presented stimuli in randomized order and verified that <italic>d′</italic> did not vary systematically between the first and second half of trials, suggesting that the GPs were not learning new associations for the manipulated exemplars. GP behavioral performance remained above a <italic>d′</italic> of 1 for all these perturbations, showing high hit rates and low FA rates (<xref ref-type="fig" rid="fig6">Figure 6B, E</xref>) leading to similar <italic>d′</italic> across probed conditions (<xref ref-type="fig" rid="fig6">Figure 6C, F</xref>). Similar trialwise GLM analysis as earlier, but including the sign of the manipulation (compression vs. expansion) as an additional predictor in the full model, revealed a weak effect (based on the value of the χ<sup>2</sup> statistic, which roughly corresponds to effect size) of tempo shift on performance (chuts vs. purrs: χ<sup>2</sup>=46; <italic>p</italic>=5.7×10<sup>–10</sup>; wheeks vs. whines: χ<sup>2</sup>=13.6; <italic>p</italic>=0.003).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Call categorization is resistant to changes in tempo.</title><p>(<bold>A</bold>) and (<bold>D</bold>) Schematic showing changes to call tempo without affecting spectral content. (<bold>B</bold>) and (<bold>E</bold>) Average (<italic>n</italic>=4 GPs for chuts vs. purrs; <italic>n</italic>=3 GPs for wheeks vs. whines) hit rate (green) and false alarm rates (red) as a function of tempo change, expressed as times change in call duration (1 corresponds to the natural call). (<bold>C</bold>) and (<bold>F</bold>) Black points correspond to average GP <italic>d′</italic>; error bars correspond to s.e.m. Orange points correspond to average model <italic>d′</italic> (<italic>n</italic>=5 model instantiations), error bars correspond to s.e.m. Dashed blue line denotes <italic>d′</italic>=1. The statistical significance of call duration value on behavior was evaluated using a generalized linear model (see main text). Source data are available in <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref>. FA, false alarm; GP, guinea pig.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for tempo-shifted vocalizations.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig6-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig6-v2.tif"/></fig><p>Similarly, model performance also remained above <italic>d′</italic> of 1 for all tempo manipulations. We emphasize that the MIFs used in the model were trained only on natural calls, and were not temporally compressed or stretched to match the test stimuli. GLM analysis as above revealed a weak effect of tempo shift on model <italic>d</italic>′ for chuts versus purrs (χ<sup>2</sup>=43.3; <italic>p</italic>=2.14×10<sup>–9</sup>), and no effect for wheeks versus whines (χ<sup>2</sup>=0.322; <italic>p</italic>=0.96). Note that while the model qualitatively captured GP behavioral trends, we obtained low <italic>R</italic><sup>2</sup> values likely because of the weak effect of tempo shift values on both animal and model behaviors. The variability of the observed behavior was likely dominated by non-stimulus-related factors such as motivation, exploratory drive, or arousal (<xref ref-type="bibr" rid="bib4">Berditchevskaia et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">de Gee et al., 2020</xref>; <xref ref-type="bibr" rid="bib39">Pisupati et al., 2021</xref>), factors not explicitly modeled here. However, the relatively low MAE for the tempo manipulations (comparable with MAEs of the SNR manipulation which showed high <italic>R</italic><sup>2</sup> values) suggests a broad correspondence between the model and behavior.</p><p>The tempo manipulations lengthened or shortened both syllables and inter-syllable intervals (ISIs). Because a recent study in mice (<xref ref-type="bibr" rid="bib37">Perrodin et al., 2020</xref>) suggested that the regularity of ISI values might be crucial for the detection of male courtship songs by female mice, we next asked whether GPs used individual syllables or temporal patterns of calls for call categorization. First, as a low-level control, we replaced the ISIs of calls with silence instead of the low level of background noise present in recordings to ensure that GPs were not depending on any residual ISI information (silent ISI). Second, since many call categories show a distribution of ISI durations (<xref ref-type="fig" rid="fig7">Figure 7A, E</xref>), we replaced the ISI durations in a call with ISI values randomly sampled from the ISI distribution of the same call category (random ISI). The hit and FA rates for both silent and random ISI stimuli were comparable to the regular calls for both categorization tasks (<xref ref-type="fig" rid="fig7">Figure 7C, G</xref>), and thus, no significant difference in <italic>d′</italic> values was observed across these conditions (<xref ref-type="fig" rid="fig7">Figure 7D, H</xref>; repeated measures ANOVA; <italic>p</italic>=0.536 for chuts vs. purrs and <italic>p</italic>=0.365 for wheeks vs. whines). Consistent with these behavioral trends, model performance was also largely unaffected by these ISI manipulations.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Call categorization is resistant to manipulations to the inter-syllable interval (ISI).</title><p>(<bold>A</bold>) and (<bold>E</bold>) Distribution of ISI lengths for the call types used in the categorization tasks. (<bold>B</bold>) and (<bold>F</bold>) Comparison of the Go rates for natural and chimeric calls. We compared Go rates rather than <italic>d′</italic> because chimeric calls were presented in a catch-trial design (see main text and Materials and methods). Chim. refers to chimeric calls with one call’s syllables and the other call’s ISIs. For example, chimeric chuts have chut syllables and purr ISIs. Label on x-axis refers to syllable identity. (<bold>C</bold>) and (<bold>G</bold>) Comparison of hit (green) and FA (red) rates for regular calls, calls where we replaced ISI values with values drawn from the same calls’ ISI distributions, and calls where we replaced the ISI with silence (rather than background noise). (<bold>D</bold>) and (<bold>H</bold>) Comparison of GP (black; <italic>n</italic>=3 GPs) and model (orange; <italic>n</italic>=5 instantiations) <italic>d′</italic> values across these manipulations. Error bars correspond to s.e.m. Source data are available in <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref>. FA, false alarm; GP, guinea pig.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for various ISI manipulations (silent ISI, random ISI, chimeric calls).</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig7-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig7-v2.tif"/></fig><p>Finally, because the Go/No-go stimulus categories vary in their ISI distributions (<xref ref-type="fig" rid="fig7">Figure 7A, E</xref>), particularly for chuts versus purrs, we generated chimeric calls with syllables of one category and ISI values of the other category (e.g., chut syllables with purr ISIs). Since we combined properties of two call categories, we presented chimeric stimuli in a catch-trial design (see Materials and methods) and compared the Go response rates using syllable identity as the label for a category. While the response rates were marginally lower for the chimeric chuts (chut syllables with purr ISI values) compared to regular chuts (paired <italic>t</italic>-test; <italic>p</italic>=0.039), responses were unaltered for regular and chimeric purrs (paired <italic>t</italic>-test; <italic>p</italic>=0.415), chimeric wheeks (paired <italic>t</italic>-test; <italic>p</italic>=0.218), and chimeric whines (paired <italic>t</italic>-test; <italic>p</italic>=0.099) (<xref ref-type="fig" rid="fig7">Figure 7B, F</xref>). We did not test the model with chimeric stimuli because ‘Go’ and ‘Nogo’ category labels could not be assigned.</p><p>As a more drastic manipulation, we tested the effects of temporally reversing the calls (<xref ref-type="fig" rid="fig8">Figure 8A, D</xref>). Given that both chuts and purrs are calls with temporally symmetric spectrotemporal features, compared to natural calls, we observed no changes in the hit and FA rates (<xref ref-type="fig" rid="fig8">Figure 8B</xref>) or <italic>d′</italic> values for reversed calls (<xref ref-type="fig" rid="fig8">Figure 8C</xref>; paired <italic>t</italic>-test; <italic>p</italic>=0.582). Wheeks and whines, however, show strongly asymmetric spectrotemporal features. Interestingly, reversal did not significantly affect the categorization performance for this task as well (<xref ref-type="fig" rid="fig8">Figure 8E, F</xref>; paired <italic>t</italic>-test; <italic>p</italic>=0.151). The model also maintained robust performance (<italic>d′</italic>&gt;1) for call reversal conditions but with an ~18% decrease in <italic>d′</italic> compared to behavior. Overall, these results suggest that GP behavioral performance is tolerant to temporal manipulations such as tempo changes, ISI manipulations, and call reversal, and this tolerance can be largely captured by the feature-based model.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Call categorization is resistant to time-reversal.</title><p>(<bold>A</bold>) and (<bold>D</bold>) Schematics showing spectrogram and waveform of natural (left) and reversed (right) purr (<bold>A</bold>) and wheek (<bold>D</bold>) calls. (<bold>B</bold>) and (<bold>E</bold>) Average (<italic>n</italic>=3 GPs) hit rate (green) and FA rate (red) for natural and reversed calls. (<bold>C</bold>) and (<bold>F</bold>) Average performance of GPs (black; <italic>n</italic>=3 GPs) and model (orange; <italic>n</italic>=5 model instantiations) for natural and reversed calls. Error bars correspond to s.e.m. Source data are available in <xref ref-type="supplementary-material" rid="fig8sdata1">Figure 8—source data 1</xref>. FA, false alarm; GP, guinea pig.</p><p><supplementary-material id="fig8sdata1"><label>Figure 8—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for natural and reversed vocalizations.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig8-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig8-v2.tif"/></fig></sec><sec id="s2-7"><title>Spectral manipulations cause similar degradation in model performance and GP behavior</title><p>Because temporal manipulations did not significantly affect GP behavioral or model classification performance, we reasoned that categorization was primarily driven by within-syllable spectral cues. To ascertain the impact of spectral manipulations on call categorization, we varied the fundamental frequency (F0) of the calls from one octave lower (–50%) to one octave higher (+100%) than the regular calls without altering call lengths (<xref ref-type="fig" rid="fig9">Figure 9A and D</xref>). As earlier, we verified that <italic>d′</italic> did not vary systematically between the first and second half of trials, suggesting that the GPs were not learning new associations for the manipulated exemplars. Both increases and decreases to the F0 of the calls significantly affected behavioral performance, revealed by a GLM analysis as described earlier (chuts vs. purrs: χ<sup>2</sup>=106.7; <italic>p</italic>=2.2×10<sup>–16</sup>; wheeks vs. whines: χ<sup>2</sup>=158.2; <italic>p</italic>=2.2×10<sup>–16</sup>). Particularly, we saw a rise in FA rates (<xref ref-type="fig" rid="fig9">Figure 9B</xref>) as the F0 deviated farther from the natural values, leading to a significant drop in <italic>d′</italic> values for several conditions (<xref ref-type="fig" rid="fig9">Figure 9C</xref>). Model performance was also significantly affected by F0 shift magnitude (chuts vs. purrs: χ<sup>2</sup>=93.9; <italic>p</italic>=2.2×10<sup>–16</sup>; wheeks vs. whines: χ<sup>2</sup>=456; <italic>p</italic>=2.2×10<sup>–16</sup>). As earlier, MIFs used in the model were trained only on natural calls, and were not frequency-shifted to match the test stimuli. Model performance mirrored behavioral trends as evidenced by high <italic>R</italic><sup>2</sup> (0.723 and 0.468 for chuts vs. purrs and wheeks vs. whines, respectively) and low MAE values.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Call categorization is sensitive to fundamental frequency (F0) shifts.</title><p>(<bold>A</bold>) and (<bold>D</bold>) Schematics showing spectrograms of natural calls (middle) and versions where the F0 has been decreased (left) or increased (right). (<bold>B</bold>) and (<bold>E</bold>) Average (<italic>n</italic>=4 GPs for chuts vs. purrs; <italic>n</italic>=3 GPs for wheeks vs. whines) hit rate (green) and FA rate (red) for F0-shifted calls Note that 0% change in F0 is the natural call, –50% change corresponds to shifting F0 one octave lower, and 100% change corresponds to shifting F0 one octave higher than the natural call. (<bold>C</bold>) and (<bold>F</bold>) Average performance of GPs (black) and model (orange; <italic>n</italic>=5 model instantiations) for natural and F0-shifted calls. Error bars correspond to s.e.m. The statistical significance of F0-shift value on behavior was evaluated using a generalized linear model (see main text). Source data are available in <xref ref-type="supplementary-material" rid="fig9sdata1">Figure 9—source data 1</xref>. FA, false alarm; GP, guinea pig.</p><p><supplementary-material id="fig9sdata1"><label>Figure 9—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for F0-shifted vocalizations.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig9-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig9-v2.tif"/></fig><p>Finally, because wheeks and whines differ in their spectral content at high frequencies (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), we asked whether GPs exclusively used the higher harmonics of wheeks to accomplish the categorization task. To answer this question, we low-pass filtered both wheeks and whines at 3 kHz (<xref ref-type="fig" rid="fig10">Figure 10A</xref>), removing the higher harmonics of the wheeks while leaving the fundamental relatively unaffected. Although GP performance showed a decreasing trend for the filtered calls (<xref ref-type="fig" rid="fig10">Figure 10B, C</xref>), it was not significantly different from regular calls (paired <italic>t</italic>-test; <italic>p</italic>=0.169), indicating that the higher harmonics might be an important but not the sole cue used by GPs for the task. Similar to behavior, the model performed slightly poorly but above a <italic>d′</italic> of 1 in the low-pass filtered condition.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Call categorization is mildly affected by low-pass filtering.</title><p>(<bold>A</bold>) Schematic spectrograms of natural calls (top) and low-pass filtered (bottom) wheek and whine calls. (<bold>B</bold>) Average (<italic>n</italic>=3 GPs) hit rate (green) and FA rate (red) for natural and low-pass filtered (cutoff=3 kHz) calls. (<bold>C</bold>) Average performance of GPs (black) and model (orange; <italic>n</italic>=5 model instantiations) for natural and low-pass filtered calls. Error bars correspond to s.e.m. Source data are available in <xref ref-type="supplementary-material" rid="fig10sdata1">Figure 10—source data 1</xref>. FA, false alarm; GP, guinea pig.</p><p><supplementary-material id="fig10sdata1"><label>Figure 10—source data 1.</label><caption><title>Hit rates, false alarm rates, and <italic>d</italic>′ values of GPs and the feature-based model for natural and low-pass filtered vocalizations.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig10-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig10-v2.tif"/></fig></sec><sec id="s2-8"><title>Feature-based model explains a high degree of variance in GP behavior</title><p>The feature-based model was developed purely based on theoretical principles, made minimal assumptions, was trained only on natural GP calls, and had no access to GP behavioral data. For training the model, we used exemplars that clearly provided net evidence for the presence of one category or the other (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; green and red tick marks in <xref ref-type="fig" rid="fig11">Figure 11A, D</xref>). We tested the model (and GPs), however, with manipulated stimuli that spanned a large range of net evidence values (histograms in <xref ref-type="fig" rid="fig11">Figure 11A, D</xref>), with many stimuli close to the decision boundary (blue ticks correspond to an SNR value of –18 dB). Despite the difficulty imposed by this wide range of manipulations, the model explained a high degree of variance in GP behavior as evidenced by high <italic>R</italic><sup>2</sup> and low MAE across individual paradigms (call manipulations) as well as overall (<xref ref-type="fig" rid="fig11">Figure 11B–F</xref>; <italic>R</italic><sup>2</sup>=0.60 for chuts vs. purrs and 0.37 for wheeks vs. whines; using model and behavior <italic>d′</italic> values pooled across all tasks).</p><fig-group><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Feature-based model explains ~50% of the overall variance in GP behavior.</title><p>(<bold>A</bold>) Stacked distributions of the evidence for the presence of Go (green) and No-go (red) stimuli (across all manipulations for the chuts vs. purrs task), showing that the output is generally &gt;0 for chuts (green; Go stimulus) and &lt;0 for purrs (red; No-go stimulus). The evidence for easy tasks, such as generalizing to new natural chuts (green ticks) or purrs (red ticks), is typically well away from 0 (decision boundary). In contrast, the evidence for difficult tasks, such as the –18 dB SNR condition (blue ticks), falls near 0. Dashed black line corresponds to the winner-take-all output as a probability of reporting a Go response. (<bold>B, C</bold>) Compared to the model trained with the specific task performed by the GP (chuts vs. purrs; one vs. one), the model trained to classify each call type from all other call types (one vs. many) was more predictive of behavior as indicated by higher R<sup>2</sup> (<bold>B</bold>) and lower MAE (<bold>C</bold>). (<bold>D–F</bold>) Same as (<bold>A–C</bold>) but for the wheeks versus whines task. The performance of an SVM classifier that uses the long-term spectrum to classify natural and manipulated calls is shown in <xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>. The performance of a feature-based classifier, with feature duration constrained to 75 ms, is shown in <xref ref-type="fig" rid="fig11s2">Figure 11—figure supplement 2</xref>. GP, guinea pig; MAE, mean absolute error; SVM, support vector machine.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig11-v2.tif"/></fig><fig id="fig11s1" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 1.</label><caption><title>Performance of an SVM classifier on call categorization based on long-term spectrum.</title><p>An SVM classifier, trained to categorize calls from long-term spectrum alone, did not capture many aspects of GP behavior across the various stimulus manipulations. Black circles and lines correspond to GP behavioral performance (mean ± SEM). Behavioral data are reproduced from <xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig9">Figure 9</xref>. Orange circles correspond to SVM performance. Left column corresponds to the chuts versus purrs task and right column corresponds to the wheeks versus whines task. GP, guinea pig; SVM, support vector machine.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig11-figsupp1-v2.tif"/></fig><fig id="fig11s2" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 2.</label><caption><title>Call categorization performance of a feature-based model with feature duration restricted to 75 ms.</title><p>Based on behavioral results from the segment length experiment, we re-trained a feature-based model with maximum feature duration restricted to 75 ms to classify calls. The restricted-MIF model outper formed the model described in the main manuscript.This result illustrates how future experimental data could be leveraged to generate additiona lconstraints for the feature-based categorization model. Black circles and lines correspond to GP behavioral performance (mean ± SEM). Behavioral data are reproduced from <xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig9">Figure 9</xref>. Orange circles correspond to SVM performance. Left column corresponds to the chuts versus purrs task and right column corresponds to the wheeks versus whines task. GP, guinea pig; MIF, most informative feature; SVM, support vector machine.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig11-figsupp2-v2.tif"/></fig></fig-group><p>To determine whether simpler models based on the overall spectral content of vocalizations could explain some of these results, we built a support vector machine (SVM) classifier that attempted to classify vocalizations based on the long-term spectra of calls (see Materials and methods). Similar to the feature-based model, a WTA stage was implemented by comparing the outputs of the target-call SVM model and the distractor-call SVM model for each input call (response=Go if target-SVM output&gt;distractor SVM output). The spectrum-based SVM model achieved high performance on natural vocalizations (<italic>d</italic>′=3.48 for novel chuts vs. purrs and 2.65 for novel wheeks vs. whines), but failed to capture many aspects of GP responses to manipulated calls, such as the robust performance at low SNRs and modulation of performance by F0-shifted calls. Overall, this resulted in a much lower fraction of the variance explained (<xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>). These analyses illustrate that although a simpler model might successfully classify natural vocalizations, applying the model to manipulated stimuli reveals the shortcomings of the spectrum-based model in capturing GP behavioral trends, further highlighting the need for using spectrotemporal features for classification.</p></sec><sec id="s2-9"><title>Comparing models with different training procedures yields insight into GP behavioral strategy</title><p>The high explanatory power of the feature-based model could be leveraged to gain further insight into what information the GPs were using or learning to accomplish these categorization tasks. On the one hand, because GPs are exposed to these call categories from birth, the GPs may simply be employing the features that they have already acquired for call categorization over their lifetimes to solve our specific categorization tasks. The model presented so far is aligned with this possibility—we trained features to categorize one call type from all other call types (one vs. many categorization) and used a large number of call exemplars for training. Alternatively, GPs could be de-novo learning stimulus features that distinguished between the particular Go and No-go exemplars we presented during training. To test this possibility, we re-trained the model only using the eight exemplars of the targets and distractors that we used to train GPs for one versus one categorization. When tested on manipulated calls, the one versus one model typically performed poorly compared to the original one versus many model. Compared to the one versus many model, the one versus one model was less consistent with behavior as indicated by lower <italic>R</italic><sup>2</sup> (<xref ref-type="fig" rid="fig11">Figure 11B, E</xref>) and higher MAE values (<xref ref-type="fig" rid="fig11">Figure 11C, F</xref>). One explanation for this result could be that eight exemplars of each category are insufficient to adequately train the model. But the one versus one model achieved suprathreshold (<italic>d′</italic>&gt;1) performance on both the training and generalization call exemplars, suggesting that these training data were, to some degree, sufficient for classifying natural calls. Furthermore, if GPs were indeed learning stimulus features from the training data set, they would also face the same constraints on training data volume as the model. A second explanation is that the one versus many model better matches GP behavior because rather than re-learning new task-specific features, GPs might be using call features that they had acquired previously over their lifespan to solve our call categorization task. These results also suggest that training a feature-based categorization system (in-silico or in-vivo) on exemplars that capture within-category variability is critical to obtain a system that can flexibly adapt and maintain robust performance to unheard stimuli that exhibit large natural or artificial variations.</p><p>The effect of training our model on the one versus many categorization task using a large number of call exemplars for training was that the model learned features that truly captured the within-class and outside-class variability of calls. This resulted in a model that accurately predicted GP performance across a range of stimulus manipulations. To understand how the model was able to achieve robustness to stimulus variations, and to gain insight into how GPs may flexibly weight features differently across the various stimulus manipulations, we examined the relative detection rates of various model MIFs across different stimulus paradigms in which we observed strong behavioral effects (<xref ref-type="fig" rid="fig12">Figure 12</xref>, <xref ref-type="fig" rid="fig12s1">Figure 12—figure supplement 1</xref>).</p><fig-group><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Different subsets of MIFs are flexibly recruited to solve categorization tasks for different manipulations.</title><p>(<bold>A</bold>) We estimated the relative detection rate (i.e., the difference between the detection rate of a given MIF for all within-category and outside-category calls) of all MIFs (discs) for each behavioral paradigm (e.g., SNR). Colors denote different instantiations of the MIFs. Disc diameter is monotonically proportional to the relative detection rate, using a power-law relationship (fourth power) to highlight the most robust features. While MIFs of all center frequencies (CFs) and bandwidths were uniformly recruited for generalizing calls of chut call type, MIFs with lower CFs were preferentially selected for SNR conditions, likely because high-frequency chut features were masked by white noise. In contrast, MIFs with high CF were preferred by the model to solve the F0-shift task. (<bold>B</bold>) Similar results were obtained for purrs. (<bold>C</bold>) MIFs of all durations and bandwidths were uniformly recruited for generalizing calls of chut call type. In contrast, shorter duration MIFs were preferred for segment-length conditions whereas longer-duration MIFs were preferentially recruited for F0-shift conditions. (<bold>D</bold>) Results were similar for purrs. Source data are available in <xref ref-type="supplementary-material" rid="fig12sdata1">Figure 12—source data 1</xref>. Similar analyses for wheeks and whines are presented in <xref ref-type="fig" rid="fig12s1">Figure 12—figure supplement 1</xref>.</p><p><supplementary-material id="fig12sdata1"><label>Figure 12—source data 1.</label><caption><title>Characteristics of most informative features (CF, bandwidth, and duration) and their relative detection rates for the various stimulus manipulations.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-78278-fig12-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig12-v2.tif"/></fig><fig id="fig12s1" position="float" specific-use="child-fig"><label>Figure 12—figure supplement 1.</label><caption><title>Different subsets of MIFs are flexibly recruited to solve categorization tasks for different manipulations (wheeks vs. whines).</title><p>(<bold>A</bold>, <bold>B</bold>) We estimated the relative detection rate (i.e., the difference between the detection rate of a given MIF for all target and all distractor calls) of all MIFs (discs) for each behavioral paradigm (e.g., SNR). Colors denote different instantiations of the MIFs. Disc diameter is monotonically proportional to the relative detection rate, using a power-law relationship (fourth power) to highlight the most robust features. While MIFs of all center frequencies (CFs) and bandwidths were uniformly recruited for generalizing calls of each call type, MIFs with lower CFs were preferentially selected for SNR conditions, likely because highfrequency features were masked by white noise. In contrast, MIFs with high CF were preferred by the model to solve the F0-shift task. These differences were especially apparent for whine calls. (<bold>C, D</bold>) MIFs of all durations and bandwidths were uniformly recruited for generalizing calls of each call type. In contrast, shorter duration MIFs were preferred for segment-length conditions whereas longer duration MIFs were preferentially recruited for F0-shift conditions. MIF, most informative feature; SNR, signal-to-noise ratio.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78278-fig12-figsupp1-v2.tif"/></fig></fig-group><p>In <xref ref-type="fig" rid="fig12">Figure 12</xref>, we plot the relative detection rates of MIFs (discs, ~20 MIFs per instantiation) from different model instantiations (colors, 5 instantiations) for the different call types used. That is, we computed the difference between the rate of detection of each MIF (for each call category) in response to within-category and outside-category stimuli, and plotted this difference (disc areas) as a function of MIF tuning properties (CF, bandwidth, and duration). For the generalization stimuli, that is, new natural calls on which the model had not been trained, almost all MIFs showed relatively large net detection rates which resulted in plots (<xref ref-type="fig" rid="fig12">Figure 12A–D</xref>, left panels) with discs of about equal area. For example, all chut MIFs were detected at high rates in novel chut calls (within-category calls), and detected at low rates in non-chut call types (outside-category calls). Note, however, that the learned MIFs are spread out across a range of CFs, bandwidths, and durations. Given these data alone, one might argue that learning ~20 MIFs per call category is highly redundant, and that high performance could be achieved using only a subset of these MIFs. But examining which features maintain high relative detection rates in other stimulus paradigms underscores the utility of learning this wide feature set. When we added white noise to the stimulus, low-CF features showed higher relative detection rates (<xref ref-type="fig" rid="fig12">Figure 12A, B</xref>, right top) and thus contributed more towards categorization. This could likely be attributed to GP calls having high power at low frequencies, resulting in more favorable local SNRs at lower frequencies. But when we altered stimulus F0, high-CF features contributed more towards categorization (<xref ref-type="fig" rid="fig12">Figure 12A, B</xref>, right bottom). Similarly, low-duration, high-bandwidth features contributed more when categorizing time-restricted calls, whereas high-duration, low-bandwidth features contributed more when categorizing F0-shifted calls (<xref ref-type="fig" rid="fig12">Figure 12C, D</xref>). That the model quantitatively matched GP behavior suggests that a similar strategy might be employed by GPs as well. Note that our contention is not that the precise MIFs obtained in our model are also the precise MIFs used by the GPs—indeed, we were able to train several distinct MIF sets that were equally proficient at categorizing calls. Rather, we are proposing a framework in which GPs learn intermediate-complexity features that account for within-category variability and best contrast a call category from all other categories, and similar to the model, recruit different subsets of these features to solve different categorization tasks.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we trained GPs to report call categories using an appetitive Go/No-go task. We then tested GP call categorization when we challenged them with spectrally and temporally manipulated calls. We found that GPs maintained their call categorization across a wide range of gross temporal manipulations such as changes to tempo and altered ISI distributions. In contrast, GP behavior was strongly affected by altering the F0 of calls. In parallel, to determine which features GPs might be using to extract knowledge of call category from stimuli, we extended a previously developed feature-based model of auditory categorization by adding a WTA feature-integration stage that enabled us to obtain a categorical decision from the model on a trial-by-trial basis. We trained the model using natural GP calls. When we challenged the model with the identical stimuli used in the GP experiments, we obtained model responses that explained ~50% of the overall variance in GP behavior. This result suggests that learning to generalize over the variability in natural call categories resulted in the model being able to generalize over some artificial call manipulations. That the model tracks GP performance on these manipulated stimuli is a key observation that supports our reasoning that GPs learn model-like features to generalize over call variability and accomplish call categorization. We had previously reported electrophysiological support for the feature-based model by demonstrating that a large fraction of neurons in the superficial layers of auditory cortex exhibited feature-selective responses, resembling the FD stage of the model (<xref ref-type="bibr" rid="bib30">Montes-Lourido et al., 2021a</xref>). The results described in the present manuscript lend further support to the model at a behavioral level. Taken together, these studies strongly suggest how a spectral content-based representation of sounds at lower levels of auditory processing may be transformed into a goal-directed representation at higher processing stages by extracting and integrating task-relevant features.</p><p>The feature-based model was highly predictive of GP behavior, although it was conceptualized from purely theoretical considerations, trained only using natural GP calls, and implemented without access to any behavioral data. We developed the feature-based model to capture how knowledge of auditory categories is learned by encoding informative features, but did not account for how this knowledge is expressed during behavior. Rather, we made minimal assumptions and used a WTA framework with a static decision criterion (with a small amount of error). Insights from our behavioral observations could be used to further refine the model by adding a realistic behavioral expression stage. For example, our data indicated that GPs altered their behavioral strategy over the course of multiple sessions within a given day. This could possibly reflect an early impulsivity in their decision-making brought on by food deprivation (evidenced by a FA rate) that gradually switches to a punishment-avoidance strategy with increasing satiation (although <italic>d′</italic> remains consistent across sessions). Additional arousal-dependent effects on behavior are likely. Nevertheless, the fact that the current model could explain ~50% of the variance in behavioral trends we observed suggests that the fundamental strategy employed by the model—that of detecting features of intermediate complexity to generalize over within-category variability—also lies at the core of GP behavior.</p><p>Furthermore, we could leverage the model to gain insight into possible behavioral strategies used by GPs in performing the tasks. For example, we could compare models trained to categorize calls in one versus many or one versus one conditions to ask which scenario was more consistent with GP behavior: (1) whether the GPs used prior features that they acquired over their lifetimes to categorize a given call type from all other calls, or (2) whether GPs were de-novo learning new features to solve the particular categorization task on which they were trained. While the relatively lower volume of data used to train the one vs. one model is a potential confound, the model trained on call features that distinguish a particular call from all other calls was more closely aligned with GP behavioral data, supporting the first possibility. Examining how different subsets of features could be employed to solve different categorization tasks revealed possible strategies that GPs might use to flexibly recruit different feature representations to solve our tasks. While we have used GPs as an animal model for call categorization in this study, we have previously shown that the feature-based model shows high performance across species (GPs, marmosets, and macaques), and that neurons in the primary auditory cortex (A1) of marmosets also exhibit feature-selective responses (<xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>). Thus, it is likely that our model reflects general principles that are applicable across species, and offers a powerful new approach to deconstruct complex auditory behaviors.</p><p>On the behavioral side, previous GP studies have largely used conditioned avoidance (e.g., <xref ref-type="bibr" rid="bib20">Heffner et al., 1971</xref>) or classical conditioning using aversive stimuli (e.g., <xref ref-type="bibr" rid="bib11">Edeline et al., 1993</xref>) to study simple tone detection and discrimination behaviors. Studies probing GP behaviors using more complex stimuli are rare (e.g., <xref ref-type="bibr" rid="bib34">Ojima et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Ojima and Horikawa, 2015</xref>). Our study of GP call categorization behavior using multiple spectrotemporally rich call types and parametric manipulations of spectral and temporal features offers comprehensive insight into cues that are critical for call categorization and builds significantly on previous studies. First, we showed that GPs can categorize calls in challenging SNRs, and that thresholds vary depending on the call types to be categorized. We demonstrated that information for GP call categorization was available in short-duration segments of calls, and consistent with some previous studies in other species (<xref ref-type="bibr" rid="bib21">Holfoth et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Knudsen and Gentner, 2010</xref>; <xref ref-type="bibr" rid="bib28">Marslen-Wilson and Zwitserlood, 1989</xref>; <xref ref-type="bibr" rid="bib40">Pitcher et al., 2012</xref>), GPs could extract call category information soon after call onset. GP call perception was robust to large temporal manipulations, such as reversal and larger changes to tempo than have been previously tested (<xref ref-type="bibr" rid="bib33">Neilans et al., 2014</xref>). These results are also consistent with the resilience of human word identification to large tempo shifts (<xref ref-type="bibr" rid="bib22">Janse et al., 2003</xref>). Our finding that GP call categorization performance is robust to ISI manipulations is also not necessarily inconsistent with results from mice (<xref ref-type="bibr" rid="bib37">Perrodin et al., 2020</xref>); in that study, while female mice were found to strongly prefer natural calls compared to calls with ISI manipulations, it is possible that mice still identified the call category correctly. For gross spectral manipulations, we found that GP call categorization was robust to a larger range of F0-shifts than have been previously tested (<xref ref-type="bibr" rid="bib33">Neilans et al., 2014</xref>). Critically, for all but one of these manipulations, the feature-based model captured GP behavioral trends with surprising accuracy both qualitatively and quantitatively.</p><p>An analysis of model deviation from behavior could suggest a roadmap for future improvements to our model that could yield further insight into auditory categorization. The one paradigm where we observed a systematic under-performance of the model compared to GP behavior was when we presented call segments of varying lengths from call onset. While the GPs were able to accomplish categorization by extracting information from as little as 75 ms segments, the model required considerably more information (~150 ms). This is likely because the model was based on the detection of informative features that were on average of ~110 ms duration, which were identified from an initial random set of features that could be up to 200 ms in duration. We set this initial limit based on an upper limit estimated from electrophysiological data recorded from A1 ( <xref ref-type="bibr" rid="bib30">Montes-Lourido et al., 2021a</xref>). We consciously did not impose further restrictions on feature duration or bandwidth to ensure that the model did not make any assumptions based on observed behavior. Upon observing this deviation from behavior, to test whether restricting feature lengths to shorter durations would further improve model performance, we repeated the modeling by constraining the initial feature length to a maximum of 75 ms (the lowest duration for which GPs show above-threshold performance). We found that the constrained MIF model better matched GP behavior on the segment-length task (<italic>R</italic><sup>2</sup> of 0.62 and 0.58 for the chuts vs. purrs and wheeks vs. whines tasks) with the model exceeding <italic>d</italic>′=1 for 75 ms segments for most tested cases. The constrained MIF model maintained similarity to behavior for the other manipulations as well (<xref ref-type="fig" rid="fig11s2">Figure 11—figure supplement 2</xref>), and yielded higher overall <italic>R</italic><sup>2</sup> values (0.66 for chuts vs. purrs, 0.51 for wheeks vs. whines), explaining ~59% of the variance in GP behavior. This result illustrates how behavioral experiments can provide constraints for the further development of theoretical models in the future.</p><p>We also observed the over-performance of the model compared to behavior in some paradigms. Some of this over-performance might be explained by the fact that the model does not exhibit motivation changes, and so forth, as outlined above. A second source of this over-performance might arise from the fact that our model integrates evidence from the FD stage perfectly, that is, we take the total evidence for the presence of a call category to be the weighted sum of the log-likelihoods of all detected features (counting detected features only once) over the entire stimulus, and do not explicitly model a leaky integration of feature evidence over time, as is the case in evidence-accumulation-to-threshold models (<xref ref-type="bibr" rid="bib7">Cheadle et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Keung et al., 2020</xref>). Future improvements to the model could include a realistic feature-integration stage, where evidence for a call category is generated when a feature is detected and degrades with a biologically realistic time constant. In this case, a decision threshold could be reached before the entire stimulus is heard, but model parameters would need to be derived from or fit to observed behavioral data (<xref ref-type="bibr" rid="bib16">Glaze et al., 2015</xref>).</p><p>How do the proposed model stages map onto the auditory system? In an earlier study, we provided evidence that feature detection likely occurs in the superficial layers of A1, in that a large fraction of neurons in this stage exhibits highly selective call responses and complex spectrotemporal receptive fields (<xref ref-type="bibr" rid="bib30">Montes-Lourido et al., 2021a</xref>). How and at what stage these features are combined to encode a call category remains an open question. Neurons in A1 can acquire categorical or task-relevant responses to simple categories, for example, low versus high tone frequencies, or low versus high temporal modulation rates, with training (<xref ref-type="bibr" rid="bib2">Bao et al., 2004</xref>; <xref ref-type="bibr" rid="bib12">Fritz et al., 2005</xref>). In contrast, categorical responses to more complex sounds or non-compact categories only seem to arise at the level of secondary or higher cortical areas or the prefrontal cortex (<xref ref-type="bibr" rid="bib43">Russ et al., 2008</xref>; <xref ref-type="bibr" rid="bib55">Yin et al., 2020</xref>), which may then modulate A1 via descending connections. These results, taken together with studies that demonstrate enhanced decodability of call identity from the activity of neurons in higher cortical areas (<xref ref-type="bibr" rid="bib13">Fukushima et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Grimsley et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Grimsley et al., 2011</xref>), suggest that secondary ventral-stream cortical areas, such as the ventral-rostral belt in GPs, are promising candidates as the site of evidence integration from call features. The WTA stage may be implemented via lateral inhibition at the same level using similar mechanisms as has been suggested in the primary visual cortex (<xref ref-type="bibr" rid="bib8">Chettih and Harvey, 2019</xref>) or may require a further upstream layer. Further experiments are necessary to explore these questions.</p><p>The feature-based model we developed offers a trade-off between performance and biological interpretability. Modern deep neural network (DNN) based models can attain human-level performance (e.g., in vision: <xref ref-type="bibr" rid="bib41">Rajalingham et al., 2015</xref>, in audition: <xref ref-type="bibr" rid="bib23">Kell et al., 2018</xref>) but what features are encoded at the intermediate network layers remain somewhat hard to interpret. These models also typically require vast quantities of training data. In contrast, our model is based on an earlier model for visual categorization (<xref ref-type="bibr" rid="bib52">Ullman et al., 2002</xref>) that is specifically trained to detect characteristic features that contrast the members of a category from non-members. Thus, we can develop biological interpretations for what features are preferably encoded and more importantly, why certain features are more advantageous to encode. Because the features used in the model are the most informative parts of the calls themselves, they can be identified without a parametric search. This approach is especially well-suited for natural sounds such as calls that are high-dimensional and difficult to parameterize. We are restricted, however, in that we do not know all possible categorization problems that are relevant to the animal. By choosing well-defined categorization tasks that are ethologically critical for an animal’s natural behavior (such as call categorization in the present study), we can maximize the insight that we can derive from these experiments as it pertains to a range of natural behaviors. In the visual domain, the concept of feature-based object recognition has yielded insight into how human visual recognition differs from modern machine vision algorithms (<xref ref-type="bibr" rid="bib53">Ullman et al., 2016</xref>). Our results lay the foundation for pursuing an analogous approach for understanding auditory recognition in animals and humans.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All experimental procedures conformed to the NIH Guide for the use and care of laboratory animals and were approved by the Institutional Animal Care and Use Committee of the University of Pittsburgh (protocol number 21069431).</p><sec id="s4-1"><title>Animals</title><p>We acquired data from four male and three female adult, wild-type, pigmented GPs (Elm Hill Labs, Chelmsford, MA), weighing ~500–1000 g over the course of the experiments. After a minimum of 2 weeks of acclimatization to handling, animals were placed on a restricted diet for the period of behavioral experiments. During this period, GPs performed auditory tasks for food pellet rewards (TestDiet, St. Louis, MO). The weight and body condition of animals were closely monitored and the weight was not allowed to drop below 90% of baseline weight. To maintain this weight, depending on daily behavioral performance, we supplemented GPs with restricted amounts of pellets (~10–30 g), fresh produce (~10–30 g), and hay (~10–30 g) in their home cages. All animals had free access to water. After behavioral testing for ~2–3 weeks, animals were provided ad-libitum food for 2–3 days to obtain an updated estimate of their baseline weights.</p></sec><sec id="s4-2"><title>Behavioral setup</title><p>All behavioral tasks were performed inside a custom behavioral booth (<xref ref-type="fig" rid="fig1">Figure 1</xref>; ~90×60×60 cm<sup>3</sup>) lined with ~1.5 cm thick sound attenuating foam (Pinta Acoustic, Minneapolis, MN) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The booth was divided into two halves (~45×60×60 cm<sup>3</sup> each) using transparent acrylic (McMaster-Carr, Los Angeles, CA). One half contained the behavioral setup. The other half was sometimes used as an observation chamber in which we placed a naive GP to observe an expert GP perform tasks; such social learning has been shown to speed up behavioral task acquisition (<xref ref-type="bibr" rid="bib36">Paraouty et al., 2020</xref>). The entire booth was uniformly lit with LED lighting. The behavioral chamber contained a ‘home base’ area and a reward region (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). A water bottle was placed in the home base to motivate animals to stay at/return to the home base after each trial. A pellet dispenser (ENV-203-45, Med Associates, Fairfax, VT) was used to deliver food pellets (TestDiet) onto a food receptacle placed in a corner of the reward area. Air puffs were delivered from a pipette tip placed near the food receptacle directed at the animal’s snout. The pipette tip was connected using silicone tubing via a pinch valve (EW98302-02, Cole-Palmer Instrument Co., Vernon Hills, IL) to a regulated air cylinder, with the air pressure regulated to be about 25 psi.</p><p>The animal’s position within the behavioral chamber was tracked using MATLAB (Mathworks, Inc, Natick, MA) at a video sampling rate of ~25 fps using a web camera (Lifecam HD-3000, Logitech, Newark, CA) placed on the ceiling of the chamber. Sound was played from a speaker (Z50, Logitech) located ~40 cm above the animal at ~70 dB SPL with a sampling frequency of 48 kHz. Pellet-delivery, illumination, and air puff hardware were controlled using a digital input/output device (USB-6501, National Instruments, Austin, TX).</p></sec><sec id="s4-3"><title>Basic task design</title><p>All behavioral paradigms were structured as Go/ No-go tasks. GPs were required to wait in the home base (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) for 3–5 s to initiate a trial. A Go or No-go stimulus was presented upon trial initiation. For Go stimuli, moving to the reward area (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) was scored as a hit and resulted in a food pellet reward; failure to do so was scored as a miss. For No-go stimuli, moving to the reward area was scored as a FA and was followed by a mild air puff and brief time-out with the lights turned off (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), whereas staying in the home base was scored as a correct rejection.</p></sec><sec id="s4-4"><title>Training GPs via social learning and appetitive reinforcement</title><p>Naïve animals were initially placed in the observer chamber while an expert GP performed the task in the active chamber. Such social learning helped accelerate forming an association between sound presentation and food reward (<xref ref-type="bibr" rid="bib36">Paraouty et al., 2020</xref>). Following an observation period of 2–3 days, naive GPs were placed in the active chamber alone and underwent a period of Pavlovian conditioning, where Go stimuli were played, and food pellets were immediately dropped until the animals built an association between the sound and the food reward. Once GPs began to reliably respond to Go stimuli, No-go stimuli along with the air puff and light-out were introduced at a gradually increasing frequency (until about equal frequency of both Go and No-go stimuli). We trained two cohorts of four adult GPs ( two males and two females) for two call categorization tasks (as discussed later), with the overlap of one GP between the tasks.</p></sec><sec id="s4-5"><title>Stimuli and behavioral paradigms</title><sec id="s4-5-1"><title>Learning</title><p>In this study, we trained GPs to categorize two similar low-frequency, temporally symmetric, affiliative call categories (‘<italic>chuts</italic>’—Go and ‘<italic>purrs</italic>’—No-go, <xref ref-type="fig" rid="fig1">Figure 1C</xref>), or two temporally asymmetric call categories with non-overlapping frequency content (‘<italic>wheeks</italic>’—Go and ‘<italic>whines</italic>’—No-go, <xref ref-type="fig" rid="fig1">Figure 1D</xref>). All calls were recorded in our laboratory as described earlier (<xref ref-type="bibr" rid="bib31">Montes-Lourido et al., 2021b</xref>) and were from animals unfamiliar to the GPs in the present study. Calls were trimmed to ~1 s length, normalized by their rms amplitudes, and presented at ~70 dB SPL (<xref ref-type="fig" rid="fig1">Figure 1C, D</xref>). Different sets of randomly selected calls, each set containing eight different exemplars, were used for the learning and generalization phases. Other paradigm-specific stimuli were generated by manipulating the call sets used during the learning phase as explained below. We first manually trained animals to associate one corner of the behavioral chamber with food pellet rewards. Following manual training, we began a conditioning phase where we only presented Go stimuli when the animal was in the home base area followed by automated pellet delivery, gradually increasing the interval between stimulus and reward. Once animals began moving toward the reward location in anticipation of the reward, we gradually introduced an increasing proportion of No-go stimuli, and began tracking the performance of the animal. During the learning phase, animals typically performed the Go/No-go task for 6 sessions each day with ~40 trials per session. Each session typically lasted ~10 min.</p></sec><sec id="s4-5-2"><title>Generalization to new exemplars</title><p>Once animals achieved <italic>d′</italic>&gt;1.5 on the training stimulus set, we replaced all training stimuli with eight new exemplars of each call category that the animals had not heard before. To minimize exposure to the new exemplars, we tested generalization for about 3 days per animal, with 1–2 sessions with training exemplars and 1–2 sessions of new exemplars.</p></sec><sec id="s4-5-3"><title>Call-in-noise</title><p>To generate call-in-noise stimuli at different SNRs, we added white noise of equal length to the calls (gated noise) such that the SNR ratio, computed using rms amplitudes, varied between –18 dB and +12 dB SNR (i.e., −18, –12, −6, 3, 0, +3, +6, and +12 dB SNR). This range of SNRs was chosen to maximize sampling over the steeply growing part of psychometric curve fits. We presented these stimuli in a block design, measuring GP behavior in sessions of ~40 trials with each session having a unique SNR value. We typically collected data for three sessions for each of the nine SNR levels including the clean call. SNR data were collected across several days per animal, with different SNRs tested each day to account for possible fluctuations in motivation levels.</p></sec><sec id="s4-5-4"><title>Restricted segments</title><p>To investigate how much information is essential for GPs to successfully categorize calls, we created call segments of different lengths (50, 75, 100, 125, 150, 175, 200, 300, 400, 500, 600 700, and 800 ms) from the call onsets. We chose 800 ms as the maximum segment length since our briefest call was ~800 ms long. We tested GPs on these 13 segment lengths, presenting 5 repetitions of 8 exemplars per category. A randomized list of all 1040 trials was created (2 categories×8 exemplars×13 time-chunk lengths×5 repetitions) and presented sequentially in sessions of ~40 trials, completing ~240 trials per day (~5 days to complete the entire list of stimuli).</p></sec><sec id="s4-5-5"><title>Tempo manipulation</title><p>To temporally compress or stretch the calls without introducing any alterations to long-term spectra, we changed the tempo of the calls using Audacity software (high-quality option, using a Subband Sinusoidal Modeling Synthesis algorithm). The algorithm detects peaks in the short-term Fourier Transform of short sound segments and resynthesizes the segment using a phase-preserving oscillator bank at the required time scale. Tempo was changed by –120%, –100%, –80%, –60%, –30%, +30%, +60%, and +80% which resulted in calls that were ~0.45, 0.5, ~0.56, ~0.63, ~0.77, ~1.43, 2.5, and 5 times the original lengths of the calls, respectively. As earlier, 720 total trials were presented (2 categories×8 exemplars×9 tempo conditions×5 repetitions).</p></sec><sec id="s4-5-6"><title>ISI manipulations</title><p>To determine if GPs used individual syllables or temporal patterns of syllables for call categorization, we introduced several ISI manipulations, while keeping the individual syllables intact. After manually identifying the beginnings and endings of each syllable within the calls, the syllables and the ISI values were extracted using MATLAB. Since our recorded calls have some level of background noise, we first created a set of control stimuli where the audio in the ISI was replaced with silence. As a second control, we changed the ISI values by randomly drawing ISI values from the ISI distribution of the same call category. Five such new calls were generated from each original call. We acquired behavioral responses using a randomized presentation strategy as above, split into (1) 640 trials with regular ISI (with background recording noise) and silent ISI (2 categories×8 exemplars×2 conditions×20 repetitions), and (2) 400 trials with random within-category ISI values (2 categories×8 exemplars×5 random ISI combinations×5 repetitions). We then generated chimeric calls with syllables belonging to one category and ISI values belonging to the other category (e.g., chut syllables with purr ISI values). Five such chimeric calls were created per original call. Because these calls contain information from both categories, we adopted a catch-trial design for this experiment. Natural calls (Syllable and ISI from the same category, both Go and No-go categories) were presented on 67% of trials, and chimeric calls on 33% of trials (catch trials). We rewarded 50% of the catch trials at random and did not provide any negative reinforcement (air puff or time-out). Thus, 1200 randomized trials were presented, with 800 trials with regular calls and 400 catch trials with chimeric calls.</p></sec><sec id="s4-5-7"><title>Call reversal</title><p>As a final gross temporal manipulation, we temporally reversed the calls. We presented a total of 160 trials in randomized order (2 categories×8 exemplars×2 conditions×5 repetitions) for this experiment.</p></sec><sec id="s4-5-8"><title>Fundamental frequency manipulation</title><p>We created calls with fundamental frequency (F0) varying from one octave lower and to one octave higher by changing the pitch of the calls by –50%, –40%, –30%, –20%, 20%, 40%–50%, and 100% using Audacity software. These pitch changes re-interpolated the calls such that call length and tempo were preserved. A total of 720 trials (2 categories×8 exemplars×9 F0 conditions×5 repetitions) were presented in randomized order for this experiment.</p></sec><sec id="s4-5-9"><title>Low pass filtering</title><p>For the wheeks versus whines task, we low pass filtered both wheeks and whines at 3 kHz using a 256-point FIR filter in MATLAB. We presented 160 trials (2 categories×8 exemplars×2 conditions×5 repetitions) in randomized order for this experiment.</p></sec></sec><sec id="s4-6"><title>Analysis of behavioral data</title><p>All analysis was performed in MATLAB. Specific functions and toolboxes used are mentioned where applicable below.</p><p>To quantify the behavioral performance of the animals, we used the sensitivity index or <italic>d′</italic> (<xref ref-type="bibr" rid="bib17">Green and Swets, 1966</xref>), defined as:<disp-formula id="equ1">  <label>(1)</label><mml:math id="m1"><mml:mi>d</mml:mi><mml:mi>`</mml:mi><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>Z</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <italic>H</italic> and <italic>FA</italic> represent the hit rate and FA rate, respectively. To avoid values that approach infinity, we imposed a floor (0.01) and ceiling (0.99) on hit rates and FA rates.</p><p>For the learning and generalization data, the <italic>d′</italic> value was estimated per session using the H and FA rates from that session. These session-wise hit rates, FA rates and <italic>d′</italic> estimates were averaged for each animal and the mean and standard error of mean (s.e.m.) across all animals are reported in the results section.</p><p>For all the call manipulation experiments (including call-in-noise), a single hit rate, FA rate and <italic>d′</italic> were estimated per condition per animal by pooling data over all trials corresponding to each condition. The mean and SEM of these indices across all animals are reported in the results section.</p><p>Additionally, for the call-in-noise data, we used the ‘fitnlm’ MATLAB function (Statistics toolbox) to fit psychometric functions of the form (<xref ref-type="bibr" rid="bib54">Wichmann and Hill, 2001</xref>):<disp-formula id="equ2">  <label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>F</italic> is the Weibull function, defined as <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>⟮</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⟯</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , α is the shift parameter, β is the slope parameter, and λ is the lapse rate.</p></sec><sec id="s4-7"><title>Statistical analyses</title><p>We used paired t-tests to compare <italic>d′</italic> values across animals in experiments with only two conditions, that is, reversal and low-pass filtering. For the remaining experiments with more than two conditions (segment length, SNR, frequency shift, and tempo shift), we fit generalized linear mixed-effect models with logit link functions to the binary (Go/Nogo) behavioral and model responses on a trial-by-trial basis using the stimulus type (Go/Nogo), parameter value (for example, SNR level or frequency shift magnitude), and for length-shift and frequency-shift experiments where positive as well as negative shifts were possible, parameter sign as predictors, an interaction term between stimulus type and parameter value, and animal ID as a random effect (<xref ref-type="disp-formula" rid="equ3 equ4">Equations 3 and 4</xref>). To evaluate whether the parameter value and sign significantly modulated behavioral responses after accounting for stimulus type, we compared the above full models to null models that had only the stimulus type as a predictor and animal ID as a random effect (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). We fit these models in R (version 4.2.1) using the ‘glmer’ function in the lme4 package (<xref ref-type="bibr" rid="bib3">Bates et al., 2014</xref>), and compared the full and null models using the ANOVA function in the ‘stats’ package in R. Statistical outputs from R are available as supplementary information (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>Full model (SNR and segment length manipulations):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>∼</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Full model (frequency shift and length shift manipulations):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>∼</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Null model:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>∼</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, for the swapped ISI stimuli in the ISI manipulation experiments, since we did not have well-defined categories for the chimeric calls, we chose to compare the Go-rates for the stimuli with syllables of one kind using a paired <italic>t</italic>-test. All statistical outputs are available in supplementary material (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p></sec><sec id="s4-8"><title>Feature-based categorization model</title><p>To gain insight into what potential spectrotemporal features GPs may be using to accomplish call categorization in the behavioral tasks, we extended a previously published feature-based model that achieves high classification performance for categorizing several call types across several species, including GPs (<xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>). The model consists of three layers: (1) a spectrotemporal representational layer, (2) a feature detection (FD) layer, and (3) a competitive WTA decision layer. The first two layers are closely based on <xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>; we briefly describe these stages below. The WTA layer combines information from the FD layer to form a Go/No-go decision.</p><p>The spectrotemporal representational layer consisted of the output of a biologically realistic model of the auditory periphery (<xref ref-type="bibr" rid="bib56">Zilany et al., 2014</xref>). Cochleagrams of training and testing calls were constructed from the inner-hair-cell voltage output of this model (<xref ref-type="bibr" rid="bib56">Zilany et al., 2014</xref>). Cochleagrams were constructed using 67 characteristic frequencies logarithmically spaced between 200 Hz and 20 kHz and were sampled at 1 kHz. Model parameters were set to follow healthy inner and outer hair cell properties and cat frequency tuning.</p><p>For the FD layer, we trained four separate sets of feature detectors to classify the four call types, where each set classified a single call type (e.g., chut) from all other call types (i.e., a mixture of purr, wheek, whine, and other calls). During training, for each call type, we identified a set of maximally informative features (MIFs; see <xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>, based on an algorithm developed by <xref ref-type="bibr" rid="bib52">Ullman et al., 2002</xref>) that yielded optimal performance in classifying the target call type from other call types (<xref ref-type="fig" rid="fig2">Figure 2</xref>). To do so, we generated an initial set of 1500 candidate features by randomly sampling rectangular spectrotemporal blocks from the target call cochleagrams. We restricted the duration of features to a maximum of 200 ms, based on typically observed temporal extents of spectrotemporal receptive fields in superficial layers of the GP primary auditory cortex (<xref ref-type="bibr" rid="bib30">Montes-Lourido et al., 2021a</xref>). Next, we evaluated how well each feature classified the target call type from other call types. To do so, we obtained the maximum normalized cross-correlation value (<inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) of each feature with target calls and other calls. Each feature was assigned a threshold that indicated if the feature was detected in the stimulus (r<sub>max</sub> &gt;threshold) or not (r<sub>max</sub> &lt; threshold). We used mutual information to determine the utility of each feature in accomplishing the classification task. By testing a range of threshold values, we obtained the optimal threshold for each feature at which its categorization was maximal. The log-likelihood ratio of this binary classification was taken to be the weight of each feature. From this initial random set of 1500 features, we used a greedy search algorithm to obtain the set of maximally informative and least redundant features that achieved optimal performance to classify the training data set. The maximum number of these features was constrained to 20. The training performance of the MIF set was assessed by first estimating the receiver operating characteristic curve and then quantifying the area under the curve (AUC), using the procedure described in <xref ref-type="bibr" rid="bib27">Liu et al., 2019</xref>. To ensure the robustness of these solutions, we generated five instantiations of the MIFs for classifying each call type by iteratively determining an MIF set and removing these features from the initial set of features when training the next MIF set. We verified that training performance did not drop for any of these five instantiations.</p><p>Next, to compare model performance with GP behavioral performance, we evaluated model performance in classifying the same stimuli used in the behavioral experiments using the sensitivity metric, <italic>d′</italic>. To simulate the Go/No-go task, we employed a WTA framework, as described below. In a single trial, the stimulus could either be a target (Go stimulus) or a distractor (No-go stimulus). For this stimulus, we estimated the target FD-layer response as the sum of detected (target) MIF weights normalized by the sum of all (target) MIF weights. This normalization scales the model response to a range between 0 (no MIFs detected) and 1 (all MIFs detected). Similarly, we estimated the distractor model response as the sum of detected (distractor) MIF weights normalized by the sum of all (distractor) MIF weights. If the target FD-stage response was greater (less) than the distractor FD-stage response, then the WTA model would predict that the stimulus in that trial was a target (distractor). To allow for non-zero guess rate and lapse rate, as typically observed in behavioral data, we set the minimum and maximum Go probability of the WTA output to 0.1 and 0.9 (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). These Go probabilities [ <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> ] were realized on a trial-by-trial basis where a random number (<inline-formula><mml:math id="inf4"><mml:mi>X</mml:mi></mml:math></inline-formula>) drawn from a uniform distribution between 0 and 1 was compared with the WTA model Go probability to decide the final response [Go if <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> ]. <italic>d′</italic> was estimated from the hit rate and false alarm rate using <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. Identical test stimuli and number of trials were used for both behavior and model. We treated each of the five instantiations of the MIFs as a unique ‘subject’ for analysis.</p></sec><sec id="s4-9"><title>Spectrum-based categorization model</title><p>To determine whether call categories could be discriminated based on long-term spectral content, we constructed a SVM classifier. The long-term power spectrum (estimated from the cochleagram as the variance of the mean rate at each center frequency) was used as input to the model. Similar to feature-based models, SVMs were trained to classify a single call type from all other call types using the Matlab function <italic>fitcsvm</italic> (linear kernel, standardized [i.e., rescaled] predictors). The tenfold cross-validation losses for chut, purr, wheek, and whine SVMs were 14%, 4%, 10.6%, and 7.1%, which indicate small bias/variance for the SVM models. Similar to the feature-based model, a WTA stage was implemented by comparing the outputs of the target-call SVM model and the distractor-call SVM model for a single input call (response=GO if target-SVM output &gt;distractor SVM output). We then used this model to classify the manipulated stimuli and derive <italic>d′</italic> values for comparison with GP behavior.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Software, Investigation, Methodology</p></fn><fn fn-type="con" id="con6"><p>Investigation</p></fn><fn fn-type="con" id="con7"><p>Investigation</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures conformed to the NIH Guide for the Care and Use of Laboratory Animals and were approved by the institutional animal care and use committee of the University of Pittsburgh (protocol number 21069431).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Statistical outputs of GLM model comparisons.</title></caption><media xlink:href="elife-78278-supp1-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Statistical outputs of other tests used in the manuscript.</title></caption><media xlink:href="elife-78278-supp2-v2.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-78278-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data generated or analyzed during this study are included in the manuscript and supporting file; Source Data files have been provided for Figures 3 - 12.</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the National Institutes of Health, NIH R01DC017141 (SS) and start-up funds from the University of Pittsburgh. Dr Shi Tong Liu developed MATLAB code implementing the feature-based auditory categorization model. The authors are grateful to Dr Bryan Pfingst and Deborah Colesa (University of Michigan) for detailed advice regarding the food pellet rewards and pellet dispenser. Samuel Li assisted with the recording and curation of GP calls used in this study. The authors thank Jillian Harr, Sarah Gray, Julia Skrinjar, Brent Barbe, and Elizabeth Chasky for assistance with animal care. This research was supported in part by the University of Pittsburgh Center for Research Computing through the resources provided.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aushana</surname><given-names>Y</given-names></name><name><surname>Souffi</surname><given-names>S</given-names></name><name><surname>Edeline</surname><given-names>JM</given-names></name><name><surname>Lorenzi</surname><given-names>C</given-names></name><name><surname>Huetz</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robust neuronal discrimination in primary auditory cortex despite degradations of spectro-temporal acoustic details: comparison between guinea pigs with normal hearing and mild age-related hearing loss</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>19</volume><fpage>163</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1007/s10162-017-0649-1</pub-id><pub-id pub-id-type="pmid">29302822</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>S</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name><name><surname>Woods</surname><given-names>J</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Temporal plasticity in the primary auditory cortex induced by operant perceptual learning</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>974</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1038/nn1293</pub-id><pub-id pub-id-type="pmid">15286790</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Machler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fitting Linear Mixed-Effects Models Using Lme4</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://www.jstatsoft.org/article/view/v067i01">https://www.jstatsoft.org/article/view/v067i01</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berditchevskaia</surname><given-names>A</given-names></name><name><surname>Cazé</surname><given-names>RD</given-names></name><name><surname>Schultz</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Performance in a GO/NOGO perceptual task reflects a balance between impulsive and instrumental components of behaviour</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1038/srep27389</pub-id><pub-id pub-id-type="pmid">27272438</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boinski</surname><given-names>S</given-names></name><name><surname>Mitchell</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Chuck vocalizations of wild female squirrel monkeys (Saimiri sciureus) contain information on caller identity and foraging activity</article-title><source>International Journal of Primatology</source><volume>18</volume><fpage>975</fpage><lpage>993</lpage><pub-id pub-id-type="doi">10.1023/A:1026300314739</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Sarkar</surname><given-names>A</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Male mice song SYNTAX depends on social contexts and influences female preferences</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id><pub-id pub-id-type="pmid">25883559</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheadle</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Myers</surname><given-names>N</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Herce Castañón</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adaptive gain control during human perceptual choice</article-title><source>Neuron</source><volume>81</volume><fpage>1429</fpage><lpage>1441</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.020</pub-id><pub-id pub-id-type="pmid">24656259</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-Neuron perturbations reveal feature-specific competition in V1</article-title><source>Nature</source><volume>567</volume><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0997-6</pub-id><pub-id pub-id-type="pmid">30842660</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coye</surname><given-names>C</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name><name><surname>Lemasson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Morphologically structured vocalizations in female diana monkeys</article-title><source>Animal Behaviour</source><volume>115</volume><fpage>97</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2016.03.010</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gee</surname><given-names>JW</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Schwabe</surname><given-names>L</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>McCormick</surname><given-names>D</given-names></name><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil-linked phasic arousal predicts a reduction of choice bias across species and decision domains</article-title><source>eLife</source><volume>9</volume><elocation-id>e54014</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54014</pub-id><pub-id pub-id-type="pmid">32543372</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edeline</surname><given-names>JM</given-names></name><name><surname>Pham</surname><given-names>P</given-names></name><name><surname>Weinberger</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Rapid development of Learning-induced receptive field plasticity in the auditory cortex</article-title><source>Behavioral Neuroscience</source><volume>107</volume><fpage>539</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1037//0735-7044.107.4.539</pub-id><pub-id pub-id-type="pmid">8397859</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Differential dynamic plasticity of A1 receptive fields during multiple spectral tasks</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>7623</fpage><lpage>7635</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1318-05.2005</pub-id><pub-id pub-id-type="pmid">16107649</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname><given-names>M</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differential coding of conspecific vocalizations in the ventral auditory cortical stream</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4665</fpage><lpage>4676</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3969-13.2014</pub-id><pub-id pub-id-type="pmid">24672012</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname><given-names>M</given-names></name><name><surname>Doyle</surname><given-names>AM</given-names></name><name><surname>Mullarkey</surname><given-names>MP</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distributed acoustic cues for caller identity in macaque vocalization</article-title><source>Royal Society Open Science</source><volume>2</volume><elocation-id>150432</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.150432</pub-id><pub-id pub-id-type="pmid">27019727</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gamba</surname><given-names>M</given-names></name><name><surname>Colombo</surname><given-names>C</given-names></name><name><surname>Giacoma</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Acoustic cues to caller identity in lemurs: a case study</article-title><source>Journal of Ethology</source><volume>30</volume><fpage>191</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/s10164-011-0291-z</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaze</surname><given-names>CM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Normative evidence accumulation in unpredictable environments</article-title><source>eLife</source><volume>4</volume><elocation-id>08825</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08825</pub-id><pub-id pub-id-type="pmid">26322383</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname><given-names>DM</given-names></name><name><surname>Swets</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimsley</surname><given-names>JMS</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name><name><surname>Wallace</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Different representations of tooth chatter and purR call in guinea pig auditory cortex</article-title><source>Neuroreport</source><volume>22</volume><fpage>613</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e3283495ae9</pub-id><pub-id pub-id-type="pmid">21734609</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimsley</surname><given-names>JMS</given-names></name><name><surname>Shanbhag</surname><given-names>SJ</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name><name><surname>Wallace</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Processing of communication calls in guinea pig auditory cortex</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e51646</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0051646</pub-id><pub-id pub-id-type="pmid">23251604</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>R</given-names></name><name><surname>Heffner</surname><given-names>H</given-names></name><name><surname>Masterton</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Behavioral measurements of absolute and frequency-difference thresholds in guinea pig</article-title><source>The Journal of the Acoustical Society of America</source><volume>49</volume><fpage>1888</fpage><lpage>1895</lpage><pub-id pub-id-type="doi">10.1121/1.1912596</pub-id><pub-id pub-id-type="pmid">5125738</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holfoth</surname><given-names>DP</given-names></name><name><surname>Neilans</surname><given-names>EG</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Discrimination of partial from whole ultrasonic vocalizations using a go/no-go task in mice</article-title><source>The Journal of the Acoustical Society of America</source><volume>136</volume><fpage>3401</fpage><lpage>3409</lpage><pub-id pub-id-type="doi">10.1121/1.4900564</pub-id><pub-id pub-id-type="pmid">25480084</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janse</surname><given-names>E</given-names></name><name><surname>Nooteboom</surname><given-names>S</given-names></name><name><surname>Quené</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Word-level intelligibility of time-compressed speech: prosodic and segmental factors</article-title><source>Speech Communication</source><volume>41</volume><fpage>287</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00130-9</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keung</surname><given-names>W</given-names></name><name><surname>Hagen</surname><given-names>TA</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A divisive model of evidence accumulation explains uneven weighting of evidence over time</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2160</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15630-0</pub-id><pub-id pub-id-type="pmid">32358501</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>DP</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mechanisms of song perception in oscine birds</article-title><source>Brain and Language</source><volume>115</volume><fpage>59</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2009.09.008</pub-id><pub-id pub-id-type="pmid">20471673</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name><name><surname>Hindmarsh Sten</surname><given-names>T</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Elnozahy</surname><given-names>S</given-names></name><name><surname>Fogelson</surname><given-names>KA</given-names></name><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name><name><surname>Holland</surname><given-names>PC</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissociating task acquisition from expression during learning reveals latent knowledge</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-10089-0</pub-id><pub-id pub-id-type="pmid">31089133</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>ST</given-names></name><name><surname>Montes-Lourido</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Sadagopan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimal features for auditory categorization</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1302</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09115-y</pub-id><pub-id pub-id-type="pmid">30899018</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Zwitserlood</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Accessing spoken words: the importance of word onsets</article-title><source>Journal of Experimental Psychology</source><volume>15</volume><fpage>576</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.15.3.576</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><name><surname>Mandel</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The communicative content of the common marmoset phee call during antiphonal calling</article-title><source>American Journal of Primatology</source><volume>72</volume><fpage>974</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1002/ajp.20854</pub-id><pub-id pub-id-type="pmid">20549761</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montes-Lourido</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Sadagopan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Neuronal selectivity to complex vocalization features emerges in the superficial layers of primary auditory cortex</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001299</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001299</pub-id><pub-id pub-id-type="pmid">34133413</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montes-Lourido</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>M</given-names></name><name><surname>Kumbam</surname><given-names>I</given-names></name><name><surname>Sadagopan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Pupillometry as a reliable metric of auditory detection and discrimination across diverse stimulus paradigms in animal models</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>3108</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-82340-y</pub-id><pub-id pub-id-type="pmid">33542266</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>S</given-names></name><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Slow or sudden: re-interpreting the learning curve for modern systems neuroscience</article-title><source>IBRO Neuroscience Reports</source><volume>13</volume><fpage>9</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.ibneur.2022.05.006</pub-id><pub-id pub-id-type="pmid">35669385</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neilans</surname><given-names>EG</given-names></name><name><surname>Holfoth</surname><given-names>DP</given-names></name><name><surname>Radziwon</surname><given-names>KE</given-names></name><name><surname>Portfors</surname><given-names>CV</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Discrimination of ultrasonic vocalizations by CBA/caj mice (<italic>Mus musculus</italic>) is related to spectrotemporal dissimilarity of vocalizations</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e85405</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0085405</pub-id><pub-id pub-id-type="pmid">24416405</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojima</surname><given-names>H</given-names></name><name><surname>Taira</surname><given-names>M</given-names></name><name><surname>Kubota</surname><given-names>M</given-names></name><name><surname>Horikawa</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Recognition of non-harmonic natural sounds by small mammals using competitive training</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e51318</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0051318</pub-id><pub-id pub-id-type="pmid">23251497</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojima</surname><given-names>H</given-names></name><name><surname>Horikawa</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Recognition of modified conditioning sounds by competitively trained guinea pigs</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>373</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00373</pub-id><pub-id pub-id-type="pmid">26858617</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paraouty</surname><given-names>N</given-names></name><name><surname>Charbonneau</surname><given-names>JA</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Social learning exploits the available auditory or visual cues</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>14117</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-71005-x</pub-id><pub-id pub-id-type="pmid">32839492</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>Verzat</surname><given-names>C</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Courtship Behaviour Reveals Temporal Regularity Is a Critical Social Cue in Mouse Communication</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.28.922773</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phatak</surname><given-names>SA</given-names></name><name><surname>Allen</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Consonant and vowel confusions in speech-weighted noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>121</volume><fpage>2312</fpage><lpage>2326</lpage><pub-id pub-id-type="doi">10.1121/1.2642397</pub-id><pub-id pub-id-type="pmid">17471744</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisupati</surname><given-names>S</given-names></name><name><surname>Chartarifsky-Lynn</surname><given-names>L</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Lapses in perceptual decisions reflect exploration</article-title><source>eLife</source><volume>10</volume><elocation-id>e55490</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55490</pub-id><pub-id pub-id-type="pmid">33427198</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>BJ</given-names></name><name><surname>Harcourt</surname><given-names>RG</given-names></name><name><surname>Charrier</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Individual identity encoding and environmental constraints in vocal recognition of pups by Australian sea lion mothers</article-title><source>Animal Behaviour</source><volume>83</volume><fpage>681</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2011.12.012</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Comparison of object recognition behavior in human and monkey</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12127</fpage><lpage>12136</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0573-15.2015</pub-id><pub-id pub-id-type="pmid">26338324</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name><name><surname>Wagner</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Black</surname><given-names>AH</given-names></name><name><surname>Prokasy</surname><given-names>WF</given-names></name></person-group><source>Classical Conditioning II</source><publisher-name>Appleton-Century-Crofts</publisher-name><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russ</surname><given-names>BE</given-names></name><name><surname>Orr</surname><given-names>LE</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Prefrontal neurons predict choices during an auditory same-different task</article-title><source>Current Biology</source><volume>18</volume><fpage>1483</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.08.054</pub-id><pub-id pub-id-type="pmid">18818080</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salasoo</surname><given-names>A</given-names></name><name><surname>Pisoni</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Interaction of knowledge sources in spoken word identification</article-title><source>Journal of Memory and Language</source><volume>24</volume><fpage>210</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/0749-596X(85)90025-7</pub-id><pub-id pub-id-type="pmid">23226691</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Screven</surname><given-names>LA</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Discrimination of frequency modulated sweeps by mice</article-title><source>The Journal of the Acoustical Society of America</source><volume>140</volume><fpage>1481</fpage><lpage>1487</lpage><pub-id pub-id-type="doi">10.1121/1.4962223</pub-id><pub-id pub-id-type="pmid">27914389</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seyfarth</surname><given-names>RM</given-names></name><name><surname>Cheney</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Meaning and emotion in animal vocalizations</article-title><source>Annals of the New York Academy of Sciences</source><volume>1000</volume><fpage>32</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1196/annals.1280.004</pub-id><pub-id pub-id-type="pmid">14766619</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>RV</given-names></name><name><surname>Zeng</surname><given-names>FG</given-names></name><name><surname>Kamath</surname><given-names>V</given-names></name><name><surname>Wygonski</surname><given-names>J</given-names></name><name><surname>Ekelid</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>ZM</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title><source>Nature</source><volume>416</volume><fpage>87</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1038/416087a</pub-id><pub-id pub-id-type="pmid">11882898</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souffi</surname><given-names>S</given-names></name><name><surname>Lorenzi</surname><given-names>C</given-names></name><name><surname>Varnet</surname><given-names>L</given-names></name><name><surname>Huetz</surname><given-names>C</given-names></name><name><surname>Edeline</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Noise-sensitive but more precise subcortical representations coexist with robust cortical encoding of natural vocalizations</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5228</fpage><lpage>5246</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2731-19.2020</pub-id><pub-id pub-id-type="pmid">32444386</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ter-Mikaelian</surname><given-names>M</given-names></name><name><surname>Semple</surname><given-names>MN</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effects of spectral and temporal disruption on cortical encoding of gerbil vocalizations</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>1190</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1152/jn.00645.2012</pub-id><pub-id pub-id-type="pmid">23761696</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Toarmino</surname><given-names>C</given-names></name><name><surname>Neilans</surname><given-names>EG</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Identification of Conspecific Calls by Budgerigars</source><publisher-name>Melopsittacus Undulatus</publisher-name><pub-id pub-id-type="doi">10.1037/e525792013-005</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S</given-names></name><name><surname>Vidal-Naquet</surname><given-names>M</given-names></name><name><surname>Sali</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual features of intermediate complexity and their use in classification</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>682</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1038/nn870</pub-id><pub-id pub-id-type="pmid">12055634</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S</given-names></name><name><surname>Assif</surname><given-names>L</given-names></name><name><surname>Fetaya</surname><given-names>E</given-names></name><name><surname>Harari</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Atoms of recognition in human and computer vision</article-title><source>PNAS</source><volume>113</volume><fpage>2744</fpage><lpage>2749</lpage><pub-id pub-id-type="doi">10.1073/pnas.1513198113</pub-id><pub-id pub-id-type="pmid">26884200</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wichmann</surname><given-names>FA</given-names></name><name><surname>Hill</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The psychometric function: I. fitting, sampling, and goodness of fit</article-title><source>Perception &amp; Psychophysics</source><volume>63</volume><fpage>1293</fpage><lpage>1313</lpage><pub-id pub-id-type="doi">10.3758/bf03194544</pub-id><pub-id pub-id-type="pmid">11800458</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>Strait</surname><given-names>DL</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamics and hierarchical encoding of non-compact acoustic categories in auditory and frontal cortex</article-title><source>Current Biology</source><volume>30</volume><fpage>1649</fpage><lpage>1663</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.02.047</pub-id><pub-id pub-id-type="pmid">32220317</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zilany</surname><given-names>MSA</given-names></name><name><surname>Bruce</surname><given-names>IC</given-names></name><name><surname>Carney</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Updated parameters and expanded simulation options for a model of the auditory periphery</article-title><source>The Journal of the Acoustical Society of America</source><volume>135</volume><fpage>283</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1121/1.4837815</pub-id><pub-id pub-id-type="pmid">24437768</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78278.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.03.09.483596" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.03.09.483596"/></front-stub><body><p>This important study combines behavioral data from guinea pigs and data from a classifier model to make a compelling case for which auditory features are important for classifying vocalisations. This study is likely to be of interest to both computational and experimental neuroscientists, in particular auditory neurophysiologists and cognitive and comparative neuroscientists. A strength of this work is that a model trained on natural calls was able to predict some aspects of responses to temporally and spectrally altered cues.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78278.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Goodman</surname><given-names>Dan FM</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f7hpc57</institution-id><institution>University of Erlangen-Nuremberg</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.03.09.483596">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.03.09.483596v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Vocalization categorization behavior explained by a feature-based auditory categorization model&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Dan FM Goodman as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Andrew King as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Patrick Krauss (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1. The statistical analysis seems to be underpowered. Reviewer #2 makes a suggestion for another approach.</p><p>2. The data and models do not seem to be sufficient to support some of the stronger claims made in the manuscript. Please revise the language.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I think this is a really nice paper. I love the approach and personally find the conclusions interesting and compelling. I think maybe the case for the strength of the predictions is somewhat overstated and the manuscript would be improved by toning that down a bit and acknowledging rather than explaining away divergences between model and data.</p><p>In figure 4, the R<sup>2</sup> values are both 0.939. Can you confirm that this isn't an error? It seems surprising that they should have this identical value. Might be good to double-check that.</p><p>In figure 6, the model seems to perform best for a time stretch value not equal to 1. This seems surprising. Any thoughts on that?</p><p>In figure 7, what is being tested for the model? ISI drawn from same-category or other-category (chimeric)? Maybe show both separately? Generally, it's a bit hard to work out what's going on in this figure.</p><p>In figure 8C the decrease in performance of the model is referred to in the text as &quot;only a slight decrease in d'&quot; but this doesn't seem very consistent with what looks like quite a large decrease in the figure.</p><p>In the methods, for the tempo manipulation, could you explain more what the method is? I guess it is some sort of pitch-preserving transformation that is built into Audacity, but without knowing how that software works it's hard to know what's going on here.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I have concerns about the statistical analysis of the behavioral data and the main conclusions drawn from the study. A major weakness in the current manuscript is the statistical approach used for analyzing the behavior which is in all cases likely to be underpowered – given that the absence of an effect (e.g. pitch roving) is often taken as a key result, a more rigorous approach is needed (see below for detailed points). This could either take the form of Bayesian statistics which the likelihood of a significant effect or a null effect can be distinguished from an inconclusive effect (my guess is most tests run on 3 animals will be inconclusive) or better a mixed effect model in which the binary trial outcome is modelled according to e.g. the call type, pitch shift/temporal modulation with the individual GP as a fixed effect. This is likely to be a more sensitive analysis and the resulting β coefficients may be informative and may enable a more direct comparison to the model output (if the model is run on single trials, like the GP, then the exact same analysis could be performed on different model instantiations). It would also be possible to combine all animals into the same analysis with the task as a factor.</p><p>In terms of the interpretation; the classifier is essentially only looking at the spectrum (few of the statistics that e.g. Josh McDermott's group has identified as critical components of natural sounds are captured, such as frequency or amplitude modulation). This approach works for these highly reduced two-choice discriminations that can essentially be solved by looking at spectral properties, although the failures of the model to cope with temporal manipulations suggest its overfitting to the trained exemplars by looking at particular temporal windows. From the spectrograms it is clear that the temporal properties of the call are likely to be important – e.g. the purr is highly regular; a property that is maintained across different presentation rates, likely explaining why the GPs cope fine with short rates. Many of the findings here replicate Ojima and Horikawa (2016) – this should be stated explicitly within the manuscript. The behavioral data here are impressive but I'm not really sure what we should take away from the manuscript beyond that, except that for an unnaturally simple task an overly simple frequency-based classifier can solve the task in some instances and not others.</p><p>Detailed comments</p><p>Figure 3 – what is the performance of the trained exemplars here? From the other figures I guess a lot higher than for the generalization stimuli; it looks like performance does take a hit on novel exemplars (possibly for GPs and the model?). The authors should justify why they didn't present stimuli as rare (and randomly rewarded) probe stimuli to assess innate rather than learned generalization.</p><p>Figure 5. – segment length analysis – here the GP is much better at short segments. The model and the GP show a clear modulation in performance with segment length that is not picked up statistically due to an underpowered analysis. These data should either be analysed as a within animal logistic regression or an across animal mixed effect model. This could combine all 6 animals, with the task as a factor.</p><p>Figure 6 – again, repeated measures anova on 7 conditions in 3 animals (χ2); the fact that the p-value is &gt;0.05 means nothing! The real question – is the model quantitatively similar (could be addressed by assessing the likelihood of getting the same performance by generating a distribution of model performance on many model instantiations and then asking if the GP data falls within 95% of the observed values) or is it qualitatively similar? – for example show the same variation in performance across different call rates. In F the GP performance is robust across all durations, with a gradual increase in performance from the shortest call durations. In contrast the model peaks around the trained duration. In E both the model and the GP peak at just less than 1x call duration, but the model does really poorly at short durations. To me, this isn't 'qualitatively similar'.</p><p>Figure 7 – why use the term chimeric in this figure and nowhere else? Are they not just the random ISI as in the second half of the figure?</p><p>Figure 8 – I'm surprised at how poorly the model does on the reversed chuts. Again, the lack of a significant difference here cannot be used as evidence for equivalent performance.</p><p>Figure 12 – I think I possibly don't understand this figure – how can the MIFs well suited for a purr vs chut distinction be different from a chut vs purr distinction? Either this needs elaboration or I think It would make far more sense to display chuts and whines (currently supplemental figure 12). Moreover, rather than these rather unintuitive bubble plots, a ranked list of each feature's contribution to the model, partial dependence plots, and individual conditional expectation plots would answer the question of which model features most contribute towards a guinea pig call categorization.</p><p>The model classifies the result based on weighted factors such as spectrotemporal information but does not consider specific biological neuronal networks that underly this categorization process. Moreover, the authors assert that intermediate-complexity features were responsible for call categorization; this assertion is expected as all MIFs selected are based on the inputted spectrotemporal data. In other words, the features in the classifier model could arguably said to be mid-complex as all vocalizations could be said to have inherent complexity. Together, the conclusions that the authors draw that this model could represent a biological basis for call categorization seem really to reach beyond the data; the model fits some aspects of the training set well but does not causally prove that GPs learn specific frequencies to represent calls early in their lives.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This is a great study! I appreciate the approach to complementing behavioral experiments with computational modelling.</p><p>Considering the temporal stretching/compressing and the pitch change experiment: it remains unclear if the MIF kernels used by the models were just stretched/compressed to compensate for the changed auditory input. If so, the modelling results are kind of trivial. In this case, the model provides no mechanistic explanation of the underlying neural processes. This issue should at least be addressed in the corresponding Results section and might be an interesting cornerstone for a follow-up study.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78278.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The statistical analysis seems to be underpowered. Reviewer #2 makes a suggestion for another approach.</p></disp-quote><p>We have implemented the alternate statistical analyses suggested by reviewer 2 in this revision. These analyses involve using a generalized linear model to fit the behavioral data on a trial-by-trial basis. The alternate statistical analyses do not substantially alter the fundamental conclusions of the manuscript, i.e., the comparison of models to behavioral data is unaltered.</p><disp-quote content-type="editor-comment"><p>2. The data and models do not seem to be sufficient to support some of the stronger claims made in the manuscript. Please revise the language.</p></disp-quote><p>We have addressed this concern in the following ways. (1) At the beginning of the Results section, we clearly state which aspects of categorization behavior the model is intended to capture, and which aspects are left unmodeled. (2) We have eschewed subjective terms and instead state that the model explains ~50% of the behavioral variance (average of overall R<sup>2</sup> of 0.6 and 0.37 for the chuts vs. purrs and wheeks vs. whine tasks respectively). We leave it to the readers to interpret this effect size.</p><p>In addition, we have also addressed all reviewer comments. Please see below for a detailed point-by-point response.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I think this is a really nice paper. I love the approach and personally find the conclusions interesting and compelling. I think maybe the case for the strength of the predictions is somewhat overstated and the manuscript would be improved by toning that down a bit and acknowledging rather than explaining away divergences between model and data.</p></disp-quote><p>Thank you for the constructive feedback! In the revised manuscript we have tried to replace subjective descriptors with actual effect size numbers to avoid overstating results. We also include additional discussion of alternate interpretations.</p><disp-quote content-type="editor-comment"><p>In figure 4, the R<sup>2</sup> values are both 0.939. Can you confirm that this isn't an error? It seems surprising that they should have this identical value. Might be good to double-check that.</p></disp-quote><p>This is indeed the case for the particular random seed we used to simulate the trial-by-trial model outcome. For other random seeds the values are not equal but in the same ballpark, and therefore we decided to leave these numbers as is in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>In figure 6, the model seems to perform best for a time stretch value not equal to 1. This seems surprising. Any thoughts on that?</p></disp-quote><p>Yes, this is indeed an interesting observation, but while this is the case for the wheeks vs. whines task (Figure 6D), such a trend is not quite clear for the chuts vs. purrs task (Figure 6C). At present we do not have an explanation for why this is the case. Statistically the model did not show a significant effect of tempo change on performance. But interestingly, the Ojima and Horikawa (2016) study observed an asymmetry where GPs maintained task performance for time-compressed stimuli (but not stretched stimuli).</p><disp-quote content-type="editor-comment"><p>In figure 7, what is being tested for the model? ISI drawn from same-category or other-category (chimeric)? Maybe show both separately? Generally, it's a bit hard to work out what's going on in this figure.</p></disp-quote><p>We did not test the model for the chimeric ISI case because we could not assign target and non-target labels to stimuli. This was not clearly mentioned in the original manuscript. Thank you for catching this; we have now added clarification (page 17, line 403).</p><disp-quote content-type="editor-comment"><p>In figure 8C the decrease in performance of the model is referred to in the text as &quot;only a slight decrease in d'&quot; but this doesn't seem very consistent with what looks like quite a large decrease in the figure.</p></disp-quote><p>In figure 8C, the d′ value decreases from 2.76 for regular calls to 2.26 for reversed calls, an 18% decrease. We have replaced “only a slight decrease” with “an ~18% decrease” in the revised manuscript (page 18, line 426).</p><disp-quote content-type="editor-comment"><p>In the methods, for the tempo manipulation, could you explain more what the method is? I guess it is some sort of pitch-preserving transformation that is built into Audacity, but without knowing how that software works it's hard to know what's going on here.</p></disp-quote><p>We used the “high quality” option in the “Change Tempo” feature of audacity. The algorithm used is Subband Sinusoidal Modeling Synthesis (SBSMS), described here: http://sbsms.sourceforge.net/. The fundamental principle is detecting the spectral peaks in the STFT of a small sound segment and resynthesizing the segment with a phase-preserving oscillator bank at the required time scale. This information is included in the Methods section (page 34, line 877).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I have concerns about the statistical analysis of the behavioral data and the main conclusions drawn from the study. A major weakness in the current manuscript is the statistical approach used for analyzing the behavior which is in all cases likely to be underpowered – given that the absence of an effect (e.g. pitch roving) is often taken as a key result, a more rigorous approach is needed (see below for detailed points). This could either take the form of Bayesian statistics which the likelihood of a significant effect or a null effect can be distinguished from an inconclusive effect (my guess is most tests run on 3 animals will be inconclusive) or better a mixed effect model in which the binary trial outcome is modelled according to e.g. the call type, pitch shift/temporal modulation with the individual GP as a fixed effect. This is likely to be a more sensitive analysis and the resulting β coefficients may be informative and may enable a more direct comparison to the model output (if the model is run on single trials, like the GP, then the exact same analysis could be performed on different model instantiations). It would also be possible to combine all animals into the same analysis with the task as a factor.</p></disp-quote><p>We thank the reviewer for this detailed suggestion. As we mention in the response to the reviewer’s public review, we now use a GLM approach to predict trial-wise behavioral and model responses when there are multiple test conditions. Details are in page 36, line 942.</p><disp-quote content-type="editor-comment"><p>In terms of the interpretation; the classifier is essentially only looking at the spectrum (few of the statistics that e.g. Josh McDermott's group has identified as critical components of natural sounds are captured, such as frequency or amplitude modulation). This approach works for these highly reduced two-choice discriminations that can essentially be solved by looking at spectral properties, although the failures of the model to cope with temporal manipulations suggest its overfitting to the trained exemplars by looking at particular temporal windows. From the spectrograms it is clear that the temporal properties of the call are likely to be important – e.g. the purr is highly regular; a property that is maintained across different presentation rates, likely explaining why the GPs cope fine with short rates.</p></disp-quote><p>The classifier is not looking at the average spectrum alone – the reason that the MIFs have a duration of ~110 ms on average is that they are looking for specific spectrotemporal features. In our previous work (Liu et al., 2019), we have shown that constraining the features to small durations (only allowing the model access to instantaneous spectrum information) degrades model performance.</p><p>To quantify this further, we developed a simple support vector machine based classifier that attempts to classify vocalization categories based purely on the long-term spectra of calls. To do so, the long-term power spectrum (estimated as the variance of mean rate at each center frequency) was used as input to the model. Similar to feature-based models, SVMs were trained to classify a single call type from all other call types using the Matlab function <italic>fitcsvm</italic> (linear kernel, standardized [i.e., rescaled] predictors). The 10-fold cross-validation losses for chut, purr, wheek, and whine SVMs were 14%, 4%, 10.6%, and 7.1%, which indicate small bias/variance for the SVM models. Similar to the feature-based model, a winner-take-all stage was implemented by comparing the outputs of the target-call SVM model and the distractor-call SVM model for a single input call (response = GO if target-SVM output &gt; distractor-SVM output). We used this model to classify the manipulated stimuli, and found that the spectrum model explained a much lower fraction of the variance in GP behavior.</p><p>We have now included this comparison in the revised manuscript (page 21, line 503; Methods in page 39, line 1035) and presented the results of this analysis as Figure 11 —figure supplement 1.</p><disp-quote content-type="editor-comment"><p>Many of the findings here replicate Ojima and Horikawa (2016) – this should be stated explicitly within the manuscript. The behavioral data here are impressive but I'm not really sure what we should take away from the manuscript beyond that, except that for an unnaturally simple task an overly simple frequency-based classifier can solve the task in some instances and not others.</p></disp-quote><p>We respectfully disagree that our findings replicate those of Ojima and Horikawa (2016). That study is indeed impressive in many respects, but the reason that we did not originally cite it is because we chose to focus on vocalizations, and more importantly, on categorization. In this respect, we felt that our stimulus manipulations were closer to some mouse studies (that we cited, studies from the Dent lab). We have now cited Ojima and Horikawa (2016) and their earlier 2012 study in the revised manuscript (page 3, line 77; page 4, line 87; page 27, line 677).</p><p>Specific differences between our study and that of Ojima and Horikawa (2016) include: they used a single footstep sound was used as a target, and single exemplars of other natural sounds (clapping hands etc) were used non-targets. They used a competitive training paradigm. For the tempo shifts in particular, they observed a strong asymmetry between compression and expansion of sounds. Finally, they did not attempt to develop a model to explain GP behavior, which is the central point of our manuscript.</p><p>A simple frequency-based classifier can solve the basic categorization task, but does not match GP behavior, as described in the new analysis above.</p><disp-quote content-type="editor-comment"><p>Detailed comments</p><p>Figure 3 – what is the performance of the trained exemplars here? From the other figures I guess a lot higher than for the generalization stimuli; it looks like performance does take a hit on novel exemplars (possibly for GPs and the model?). The authors should justify why they didn't present stimuli as rare (and randomly rewarded) probe stimuli to assess innate rather than learned generalization.</p></disp-quote><p>On the last day of the training phase, the GPs achieved an average d′ of 1.94 and 1.9 for the chuts vs. purrs and wheeks vs. whines tasks respectively (now stated in page 6, line 148). This is somewhat greater than the d′ values for generalization (1.72 and 1.51 respectively). The d′ values do continue to improve slowly over the course of the other testing (for the unmanipulated stimuli in each set).</p><p>We initially presented novel stimuli as a separate block in order to minimize the number of trials required. We required about 200 trials to assess performance, and presenting these trials 20% of the time would have resulted in us having to present 1000 trials (5 days). But after receiving feedback at a conference, we indeed also tested generalization in the wheeks vs. whines task by presenting novel stimuli rarely (this was described in page 9, line 228 of the original manuscript, sentence beginning “<italic>As an additional control…</italic>”). We verified that GPs also generalized with this mode of stimulus presentation.</p><disp-quote content-type="editor-comment"><p>Figure 5. – segment length analysis – here the GP is much better at short segments. The model and the GP show a clear modulation in performance with segment length that is not picked up statistically due to an underpowered analysis. These data should either be analysed as a within animal logistic regression or an across animal mixed effect model. This could combine all 6 animals, with the task as a factor.</p></disp-quote><p>We thank the reviewer for this suggestion – we agree that the statistical analysis was likely under-powered. In the revised manuscript, we now use a generalized linear model with a logit link function, with animal ID as a random effect, to model both model and GP responses on a trial-by-trial basis. To evaluate the significance of the segment-length (and other manipulations), we compared this ‘full’ model to a ‘null’ model that only had stimulus type as a predictor and animal ID as a random effect. The new analysis does reveal, as expected by the reviewer, a statistically significant effect of segment length on both GP and model performance. These analyses are now described in page 13, line 315 and page 14, line 326 of the revised manuscript. Detailed methods are described in page 36, line 942.</p><disp-quote content-type="editor-comment"><p>Figure 6 – again, repeated measures anova on 7 conditions in 3 animals (χ2); the fact that the p-value is &gt;0.05 means nothing! The real question – is the model quantitatively similar (could be addressed by assessing the likelihood of getting the same performance by generating a distribution of model performance on many model instantiations and then asking if the GP data falls within 95% of the observed values) or is it qualitatively similar? – for example show the same variation in performance across different call rates. In F the GP performance is robust across all durations, with a gradual increase in performance from the shortest call durations. In contrast the model peaks around the trained duration. In E both the model and the GP peak at just less than 1x call duration, but the model does really poorly at short durations. To me, this isn't 'qualitatively similar'.</p></disp-quote><p>Similar to the above model, we implemented a GLM to model both behavioral and model results on a trial-by-trial basis (page 15, line 352). This analysis revealed a weak effect of tempo shift on the behavioral performance of GPs for both the chuts vs. purrs and wheeks vs. whines tasks. Trial-by-trial fits to the model revealed a weak effect of tempo shift for chuts vs. purrs but no effect for wheeks vs. whines (page 16, line 370). In the revision, we state that there is a broad correspondence between model and data (page 16, line 379).</p><p>We performed similar GLM analyses for the SNR data (page 11, line 261) and F0 shift data (page 18, line 448) as well.</p><disp-quote content-type="editor-comment"><p>Figure 7 – why use the term chimeric in this figure and nowhere else? Are they not just the random ISI as in the second half of the figure?</p></disp-quote><p>Random ISI refers to replacing the ISIs of a call, say a purr, with ISIs drawn from the overall distribution of purr ISIs, i.e., the same call type. Chimeric calls are when ISIs of a purr are replaced with ISIs randomly drawn from the chut ISI distribution. The difference between random ISI and chimeric ISI is described in Figure 7’s legend as well as in the main text (page 16, 387 and page 16, line 395 respectively).</p><disp-quote content-type="editor-comment"><p>Figure 8 – I'm surprised at how poorly the model does on the reversed chuts. Again, the lack of a significant difference here cannot be used as evidence for equivalent performance.</p></disp-quote><p>We removed the phrase “Similar to GP behavior…” from our description of this result, and now simply state that “The model also maintained robust performance (<italic>d′</italic> &gt; 1) for call reversal conditions but with an ~18% decrease in <italic>d′</italic> compared to behavior.” (page 18, line 426).</p><disp-quote content-type="editor-comment"><p>Figure 12 – I think I possibly don't understand this figure – how can the MIFs well suited for a purr vs chut distinction be different from a chut vs purr distinction? Either this needs elaboration or I think It would make far more sense to display chuts and whines (currently supplemental figure 12). Moreover, rather than these rather unintuitive bubble plots, a ranked list of each feature's contribution to the model, partial dependence plots, and individual conditional expectation plots would answer the question of which model features most contribute towards a guinea pig call categorization.</p></disp-quote><p>Each call type has associated with it ~20 MIFs that are best suited for distinguishing that call type from all other call types. When a stimulus is presented, we used template matching to detect the MIFs of all call types in the stimulus and weight the detected features by their log-likelihood ratios to provide the total evidence for the presence of a call type (Figure 2). Thus, when stimuli are presented (and their category known), we can determine the number of times a given MIF will be detected in within-category stimuli and ouside-category stimuli. The relative detection rate can be calculated from these values.</p><p>The reviewer is correct in pointing out that in the original manuscript, the main text describing Figure 12 used the Go/No-go terminology confusingly. We intended to mean “within-category” and “outside-category” stimuli. We have know rewritten the main text in the revised manuscript, and hope that this revised description is clearer (page 25, lines 589, 593).</p><p>There are 100 data points per panel in this figure, and a ranked list is also not easy to interpret. We realized that we had not uploaded the raw data underlying this figure as a supplement to the original manuscript. We have now uploaded an.xls file (Figure 12 – source data 1) containing a list of all MIF properties used in Figure 12, which can be sorted along multiple parameters as necessary. For visualization purposes, we have chosen to retain bubble plots from the original manuscript.</p><disp-quote content-type="editor-comment"><p>The model classifies the result based on weighted factors such as spectrotemporal information but does not consider specific biological neuronal networks that underly this categorization process. Moreover, the authors assert that intermediate-complexity features were responsible for call categorization; this assertion is expected as all MIFs selected are based on the inputted spectrotemporal data. In other words, the features in the classifier model could arguably said to be mid-complex as all vocalizations could be said to have inherent complexity. Together, the conclusions that the authors draw that this model could represent a biological basis for call categorization seem really to reach beyond the data; the model fits some aspects of the training set well but does not causally prove that GPs learn specific frequencies to represent calls early in their lives.</p></disp-quote><p>The reviewer is correct that we do not extensively comment on the biological basis of detecting MIFs or the complexity of the features. The reason for this is that this manuscript is the third in a series of studies that our lab has published on using such informative features for categorization. In the first study (Liu ST et al., Nat. Comm., 2019), we showed that restrictions to feature duration or bandwidth typically degraded model performance (Figure 7). We also showed that highly informative features tended to have intermediate complexity (Supplementary Figure 2). The second study more directly addressed the biological bases (Montes-Lourido et al., PLoS Biology, 2021), and showed that neurons in superficial cortical layers displayed receptive fields and call selectivity consistent with their being selective for some call features. Both these publications contain extensive discussions on the possible biological implementation of feature selectivity, and we did not want to repeat those points in the present manuscript. Instead, we briefly stated these results in the introduction section. In the revised manuscript, we have added a sentence describing the complexity results from the earlier study as well (page 2, line 49).</p><p>We agree with the reviewer that the model does not establish causality, nor do we claim in the manuscript that we are providing a causal explanation. As we state at the end of the Results section of the original (and revised) manuscript:</p><p>“Note that our contention is not that the precise MIFs obtained in our model are also the precise MIFs used by the GPs – indeed, we were able to train several distinct MIF sets that were equally proficient at categorizing calls. Rather, we are proposing a framework in which GPs learn intermediate-complexity features that account for within-category variability and best contrast a call category from all other categories, and similar to the model, recruit different subsets of these features to solve different categorization tasks.” (page 25, line 608)</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This is a great study! I appreciate the approach to complementing behavioral experiments with computational modelling.</p></disp-quote><p>Thank you for these kind words!</p><disp-quote content-type="editor-comment"><p>Considering the temporal stretching/compressing and the pitch change experiment: it remains unclear if the MIF kernels used by the models were just stretched/compressed to compensate for the changed auditory input. If so, the modelling results are kind of trivial. In this case, the model provides no mechanistic explanation of the underlying neural processes. This issue should at least be addressed in the corresponding Results section and might be an interesting cornerstone for a follow-up study.</p></disp-quote><p>As stated above, we did not alter the MIFs in any way for the tests – the MIFs were purely derived by training the animal on natural calls. In learning to generalize over the variability in natural calls, the model also achieved the ability to generalize over some manipulated stimuli. The fact that the model tracks GP behavior is a key observation supporting our argument that GPs also learn MIF-like features to accomplish call categorization.</p><p>We had mentioned at a few places that the model was only trained on natural calls. To add clarity, we have now included sentences in the time-compression and frequency-shifting results affirming that we did not manipulate the MIFs to match test stimuli. We also include a couple of sentences in the Discussion section’s first paragraph stating the above argument (page 26, line 627).</p></body></sub-article></article>