<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">104833</article-id><article-id pub-id-type="doi">10.7554/eLife.104833</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.104833.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Novel and optimized mouse behavior enabled by fully autonomous HABITS: Home-cage assisted behavioral innovation and testing system</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Bowen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1119-0458</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Penghai</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Haoze</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yueming</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7742-0722</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Kedi</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Hao</surname><given-names>Yaoyao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0005-2628-8297</contrib-id><email>yaoyaoh@zju.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>The State Key Lab of Brain-Machine Intelligence, Zhejiang University</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Nanhu Brain-computer Interface Institute</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Department of Biomedical Engineering, Zhejiang University</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>College of Computer Science and Technology, Zhejiang University</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Flagel</surname><given-names>Shelly B</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jmfr291</institution-id><institution>University of Michigan</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Taffe</surname><given-names>Michael A</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>09</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP104833</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-11-14"><day>14</day><month>11</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-11-14"><day>14</day><month>11</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.29.615652"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-02-14"><day>14</day><month>02</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104833.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-08-06"><day>06</day><month>08</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.104833.2"/></event></pub-history><permissions><copyright-statement>© 2025, Yu et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-104833-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-104833-figures-v1.pdf"/><abstract><p>Mice are among the most prevalent animal models used in neuroscience, benefiting from the extensive physiological, imaging, and genetic tools available to study their brain. However, the development of novel and optimized behavioral paradigms for mice has been laborious and inconsistent, impeding the investigation of complex cognitions. Here, we present a home-cage assisted mouse behavioral innovation and testing system (HABITS), enabling free-moving mice to learn challenging cognitive behaviors in their home-cage without any human involvement. Supported by the general programming framework, we have not only replicated established paradigms in current neuroscience research but also developed novel paradigms previously unexplored in mice, resulting in more than 300 mice demonstrated in various cognition functions. Most significantly, HABITS incorporates a machine-teaching algorithm, which comprehensively optimized the presentation of stimuli and modalities for trials, leading to more efficient training and higher-quality behavioral outcomes. To our knowledge, this is the first instance where mouse behavior has been systematically optimized by an algorithmic approach. Altogether, our results open a new avenue for mouse behavioral innovation and optimization, which directly facilitates investigation of neural circuits for novel cognitions with mice.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Mice are widely used in neuroscience research due to the many tools available to study their brain function and behavior. However, training mice for complex tasks requires extensive human involvement, which can stress the animals and introduce inconsistencies in methods and results.</p><p>Automated systems can reduce any potential bias, but most focus on single tasks only and lack optimization. To address these issues, Yu et al. developed the Home-cage Assisted Behavioral Innovation and Testing System (HABITS) – a fully autonomous platform where mice learn tasks in their cages without human intervention.</p><p>Using HABITS, mice successfully acquired a wide range of cognitive skills – including decision-making, working memory, and attention – entirely without handling. The system uses machine learning to adjust training sequences, improving learning speed and minimizing bias. In tests with over 300 mice across more than 20 paradigms, including some never attempted in mice, HABITS also improves the overall health of mice compared to the conventionally used water-restriction training. The system’s AI-driven adjustments help mice learn challenging tasks more efficiently and with fewer errors.</p><p>Its low cost and automation make it an efficient and reliable tool to study behavior, making it suitable for large-scale studies. Future developments may incorporate wireless neural recordings to directly link behavior with brain activity, providing deeper insight into learning and decision-making mechanisms.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>mouse cognition</kwd><kwd>autonomous training</kwd><kwd>behavioral innovation</kwd><kwd>machine teaching</kwd><kwd>behavioral optimization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>STI 2030—Major Projects</institution></institution-wrap></funding-source><award-id>2021ZD0200405</award-id><principal-award-recipient><name><surname>Xu</surname><given-names>Kedi</given-names></name><name><surname>Hao</surname><given-names>Yaoyao</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>62336007</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Yueming</given-names></name><name><surname>Hao</surname><given-names>Yaoyao</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study</institution></institution-wrap></funding-source><award-id>SN-ZJU-SIAS-002</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Yueming</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><award-id>2023ZFJH01-01</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Yueming</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><award-id>2024ZFJH01-01</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Yueming</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Pioneer R&amp;D Program of Zhejiang</institution></institution-wrap></funding-source><award-id>2024C03001</award-id><principal-award-recipient><name><surname>Hao</surname><given-names>Yaoyao</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Fully autonomous home-cage training system HABITS enables mice to learn complex cognitive behaviors without human intervention, expanding behavioral paradigms and optimizing training efficiency through machine teaching algorithms.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Complex goal-directed behaviors are the macroscopic manifestation of high-dimensional neural activity, making animal training in well-defined tasks a cornerstone of neuroscience (<xref ref-type="bibr" rid="bib15">Carandini, 2012</xref>; <xref ref-type="bibr" rid="bib42">Krakauer et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Niv, 2021</xref>). Over the past decades, researchers have developed a diverse array of cognitive tasks and achieved significant insights into the underlying cognitive computations (<xref ref-type="bibr" rid="bib55">Nau et al., 2024</xref>). Mouse has been increasingly utilized as model animal for investigating neural mechanisms of decision making, due to the abundant tools available in monitoring and manipulating individual neurons in intact brain (<xref ref-type="bibr" rid="bib79">Vázquez-Guardado et al., 2020</xref>). One notable example is the field of motor planning <xref ref-type="bibr" rid="bib76">Tanji and Evarts, 1976</xref>, which, after introduced into mouse models, was significantly advanced by obtaining causal results from whole brain circuits to genetics (<xref ref-type="bibr" rid="bib75">Svoboda and Li, 2018</xref>).</p><p>Traditionally, training mice in cognitive tasks was inseparable from human involvement in frequent handling and modifying shaping strategies according to the performance, thus labor-intensive and inconsistent as well as introducing unnecessary noise and stress (<xref ref-type="bibr" rid="bib7">Balcombe et al., 2004</xref>). Recently, many works dedicated to design standard training setups and workflows, aiming for more stable and reproducible outcomes (<xref ref-type="bibr" rid="bib20">Findling et al., 2023</xref>; <xref ref-type="bibr" rid="bib27">Han et al., 2018</xref>; <xref ref-type="bibr" rid="bib8">Benson et al., 2023</xref>; <xref ref-type="bibr" rid="bib46">Mah et al., 2023</xref>; <xref ref-type="bibr" rid="bib50">Mohammadi et al., 2024</xref>; <xref ref-type="bibr" rid="bib59">Pan-Vazquez et al., 2024</xref>; <xref ref-type="bibr" rid="bib65">Rich et al., 2024</xref>; <xref ref-type="bibr" rid="bib71">Scott et al., 2013</xref>; <xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib85">Zhou et al., 2024</xref>). For example, the International Brain Laboratory has recently shown mice can perform the task comparably across labs after a standard training within identical experimental setup (<xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>). However, human intervention still has been a significant factor, which inevitably introduced the variability. Furthermore, training efficiency was still restricted by the limited experimental time as before, which necessitates motivational techniques like water restriction. The training sessions were often restricted to a specific task and have not been extensively tested in other complex paradigms because of the requirement of prolonged training duration. These limitations highlighted the challenges to broadly and swiftly study comprehensive cognitive behaviors in mice.</p><p>A promising solution to this scenario is to implement fully autonomous training systems. In recent years, researchers have focused on combining home-cage environments with automated behavioral training methodologies, offering a viable avenue to realize autonomous training (<xref ref-type="bibr" rid="bib3">Aoki et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Bernhard et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Bollu et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Caglayan et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Francis and Kanold, 2017</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>; <xref ref-type="bibr" rid="bib41">Kiryk et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib53">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Poddar et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Qiao, 2019</xref>; <xref ref-type="bibr" rid="bib68">Salameh et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Silasi et al., 2018</xref>). For instance, efforts have been made to incorporate voluntary head fixation within the home-cage to train mice on cognitive tasks (<xref ref-type="bibr" rid="bib3">Aoki et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib53">Murphy et al., 2016</xref>). At the cost of increased training difficulty, they successfully integrate large-scale whole-brain calcium imaging (<xref ref-type="bibr" rid="bib3">Aoki et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib53">Murphy et al., 2016</xref>) and optogenetic modulations <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref> with fully automated home-cage behavior training. Moreover, other groups have conducted mouse behavior training in group-housed home-cages and utilize RFID technology to distinguish individual mice (<xref ref-type="bibr" rid="bib41">Kiryk et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Qiao, 2019</xref><xref ref-type="bibr" rid="bib63">Qiao, 2019</xref>; <xref ref-type="bibr" rid="bib73">Silasi et al., 2018</xref>). There are also studies which directly train freely moving animals in their home-cage to reduce stress and facilitate deployment (<xref ref-type="bibr" rid="bib7">Balcombe et al., 2004</xref>; <xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Kiryk et al., 2020</xref>; <xref ref-type="bibr" rid="bib62">Poddar et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Qiao, 2019</xref><xref ref-type="bibr" rid="bib63">Qiao, 2019</xref>; <xref ref-type="bibr" rid="bib73">Silasi et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Jankowski et al., 2023</xref>; <xref ref-type="bibr" rid="bib69">Schaefer and Claridge-Chang, 2012</xref>). However, many of these studies have focused on single paradigms and incorporated complex components in their systems, which hindered high-throughput deployment for high-efficiency and long-term behavioral testing and exploring.</p><p>The training protocols employed in existing studies, no matter in manual or autonomous training, were often artificially designed (<xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref><xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>), potentially failing to achieve optimal outcomes. For instance, a key issue in cognitive behavioral training is that the mouse is likely to develop bias, that is obtaining reward only from one side. Various ‘anti-bias’ techniques <xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Do et al., 2023</xref> have been implemented to counteract the bias, yet their efficacy in accelerating training or enhancing testing reliability remains unproven. From a machine learning standpoint, if we can accurately infer the animal’s internal models, it is possible to select a specific sequence of stimuli that will reduce the ‘teaching’ dimension of the animal and thus maximize the learning rate (<xref ref-type="bibr" rid="bib86">Zhu et al., 2018</xref>). Recently, an adaptive optimal training policy, known as AlignMax, was developed to generate an optimal sequence of stimuli to expedite the animal’s training process in simulation experiments (<xref ref-type="bibr" rid="bib6">Bak et al., 2016</xref>). While many relative works have realized the theoretical demonstration of the validity of machine teaching algorithms under specific conditions, these have been limited to teaching silicon-based learners in simulated environments (<xref ref-type="bibr" rid="bib6">Bak et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Jha et al., 2024</xref>; <xref ref-type="bibr" rid="bib45">Liu et al., 2017</xref>). The direct application and empirical demonstration of these algorithms in real-world scenarios, particularly in teaching animals to master complex cognitive tasks, remains unexplored. There are two fundamental barriers for testing these algorithms in real animals training: firstly, traditional session-based behavioral training results in a discontinuous training process, introducing abrupt variation of learning rate and uncontrollable noise (<xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib67">Roy et al., 2021</xref>), which could undermine the algorithm’s capability; secondly, the high computation complexity of model fitting (<xref ref-type="bibr" rid="bib6">Bak et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Jha et al., 2024</xref>) pose challenges for deployment on microprocessors, thereby impeding extensive and high-throughput experiments. In the lack of human supervision, a fully autonomous behavioral training system necessitates an optimized training algorithm. Therefore, integrating fully automated training with machine teaching-based algorithms could yield mutually beneficial outcomes.</p><p>To address these challenges, we introduced a home-cage assisted behavioral innovation and testing system, referred to as HABITS, which is a comprehensive platform featuring adaptable hardware and a universal programming framework. This system facilitates the creation of a wide array of mouse behavioral paradigms, regardless of complexity. With HABITS, we have not only replicated established paradigms commonly used in contemporary neuroscience research but have also pioneered novel paradigms that have never been explored in mice. Crucially, we have integrated a machine teaching-based optimization algorithm into HABITS, which significantly enhances the efficiency of both training and testing. Consequently, this study provides a holistic system and workflow for a variety of complex, long-term mouse behavioral experiments, which has the potential to greatly expand the behavioral reservoir for neuroscience and pathology research.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>System design of HABITS</title><p>The entire architecture of HABITS was comprised of two parts: a custom home-cage and behavioral components embedded in the home-cage (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The home-cage was made of acrylic plates with a dimension of 20×20 × 30 cm, which is more extensive than most of the commercial counterparts for single-housed mice. A tray was located at the bottom of the cage where ad libitum food, bedding, nesting material (cotton), and enrichment (a tube) were placed (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Experimenters can change the bedding effortlessly just by exchanging the tray. The home-cage also included an elevated, arc-shaped weighting platform inside, providing a loose body constraint for the mouse during task performing (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Notably, a micro load cell was installed beneath the platform, which can read out body weight of the mouse for long-term health monitoring. The cage was compatible with the standard mouse rack and occupied as small a space as standard mouse cage (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>System setup for HABITS.</title><p>(<bold>A</bold>), Front (left) and side (right) view of HABITS, showing components for stimulus presenting (LEDs &amp; buzzers), rewarding (water tanks and pumps), behavioral reporting (lickports) and health monitoring (weight platform). These components are coordinated by the controller unit and integrated into the mouse home-cage with a tray for bedding change. (<bold>B</bold>), HABITS installed on standard mouse cage rack. (<bold>C</bold>), Mouse, living in home-cage with food, bedding, nesting material (cotton), and enrichment (tube), is performing task on the weight platform. (<bold>D</bold>), System architecture for high-throughput behavioral training, showing different tasks are running in parallel groups of HABITS, which further wirelessly connect to one single PC through Wi-Fi to stream real-time data to the graphic user interface (GUI).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>HABITS system.</title><p>(<bold>A</bold>) Block diagram of control system of HABITS, showing peripherals connected with microcontroller through digital input/output (DIO) or serial port. (<bold>B</bold>) Graphic user interface (GUI) of a specific cage (left, magnified) and data plot window (right) when clicking the ‘plot’ button in the GUI, showing daily performance in all previous days, trial performance (green for correct and red for error trials) in last 24 hours, and body weight data in last 24 hours. (<bold>C</bold>) Example protocol programs for HABITS. (<bold>D</bold>) Around 100 HABITS are packed on standard racks for large-scale mouse behavioral testing. (<bold>E</bold>) Workflow pipeline for HABITS, showing fully autonomous mouse behavioral training after initialization of HABITS, before data harvest from SD card for analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig1-figsupp1-v1.tif"/></fig></fig-group><p>To perform behavioral training and testing in HABITS, we constructed a group of training-related components embedded in the front panel of the home-cage (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Firstly, three stimulus modules (LEDs and buzzers) for light and sound presenting were protruded from the front panel and placed in the left, right, and top positions around the weighting platform, enabling generation of visual and auditory stimulus modalities in three different spatial locations. The mouse reported the decisions about the stimuli by licking either left, right, or middle lickports installed in the front of the weighting platform. Finally, peristaltic pumps draw water from water tanks into lickports, serving as the reward for the task, which was the sole water source for the mouse throughout the period living in the home-cage. In the most common scenario, mice living in home-cage stepped on the weighting platform and triggered trials by licking on the lickports to obtain water (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="video" rid="video1">Video 1</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-104833-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Free-moving mouse performing task in HABITS.</title></caption></media><p>To endow autonomy to HABITS, a microcontroller was used to interface with all training-related components and coordinate the training procedure (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). We implemented a microcontroller-based general programming framework to run finite state machine with millisecond-level precision (see Materials and methods). By using the APIs provided by the framework, we can easily construct arbitrarily complex behavioral paradigms and deploy them into HABITS (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Meanwhile, the paradigms were usually divided into small steps from easy to hard and advanced according to the performance of the mouse. Another important role of the microcontroller was to connect the HABITS to PC wirelessly and stream real-time behavioral data via a Wi-Fi module. A graphic user interface (GUI), designed to remotely monitor each individual mouse’s performance, displayed history performance, real-time trial outcomes, body weights, etc. (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). Meanwhile, the program running in the microcontroller could be updated remotely by the GUI when changing mouse and/or task. As a backup, information of training setup, task parameters, and all behavioral data were also stored in an SD card for offline analysis.</p><p>To increase the throughput of behavioral testing, we built more than a hundred independent HABITS and installed them on standard mouse racks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>). The total material cost for each HABITS was less than $100 (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). All HABITS, operating different behavioral tasks across different cohorts of mice, were organized into groups according to the tasks and wirelessly connected to a single PC (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The states of each individual HABITS can be accessed and controlled remotely by monitoring the corresponding panels in the GUI, thereby significantly improving the experimental efficiency. We developed a workflow to run behavioral training and testing experiments in HABITS (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>). Firstly, initiate the HABITS system by preparing the home-cage, deploying training protocols, and weighing the naive mouse that did not need to undergo any water restriction. Then, the mouse interacted with fully autonomous HABITS at their own pace without any human intervention. In this study, mice housed in HABITS went through 24/7 behavioral training and testing for up to 3 months (<xref ref-type="video" rid="video2">Video 2</xref>), although longer duration was supported. Finally, data were harvested from the SD card and offline analyzed.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-104833-video2.mp4" id="video2"><label>Video 2.</label><caption><title>The 24 hr activities of mice living in HABITS.</title></caption></media><p>Therefore, HABITS permitted high-throughput, parallel behavioral training and testing in a fully autonomous way, which, contrasting with manual training, allows for possible behavioral innovation and underlining neural mechanism investigation.</p></sec><sec id="s2-2"><title>HABITS performance probed by multimodal d2AFC task</title><p>We next deployed a well-established mouse behavioral paradigm, delayed two-alternative forced choice (d2AFC), which was used to study motor planning in mouse <xref ref-type="bibr" rid="bib26">Guo et al., 2014b</xref>; <xref ref-type="bibr" rid="bib35">Inagaki et al., 2018</xref>, in HABITS to demonstrate the performance of our system (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>HABITS performance in d2AFC task.</title><p>(<bold>A</bold>) Task structure for d2AFC based on sound frequency. (<bold>B</bold>) Example licks for correct (blue), error (red), and early lick (gray) trials. Choice is the first lick after response onset. (<bold>C</bold>) Correct rate (black line) and early lick rate (gray line) of an example mouse during training in HABITS for the first 13 days. Shaded blocks indicate trials occurred in the dark cycle. Trials with early lick inhabitation only occur after the blue vertical line. Red vertical dash lines represent delay duration advancement from 0.2 s to 1.2 s. (<bold>D</bold>) Averaged correct rate (left) and early lick rate (right) for all mice trained in d2AFC. Criterion level (75%) and chance level (50%) are labeled as gray and red dash lines, respectively. (<bold>E</bold>) Same as (<bold>D</bold>) but for manual training (1~3 hr/day in home-cage). (<bold>F</bold>) Averaged correct rate (left), early lick rate (middle), and no response rate (right) of expert mice trained with the two protocols. (<bold>G</bold>) Averaged number of trials (left) and days (right) to reach the criterion performance for the two training protocols. Circles, individual mice. Error bar, mean, and 95% CI across mice. (<bold>H</bold>) Left, number of trials performed per day throughout the training schedule for three different protocols. Error bar indicates the mean and 95% confidence interval (CI) across mice. Middle, volume of water harvested per day. Right, Relative body weights of mice in days 0, 8, 16, 26. Bold line and shades indicate mean and 95% CI across mice. (<bold>I</bold>) Behavioral performance of all mice training in d2AFC task based on sound orientation (left), light orientation (middle), and light color (right). (<bold>J</bold>) Box plot of average number of trials (left) and days (right) to reach the criterion performance for d2AFC tasks with different sensory modalities. (<bold>K</bold>), Left, percentage of trials performed as a function of time in a day for the four modalities trained autonomously (thick black shows the average). Shaded area indicates the dark cycle. Top right, averaged correct rate of grouped mice in dark cycle versus light cycle. Error bars show 95% CI across mice. Bottom right, box plot of the averaged proportion of trials performed in dark cycle for the four modalities. Data collected from expert mice. (<bold>L</bold>) Left, percentage of trials in blocks with varying number of consecutive trials for automated training in home-cage. Right, correct rate and early lick rate as functions of trial block size. Gray dash line, the criterion performance; Red dash line, chance performance level. Data collected from trials of expert mice. For significance levels not mentioned in all figures, n.s., not significant, p&gt;0.05; *, p&lt;0.05; **, p&lt;0.01 (two-sided Wilcoxon rank-sum tests).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Autonomous versus manual training in home-cage.</title><p>(<bold>A</bold>) Flow chart of the task training protocol in home-cage (Materials and methods). (<bold>B</bold>) Logistic regression model. (<bold>C</bold>) Top, behavioral performance of example mouse in the autonomous training. Bottom, the significance of individual regressors; Circle size corresponds to p values; The significance of a regressor is evaluated by comparing the prediction of the full model to a partial model with the regressor of interest excluded. p-values are based on cross-validation t-test (Materials and methods). (<bold>D</bold>) Percentage of trials significantly predicted by different regressors during task learning. Cycles and light lines, individual mice; Bars and bold lines, average across mice; Shades and error bars, 0.95 CI. *, p&lt;0.05, n.s., p&gt;0.05, two-sided Wilcoxon rank-sum tests. (<bold>E</bold>) averaged water harvested per day (left) and number of trials per day (right) changing from manual to autonomous training in home-cage. Cycles, individual mice; Bar plot and error bar, mean and 0.95 CI across mice. (<bold>F</bold>) Averaged relative body weights as a function of training days for free water (blue) and all d2AFC training mice (black). Shaded area shows 95% CI. (<bold>G</bold>), Performance of all 6 female mice performing d2AFC task in home-cage automatically. (<bold>H</bold>) The histogram of inter-trial interval for both autonomous and manual training in HABITS.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Reaction-time-based 2AFC task training in home-cage automatically.</title><p>(<bold>A</bold>) Task structure of RT-based 2AFC task. (<bold>B</bold>) Flow chart of training protocol in home-cage. (<bold>C</bold>) Conditioned behavioral data of example trials for correct (blue block) and error (red block) choice. (<bold>D</bold>) Performance of example mouse performing task in home-cage. The color of the background corresponding to (<bold>B</bold>). Gray blocks indicate dark cycle. Gray dash line, the criterion performance. Red horizontal dash line, chance performance level. (<bold>E</bold>) Correct rate of all mice. (<bold>F</bold>) Reaction time of all mice. Black line fitting to all mice from the onset to the end of training. (<bold>G</bold>) Histogram of reaction time. Data collected from all mice. The bold vertical line represents the median of RT. (<bold>H</bold>) Conditioned histogram of inter-trial interval (ITI) for correct (blue) and error (red) trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Value-based dynamic foraging task.</title><p>(<bold>A</bold>) Task structure. (<bold>B</bold>) Example performance of a mouse in the early (top, first 6000 trials) and late (bottom, last 6000 trials) training stages with block size 500. Blue lines represent moving averaged behavioral probability of left choice within 40 trials. Purple lines show the assignment probability for left reward. (<bold>C</bold>) Averaged probability of choosing the lickport with the higher assignment probability (P(high)) across mice gradually increases following the number of trials. Black line indicates the assignment probability for left and right lickports is 60% (gray line, 52.5%) and 10% (gray line, 17.5%), respectively. Dots and error bars, mean and 95% CI. (<bold>D</bold>) Left, averaged P(high) across mice follows training sub-protocols with different block size. Right, the number of days to complete all training protocols from block size 500–100. Square dots indicate individual mice. (<bold>E</bold>) Same as (<bold>B</bold>) but data collected from the sub-protocol with block size 100.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig2-figsupp3-v1.tif"/></fig></fig-group><p>Mice, living in HABITS all the time, licked any of the lickports to trigger a trial block at any time. In the d2AFC task with sound frequency modality (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), mice needed to discriminate two tone frequencies presented at the beginning of the trial for a fixed duration (sample epoch, 1.2 s) and responded to the choice by licking left (low tone) or right (high tone) lickports following a brief ‘go’ cue after a fixed delay duration (1.2 s). Licks during the delay epoch were prohibited, and unwished licks (early licks) will pause the trial for a while. Correct choices were rewarded, while incorrect choices resulted in noise and timeout. If mice did not lick any of the lickports after the ‘go’ cue for a fixed period (i.e. no-response trial), the trial block was terminated. The next trial block was immediately entering the state of to be triggered. Mice can only learn the stimulus-action contingency by trial-and-error. To promote learning, we designed an algorithm comprised of many subprotocols to shape the behavior step by step (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). <xref ref-type="fig" rid="fig2">Figure 2B</xref> illustrated example licks during correct, error, and early lick trials for the task. As training progressed, the correct rate increased and early lick rate declined gradually for the example mouse within the first 2 week (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). All the 10 mice enrolled in this task effectively learned the task and suppressed licking during the delay period within 15 days (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), achieving an average of 980±25 (mean ± SEM) trials per day.</p><p>We also tested another training scheme that is limited duration per day in HABITS to simulate the situation of manual training with human supervision. These mice were water-restricted and manually transferred from traditional home-cage to HABITS daily for 1–3 hr training sessions. The duration of sessions was determined by the speed of harvesting water as we controlled the daily available volumes of water to approximately 1 ml (<xref ref-type="bibr" rid="bib25">Guo et al., 2014a</xref>). All six mice learned the task as the autonomous counterpart (<xref ref-type="fig" rid="fig2">Figure 2</xref>) with similar final correct and early lick rate (except that the no-response rate was significantly lower for manual training; <xref ref-type="fig" rid="fig2">Figure 2F</xref>). Logistic regression of the mice’s choice revealed similar behavioral strategies were utilized throughout the training for both groups (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B-D</xref>). Autonomous training needed significantly more trials than manual training (10,164 ± 1062 vs. 6845 ± 782) to reach the criterion performance; however, the number of days was slightly less due to the high trial number per day (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). As shown in <xref ref-type="fig" rid="fig2">Figure 2H</xref>, autonomous training yielded significantly higher number of trial/day (980 ± 25 vs. 611 ± 26, <xref ref-type="fig" rid="fig2">Figure 2H</xref> left) and more volume of water consumption/day (1.65 ± 0.06 vs. 0.97 ± 0.03 ml, <xref ref-type="fig" rid="fig2">Figure 2H</xref> middle), which resulted in monotonic increase of body weight that was even comparable to the free water group (<xref ref-type="fig" rid="fig2">Figure 2H</xref> right). In contrast, the body weight in the manual training group experienced a sharp drop at the beginning of training and was constantly lower than the autonomous group throughout the training stage (<xref ref-type="fig" rid="fig2">Figure 2H</xref> right). As the training advanced, the number of trials triggered by mice per day decreased gradually for both groups of mice, but the water consumption per day kept relatively stable. At the end of manual training, we transferred all mice to autonomous testing and found that the number of trial and consumption water per day dramatically increased to the level of the autonomous training, suggesting mice actually needed more water throughout the day (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E</xref>). These results indicated that autonomous training achieved similar training performance as manual training and maintained a more healthy state of mice.</p><p>Three more cohorts of mice were used in d2AFC tasks with different modalities, which included sound orientation (left vs. right sound), light orientation (left vs. right light), and light color (red vs. blue). Using the same training protocol as in the sound frequency modality, we trained 10, 11, and 8 mice on these tasks, respectively (<xref ref-type="fig" rid="fig2">Figure 2I</xref>). Mice required different amounts of trials or days to discriminate these modalities, with light color discrimination being the most challenging (an average of 82,932±6817 trials), consistent with the limited sensitivity to light wavelength of mice (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). We also tried other modalities, like blue vs. green and flashed blue vs. green, but all failed (see <xref ref-type="table" rid="table1">Table 1</xref>). The learning rate between sound and light orientation discrimination tasks was similar (p=0.439, two-sided Wilcoxon rank-sum test), but the variation for light orientation was large, indicating possible individual differences (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). All modalities maintained good health state indicated by the body weight for up to 2 months (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1F</xref>). Another two types of 2AFC, reaction time (<xref ref-type="bibr" rid="bib52">Munoz and Everling, 2004</xref>) (RT-2AFC, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) and random foraging (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) task, were also successfully tested in HABITS. Notably, the dynamic foraging task (<xref ref-type="bibr" rid="bib33">Hattori et al., 2023</xref>; <xref ref-type="bibr" rid="bib32">Hattori et al., 2019</xref>), which heavily relies on historical information, was first demonstrated in a fully autonomous training scheme for freely moving mice with similar block size and performance.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>All tasks training in HABITS.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Protocol name (abbreviation)</th><th align="left" valign="bottom">Modality</th><th align="left" valign="bottom">Animals trained(trained / used)</th><th align="left" valign="bottom">Note</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="6">delayed 2-Alternative Forced Choice (d2AFC)</td><td align="left" valign="bottom">Sound frequency (3 k vs. 10 kHz)</td><td align="left" valign="bottom">11/11</td><td align="left" valign="bottom" rowspan="4"><xref ref-type="fig" rid="fig2">Figure 2</xref></td></tr><tr><td align="left" valign="bottom">Sound orientation (Left vs. Right)</td><td align="left" valign="bottom">10/11</td></tr><tr><td align="left" valign="bottom">Light orientation (Left vs. Right)</td><td align="left" valign="bottom">10/11</td></tr><tr><td align="left" valign="bottom">Light color (Blue VS. Red)</td><td align="left" valign="bottom">8/11</td></tr><tr><td align="left" valign="bottom">Light color (Green VS. Blue)</td><td align="left" valign="bottom">0/10</td><td align="left" valign="bottom" rowspan="2">Mice are insensitive to light colors.</td></tr><tr><td align="left" valign="bottom">Light color (flashed Green VS. Blue)</td><td align="left" valign="bottom">0/10</td></tr><tr><td align="left" valign="bottom">Reaction time 2AFC (RT-2AFC)</td><td align="left" valign="bottom">Sound frequency (3 k vs. 12 kHz)</td><td align="left" valign="bottom">6/6</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref></td></tr><tr><td align="left" valign="bottom">Contingency reversal</td><td align="left" valign="bottom">RT-2AFC, sound frequency (3 k vs. 12 kHz)</td><td align="left" valign="bottom">8/8</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3A</xref></td></tr><tr><td align="left" valign="bottom">Continuous learning</td><td align="left" valign="bottom">Sound freq. (3 k vs. 12 kHz), reversal sound freq., sound orient. (Left vs. Right), reversal sound orient., light orient. (Left vs. Right)</td><td align="left" valign="bottom">10/30</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4A</xref>;<break/>20 mice did not learn light oriental modal within 90 days.</td></tr><tr><td align="left" valign="bottom">Evidence accumulation with spatial cue</td><td align="left" valign="bottom">Poisson distributed clicks with spatial diff. (Left vs. Right)</td><td align="left" valign="bottom">8/10</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3C</xref></td></tr><tr><td align="left" valign="bottom" rowspan="2">Multimodal Integration</td><td align="left" valign="bottom">Poisson distributed clicks and flashes (4 vs 20 events/s)</td><td align="left" valign="bottom">13/15</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3D</xref></td></tr><tr><td align="left" valign="bottom">Poisson distributed light flashes (4 vs 20 events/s)</td><td align="left" valign="bottom">3/15</td><td align="left" valign="bottom">12/15 mice failed to discriminate light flash</td></tr><tr><td align="left" valign="bottom" rowspan="2">Confidence probing task</td><td align="left" valign="bottom">Sound frequency (8 k vs. 32 kHz)</td><td align="left" valign="bottom">5/6</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3E</xref></td></tr><tr><td align="left" valign="bottom">Sound frequency (8 k vs. 32 kHz) and Poisson distributed clicks with spatial diff. (Left vs. Right)</td><td align="left" valign="bottom">0/12</td><td align="left" valign="bottom">Delay period up to 8 sec, failed</td></tr><tr><td align="left" valign="bottom">Value-based dynamic foraging task</td><td align="left" valign="bottom">No sensory cues (Block size from 500 to 100)</td><td align="left" valign="bottom">6/6</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref></td></tr><tr><td align="left" valign="bottom" rowspan="2">Working memory task</td><td align="left" valign="bottom">Temporal regular clicks with 5 alternative rates (8, 16, 32, 64, 128 Hz)</td><td align="left" valign="bottom">5/6</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3">Figure 3B</xref></td></tr><tr><td align="left" valign="bottom">Temporal regular clicks with 3 alternative rates (8, 32, 128 Hz)</td><td align="left" valign="bottom">8/8</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> C</td></tr><tr><td align="left" valign="bottom" rowspan="2">double Delayed Match Sample (dDMS)</td><td align="left" valign="bottom">Sound frequency (3 k &amp; 12 kHz)</td><td align="left" valign="bottom">10/10</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4B</xref>;<break/>Sample and test period: 500ms</td></tr><tr><td align="left" valign="bottom">Sound frequency (3 k &amp; 12 kHz)</td><td align="left" valign="bottom">0/10</td><td align="left" valign="bottom">Random sample and test period: 100 or 1000ms</td></tr><tr><td align="left" valign="bottom" rowspan="3">delayed 3-Alternative Forced Choice (d3AFC)</td><td align="left" valign="bottom">Sound frequency (8 k vs. 16 k vs. 32 kHz)</td><td align="left" valign="bottom">14/14</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4C</xref></td></tr><tr><td align="left" valign="bottom">Sound frequency reversal (8 k vs. 16 k vs. 32 kHz)</td><td align="left" valign="bottom">4/4</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref></td></tr><tr><td align="left" valign="bottom">Sound orientation (Left vs. Middle vs. Right)</td><td align="left" valign="bottom">6/6</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref></td></tr><tr><td align="left" valign="bottom" rowspan="3">Context-dependent attention task</td><td align="left" valign="bottom">Sound frequency (3 k vs. 12 kHz) or sound orientation (Left vs. Right); regular click rates (16 vs 64 Hz) as context cue</td><td align="left" valign="bottom">6/6</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig4">Figure 4D</xref></td></tr><tr><td align="left" valign="bottom">Sound orientation (Left vs. Right) or light orientation (Left vs. Right); regular click rates (16 vs 64 Hz) as context cue</td><td align="left" valign="bottom">0/10</td><td align="left" valign="bottom" rowspan="2">Mice failed in light modality</td></tr><tr><td align="left" valign="bottom">Sound orientation (Left vs. Right) or light orientation (Left vs. Right); Sound frequency (3 k vs. 12 kHz) as context cue</td><td align="left" valign="bottom">0/20</td></tr><tr><td align="left" valign="bottom" rowspan="3">Machine teaching algorithm</td><td align="left" valign="bottom">Working memory task, Temporal regular clicks with 5 alternative rates and full stimulus matrix (8, 16, 32, 64, 128 Hz)</td><td align="left" valign="bottom">7/7 (MT)<break/>5/8(Random)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5B</xref></td></tr><tr><td align="left" valign="bottom">RT-2AFC, Sound frequency (3 k vs. 12 kHz)</td><td align="left" valign="bottom">10/10 (MT)<break/>10/10 (random)<break/>10/10 (antibias)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5F</xref>;<break/>Same group of mice used in continuous learning</td></tr><tr><td align="left" valign="bottom">2AFC with Sound frequency (3 k vs. 12 kHz), sound orientation (Left vs. Right) and sound orientation reversal, respectively</td><td align="left" valign="bottom">10/10 (MT)<break/>8/8 (random)</td><td align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6</xref></td></tr><tr><td align="left" valign="bottom"><bold>Total</bold></td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom"><bold>200/284</bold></td><td align="left" valign="bottom">N/A</td></tr></tbody></table></table-wrap><p>Two important behavioral characteristics were revealed across all the modalities in 2AFC task. Firstly, as our high-throughput behavioral training platform operated on a 12:12 light-dark cycle, the long-term circadian rhythm of mice can be evaluated based on the number of triggered trials and performance during both cycles. We found all mice exhibited clear nocturnal behavior with peaks in trial proportion at the beginning and end of the dark period (<xref ref-type="fig" rid="fig2">Figure 2K</xref>), which was consistent with previous studies (<xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>). The light color modality exhibited a slightly lower percentage of trials during dark (57.89% ± 3.18% vs. above 66% for other modalities; <xref ref-type="fig" rid="fig2">Figure 2K</xref>), possibly the light stimulus during trials affected the circadian rhythms of the mice. It was also observed that all mice except the light color modality showed no significant differences in correct rate between light and dark cycle after they learned the task (<xref ref-type="fig" rid="fig2">Figure 2K</xref>). The higher performance in a dark environment for light color modality implied that light stimulus presented in a dark environment was with higher contrast and thus better discernibility. Secondly, as we organized the trials into blocks, the training temporal dynamic at trial-level could be examined. We found more than two-thirds of the trials were done in &gt;5-trial blocks (<xref ref-type="fig" rid="fig2">Figure 2L</xref> left), which resulted in more than 55% of the trials being with inter-trial intervals less than 2 s (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1H</xref>). The averaged block duration was 27.64±1.73 s and mice triggered another block of trials within 60 s in more than 60% of cases. Meanwhile, we also found that the averaged correct rate increased and the early lick rate decreased as the length of the block increased (<xref ref-type="fig" rid="fig2">Figure 2L</xref> right), which suggested that mice were more engaged in the task during longer blocks.</p><p>These results showed that mice can learn and perform cognitive tasks in HABITS with various modalities in a fully autonomous way. During the training process, mice maintained good health conditions, although without any human intervention. Due to the high-efficiency training with less labor and time, it gave us an opportunity to explore and study more widespread cognitive behavioral tasks in mice.</p><sec id="s2-2-1"><title>Various cognitive tasks demonstrated in HABITS</title><p>We next tested several representative tasks that were commonly used in the field of cognitive neuroscience to demonstrate the universality of HABITS. It is worth noting that many new features of the behavior could be explored due to the autonomy and advantages of HABITS, in terms of either quantity or quality (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Representative cognitive task performed in HABITS.</title><p>(<bold>A</bold>) Contingency reversal task. (<bold>A1</bold>) Task structure. (<bold>A2</bold>) Correct rate of example mice with different learning rates. Gray vertical lines indicate contingency reversal. (<bold>A3</bold>) Relative number of trials to reach the criterion as a function of reverse times. Gray lines, individual mice. Black lines, linear fit. (<bold>A4</bold>) Number of trials in the first reversal learning versus the average number of trials of the rest of contingency reversal learning for each mouse (each dot). Black line, linear regression. Red dash line, diagonal line. (<bold>B</bold>) Working memory task with sound frequency modality. (<bold>B1</bold>) Task structure. (<bold>B2</bold>) Stimulus generation matrix (SGM) for left (orange) and right (green) trials. (<bold>B3</bold>) Left, averaged correct rate for each stimulus combination tested. Right, averaged correct rate for each (S1 +S2) stimulus combination across mice. Black line and shade, linear regression and 95% CI. (<bold>B4</bold>) Averaged psychometric curves, that is percentage of right choice as a function of frequency difference between sample 1 and sample 2. (<bold>B5</bold>) Averaged correct rate as a function of delay duration. (<bold>C</bold>) Evidence accumulation with spatial cue task. (<bold>C1</bold>) Task structure. (<bold>C2</bold>) Averaged psychometric curves, that is performance as a function of the difference between right and left clicks rates. (<bold>C3</bold>) Averaged correct rate across all mice as a function of sample duration for different Poisson rates (different colors). Error bar represents 95% CI. (<bold>D</bold>) Multimodal integration task. (<bold>D1</bold>) Task structure. (<bold>D2</bold>) Averaged correct rate across all mice as a function of sample duration for different stimulus modalities (different colors). (<bold>D3</bold>) Averaged event rates during sample period for left (red) and right (blue) choice trials. (<bold>D4</bold>) Averaged weights (black line) of logistic regression fitting to the choice of trials across expert mice tested in &gt;1000 trials (N=11 mice) from the first bin (40ms) to the last bin (1000ms) of the sample period. A gray dash line represents the null hypothesis. Gray dots indicate significance, p&lt;0.05, two-sided <italic>t</italic>-tests. (<bold>D5</bold>), Psychometric curve for trials with multimodal stimulus. (<bold>E</bold>) Confidence probing task. (<bold>E1</bold>) Task structure. (<bold>E2</bold>) Psychometric curve, that is right choice rate as a function of relative contrast (log scaled relative frequency). (<bold>E3</bold>) Histogram of time invested (TI) for both correct and error trials. (<bold>E4</bold>) Averaged correct rate across all mice as a function of TI. (<bold>E5</bold>) Averaged TI as a function of absolute relative contrast for both correct and error trials. Cycles, individual mice; *, p&lt;0.05; **, p&lt;0.01, two-sided Wilcoxon rank-sum tests.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Other complex cognitive behavioral tasks training in home-cage automatically.</title><p>(<bold>A</bold>), Left, stimulus generation matrix of working memory task. Middle, number of days to train. Right, correct rate for SGM. Values lying in the diagonal line corresponding to the correct rate of probe trials. (<bold>B</bold>), Top, d3AFC task according to sound orientation and number of days to reach the criterion performance. Dots indicate individual mice. Performance (middle) and early lick rate (bottom) of all mice performing the d3AFC task. Red dash line, chance performance level; Grey dash line, the criterion performance. (<bold>C</bold>), Left, contingency reversal of d3AFC task according to sound frequency (top) and performance of an example mouse (bottom). Right, averaged correct rate across all mice for different reverse times (top). Number of trials needed to learn as a function of reverse training times (bottom). Dots, individual mice. Line and shades, linear regression.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig3-figsupp1-v1.tif"/></fig></fig-group><p>Contingency reversal task was a cognition-demanding task and previously used to investigate cognitive flexibility (<xref ref-type="bibr" rid="bib36">Izquierdo et al., 2017</xref>; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). In the task, the contingency for reward switched without any cues once mice hit the criteria performance (<xref ref-type="fig" rid="fig3">Figure 3A1</xref>). Mice can dynamically reverse their stimulus-action contingency, though with different learning rates across individuals (<xref ref-type="fig" rid="fig3">Figure 3A2</xref>). Given the advantages of long-term autonomous training in HABITS, all mice underwent contingency reversal for an average of 52.25±16.39 times, and one mouse achieved up to 125 times in 113 days. Most of the mice (6/8) gradually decreased the number of trials to reach criterion across multiple contingency reversals representing an effect of learning to learn (<xref ref-type="bibr" rid="bib33">Hattori et al., 2023</xref>; <xref ref-type="bibr" rid="bib2">Akrami et al., 2018</xref>; <xref ref-type="fig" rid="fig3">Figure 3A3</xref>). Meanwhile, the average number of trials to reach criterion during the reversal was highly correlated with the trial number in the first reversal learning which represented the initial cognitive ability or the sensory sensitivity of individual mice (<xref ref-type="fig" rid="fig3">Figure 3A4</xref>).</p><p>Working memory was another important cognition that was vastly investigated using rodent model with auditory and somatosensory modalities (<xref ref-type="bibr" rid="bib2">Akrami et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Fassihi et al., 2014</xref>). Here, we utilized a novel modality, regular sound clicks, to implement a self-initiated working memory task, which required mice to compare two click rates separated by a random delay period (<xref ref-type="fig" rid="fig3">Figure 3B1</xref>). We initially validated that mice did employ a comparison strategy in a 3×3 stimulus generation matrix (SGM), instead of just taking the first cue as a context (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). Subsequently, we expanded the paradigm’s perception dynamics to 5×5 and reduced the relative perceptual contrast between two neighboring stimuli to one octave (<xref ref-type="fig" rid="fig3">Figure 3B2</xref>). HABITS enabled investigation of detailed behavioral parameters swiftly. We noticed that the discrimination ability for mice was significantly better in the higher frequency range (<xref ref-type="fig" rid="fig3">Figure 3B3</xref>), which may be caused by the different sensitivity of mice across the spectrum of regular click rate. During the testing stage, we varied the contrast of the preceding stimulus while maintaining the succeeding one; the psychometric curves affirmed mice’s decision-making based on perceptual comparison of two stimuli and validated their perceptual and memory capacities (<xref ref-type="fig" rid="fig3">Figure 3B4</xref>). Meanwhile, mice could maintain stable working memory during up to 1.8 s delay, demonstrating mice can perform this task robustly (<xref ref-type="fig" rid="fig3">Figure 3B5</xref>).</p><p>Evidence accumulation introduced more dynamics to the decision-making within individual trials and was widely utilized (<xref ref-type="bibr" rid="bib13">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Erlich et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Hanks et al., 2015</xref>). In the task implemented in HABITS, mice needed to compare the rate of sound clicks that randomly appeared over two sides during the sample epoch and made decisions following a ‘go’ cue after a brief delay (<xref ref-type="fig" rid="fig3">Figure 3C1</xref>). We successfully trained 8/10 mice to complete this task as revealed by the psychometric curves (<xref ref-type="fig" rid="fig3">Figure 3C2</xref>). After mice learned the task, we systematically tested the effect of evidence accumulation versus the task performance. All mice exhibited consistent behavioral patterns, which were correlated with the evidence of stimulus, that is longer sample period and/or higher stimulus contrast led to higher performance (<xref ref-type="fig" rid="fig3">Figure 3C3</xref>). It showed an evident positive correlation between evidence accumulation and task performance.</p><p>Multimodal integration <xref ref-type="bibr" rid="bib49">Meijer et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Odoemene et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Raposo et al., 2012</xref> was also tested in HABITS with sound clicks and light flashes as dual-modality events in the evidence accumulation framework. Mice were required to differentiate whether event rates were larger or smaller than 12 event/s (<xref ref-type="fig" rid="fig3">Figure 3D1</xref>). We successfully trained 13/15 mice in this paradigm with multimodal or sound clicks stimulus (<xref ref-type="fig" rid="fig3">Figure 3D2</xref>), and only three mice achieved performance criteria in trials with light flashes stimulus. Since the events were presented non-uniformly within each trial, we wondered about the dynamics of the decision-making process along the trial. Firstly, we divided all trials with multimodal stimulus into two groups according to the choice of expert mice. The uniform distribution of events within a trial indicated that mice considered the whole sample period to make a decision (<xref ref-type="fig" rid="fig3">Figure 3D3</xref>). Secondly, we used a logistic regression model to illustrate the mice’s dependency on perceptual decisions throughout the entire sample period. We found that mice indeed tended to favor earlier stimuli (i.e. higher weights) in making their final choices (<xref ref-type="fig" rid="fig3">Figure 3D4</xref>), consistent with previous research findings (<xref ref-type="bibr" rid="bib57">Odoemene et al., 2018</xref>). Lastly, we further tested modulated unimodal stimuli with different sample periods in the testing stage, in which accuracy correlated positively with sample period and conditioned test for different combinations of modalities demonstrated evidence accumulation and multimodal integration effect, respectively. (<xref ref-type="fig" rid="fig3">Figure 3D5</xref>).</p><p>Confidence was another important cognition along with the process of decision-making, which was investigated in rat (<xref ref-type="bibr" rid="bib40">Kepecs et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Masset et al., 2020</xref>) and more recently in mice (<xref ref-type="bibr" rid="bib70">Schmack et al., 2021</xref>) model previously. We introduced a confidence-probing task in HABITS (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), in which mice needed to lick twice the correct side for acquiring a reward; the two licks were separated by a random delay during which licking at other lickports prematurely ended the trial (<xref ref-type="fig" rid="fig3">Figure 3E1</xref>). This unique design connects mice’s confidence about the choice, which was hidden, with the time investment (TI) of mice between two licks, which was an explicit and quantitative metric. We successfully trained 5/6 mice (<xref ref-type="fig" rid="fig3">Figure 3E2</xref>), and most importantly, there was a noticeable difference of TI in correct versus incorrect trials (<xref ref-type="fig" rid="fig3">Figure 3E3</xref>). In detail, trials with longer TIs tended to have higher accuracies (<xref ref-type="fig" rid="fig3">Figure 3E4</xref>). Furthermore, the TI was also modulated by the contrasts of stimuli; as contrast decreased, mice exhibited reduced confidence about their choices, manifesting as decreased willingness to wait in correct trials, and conversely in error trials (<xref ref-type="fig" rid="fig3">Figure 3E5</xref>).</p><p>In summary, mice could undergo stable and effective long-term training in HABITS with various cognitive tasks commonly used in state-of-the-art neuroscience. These tasks running in HABITS were demonstrated to exhibit similar behavioral characteristics to previous studies. In addition, some new aspects of the behavior could be systematically tested in HABITS due to its key advantage of autonomy. This high level of versatility, combined with the ability to support arbitrary paradigm designs, suggests that more specialized behavioral paradigms could potentially benefit from HABITS to enhance experimental novelty.</p></sec></sec><sec id="s2-3"><title>Innovating mouse behaviors in HABITS</title><p>One of the main goals of HABITS was to expand mouse behavioral reservoir by developing complex and innovative paradigms that had previously proven challenging or even impossible for mice. These paradigms imposed higher cognitive abilities demands, which required an extensively long period to test in a mouse model. HABITS enabled unsupervised, efficient, and standardized training of these challenging paradigms at scale, and thus was suitable for behavioral innovations.</p><p>Firstly, leveraging the autonomy of HABITS, we tested mouse’s ability to successively learn up to 5 tasks one after another without any cues (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). These tasks included 2AFC based on sound frequency, sound frequency reversal, sound orientation (pro), sound orientation (anti), and light orientation (<xref ref-type="fig" rid="fig4">Figure 4A1</xref>). Firstly, the results showed that mice could quickly switch from one task to another and the learning rates across these sub-tasks roughly followed the learning difficulty of modalities (<xref ref-type="fig" rid="fig4">Figure 4A2</xref>). Specifically, reversal of sound frequency was cognitively different from reversal of sound orientation (i.e., from pro to anti), which resulted in significantly longer learning duration (<xref ref-type="fig" rid="fig4">Figure 4A2</xref>). Secondly, mice dealt with new tasks with higher reaction time and gradually decreased as training progressed (<xref ref-type="fig" rid="fig4">Figure 4A3</xref>). It implied a uniform strategy mice applied: mice chose to respond more slowly in order to learn quickly (<xref ref-type="bibr" rid="bib47">Masís et al., 2023</xref>). Lastly, mice exhibited large bias at the beginning of each task in all tasks, including tasks without reversals (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This means that mice acquired reward only from one lickport in the early training and switched strategy to follow current stimulus gradually, which implied a changing strategy from exploration to exploitation.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Challenging mouse tasks innovated in HABITS.</title><p>(<bold>A</bold>) Continuous learning task. (<bold>A1</bold>) Task structure showing mice learning five subtasks one by one. (<bold>A2</bold>) Left, averaged correct rate of all mice performing the five tasks (different colors) continually. All task schedules are normalized to their maximum number of trials and divided into 10 stages equally. Right, box plot of number of trials to criteria for each task. (<bold>A3</bold>) Left, averaged reaction time of all mice performing the five tasks continually. Right, averaged median reaction time across the five tasks during early (perf. &lt;0.55), middle (perf. &lt;0.75), and trained (perf. &gt;0.75) stage. Error bar indicates 95% CI. (<bold>A4</bold>) Same as (<bold>A3</bold>) but for absolute performance bias. n.s., p&gt;0.05; **, p&lt;0.01, two-sided Wilcoxon signed-rank tests. (<bold>B</bold>) Double delayed match sample task (dDMS) with sound frequency modality. (<bold>B1</bold>) Task structure. (<bold>B2</bold>) Averaged correct rate across all mice during training (left) and averaged number of days to reach the criterion (right). (<bold>B3</bold>) Averaged early lick rate across all mice. (<bold>B4</bold>) Averaged correct rate (black) and early lick rate (gray) for all combination of sample and test stimulus. (<bold>B5</bold>) Heatmap of error rate (left) and early lick rate (right) varies with different combination of delay1 and delay2 durations. (<bold>C</bold>) Delayed 3 alternative forced choice (d3AFC). (<bold>C1</bold>) Task structure. (<bold>C2</bold>) Averaged correct rate across all mice during training (left, colors indicate trial types) and averaged number of days to reach the criterion performance (right). (<bold>C3</bold>) Averaged correct rate (colors indicate trial types) and early lick rate (gray) for different trial types. (<bold>C4</bold>) Averaged error rate of choices conditioning trial types. In each subplot, the position of bars corresponds to different choices. ****, p&lt;0.0001, n.s., p&gt;0.05, two-sided t-tests. (<bold>C5</bold>) Averaged choice rates for the three lickports (colors) as a function of sample frequency. Data collected from trained mice. (<bold>D</bold>) Context-dependent attention task. (<bold>D1</bold>) Task structure. (<bold>D2</bold>) Averaged correct rate across all mice during training (left, data only from trials with multimodal w/ conflict) and averaged number of days to reach the criterion (right). (<bold>D3</bold>) Correct rate (left) and reaction time (right) conditioning modalities. (<bold>D4</bold>) Averaged psychometric curve and partitioned linear regression for the multimodal with and without conflict conditions, respectively. (<bold>D5</bold>) Performance bias to sound orientation modal as a function of pre-cue contrast, for the two multimodal conditions. (<bold>D6</bold>) Averaged correct rate as a function of delay duration.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig4-v1.tif"/></fig><p>The delayed match sample task was quite challenging for mice, and only olfactory and tactile modalities were implemented previously <xref ref-type="bibr" rid="bib16">Condylis et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Liu et al., 2014</xref>; <xref ref-type="bibr" rid="bib77">Taxidis et al., 2020</xref>. Recently, auditory modality was introduced but only in a go/no-go paradigm <xref ref-type="bibr" rid="bib84">Yu, 2021</xref>. We next constructed a novel double delayed match sample task (dDMS) task (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), which required mice to keep working memory of first sound frequency (low or high) during the first delay, match to the second sound based on XOR rules, make a motor planning during the second delay, and finally make a 2AFC choice (<xref ref-type="fig" rid="fig4">Figure 4B1</xref>). All the 10 mice achieved the performance criteria during the automated training process, though, an averaged 64.45±7.88 days was required, which was equivalent to more than 120,000 trials (<xref ref-type="fig" rid="fig4">Figure 4B2-3</xref>). After training, the four trial types (i.e. four combinations of frequencies) achieved equally well performance (<xref ref-type="fig" rid="fig4">Figure 4B4</xref>). During the testing stage, we systematically randomized the duration of the two delays (ranging from 1 to 3 s) and revealed increased error rates and early lick rates as the delay increased (<xref ref-type="fig" rid="fig4">Figure 4B5</xref>). Challenging tasks that demanded months of training were well suited for HABITS; otherwise, they were difficult or even impossible for manual training.</p><p>Subsequently, we attempted to expand the choice repertoire of d2AFC into 3-alternative forced choice (d3AFC), utilizing the three lickports installed in HABITS (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Previous studies have implemented multiple choice tasks, but only based on spatial modalities <xref ref-type="bibr" rid="bib5">Asinof and Paine, 2014</xref>; <xref ref-type="bibr" rid="bib11">Birtalan et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Piantadosi et al., 2019</xref>. In our system, mice learned to discriminate low (8 kHz), medium (16 kHz), and high (32 kHz) sound frequencies and lick respectively left, middle, and right lickport to get reward (<xref ref-type="fig" rid="fig4">Figure 4C1</xref>). Mice needed to construct two psychological thresholds to conduct correct choices. We successfully trained all the 14 mice to perform the tasks in 13.85±1.05 days, with similar performance among the left, middle, and right trial types (<xref ref-type="fig" rid="fig4">Figure 4C2</xref>). The final correct rate and early lick rate made no differences for the three trial types (<xref ref-type="fig" rid="fig4">Figure 4C3</xref>). Interestingly, mice made error choices more in the most proximity lickport; for the middle trials, mice made error choices equally in the left and right sides (<xref ref-type="fig" rid="fig4">Figure 4C4</xref>). In addition, we tested the whole spectrum of sound frequencies between 8 and 32 kHz and found that mice presented two evident psychological thresholds to deal with this three-choice task (<xref ref-type="fig" rid="fig4">Figure 4C5</xref>). Finally, we also implemented sound orientation-based d3AFC in another separated group of mice, which actually required longer training duration (45.16±11.46 days) (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). The d3AFC was also tested for reversal contingency paradigm, and an accelerated learning was revealed (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>), which potentially provides new insight into the cognitive flexibility <xref ref-type="bibr" rid="bib60">Piantadosi et al., 2019</xref><bold>.</bold></p><p>Finally, we introduced one of the most challenging cognitive tasks in mouse model, delayed context-dependent attention task, in HABITS (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). This task was previously implemented by light and sound orientation modalities <xref ref-type="bibr" rid="bib51">Mukherjee et al., 2021</xref>; <xref ref-type="bibr" rid="bib82">Wimmer et al., 2015</xref>; however, due to the difficulty, it was not well repeated broadly. In HABITS, we tailored this task into a sound-only-based but multimodal decision-making task (<xref ref-type="fig" rid="fig4">Figure 4D1</xref>). We constructed this task using three auditory modalities: regular clicks (16 vs 64 clicks/s) as context, sound frequency (3 k <italic>vs.</italic> 12 kHz), and sound orientation (left vs. right) as the two stimulus modalities. Mice needed to pay attention to one of the modalities, which presented simultaneously during sample epoch, according to the context cue indicated by the clicks (low click rate to sound frequency and high rate to sound orientation), and make a 2AFC decision accordingly. We successfully trained all six mice enrolled in this task, with an average of 48.09±7.54 days (<xref ref-type="fig" rid="fig4">Figure 4D2</xref>). To validate the paradigm’s stability and effectiveness, the direction of stimulus features was presented randomly and independently during the final testing stage. Mice exceeded criterion performance across different trial types (i.e. unimodal, multimodal w/ conflict, multimodal w/o conflict), indicating effective attention to both stimulus features (<xref ref-type="fig" rid="fig4">Figure 4D3</xref>). Trials with conflicting stimulus features, requiring mice to integrate context information for correct choice, exhibited reduced decision speeds and accuracy compared to trials without conflicting for all tested mice (<xref ref-type="fig" rid="fig4">Figure 4D3</xref>). We further systematically varied the click rate from 16 to 64 Hz to change context contrast. For trials with conflicts, mice decreased their accuracy following the decline of context contrast, formulating a flat psychometric curve. However, for trials without conflicts, mice performed as a near-optimal learner (<xref ref-type="fig" rid="fig4">Figure 4D4</xref>). Meanwhile, as the contrast decreased, mice tended to bias to orientation feature against frequency in conflicting trials, but not for the trials without conflicts (<xref ref-type="fig" rid="fig4">Figure 4D5</xref>). All these results represented mice dealt with different conditioned trials by a dynamic decision strategy synthesizing context-dependent, multimodal integration, and perceptual bias. Lastly, the performance of both trial types declined with increased delay duration but maintained criterion above up to 2 s delay (<xref ref-type="fig" rid="fig4">Figure 4D6</xref>), confirming mice could execute this paradigm robustly in our system.</p><p>As a summary, by introducing changes in trial structure, cognition demands, and perceptual modalities, we extended mice behavior patterns in HABITS. These behaviors were usually challenging and very difficult to test previously with manual training. Thus, the training workflow of our system potentially allows for large-scale and efficient validation and iteration of innovative paradigms aimed to explore unanswered cognitive questions with mouse models.</p></sec><sec id="s2-4"><title>Machine-learning-aided behavioral optimization in HABITS</title><p>Mice can be trained to learn challenging tasks in a fully autonomous way in HABITS; however, whether the training efficiency is optimal was unknown. We hypothesized that an optimal train sequence generated by integrating all histories could enhance training procedure, compared with commonly used random or anti-bias strategies. Benefited from recent advances in machine teaching (MT) <xref ref-type="bibr" rid="bib86">Zhu et al., 2018</xref>, and inspired by previous simulations in optimal animal behavior training experiments <xref ref-type="bibr" rid="bib6">Bak et al., 2016</xref>, we developed a MT-based automated training sequence generation framework in HABITS to further improve the training qualities.</p><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> illustrated the architecture of the MT-based training framework. Initially, mice made an action corresponding to the stimuli presented in current trial <italic>t</italic> in HABITS; subsequently, an online logistic regression model was constructed to fit the mice’s history choices by weighted sum of multiple features including current stimulus, history, and rules. This model was deemed as the surrogate of the mouse and was used in the following steps; finally, sampling was performed across the entire trial type repertoire and the fitted model predicted positions of potential future trials in the latent weight space; the trial type with closest position to the goal was selected as the next trials. This entire process forms a closed-loop behavioral training framework, ensuring that the mice’s training direction continually progresses towards the goal.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>MT enabled faster learning with higher quality.</title><p>(<bold>A</bold>) The framework of machine teaching (MT) algorithm (see text for details). (<bold>B</bold>) Working memory task as in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, but with full stimulus generation matrix. (<bold>C</bold>) Averaged number of trials needed to reach the criterion for MT-based and random trial type selection strategies. **, p&lt;0.01, two-sided Wilcoxon rank-sum test. (<bold>D</bold>) The absolute difference between contrast (contr.) of sample1 (<bold>S1</bold>) and sample2 (<bold>S2</bold>) during training process for the two strategies. (<bold>E</bold>) Same as (<bold>D</bold>) but for correct rate. (<bold>F</bold>) MT-based d2AFC task training. Box plot of correct rate of expert mice (left) and number of trials needed to reach the criterion (right) for different training strategies (MT, anti-bias, and random). n.s., p&gt;0.05, Kruskal–Wallis tests. (<bold>G</bold>) Left, averaged absolute performance bias for the three strategies during different training stages. Right, averaged across training stages. (<bold>H</bold>) Same as (<bold>G</bold>) but for absolute trial type bias. (<bold>I</bold>) Percentage of trials showing significance for different regressors during task learning. (<bold>J–K</bold>) Box plot of correct rate (<bold>J</bold>) and prediction performance difference between the full model and partial model excluding current stimulus (<bold>S0</bold>) (<bold>K</bold>) for different trained stage, including early (perf. &gt;75%), middle (perf. &gt;80%), and well (perf. &gt;85%) trained. *, p&lt;0.05, **, p&lt;0.01, ***, p&lt;0.001, n.s., p&gt;0.05, two-sided Wilcoxon rank-sum tests with Bonferroni correction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Simulation of machine teaching algorithm in decision-making scenario.</title><p>(<bold>A</bold>) The weight of regressors in an ideal learner varies during learning a 2AFC task. Note that the initial weights of bias and S1 regressors are not zero. (<bold>B</bold>) The presented trial types generated by random (black) and MT (red) during the entire training process. (<bold>C</bold>) Same as (<bold>A</bold>) but weights of all regressors begin at zero.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig5-figsupp1-v1.tif"/></fig></fig-group><p>We first validated the theoretical feasibility and efficiency of the algorithms in simulated 2AFC experiments (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Faster increasement in sensitivity to current stimuli was observed through effective suppression of noise-like biases and history dependence with the MT algorithm (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A–B</xref>). Notably, if the learner was ideal (i.e. without any noise), there was no difference between random and MT strategies to train (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>). This implied that the training efficiency was improved by suppression of noise in MT.</p><p>To demonstrate MT in real animal training, we initially tested a working memory task similar to <xref ref-type="fig" rid="fig3">Figure 3B</xref>, but with a fully stimulus generation matrix. As shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, this task utilized a complete set of twenty trial types (colored dots), categorized into four levels of difficulty (dot sizes) according to the distance from decision boundary. Trial type selection using a MT algorithm against a baseline of random selection was tested in two separate groups. Mice trained with the MT algorithm achieved criteria performance with significantly fewer trials compared with the random group (<xref ref-type="fig" rid="fig5">Figure 5C</xref>); three out of the eight mice in the random group even did not reach the criteria performance at the end of training (60 days). We then asked what kind of strategy the algorithm used that supported an accelerated learning. Analysis of the trial type across learning revealed that the MT-based training presented easier trials first, then gradually increased the difficulty, that is exhibiting a clear curriculum learning trajectory (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). However, this did not mean that the MT only presented easy trials at the beginning; hard trials were occasionally selected when the model deemed that a hard trial could facilitate the learning. This strategy enabled mice to maintain consistently higher performance than random groups throughout the training process (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). These results suggested that MT-based method enabled more efficient training for specific challenging tasks.</p><p>To further validate the effectiveness of MT in more generalized perceptual decision-making tasks, we trained three groups of mice using random, antibias, and MT strategy, respectively, in sound-frequency-based 2AFC task. Due to the fact that this task was relatively simple, all three groups of mice achieved successful training, with comparable efficiency and final performance (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). But interestingly, the MT algorithm effectively reduced mice’s preference towards a specific lickport (i.e. bias; <xref ref-type="fig" rid="fig5">Figure 5G</xref>) throughout the training process by generating trial types with opposite bias more aggressively (<xref ref-type="fig" rid="fig5">Figure 5H</xref>). Using a model-based methodology, we demonstrated that while the MT algorithm minimized bias dependency, it did not increase, and even decreased, mice’s reliance on other noise variables, like previous action <italic>A<sub>1</sub></italic>, reward <italic>R<sub>1</sub></italic> and stimulus <italic>S<sub>1</sub></italic> (<xref ref-type="fig" rid="fig5">Figure 5I</xref>). Notably, we noticed that all trained mice demonstrated similar low bias (<xref ref-type="fig" rid="fig5">Figure 5G</xref>), while only the MT algorithm still exhibited relatively high anti-bias strategy during the trained stage (<xref ref-type="fig" rid="fig5">Figure 5H</xref>). This suggests that the MT algorithm might keep regulating cognitive processes actively even in expert mice. To verify this, we segmented the trained trials into early, middle, and well-trained stages based on performance level and showed that all three groups of mice had similar overall accuracies across stages (<xref ref-type="fig" rid="fig5">Figure 5J</xref>). However, when we examined the reliance on the current stimulus <italic>S<sub>0</sub></italic>, that is to what extent the decision was made according to current stimulus, we found that the MT group had significantly higher weights for <italic>S<sub>0</sub></italic> than both anti-bias and random groups (<xref ref-type="fig" rid="fig5">Figure 5K</xref>). This means that MT-generated sequences across all stages encouraged mice to rely only on current stimuli, rather than noise factors. These results suggested that MT-based training had higher quality for both training and trained stage.</p><p>In summary, the MT algorithm automatically chose optimal trial type sequences to enable faster and more efficient training. By modeling the effect of history, choice, and other noise behavioral variables, the MT method manifested higher quality training results. Based on these characteristics, the MT algorithm could enhance training efficiency in challenging paradigms and promote testing robustness in more general paradigms.</p></sec><sec id="s2-5"><title>Behavioral optimization for multi-dimensional tasks</title><p>One of the advantages of the MT algorithm remains that it considers multi-dimensional features altogether and gives the optimal trial sequences to guide the subject to learn. To test this feature, we next expanded the 2AFC task to two stimulus dimensions and trained mice to learn a dynamic stimuli-action contingency. The task was similar to the one applied in <xref ref-type="fig" rid="fig4">Figure 4D</xref> which presented both sound frequency and orientation features but without context cues. As illustrated in <xref ref-type="fig" rid="fig6">Figure 6A</xref>, the mice were initially required to focus on sound orientation to obtain reward, while ignoring the frequency (Goal 1). Subsequently, the stimulus-action contingency changed, making sound frequency, rather than orientation, the relevant stimuli dimension for receiving reward (Goal 2). Finally, the relevant cue was still sound frequency but with reversed stimulus-action contingency (Goal 3). Throughout the training process, we employed the MT algorithm to adaptively generate trial types about not only the reward location (left or right) but also the components of the sound stimuli (frequency and orientation combinations) and compared with the random control group. The MT algorithm allowed for the straightforward construction of a dynamic multi-goal training just by setting the coordinates of target goals within the latent weight space (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>MT manifested distinct learning path with faster forgetting and higher learning rate.</title><p>(<bold>A</bold>) Task structure. (<bold>B</bold>) Chart of training path in latent decision space following three goals one by one. (<bold>C</bold>) Top, averaged correct rate across grouped mice during training (color, machine teaching; black, random). Bottom, same as top but performance for non-relative cue. (<bold>D</bold>) Top, the slopes of linear regression between trial number and correct rate. Bottom, same as top but between trial number and performance for non-relative cue. **, p&lt;0.01; n.s., p&gt;0.05; two-sided Wilcoxon rank-sum tests. (<bold>E</bold>) The learning path of mice (lines) in latent decision space for machine teaching and random training strategies. Light dots represent model weights fitted by individual mice’s behavioral data. Shaded dots, averaged across mice. (Square dots, testing protocol; Cross dots, the first or the last half of trials in learning protocol; Cycle dots, all trials in learning protocol) (<bold>F</bold>) Left, averaged absolute trial type bias between stay and switch conditions across grouped mice for the MT and random strategies from L1 to L3. Right, same as middle but for the bias between left and right trials. (<bold>G</bold>) Same as (<bold>H</bold>) but for absolute performance bias in T1 and T2 protocols. L1, the first 500 trials of frequency learning protocol; L2, intermediate trials of frequency learning protocol; L3, the last 200 trials of frequency learning protocol; T1, testing orientation protocol; T2, testing frequency protocol. *, p&lt;0.05; n.s., p&gt;0.05; two-sided t-tests.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Details of behavioral analysis for multi-dimensional tasks.</title><p>(<bold>A</bold>) Left, linear regression between trial number and correct rate in task requiring mice to attend to sound frequency. Right, the R-square of every individual linear regression. (<bold>B</bold>) Same as (<bold>A</bold>) but for performance following non-relative cue. (<bold>C</bold>) The number of trials to reach criterion performance for MT and random group. (<bold>D</bold>) Performance of both grouped mice in T1 and T2 protocol. n.s., no significant. two-sided Wilcoxon rank-sum tests. (<bold>E</bold>) The presented individual trials with Stay/Switch (top) and Left/Right (bottom) trial type generated by MT (L3) and Random (T2). (<bold>F, G</bold>) After mice were trained by MT as in <xref ref-type="fig" rid="fig6">Figure 6A</xref>, they were intermediately set the training protocol to the beginning and retrained with randomly generated trial sequence. We compared the correct rate of trials with sound frequency stimulus in the first and the second training, presented in (<bold>F</bold>). (<bold>G</bold>) shows the learning rate (left) and training efficiency (right) of the first and the second training processes. **, p&lt;0.01; two-sided Wilcoxon signed-rank tests. (<bold>H</bold>), correct rate of both grouped mice for stay and switch trials in T2 protocol. n.s., no significant. two-sided Wilcoxon rank-sum tests.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104833-fig6-figsupp1-v1.tif"/></fig></fig-group><p>Both groups of mice successfully completed all the goals, achieving an overall correct rate of over 80% (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). During the first and the third goal of training, the learning rates of mice in both groups were similar, showing low sensitivity to irrelevant cue. However, in the second goal of training (i.e. transition from orientation to frequency modality), the MT group exhibited a significantly higher learning rate compared to the random group, along with a significantly faster rate of forgetting of the irrelevant cue (<xref ref-type="fig" rid="fig6">Figure 6D</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A–D</xref>). To construct a paired comparison, we also retrained the MT group mice in the same protocols but with random sequences after forgetting and confirmed the improvements for both the learning rate and training efficiency (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1F–G</xref>). We again employed the logistic regression model to extract the weights for each variable and plotted the learning trajectories in the latent weight space. MT algorithm manifested distinct learning path against random group in the space (<xref ref-type="fig" rid="fig6">Figure 6E</xref>); MT algorithm quickly suppressed the sensitivity of irrelevant modality (i.e. <italic>W<sub>orient</sub></italic>.), keeping low sensitivity to noise dimensions (i.e. <italic>W<sub>noise</sub></italic>) in the meantime. This resulted in a circuitous learning path in the space compared to the random group.</p><p>We then asked how MT achieved this learning strategy and what the benefit is. Compared to the random sequence, the MT algorithm effectively suppressed the mice’s reliance on irrelevant strategies by dynamically adjusting the ratio of stay/switch trials and left/right trial types (<xref ref-type="fig" rid="fig6">Figure 6F</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1E</xref>). After training, we employed the same random trial sequence to test the performance of both groups. Notably, those mice trained with the MT algorithm exhibited significantly lower left/right bias and stay/switch bias compared to the randomly trained mice (<xref ref-type="fig" rid="fig6">Figure 6G</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1H</xref>). This suggested that the MT algorithm enabled mice to exhibit more stable and task-aligned behavioral training, which implied an internal influence to psychological decision strategies after the MT conditioning.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we developed a fully autonomous behavioral training system known as HABITS, which facilitates free-moving mice to engage in 24/7 self-triggered training in their home-cage without the need for water restriction. The HABITS is equipped with a versatile hardware and software framework, empowering us to swiftly deploy a spectrum of cognitive functions, including motor planning, working memory, confidence, attention, evidence accumulation, multimodal integration, etc. Leveraging the advantages of long-term and parallel running of HABITS, we explored several challenging and novel tasks, some of which introduced new modalities or had never been previously attempted in mouse models. Notably, the acquisition of several tasks spanned more than three months to learn. Benefited from the power of machine teaching algorithms, we endowed the automated training in HABITS with an optimal training sequence which further significantly improved the training efficiency and testing robustness. Furthermore, we extended the machine teaching algorithm to a more generalized form, incorporating multi-dimensional stimuli, which has resulted in diverse training trajectories in the latent space and thus elevated training qualities. Altogether, this study presents a fully autonomous platform that offers innovative and optimal mouse behaviors that could advance the field of behavioral, cognitive, and computational neuroscience.</p><p>One of the pivotal contributions of this research is the provision of an extensive behavioral dataset, derived from ~300 mice training and testing in more than 20 diverse paradigms. This comprehensive dataset consisted of the entire learning trajectory with well-documented training strategies and behavioral outcomes. This unprecedented scale of data generated in a single study is mainly attributed to the three distinct features of the HABITS system. Firstly, the hardware was engineered to fit a broad spectrum of cognitive tasks for mice, diverging from the typical focus on specific tasks in previous studies (<xref ref-type="bibr" rid="bib3">Aoki et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Bernhard et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Bollu et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Caglayan et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Francis and Kanold, 2017</xref>; <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>; <xref ref-type="bibr" rid="bib41">Kiryk et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib53">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Poddar et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Qiao, 2019</xref><xref ref-type="bibr" rid="bib63">Qiao, 2019</xref>; <xref ref-type="bibr" rid="bib68">Salameh et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Silasi et al., 2018</xref>). It integrated both visual and auditory stimuli across three spatial locations, as well as up to three lickports for behavioral report, offering various combinations to explore. Secondly, the software within HABITS implemented a general-purpose state machine runner for step-by-step universal task learning and ran standalone without PC in the loop, which contrasted with previous system running on PCs (<xref ref-type="bibr" rid="bib3">Aoki et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Caglayan et al., 2021</xref>; <xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Francis and Kanold, 2017</xref>; <xref ref-type="bibr" rid="bib62">Poddar et al., 2013</xref>). Thirdly, the cost of the HABITS was quite low (less than $100) compared to previous systems (ranging from $250 to $1500; <xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Francis and Kanold, 2017</xref>; <xref ref-type="bibr" rid="bib63">Qiao, 2019</xref><xref ref-type="bibr" rid="bib63">Qiao, 2019</xref>; <xref ref-type="bibr" rid="bib73">Silasi et al., 2018</xref>), which facilitated large-scale deployment and thus high-throughput training and testing. Together, these unique attributes have simplified behavioral exploration, which would otherwise be a time- and labor-intensive endeavor.</p><p>Another significant contribution was the expansion of the behavioral repertoire for mice, made possible again by the autonomy of HABITS. We have introduced auditory stimuli and multiple delay epochs into the DMS paradigm (<xref ref-type="bibr" rid="bib16">Condylis et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Liu et al., 2014</xref>; <xref ref-type="bibr" rid="bib77">Taxidis et al., 2020</xref>; <xref ref-type="bibr" rid="bib84">Yu, 2021</xref><xref ref-type="bibr" rid="bib84">Yu, 2021</xref>), allowing for investigation of working memory across different modality and stages within single trials. Additionally, we have advanced the traditional d2AFC task (<xref ref-type="bibr" rid="bib26">Guo et al., 2014b</xref>; <xref ref-type="bibr" rid="bib35">Inagaki et al., 2018</xref>) to d3AFC, enabling the study of motor planning in multi-classification scenarios. Furthermore, we have implemented delayed context-dependent tasks <xref ref-type="bibr" rid="bib51">Mukherjee et al., 2021</xref>; <xref ref-type="bibr" rid="bib82">Wimmer et al., 2015</xref> based on multi-dimensional auditory stimuli, facilitating research of complex and flexible decision-making processes. Collectively, the advancement of our high-throughput platform was anticipated to improve the experimental efficiency and reproducibility, in either the creation of standardized behavioral datasets for individual paradigms or in the exploration of a multitude of complex behavioral training paradigms.</p><p>Last but not least, we have incorporated machine teaching algorithms into the mouse behavioral training process and significantly enhanced the training efficacy and quality. To our knowledge, this is the first study demonstrating the utility of machine teaching in augmenting animal behavioral training, which complements previous simulation studies <xref ref-type="bibr" rid="bib6">Bak et al., 2016</xref>. The impact of machine teaching algorithms is threefold. First, the training duration of complex tasks was substantially reduced, primarily due to the real-time optimization of trial sequences based on mouse performance, which significantly reduces the mice’s reliance on suboptimal strategies. Second, the final training outcomes were demonstrated to be less influenced by the task-irrelevant variables. A prior study has indicated that suboptimal strategies, such as biases, are common among expert mice trained in various paradigms, potentially stemming from their exploration in real-world uncertain environments (<xref ref-type="bibr" rid="bib61">Pisupati et al., 2021</xref>; <xref ref-type="bibr" rid="bib66">Rosenberg et al., 2021</xref>; <xref ref-type="bibr" rid="bib80">Wang et al., 2023</xref>; <xref ref-type="bibr" rid="bib87">Zhu and Kuchibhotla, 2024</xref>). Machine teaching-based techniques can significantly reduce the noise dependency, thus facilitating the analysis of the relationship between behavior and neural signals. Third, the machine learning algorithm lowers the barriers for designing effective anti-bias strategies, which were challenging and prone to lopsided in multidimensional tasks. By simply setting the task goals in the fitted decision model, machine teaching can automatically guide the mouse to approach the goal optimally and robustly.</p><p>Our study was designed to standardize behavior for the precise interrogation of neural mechanisms, specifically addressing within-subject questions. However, investigators are often interested in between-subject differences—such as sex differences or genetic variants—which can have long-term behavioral and cognitive implications (<xref ref-type="bibr" rid="bib43">Lassalle et al., 2008</xref>; <xref ref-type="bibr" rid="bib81">Weekes, 1994</xref>). This is particularly relevant in mouse models due to their genetic tractability (<xref ref-type="bibr" rid="bib34">Hemann et al., 2012</xref>). Although our primary focus was not on between-subject differences, the dataset we generated provides preliminary evidence for such investigations. Several behavioral readouts revealed individual variability among mice, including large disparities in learning rates across individuals (<xref ref-type="fig" rid="fig2">Figure 2I</xref>), differences in overall learning rates between male and female subjects (<xref ref-type="fig" rid="fig2">Figure 2D</xref> vs. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1G</xref>), variations in nocturnal behavioral patterns (<xref ref-type="fig" rid="fig2">Figure 2K</xref>), etc. Furthermore, a detailed logistic regression analysis dissected the strategies mice employed during training (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). Notably, the regression identified variables associated with individual task-performance strategies (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>), which also differed between manually and autonomously trained groups (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>). Thus, our system could facilitate high-throughput behavioral studies exploring between-subject differences in the future.</p><p>Our study marks the inaugural endeavor to innovate mouse behavior through autonomous setups, yet it comes with several limitations. Firstly, our experiments were confined to single-housed mice, which is known to influence murine behavior and physiology, potentially affecting social interaction and stress levels (<xref ref-type="bibr" rid="bib4">Arndt et al., 2009</xref>). In our study, individual housing was necessary to ensure precise behavioral tracking, eliminate competitive interactions during task performance, and maintain consistent training schedules without disruptions from cage-mate disturbances. However, the potential of group-housed training has been explored with technologies such as RFID <xref ref-type="bibr" rid="bib41">Kiryk et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Qiao, 2019</xref>; <xref ref-type="bibr" rid="bib68">Salameh et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Silasi et al., 2018</xref> to distinguish individual mice, which potentially improves the training efficiency and facilitates research of social behaviors (<xref ref-type="bibr" rid="bib78">Torquet et al., 2018</xref>). Notably, it has shown that simultaneous training of group-housed mice, without individual differentiation, can still achieve criterion performance (<xref ref-type="bibr" rid="bib22">Francis et al., 2019</xref>). Secondly, we have not yet analyzed any videos or neural signals from mice trained in the home-cage environment. Recent studies have harnessed a variety of technologies and methodologies to gain a deeper understanding of natural animal behavior in home-cage environments (<xref ref-type="bibr" rid="bib24">Grieco et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Jhuang et al., 2010</xref>). Voluntary head fixation, employed in previous studies, has facilitated real-time brain imaging (<xref ref-type="bibr" rid="bib65">Rich et al., 2024</xref>; <xref ref-type="bibr" rid="bib71">Scott et al., 2013</xref>; <xref ref-type="bibr" rid="bib3">Aoki et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Murphy et al., 2020</xref>; <xref ref-type="bibr" rid="bib53">Murphy et al., 2016</xref>). Future integration of commonly used tethered, wireless head-mounted (<xref ref-type="bibr" rid="bib72">Shin et al., 2022</xref>), or fully implantable devices (<xref ref-type="bibr" rid="bib58">Ouyang et al., 2023</xref>; <xref ref-type="bibr" rid="bib83">Wright et al., 2022</xref>), could allow for investigation of neural activity during the whole period in home-cage. Lastly, while HABITS achieves criterion performance in a similar or even shorter overall days compared to manual training, it requires more trials to reach the same learning criterion (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). We hypothesize that this difference in trial efficiency may stem from the constrained engagement duration imposed by the experimenter in manual training, which could compel mice to focus more intensely on task execution, resulting in less trial omissions (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). In contrast, the self-paced nature of autonomous training may permit greater variability in attentional engagement (<xref ref-type="bibr" rid="bib74">Smolen et al., 2016</xref>) and inter-trial intervals, which could be problematic for data analysis relying on consistent intervals and/or engagements. Future studies should explore how controlled contextual constraints enhance learning efficiency and whether incorporating such measures into HABITS could optimize its performance.</p><p>The large-scale autonomous training system we proposed can be readily integrated into current fundamental neuroscience research, offering novel behavioral paradigms, extensive datasets on mouse behavior and learning, and a large cohort of mice trained on specific tasks for further neural analysis. Additionally, our research provides a potential platform for testing computational models of cognitive learning, contributing to the field of computational neuroscience.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Design and implementation of HABITS</title><sec id="s4-1-1"><title>Architecture</title><p>A single HABITS was comprised of a custom home-cage and integrated behavioral apparatuses. All the building materials were listed in the <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, with source and price information provided. The home-cage was made of acrylic panels, with a dimension of 20×20 × 30 cm<sup>3</sup>. The top panel was movable and could be equipped with cameras to record mouse natural behaviors. A compatible tray was located at the bottom of the home-cage, facilitating bedding materials changing. A notch was designed in the front of the tray where an elevated platform was installed. The platform formed an arch shape to loosely constrain the mouse body when the mouse stepped on it to perform the task. A micro load cell was installed beneath the platform and used for daily body weighing.</p><p>Most of the behavioral apparatuses were installed in the front panel of the home-cage. A lickport holder with up to seven slots was installed in front of the weighting platform. Three lickports (1.5 mm diameter, 10 mm apart) were used in this study. Water was drawn by peristaltic pumps from water tanks (centrifuge tube, 50 ml) to the lickports. Three groups of LEDs and buzzers for light and sound stimuli were extruded from the front panel and placed in the left, right, and top positions around the weighting platform. Notably, the top module contained an RGB LED, but white LEDs for the others. Buzzers were the same in all stimulus modules and produced 3–15 kHz pure tones at 80 dB. In some experiments (<xref ref-type="fig" rid="fig3">Figures 3E</xref> and <xref ref-type="fig" rid="fig4">4C</xref>), the top buzzer was replaced with a micro ultrasound speaker (Elecfans Inc) which was able to emit 40 khz pure tone for up to 100 dB.</p></sec><sec id="s4-1-2"><title>Control system</title><p>The core of the control system was a microprocessor (Teensy 3.6) which interacted with all peripheral devices and coordinated the training processes (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). The microprocessor generated a PWM signal to directly control the sound and light stimuli. Reward water was dispersed by sending pulses to solid-state relays which controlled the pumps. Two toggle switches were used for flushing the tubing. Each lickports was electrically connected to a capacitive sensing circuit for lick detection. Additionally, another switch was used for manually controlling the start and pause of the training process. Real-time weight data were read from the load cell at a sampling rate of 1 Hz. A Wi-Fi module was connected with the microprocessor to transmit data wirelessly to a host computer. Meanwhile, all the data were also stored on a local SD card, with the microprocessor’s clock as the timestamps for all behavioral events.</p><p>We have developed a software framework for constructing behavioral training programs, which is a general-purpose state machine runner for training animal behaviors (gpSMART, <ext-link ext-link-type="uri" xlink:href="https://github.com/Yaoyao-Hao/gpSMART">https://github.com/Yaoyao-Hao/gpSMART</ext-link>, copy archived at <xref ref-type="bibr" rid="bib31">Hao, 2024b</xref>). This framework supported the construction of arbitrarily complex cognitive behavioral paradigms as state machines (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Basically, each state was comprised of a unique name, output actions, transition conditions, and maximum timing. Within each trial, the microprocessor generates the state matrix based on the defined state machine and executes the state transition according to the external events (e.g. licks) or timing (e.g. a delay period of 1.2 s). This is similar to the commonly used Bpod system (Sanworks Inc), but gpSMART could run on microprocessors with a hardware-level time resolution. Between trials, training protocol updating, behavioral data recording, and wireless data communication were executed. Various training assistances (e.g. free reward) were also performed when necessary to help the training processes. All the training progress and protocols were stored on the SD card for each mouse; thus, training can be resumed after any pause event and supports seamless switching between multiple HABITS systems. The system was designed to operate standalone without a PC connected. Finally, the firmware on the microprocessor could be updated wirelessly to facilitate paradigm changing.</p></sec></sec><sec id="s4-2"><title>High-throughput training and GUI</title><p>We constructed over a hundred of HABITS to facilitate large-scale, fully autonomous in-cage behavioral training (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>). Each HABITS was piled on standard mouse cage racks, with sound-proof foams installed between them to minimize cross-cage auditory interference. The cage operated independently with each other, with only a 12 V standard power supply connected. The training room was maintained under a standard 12:12 light-dark cycle. All the cages communicated with a single PC via unique IP addresses using the UDP protocol.</p><p>To monitor the training process of all the cages, a MATLAB-based GUI running on a PC was developed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). The GUI displayed essential information for each mouse, such as the paradigm name, training duration, training progress, and performance metrics like long-term task accuracy, weight changes, and daily trial numbers, etc. Meanwhile, the whole history of training performance, detailed trial outcomes in the last 24 hr and real-time body weight could be plotted. The GUI also enabled real-time updating of each cage’s training parameters for occasional manual adjusting. Training settings can also be modified by physically or remotely updating the SD card files.</p></sec><sec id="s4-3"><title>Mice and overall training procedures</title><sec id="s4-3-1"><title>Mice</title><p>All experimental data used in this study were collected from a total of 302 mice (C57BL/6 J). For most of the autonomous experiments, males were used with starting age at around 8 weeks (see <xref ref-type="table" rid="table1">Table 1</xref>). A separate group of six females was tested in a sound-frequency-based 2AFC task (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1G</xref>). Mice were single housed in our home-cage systems for ranging from 1 to more than 3 months. A group of six mice was used for supervised manual training. Another six mice were used for ad libitum reward testing in HABITS. All experiments were approved by the Laboratory Animal Welfare and Ethics Committee of Zhejiang University (Ethics Code: ZJU20210298).</p></sec></sec><sec id="s4-4"><title>Workflow for behavioral testing in HABITS</title><p>The entire workflow for fully automated behavioral training experiment in HABITS can be divided into three stages (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>). The first stage was the initialization of HABITS. This involved setting up the home cage by placing an appropriate amount of bedding, food, cotton, and enrichments into the drawer of the home cage. Behavioral paradigms and training protocols, programmed within our software framework, were then deployed on the microcontroller of HABITS. Each mouse was provided with a unique SD card that stores its specific behavioral training data, including the training paradigm, cage number, initial paradigm parameters, and progresses. The load cell was initialized through the host computer, which includes zeroing and calibration processes. The flush switch of the peristaltic pump was activated to fill the tubing with water. Finally, the mouse was placed into the HABITS after initial body weight measuring. Note that any habituations or water restrictions were not required.</p><p>The second stage was the fully autonomous training phase, during which no intervention from the experimenter was needed. Typically, this stage included three main training sub-protocols: habituation, training, and testing. During the habituation phase, free rewards are randomly given on either lickports to guide the mouse in establishing a connection between lickports and water reward. Subsequently, in the training phase, the protocols are gradually advanced, from very easy ones to the final paradigm, based on the learning performance of the mouse. Assistance, like reward at correct lickport, was gradually decreased as the mouse learned the task. Finally, predefined behavioral tests, such as psychometric curve testing, random trials, additional delays, etc. were conducted. The entire training process of all cages was remotely monitored via the GUI. The bedding in the drawer was replaced every other week to ensure that the mouse lives in a clean environment.</p><p>The third stage involved data collection and analysis. All raw data, including detailed event, trail, and configuration information, was stored on the SD card; data wirelessly transmitted to PC were mainly used for monitoring. These behavioral data were analyzed offline with Python, and the mice were ready for other subsequent testing.</p></sec><sec id="s4-5"><title>Manual training</title><p>To compare with fully autonomous training, we also used HABITS as a behavioral chamber to perform manual training protocol for freely moving mice. The mice were first single-housed in standard home cages and subjected to water restriction. After several days, when the mice’s body weight dropped to approximately 85% of their original weight (<xref ref-type="bibr" rid="bib25">Guo et al., 2014a</xref>), behavioral training began. The mice were trained in a session-based manner; in each session, experimenters transferred the mice from the standard home-cage to HABITS, where they underwent 1–3 hr of training to receive around 1 ml of water. The amount of water consumed was estimated by HABITS based on the number of rewards. HABITS weighed the mice daily, ensuring that all mice maintained stable body weight throughout the training process. The manually trained mice underwent the same training protocols as in the autonomous ones (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). Once the mice completed the final protocol and reached the criterion performance (75%), they were considered successfully trained. After completing the manual training, the mice were then transitioned into autonomous testing in HABITS (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E</xref>).</p></sec><sec id="s4-6"><title>Behavioral data analysis</title><sec id="s4-6-1"><title>Bias calculation</title><p>We calculated mice’s bias toward different trial types, for example left and right, by evaluating their performance under these trial types (perf. bias). The strength of the bias was quantified by calculating the absolute difference between the proportion of performance under specific trial type relative to the summed value across trial types, and the balance point, that is 50%. Similarly, we applied this method for presentation of trial sequence to compute the trial type bias during paradigm training, illustrating the dynamic changes in training strategies.</p></sec></sec><sec id="s4-7"><title>Data preprocessing</title><p>For the fully autonomous training, we excluded data from the habituation phase, as we believed the mice had not yet understood the structure of the trials during that stage. Additionally, we removed trials where the mice did not make a choice, that is, no-response trials. For each mouse, the trials were concatenated in chronological order, ignoring the time span between trials during the continuous multi-day home-cage training sessions; the same approach was applied to manual training data. We then organized the data for each mouse into multiple 500-trial windows, sliding from the beginning to the end of the training with a step size of 100-trial. Windows containing fewer than 500 trials at the end of the dataset were discarded. We assumed that within each window, the mouse employed a consistent strategy, and a new logistic regression model was fit in each window.</p></sec><sec id="s4-8"><title>Logistic regression of behavioral data</title><p>Similar to our previous study <xref ref-type="bibr" rid="bib29">Hao et al., 2021</xref>, we employed an offline logistic regression model to predict the choices made by the mice (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B–D</xref>, <xref ref-type="fig" rid="fig5">Figure 5I</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>). This model calculates a weighted sum of all behavioral variables and transforms the decision variable into a probability value between 0 and 1 using a sigmoid function, representing the probability of choosing the left side. The variables include the current stimulus (<italic>S<sub>0</sub></italic>; –1 for licking left trials; 1 for licking right trials), the previous stimulus (<italic>S<sub>1</sub></italic>), reward (<italic>R<sub>1</sub></italic>; –1 for no reward; 1 for reward), action (<italic>A<sub>1</sub></italic>; –1 for left choice; 1 for right choice), win-stay-loss-switch (<italic>WSLS</italic>; which is <italic>A<sub>1</sub></italic> ×<italic>R<sub>1</sub></italic>), and a constant bias term (<italic>bias</italic>). The model can be formulated by the following equation:<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle P(right)=\frac{1}{1+e^{-(z)}}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ2"><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mi>W</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle z=\beta_{s_0} S_0+\beta_{S_1} S_1+\beta_{R_1} R_1+\beta_{A_1} A_1+\beta_{W S L S} W S L S+\beta_{b i a s}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the <italic>β</italic>’s were the weights for the regressors. We used 0.5 as the decision threshold: predictions above 0.5 were classified as right choices, while below were classified as left choices.</p><p>Model performance was assessed using 10-fold cross-validation. For each cross-validation iteration, 450 trials were randomly selected as the training set, and gradient descent was employed to minimize the cross-entropy loss function. The remaining 50 trials were used as the test set. Training was considered complete (early stopping) once the calculated loss in the test set stabilized. The accuracy of the model in predicting the mouse’s choices in the test set was recorded as the result of one cross-validation iteration. This process was repeated 10 times, and the final performance of the model was averaged across all iterations.</p></sec><sec id="s4-9"><title>Significance calculation</title><p>To evaluate the contribution of each regressor, we compared the performance of a partial model, where a specific variable was removed, with that of the full model. Specifically, the value of the variable in testing was set to zero, and we checked whether the performance of the partial model showed a significant decline. We applied a corrected t-test using a 10×10 cross-validation model comparison method to compute the <italic>p</italic>-value (<xref ref-type="bibr" rid="bib23">Gardner and Brooks, 2017</xref>). For each window, we trained 100 models, and the performance differences between the partial and full models formed a <italic>t</italic>-distribution. By examining the distribution of performance differences, we determined the significance level of each regression variable’s contribution. When p&lt;0.05, the regression variable was considered to have a significant contribution to predicting the mouse’s choice in that window. Additionally, we calculated the proportion of windows across the entire training process in which a particular regression variable had a significant contribution, to estimate the degree to which the mouse relied on that variable. The same significance evaluation method was applied to both autonomous and manual training, allowing for direct comparison of the learning strategies employed in two conditions at the individual mouse level (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B–D</xref>, <xref ref-type="fig" rid="fig5">Figure 5I</xref>).</p></sec><sec id="s4-10"><title>Logistic regression in evidence accumulation: Multimodal integration</title><p>For each mouse in evidence accumulation task (<xref ref-type="fig" rid="fig3">Figure 3 D4</xref>), we trained a group of logistic regression models to estimate the psychophysical kernel. The entire sample period was divided into 25 bins of 40ms each, with each bin assigned a weight to predict the mouse’s choice. An event occurring in a bin was set to 1; otherwise, set to 0. We trained 100 pairs of models for each mouse. Each pair of models was trained using 10% total trials randomly. Each pair of models included one model trained on the original data and another trained on data with bin-wised shuffled within each trial. The psychophysical kernel for each mouse was derived by averaging the first 100 models, compared to a baseline kernel obtained from the second. Finally, we averaged the results across all 13 mice to statistically estimate the temporal dynamics of mice’s evidence dependence in this task.</p></sec><sec id="s4-11"><title>Behavioral tasks and training protocols</title><sec id="s4-11-1"><title>General training methodology</title><p>In the fully autonomous behavioral training process, all mice learn the required behavioral patterns through trials and errors. The training protocols were pre-defined based on experience. Given that the entire training process is long-term and continuous, a free reward is triggered if a mouse fails to obtain water within the last 3- or 6-hr period, ensuring the mouse receives sufficient hydration. Throughout the training process, we employed a custom-designed ‘anti-bias’ algorithm to avoid mice always licking one side. Basically, we implemented several priority-based constraints to prevent mice from developing a preference for a particular reward direction:</p><list list-type="bullet" id="list1"><list-item><p>- <italic>Highest Priority</italic>: If a mouse consecutively made three errors or no response in a specific trial type, the next trial would maintain the same reward direction.</p></list-item><list-item><p>- <italic>Second Priority</italic>: If three consecutive trials shared the same reward direction (including no-response trials), the reward direction would switch in the subsequent trial.</p></list-item><list-item><p>- <italic>Third Priority</italic>: The reward direction of the next trial was sampled based on the average performance of left and right trials, using the following formula:<disp-formula id="equ3"><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>left </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Corr</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Corr</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Corr</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Err</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Err</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Err</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  P_{\text {left }}=\frac{1}{2} \times\left(\frac{\sum_{i=1}^n R_i \operatorname{Corr}_i}{\sum_{i=1}^n R_i \operatorname{Corr}_i+\sum_{i=1}^n L_i \operatorname{Corr}_i}+\frac{\sum_{i=1}^n L_i \operatorname{Err}_i}{\sum_{i=1}^n L_i \operatorname{Err}_i+\sum_{i=1}^n R_i \operatorname{Err}_i}\right)$$\end{document}</tex-math></alternatives></disp-formula></p></list-item></list><p>where <italic>N</italic> represented the number of recorded historical trials (set to 50 in our case). <italic>Ri</italic> and <italic>Li</italic> were set to 1 if the reward direction of the <italic>i<sub>th</sub></italic> historical trial was right or left, respectively; otherwise, they were set to 0. Similarly, <italic>Corr<sub>i</sub></italic> and <italic>Err<sub>i</sub></italic> were set to 1 or –1 if the mouse’s choice in the <italic>i<sub>th</sub></italic> trial was correct or incorrect, respectively; otherwise, they were set to 0. <italic>P<sub>left</sub></italic> represented the probability of left trial type in the next trial.</p></sec></sec><sec id="s4-12"><title>d2AFC with multi-modal</title><p>The task of d2AFC, delayed two-alternative forced choice, required mice to learn the stimulus-action contingency separated by a delay for motor planning. A complete trial consisted of three parts: the sample, delay, and response epochs. The sample epoch lasted for 1.2 s and is accompanied by auditory or visual stimuli. The delay epoch elapsed for another 1.2 s, during which mice were required to withhold licking until a 100ms response cue (6 kHz tone) was played. Any premature licking (early licks) during this period immediately paused the trial for 300ms. Response epoch lasted for 1 s. The first lick made by the mouse during this period was recorded as its choice for the current trial, and feedback is provided accordingly. A correct lick delivered approximately 0.25 µl of water to the corresponding spout (achieved by activating the peristaltic pump for 30ms), while an incorrect choice results in an immediate 500ms white noise and 8000ms timeout for penalty. After each trial, the mouse must refrain from licking for 1000ms before the next trial began automatically. If the mouse failed to make a choice during the response period, the trial was marked as a no-response trial, and the mouse must lick either spout to initiate the next trial. The stimuli modalities tested in this study were as follows:</p><list list-type="bullet" id="list2"><list-item><p>- Sound frequency modality: A 3 kHz tone corresponds to a left choice, while a 10 kHz tone corresponds to a right choice.</p></list-item><list-item><p>- Sound orientation modality: A sound from the left speaker corresponds to a left choice, while a sound from the right speaker corresponds to a right choice.</p></list-item><list-item><p>- Light orientation modality: The left white LED lighting up corresponds to a left choice, and the right white LED lighting up corresponds to a right choice.</p></list-item><list-item><p>- Light color modality: The top tricolor LED lighting up blue corresponds to a left choice, and red corresponds to a right choice. For light color modality, we tested multiple variations since the mouse did not learn the task very well, including green <italic>vs.</italic> blue and flashed green <italic>vs.</italic> blue.</p><p>In the reaction time version of the paradigm (RT task, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), a central spout was introduced in addition to the left and right lickports, to allow the mouse to self-initiate a trial. The mouse must lick the central spout to initiate a trial; licking either side following would result in a brief (100ms) white noise and immediate termination of the trial, followed by a timeout period. During the sample epoch, a tone from the top speaker (3 kHz for left reward; 10 kHz for right reward) plays for 1000ms. The mouse can immediately indicate its choice by licking either side lickports, which terminated the sample period and triggered trial outcomes as above. An inter-trial interval (ITI) of 1000ms was followed. The next trial required the mouse to lick the central spout again. The central spout did not provide any rewards; all rewards are contingent upon the mouse’s choice of the left or right spouts.</p></list-item></list></sec><sec id="s4-13"><title>Training and testing of other tasks</title><p>The training and testing method for all other tasks was detailed in the text of <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>.</p></sec><sec id="s4-14"><title>Machine teaching algorithms</title><p>We employed machine teaching (MT) algorithm <xref ref-type="bibr" rid="bib45">Liu et al., 2017</xref>, to design an optimal trial sequence that enables the mice to rapidly approach a target state, i.e., trained in a specific task. In this context, the MT algorithm can be referred to as the ‘teacher’ while the mice are the ‘students’; the size of the training dataset is termed ‘teaching dimension’ of the student model <xref ref-type="bibr" rid="bib86">Zhu et al., 2018</xref>. Specifically, the teacher samples from a pre-defined discrete dataset, and the student updates its internal model using the sampled data. The teacher then prepares the next round of training based on the student’s progress, creating a closed-loop system. In this study, a logistic regression model was employed to infer the internal decision-making model of the mice based on their choices trial-by-trial (i.e. model-based). The model was updated in real-time and used for the optimization and sampling of subsequent trials. L1 regularization and momentum were introduced to smooth the fitted weights, mitigating overfitting and oscillations. The mice’s choices and outcomes served as feedback for MT. <xref ref-type="fig" rid="fig5">Figure 5A</xref> illustrated the complete closed-loop optimization process of MT algorithm. It was similar to an imitation teacher <xref ref-type="bibr" rid="bib45">Liu et al., 2017</xref> whose objective was to iteratively minimize the distance between the model weights of next trial and target.</p><p>In detail, the logistic regression model to fit the choices of mouse was updated trial-by-trial according to the following formula:<disp-formula id="equ4"><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:msup><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>sgn</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle \begin{array}{ll} m^t=(1-\eta)\left(\left\langle\omega^t, x^t\right\rangle-y_{\text {choice}}\right) x^t+\eta m^{t-1} \quad m^0=0 \\ \omega^{t+1}=\omega^t-\alpha\left(\frac{m^t}{1-\eta^t}+\lambda \operatorname{sgn}\left(\omega^t\right)\right) \end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the parameter <italic>ω<sup>t</sup></italic> represents the decision model parameters fitted to the current and past choices (i.e. <italic>y<sub>choice</sub></italic>) performed by the mice at the <italic>t</italic>-th trial. The hyperparameter <italic>λ</italic> controls the strength of L1 regularization. Momentum parameter <italic>η</italic> determines the window width for exponential smoothing of the loss gradient. <italic>m</italic> represents an exponential smoothed gradient across past trials.</p><p>Then, the objective of this algorithm can be formalized as the following equation:<disp-formula id="equ5"><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>γ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>ω</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  &amp; \left\|\omega^{t+1}-\omega^*\right\|_2^2=\left\|\omega^t-\omega^*\right\|_2^2+\gamma^2 T_1\left(x, y \mid \omega^t\right)-2 \gamma T_2\left(x, y \mid \omega^t\right) \\ &amp; T_1\left(x, y \mid \omega^t\right)=\left\|\frac{1}{1+e^{\left(y\left(\omega^t, x\right)\right)}}\right\| \\ &amp; T_2\left(x, y \mid \omega^t\right)=\left\langle\omega^t-\omega^*,\left(\left\langle\omega^t, x\right\rangle-y\right) x\right\rangle $$\end{document}</tex-math></alternatives></disp-formula></p><p>where, (<italic>x, y</italic>) represents a pair of stimuli-action contingency. The parameter <italic>ω<sup>*</sup></italic> denotes the target weight within the implicit decision space of the simulated mouse model, typically set to converge to the model weights according to the current task rules. <italic>T<sub>1</sub></italic> can be interpreted as the trial difficulty, predicting the probability of incorrect choices performed by mice in this trial, and <italic>T<sub>2</sub></italic> as the effectiveness of this trial, predicting the correlation between the upcoming mouse behavioral strategy updates and the shortest learning path between <italic>ω<sup>t</sup></italic> and <italic>ω<sup>*</sup></italic>. The balance between these two metrics is achieved through hyperparameters <italic>γ</italic>, that is hypothetical learning rate of mouse. In this study, we assume that this model reflects the actual decision-making process of the mice. Subsequently, this algorithm can select the next trial type using the following formula:<disp-formula id="equ6"><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>γ</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle (x^{t+1},y^{t+1} )=\underset{x\in X,y\in Y}{\rm arg\,min} (\gamma ^{2}T_{1}-2\gamma T_{2})$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>X</italic> and <italic>Y</italic> represent the repertoires of available trial types and action, respectively.</p><p>Finally, we presented the selected stimuli <italic>x</italic> in the next trials and expected the mouse to make a correct/incorrect choice <italic>y</italic> and receive a reward/punishment.</p><p>We have implemented the aforementioned MT-based optimization algorithm on the microprocessors of HABITS, as a superior alternative to existing ‘anti-bias’ algorithms. The computation load for running MT optimization on the microprocessors was high, and latency was relatively long compared to trial resolution. However, since the computation was conducted between trials, it did not interfere with the execution of the trials themselves under the gpSMART framework. Additionally, for each mouse, additional files were used to record the hyperparameters and weight changes of the online logistic regression model for each trial. No-response trials were not used for model fitting. However, if the previous trial was a no-response trial, the history-dependence regressors of the current trial were set to 0.</p></sec><sec id="s4-15"><title>Simulation experiments</title><p>We employed a logistic regression model as the student, tasked with completing a 2AFC task, which involved mapping the current stimulus <italic>S<sub>0</sub></italic> to a choice while ignoring interference from other features. Another logistic regression model, serving as the imitation teacher, was then used to fit the choices made by the student. Both models operated within the same feature space and utilized the same update algorithm. The hyperparameters were set as follows: <italic>α</italic>=0.1, <italic>η</italic>=0.9, <italic>γ</italic>=1, and λ=0.1.</p><p>We first simulated the biases and history dependence typically observed in naive mice during the early stages of training by setting the initial values of <italic>S<sub>1</sub></italic> and <italic>bias</italic> to –2 and 2, respectively. During the trial-by-trial update process, we tracked the changes in the student’s weights under the MT algorithm and compared them to those under random training, which served as the baseline. Additionally, we simulated conditions without noise to further examine the differences between the MT algorithm and random training in influencing the student’s weight updates.</p><sec id="s4-15-1"><title>Animal experiments 1: Working memory task with full SGM</title><p>The first task we tested with the MT algorithm was working memory task with full 5×5 stimulus matrix (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The delay duration was set to 500ms. Mice were randomly assigned to two groups: random (N=8) and MT (N=7). Once the mice achieved a 75% correct response rate in the eight most challenging trial types, they advanced to the testing phase. At this stage, only the eight most challenging trial types were randomly presented. When the correct rate reached 75%, the mice were considered to have learned the task. MT used seven features as the inputs for the logistic regression model: <italic>bias</italic>, <italic>S<sub>0</sub></italic> (the frequency of the first stimulus, where {8, 16, 32, 64, 128} Hz corresponds to values of {−1,–0.5, 0, 0.5, 1}), <italic>T<sub>0</sub></italic> (the frequency of the second stimulus), <italic>S<sub>1</sub></italic>, <italic>A<sub>1</sub></italic>, <italic>R<sub>1</sub></italic>, and <italic>WSLS</italic>. Hyperparameters were set as follows: <italic>α</italic>=0.01, <italic>η</italic>=0, <italic>γ</italic>=0.03, and λ=0.</p></sec><sec id="s4-15-2"><title>Animal experiments 2: 2AFC task</title><p>In 2AFC task (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), mice were divided into three groups, each using different methods to generate the stimulus frequency (3 kHz or 10 kHz) for the next trial: random selection (N=10), the anti-bias strategy (N=10), and the MT algorithm (N=10). The online logistic regression model settings for this task remained consistent with those in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The hyperparameters for the MT algorithm were set as follows: <italic>α</italic>=0.1, <italic>η</italic>=0.9, <italic>γ</italic>=1, and λ=0.1.</p></sec><sec id="s4-15-3"><title>Animal experiments 3: dynamic 2AFC task with multi-dimensional stimuli</title><p>In the third task, we expanded the stimulus dimension of 2AFC with the combination of sound frequency (high and low) and orientation (left and right), to test MT algorithm (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). In each trial, the stimulus could be one of the four combinations, but whether the mouse should attend to frequency or orientation modality was not informed. In other words, no modality cues were presented for mice, and mice must rely solely on feedback from past trials to identify what the current modality was. Mice were divided into random (N=8) and MT groups (N=10). In the MT group, both the stimulus combinations and reward direction are determined by the MT algorithm simultaneously. The entire task involved learning three different rules one by one, including sound orientation, sound frequency, and reversed sound frequency. When the mice in each group achieved an accuracy of 80% in the first 500 choice trials under the current rule, they advanced to a testing protocol consisting of at least 100 random trials. When a mouse achieved 80% accuracy in the last 100 trials of the testing protocol, it transitioned to the next rule (<xref ref-type="bibr" rid="bib10">Bernklau et al., 2024</xref>). Hyperparameters were set as follows: <italic>α</italic>=0.1, <italic>η</italic>=0.9, <italic>γ</italic>=1, and λ=0.1.</p></sec></sec><sec id="s4-16"><title>Statistics</title><p>To maximize the utility of HABITS in a wider range of paradigms, we usually employed six mice per paradigm. For experiments where we aimed to conduct between-group comparisons, we increased the sample size to 10 to ensure the stability and reliability of statistical significance. All significance tests were conducted by comparing different groups of animals (e.g. comparing performance levels across different mouse groups). Non-parametric tests, such as the Wilcoxon signed-rank test or rank-sum test, were used for comparisons between two groups, and the Kruskal–Wallis test was used for comparisons among three groups, unless otherwise stated in the figure legends. Data are presented as mean ± 0.95 confidence intervals (CI) across animals, as specified in the figure legends. In the box plots, the center lines represent the median values, the box limits indicate the upper and lower quartiles, the whiskers show 1.5 times the interquartile range, and the points represent outliers. Significance levels are denoted as *, p&lt;0.05, **, p&lt;0.01, and ***, p&lt;0.001 in all figures. All data analyses were performed using Python (version 3.8.13) with the following packages: NumPy (1.21.5), SciPy (1.10.1), matplotlib-inline (0.1.3), pandas (1.3.4), PyTorch (1.11.0), and seaborn (0.13.0).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Supervision, Funding acquisition, Project administration</p></fn><fn fn-type="con" id="con5"><p>Supervision, Funding acquisition, Methodology, Project administration</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the Laboratory Animal Welfare and Ethics Committee of Zhejiang University (Ethics Code: ZJU20210298).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-104833-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>List of building materials and prices of HABITS.</title></caption><media xlink:href="elife-104833-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>The implementation details of tasks tested in HABITS.</title></caption><media xlink:href="elife-104833-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The guidance for construction of HABITS, all the training programs, and example behavioral data are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/Yaoyao-Hao/HABITS">https://github.com/Yaoyao-Hao/HABITS</ext-link>, copy archived at <xref ref-type="bibr" rid="bib30">Hao, 2024a</xref>). The general-purpose state machine runner for training animal behaviors (gpSMART) is also available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/Yaoyao-Hao/gpSMART">https://github.com/Yaoyao-Hao/gpSMART</ext-link>, copy archived at <xref ref-type="bibr" rid="bib31">Hao, 2024b</xref>). All data in the main text or the supplementary materials are available on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.27192897">figshare</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Bowen</surname><given-names>Y</given-names></name><name><surname>Penghai</surname><given-names>L</given-names></name><name><surname>Haoze</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Hao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Behavioral raw data recorded by HABITS autonomously</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.27192897.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by STI 2030—Major Projects (2021ZD0200405), National Natural Science Foundation of China (62336007), Pioneer R&amp;D Program of Zhejiang (2024C03001), the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SN-ZJU-SIAS-002), and the Fundamental Research Funds for the Central Universities (2023ZFJH01-01, 2024ZFJH01-01).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguillon-Rodriguez</surname><given-names>V</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bayer</surname><given-names>H</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Cazettes</surname><given-names>F</given-names></name><name><surname>Chapuis</surname><given-names>G</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Dewitt</surname><given-names>E</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Forrest</surname><given-names>H</given-names></name><name><surname>Haetzel</surname><given-names>L</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Krasniak</surname><given-names>C</given-names></name><name><surname>Laranjeira</surname><given-names>I</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Meijer</surname><given-names>G</given-names></name><name><surname>Miska</surname><given-names>NJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Murakami</surname><given-names>M</given-names></name><name><surname>Noel</surname><given-names>J-P</given-names></name><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sanders</surname><given-names>J</given-names></name><name><surname>Socha</surname><given-names>K</given-names></name><name><surname>Terry</surname><given-names>R</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Vergara</surname><given-names>H</given-names></name><name><surname>Wells</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>CJ</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Wool</surname><given-names>LE</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2021">2021</year><article-title>Standardized and reproducible measurement of decision-making in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e63711</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63711</pub-id><pub-id pub-id-type="pmid">34011433</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title><source>Nature</source><volume>554</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nature25510</pub-id><pub-id pub-id-type="pmid">29414944</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aoki</surname><given-names>R</given-names></name><name><surname>Tsubota</surname><given-names>T</given-names></name><name><surname>Goya</surname><given-names>Y</given-names></name><name><surname>Benucci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An automated platform for high-throughput mouse behavior and physiology with voluntary head-fixation</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1196</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01371-0</pub-id><pub-id pub-id-type="pmid">29084948</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arndt</surname><given-names>SS</given-names></name><name><surname>Laarakker</surname><given-names>MC</given-names></name><name><surname>van Lith</surname><given-names>HA</given-names></name><name><surname>van der Staay</surname><given-names>FJ</given-names></name><name><surname>Gieling</surname><given-names>E</given-names></name><name><surname>Salomons</surname><given-names>AR</given-names></name><name><surname>van’t Klooster</surname><given-names>J</given-names></name><name><surname>Ohl</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Individual housing of mice--impact on behaviour and stress responses</article-title><source>Physiology &amp; Behavior</source><volume>97</volume><fpage>385</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2009.03.008</pub-id><pub-id pub-id-type="pmid">19303031</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asinof</surname><given-names>SK</given-names></name><name><surname>Paine</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The 5-choice serial reaction time task: a task of attention and impulse control for rodents</article-title><source>Journal of Visualized Experiments</source><volume>01</volume><elocation-id>e51574</elocation-id><pub-id pub-id-type="doi">10.3791/51574</pub-id><pub-id pub-id-type="pmid">25146934</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bak</surname><given-names>JH</given-names></name><name><surname>Choi</surname><given-names>JY</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Witten</surname><given-names>I</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adaptive optimal training of animal behavior</article-title><conf-name>Advances in Neural Information Processing Systems 29</conf-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balcombe</surname><given-names>JP</given-names></name><name><surname>Barnard</surname><given-names>ND</given-names></name><name><surname>Sandusky</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Laboratory routines cause animal stress</article-title><source>Journal of the American Association for Laboratory Animal Science: JAALAS</source><volume>43</volume><fpage>42</fpage><lpage>51</lpage><pub-id pub-id-type="pmid">15669134</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>B</given-names></name><name><surname>Benson</surname><given-names>J</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Catarino</surname><given-names>JA</given-names></name><name><surname>Chapuis</surname><given-names>GA</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>DeWitt</surname><given-names>EE</given-names></name><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Fabbri</surname><given-names>M</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Findling</surname><given-names>C</given-names></name><name><surname>Freitas-Silva</surname><given-names>L</given-names></name><name><surname>Gercek</surname><given-names>B</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Hausser</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Hubert</surname><given-names>F</given-names></name><name><surname>Huntenburg</surname><given-names>JM</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Krasniak</surname><given-names>C</given-names></name><name><surname>Langdon</surname><given-names>C</given-names></name><name><surname>Lau</surname><given-names>PYP</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Meijer</surname><given-names>GT</given-names></name><name><surname>Miska</surname><given-names>NJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Noel</surname><given-names>J-P</given-names></name><name><surname>Nylund</surname><given-names>K</given-names></name><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Roth</surname><given-names>N</given-names></name><name><surname>Schaeffer</surname><given-names>R</given-names></name><name><surname>Schartner</surname><given-names>M</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Socha</surname><given-names>KZ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>West</surname><given-names>SJ</given-names></name><name><surname>Whiteway</surname><given-names>MR</given-names></name><name><surname>Winter</surname><given-names>O</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Gercek</surname><given-names>B</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Bruijns</surname><given-names>SA</given-names></name><name><surname>Davatolhagh</surname><given-names>F</given-names></name><collab>International Brain Lab</collab></person-group><year iso-8601-date="2023">2023</year><article-title>A brain-wide map of neural activity during complex behaviour</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.04.547681</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernhard</surname><given-names>SM</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Erskine</surname><given-names>A</given-names></name><name><surname>Hires</surname><given-names>SA</given-names></name><name><surname>Barth</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An automated homecage system for multiwhisker detection and discrimination learning in mice</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0232916</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0232916</pub-id><pub-id pub-id-type="pmid">33264281</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernklau</surname><given-names>TW</given-names></name><name><surname>Righetti</surname><given-names>B</given-names></name><name><surname>Mehrke</surname><given-names>LS</given-names></name><name><surname>Jacob</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Striatal dopamine signals reflect perceived cue-action-outcome associations in mice</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>747</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01567-2</pub-id><pub-id pub-id-type="pmid">38291283</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birtalan</surname><given-names>E</given-names></name><name><surname>Bánhidi</surname><given-names>A</given-names></name><name><surname>Sanders</surname><given-names>JI</given-names></name><name><surname>Balázsfi</surname><given-names>D</given-names></name><name><surname>Hangya</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient training of mice on the 5-choice serial reaction time task in an automated rodent training system</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>22362</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-79290-2</pub-id><pub-id pub-id-type="pmid">33349672</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bollu</surname><given-names>T</given-names></name><name><surname>Whitehead</surname><given-names>SC</given-names></name><name><surname>Prasad</surname><given-names>N</given-names></name><name><surname>Walker</surname><given-names>J</given-names></name><name><surname>Shyamkumar</surname><given-names>N</given-names></name><name><surname>Subramaniam</surname><given-names>R</given-names></name><name><surname>Kardon</surname><given-names>B</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automated home cage training of mice in a hold-still center-out reach task</article-title><source>Journal of Neurophysiology</source><volume>121</volume><fpage>500</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1152/jn.00667.2018</pub-id><pub-id pub-id-type="pmid">30540551</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title><source>Science</source><volume>340</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1126/science.1233912</pub-id><pub-id pub-id-type="pmid">23559254</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caglayan</surname><given-names>A</given-names></name><name><surname>Stumpenhorst</surname><given-names>K</given-names></name><name><surname>Winter</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning set formation and reversal learning in mice during high-throughput home-cage-based olfactory discrimination</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>15</volume><elocation-id>684936</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2021.684936</pub-id><pub-id pub-id-type="pmid">34177482</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>From circuits to behavior: a bridge too far?</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>507</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1038/nn.3043</pub-id><pub-id pub-id-type="pmid">22449960</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Condylis</surname><given-names>C</given-names></name><name><surname>Lowet</surname><given-names>E</given-names></name><name><surname>Ni</surname><given-names>J</given-names></name><name><surname>Bistrong</surname><given-names>K</given-names></name><name><surname>Ouellette</surname><given-names>T</given-names></name><name><surname>Josephs</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Context-dependent sensory processing across primary and secondary somatosensory cortex</article-title><source>Neuron</source><volume>106</volume><fpage>515</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.02.004</pub-id><pub-id pub-id-type="pmid">32164873</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Do</surname><given-names>J</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Automating licking bias correction in a two-choice delayed match-to-sample task to accelerate learning</article-title><source>Scientific Reports</source><volume>13</volume><elocation-id>22768</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-49862-z</pub-id><pub-id pub-id-type="pmid">38123637</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct effects of prefrontal and parietal cortex inactivations on an accumulation of evidence task in the rat</article-title><source>eLife</source><volume>4</volume><elocation-id>e05457</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05457</pub-id><pub-id pub-id-type="pmid">25869470</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fassihi</surname><given-names>A</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Esmaeili</surname><given-names>V</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Tactile perception and working memory in rats and humans</article-title><source>PNAS</source><volume>111</volume><fpage>2331</fpage><lpage>2336</lpage><pub-id pub-id-type="doi">10.1073/pnas.1315171111</pub-id><pub-id pub-id-type="pmid">24449850</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Findling</surname><given-names>C</given-names></name><name><surname>Hubert</surname><given-names>F</given-names></name><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Benson</surname><given-names>B</given-names></name><name><surname>Benson</surname><given-names>J</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Bruijns</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Catarino</surname><given-names>JA</given-names></name><name><surname>Chapuis</surname><given-names>GA</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Davatolhagh</surname><given-names>F</given-names></name><name><surname>DeWitt</surname><given-names>EE</given-names></name><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Fabbri</surname><given-names>M</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Freitas-Silva</surname><given-names>L</given-names></name><name><surname>Gerçek</surname><given-names>B</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Huntenburg</surname><given-names>JM</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Krasniak</surname><given-names>C</given-names></name><name><surname>Langdon</surname><given-names>C</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Lau</surname><given-names>PYP</given-names></name><name><surname>Mainen</surname><given-names>Z</given-names></name><name><surname>Meijer</surname><given-names>GT</given-names></name><name><surname>Miska</surname><given-names>NJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Noel</surname><given-names>J-P</given-names></name><name><surname>Nylund</surname><given-names>K</given-names></name><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Pillow</surname><given-names>J</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Roth</surname><given-names>N</given-names></name><name><surname>Schaeffer</surname><given-names>R</given-names></name><name><surname>Schartner</surname><given-names>M</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Socha</surname><given-names>KZ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Tessereau</surname><given-names>C</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>West</surname><given-names>SJ</given-names></name><name><surname>Whiteway</surname><given-names>MR</given-names></name><name><surname>Winter</surname><given-names>O</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Zador</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Brain-wide representations of prior information in mouse decision-making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.04.547684</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>NA</given-names></name><name><surname>Kanold</surname><given-names>PO</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automated operant conditioning in the mouse home cage</article-title><source>Frontiers in Neural Circuits</source><volume>11</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2017.00010</pub-id><pub-id pub-id-type="pmid">28298887</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>NA</given-names></name><name><surname>Bohlke</surname><given-names>K</given-names></name><name><surname>Kanold</surname><given-names>PO</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automated behavioral experiments in mice reveal periodic cycles of task engagement within circadian rhythms</article-title><source>eNeuro</source><volume>6</volume><elocation-id>ENEURO.0121-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0121-19.2019</pub-id><pub-id pub-id-type="pmid">31488550</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>J</given-names></name><name><surname>Brooks</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Statistical Approaches to the Model Comparison Task in Learning Analytics</article-title><conf-name>Joint Proceedings of the Workshop on Methodology in Learning Analytics (MLA) and the Workshop on Building the Learning Analytics Curriculum (BLAC) Co-Located with 7th International Learning Analytics and Knowledge Conference (LAK 2017)</conf-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieco</surname><given-names>F</given-names></name><name><surname>Bernstein</surname><given-names>BJ</given-names></name><name><surname>Biemans</surname><given-names>B</given-names></name><name><surname>Bikovski</surname><given-names>L</given-names></name><name><surname>Burnett</surname><given-names>CJ</given-names></name><name><surname>Cushman</surname><given-names>JD</given-names></name><name><surname>van Dam</surname><given-names>EA</given-names></name><name><surname>Fry</surname><given-names>SA</given-names></name><name><surname>Richmond-Hacham</surname><given-names>B</given-names></name><name><surname>Homberg</surname><given-names>JR</given-names></name><name><surname>Kas</surname><given-names>MJH</given-names></name><name><surname>Kessels</surname><given-names>HW</given-names></name><name><surname>Koopmans</surname><given-names>B</given-names></name><name><surname>Krashes</surname><given-names>MJ</given-names></name><name><surname>Krishnan</surname><given-names>V</given-names></name><name><surname>Logan</surname><given-names>S</given-names></name><name><surname>Loos</surname><given-names>M</given-names></name><name><surname>McCann</surname><given-names>KE</given-names></name><name><surname>Parduzi</surname><given-names>Q</given-names></name><name><surname>Pick</surname><given-names>CG</given-names></name><name><surname>Prevot</surname><given-names>TD</given-names></name><name><surname>Riedel</surname><given-names>G</given-names></name><name><surname>Robinson</surname><given-names>L</given-names></name><name><surname>Sadighi</surname><given-names>M</given-names></name><name><surname>Smit</surname><given-names>AB</given-names></name><name><surname>Sonntag</surname><given-names>W</given-names></name><name><surname>Roelofs</surname><given-names>RF</given-names></name><name><surname>Tegelenbosch</surname><given-names>RAJ</given-names></name><name><surname>Noldus</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Measuring behavior in the home cage: study design, applications, challenges, and perspectives</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>15</volume><elocation-id>735387</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2021.735387</pub-id><pub-id pub-id-type="pmid">34630052</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Hires</surname><given-names>SA</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Bonardi</surname><given-names>C</given-names></name><name><surname>Morandell</surname><given-names>K</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Peron</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>N</given-names></name><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Procedures for behavioral experiments in head-fixed mice</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e88678</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0088678</pub-id><pub-id pub-id-type="pmid">24520413</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><volume>81</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id><pub-id pub-id-type="pmid">24361077</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>CT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-throughput automatic training system for odor-based learned behaviors in head-fixed mice</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00015</pub-id><pub-id pub-id-type="pmid">29487506</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct relationships of parietal and prefrontal cortices to evidence accumulation</article-title><source>Nature</source><volume>520</volume><fpage>220</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1038/nature14066</pub-id><pub-id pub-id-type="pmid">25600270</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>Y</given-names></name><name><surname>Thomas</surname><given-names>AM</given-names></name><name><surname>Li</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fully autonomous mouse behavioral and optogenetic experiments in home-cage</article-title><source>eLife</source><volume>10</volume><elocation-id>e66112</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66112</pub-id><pub-id pub-id-type="pmid">33944781</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024a</year><data-title>HABITS</data-title><version designator="swh:1:rev:8778c33e1e5f1a59fa461fbaa86ed7b8d0e15afb">swh:1:rev:8778c33e1e5f1a59fa461fbaa86ed7b8d0e15afb</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b3d0bf6c416f798975685924630804f228fd808a;origin=https://github.com/Yaoyao-Hao/HABITS;visit=swh:1:snp:b23195eee66697990ad9fc47401f41b25c92d1c8;anchor=swh:1:rev:8778c33e1e5f1a59fa461fbaa86ed7b8d0e15afb">https://archive.softwareheritage.org/swh:1:dir:b3d0bf6c416f798975685924630804f228fd808a;origin=https://github.com/Yaoyao-Hao/HABITS;visit=swh:1:snp:b23195eee66697990ad9fc47401f41b25c92d1c8;anchor=swh:1:rev:8778c33e1e5f1a59fa461fbaa86ed7b8d0e15afb</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2024">2024b</year><data-title>GpSMART</data-title><version designator="swh:1:rev:abe8893436e3d20b472bc605ab07d4b311abc83e">swh:1:rev:abe8893436e3d20b472bc605ab07d4b311abc83e</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:6f8651767d170d02c3df8442c6c287dbce80067c;origin=https://github.com/Yaoyao-Hao/gpSMART;visit=swh:1:snp:45e04cd06b5fadf4206485aa21ec09d027be524f;anchor=swh:1:rev:abe8893436e3d20b472bc605ab07d4b311abc83e">https://archive.softwareheritage.org/swh:1:dir:6f8651767d170d02c3df8442c6c287dbce80067c;origin=https://github.com/Yaoyao-Hao/gpSMART;visit=swh:1:snp:45e04cd06b5fadf4206485aa21ec09d027be524f;anchor=swh:1:rev:abe8893436e3d20b472bc605ab07d4b311abc83e</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattori</surname><given-names>R</given-names></name><name><surname>Danskin</surname><given-names>B</given-names></name><name><surname>Babic</surname><given-names>Z</given-names></name><name><surname>Mlynaryk</surname><given-names>N</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Area-specificity and plasticity of history-dependent value coding during learning</article-title><source>Cell</source><volume>177</volume><fpage>1858</fpage><lpage>1872</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.04.027</pub-id><pub-id pub-id-type="pmid">31080067</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattori</surname><given-names>R</given-names></name><name><surname>Hedrick</surname><given-names>NG</given-names></name><name><surname>Jain</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>You</surname><given-names>H</given-names></name><name><surname>Hattori</surname><given-names>M</given-names></name><name><surname>Choi</surname><given-names>J-H</given-names></name><name><surname>Lim</surname><given-names>BK</given-names></name><name><surname>Yasuda</surname><given-names>R</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Meta-reinforcement learning via orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>2182</fpage><lpage>2191</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01485-3</pub-id><pub-id pub-id-type="pmid">37957318</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hemann</surname><given-names>MT</given-names></name><name><surname>Green</surname><given-names>JE</given-names></name><name><surname>Ried</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>The development and use of genetically tractable preclinical mouse models</chapter-title><person-group person-group-type="editor"><name><surname>Green</surname><given-names>JE</given-names></name><name><surname>Ried</surname><given-names>T</given-names></name></person-group><source>Genetically Engineered Mice for Cancer Research: Design, Analysis, Pathways, Validation and Pre-Clinical Testing</source><publisher-name>Springer</publisher-name><fpage>477</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-69805-2_23</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inagaki</surname><given-names>HK</given-names></name><name><surname>Inagaki</surname><given-names>M</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Low-dimensional and monotonic preparatory activity in mouse anterior lateral motor cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>4163</fpage><lpage>4185</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3152-17.2018</pub-id><pub-id pub-id-type="pmid">29593054</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izquierdo</surname><given-names>A</given-names></name><name><surname>Brigman</surname><given-names>JL</given-names></name><name><surname>Radke</surname><given-names>AK</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural basis of reversal learning: an updated perspective</article-title><source>Neuroscience</source><volume>345</volume><fpage>12</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2016.03.021</pub-id><pub-id pub-id-type="pmid">26979052</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jankowski</surname><given-names>MM</given-names></name><name><surname>Polterovich</surname><given-names>A</given-names></name><name><surname>Kazakov</surname><given-names>A</given-names></name><name><surname>Niediek</surname><given-names>J</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>An automated, low-latency environment for studying the neural basis of behavior in freely moving rats</article-title><source>BMC Biology</source><volume>21</volume><elocation-id>172</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-023-01660-9</pub-id><pub-id pub-id-type="pmid">37568111</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>A</given-names></name><name><surname>Ashwood</surname><given-names>ZC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Active learning for discrete latent variable models</article-title><source>Neural Computation</source><volume>36</volume><fpage>437</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01646</pub-id><pub-id pub-id-type="pmid">38363661</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jhuang</surname><given-names>H</given-names></name><name><surname>Garrote</surname><given-names>E</given-names></name><name><surname>Mutch</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Khilnani</surname><given-names>V</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Steele</surname><given-names>AD</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automated home-cage behavioural phenotyping of mice</article-title><source>Nature Communications</source><volume>1</volume><elocation-id>68</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms1064</pub-id><pub-id pub-id-type="pmid">20842193</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Zariwala</surname><given-names>HA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates, computation and behavioural impact of decision confidence</article-title><source>Nature</source><volume>455</volume><fpage>227</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nature07200</pub-id><pub-id pub-id-type="pmid">18690210</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiryk</surname><given-names>A</given-names></name><name><surname>Janusz</surname><given-names>A</given-names></name><name><surname>Zglinicki</surname><given-names>B</given-names></name><name><surname>Turkes</surname><given-names>E</given-names></name><name><surname>Knapska</surname><given-names>E</given-names></name><name><surname>Konopka</surname><given-names>W</given-names></name><name><surname>Lipp</surname><given-names>HP</given-names></name><name><surname>Kaczmarek</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>IntelliCage as a tool for measuring mouse behavior - 20 years perspective</article-title><source>Behavioural Brain Research</source><volume>388</volume><elocation-id>112620</elocation-id><pub-id pub-id-type="doi">10.1016/j.bbr.2020.112620</pub-id><pub-id pub-id-type="pmid">32302617</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lassalle</surname><given-names>JM</given-names></name><name><surname>Halley</surname><given-names>H</given-names></name><name><surname>Daumas</surname><given-names>S</given-names></name><name><surname>Verret</surname><given-names>L</given-names></name><name><surname>Francés</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of the genetic background on cognitive performances of TG2576 mice</article-title><source>Behavioural Brain Research</source><volume>191</volume><fpage>104</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2008.03.017</pub-id><pub-id pub-id-type="pmid">18433892</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Gu</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Han</surname><given-names>Z</given-names></name><name><surname>Yan</surname><given-names>W</given-names></name><name><surname>Cheng</surname><given-names>Q</given-names></name><name><surname>Hao</surname><given-names>J</given-names></name><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Hou</surname><given-names>R</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>CT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Medial prefrontal activity during delay period contributes to learning of a working memory task</article-title><source>Science</source><volume>346</volume><fpage>458</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1126/science.1256573</pub-id><pub-id pub-id-type="pmid">25342800</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Dai</surname><given-names>B</given-names></name><name><surname>Humayun</surname><given-names>A</given-names></name><name><surname>Tay</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Iterative machine teaching</article-title><conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mah</surname><given-names>A</given-names></name><name><surname>Schiereck</surname><given-names>SS</given-names></name><name><surname>Bossio</surname><given-names>V</given-names></name><name><surname>Constantinople</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Distinct value computations support rapid sequential decisions</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>7573</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-43250-x</pub-id><pub-id pub-id-type="pmid">37989741</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masís</surname><given-names>J</given-names></name><name><surname>Chapman</surname><given-names>T</given-names></name><name><surname>Rhee</surname><given-names>JY</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>Saxe</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Strategically managing learning during perceptual decision making</article-title><source>eLife</source><volume>12</volume><elocation-id>e64978</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.64978</pub-id><pub-id pub-id-type="pmid">36786427</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masset</surname><given-names>P</given-names></name><name><surname>Ott</surname><given-names>T</given-names></name><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Hirokawa</surname><given-names>J</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Behavior- and modality-general representation of confidence in Orbitofrontal cortex</article-title><source>Cell</source><volume>182</volume><fpage>112</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.05.022</pub-id><pub-id pub-id-type="pmid">32504542</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname><given-names>GT</given-names></name><name><surname>Pie</surname><given-names>JL</given-names></name><name><surname>Dolman</surname><given-names>TL</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name><name><surname>Lansink</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Audiovisual integration enhances stimulus detection performance in mice</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>12</volume><elocation-id>e231</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00231</pub-id><pub-id pub-id-type="pmid">30337861</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mohammadi</surname><given-names>Z</given-names></name><name><surname>Ashwood</surname><given-names>ZC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2024">2024</year><article-title>Identifying the factors governing internal state switches during nonstationary sensory decision-making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.02.02.578482</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukherjee</surname><given-names>A</given-names></name><name><surname>Lam</surname><given-names>NH</given-names></name><name><surname>Wimmer</surname><given-names>RD</given-names></name><name><surname>Halassa</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Thalamic circuits for independent control of prefrontal signal and noise</article-title><source>Nature</source><volume>600</volume><fpage>100</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04056-3</pub-id><pub-id pub-id-type="pmid">34614503</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munoz</surname><given-names>DP</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Look away: the anti-saccade task and the voluntary control of eye movement</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>218</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1038/nrn1345</pub-id><pub-id pub-id-type="pmid">14976521</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>TH</given-names></name><name><surname>Boyd</surname><given-names>JD</given-names></name><name><surname>Bolaños</surname><given-names>F</given-names></name><name><surname>Vanni</surname><given-names>MP</given-names></name><name><surname>Silasi</surname><given-names>G</given-names></name><name><surname>Haupt</surname><given-names>D</given-names></name><name><surname>LeDue</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>High-throughput automated home-cage mesoscopic functional imaging of mouse cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>e11611</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11611</pub-id><pub-id pub-id-type="pmid">27291514</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>TH</given-names></name><name><surname>Michelson</surname><given-names>NJ</given-names></name><name><surname>Boyd</surname><given-names>JD</given-names></name><name><surname>Fong</surname><given-names>T</given-names></name><name><surname>Bolanos</surname><given-names>LA</given-names></name><name><surname>Bierbrauer</surname><given-names>D</given-names></name><name><surname>Siu</surname><given-names>T</given-names></name><name><surname>Balbi</surname><given-names>M</given-names></name><name><surname>Bolanos</surname><given-names>F</given-names></name><name><surname>Vanni</surname><given-names>M</given-names></name><name><surname>LeDue</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automated task training and longitudinal monitoring of mouse mesoscale cortical circuits using home cages</article-title><source>eLife</source><volume>9</volume><elocation-id>e55964</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55964</pub-id><pub-id pub-id-type="pmid">32412409</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Schmid</surname><given-names>AC</given-names></name><name><surname>Kaplan</surname><given-names>SM</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Centering cognitive neuroscience on task demands and generalization</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>1656</fpage><lpage>1667</lpage><pub-id pub-id-type="doi">10.1038/s41593-024-01711-6</pub-id><pub-id pub-id-type="pmid">39075326</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The primacy of behavioral research for understanding the brain</article-title><source>Behavioral Neuroscience</source><volume>135</volume><fpage>601</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1037/bne0000471</pub-id><pub-id pub-id-type="pmid">34096743</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odoemene</surname><given-names>O</given-names></name><name><surname>Pisupati</surname><given-names>S</given-names></name><name><surname>Nguyen</surname><given-names>H</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual evidence accumulation guides decision-making in unrestrained mice</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>10143</fpage><lpage>10155</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3478-17.2018</pub-id><pub-id pub-id-type="pmid">30322902</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>W</given-names></name><name><surname>Lu</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>JU</given-names></name><name><surname>Shen</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Luan</surname><given-names>H</given-names></name><name><surname>Kilner</surname><given-names>K</given-names></name><name><surname>Lee</surname><given-names>SP</given-names></name><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Wegener</surname><given-names>AJ</given-names></name><name><surname>Moreno</surname><given-names>JA</given-names></name><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Won</surname><given-names>SM</given-names></name><name><surname>Kwon</surname><given-names>K</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Bai</surname><given-names>W</given-names></name><name><surname>Guo</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>T-L</given-names></name><name><surname>Bai</surname><given-names>H</given-names></name><name><surname>Monti</surname><given-names>G</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Madhvapathy</surname><given-names>SR</given-names></name><name><surname>Trueb</surname><given-names>J</given-names></name><name><surname>Stanslaski</surname><given-names>M</given-names></name><name><surname>Higbee-Dempsey</surname><given-names>EM</given-names></name><name><surname>Stepien</surname><given-names>I</given-names></name><name><surname>Ghoreishi-Haack</surname><given-names>N</given-names></name><name><surname>Haney</surname><given-names>CR</given-names></name><name><surname>Kim</surname><given-names>T-I</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Ghaffari</surname><given-names>R</given-names></name><name><surname>Banks</surname><given-names>AR</given-names></name><name><surname>Jhou</surname><given-names>TC</given-names></name><name><surname>Good</surname><given-names>CH</given-names></name><name><surname>Rogers</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A wireless and battery-less implant for multimodal closed-loop neuromodulation in small animals</article-title><source>Nature Biomedical Engineering</source><volume>7</volume><fpage>1252</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1038/s41551-023-01029-x</pub-id><pub-id pub-id-type="pmid">37106153</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Sanchez Araujo</surname><given-names>Y</given-names></name><name><surname>McMannon</surname><given-names>B</given-names></name><name><surname>Louka</surname><given-names>M</given-names></name><name><surname>Bandi</surname><given-names>A</given-names></name><name><surname>Haetzel</surname><given-names>L</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2024">2024</year><article-title>Pre-existing visual responses in a projection-defined dopamine population explain individual learning trajectories</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.02.26.582199</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piantadosi</surname><given-names>PT</given-names></name><name><surname>Lieberman</surname><given-names>AG</given-names></name><name><surname>Pickens</surname><given-names>CL</given-names></name><name><surname>Bergstrom</surname><given-names>HC</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A novel multichoice touchscreen paradigm for assessing cognitive flexibility in mice</article-title><source>Learning &amp; Memory</source><volume>26</volume><fpage>24</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1101/lm.048264.118</pub-id><pub-id pub-id-type="pmid">30559117</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisupati</surname><given-names>S</given-names></name><name><surname>Chartarifsky-Lynn</surname><given-names>L</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Lapses in perceptual decisions reflect exploration</article-title><source>eLife</source><volume>10</volume><elocation-id>e55490</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55490</pub-id><pub-id pub-id-type="pmid">33427198</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Kawai</surname><given-names>R</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A fully automated high-throughput training system for rodents</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e83171</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0083171</pub-id><pub-id pub-id-type="pmid">24349451</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mouse academy: High-throughput automated training and trial-by-trial behavioral analysis during learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/467878</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Sheppard</surname><given-names>JP</given-names></name><name><surname>Schrater</surname><given-names>PR</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory decision-making in rats and humans</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3726</fpage><lpage>3735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4998-11.2012</pub-id><pub-id pub-id-type="pmid">22423093</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Thiberge</surname><given-names>SY</given-names></name><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Tervo</surname><given-names>DGR</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Karpova</surname><given-names>AY</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Magnetic voluntary head-fixation in transgenic rats enables lifespan imaging of hippocampal neurons</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>e4154</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-48505-9</pub-id><pub-id pub-id-type="pmid">38755205</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mice in a labyrinth show rapid learning, sudden insight, and efficient exploration</article-title><source>eLife</source><volume>10</volume><elocation-id>e66175</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66175</pub-id><pub-id pub-id-type="pmid">34196271</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Bak</surname><given-names>JH</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2021">2021</year><article-title>Extracting the dynamics of behavior in sensory decision-making experiments</article-title><source>Neuron</source><volume>109</volume><fpage>597</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.004</pub-id><pub-id pub-id-type="pmid">33412101</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salameh</surname><given-names>G</given-names></name><name><surname>Jeffers</surname><given-names>MS</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Pitney</surname><given-names>J</given-names></name><name><surname>Silasi</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The home-cage automated skilled reaching apparatus (HASRA): Individualized training of group-housed mice in a single pellet reaching task</article-title><source>ENEURO</source><volume>7</volume><elocation-id>ENEURO</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0242-20.2020</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaefer</surname><given-names>AT</given-names></name><name><surname>Claridge-Chang</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The surveillance state of behavioral automation</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>170</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.11.004</pub-id><pub-id pub-id-type="pmid">22119142</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmack</surname><given-names>K</given-names></name><name><surname>Bosc</surname><given-names>M</given-names></name><name><surname>Ott</surname><given-names>T</given-names></name><name><surname>Sturgill</surname><given-names>JF</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Striatal dopamine mediates hallucination-like perception in mice</article-title><source>Science</source><volume>372</volume><elocation-id>eabf4740</elocation-id><pub-id pub-id-type="doi">10.1126/science.abf4740</pub-id><pub-id pub-id-type="pmid">33795430</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cellular resolution functional imaging in behaving rats using voluntary head restraint</article-title><source>Neuron</source><volume>80</volume><fpage>371</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.002</pub-id><pub-id pub-id-type="pmid">24055015</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>H</given-names></name><name><surname>Byun</surname><given-names>J</given-names></name><name><surname>Roh</surname><given-names>D</given-names></name><name><surname>Choi</surname><given-names>N</given-names></name><name><surname>Shin</surname><given-names>H-S</given-names></name><name><surname>Cho</surname><given-names>I-J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Interference-free, lightweight wireless neural probe system for investigating brain activity during natural competition</article-title><source>Biosensors &amp; Bioelectronics</source><volume>195</volume><elocation-id>113665</elocation-id><pub-id pub-id-type="doi">10.1016/j.bios.2021.113665</pub-id><pub-id pub-id-type="pmid">34610533</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silasi</surname><given-names>G</given-names></name><name><surname>Boyd</surname><given-names>JD</given-names></name><name><surname>Bolanos</surname><given-names>F</given-names></name><name><surname>LeDue</surname><given-names>JM</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name><name><surname>Murphy</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Individualized tracking of self-directed motor learning in group-housed mice performing a skilled lever positioning task in the home cage</article-title><source>Journal of Neurophysiology</source><volume>119</volume><fpage>337</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1152/jn.00115.2017</pub-id><pub-id pub-id-type="pmid">29070625</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smolen</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Byrne</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The right time to learn: mechanisms and optimization of spaced learning</article-title><source>Nature Reviews. Neuroscience</source><volume>17</volume><fpage>77</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/nrn.2015.18</pub-id><pub-id pub-id-type="pmid">26806627</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mechanisms of movement planning: motor cortex and beyond</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>33</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.10.023</pub-id><pub-id pub-id-type="pmid">29172091</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanji</surname><given-names>J</given-names></name><name><surname>Evarts</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Anticipatory activity of motor cortex neurons in relation to direction of an intended movement</article-title><source>Journal of Neurophysiology</source><volume>39</volume><fpage>1062</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1152/jn.1976.39.5.1062</pub-id><pub-id pub-id-type="pmid">824409</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taxidis</surname><given-names>J</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Dorian</surname><given-names>CC</given-names></name><name><surname>Mylavarapu</surname><given-names>AL</given-names></name><name><surname>Arora</surname><given-names>JS</given-names></name><name><surname>Samadian</surname><given-names>KD</given-names></name><name><surname>Hoffberg</surname><given-names>EA</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Differential emergence and stability of sensory and temporal representations in context-specific hippocampal sequences</article-title><source>Neuron</source><volume>108</volume><fpage>984</fpage><lpage>998</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.08.028</pub-id><pub-id pub-id-type="pmid">32949502</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torquet</surname><given-names>N</given-names></name><name><surname>Marti</surname><given-names>F</given-names></name><name><surname>Campart</surname><given-names>C</given-names></name><name><surname>Tolu</surname><given-names>S</given-names></name><name><surname>Nguyen</surname><given-names>C</given-names></name><name><surname>Oberto</surname><given-names>V</given-names></name><name><surname>Benallaoua</surname><given-names>M</given-names></name><name><surname>Naudé</surname><given-names>J</given-names></name><name><surname>Didienne</surname><given-names>S</given-names></name><name><surname>Debray</surname><given-names>N</given-names></name><name><surname>Jezequel</surname><given-names>S</given-names></name><name><surname>Le Gouestre</surname><given-names>L</given-names></name><name><surname>Hannesse</surname><given-names>B</given-names></name><name><surname>Mariani</surname><given-names>J</given-names></name><name><surname>Mourot</surname><given-names>A</given-names></name><name><surname>Faure</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Social interactions impact on the dopaminergic system and drive individuality</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>e3081</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05526-5</pub-id><pub-id pub-id-type="pmid">30082725</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vázquez-Guardado</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Bandodkar</surname><given-names>AJ</given-names></name><name><surname>Rogers</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recent advances in neurotechnologies with broad potential for neuroscience research</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1522</fpage><lpage>1536</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00739-8</pub-id><pub-id pub-id-type="pmid">33199897</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Gerken</surname><given-names>B</given-names></name><name><surname>Wieland</surname><given-names>JR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Fellous</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The effects of time horizon and guided choices on explore-exploit decisions in rodents</article-title><source>Behavioral Neuroscience</source><volume>137</volume><fpage>127</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1037/bne0000549</pub-id><pub-id pub-id-type="pmid">36633987</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weekes</surname><given-names>NY</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>CHAPTER 13 - Sex Differences in the Brain</source><publisher-loc>San Diego</publisher-loc><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1016/B978-0-08-092668-1.50019-3</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>RD</given-names></name><name><surname>Schmitt</surname><given-names>LI</given-names></name><name><surname>Davidson</surname><given-names>TJ</given-names></name><name><surname>Nakajima</surname><given-names>M</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Halassa</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Thalamic control of sensory selection in divided attention</article-title><source>Nature</source><volume>526</volume><fpage>705</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nature15398</pub-id><pub-id pub-id-type="pmid">26503050</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>JP</given-names></name><name><surname>Mughrabi</surname><given-names>IT</given-names></name><name><surname>Wong</surname><given-names>J</given-names></name><name><surname>Mathew</surname><given-names>J</given-names></name><name><surname>Jayaprakash</surname><given-names>N</given-names></name><name><surname>Crosfield</surname><given-names>C</given-names></name><name><surname>Chang</surname><given-names>EH</given-names></name><name><surname>Chavan</surname><given-names>SS</given-names></name><name><surname>Tracey</surname><given-names>KJ</given-names></name><name><surname>Pavlov</surname><given-names>VA</given-names></name><name><surname>Al-Abed</surname><given-names>Y</given-names></name><name><surname>Zanos</surname><given-names>TP</given-names></name><name><surname>Zanos</surname><given-names>S</given-names></name><name><surname>Datta-Chaudhuri</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A fully implantable wireless bidirectional neuromodulation system for mice</article-title><source>Biosensors &amp; Bioelectronics</source><volume>200</volume><elocation-id>e113886</elocation-id><pub-id pub-id-type="doi">10.1016/j.bios.2021.113886</pub-id><pub-id pub-id-type="pmid">34995836</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The causal role of auditory cortex in auditory working memory</article-title><source>eLife</source><volume>01</volume><elocation-id>e4457</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.64457</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Burke</surname><given-names>DA</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>An open-source behavior controller for associative learning and memory (B-CALM)</article-title><source>Behavior Research Methods</source><volume>56</volume><fpage>2695</fpage><lpage>2710</lpage><pub-id pub-id-type="doi">10.3758/s13428-023-02182-6</pub-id><pub-id pub-id-type="pmid">37464151</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>Singla</surname><given-names>A</given-names></name><name><surname>Zilles</surname><given-names>S</given-names></name><name><surname>Rafferty</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An overview of machine teaching</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1801.05927">http://arxiv.org/abs/1801.05927</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Performance errors during rodent learning reflect a dynamic choice strategy</article-title><source>Current Biology</source><volume>34</volume><fpage>2107</fpage><lpage>2117</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2024.04.017</pub-id><pub-id pub-id-type="pmid">38677279</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104833.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Flagel</surname><given-names>Shelly B</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jmfr291</institution-id><institution>University of Michigan</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This manuscript describes a novel approach for assessing cognitive function in freely moving mice in their home-cage, without human involvement. The authors provide <bold>convincing</bold> evidence in support of the tasks they developed to capture a variety of complex behaviors and demonstrate the utility of a machine learning approach to expedite the acquisition of task demands. This work is <bold>important</bold> given its potential utility for other investigators interested in studying mouse cognition.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104833.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This is a new and important system that can efficiently train mice to perform a variety of cognitive tasks in a flexible manner. It is innovative and opens the door to important experiments in the neurobiology of learning and memory.</p><p>Strengths:</p><p>Strengths include: high n's, a robust system, task flexibility, comparison of manual-like training vs constant training, circadian analysis, comparison of varying cue types, long-term measurement, and machine teaching.</p><p>Weaknesses:</p><p>I find no major problems with this report.</p><p>Comments on revisions:</p><p>My concerns have been addressed now.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104833.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The manuscript by Yu et al. describes a novel approach for collecting complex and different cognitive phenotypes in individually housed mice in their home cage. The authors report a simple yet elegant design that they developed for assessing a variety of complex and novel behavioral paradigms autonomously in mice.</p><p>Strengths:</p><p>The data are strong, the arguments are convincing, and I think the manuscript will be highly cited given the complexity of behavioral phenotypes one can collect using this relatively inexpensive ($100/box) and high-throughput procedure (without the need of human interaction). Additionally, the authors include a machine learning algorithm to correct for erroneous strategies that mice develop which is incredibly elegant and important for this approach, as mice will develop odd strategies when given complete freedom.</p><p>Weaknesses:</p><p>A limitation to this approach is that it requires mice to be individually housed for days to months. This is now adequately addressed in the discussion.</p><p>A major issue with continuous self-paced tasks such as the autonomous d2AFC used by the authors is that the inter-trial intervals can vary significantly. Mice may do a few trials, lose interest and disengage from the task for several hours. This is problematic for data analysis that relies on trial duration to be similar between trials (e.g., reinforcement learning algorithms). The authors now provide information regarding task engagement of the mice across a 24 hour cycle (e.g., trials started, trials finished across a 24 h period).</p><p>Movies - it would be beneficial for the authors to add commentary to the video (hit, miss trials). It was interesting watching the mice but not clear whether they were doing the task correctly or not. The new videos adequately address these concerns.</p><p>The strength of this paper (from my perspective) is the potential utility it has for other investigators trying to get mice to do behavioral tasks. However, not enough information was provided about the construction of the boxes, interface, and code for running the boxes. If the authors are not willing to provide this information through eLife, GitHub, or their own website then my evaluation of impact and significance of this paper would go down significantly. This information is now available to readers.</p><p>Minor concerns</p><p>Learning rate is confusing for Figure 3 results as it actually refers to trials to reach criterion, and not the actual rate of learning (e.g., slope). This has been modified in the manuscript.</p><p>Comments on revisions:</p><p>The authors have addressed all my concerns regarding this very exciting manuscript.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104833.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this set of experiments, the authors describe a novel research tool for studying complex cognitive tasks in mice, the HABITS automated training apparatus, and a novel &quot;machine teaching&quot; approach they use to accelerate training by algorithmically providing trials to animals that provide the most information about the current rule state for a given task.</p><p>Strengths:</p><p>There is much to be celebrated in an inexpensively constructed, replicable training environment that can be used with mice, which have rapidly become the model species of choice for understanding the roles of distinct circuits and genetic factors in cognition. Lingering challenges in developing and testing cognitive tasks in mice remain, however, and these are often chalked up to cognitive limitations in the species. The authors' findings, however, suggest that instead we may need to work creatively to meet mice where they live. In some cases it may be that mice may require durations of training far longer than laboratories are able to invest with manual training (up to over 100k trials, over months of daily testing) but that the tasks are achievable. The &quot;machine teaching&quot; approach further suggests that this duration could be substantially reduced by algorithmically optimizing each trial presented during training to maximize learning.</p><p>Weaknesses:</p><p>Cognitive training and testing in rodent models fill a number of roles. Sometimes, investigators are interested in within-subjects questions - querying a specific circuit, genetically defined neuron population, or molecule/drug candidate, by interrogating or manipulating its function in a highly trained animal. In this scenario, a cohort of highly trained animals which have been trained via a method that aims to make their behavior as similar as possible is a strength.</p><p>However, often investigators are interested in between-subjects questions - querying a source of individual differences that can have long term and/or developmental impacts, such as sex differences or gene variants. This is likely to often be the case in mouse models especially, because of their genetic tractability. In scenarios where investigators have examined cognitive processes between subjects in mice who vary across these sources of individual difference, the process of learning a task has been repeatedly shown to be different. The authors recognize that their approach is currently optimized for testing within-subjects questions, but begin to show how between-subjects questions might be addressed with this system.</p><p>The authors have perhaps shown that their main focus is highly-controlled within-subjects questions, as their dataset is almost exclusively made up of several hundred young adult male mice, with the exception of 6 females in a supplemental figure. It is notable that these female mice do appear to learn the two-alternative forced choice task somewhat more rapidly than the males in their cohort, and the authors suggest that future work with this system could be used to uncover strategies that differ across individuals.</p><p>Considering the implications for mice modeling relevant genetic variants, it is unclear to what extent the training protocols and especially the algorithmic machine teaching approach would be able to inform investigators about the differences between their groups during training. For investigators examining genetic models, it is unclear whether this extensive training experience would mitigate the ability to observe cognitive differences, or select for the animals best able to overcome them - eliminating the animals of interest. Likewise, the algorithmic approach aims to mitigate features of training such as side biases, but it is worth noting that the strategic uses of side biases in mice, as in primates, can benefit learning, rather than side biases solely being a problem. However, the investigators may be able to highlight variables selected by the algorithm that are associated with individual strategies in performing their tasks, and this would be a significant contribution.</p><p>A final, intriguing finding in this manuscript is that animal self-paced training led to much slower learning than &quot;manual&quot; training, by having the experimenter introduce the animal to the apparatus for a few hours each day. Manual training resulted in significantly faster learning, in almost half the number of trials on average, and with significantly fewer omitted trials. This finding does not necessarily argue that manual training is universally a better choice, because it led to more limited water consumption. However, it suggests that there is a distinct contribution of experimenter interactions and/or switching contexts in cognitive training, for example, by activating an &quot;occasion setting&quot; process to accelerate learning for a distinct period of time. Limiting experimenter interactions with mice may be a labor saving intervention, but may not necessarily improve performance. This could be an interesting topic of future investigation, of relevance to understanding how animals of all species learn.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104833.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Bowen</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Penghai</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Haoze</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yueming</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Kedi</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Hao</surname><given-names>Yaoyao</given-names></name><role specific-use="author">Author</role><aff><institution>Zhejiang University</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>This is a new and important system that can efficiently train mice to perform a variety of cognitive tasks in a flexible manner. It is innovative and opens the door to important experiments in the neurobiology of learning and memory.</p><p>Strengths:</p><p>Strengths include: high n's, a robust system, task flexibility, comparison of manual-like training vs constant training, circadian analysis, comparison of varying cue types, long-term measurement, and machine teaching.</p><p>Weaknesses:</p><p>I find no major problems with this report.</p><p>Minor weaknesses:</p><p>(1) Line 219: Water consumption per day remained the same, but number of trails triggered was more as training continued. First, is this related to manual-type training? Also, I'm trying to understand this result quantitatively, since it seems counter-intuitive: I would assume that with more trials, more water would be consumed since accuracy should go up over training (so more water per average trial). Am I understanding this right? Can the authors give more detail or understanding to how more trials can be triggered but no more water is consumed despite training?</p></disp-quote><p>Thanks for the comment. We would like to clarify the phenomenon described in Line 219: As the training advanced, the number of trials triggered by mice per day decreased (rather than increased as you mentioned in the comment) gradually for both manual and autonomous groups of mice (Fig. 2H left). The performance, as you mentioned, improved over time (Fig. 2D and 2E), leading to an increased probability of obtaining water and thus relatively stable daily water intake (Fig. 2H middle). We believe the stable daily intake is the minimum amount of water required by the mice under circumstance of autonomous behavioral training. To make the statement more clearly, we indicated the corresponding figure numbers in the text.</p><p>Results “… As shown in Fig. 2H, autonomous training yielded significantly higher number of trial/day (980 ± 25 vs. 611 ± 26, Fig. 2H left) and more volume of water consumption/day (1.65 ± 0.06 vs. 0.97 ± 0.03 ml, Fig. 2H middle), which resulted in monotonic increase of body weight that was even comparable to the free water group (Fig.2H right). In contrast, the body weight in manual training group experienced a sharp drop at the beginning of training and was constantly lower than autonomous group throughout the training stage (Fig. 2H right).”</p><disp-quote content-type="editor-comment"><p>(2) Figure 2J: The X-axis should have some label: at least &quot;training type&quot;. Ideally, a legend with colors can be included, although I see the colors elsewhere in the figure. If a legend cannot be added, then the color scheme should be explained in the caption.</p></disp-quote><p>Thanks for the suggestion. The labels with corresponding colors for x-axis have been added for Fig. 2J.</p><disp-quote content-type="editor-comment"><p>(3) Figure 2K: What is the purple line? I encourage a legend here. The same legend could apply to 2J.</p></disp-quote><p>Thanks for the suggestion. The legend has been added for Fig. 2K.</p><disp-quote content-type="editor-comment"><p>(4) Supplementary Figure S2 D: I do not think the phrase &quot;relying on&quot; is correct. Instead, I think &quot;predicted by&quot; or &quot;correlating with&quot; might be better.</p></disp-quote><p>We thank the reviewer for the valuable suggestion. The phrase has been changed to ‘predicted by’ for better suitability.</p><p>Figure S2 “(D), percentage of trials significantly predicted by different regressors during task learning. …”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The manuscript by Yu et al. describes a novel approach for collecting complex and different cognitive phenotypes in individually housed mice in their home cage. The authors report a simple yet elegant design that they developed for assessing a variety of complex and novel behavioral paradigms autonomously in mice.</p><p>Strengths:</p><p>The data are strong, the arguments are convincing, and I think the manuscript will be highly cited given the complexity of behavioral phenotypes one can collect using this relatively inexpensive ($100/box) and high throughput procedure (without the need for human interaction). Additionally, the authors include a machine learning algorithm to correct for erroneous strategies that mice develop which is incredibly elegant and important for this approach as mice will develop odd strategies when given complete freedom.</p><p>Weaknesses:</p><p>(1) A limitation of this approach is that it requires mice to be individually housed for days to months. This should be discussed in depth.</p></disp-quote><p>Thank you for raising this important point. We agree that the requirement for individual housing of mice during the training period is a limitation of our approach, and we appreciate the opportunity to discuss this in more depth. In the manuscript, we add a section to the Discussion to address this limitation, including the potential impact of individual housing on the mice, the rationale for individual housing in our study, and efforts or alternatives made to mitigate the effects of individual housing.</p><p>Discussion “… Firstly, our experiments were confined to single-housed mice, which is known to influence murine behavior and physiology, potentially affecting social interaction and stress levels [76]. In our study, individual housing was necessary to ensure precise behavioral tracking, eliminate competitive interactions during task performance, and maintain consistent training schedules without disruptions from cage-mate disturbances. However, the potential of group-housed training has been explored with technologies such as RFID <bold>[</bold>28,29,32–34] to distinguish individual mice, which potentially improving the training efficiency and facilitating research of social behaviors [77]. Notably, it has shown that simultaneous training of group-housed mice, without individual differentiation, can still achieve criterion performance [25].”</p><disp-quote content-type="editor-comment"><p>(2) A major issue with continuous self-paced tasks such as the autonomous d2AFC used by the authors is that the inter-trial intervals can vary significantly. Mice may do a few trials, lose interest, and disengage from the task for several hours. This is problematic for data analysis that relies on trial duration to be similar between trials (e.g., reinforcement learning algorithms). It would be useful to see the task engagement of the mice across a 24-hour cycle (e.g., trials started, trials finished across a 24-hour period) and approaches for overcoming this issue of varying inter-trial intervals.</p></disp-quote><p>Thank you for your insightful comment regarding the variability in inter-trial intervals and its potential impact on data analysis. We agree that this is an important consideration for continuous self-paced tasks.</p><p>In our original manuscript, we have showed the general task engagement across 24-hour cycle (Fig. 2K), which revealed two peaks of engagements during the dark cycle with relatively fewer trials during the light cycle. To facilitate analyses requiring consistent trial durations, we defined trial blocks as sequences between two no-response trials. Notably, approximately 66.6% of trials occurred within blocks of &gt;5 consecutive trials (Fig. 2L), which may be particularly suitable for such analyses.</p><p>In the revised manuscript, we also added the analysis of the histogram of inter-trial-interval for both the autonomous and manual training paradigms in HABITS (Fig. S2H), which shows that around 55.2% and 77.5% of the intervals are less than 2 seconds in autonomous and manual training, respectively.</p><p>Results “… We found more than two-third of the trials was done in &gt;5-trial blocks (Fig. 2L left) which resulted in more than 55% of the trials were with inter-trial-interval less than 2 seconds (Fig. S2H).”</p><p>Regarding the approaches to mitigate the issue of varying inter-trial interval, we observed that manual training (i.e., manually transferring to HABITS for ~2 hr/day) in Fig. S2H resulted in more trials with short inter-trial-interval, suggesting that constrained access time promotes task engagement and reduces interval variability. Fig. 2L also indicated that the averaged correct rate increased and the earlylick rate decreased as the length of block increased. This approach could be valuable for studies where consistent trial timing is critical. In the context of our study, we could actually introduce a light, for example, to serve as the cue that prompt the animals to engage during a fixed time duration in a day.</p><p>Discussion “… In contrast, the self-paced nature of autonomous training may permit greater variability in attentional engagement 83 and inter-trial-intervals, which could be problematic for data analysis relaying on consistent intervals and/or engagements. Future studies should explore how controlled contextual constraints enhance learning efficiency and whether incorporating such measures into HABITS could optimize its performance.”</p><disp-quote content-type="editor-comment"><p>(3) Movies - it would be beneficial for the authors to add commentary to the video (hit, miss trials). It was interesting watching the mice but not clear whether they were doing the task correctly or not.</p></disp-quote><p>Thanks for the reminder. We have added subtitles to both of the videos. Since the supplementary video1 was not recorded with sound, the correctness of the trials was hard to judge. We replaced the video with another one with clear sound recordings, and the subtitles were commented in detail.</p><disp-quote content-type="editor-comment"><p>(4) The strength of this paper (from my perspective) is the potential utility it has for other investigators trying to get mice to do behavioral tasks. However, not enough information was provided about the construction of the boxes, interface, and code for running the boxes. If the authors are not willing to provide this information through eLife, GitHub, or their own website then my evaluation of the impact and significance of this paper would go down significantly.</p></disp-quote><p>Thanks for this important comment. We would like to clarify that the construction methods, GUI, code for our system, PCB and CAD files (newly uploaded) have already been made publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/Yaoyao-Hao/HABITS">https://github.com/Yaoyao-Hao/HABITS</ext-link>. Additionally, we have open-sourced all the codes and raw data for all training protocols (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.27192897">https://doi.org/10.6084/m9.figshare.27192897</ext-link>). We will continue to maintain these resources in the future.</p><disp-quote content-type="editor-comment"><p>Minor concerns:</p><p>(5) Learning rate is confusing for Figure 3 results as it actually refers to trials to reach the criterion, and not the actual rate of learning (e.g., slope).</p></disp-quote><p>Thanks for pointing this out. The ‘learning rate’ which refers to trial number to reach criterion has been changed to ‘the number of trials to reach criterion’.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>In this set of experiments, the authors describe a novel research tool for studying complex cognitive tasks in mice, the HABITS automated training apparatus, and a novel &quot;machine teaching&quot; approach they use to accelerate training by algorithmically providing trials to animals that provide the most information about the current rule state for a given task.</p><p>Strengths:</p><p>There is much to be celebrated in an inexpensively constructed, replicable training environment that can be used with mice, which have rapidly become the model species of choice for understanding the roles of distinct circuits and genetic factors in cognition. Lingering challenges in developing and testing cognitive tasks in mice remain, however, and these are often chalked up to cognitive limitations in the species. The authors' findings, however, suggest that instead, we may need to work creatively to meet mice where they live. In some cases, it may be that mice may require durations of training far longer than laboratories are able to invest with manual training (up to over 100k trials, over months of daily testing) but the tasks are achievable. The &quot;machine teaching&quot; approach further suggests that this duration could be substantially reduced by algorithmically optimizing each trial presented during training to maximize learning.</p><p>Weaknesses:</p><p>(1) Cognitive training and testing in rodent models fill a number of roles. Sometimes, investigators are interested in within-subjects questions - querying a specific circuit, genetically defined neuron population, or molecule/drug candidate, by interrogating or manipulating its function in a highly trained animal. In this scenario, a cohort of highly trained animals that have been trained via a method that aims to make their behavior as similar as possible is a strength.</p><p>However, often investigators are interested in between-subjects questions - querying a source of individual differences that can have long-term and/or developmental impacts, such as sex differences or gene variants. This is likely to often be the case in mouse models especially, because of their genetic tractability. In scenarios where investigators have examined cognitive processes between subjects in mice who vary across these sources of individual difference, the process of learning a task has been repeatedly shown to be different. The authors do not appear to have considered individual differences except perhaps as an obstacle to be overcome.</p><p>The authors have perhaps shown that their main focus is highly-controlled within-subjects questions, as their dataset is almost exclusively made up of several hundred young adult male mice, with the exception of 6 females in a supplemental figure. It is notable that these female mice do appear to learn the two-alternative forced-choice task somewhat more rapidly than the males in their cohort.</p></disp-quote><p>Thank you for your insightful comments and for highlighting the importance of considering both within-subject and between-subject questions in cognitive training and testing in rodent models. We acknowledge that our study primarily focused on highly controlled within-subject questions. However, the datasets we provided did show preliminary evidences for the ‘between-subject’ questions. Key observations include:</p><p>The large variability in learning rates among mice observed in Fig. 2I;</p><p>The overall learning rate difference between male and female subjects (Fig. 2D vs. Fig. S2G);</p><p>The varying nocturnal behavioral patterns (Fig. 2K), etc.</p><p>We recognize the value of exploring between-subjects differences in mouse model and discussed more details in the Discussion part.</p><p>Discussion “Our study was designed to standardize behavior for the precise interrogation of neural mechanisms, specifically addressing within-subject questions. However, investigators are often interested in between-subject differences—such as sex differences or genetic variants—which can have long-term behavioral and cognitive implications [72,74]. This is particularly relevant in mouse models due to their genetic tractability [75]. Although our primary focus was not on between-subject differences, the dataset we generated provides preliminary evidence for such investigations. Several behavioral readouts revealed individual variability among mice, including large disparities in learning rates across individuals (Fig. 2I), differences in overall learning rates between male and female subjects (Fig. 2D vs. Fig. S2G), variations in nocturnal behavioral patterns (Fig. 2K), etc.”</p><disp-quote content-type="editor-comment"><p>(2) Considering the implications for mice modeling relevant genetic variants, it is unclear to what extent the training protocols and especially the algorithmic machine teaching approach would be able to inform investigators about the differences between their groups during training. For investigators examining genetic models, it is unclear whether this extensive training experience would mitigate the ability to observe cognitive differences, or select the animals best able to overcome them - eliminating the animals of interest. Likewise, the algorithmic approach aims to mitigate features of training such as side biases, but it is worth noting that the strategic uses of side biases in mice, as in primates, can benefit learning, rather than side biases solely being a problem. However, the investigators may be able to highlight variables selected by the algorithm that are associated with individual strategies in performing their tasks, and this would be a significant contribution.</p></disp-quote><p>Thank you for the insightful comments. We acknowledge that the extensive training experience, particularly through the algorithmic machine teaching approach, could potentially influence the ability to observe cognitive differences between groups of mice with relevant genetic variants. However, our study design and findings suggest that this approach can still provide valuable insights into individual differences and strategies used by the animals during training. First, the behavioral readout (including learning rate, engagement pattern, etc.) as mentioned above, could tell certain number of differences among mice. Second, detailed modelling analysis (with logistical regression modelling) could further dissect the strategy that mouse use along the training process (Fig. S2B). We have actually highlighted some variables selected by the regression that are associated with individual strategies in performing their tasks (Fig. S2C) and these strategies could be different between manual and autonomous training groups (Fig. S2D). We included these comments in the Discussion part for further clearance.</p><p>Discussion “… Furthermore, a detailed logistic regression analysis dissected the strategies mice employed during training (Fig. S2B). Notably, the regression identified variables associated with individual task-performance strategies (Fig. S2C), which also differed between manually and autonomously trained groups (Fig. S2D). Thus, our system could facilitate high-throughput behavioral studies exploring between-subject differences in the future.”</p><disp-quote content-type="editor-comment"><p>(3) A final, intriguing finding in this manuscript is that animal self-paced training led to much slower learning than &quot;manual&quot; training, by having the experimenter introduce the animal to the apparatus for a few hours each day. Manual training resulted in significantly faster learning, in almost half the number of trials on average, and with significantly fewer omitted trials. This finding does not necessarily argue that manual training is universally a better choice because it leads to more limited water consumption. However, it suggests that there is a distinct contribution of experimenter interactions and/or switching contexts in cognitive training, for example by activating an &quot;occasion setting&quot; process to accelerate learning for a distinct period of time. Limiting experimenter interactions with mice may be a labor-saving intervention, but may not necessarily improve performance. This could be an interesting topic of future investigation, of relevance to understanding how animals of all species learn.</p></disp-quote><p>Thank you for your insightful comments. We agree that the finding that manual training led to significantly faster learning compared to self-paced training is both intriguing and important. One of the possible reasons we think is due to the limited duration of engagement provided by the experimenter in the manual training case, which forced the mice to concentrate more on the trials (thus with fewer omitting trials) than in autonomous training. Your suggestion that experimenter interactions might activate an &quot;occasion setting&quot; process is particularly interesting. In the context of our study, we could actually introduce, for example, a light, serving as the cue that prompt the animals to engage; and when the light is off, the engagement was not accessible any more for the mice to simulate the manual training situation. We agree that this could be an interesting topic for future investigation that might create a more conducive environment for learning, thereby accelerating the learning rate.</p><p>Discussion “… Lastly, while HABITS achieves criterion performance in a similar or even shorter overall days compared to manual training, it requires more trials to reach the same learning criterion (Fig. 2G). We hypothesize that this difference in trial efficiency may stem from the constrained engagement duration imposed by the experimenter in manual training, which could compel mice to focus more intensely on task execution, resulting in less trial omissions (Fig. 2F). In contrast, the self-paced nature of autonomous training may permit greater variability in attentional engagement 83 and inter-trial-intervals, which could be problematic for data analysis relaying on consistent intervals and/or engagements. Future studies should explore how controlled contextual constraints enhance learning efficiency and whether incorporating such measures into HABITS could optimize its performance.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>As I mentioned in the weaknesses, I did not see code or CAD drawings for their home cages and how these interact with a computer.</p></disp-quote><p>Thanks for the comment. We would like to clarify that the construction methods, GUI, code for our system, PCB and CAD files (newly uploaded) have already been made publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/Yaoyao-Hao/HABITS">https://github.com/Yaoyao-Hao/HABITS</ext-link>.</p></body></sub-article></article>