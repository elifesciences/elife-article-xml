<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">76145</article-id><article-id pub-id-type="doi">10.7554/eLife.76145</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural dynamics of causal inference in the macaque frontoparietal circuit</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-264539"><name><surname>Qi</surname><given-names>Guangyao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0479-7320</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-264540"><name><surname>Fang</surname><given-names>Wen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7748-6772</contrib-id><email>wenfang@ion.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-264541"><name><surname>Li</surname><given-names>Shenghao</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-264542"><name><surname>Li</surname><given-names>Junru</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-52172"><name><surname>Wang</surname><given-names>Liping</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2038-0234</contrib-id><email>liping.wang@ion.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>Institute of Neuroscience, Key Laboratory of Primate Neurobiology, CAS Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Merchant</surname><given-names>Hugo</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tmp8f25</institution-id><institution>National Autonomous University of Mexico</institution></institution-wrap><country>Mexico</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>24</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e76145</elocation-id><history><date date-type="received" iso-8601-date="2021-12-06"><day>06</day><month>12</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-10-23"><day>23</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-12-07"><day>07</day><month>12</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.12.06.469042"/></event></pub-history><permissions><copyright-statement>© 2022, Qi, Fang et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Qi, Fang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-76145-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-76145-figures-v2.pdf"/><abstract><p>Natural perception relies inherently on inferring causal structure in the environment. However, the neural mechanisms and functional circuits essential for representing and updating the hidden causal structure and corresponding sensory representations during multisensory processing are unknown. To address this, monkeys were trained to infer the probability of a potential common source from visual and proprioceptive signals based on their spatial disparity in a virtual reality system. The proprioceptive drift reported by monkeys demonstrated that they combined previous experience and current multisensory signals to estimate the hidden common source and subsequently updated the causal structure and sensory representation. Single-unit recordings in premotor and parietal cortices revealed that neural activity in the premotor cortex represents the core computation of causal inference, characterizing the estimation and update of the likelihood of integrating multiple sensory inputs at a trial-by-trial level. In response to signals from the premotor cortex, neural activity in the parietal cortex also represents the causal structure and further dynamically updates the sensory representation to maintain consistency with the causal inference structure. Thus, our results indicate how the premotor cortex integrates previous experience and sensory inputs to infer hidden variables and selectively updates sensory representations in the parietal cortex to support behavior. This dynamic loop of frontal-parietal interactions in the causal inference framework may provide the neural mechanism to answer long-standing questions regarding how neural circuits represent hidden structures for body awareness and agency.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>causal inference</kwd><kwd>multisensory perception</kwd><kwd>body representation</kwd><kwd>frontoparietal circuit</kwd><kwd>bayesian inference</kwd><kwd>virtual reality</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>National Science and Technology Innovation 2030 Major Program</institution></institution-wrap></funding-source><award-id>2021ZD0204204</award-id><principal-award-recipient><name><surname>Fang</surname><given-names>Wen</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Shanghai Municipal Science and Technology Major Project</institution></institution-wrap></funding-source><award-id>2021SHZDZX</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Liping</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Lingang Laboratory Grant</institution></institution-wrap></funding-source><award-id>LG202105-02-01</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Liping</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Strategic Priority Research Programs</institution></institution-wrap></funding-source><award-id>XDB32070201</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Liping</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32100830</award-id><principal-award-recipient><name><surname>Fang</surname><given-names>Wen</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In causal inference, the premotor cortex dynamically integrates prior information and current sensory inputs to infer hidden structures, and selectively updates sensory representations in the parietal cortex to support behavior.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain is constantly confronted with a myriad of sensory signals. Natural perception relies inherently on inferring the environment’s hidden causal structure (<xref ref-type="bibr" rid="bib21">Deroy et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">French and DeAngelis, 2020</xref>; <xref ref-type="bibr" rid="bib52">Lochmann and Deneve, 2011</xref>). For instance, in the ventriloquism illusion, when the audience is presented with a temporally synchronous but spatially discrepant audiovisual stimulus (e.g., a speech sound from the speaker and a visibly moving mouth of the puppet), they usually infer these audiovisual stimuli are coming from a common source and illusive perceive the speech coming from the puppet. In the process of building representation of the bodily self, the brain combines, in a near-optimal manner, information from multiple sensory inputs. When a single entity (e.g., the bodily self) evokes correlated noisy signals, our brain combines the information to infer the properties of this entity based on the quality and uncertainty of the sensory stimuli. As a result, behavioral performance often benefits from combining information using uncertainty-based weighting across sensory systems (<xref ref-type="bibr" rid="bib71">Stein and Stanford, 2008</xref>). However, in a natural environment, multiple sensory cues are typically produced by more than one source (e.g., two entities), which should not be integrated in the brain, especially when the superposing cues are sufficiently dissimilar and uncorrelated. Instead, the brain’s inferential process of integration fades out, leading to the perception that these cues originate from distinct entities. This process of inferring the causes of sensory inputs for perception is known as causal inference (<xref ref-type="bibr" rid="bib49">Körding et al., 2007</xref>).</p><p>Thus far, most of the neurobiological studies of multisensory processing have operated under the assumption that different streams of sensory information can arise from the same source. For example, previous neurophysiological research in monkeys showed that neurons implement reliability-weighted integration on the premise that visual and vestibular signals are from a common source (<xref ref-type="bibr" rid="bib28">Fetsch et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Morgan et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Porter et al., 2007</xref>). Therefore, despite the ubiquity of the phenomenon of causal inference and many psychophysical and theoretical research (<xref ref-type="bibr" rid="bib1">Acerbi et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Dokka et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Kayser and Shams, 2015</xref>; <xref ref-type="bibr" rid="bib49">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib54">Mohl et al., 2020</xref>; <xref ref-type="bibr" rid="bib65">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib69">Sato et al., 2007</xref>), its neural mechanisms and functional circuits remain largely unknown. Recent neuroimaging studies have started to show the sequential causal inference process in the human brain (<xref ref-type="bibr" rid="bib3">Aller and Noppeney, 2019</xref>; <xref ref-type="bibr" rid="bib17">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib66">Rohe and Noppeney, 2016</xref>). However, little was known about the process at the single-neuron level in animals (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>). In particular, the updating of prior and sensory information during causal inference has not been examined in animals.</p><p>In the present study, we established an objective and quantitative signature of causal inference at a single-trial level using a reaching task and a virtual reality system in macaque monkeys. We showed that monkeys combined previous experience and current multisensory signals to estimate the hidden common source and, more importantly, subsequently updated both the causal structure and sensory representation during the inference. We then further recorded from the premotor and parietal (area 5 and area 7) cortices of three monkeys to investigate the neural dynamics and functional circuits of causal inference in multisensory processing. Our behavioral and neural results reveal the neural computation that appears to mediate causal inference behavior, including inferring a hidden common source and updating prior and sensory representations at different hierarchies.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral paradigm and hierarchical Bayesian causal inference model</title><p>Using a virtual-reality system, we trained three monkeys (monkeys H, N, and S) to reach for a visual target with their nonvisible (proprioceptive) arm while viewing a virtual arm moving in synchrony with a preset spatial visual-proprioceptive (VP) disparity (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). On each trial of the experiment, the monkeys were required to initiate the trial by placing their hand on the starting position (blue dot) for 1 s and were instructed not to move. After the initiation period, the starting point disappeared, and the visual virtual arm was rotated; this mismatch arm was maintained for 0.5 s as the preparation period. The reaching target was presented as a ‘go’ signal, and monkeys had to reach toward the visual target within 2.5 s and place their hand in the target area for 0.5 s, referred to as the target-holding period, to receive a reward. Any arm movement during the target-holding period automatically terminated the trial. The proprioceptive drift due to the disparity between visual and proprioceptive inputs was measured at the endpoint of the reach and was defined as the angle difference between the proprioceptive arm and the visual target (the estimated arm) (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, see details of animal training and reward in Materials and methods). In addition to this VP conflict (VPC) task, two control experiments were conducted: (i) where the visual and proprioceptive signals were perfectly aligned (VP task) and (ii) where there was only a proprioceptive signal (P task). The procedures of the three tasks (VPC, VP, and P) were identical, except that the visual or proprioceptive information presented to monkeys varied according to the context of the experiment (see Materials and methods). Using a block design, the order of three different blocks (tasks) in each training or recording session was randomized.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Behavioral task, the dynamic hierarchical causal inference model, and proprioceptive drift results.</title><p>(<bold>A</bold>) Overview of the behavioral task. The monkey was instructed to hold its proprioceptive arm over the starting position (blue dot) to initiate one trial. After the virtual visual arm rotation, a virtual red dot was presented, and the monkey was required to place its proprioceptive arm on the target and hold it to get a reward. (<bold>B</bold>) Schematic drawing of reward area, proprioceptive drift, and the different types of arms (proprioceptive and virtual/visual). Here, proprioceptive drift was defined as the rotated degree from the proprioceptive arm position to the estimated arm position (the same as the target location) measured from the shoulder. The reward area is defined by the green area, which ensures the monkey performed the task rationally and without visual feedback (see animal training in Materials and methods). (<bold>C–F</bold>) Schematic drawing of the dynamic hierarchical causal inference model. V-arm, visual arm signal; P-arm, proprioceptive arm signal; <italic>C</italic>=1: both V-arm and P-arm come from a common source; <italic>C</italic>=2: V-arm and P-arm come from different sources. (<bold>G</bold>) Example behavioral results from one session of one monkey (also see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). CCW, counterclockwise; CW, clockwise. The black line represents raw data. The gray line represents the Bayesian causal inference (BCI) model fitting result. The black dots represent experimental trials and the gray dots represent simulated trials by the BCI model.</p><p><supplementary-material id="fig1scode1"><label>Figure 1—source code 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig1">Figure 1G</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig1-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig1">Figure 1G</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Behavior performance and causal inference model predict results in individual monkeys.</title><p>(<bold>A</bold>) The pattern of drift is consistent across all three monkeys (black lines and dots), and the predictions of the causal inference model (gray lines and dots) characterized the monkeys’ behavior data. Each dot represents a single trial, and lines represent the average result. The blue and orange solid lines represent the visual and proprioceptive bias, respectively. (<bold>B</bold>) Model prediction of the posterior probability of a common source (<italic>P<sub>com</sub></italic>). Each dot represents the averaged <italic>P<sub>com</sub></italic> in a cluster grouped by the disparity and drift based on the monkey’s behavior. (<bold>C</bold>) Average <italic>P<sub>com</sub></italic> as the function of disparity. The black lines represent the average <italic>P<sub>com</sub></italic> of each monkey.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Histograms of behavior and model-simulated results in an example session.</title><p>The predictions of the causal inference model (red histograms) significantly better characterized monkeys’ behavior data (blue histograms) than the predictions of the forced-fusion (optimal integration) model (green histograms). The details of model comparisons are shown in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. Data distributions were plotted using an example session. Model results were generated from 5000 trials per disparity and were plotted the distribution using all these simulated trials. Vertical black solid and dotted gray lines indicate the normalized pure proprioceptive and pure visual bias, respectively. CI: causal inference; FF: forced-fusion.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig1-figsupp2-v2.tif"/></fig></fig-group><p>We used this paradigm to test the hypothesis of the causal inference process, which predicts how the brain infers and updates hidden structures on the basis of multiple sensory inputs.</p><p>First, the BCI model encodes probability distributions over the sensory (visual and proprioceptive) signals and incorporates rules that govern how a prior belief about the sensory causal structure is combined with incoming information to judge the event probability in proprioception (<xref ref-type="fig" rid="fig1">Figure 1C–F</xref>). Thus, the monkey’s behavior output (the proprioceptive drift distribution under each disparity) should show the dynamics of integration and segregation, which is the hallmark of causal inference. That is, the drift should increase for small disparities and decrease when the disparities become larger (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Körding et al., 2007</xref>).</p><p>Second, according to the model, the prior of common source in the current trial should be modulated by the experience of the environmental structure. Thus, at a single trial level, the prior of a common source in the current trial should be updated based on the posterior of a common source from previous trials (<xref ref-type="fig" rid="fig1">Figure 1C and D</xref>).</p><p>Third, the sensory uncertainty is also proposed to update to maintain consistency with the prior beliefs of the causal structure of the world (<xref ref-type="bibr" rid="bib30">French and DeAngelis, 2020</xref>). Therefore, the sensory uncertainty should increase when there is a conflict between the proprioceptive and visual signals (e.g., the VPC task) (<xref ref-type="fig" rid="fig1">Figure 1D and E</xref>).</p><p>To test these hypotheses, we adopted the Bayesian causal inference (BCI) model to assess monkeys’ behavior and investigated whether the neural activities in multiple brain regions correlate to proposed components in the behavior.</p></sec><sec id="s2-2"><title>The probability of common source in monkey’s behavior</title><p>To examine whether the monkeys inferred the causal structure during multisensory processing, we first examined the proprioceptive drift as a function of disparity in the VPC task. Overall, the three monkeys showed a very consistent behavioral pattern, with the proprioceptive drift increasing for small levels of disparity and plateauing or even decreasing when the disparity became larger (e.g., exceeded 20°) (<xref ref-type="fig" rid="fig1">Figure 1G</xref>; for data on individual monkeys, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The BCI model qualitatively explains the nonlinear dependence of drift as a function of disparity. For small disparities, there is a high probability that the proprioceptive and visual signals came from the same source. Hence, the visual information is fully integrated with the proprioceptive information. For large disparities, however, the proprioceptive and visual signals are likely from different sources, leading to a breakdown of integration and consideration of only the proprioceptive information (segregation). In this case, visual information has a weaker weight for perception. Consequently, the effect of disparity on the drift is reduced by shifting integration to segregation. The BCI model quantified the nonlinear dependence between disparity and proprioceptive drift to measure the posterior probability of a common source (<italic>P<sub>com</sub></italic>), the consequence of causal inference. We fitted the behavioral data using the BCI model. The results showed two signatures of the <italic>P<sub>com</sub></italic> pattern: (i) the averaged <italic>P<sub>com</sub></italic> decreased as the disparity increased (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left) and (ii) within each disparity, especially the large ones, the <italic>P<sub>com</sub></italic> decreased as the proprioceptive drift decreased (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right) (see individual monkeys’ behavior in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The causal inference model predicts the dynamic updating of monkey behavior.</title><p>(<bold>A</bold>) Left: The average <italic>P<sub>com</sub></italic> as a function of disparity. The black line represents the average <italic>P<sub>com</sub></italic> across monkeys. The dashed lines represent the average <italic>P<sub>com</sub></italic>s across sessions of three monkeys separately. Error bars indicate standard errors of the means (SEMs). Right: Model prediction of the <italic>P<sub>com</sub></italic>. Each point represents the average <italic>P<sub>com</sub></italic> in each cluster grouped by specific disparity and proprioceptive drift according to the clustering of disparity and drift (see Materials and methods). (<bold>B</bold>) The transition probability from the previous trial’s <italic>P<sub>com</sub></italic> to the current trial’s <italic>P<sub>prior</sub></italic>. Left: The transition probability of an example session. Right: The transition probabilities across all sessions from three monkeys (Wilcoxon signed-rank test, <italic>W</italic>=6996.0, <italic>df</italic> = 242, p&lt;0.001, <italic>r</italic><sub><italic>rb</italic></sub> = 0.52). Each circle represents a behavior session. The most right insert scatter represents a single session’s transition probability. (<bold>C</bold>) After-trial effect of sensory updating. Left: The distribution of arm locations in P blocks after visual-proprioceptive (VP) and VP conflict (VPC) tasks in an example session. The solid lines represent fitted Gaussian distributions. Right: The standard deviations of drift in P blocks after VP and VPC tasks across all sessions from three monkeys in early trials (Wilcoxon signed-rank test, <italic>W</italic>=851.0, <italic>df</italic> = 72, p=0.012, false-discovery rate [FDR] corrected, see Materials and methods, <italic>r</italic><sub><italic>rb</italic></sub> = 0.38) and in latter trials (<italic>W</italic>=1024.0, <italic>df</italic> = 72, p=0.073, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.24). The uncertainty of P trials after the VPC task in the early part of the session was significantly larger than that in the later part (<italic>W</italic>=917.0, <italic>df</italic> = 72, p=0.035, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.29); this is not the case for P trials after the VP task (<italic>W</italic>=1086.0, <italic>df</italic> = 72, p=0.15, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.20). (<bold>D</bold>) Within-trial effect of sensory updating. Left: The distribution of arm locations in VP and VPC (0°) tasks. The solid lines represent fitted Gaussian distributions. Right: The standard deviation of drift in VPC (0°) trials was significantly higher than that in VP trials (Wilcoxon signed-rank test, <italic>W</italic>=10,035.0, <italic>df</italic> = 237, p&lt;0.001, <italic>r</italic><sub><italic>rb</italic></sub> = 0.29). In (<bold>C</bold>) and (<bold>D</bold>), each circle represents a behavior session. The effect sizes (<italic>r</italic><sub><italic>rb</italic></sub>) were performed using the rank-biserial correlation (<xref ref-type="bibr" rid="bib44">Kerby, 2014</xref>). *p&lt;0.05; ***p&lt;0.001; n.s., not significant.</p><p><supplementary-material id="fig2scode1"><label>Figure 2—source code 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig2">Figure 2A-D</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig2-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig2">Figure 2A–D</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Sensory updating is not reflected in the mean of drift.</title><p>Left: Mean drift in P blocks after visual-proprioceptive (VP) and VP conflict (VPC) tasks. The boxes represent the means of drift in VP and VPC (0°) tasks across all sessions of all monkeys in the early part of the sessions (Wilcoxon signed-rank test, <italic>W</italic>=3591.0, <italic>df</italic> = 127, <italic>r</italic>=0.13, p=0.37, false-discovery rate [FDR] corrected) and the late part of the sessions (<italic>W</italic>=3749.0, <italic>df</italic> = 127, <italic>r</italic>=0.092, p=0.37, FDR corrected). Right: Mean of drift in VPC (0°) trials was not significantly different from that in VP trials (Wilcoxon signed-rank test, <italic>W</italic>=13,668.0, <italic>df</italic> = 242, <italic>r</italic>=0.078, p=0.29). Each circle represents a behavior session. The effect sizes (<bold><italic>r</italic></bold>) were performed using the rank-biserial correlation. n.s., not significant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>No significant difference of the divergence of eye fixation positions between visual-proprioceptive (VP) and VP conflict (VPC) (0°) tasks (see Materials and Methods, Wilcoxon signed-rank test, <italic>W</italic>=98.0, <italic>df</italic> = 19, p=0.81).</title><p>n.s., not significant.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig2-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-3"><title><italic>P<sub>com</sub></italic> in the current trial depended on the experience</title><p>More importantly, the model posits that not only the inference of the causal structure is based on visual and proprioceptive inputs but also the subsequent updating of (i) the prior belief of causal structure based on the experience (e.g., probability of a common source in the previous trials) and (ii) the uncertainty of sensory signals for the visual and proprioceptive recalibration (<xref ref-type="fig" rid="fig1">Figure 1C–F</xref>). To test these hypotheses, we first implemented the Markov analysis of the prior belief and <italic>P<sub>com</sub></italic> (see Materials and methods) to see whether the prior probability of a common source (<italic>P<sub>prior</sub></italic>) in the current trial depended on the previous <italic>P<sub>com</sub></italic> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The Markov model included the transition probability of <italic>P<sub>prior</sub></italic> between the current (<italic>n</italic>th) and previous (<italic>n</italic><sup>th</sup>− 1) trial to account for the trial-by-trial variability in spatial drifts observed in the three monkeys (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left). The fit to the model demonstrated that the <italic>P<sub>com</sub></italic> observed in the <italic>n</italic>th trial was significantly affected by that in the previous (<italic>n</italic><sup>th</sup>− 1) trial (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right, Wilcoxon signed-rank test, p&lt;0.001), indicating that the <italic>P<sub>com</sub></italic> was computed based on both <italic>P<sub>prior</sub></italic> from the previous trial and the sensory inputs, with their disparity, from the current trial. Note that the transition probabilities (<italic>P</italic><sub><italic>(C=1|C=1)</italic></sub> and <italic>P</italic><sub><italic>(C=1|C=2)</italic></sub>) remained relatively high (larger than 0.8 in three monkeys) because overall, the number of high <italic>P<sub>com</sub></italic> trials was much more than low <italic>P<sub>com</sub></italic> trials in either training or recording sessions. This was consistent with high baseline <italic>P<sub>prior</sub></italic> in three monkeys (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec><sec id="s2-4"><title>The common-source belief modulated the sensory uncertainty</title><p>We next examined whether the sensory representation is updated to maintain consistency with the causal structure of the environment. That is, the estimates of physical arm locations should tradeoff systematically depending on the current common-source belief (e.g., <italic>P<sub>com</sub></italic> in different tasks: VP, P, and VPC). For example, when the monkey incorrectly infers that the visual and proprioceptive arms come from the same source when a disparity is presented, the uncertainty of proprioception should increase to ‘explain away’ the conflict between the two inputs. According to this idea, since the block design in the current experiment resulted in P trials (in the P task) sometimes following the VPC task and other times following the VP task, we then reasoned that because the overall <italic>P<sub>com</sub></italic> was lower in the VPC task than in the VP task, the uncertainty of proprioception (i.e., the distribution of proprioceptive drifts in the P trials) would be larger after the VPC task than after the VP task. We analyzed the drift variation in P trials and found that, in the early trials (first third of each P block), the uncertainty of P trials following the VPC task was significantly larger than that following the VP task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, right, Wilcoxon signed-rank test, p=0.012). The increase in the uncertainty of proprioception was recovered in the late trials (last third of each P block), evident by a significant difference in the uncertainty between early and late P trials (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, right, Wilcoxon signed-rank test, p=0.035). The decrease in the uncertainty of proprioception was reasonable, as the tradeoff effect in the VPC task gradually recovered.</p><p>Furthermore, we hypothesized that if a tradeoff of sensory representation occurs during the process of causal inference, the tradeoff would also affect the uncertainty of VP integration in both VP and VPC tasks. We examined the distribution of proprioceptive drifts using the trials with 0° disparity in the VPC task, in which the V and P information were congruent, and compared it with the distribution in the VP task. As predicted, we found that the variance of the proprioceptive drift was significantly larger in the VPC task than in the VP task (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, right, Wilcoxon signed-rank test, p&lt;0.001). Note that the difference between VP and VPC (0°) tasks could not be explained by the divergence of eye fixation positions (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). As a control, we also investigated whether the mean of drift, representing the perceptual accuracy of the proprioceptive arm, was affected by the causal structure of the environment. We found there was no significant difference between the mean of drift for P trials following the VPC task and that following the VP task in both early parts (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, left, Wilcoxon signed-rank test, p=0.37, false-discovery rate [FDR] corrected) and late parts (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, right, Wilcoxon signed-rank test, p=0.37, FDR corrected). Besides these, we also found that the mean of proprioceptive drift was not updated in the VPC task compared with the VP task (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, right, Wilcoxon signed-rank test, p=0.29). Thus, these results supported the notion of a tradeoff in proprioception according to causal inference environments; that is, sensory representation’s uncertainty, not accuracy, is updated dynamically based on the task environment (<italic>P<sub>com</sub></italic>).</p><p>To summarize the above-described behavioral results, we found that monkeys’ proprioceptive drift shows a nonlinear dependency on the disparity between proprioceptive and visual input, which was well explained by the causal inference model. Second, we showed that the <italic>P<sub>com</sub></italic> integrated with VP sensory inputs and is updated by previous experience on a trial-by-trial basis. Third, to maintain a consistency of causal inference, sensory uncertainty, reflected by the variance of proprioceptive drift, is updated in the inference along with the change of <italic>P<sub>com</sub></italic>. Taken together, we established the behavioral paradigm in which monkeys infer the hidden cause by integrating prior information and sensory inputs while dynamically updating both <italic>P<sub>com</sub></italic> and sensory representation. The behavioral responses of the monkeys enabled us to examine the underlying neural mechanisms and functional circuits.</p></sec><sec id="s2-5"><title>Causal inference in individual premotor and parietal neurons</title><p>Previous studies showed that the premotor and parietal cortices were highly involved in body representation and multisensory perception (see reviews in <xref ref-type="bibr" rid="bib8">Blanke, 2012</xref>; <xref ref-type="bibr" rid="bib38">Graziano and Botvinick, 2002</xref>). In monkeys, bimodal neurons with visual and somatosensory receptive fields were found in both premotor (including F2vr in dorsal premotor and F4/F5 in ventral premotor) and posterior parietal cortices (including area 5 and area 7) (<xref ref-type="bibr" rid="bib29">Fogassi et al., 1999</xref>; <xref ref-type="bibr" rid="bib37">Graziano et al., 2000</xref>; <xref ref-type="bibr" rid="bib34">Graziano and Gross, 1993</xref>; <xref ref-type="bibr" rid="bib36">Graziano and Gross, 1998</xref>; <xref ref-type="bibr" rid="bib35">Graziano et al., 1994</xref>). Specifically, ventral premotor neurons responded to visual stimuli in the space adjacent to the arm (<xref ref-type="bibr" rid="bib36">Graziano and Gross, 1998</xref>; <xref ref-type="bibr" rid="bib35">Graziano et al., 1994</xref>). The bimodal neurons in the parietal cortex (area 5 and area 7) showed to respond to both the real arm position and the seen position of a dummy arm (<xref ref-type="bibr" rid="bib37">Graziano et al., 2000</xref>), which have a significant projection of the premotor cortex (<xref ref-type="bibr" rid="bib36">Graziano and Gross, 1998</xref>). Consistently, human fMRI studies found that the posterior parietal and premotor (dorsal and ventral) cortices selectively respond to visual stimulation near the hand (<xref ref-type="bibr" rid="bib11">Brozzoli et al., 2011</xref>) or the dummy hand near one’s corresponding hand (<xref ref-type="bibr" rid="bib9">Blanke et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Ehrsson et al., 2004</xref>). A human MEG study also revealed that the activities in the prefrontal and intraparietal sulcus were related to the causal inference computation in visual-auditory integration (<xref ref-type="bibr" rid="bib17">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>). Therefore, we determined to record from two brain regions, the premotor cortex (dorsal and ventral, 412 neurons) and parietal cortex (area 5 and area 7; 238 neurons), in the three monkeys performing the reaching tasks (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, for details, see Materials and methods). We first examined whether neurons in the premotor and parietal cortices during the target-holding period (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) were selective to basic task components, including condition (VP or P), arm location, and visual disparity. In the premotor cortex, 40% (163/412) of neurons were selective to condition, 23% to arm location, and 37% to visual disparity (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, upper panel). In the parietal cortex, 35% (83/238) of neurons were selective to condition, 27% to arm location, and 31% to visual disparity (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, lower panel, ANOVA, main effect, p&lt;0.05). We also examined the neural representations of the visual and proprioceptive arm locations in each trial during the target-holding period in the VPC, VP, and P tasks, measured by a bias-corrected percent explained variance (ωPEV) (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Both brain regions conveyed vital information about the arm location in the three tasks. In the VP and P tasks with no VP disparities, both premotor and parietal cortices showed similar visual and proprioceptive arm information (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). However, when disparities were introduced in the VPC task, the premotor cortex showed a more robust signal for visual arm information (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). In contrast, the parietal cortex showed stronger signals for information related to the proprioceptive arm (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Casual inference neurons in premotor and parietal cortices.</title><p>(<bold>A</bold>) Recording sites. Left: Two regions of interest were recorded through single electrodes in macaque monkeys. Middle and right: Specific recording sites in three monkeys. AS, arcuate sulcus; CS, central sulcus; IPS, intraparietal sulcus; SPD, superior precentral dimple. L, left hemisphere; R, right hemisphere; A, anterior; P, posterior; M, medial. The straight dash gray line separated the dorsal and ventral part of the premotor cortex in the middle panel. The straight dash gray line indicates the middle of IPS and CS. The circular dash lines indicate the recording chambers. (<bold>B</bold>) Temporal structure of a single trial for the visual-proprioceptive conflict (VPC) task. (<bold>C</bold>) Neural information of arm locations in premotor and parietal cortices. Upper: No significant difference between the brain regions for the neural information of VP arm (Wilcoxon rank-sum test, <italic>W</italic>=0.64, <italic>df</italic><sub><italic>premotor</italic></sub> = 411, <italic>df</italic><sub><italic>parietal</italic></sub> = 237, p=0.52, false-discovery rate [FDR] corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.030) and P arm (<italic>W</italic>=0.51, <italic>df</italic><sub><italic>premotor</italic></sub> = 411, <italic>df</italic><sub><italic>parietal</italic></sub> = 237, p=0.52, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.031), respectively. Bottom: There were significant differences between the brain regions for both the neural information of proprioceptive arm (Wilcoxon rank-sum test, <italic>W</italic>=–3.92, <italic>df</italic><sub><italic>premotor</italic></sub> = 411, <italic>df</italic><sub><italic>parietal</italic></sub> = 237, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.18) and visual arm (<italic>W</italic>=6.34, <italic>df</italic><sub><italic>premotor</italic></sub> = 411, <italic>df</italic><sub><italic>parietal</italic></sub> = 237, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.30) in VPC task, respectively. Both brain regions conveyed significant information about the arm location in the three tasks (premotor: VP arm, Wilcoxon signed-rank test, <italic>W</italic> = 27,712.0, <italic>df</italic> = 474, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.35; P arm, <italic>W</italic> = 25,614.0, <italic>df</italic> = 411, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.40; proprioceptive arm (VPC), <italic>W</italic>=22,316.0, <italic>df</italic> = 411, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.48; visual arm (VPC), <italic>W</italic>=14,874.0, <italic>df</italic> = 411, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.65. Parietal: VP arm, <italic>W</italic>=9466.0, <italic>df</italic> = 237, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.33; P arm, <italic>W</italic>=7414.0, <italic>df</italic> = 237, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.48; proprioceptive arm (VPC), <italic>W</italic>=3745.0, <italic>df</italic> = 237, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.74; visual arm (VPC), <italic>W</italic>=10,138.0, <italic>df</italic> = 237, p&lt;0.001, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.29). Each circle indicates a neuron. The effect sizes (<italic>r</italic><sub><italic>rb</italic></sub>) were performed using the rank-biserial correlation. (<bold>D</bold>) Raster plots and mean firing rates from an example neuron in the parietal cortex that exhibited responses varied with visual disparity, showing the preference for the P task during the target-holding period (gray zones). The yellow curve was fitted with a von Mises distribution. (<bold>E</bold>) Schematic drawing of VP weight analysis (see Materials and methods) in one example trial for the VPC task. In brief, we first mapped the tuning curves of arm position in VP (left red curve) and P (left blue curve) tasks as integration and segregation templates, respectively. Then, during the VPC task, for a single trial, we mapped the visual and proprioceptive arm position onto the these templates to get the probabilities of integration and segregation. Then, we normalized the probability to get the VP weight. (<bold>F</bold>) Two examples of causal inference neurons in premotor and parietal cortices during the target-holding period (the same neurons shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> and (<bold>D</bold>), respectively). Each point represents one single trial, and the color represents the value of VP weight. The color bar represents VP weight, larger values indicate higher VP weights (higher probability of integration). (<bold>G</bold>) Population causal inference patterns in two brain regions. Each point was a pseudo-trial that was generated through bootstrapping, and the color represents the value of VP weight. (<bold>H</bold>) An example neuron in the parietal cortex shows the causal inference pattern defined by a significant positive correlation between VP weight and <italic>P<sub>com</sub></italic> (Pearson correlation). Each point represents the average <italic>P<sub>com</sub></italic> and VP weight in a cluster from the behavioral <italic>P<sub>com</sub></italic> pattern. The solid line was fitted with linear regression, and the shaded area indicates the 95% confidence interval. The bar plot represents the fraction of causal inference neurons in the premotor cortex and parietal cortex. ***p&lt;0.001.</p><p><supplementary-material id="fig3scode1"><label>Figure 3—source code 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig3">Figure 3C, D, F, G and H</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig3-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig3">Figure 3C, D, F, G and H</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Heterogeneity in the responses of neurons to task components in the premotor and parietal cortices.</title><p>(<bold>A</bold>) The fraction of selective neurons in the premotor and parietal cortices (ANOVA, main effect, p&lt;0.05). Red: selective to condition; blue: selective to arm location; green: selective to the visual disparity. The number in the circular means how many neurons are significantly selective to task components. (<bold>B</bold>) Example neurons selective to the the condition (left panel), the arm location (middle panel), or the disparity (right panel). The upper row shows the premotor neurons; the low row means parietal neurons. The error bar indicates the SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Left and middle: Raster plots and mean firing rates from an example neuron in the premotor cortex that exhibited responses varied with visual disparity that preferred to the P task during the target-holding period (gray zones).</title><p>Right: Mean activity during the target-holding period. The yellow curve was fitted with a von Mises distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Histograms of Pearson correlation coefficients between eye fixation position and visual-proprioceptive (VP) weight.</title><p>The correlation coefficients of both regions (premotor and parietal cortices) at both directions (horizontal and vertical) are not significantly different from 0 (Wilcoxon signed-rank test, premotor (horizontal): <italic>W</italic>=1217.0, <italic>df</italic> = 77, p=0.11; premotor (vertical): <italic>W</italic>=1506.0, <italic>df</italic> = 77, p=0.86; parietal (horizontal): <italic>W</italic>=435.0, <italic>df</italic> = 44, p=0.35; parietal (vertical): <italic>W</italic>=503.0, <italic>df</italic> = 44, p=0.87).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig3-figsupp3-v2.tif"/></fig></fig-group><p>Next, to define causal inference response in the VPC task at the single-neuron and single-trial levels, we utilized the VP and P tasks to characterize neural responses, as these tasks involve expected stereotypical behaviors in the two extreme regimes: full integration and segregation. Thus, neurons that are more active during the P task are likely candidates for ‘segregation (P) neurons’, which exhibited increased activity under the large disparities in the VPC task (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). By contrast, neurons that are more active during the VP task reflect a preference for integrating congruent VP information and, hence, constitute a natural candidate for ‘integration (VP) neurons’ (example in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). We then implemented a linear probabilistic model which combined how the neural response pattern aligned with the VP and P response profiles and used this model to implement a probabilistic decoding analysis to calculate the probability of VP or P (VP weight = <italic>P<sub>vp</sub></italic>/[<italic>P<sub>vp</sub> + P<sub>p</sub></italic>]) based on the firing rate in each trial (<xref ref-type="fig" rid="fig3">Figure 3E</xref>; also see Materials and methods). Thus, a larger VP weight for a single trial denotes a higher probability of integration (high <italic>P<sub>com</sub></italic>). We first focused on the target-holding period in a trial, as the neurons could well display their spatial tunings when monkeys holding their arms on the target. We found that both premotor and parietal cortices carry information about <italic>P<sub>com</sub></italic> at the single-neuron (<xref ref-type="fig" rid="fig3">Figure 3F</xref>; the same example neurons in <xref ref-type="fig" rid="fig3">Figure 3D</xref>) and population levels (<xref ref-type="fig" rid="fig3">Figure 3G</xref>; see Materials and methods) during the target-holding period. That is, the VP weight of the neuron or population progressively decreased along with the disparity, and in trials with large disparity (e.g., 35° and 45°), the neuron(s) had a higher VP weight when the drift was large (i.e., the monkey integrated the visual information; thus, a high <italic>P<sub>com</sub></italic> predicted by the BCI model) and shifted gradually toward higher P weights when the drift shifted to 0 (i.e., the monkey segregated the visual information; thus, a low <italic>P<sub>com</sub></italic> predicted by the BCI model). The VP weight was highly correlated with the <italic>P<sub>com</sub></italic> from behavior (<xref ref-type="fig" rid="fig3">Figure 3H</xref>). Note that the premotor cortex had a slightly higher proportion of causal inference neurons (11.7%) than the parietal cortex (7.6%, Pearson chi-square test, <italic>χ</italic><sup>2</sup>=2.33, p=0.063).</p><p>As neuronal activities in the premotor and the parietal cortices are reported to correlate with the eye position in the reaching task (<xref ref-type="bibr" rid="bib13">Buneo and Andersen, 2006</xref>; <xref ref-type="bibr" rid="bib60">Pesaran et al., 2006</xref>), one might ask whether the <italic>P<sub>com</sub></italic> signals can be explained by the eye position. However, the result showed that the VP weights in the population could not be predicted by eye fixation positions during the target-holding period (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p></sec><sec id="s2-6"><title>Population states encode <italic>P<sub>com</sub></italic> during causal inference</title><p>We next focused on the overall populations of neurons in both regions and asked whether and how their population states reflect the uncertainty of causal structure, <italic>P<sub>com</sub></italic>. We were guided by the results from single-neuron analyses during the target-holding period described above, in which neurons responsive to high <italic>P<sub>com</sub></italic> (prefer integration) are more likely to show neural tuning similar to that during the VP task, and neurons responsive to low <italic>P<sub>com</sub></italic> (prefer segregation) show a tuning profile similar to that in the P task. We thus hypothesized that neural components or subspaces embedded in the population activity represent the dynamic change in the coding of <italic>P<sub>com</sub></italic> in the VPC task, which would lie between the components representing the VP and P profiles. Furthermore, the computation of <italic>P<sub>com</sub></italic> in the BCI model is determined by the relation and disparities between the visual information from the artificial arm and proprioceptive information from the monkey’s actual arm. In other words, according to the model, the causal inference can be constructed before the visual target appears, and the participant uses this information to guide the reach. We thus further hypothesized that the dynamics of the population states also reflect the <italic>P<sub>com</sub></italic> during the preparation period, during which there is no motor planning or preparation.</p><p>Thus, we grouped trials from each neuron into high and low <italic>P<sub>com</sub></italic> classes according to the drift under each disparity (high, the top third of the trials [in red]; low, bottom third of the trials [in blue]) (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We conducted demixed principal component analysis (dPCA) to visualize any neural component that represents the <italic>P<sub>com</sub></italic> in the VPC task in relation to that in the VP and P tasks (see Materials and methods). dPCA decomposes population activity into a set of dimensions that each explain the variance of one factor of the data (<xref ref-type="bibr" rid="bib47">Kobak et al., 2016</xref>). We included the factors of time, arm location, and <italic>P<sub>com</sub></italic> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In the analysis, VP and P trials were included, which served as the templates of integration and segregation, respectively. As shown in the schema (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), if the decomposed neural components indeed represent the <italic>P<sub>com</sub></italic>, the population activity of high and low classes in this subspace should lie between that of the VP and P classes and the four classes (high, low, VP, and P) should be separated from each other. The dPCA results indicated that the <italic>P<sub>com</sub></italic> components, unrelated to the arm location, represented 29.9% and 20.5% of the total firing rate variance in the premotor and parietal cortices, respectively (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, in red). Notably, the activity in <italic>P<sub>com</sub></italic> dimensions seems consistent with our hypothesis, demonstrating the dynamics of <italic>P<sub>com</sub></italic> between integration (VP) and segregation (P). In addition, compared to the activity in the parietal cortex, the neural trajectories of the premotor populations showed an earlier divergence in <italic>P<sub>com</sub></italic> dimensions (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Dynamic population decoding of <italic>P<sub>com</sub></italic>.</title><p>(<bold>A</bold>) Schematic drawing of the high <italic>P<sub>com</sub></italic> group (top third of trials) and the low <italic>P<sub>com</sub></italic> group (bottom third of trials) based on the relative drift (drift/disparity). (<bold>B</bold>) Schematic drawing of the demixed principal component analysis (dPCA). All trials of each neuron were grouped into 20 classes (5 targets × 4 conditions, including visual-proprioceptive (VP) and P tasks and high and low groups in the VP conflict [VPC] task). The marginalization matrix was generated by averaging all trials in each class. (<bold>C</bold>) dPCA decomposes population activity into a set of components given the task parameters of interest. (<bold>D</bold>) Temporal evolution of dPCA components of <italic>P<sub>com</sub></italic>. The gray points represent the disparity onset; the black points represent the target onset. (<bold>E</bold>) Population decoding of <italic>P<sub>com</sub></italic>. The decoding accuracy was plotted as a function of time. The gray shaded area represents the preparation period. The horizontal dashed black line represents the chance level. The horizontal solid-colored bars at the top represent the time of significant decoding accuracy (cluster-based permutation test, p&lt;0.05). Shaded areas indicate 95% confidence intervals. (<bold>F</bold>) Joint peri-event canonical correlation (jPECC) results averaged across all sessions. Left: x-axis represents the time of parietal from target onset; y-axis: defines the time of premotor from target onset. The color bar represents the cross-validated correlation coefficient. Right: Lead-lag interactions as a function of time relative to target onset. The horizontal black bar represents the time of significant jPECC asymmetry index versus shuffled data (cluster-based permutation test, p&lt;0.05).</p><p><supplementary-material id="fig4scode1"><label>Figure 4—source code 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig4">Figure 4C–F</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig4-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig4">Figure 4C–F</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Joint peri-event canonical correlation (jPECC) analysis with shuffled temporal alignment trials.</title><p>To determine whether correlations occur with a temporal offset between premotor and parietal cortices after shuffling the trials’ alignment. Left: Cross-validated correlation coefficient between premotor and parietal cortices. The trial’s temporal alignment was shuffled to determine whether correlations occur with a temporal offset between the paired brain regions. Right: The black line represents the lead-lag interactions as a function of time relative to target onset, and the gray dashed line represents the chance level (chance level = 0).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Population decoding of <italic>P<sub>com</sub></italic> in the premotor and parietal cortices from Monkey N.</title><p>The decoding accuracy was plotted as a function of time. The horizontal dashed black line represents the chance level. The horizontal solid colored bars at the top represent the time of significant decoding accuracy (cluster-based permutation test, p&lt;0.05). Shaded areas indicate 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>The <italic>P<sub>com</sub></italic> information occurred significantly earlier in the premotor cortex than in the parietal cortex.</title><p>The null distribution of time lags (blue bar) was generated by a corresponding number of neurons randomly exchanged 1000 times between paired regions and their significance was determined by permutation tests of the observed time lags (black line) of the original data and the null distribution (see Materials and methods). The blue curve was fitted with a Gaussian distribution. **p&lt;0.01, permutation test.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig4-figsupp3-v2.tif"/></fig></fig-group><p>To further quantify their dynamics statistically, we trained a linear support vector machine (SVM) using pooled activities in each brain region throughout the entire trial. The dynamic decoding results showed that the <italic>P<sub>com</sub></italic> information is correctly predicted by neuronal population activities in both areas after target onset but is decoded only by premotor neurons during the preparation period when there was no visual target or motor preparation (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, cluster-based permutation test, p&lt;0.05). Randomization test confirmed the time difference that the <italic>P<sub>com</sub></italic> information occurred significantly earlier in the premotor cortex than the parietal cortex (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>, randomization test, p&lt;0.01, see Materials and methods). This may suggest that the premotor cortex is where causal inference is computed and sends the information to the parietal cortex during the reaching period.</p><p>Next, we tested the relationship between the population activities in the two areas. We performed a joint peri-event canonical correlation (jPECC) analysis, which detects correlations in a ‘communication subspace’ between two brain regions (<xref ref-type="bibr" rid="bib72">Steinmetz et al., 2019</xref>). In brief, we conducted a canonical correlation analysis for every pair of time points containing the population neural firing rates from the two regions. If the shared neural activity emerges at different times in the two areas, that is, activity in one region potentially leads to activity in the other, then we should observe a temporal offset between them. The jPECC results revealed a significant time lag for activity correlations between premotor and parietal areas in <italic>P<sub>com</sub></italic> dimensions (<xref ref-type="fig" rid="fig4">Figure 4F</xref>, cluster-based permutation test, p&lt;0.05), suggesting a potential feedback signal of <italic>P<sub>com</sub></italic> from the premotor cortex to the parietal cortex. As a control, we performed the same procedure with misalignment trials (see Materials and methods) to exclude the probability that the observed time lag resulted from the intrinsic temporal property of neuronal activities in these regions. There was no significant time lag between premotor and parietal areas when the trials were misaligned (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p></sec><sec id="s2-7"><title>Experience-dependent <italic>P<sub>com</sub></italic> in the premotor cortex</title><p>The behavioral experiments showed that the <italic>P<sub>com</sub></italic> could be updated by previous sensory experience on a trial-by-trial basis. To test the effect of the previous <italic>P<sub>com</sub></italic> on the causal inference in each trial, we examined neural activities during the baseline period in the VPC task before a disparity in the visual and proprioceptive arm was introduced (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We again classified the trials according to high and low <italic>P<sub>com</sub></italic>. <xref ref-type="fig" rid="fig5">Figure 5A</xref> depicts the results from an example premotor neuron, showing that during the baseline period, the neural activity exhibited selectivity toward the previous trial’s <italic>P<sub>com</sub></italic>, and at the same time, its neural trajectories in high and low prior classes lay between the VP and P templates. Of 412 neurons in the premotor cortex, 29 (7.0%) showed such selectivity in the previous trial (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Premotor neurons encode prior information (previous trial’s <italic>P<sub>com</sub></italic>) during the baseline period.</title><p>(<bold>A</bold>) Example neuron in the premotor cortex showing selectivity to prior information during the baseline period. The trials in the raster plot were sorted by the <italic>P<sub>com</sub></italic> in the previous trial and grouped into high (red dots) and low (blue dots) groups. Bottom: temporal evolution of the average firing rate of ‘high prior’ and ‘low prior’ groups. The black horizontal line at the top represents the time window with a significant difference (two-sided <italic>t</italic>-test, <italic>t</italic>=2.36, p=0.019). Shaded areas indicate SEMs. (<bold>B</bold>) Dynamic population decoding of prior information (<italic>n</italic>th–1 trial). The gray shaded window represents the baseline period. The horizontal solid colored bar at the top represents the time with significant decoding accuracy with a cluster-based permutation test (p&lt;0.05). Shaded areas indicate 95% confidence intervals. The horizontal dashed black line represents the chance level. (<bold>C</bold>) Decoding accuracy of prior trials (<italic>n</italic>th−1 to <italic>n</italic>th−4). Lag 0 represents the decoding of <italic>P<sub>com</sub></italic> in the current (<italic>n</italic>th) trial. The horizontal dashed black line represents the chance level (permutation test, p&lt;0.001). The solid lines were fitted with exponential functions. Error bars indicate 95% confidence intervals. (<bold>D</bold>) Schematic drawing of orthogonal subspaces of <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic>. The solid-line circles represent <italic>P<sub>com</sub></italic> and dotted circles represent <italic>P<sub>prior</sub></italic>. Red represents high <italic>P<sub>com</sub></italic>, blue represents low <italic>P<sub>com</sub></italic>. (<bold>E</bold>) Left: Percentage of baseline-period (<italic>P<sub>prior</sub></italic>) data variance (black bars, explained variance: about 99.63%) and target-holding period data variance (gray bars, explained variance: about 8.34%) explained by the top 10 prior PCs. Right: Percentage of baseline-period (<italic>P<sub>prior</sub></italic>) data variance (black bars, explained variance: about 11.30%) and target-holding (<italic>P<sub>com</sub></italic>) period data variance (gray bars, explained variance: about 99.99%) explained by the top 10 <italic>P<sub>com</sub></italic> PCs. (<bold>F</bold>) Premotor encoded prior information during the baseline period quickly decreased after the disparity onset while the <italic>P<sub>com</sub></italic> information emerged. The orange line represents the population decoding accuracy of <italic>P<sub>prior</sub></italic> (<italic>n</italic>th–1 trial). The black line represents the population decoding accuracy of <italic>P<sub>com</sub></italic>. The orange and black horizontal solid-colored bars at the top represent the time with significant decoding accuracy with a cluster-based permutation test (p&lt;0.05) for prior information and <italic>P<sub>com</sub></italic> information, respectively. *p&lt;0.05.</p><p><supplementary-material id="fig5scode1"><label>Figure 5—source code 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig5">Figure 5A–F</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig5-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig5">Figure 5A–F</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Percentage of prior selective neurons and causal inference (CI) neurons.</title><p>Red, the number of pure prior selective neurons; blue, the number of pure CI neurons; purple, the number of dual selective neurons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Population decoding of disparity.</title><p>The decoding accuracy was plotted as a function of time. The gray shaded area represents the preparation period. The horizontal dashed black line represents the chance level. The horizontal solid colored bars at the top represent the time of significant decoding accuracy (cluster-based permutation test, p&lt;0.05). Shaded areas indicate 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig5-figsupp2-v2.tif"/></fig></fig-group><p>To further test the relation between baseline neural activity and behavior quantitatively, we examined whether the population activities of these neurons can predict the <italic>P<sub>com</sub></italic> from previous trials. We trained an SVM using pooled activities across recording sessions. The previous <italic>P<sub>com</sub></italic> was only correctly decoded from the baseline activity in the premotor cortex (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, cluster-based permutation test, p&lt;0.05). Moreover, only recent experience (<italic>n</italic>th−1 trial) had a significant impact on the current trial (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, permutation test, p&lt;0.001).</p><p>As both <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> were represented in premotor neural activities, we wanted to examine their relationship in the neural states. We first found that very few neurons responded to both information types (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We then hypothesized that <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> might be represented independently at a population level. To validate this hypothesis, we conducted PCA on the population activities during baseline and target-holding periods for <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic>, respectively. If they are independent, the subspaces of <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> will be near orthogonal, and the PCs of <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> will capture little variance from each other (<xref ref-type="bibr" rid="bib25">Elsayed et al., 2016</xref>). To quantify this, we projected the <italic>P<sub>prior</sub></italic> data onto the <italic>P<sub>com</sub></italic> subspace to calculate the percent variance explained by the <italic>P<sub>com</sub></italic> PCs and repeated the same procedure for the <italic>P<sub>com</sub></italic> data (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). The results show that the top 10 <italic>P<sub>prior</sub></italic> PCs captured very little <italic>P<sub>com</sub></italic> variance; similarly, the top 10 <italic>P<sub>com</sub></italic> PCs captured very little <italic>P<sub>prior</sub></italic> variance (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). These results support the hypothesis that the two information types are represented independently in the premotor cortex. However, such independence between <italic>P<sub>com</sub></italic> and <italic>P<sub>prior</sub></italic> could also be caused by their different temporal structures in the task. Thus, we examined their neural dynamics within a trial. <xref ref-type="fig" rid="fig5">Figure 5F</xref> shows the time course of decoding results of prior and posterior information, where the <italic>P<sub>prior</sub></italic> quickly decreased after the disparity onset. At the same time, the <italic>P<sub>com</sub></italic> information increased and was retained until the end of the trial. These results demonstrated the dynamics in the computation of causal inference, where the information from the last trial is only preserved transiently and then used to integrate with sensory inputs to generate <italic>P<sub>com</sub></italic> information.</p></sec><sec id="s2-8"><title>Update sensory uncertainty of arm location in the parietal cortex</title><p>Finally, we investigated the neural activities associated with updating sensory uncertainty. The behavior results revealed a significantly greater uncertainty of proprioception in VP trials in the VPC task (low belief of a common source) than in the VP task (high belief of a common source) (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). We hypothesized that the sensory signals, which were used to make causal inference, in turn, updated their neuronal tunings to match inferred causal structure. We first examined the difference in neural tuning for arm location using the VP trials in the VP and VPC (VPC (0°), trials with no disparity) tasks. To test whether the tuning functions of arm location selective neurons changed between the VP condition and VPC (0°) condition at the single-neuron level, we fitted the tuning curve with the von Mises distribution by using the neuron response in different arm locations (five levels: [−30°, −20°, 0°, 20°, and 30°]) for these two conditions respectively (see Materials and methods). We found that the averaged firing rates during the target-holding period under the VP condition were higher than that under the VPC condition (0°) in the parietal cortex (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, left, Wilcoxon signed-rank test, p=0.017) but not in the premotor cortex (p=0.71). The gain index under VP condition were higher than the VPC condition (0°) in the parietal cortex (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, middle, Wilcoxon signed-rank test, p=0.0016, FDR corrected) but not the premotor cortex (p=0.11, FDR corrected). <xref ref-type="fig" rid="fig6">Figure 6A</xref> (right) shows an example neuron from the parietal cortex tuned to the center (0°) of arm location in the VP task, and the tuning range/uncertainty of the arm location was broader/lower in the VPC task. Here, for visualization purposes, we selected the time point when this neuron demonstrated the highest difference of ωPEV in the VP trials between VP and VPC tasks for the tuning calculation (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, left, peak delta ωPEV). The averaged dynamic spatial selectivity of all neurons revealed a significant decrease of the total spike rate variance explained by the arm location in the parietal cortex but not in the premotor cortex (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, cluster-based permutation test, p&lt;0.05). Note that the updating of sensory uncertainty was not correlated with the uncertainty of eye position between VP and VPC (0°) tasks (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Representation of arm location is updated in the parietal cortex.</title><p>(<bold>A</bold>) Left: The difference of ωPEV between visual-proprioceptive (VP) and VP conflict (VPC) (0°) tasks for an example neuron in the parietal cortex. Right: Snapshot of the arm location tuning for VP and VPC (0°) tasks at the time point showed in the left panel (peak delta ωPEV). At the given time point of this neuron, there is no significant main effect for condition (two-way ANOVA, condition VP and VPC (0°)×hand location; condition, <italic>F</italic><sub>(1,154)</sub>=1.450, p=0.23), but for hand location (<italic>F</italic><sub>(4,154)</sub>=6.736, p&lt;0.001). The solid curves were fitted with von Mises distributions. (<bold>B</bold>) Dynamic average ωPEV for VP and VPC (0°) tasks. The horizontal bar at the top represents the time bins in which the ωPEV for the VPC (0°) task was significantly lower than that for the VP task (cluster-based permutation test, p&lt;0.05). (<bold>C</bold>) Dynamic population decoding of arm locations. The horizontal bar at the top represents the time bins in which the decoding accuracy for the VPC (0°) task was significantly lower than that for the VP task (cluster-based permutation test, p&lt;0.05). Shaded areas indicate 95% confidence intervals. The horizontal dashed black lines represent the chance level.</p><p><supplementary-material id="fig6scode1"><label>Figure 6—source code 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig6">Figure 6A–C</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig6-code1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig6">Figure 6A–C</xref>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-76145-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>The comparisons of tuning curve parameters between the visual-proprioceptive (VP) and VP conflict (VPC) (0°) tasks.</title><p>Left: The spontaneous firing rates during VP task were higher than that during VPC (0°) task in the parietal cortex (parietal, <italic>W</italic>=646.0, <italic>df</italic> = 63, p=0.017, false-discovery rate [FDR] corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.38) but not in the premotor cortex (<italic>W</italic>=2090.0, <italic>df</italic> = 92, p=0.71, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.044). Middle: The gain index during VP task was higher than that during VPC (0°) task in the parietal cortex (parietal, <italic>W</italic>=539.0, <italic>df</italic> = 63, p=0.0016, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.48) but not the premotor cortex (<italic>W</italic>=1767.0, <italic>df</italic> = 92, p=0.11, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.19). Right: The preferred arm location was not significantly changed in the premotor and parietal cortices (premotor, <italic>W</italic>=1964.0, <italic>df</italic> = 92, p=0.50, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.081; parietal, <italic>W</italic>=897.0, <italic>df</italic> = 63, p=0.50, FDR corrected, <italic>r</italic><sub><italic>rb</italic></sub> = 0.14). Pair-wise comparisons were performed using Wilcoxon signed rank test. Effect sizes (<italic>r</italic><sub><italic>rb</italic></sub>) were performed using the rank-biserial correlation. *p&lt;0.05; **p&lt;0.01.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-fig6-figsupp1-v2.tif"/></fig></fig-group><p>Furthermore, at the population level, we performed the SVM decoding analysis of arm locations and found that only the parietal cortex showed a significantly decreased decoding accuracy in the VPC task (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, cluster-based permutation test, p&lt;0.05). We also confirmed that the change of decoding accuracy in the parietal cortex was significantly larger than the change in the premotor cortex (two-way ANOVA, Condition (<italic>VP and VPC (0°)</italic>)×Region (<italic>premotor and parietal</italic>), significant interaction effect, p&lt;0.05).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our data of behavior and multi-area neural recordings revealed, for the first time, the dynamic computation of causal inference in the frontal and parietal regions at single-neuron resolution during multisensory processing. Complementary to the previous findings focused on the feedforward sequential processing of BCI, the present results demonstrate parallel top-down processing of the hidden variable of <italic>P<sub>com</sub></italic> from the premotor cortex, which monitors the weights of sensory combinations in the parietal cortex. By resolving the experience and causal belief, the hidden causal structure and sensory representation are dynamically updated in the premotor and parietal cortices, respectively.</p><p>In the last 15 years, the BCI model has been extended to account for a large number of perceptual and sensorimotor phenomena and a vast behavioral data (<xref ref-type="bibr" rid="bib70">Shams and Beierholm, 2010</xref>). Recent studies have begun to map the algorithms and neural implementation in the human brain. Noninvasive human functional magnetic resonance imaging studies revealed a neural correlation to causal inference in the parietal cortex, and magnetoencephalography showed that frontal neural activities are also involved in the causal inference (<xref ref-type="bibr" rid="bib17">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib66">Rohe and Noppeney, 2016</xref>). However, at the single-neuron level, very few studies have examined the neural mechanism in animals. More importantly, none of the human studies have investigated the neural representation of the hidden variable, <italic>P<sub>com</sub></italic>. How the frontoparietal circuit contributes to the encoding and updating of <italic>P<sub>com</sub></italic> has not been explored. Our results reconciled and extended previous findings by showing that <italic>P<sub>com</sub></italic> is successively represented by premotor and parietal neural activities (<xref ref-type="bibr" rid="bib17">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>). Unlike previous human imaging studies, which used the final behavioral estimation as the index of the causal inference (<xref ref-type="bibr" rid="bib17">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>), our study directly examined the neural representation and dynamics of the hidden variable <italic>P<sub>com</sub></italic> at single-neuron and neural population levels. We showed that, even within a trial, the inference of a common source was dynamic. We thus propose a dynamic flow of information processing during causal inference, where the <italic>P<sub>com</sub></italic> is estimated from the information of sensory uncertainties and the disparity between them in the premotor cortex and then used for later sensory integration or segregation (model-weighted average) (<xref ref-type="bibr" rid="bib49">Körding et al., 2007</xref>); finally, these signals are maintained in the frontoparietal circuit to guide the reaching behavior (<xref ref-type="bibr" rid="bib4">Archambault et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Caminiti et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Cisek and Kalaska, 2005</xref>; <xref ref-type="bibr" rid="bib32">Gail and Andersen, 2006</xref>).</p><p>Experience creates our prior beliefs of the surrounding environment. It was proposed that various cognitive functions, such as sensory perception, motor control, and working memory, can be modulated by experience (<xref ref-type="bibr" rid="bib2">Akrami et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib63">Rao et al., 2012</xref>). Computationally, the prior updating and its modulation of behavior can be well understood within the Bayesian framework (<xref ref-type="bibr" rid="bib5">Badde et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Beierholm et al., 2020</xref>; <xref ref-type="bibr" rid="bib48">Körding and Wolpert, 2004</xref>; <xref ref-type="bibr" rid="bib67">Rohe et al., 2019</xref>). For instance, by imposing the BCI model in the present study, we showed that prior knowledge of a common source is updated by the hidden probability of the common source (<italic>P<sub>com</sub></italic>) in the previous trial and then integrated with the sensory inputs in a Bayesian manner. Such prior updating was also reported in a recent sensorimotor study, in which the posterior signals in the frontal cortex were used to update the prior (<xref ref-type="bibr" rid="bib20">Darlington et al., 2018</xref>). Intriguingly, the empirical findings in this study could be reproduced by a biologically plausible recurrent neural network, which suggests that using the feedback of posterior from a Bayesian computation to update prior is an essential feature of a hierarchical recurrent Bayesian model (<xref ref-type="bibr" rid="bib20">Darlington et al., 2018</xref>). From this perspective, the prior updating and its modulation of behavior may also serve as a plausible computational mechanism of multisensory recalibration in various sensorimotor behaviors (<xref ref-type="bibr" rid="bib5">Badde et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib58">Park and Kayser, 2019</xref>; <xref ref-type="bibr" rid="bib73">Van der Burg et al., 2013</xref>).</p><p>The frontoparietal circuit, including the premotor and parietal cortices, has long been recognized as a central area in sensorimotor representations (<xref ref-type="bibr" rid="bib16">Caminiti et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Caminiti et al., 1991</xref>). Although the present experiments shared many movement features in the reaching task, the key findings of causal inference processing are unlikely to be explained by the kinematical components. First, previous studies have demonstrated that the neuronal activities in the premotor cortex are related to hand kinematics (e.g., hand position, speed, and direction) in the motor planning and execution (<xref ref-type="bibr" rid="bib15">Caminiti et al., 1991</xref>; <xref ref-type="bibr" rid="bib18">Churchland et al., 2006</xref>), which lead the neural activities in the parietal cortex (<xref ref-type="bibr" rid="bib4">Archambault et al., 2011</xref>). However, in our study, the early activities of <italic>P<sub>com</sub></italic> in the premotor cortex cannot be purely induced by the sequential activities of kinematics in the premotor and parietal cortices. Because the <italic>P<sub>com</sub></italic> is abstract information, and its activity pattern is not correlated with any kinematical components. Expressly, under a given value of <italic>P<sub>com</sub></italic>, the reaching kinematics can be varied (e.g., the hand position can be anywhere on the table according to the target position and disparity in a given trial). Moreover, the neural signals about <italic>P<sub>com</sub></italic> in the premotor cortex were observed before the target onset, where no motor planning was possible during this period. Thus, our results are consistent with the idea that the high-level information, such as abstract and hidden structures, potential probability of multiple motor options, and VP integration, are encoded in the frontoparietal circuit, which could later integrate with the low-level sensory representations to guide the desired movement (<xref ref-type="bibr" rid="bib19">Cisek and Kalaska, 2005</xref>; <xref ref-type="bibr" rid="bib32">Gail and Andersen, 2006</xref>; <xref ref-type="bibr" rid="bib51">Limanowski and Blankenburg, 2016</xref>).</p><p>Second, the dynamic updating of prior and sensory representation proposed a putative mechanism for multisensory recalibration in sensorimotor tasks. At the behavioral level, our results are in accord with the observations that sensory perception is modulated by a multisensory context with sensory conflicts. The BCI theory thus provides a framework to explain how the multisensory context (e.g., the prior of common source) modulates the sensory representations, such as sensory uncertainty in our study and sensory estimation (e.g., spatial localizations) in previous sensorimotor studies (<xref ref-type="bibr" rid="bib5">Badde et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Bruns and Röder, 2015</xref>; <xref ref-type="bibr" rid="bib58">Park and Kayser, 2019</xref>; <xref ref-type="bibr" rid="bib73">Van der Burg et al., 2013</xref>). The results support the notion of dynamic representations of <italic>P<sub>com</sub></italic> in the present study – the top-down signal of common source from the premotor cortex modulates the spatial tuning in the parietal cortex and then guides hand estimation.</p><p>Previous research over the past two decades has revealed that even the perceptions of body ownership and agency are remarkably malleable and involve continuous processing of multisensory information and causal inference (<xref ref-type="bibr" rid="bib46">Kilteni et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Legaspi and Toyoizumi, 2019</xref>). Thus, our study provides unique data for understanding self-relative awareness (e.g., bodily self-consciousness) in macaque monkeys, showing neural implementation of causal inference at the neural circuit level. Using a VP task, we also identified the hidden components of causal inference in macaque monkeys’ parietal and premotor cortices. This is important because, unlike most sensory and cognitive functions, the subjective perceptions of body ownership and agency cannot be directly measured from explicit reports from animals. Using the BCI model and neural activities recorded from multiple brain areas, we can now begin exploring body ownership and agency qualitatively by examining the hidden variable in both behavior and neural representations.</p><p>In the BCI framework, there are two key components, inferring the hidden variables (e.g., <italic>P<sub>com</sub></italic>) and updating the causal structure and sensory representation. First, our results suggested that the representation and core computation of the hidden common source most likely takes place in the premotor cortex (<xref ref-type="bibr" rid="bib24">Ehrsson and Chancel, 2019</xref>; <xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>), which is consistent with findings in the body awareness (<xref ref-type="bibr" rid="bib9">Blanke et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Ehrsson et al., 2004</xref>). Our results were also consistent with previous finding in monkeys that the higher order representations (e.g., the multisensory response of body recognition) of the body were encoded in both dorsal and ventral premotor cortex and posterior parietal cortex (<xref ref-type="bibr" rid="bib29">Fogassi et al., 1999</xref>; <xref ref-type="bibr" rid="bib37">Graziano et al., 2000</xref>; <xref ref-type="bibr" rid="bib34">Graziano and Gross, 1993</xref>; <xref ref-type="bibr" rid="bib36">Graziano and Gross, 1998</xref>; <xref ref-type="bibr" rid="bib35">Graziano et al., 1994</xref>). Intriguingly, our results seem complementary to previous findings of mirror neuron systems in the premotor and parietal cortices in both humans and monkeys. Typically, a mirror neuron fires both when individual acts and when the individual observes the same action performed by another. That is, the mirror neuron is believed to mediate the understanding of others’ behavior (<xref ref-type="bibr" rid="bib41">Jerjian et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Jiang et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Pezzulo et al., 2022</xref>). By contrast, the role of causal inference neurons in our study was putatively participating in self-identification and self-other discrimination. Future studies are needed to examine how these two systems work together to identify both self and foreign agents.</p><p>Second, the posterior belief of a common source is calculated using a Bayesian approach by integrating prior knowledge and sensory entities, and theoretically, these components should be dynamically updated at different time hierarchies. For example, the prior configuration of the body, known as the body schema in psychology, constrains the possible distribution of the body states but is dynamically updated when the context changes to maintain consistency between the internal body model and sensory inputs (e.g., rubber hand illusion or body illusion) (<xref ref-type="bibr" rid="bib10">Botvinick and Cohen, 1998</xref>; <xref ref-type="bibr" rid="bib46">Kilteni et al., 2015</xref>). Pathological impairment in inferring the sensory source can result in somatoparaphrenia, in which the patient declares that their body part belongs to another person despite the visual and proprioceptive signals from the common source of their body (<xref ref-type="bibr" rid="bib45">Keromnes et al., 2019</xref>). Similarly, schizophrenia patients suffering from delusions of the agency have shown impairments in updating their internal causal structures. They show a deficit in detecting the source of their thoughts and actions and thus incorrectly attribute them to external agents (<xref ref-type="bibr" rid="bib40">Haggard, 2017</xref>). Therefore, although we demonstrated the neural representations and their updating by using the multisensory and reaching task in monkeys, the computational mechanism and underlying neural circuits might contribute to learning and inference in any task that relies on causal inference.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental model and subject details</title><p>All animal procedures were approved by the Animal Care Committee of the Center for Excellence in Brain Science and Intelligence Technology, Institute of Neuroscience, Chinese Academy of Sciences (Permit Number: CEBSIT-2020034), and were described previously in detail (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>). Three male adult rhesus monkeys (<italic>Macaca mulatta</italic>; Monkeys H, N, and S, weighting 6–10 kg) participated in the experiment. During the experiment, the monkeys were seated comfortably in the monkey chairs, and their heads were fixed. All monkeys were implanted with chambers for recordings.</p></sec><sec id="s4-2"><title>Method details</title><p>Some of the following methods are similar to those previously published (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>).</p></sec><sec id="s4-3"><title>Apparatus</title><p>The monkeys were seated in front of a chest-height table on which a lab-made virtual reality system was placed (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>). During the experiment, the monkey’s left arm (and the right arm in the case of Monkey H, who was right-handed) was placed in the system and blocked from sight. A CCD camera (MV-VEM120SC; Microvision Co., China) captured the image of the monkey’s arm reflected in a 45° mirror. This image was projected to the rear screen by a high-resolution projector (BenQ MX602, China). Therefore, when the monkey looked in the horizontal mirror suspended between the screen and the table, the visual arm image appeared to be its real arm on the table. The lower edge of the screen was aligned to the table edge. The monkey’s trunk was close to the edge of the table, and the left shoulder was aligned with the midline of the screen. Using the OpenCV graphics libraries in C++ (Visual Studio 2010; Microsoft Co., Redmond, WA, USA), the arms image and the visual target were generated and manipulated. Using CinePlex Behavioral Research Systems (Plexon Inc, Dallas, TX, USA), sampled at 80 Hz, the hand position was tracked and recorded. The tracking color marker was painted onto the monkey’s first segment of the middle finger, which was not visible after adjusting the light exposure settings of the video.</p></sec><sec id="s4-4"><title>Behavioral task procedures</title><p>The monkey was trained to report its proprioceptive arm location by reaching for a target in a VPC causal inference task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>). The monkey initiated a trial by placing its hand on the starting point (a blue dot with a 1.5 cm diameter) for 1000 ms and was instructed not to move. After the initiation period, the starting point disappeared, and the visual arm was rotated (within one video frame, 16.7 ms) for the VPC task. The rotation was maintained for 500 ms (the preparation period). After that, the reaching target was presented as a ‘go’ signal. The monkey had to reach the target (chosen from T1 to T5 randomly trial by trial [<xref ref-type="fig" rid="fig1">Figure 1A</xref>]) within 2500 ms and hold its hand in the target area (see as follows) for 500 ms to receive a drop of juice as the reward. Any arm movement during the target-holding period automatically terminated the trial. The rotated arm was maintained throughout the entire trial along with the arm movement. The intertrial interval (ITI) was ~1.5–2 s, after which the monkey was allowed to start the subsequent trial. During the ITI, the visual scene was blank. Under the VPC task, across trials, the visual arm was randomly presented with a disparity of 0°, ±10°,± 20°, ±35°, or ±45° (+, clockwise [CW]; −, counterclockwise [CCW] direction) from the subject’s proprioceptive arm, with its shoulder as the center point. The starting point was fixed 25 cm away from the monkey’s shoulder. The target position was selected randomly trial by trial from one of five possible positions located on an arc (a ±4° jitter was added to the original position trial by trial to ensure the monkey did not perform the task by memorizing all the target positions).</p><p>Besides the VPC task, the monkey was also instructed to perform a VP congruent and P task during the recording session. The only difference between the VPC and VP task was that during the entire trial under the VP task, the visual arm was always congruent with the proprioceptive arm. The only difference between VP and P tasks was that during the single trial for the P task, the visual arm information was blocked starting from the onset of the preparation period.</p><p>Each VPC block contained 55 trials in which the nine disparities and five targets were randomly combined. Each VP and P block had 27 trials in which five targets randomly occurred in every single trial. In one recording session, typically, one or two P blocks were given first to ensure that the monkey performed the task with its proprioceptive arm, and then in the following blocks, VP, P, and VPC tasks were randomly mixed. One recording session contained more than three VP and P blocks and more than eight VPC blocks.</p></sec><sec id="s4-5"><title>Target (with reward) area</title><p>To ensure the monkeys indeed performed the reaching-to-target task with their proprioceptive hand, under the VPC task, the reaching target area (with reward) was defined as follows: the radial distance from the hand to the center of the target was less than 5 cm to ensure that the monkey did reach out to the target; with the target as the center, the azimuth range was set from [−7 (8 for some sessions, same below) + rotation degree/disparity] to +7° when the rotation degree was negative (counter-clockwise), and from –7° to [+7 + rotation degree/disparity] when the rotation degree was positive (clockwise). As shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref> (green zone), the reward area ensured the monkey performed the task rationally and without visual feedback. That is monkey’s reaching position between two extreme conditions: one is that the monkey reaches the target purely relying on the visible arm (the drift is equal to the disparity); the other is that the monkey relies on the proprioceptive arm (the drift is equal to zero). Only the correct trials (when the monkey’s arm was located within the reward zone) were used in the subsequent analysis.</p></sec><sec id="s4-6"><title>Electrophysiology</title><p>Extracellular single-unit recordings were performed as described previously (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>; <xref ref-type="bibr" rid="bib53">Merchant et al., 2013</xref>) from three hemispheres in three monkeys. Briefly, under strictly sterile tasks and general anesthesia with isoflurane, a cylindrical recording chamber (Crist Instrument Co., Inc, Hagerstown, MD, USA) of 22 mm diameter was implanted in the premotor cortex and the parietal cortex (area 5 and area 7). We collected the structural magnetic resonance images (MRI) of three monkeys (3T, Center for Excellence in Brain Science and Intelligence Technology, Institute of Neuroscience, Chinese Academy of Sciences), while they were in an MRI-compatible Horsley-Clarke stereotaxic apparatus. The location of the recording chamber on each animal was determined by the atlas with the origin at the Ear Bar Zero (<xref ref-type="bibr" rid="bib68">Saleem and Logothetis, 2012</xref>). The centers of implanting recording chambers were [right: 20.0 mm; forward: 10.0 mm] for the premotor cortex in Monkey N, [left: 21.9 mm; forward: 24.9 mm] for the premotor cortex in Monkey H, [right: 14.7 mm; forward: 1.1 mm] for the parietal cortex in Monkey N, and, [left: 17.0 mm; forward: 3.5 mm] for the parietal cortex in Monkey S. During the recording session, glass-coated tungsten electrodes (1–2 MΩ; Alpha Omega, Israel) were inserted into the cortex via a guide tube using a multi-electrode driver (NAN electrode system; Plexon Inc, Dallas, TX, USA). All isolated neurons were recorded regardless of their activity during the task, with the recording locations varying from session to session. At each location, the raw extracellular membrane potential was sampled at 40 kHz. On-line raw neural signals were processed offline to obtain a single unit by Offline Sorter (Plexon Inc, Dallas, TX, USA). All spike data were re-sorted using off-line spike sorting clustering algorithms (Offline Sorter, PCA) (<xref ref-type="bibr" rid="bib53">Merchant et al., 2013</xref>). With manual adjustments, only well-isolated units were considered for further analysis (signal-to-noise is larger than 3). The sorted files were then exported in MATLAB format for further analysis in MATLAB (Mathworks, Natick, MA, USA) and Python (The Python Software Foundation).</p></sec><sec id="s4-7"><title>Quantification and statistical analysis</title><p>All statistical analyses were implemented with scripts written in MATLAB or Python. In the premotor cortex, 412 neurons were recorded from two monkeys (231 neurons from Monkey H and 181 neurons from Monkey N); in the parietal cortex (area 5 and area 7), 238 neurons were recorded from two monkeys (116 neurons from Monkey N and 122 neurons from Monkey S). As all monkeys’ behavior and model fitting results were similar, for all analyses, data were combined across monkeys. All related statistics are reported in the figure legends.</p></sec><sec id="s4-8"><title>Analysis of behavior data</title><sec id="s4-8-1"><title>BCI model</title><p>To capture the uncertainty of causal structure, the core of causal inference, the BCI model described in a previous study (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>) was adopted. In the present study, the BCI framework included three models: (i) the full-segregation model, which assumes that visual and proprioceptive estimates of the arm’s locations are drawn independently from different sources (<italic>C</italic>=2) and processed independently; (ii) the forced-fusion model, which assumes that visual and proprioceptive estimates of the arm’s locations are drawn from a common source (<italic>C</italic>=1) and integrated optimally, weighted by their reliabilities; and (iii) the BCI model, which computes the final proprioceptive estimate by averaging the spatial estimates under full-segregation and forced-fusion assumptions weighted by the posterior probabilities of a common source. Here, the BCI model assumes that both visual and proprioceptive location information (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) are represented as <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the neural system, respectively, which are drawn from the normal distribution with sensory noise [<inline-formula><mml:math id="inf5"><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> ]. The causal inference structure is determined by the joint distribution of two sensory signals (sensory likelihood) and the prior probability of a common source (<italic>P<sub>prior</sub></italic>). Thus, according to the Bayesian rule, the posterior probability of a common source (<italic>P<sub>com</sub></italic>) is calculated as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and the two sources of probability are <inline-formula><mml:math id="inf7"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> . Here, the likelihood of observed data (<inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) given common source [<inline-formula><mml:math id="inf9"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> ] is calculated as follows <xref ref-type="bibr" rid="bib49">Körding et al., 2007</xref>:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msqrt><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> represents a prior distribution of arm locations. In this experiment, the <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 0 and <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 10,000 to approximate a uniform distribution.</p><p>If the system completely ‘believes’ the two sensory signals are from different sources (full-segregation situation), the proprioceptive arm position is estimated independently from the visual information, as follows:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>If the system completely ‘believes’ there is only a common source for the two sensory signals (forced-fusion situation), then the estimate of arm position is determined by the optimal integration rule, as follows:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, we used the model average decision function to estimate final arm location (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>):<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the model simulation, the proprioceptive arm position at the end of the trial was set to zero (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>), so that the visual arm position is the VP (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:math></inline-formula>). In the task, monkeys were required to report their proprioceptive arm position; thus, only the proprioceptive estimate was simulated.</p></sec><sec id="s4-8-2"><title>Model fitting</title><p>To estimate the best-fitting model parameters in the BCI model, for each recording session, an optimization search was implemented that maximized the log-likelihood of each model given the monkey’s data under the VPC task. The prior probability of a common source (<italic>P<sub>prior</sub></italic>) and visual and proprioceptive standard deviations, σ<italic><sub>V</sub></italic> and σ<italic><sub>P</sub></italic>, respectively, were set as free parameters to be optimized. For each optimization step, 5000 trials per disparity were simulated to obtain the distribution, and the sum log-likelihood of the observations given the model was calculated for each disparity. Then, the parameters were optimized by minimizing the sum log-likelihood using a genetic algorithm (ga function in MATLAB). The procedure was the same as for the optimal integration model, except that there were no causal structures and only two free parameters (<inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) needed to be optimized. All simulation and optimization processes were performed in MATLAB. Only correct trials were included.</p></sec><sec id="s4-8-3"><title>Model comparison</title><p>To determine the model that best explained the data at the group level using the Bayesian information criterion (BIC), a Bayesian random-effects model comparison was used (<xref ref-type="bibr" rid="bib64">Rigoux et al., 2014</xref>). <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , where <italic>LL</italic> denotes the log-likelihood, <italic>k</italic> is the number of free parameters, <italic>n</italic> is the total number of data points, and <italic>ln</italic> is the natural logarithm. The BIC is a criterion for model selection among a finite set of models; models with lower BIC are generally preferred. Finally, the better model was identified at the group level by the exceedance of the probability based on all sessions of monkeys’ BICs (<xref ref-type="bibr" rid="bib74">Wozny et al., 2010</xref>). We used the exceedance probability to evaluate how likely it is that any given model is more frequent than all other models in the comparison set.</p><p>The models’ goodness-of-fit was reported using the coefficient of determination (<italic>R</italic><sup>2</sup>) (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>),<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the log-likelihoods of the fitted and the null model, respectively, and <italic>n</italic> is the number of observations. The null model assumes that monkeys report the perceived arm position randomly over the disparity range from the leftmost to the rightmost. Thus, a uniform distribution over this span was predicted.</p></sec><sec id="s4-8-4"><title><italic>P</italic><sub><italic>prior</italic></sub> updating in causal inference</title><p>To evaluate how the previous posterior probability of a common source (<italic>P<sub>com</sub></italic>) influences the prior probability of a common source (<italic>P<sub>prior</sub></italic>), a Markov process was adopted to model the updating of <italic>P<sub>prior</sub></italic>. That is,<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> respectively, and <italic>n</italic> denotes the <italic>n</italic>th trial under the VPC task. Two prior states were included: <italic>C</italic>=1 (a common source) and <italic>C</italic>=2 (two different sources) at each trial. <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the transition probability from a common source (<italic>C</italic>=1) to a common source (<italic>C</italic>=1), and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the transition probability from different sources (<italic>C</italic>=2) to a common source (<italic>C</italic>=1). For statistical significance analysis between <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , the Wilcoxon signed-rank test was used for paired data.</p><p>Note both <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> are latent variables. During the model fitting, we first used the BCI model (as mentioned before) to search the overall <italic>P<sub>prior</sub></italic>, <italic>σ<sub>P</sub></italic>, and <italic>σ<sub>V</sub></italic> for each session/day, which were used as initial parameters in the subsequent Markov model. The <italic>σ<sub>P</sub></italic> and <italic>σ<sub>V</sub></italic> were fixed during the Markov model fitting. For all subsequent trials (except the first trial), both <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> are unknown. As time goes on, starting from the first trial, the <italic>P<sub>com</sub></italic> of the current trial is obtained through the BCI model, and the <italic>P<sub>prior</sub></italic> of the next trial is obtained through the integration probability (<italic>P<sub>com</sub></italic>) or separation probability (1 − <italic>P<sub>com</sub></italic>) which are multiplied and added by the corresponding transition probability. Here, we fitted the observed data-drift to get the two free parameters transition probability. Through the transition probability, we define the influence of the <italic>P<sub>com</sub></italic> of the previous trial on the <italic>P<sub>prior</sub></italic> of the next trial.</p></sec><sec id="s4-8-5"><title>Updating of proprioceptive representation</title><p>To evaluate whether the primary sensory representation was modulated by the belief of causal structure, the proprioceptive variance within and after VPC tasks was compared to the baseline condition. For the within effect, the proprioceptive drift was calculated using the trials with 0° disparity in the VPC task and trials in the VP task (baseline condition). Here, the standard deviation (SD) of proprioceptive drift was used as a measurement for the uncertainty of proprioceptive representation, in which higher SD indicates higher uncertainty and vice versa. The mean of the proprioceptive drift for each target was normalized to zero. For the after-effect, the SDs of proprioceptive drift under the P task were compared between after the VP task and after the VPC task. To characterize the temporal dynamic of the proprioceptive updating (after-effect), trials in the first third and trials in the last third of the P task were compared. As a control, a similar analysis was conducted for the raw mean of proprioceptive drift (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For statistical significance analysis, Wilcoxon signed-rank test was used for paired data.</p></sec><sec id="s4-8-6"><title>Eye movement analysis</title><p>We trained the monkeys to perform the task without their eye fixed, but the eye movement during the recording sessions was recorded. To examine whether the updating of sensory uncertainty was correlated with the uncertainty of eye position between VP and VPC (0°) tasks. We identified the eye fixation position at the target-holding period. We examined the divergence of eye fixation position in VP and VPC (0°) tasks (see below). The average distance from the central point was used to measure the divergence at each target for each session. Each session’s divergence was obtained by averaging all the trials (see follows). The eye-tracking data were imported into MATLAB using EDF Converter (SR Research). The fixations and saccades in eye movements were separated with the default algorithm of the software with the velocity (30°/s), acceleration (8000°/s<sup>2</sup>), and motion thresholds (0.1°), respectively. The fixation positions were averaged during the target-holding period of each trial.</p><p>The normalized divergence of 2D eye fixation positions at each target was determined as follows:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>n</italic> is the sample size, <inline-formula><mml:math id="inf26"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is center of the eye fixation position, and <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula> is <italic>i</italic>th eye fixation position. The divergence was averaged across different target positions for each session. <inline-formula><mml:math id="inf28"><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> indicates the Euclidean distance between <italic>c</italic> and <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . For statistical significance analysis, Wilcoxon signed-rank test was used for paired data.</p><p>Moreover, to examine whether the neural activity of <italic>P<sub>com</sub></italic> was correlated with the eye position, we calculated the Pearson correlation coefficients between eye fixation position and VP weight. We found that there was no correlation between the VP weight and the eye fixation position at the population level for both regions (the premotor and parietal cortices) at both horizontal and vertical directions (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, Wilcoxon signed-rank test, premotor (horizontal): p=0.11; premotor (vertical): p=0.86; parietal (horizontal): p=0.35; parietal (vertical): p=0.87). Note that the recorded eye movement data used in this analysis included 78 sessions for the premotor cortex and 45 sessions for the parietal cortex.</p></sec></sec><sec id="s4-9"><title>Correction for FDR</title><p>In all cases, we used the Benjamini-Hochberg procedure (<xref ref-type="bibr" rid="bib7">Benjamini and Hochberg, 1995</xref>) to control FDR at an <italic>α</italic>=0.05 level, as follows. The <italic>p</italic>-values of a given set of hypothesis tests were sorted in ascending order, {<italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>, …, <italic>p<sub>n</sub></italic>}, and we found the first rank <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> such that <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mn>0.05</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> . Then we considered tests to be significantly above chance (rejecting null hypotheses) for all <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> .</p></sec><sec id="s4-10"><title>Preprocessing of single-unit data</title><p>To estimate continuous time-dependent firing rates, timestamps of spiking events were resampled at 1 kHz and converted into binary spikes for single trials. Spike trains were then convolved with a symmetric Hann kernel (MATLAB, MathWorks),<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mfrac><mml:mi>n</mml:mi><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>n</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>A</italic> is a normalization factor ensuring the sum of the kernel values equals 1. Window width <italic>L</italic> was set to 300 ms. Single neurons were included in the analysis only if they had been recorded for a full set of tasks (VP, P, and VPC tasks with nine disparities: 0°, ±10°, ±20°, ±35°, and ±45°).</p><p>Peri-stimulus time histograms (PSTHs) were then calculated for four periods of interest in a trial: (i) the baseline period (500 ms before the onset of visual arm rotation), (ii) the preparation period (500 ms after the onset of the visual arm rotation), (iii) the target-onset period (1000 ms after the onset of target onset), and (iv) the target-holding period (500 ms after the onset of target-holding). To smooth the firing rate at each time point, the neural firing rate was calculated by averaging in sliding windows (window size, 400 ms; step size, 100 ms) in a single trial (<xref ref-type="bibr" rid="bib31">Fried et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Gu et al., 2016</xref>), resulting in 22 time bins of mean firing rate for every single trial for subsequent dynamic analysis.</p></sec><sec id="s4-11"><title>Task selective neurons</title><p>To examine whether neurons in the premotor and parietal cortices during the target-holding period were selective to basic task components, including condition (VP or P task), arm location, and visual disparity. For each neuron, we conducted a two-way ANOVA in two datasets. One dataset contains the VP and P tasks (condition (two levels: VP and P tasks)×arm location (five levels: [–30°, –20°, 0°, 20°, and 30°]); the response variable is the mean firing rate during the holding period of each neuron). If a main effect of condition (or arm location) in the two-way ANOVA was found (p&lt;0.05), this neuron was classified as a condition (or arm location) selective neuron. The other dataset is the VPC task (the visual disparity (nine levels: [–45°, –35°, –20°, –10°, 0°, 10°, 20°, 35°, 45°])×target position (five levels: [–30°, –20°, 0°, 20°, and 30°]); the response variable is the mean firing rate during the holding period of each neuron). If the main effect of visual disparity in the two-way ANOVA was found (p&lt;0.05), this neuron was classified as a visual disparity selective neuron.</p></sec><sec id="s4-12"><title>Tuning curve analysis of arm location selective neurons</title><p>To investigate whether the tuning functions of arm location selective neurons (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) changed between VP condition and VPC (0°) condition at single-neuron level, we fitted the tuning curve with a reduced von Mises function by using the neuron response in different arm location (five levels: [−30°, −20°, 0°, 20°, and 30°]) for these two conditions separately. Here, VPC (0°) condition represents the trials in VPC condition where the disparity equals to 0. And the fitting function was defined as:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo>∗</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>b</italic> is the spontaneous firing rate of the neuron, <italic>a</italic> is defined as the gain index, and <italic>μ</italic> is preferred arm location. <inline-formula><mml:math id="inf33"><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> represents the firing rate when the arm location is <italic>x</italic>. We analyzed the spontaneous firing rate of the neuron, gain index, and preferred arm location between VP condition and VPC (0°) condition in both premotor and parietal cortices (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p></sec><sec id="s4-13"><title>Causal inference neuron</title><p>To measure the representation of a single neuron for causal inference on a single trial, the probability that a single neuron would integrate or segregate the sensory information on a single trial was calculated (<xref ref-type="bibr" rid="bib27">Fang et al., 2019</xref>). The basic assumption here is that in a single trial under the VPC task, if the neuron is more inclined to represent integrated information, then its firing rate will be closer to its response under VP tasks and farther away from the response under P tasks, and vice versa. The normalized weight of integration (VP weight) was calculated as follows:</p><p>(1) First, obtain the neuron response to the arm position under P and VP tasks and fit the von Mises distribution to get the tuning curve.</p><p>(2) Under VPC tasks, obtain the current visual arm and the real arm positions, and at the same time, obtain the neuron’s firing rate when the arm is in the corresponding position under VP and P tasks, <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively.</p><p>(3) The VP and P templates can be generated through the Poisson distribution:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>(4) According to the corresponding probabilities, <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the two templates are obtained, and the integration weights for this neuron in the VPC task can be obtained through standardization:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To quantitatively describe whether a single neuron is encoding causal inference, the correlation between <italic>P<sub>com</sub></italic> and VP weight is calculated. The logic is as follows: the <italic>P<sub>com</sub></italic> can be used to measure the degree of integration or segregation of sensory information at the behavioral level, whereas VP weight can measure this characteristic at the electrophysiological level. Therefore, if a neuron is performing causal inference, there should be a significant positive correlation between the <italic>P<sub>com</sub></italic> and VP weight for the corresponding behavior. Neurons that (i) respond to VP/P tasks and (ii) for which <italic>P<sub>com</sub></italic> and VP weight are significantly positively correlated in the final holding stage are called causal inference neurons. The specific algorithm was as follows:</p><list list-type="order"><list-item><p>First, obtain neurons with significant selectivity under VP and P tasks (condition selective neuron, see Materials and methods: Task selective neurons).</p></list-item><list-item><p>According to proprioception drift, all trials were divided into 29 classes. Continuous drift values were grouped into nine clusters: &lt; –35°, [–35° –25°], [–25° –15°], [–15° –6°], [–6°+6°], [+6°+15°], [+15°+25°], [+25°+35°], &gt;+35°. To be noticed, ±6° covers approximately 99% of drift distribution under the VP and P task. Thus, for the disparity of 0°, there was only one cluster [–6°+6°]. Since the distribution of drift becomes wider (higher variance) the larger the disparity, the more clusters would be assigned for the big disparity. For example, for the disparity ±45°, there were five clusters of drifts. <italic>P<sub>com</sub></italic> and VP weight were assigned for each class by averaging all trials within it. The Pearson correlation coefficient was then calculated between <italic>P<sub>com</sub></italic> and VP weight. If the <italic>P<sub>com</sub></italic> and VP weight were correlated significantly and positively (p&lt;0.05 and <italic>r</italic>&gt;0), the neuron was called a causal inference neuron.</p></list-item></list></sec><sec id="s4-14"><title>Population pattern of causal inference</title><p>To visualize the VP weight pattern at the brain region level, the VP weight of each trial of a single neuron under VPC tasks was calculated and then divided into 29 clusters as described above. Then, the bootstrap method was used to randomly select 50 trials from each cluster for averaging. This was repeated 50 times to obtain the VP weight (50×29) of a neuron for visualization. This results in a 50 × 29 × <italic>N</italic> matrix, where <italic>N</italic> indicates the number of neurons in each brain region (all neurons were used). The trial corresponding to each neuron was averaged to obtain a 50×29 matrix. The VP weights of a brain region were visualized in a heatmap.</p></sec><sec id="s4-15"><title>High/low <italic>P<sub>com</sub></italic> groups</title><p>To characterize the dynamic representation of the <italic>P<sub>com</sub></italic> in the entire session, all trials in a recording session were divided into high <italic>P<sub>com</sub></italic> trials and low <italic>P<sub>com</sub></italic> trials based on the relative proprioception drift (RD). Each trial’s relative proprioception drift (RD = drift/disparity) was calculated. The basic idea was that the larger the <italic>P<sub>com</sub></italic>, the more likely the monkey would integrate the visual and proprioceptive information, and the corresponding RD is closer to 1. The top third and bottom third of the trials were designated the high <italic>P<sub>com</sub></italic> class and the low <italic>P<sub>com</sub></italic> class, respectively. These grouping methods were verified by the dPCA.</p></sec><sec id="s4-16"><title>dPCA</title><p>The method for dPCA was adopted from that published in a previous study (<xref ref-type="bibr" rid="bib47">Kobak et al., 2016</xref>). Time, target position/arm location (−30°, −20°, 0°, 20°, and 30°), and <italic>P<sub>com</sub></italic> (VP, P, high <italic>P<sub>com</sub>,</italic> and low <italic>P<sub>com</sub></italic>) were combined to obtain the marginalized covariance matrix of the three. The neurons whose trial number was not less than five under a single condition were selected for dPCA. Population activity was then projected on the decoding axes and ordered by their explained total variance for each marginalization.</p></sec><sec id="s4-17"><title>Information encoded by individual neurons</title><p>The percentage of explained variance (PEV) (<xref ref-type="bibr" rid="bib14">Buschman et al., 2011</xref>) was used to measure the basic task components encoded by a single neuron, in which PEV reflected the degree to which the variance of a single neuron can be explained for a specific task component. Generally, PEV can be expressed as a statistical value of <inline-formula><mml:math id="inf38"><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> , that is, the variance ratio between groups to the total variance. As the statistical value of <inline-formula><mml:math id="inf39"><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> has a strong positive bias for a small sample, the unbiased <inline-formula><mml:math id="inf40"><mml:msup><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> statistical value (ωPEV) (<xref ref-type="bibr" rid="bib56">Olejnik and Algina, 2003</xref>) was used.</p><p>To evaluate the information about the locations of the proprioceptive arm, visual arm, and estimated arm encoded by a single neuron in the VPC task, an analysis of covariance was used to decompose the variance, and the ωPEV was calculated. In detail, for a single neuron, ωPEV was calculated for each type of arm when setting the other two types of arm locations as covariates. The whole reaching space was divided into 11 parts from −45° to 45° to transform it from a continuous variable to a discrete variable. A nonparametric Wilcoxon rank-sum test was used for unpaired data for statistical significance analysis comparing two brain regions.</p><p>The ωPEV was calculated in each time bin to characterize the temporal dynamics of neural information under VP and VPC (0°) tasks. The baseline was defined as the period 500 ms before the onset of visual arm rotation. A one-sided, paired Wilcoxon signed-rank with FDR correction determined the time bins significantly different from the baseline. The time bins showing significant differences between VP and VPC (0°) tasks were determined by a cluster-based permutation test (<xref ref-type="bibr" rid="bib33">Gramfort et al., 2013</xref>).</p></sec><sec id="s4-18"><title>Population decoding analysis</title><sec id="s4-18-1"><title>Decoding of <italic>P</italic><sub><italic>com</italic></sub></title><p>The population decoding analysis of <italic>P<sub>com</sub></italic> was performed by the linear SVM classifiers with the scikit-learn toolbox (<xref ref-type="bibr" rid="bib59">Pedregosa et al., 2011</xref>). All neurons were included in this analysis without considering their <italic>P<sub>com</sub></italic> selectivity. The classifier was trained to classify the <italic>P<sub>com</sub></italic> (high/low <italic>P<sub>com</sub></italic>) with neural activity (PSTHs) from each brain region. All recording sessions were pooled to form a pseudo-population. Neurons with more than 50 trials in each <italic>P<sub>com</sub></italic> group were included in this analysis. Tenfold cross-validation was then implemented by splitting the neural data into 10 subsamples, each randomly drawn from the entire dataset. Decoders were then trained on nine of the subsamples and tested on the remaining one. This process was repeated 10 times to obtain the decoding accuracy by averaging across all 10 decoders. This cross-validation process was repeated 1000 times, and the overall decoding accuracy was taken as the mean across the 1000 repetitions. The decoding analysis was conducted for all time points. The significance of decoding accuracy was determined by comparing the mean decoding accuracy to the null distribution from the shuffled data. The significant time duration was determined using a cluster-based permutation test for multiple comparisons across time intervals (permutations = 5000; cluster-level statistic: sum of the <italic>t</italic> values in a cluster; auxiliary cluster defining threshold <italic>t</italic>=3) (<xref ref-type="bibr" rid="bib33">Gramfort et al., 2013</xref>). For visualization, we plotted the mean of decoding accuracy with 95% confidence interval using 50 repetitions.</p><p>To test whether the premotor cortex neurons encode <italic>P<sub>com</sub></italic> earlier than parietal cortex, a randomization test was performed between them. Neurons with more than 50 trials in each <italic>P<sub>com</sub></italic> group were included in this analysis. The corresponding numbers (here, 200 neurons per region) of neurons were randomly exchanged between the paired regions 1000 times to generate a null distribution (chance level) of time lags, and the significance was determined by a permutation test of the true time lag from the original data and the null distribution (<xref ref-type="bibr" rid="bib57">Panichello and Buschman, 2021</xref>).</p></sec><sec id="s4-18-2"><title>Decoding of <italic>P</italic><sub><italic>prior</italic></sub></title><p>Neurons with more than 50 trials in each <italic>P<sub>com</sub></italic> group (high and low <italic>P<sub>com</sub></italic> groups, same as for the <italic>P<sub>com</sub></italic> decoding analysis described above) were selected for the <italic>P<sub>prior</sub></italic> updating decoding. The decoding procedure was the same as described for ‘<italic>Decoding of P<sub>com</sub></italic> ’ unless the trials were sorted and labeled by the previous trial’s <italic>P<sub>com</sub></italic> (<italic>n</italic><sup>th</sup>−1 to <italic>n</italic><sup>th</sup>−4) under the VPC task. The statistical significance was determined by a cluster-based permutation test (<xref ref-type="bibr" rid="bib33">Gramfort et al., 2013</xref>).</p></sec><sec id="s4-18-3"><title>Subspace overlap analysis</title><p>PCA was performed on neural activities during the baseline period and during the target-holding period. The first 10 principal components (PCs) during each period were used to obtain the <italic>P<sub>prior</sub></italic> and <italic>P<sub>com</sub></italic> subspaces. To test the overlap of these subspaces, the baseline-period activity was projected onto the <italic>P<sub>prior</sub></italic> subspace, and the percent variance explained relative to the total variance of the baseline period data was quantified; similarly, the target-holding period activity was projected onto the <italic>P<sub>com</sub></italic> subspace, and the percent variance explained relative to the total variance of the target-holding period data was quantified (<xref ref-type="bibr" rid="bib25">Elsayed et al., 2016</xref>).</p></sec><sec id="s4-18-4"><title>Decoding of arm locations</title><p>All arm locations were separated into five spatial bins: −30°, −20°, 0°, 20°, and 30°. The basic decoding procedure was the same as described above for ‘<italic>Decoding of P<sub>com</sub></italic>’. Neurons with more than six trials in each arm location bin were selected. Leave-one-out cross-validation was then implemented, and this process was repeated 1000 times to obtain the averaged decoding accuracy. The decoding analysis was conducted for all time points. Statistical significance for decoding accuracy was determined by comparing the mean decoding accuracy to the null distribution from shuffled data. The time bins with significant differences between tasks (VP and VPC (0°)) were determined by the cluster-based permutation test for multiple comparisons across time intervals (<xref ref-type="bibr" rid="bib33">Gramfort et al., 2013</xref>).</p></sec></sec><sec id="s4-19"><title>jPECC analysis</title><p>To test the relationship between population activities in the two brain regions, the jPECC method described in a previous study (<xref ref-type="bibr" rid="bib72">Steinmetz et al., 2019</xref>) was utilized. First, the neuronal responses in two brain regions under the same behavior conditions, namely, high <italic>P<sub>com</sub></italic> and low <italic>P<sub>com</sub></italic>, were aligned. Then, a PCA was conducted across time and trials to reduce the dimensionality to obtain the first 10 PCs for each brain region. The trials were then divided into 10 equal parts (training set and testing set) for cross-validation (10-fold cross-validation). The PCs of the training set of each brain region were used to perform canonical correlation analysis to obtain the first pair of canonical correlation components (L2 regularization, <italic>λ</italic>=0.5). Then, the PCs of the testing set from each brain region were projected onto the first pair of canonical correlation components, and the correlation was determined by the Pearson correlation coefficient between these projections from each region. This analysis was performed for each pair of time bins to construct a cross-validated correlation coefficient matrix. Fifty trials for each group (high <italic>P<sub>com</sub></italic> and low <italic>P<sub>com</sub></italic>) from each brain region were randomly selected by bootstrapping in this analysis. Finally, a heatmap was obtained by averaging the correlation coefficient matrix repeated 1000 times.</p><p>To quantify the lead-lag relationship of information exchange between brain regions, an asymmetric index was calculated by diagonally slicing the jPECC matrix from +300 ms to +300 ms relative to each time point (<xref ref-type="bibr" rid="bib72">Steinmetz et al., 2019</xref>). For time point <italic>t</italic>, the average correlation coefficient across the left half of this slice (i.e., the average along a vector from [<italic>t</italic> − 300, <italic>t</italic> + 300] to [<italic>t</italic>, <italic>t</italic>]) was subtracted from the right half of this slice (from [<italic>t</italic>, <italic>t</italic>] to [<italic>t</italic> + 300, <italic>t</italic> − 300]) to yield the asymmetry index. To test the leading significant time point across brain regions, the data from neurons in these brain regions were exchanged, and the above-described analysis was repeated 1000 times to obtain the null distribution of the asymmetric index. Then, a cluster-based permutation test was performed to test whether the symmetric index was significantly greater than the chance level (<xref ref-type="bibr" rid="bib33">Gramfort et al., 2013</xref>).</p><p>To further exclude the possibility that the observed lead-lag relationship resulted from the intrinsic properties of neuronal activities rather than the encoded information in these regions, all trials in each brain region were shuffled to ensure that the inter-region trials were not aligned. Then, the analysis was repeated as described above to obtain the asymmetric index.</p><p>Note that, due to the limitations of the asynchronous recording (the premotor and parietal neurons were grouped from different individual animals and only Monkey N was recorded in both areas), further studies are required to clarify the dynamics and functional interactions between regions using a simultaneous recording.</p></sec><sec id="s4-20"><title>Resource availability</title><sec id="s4-20-1"><title>Lead contact</title><p>Further information and requests for resources should be directed to and will be fulfilled by the Lead Contact, Liping Wang (liping.wang@ion.ac.cn).</p></sec><sec id="s4-20-2"><title>Materials availability</title><p>This study did not generate new unique reagents.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing, Funding acquisition</p></fn><fn fn-type="con" id="con3"><p>Data curation</p></fn><fn fn-type="con" id="con4"><p>Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All animal procedures were approved by the Animal Care Committee of Center for Excellence in Brain Science and Intelligence Technology, Institute of Neuroscience, Chinese Academy of Sciences (Permit Number: CEBSIT-2020034).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-76145-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Model parameters and fitting evaluations of four models for monkeys.</title></caption><media xlink:href="elife-76145-supp1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Source data files have been provided for Figures 1–6. Code and dataset have been uploaded to Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.rr4xgxd9h">https://doi.org/10.5061/dryad.rr4xgxd9h</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>G</given-names></name><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Code and dataset for neural dynamics of causal inference in the macaque frontoparietal circuit</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.rr4xgxd9h</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Florent Meyniel and Tianming Yang for their comments on the manuscript, and Xinjian Jiang, Jian Jiang, and Juntao Feng for experimental assistance. This work was supported by the National Science and Technology Innovation 2030 Major Program 2021ZD0204204 to WF, the Shanghai Municipal Science and Technology Major Project 2021SHZDZX to LW, the Lingang Laboratory Grant LG202105-02-01 to LW, the Strategic Priority Research Programs XDB32070201, the Strategic Priority Research Programs XDB32070201 to LW, and the National Natural Science Foundation of China 32100830 to WF.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian comparison of explicit and implicit causal inference strategies in multisensory heading perception</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006110</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006110</pub-id><pub-id pub-id-type="pmid">30052625</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title><source>Nature</source><volume>554</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nature25510</pub-id><pub-id pub-id-type="pmid">29414944</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aller</surname><given-names>M</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>To integrate or not to integrate: temporal dynamics of hierarchical bayesian causal inference</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000210</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000210</pub-id><pub-id pub-id-type="pmid">30939128</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Archambault</surname><given-names>PS</given-names></name><name><surname>Ferrari-Toniolo</surname><given-names>S</given-names></name><name><surname>Battaglia-Mayer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Online control of hand trajectory and evolution of motor intention in the parietofrontal system</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>742</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2623-10.2011</pub-id><pub-id pub-id-type="pmid">21228183</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badde</surname><given-names>S</given-names></name><name><surname>Navarro</surname><given-names>KT</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modality-specific attention attenuates visual-tactile integration and recalibration effects by reducing prior expectations of a common source for vision and touch</article-title><source>Cognition</source><volume>197</volume><elocation-id>104170</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2019.104170</pub-id><pub-id pub-id-type="pmid">32036027</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Ferrari</surname><given-names>A</given-names></name><name><surname>Stegle</surname><given-names>O</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Using the past to estimate sensory uncertainty</article-title><source>eLife</source><volume>9</volume><elocation-id>e54172</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54172</pub-id><pub-id pub-id-type="pmid">33319749</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanke</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory brain mechanisms of bodily self-consciousness</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>556</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1038/nrn3292</pub-id><pub-id pub-id-type="pmid">22805909</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanke</surname><given-names>O</given-names></name><name><surname>Slater</surname><given-names>M</given-names></name><name><surname>Serino</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Behavioral, neural, and computational principles of bodily self-consciousness</article-title><source>Neuron</source><volume>88</volume><fpage>145</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.029</pub-id><pub-id pub-id-type="pmid">26447578</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Rubber hands “ feel” touch that eyes see</article-title><source>Nature</source><volume>391</volume><elocation-id>756</elocation-id><pub-id pub-id-type="doi">10.1038/35784</pub-id><pub-id pub-id-type="pmid">9486643</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brozzoli</surname><given-names>C</given-names></name><name><surname>Gentile</surname><given-names>G</given-names></name><name><surname>Petkova</surname><given-names>VI</given-names></name><name><surname>Ehrsson</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FMRI adaptation reveals a cortical mechanism for the coding of space near the hand</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>9023</fpage><lpage>9031</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1172-11.2011</pub-id><pub-id pub-id-type="pmid">21677185</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname><given-names>P</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory recalibration integrates information from the immediate and the cumulative past</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>12739</elocation-id><pub-id pub-id-type="doi">10.1038/srep12739</pub-id><pub-id pub-id-type="pmid">26238089</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buneo</surname><given-names>CA</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The posterior parietal cortex: sensorimotor interface for the planning and online control of visually guided movements</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>2594</fpage><lpage>2606</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.10.011</pub-id><pub-id pub-id-type="pmid">16300804</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Roy</surname><given-names>JE</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural substrates of cognitive capacity limitations</article-title><source>PNAS</source><volume>108</volume><fpage>11252</fpage><lpage>11255</lpage><pub-id pub-id-type="doi">10.1073/pnas.1104666108</pub-id><pub-id pub-id-type="pmid">21690375</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caminiti</surname><given-names>R</given-names></name><name><surname>Johnson</surname><given-names>PB</given-names></name><name><surname>Galli</surname><given-names>C</given-names></name><name><surname>Ferraina</surname><given-names>S</given-names></name><name><surname>Burnod</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Making arm movements within different parts of space: the premotor and motor cortical representation of a coordinate system for reaching to visual targets</article-title><source>The Journal of Neuroscience</source><volume>11</volume><fpage>1182</fpage><lpage>1197</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.11-05-01182.1991</pub-id><pub-id pub-id-type="pmid">2027042</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caminiti</surname><given-names>R</given-names></name><name><surname>Borra</surname><given-names>E</given-names></name><name><surname>Visco-Comandini</surname><given-names>F</given-names></name><name><surname>Battaglia-Mayer</surname><given-names>A</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Luppino</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computational architecture of the parieto-frontal network underlying cognitive-motor control in monkeys</article-title><source>ENeuro</source><volume>4</volume><elocation-id>ENEURO.0306-16.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0306-16.2017</pub-id><pub-id pub-id-type="pmid">28275714</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference in the multisensory brain</article-title><source>Neuron</source><volume>102</volume><fpage>1076</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.03.043</pub-id><pub-id pub-id-type="pmid">31047778</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural variability in premotor cortex provides a signature of motor preparation</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>3697</fpage><lpage>3712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3762-05.2006</pub-id><pub-id pub-id-type="pmid">16597724</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural correlates of reaching decisions in dorsal premotor cortex: specification of multiple direction choices and final selection of action</article-title><source>Neuron</source><volume>45</volume><fpage>801</fpage><lpage>814</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.01.027</pub-id><pub-id pub-id-type="pmid">15748854</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darlington</surname><given-names>TR</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Lisberger</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural implementation of bayesian inference in a sensorimotor behavior</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1442</fpage><lpage>1451</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0233-y</pub-id><pub-id pub-id-type="pmid">30224803</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deroy</surname><given-names>O</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Metacognition in multisensory perception</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>736</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.08.006</pub-id><pub-id pub-id-type="pmid">27612983</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Jansen</surname><given-names>M</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference accounts for heading perception in the presence of object motion</article-title><source>PNAS</source><volume>116</volume><fpage>9060</fpage><lpage>9065</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820373116</pub-id><pub-id pub-id-type="pmid">30996126</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehrsson</surname><given-names>HH</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Passingham</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>That’s my hand! activity in premotor cortex reflects feeling of ownership of a limb</article-title><source>Science</source><volume>305</volume><fpage>875</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1126/science.1097011</pub-id><pub-id pub-id-type="pmid">15232072</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehrsson</surname><given-names>HH</given-names></name><name><surname>Chancel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Premotor cortex implements causal inference in multisensory own-body perception</article-title><source>PNAS</source><volume>116</volume><fpage>19771</fpage><lpage>19773</lpage><pub-id pub-id-type="doi">10.1073/pnas.1914000116</pub-id><pub-id pub-id-type="pmid">31506356</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13239</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13239</pub-id><pub-id pub-id-type="pmid">27807345</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Qi</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Statistical inference of body representation in the macaque brain</article-title><source>PNAS</source><volume>116</volume><fpage>20151</fpage><lpage>20157</lpage><pub-id pub-id-type="doi">10.1073/pnas.1902334116</pub-id><pub-id pub-id-type="pmid">31481617</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>429</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1038/nrn3503</pub-id><pub-id pub-id-type="pmid">23686172</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fogassi</surname><given-names>L</given-names></name><name><surname>Raos</surname><given-names>V</given-names></name><name><surname>Franchi</surname><given-names>G</given-names></name><name><surname>Gallese</surname><given-names>V</given-names></name><name><surname>Luppino</surname><given-names>G</given-names></name><name><surname>Matelli</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Visual responses in the dorsal premotor area F2 of the macaque monkey</article-title><source>Experimental Brain Research</source><volume>128</volume><fpage>194</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1007/s002210050835</pub-id><pub-id pub-id-type="pmid">10473758</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>RL</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multisensory neural processing: from cue integration to causal inference</article-title><source>Current Opinion in Physiology</source><volume>16</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2020.04.004</pub-id><pub-id pub-id-type="pmid">32968701</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fried</surname><given-names>I</given-names></name><name><surname>Mukamel</surname><given-names>R</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Internally generated preactivation of single neurons in human medial frontal cortex predicts volition</article-title><source>Neuron</source><volume>69</volume><fpage>548</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.11.045</pub-id><pub-id pub-id-type="pmid">21315264</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gail</surname><given-names>A</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural dynamics in monkey parietal reach region reflect context-specific sensorimotor transformations</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>9376</fpage><lpage>9384</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1570-06.2006</pub-id><pub-id pub-id-type="pmid">16971521</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Meg and EEG data analysis with MNE-python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MS</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A bimodal map of space: somatosensory receptive fields in the macaque putamen with corresponding visual receptive fields</article-title><source>Experimental Brain Research</source><volume>97</volume><fpage>96</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1007/BF00228820</pub-id><pub-id pub-id-type="pmid">8131835</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MS</given-names></name><name><surname>Yap</surname><given-names>GS</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Coding of visual space by premotor neurons</article-title><source>Science</source><volume>266</volume><fpage>1054</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.1126/science.7973661</pub-id><pub-id pub-id-type="pmid">7973661</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MS</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Spatial maps for the control of movement</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>195</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(98)80140-2</pub-id><pub-id pub-id-type="pmid">9635202</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MS</given-names></name><name><surname>Cooke</surname><given-names>DF</given-names></name><name><surname>Taylor</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Coding the location of the arm by sight</article-title><source>Science</source><volume>290</volume><fpage>1782</fpage><lpage>1786</lpage><pub-id pub-id-type="doi">10.1126/science.290.5497.1782</pub-id><pub-id pub-id-type="pmid">11099420</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MSA</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>How the brain represents the body: insights from neurophysiology and psychology</article-title><source>Common Mechanisms in Perception and Action</source><volume>19</volume><fpage>136</fpage><lpage>157</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Cheng</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multisensory convergence of visual and vestibular heading cues in the pursuit area of the frontal eye field</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3785</fpage><lpage>3801</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv183</pub-id><pub-id pub-id-type="pmid">26286917</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haggard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sense of agency in the human brain</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>196</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.14</pub-id><pub-id pub-id-type="pmid">28251993</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerjian</surname><given-names>SJ</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Kraskov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Movement initiation and GRASP representation in premotor and primary motor cortex mirror neurons</article-title><source>eLife</source><volume>9</volume><elocation-id>e54139</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54139</pub-id><pub-id pub-id-type="pmid">32628107</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Saggar</surname><given-names>H</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structure in neural activity during observed and executed movements is shared at the neural population level, not in single neurons</article-title><source>Cell Reports</source><volume>32</volume><elocation-id>108006</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2020.108006</pub-id><pub-id pub-id-type="pmid">32783934</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multisensory causal inference in the brain</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002075</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002075</pub-id><pub-id pub-id-type="pmid">25710476</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerby</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The simple difference formula: an approach to teaching nonparametric correlation</article-title><source>Comprehensive Psychology</source><volume>3</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.2466/11.IT.3.1</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keromnes</surname><given-names>G</given-names></name><name><surname>Chokron</surname><given-names>S</given-names></name><name><surname>Celume</surname><given-names>MP</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name><name><surname>Botbol</surname><given-names>M</given-names></name><name><surname>Canitano</surname><given-names>R</given-names></name><name><surname>Du Boisgueheneuc</surname><given-names>F</given-names></name><name><surname>Jaafari</surname><given-names>N</given-names></name><name><surname>Lavenne-Collot</surname><given-names>N</given-names></name><name><surname>Martin</surname><given-names>B</given-names></name><name><surname>Motillon</surname><given-names>T</given-names></name><name><surname>Thirioux</surname><given-names>B</given-names></name><name><surname>Scandurra</surname><given-names>V</given-names></name><name><surname>Wehrmann</surname><given-names>M</given-names></name><name><surname>Ghanizadeh</surname><given-names>A</given-names></name><name><surname>Tordjman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exploring self-consciousness from self- and other-image recognition in the mirror: concepts and evaluation</article-title><source>Frontiers in Psychology</source><volume>10</volume><elocation-id>719</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2019.00719</pub-id><pub-id pub-id-type="pmid">31133909</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilteni</surname><given-names>K</given-names></name><name><surname>Maselli</surname><given-names>A</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Slater</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Over my fake body: body ownership illusions for studying the multisensory basis of own-body perception</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>141</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00141</pub-id><pub-id pub-id-type="pmid">25852524</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>D</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Qi</surname><given-names>XL</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Demixed principal component analysis of neural population data</article-title><source>eLife</source><volume>5</volume><elocation-id>e10989</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10989</pub-id><pub-id pub-id-type="pmid">27067378</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bayesian integration in sensorimotor learning</article-title><source>Nature</source><volume>427</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1038/nature02169</pub-id><pub-id pub-id-type="pmid">14724638</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legaspi</surname><given-names>R</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A bayesian psychophysics model of sense of agency</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4250</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12170-0</pub-id><pub-id pub-id-type="pmid">31534122</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Limanowski</surname><given-names>J</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Integration of visual and proprioceptive limb position information in human posterior parietal, premotor, and extrastriate cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>2582</fpage><lpage>2589</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3987-15.2016</pub-id><pub-id pub-id-type="pmid">26937000</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochmann</surname><given-names>T</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural processing as causal inference</article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.05.018</pub-id><pub-id pub-id-type="pmid">21742484</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merchant</surname><given-names>H</given-names></name><name><surname>Pérez</surname><given-names>O</given-names></name><name><surname>Zarco</surname><given-names>W</given-names></name><name><surname>Gámez</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Interval tuning in the primate medial premotor cortex as a general timing mechanism</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>9082</fpage><lpage>9096</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5513-12.2013</pub-id><pub-id pub-id-type="pmid">23699519</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohl</surname><given-names>JT</given-names></name><name><surname>Pearson</surname><given-names>JM</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Monkeys and humans implement causal inference to simultaneously localize auditory and visual stimuli</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>715</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1152/jn.00046.2020</pub-id><pub-id pub-id-type="pmid">32727263</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>ML</given-names></name><name><surname>Deangelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory integration in macaque visual cortex depends on cue reliability</article-title><source>Neuron</source><volume>59</volume><fpage>662</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.06.024</pub-id><pub-id pub-id-type="pmid">18760701</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olejnik</surname><given-names>S</given-names></name><name><surname>Algina</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Generalized ETA and omega squared statistics: measures of effect size for some common research designs</article-title><source>Psychological Methods</source><volume>8</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1037/1082-989X.8.4.434</pub-id><pub-id pub-id-type="pmid">14664681</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shared mechanisms underlie the control of working memory and attention</article-title><source>Nature</source><volume>592</volume><fpage>601</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id><pub-id pub-id-type="pmid">33790467</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration in humans</article-title><source>eLife</source><volume>8</volume><elocation-id>e47001</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47001</pub-id><pub-id pub-id-type="pmid">31246172</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Brucher</surname><given-names>M</given-names></name><name><surname>Perrot</surname><given-names>M</given-names></name><name><surname>Duchesnay</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Nelson</surname><given-names>MJ</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dorsal premotor neurons encode the relative position of the hand, eye, and goal during reach planning</article-title><source>Neuron</source><volume>51</volume><fpage>125</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.05.025</pub-id><pub-id pub-id-type="pmid">16815337</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Donnarumma</surname><given-names>F</given-names></name><name><surname>Ferrari-Toniolo</surname><given-names>S</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Battaglia-Mayer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Shared population-level dynamics in monkey premotor cortex during solo action, joint action and action observation</article-title><source>Progress in Neurobiology</source><volume>210</volume><elocation-id>102214</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2021.102214</pub-id><pub-id pub-id-type="pmid">34979174</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Porter</surname><given-names>KK</given-names></name><name><surname>Metzger</surname><given-names>RR</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual- and saccade-related signals in the primate inferior colliculus</article-title><source>PNAS</source><volume>104</volume><fpage>17855</fpage><lpage>17860</lpage><pub-id pub-id-type="doi">10.1073/pnas.0706249104</pub-id><pub-id pub-id-type="pmid">17978183</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>V</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural correlates of prior expectations of motion in the lateral intraparietal and middle temporal areas</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>10063</fpage><lpage>10074</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5948-11.2012</pub-id><pub-id pub-id-type="pmid">22815520</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoux</surname><given-names>L</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian model selection for group studies-revisited</article-title><source>NeuroImage</source><volume>84</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id><pub-id pub-id-type="pmid">24018303</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical hierarchies perform bayesian causal inference in multisensory perception</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002073</pub-id><pub-id pub-id-type="pmid">25710328</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct computational principles govern multisensory integration in primary sensory and association cortices</article-title><source>Current Biology</source><volume>26</volume><fpage>509</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.056</pub-id><pub-id pub-id-type="pmid">26853368</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Ehlis</surname><given-names>AC</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural dynamics of hierarchical bayesian causal inference in multisensory perception</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1907</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09664-2</pub-id><pub-id pub-id-type="pmid">31015423</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>A Combined MRI and Histology Atlas of the Rhesus Monkey Brain in Stereotaxic Coordinates</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>Y</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Bayesian inference explains perception of unity and ventriloquism aftereffect: identification of common sources of audiovisual stimuli</article-title><source>Neural Computation</source><volume>19</volume><fpage>3335</fpage><lpage>3355</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.12.3335</pub-id><pub-id pub-id-type="pmid">17970656</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Beierholm</surname><given-names>UR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Causal inference in perception</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>425</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.07.001</pub-id><pub-id pub-id-type="pmid">20705502</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory integration: current issues from the perspective of the single neuron</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>255</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nrn2331</pub-id><pub-id pub-id-type="pmid">18354398</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title><source>Nature</source><volume>576</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1787-x</pub-id><pub-id pub-id-type="pmid">31776518</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname><given-names>E</given-names></name><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Cass</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid recalibration to audiovisual asynchrony</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>14633</fpage><lpage>14637</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1182-13.2013</pub-id><pub-id pub-id-type="pmid">24027264</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wozny</surname><given-names>DR</given-names></name><name><surname>Beierholm</surname><given-names>UR</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Probability matching as a computational strategy used in perception</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000871</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000871</pub-id><pub-id pub-id-type="pmid">20700493</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76145.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Merchant</surname><given-names>Hugo</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tmp8f25</institution-id><institution>National Autonomous University of Mexico</institution></institution-wrap><country>Mexico</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.12.06.469042" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.12.06.469042"/></front-stub><body><p>This study investigates the neural basis of the hidden causal structure between visual and proprioceptive signals in the primate premotor and parietal circuit during reaching tasks executed in a virtual reality environment, where information between the two modalities can be dissociated. The key novel result is that premotor neurons represent the integration of bimodal information for small disparities and the segregation for large disparities between the proprioceptive and visual information, while parietal cells show reaching tuning changes that support the updating sensory uncertainty between tasks.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76145.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Merchant</surname><given-names>Hugo</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tmp8f25</institution-id><institution>National Autonomous University of Mexico</institution></institution-wrap><country>Mexico</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Merchant</surname><given-names>Hugo</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tmp8f25</institution-id><institution>National Autonomous University of Mexico</institution></institution-wrap><country>Mexico</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.12.06.469042">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.12.06.469042v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neural dynamics of causal inference in the macaque frontoparietal circuit&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Hugo Merchant as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Hugo Merchant (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The present results are novel and provide important notions. However, we have a series of important concerns.</p><p>1) First, the paper is difficult to follow. The authors should use a simpler and more intuitive framing of the paper to make it more accessible to a general audience. A reader needs to integrate a large part of the results to really have an idea of what is the paper about and how the authors tested their hypothesis. Indeed, the writing in part uses words or phrases that are 'unusual' or imprecise, which makes the text difficult to understand.</p><p>2) Another important concern is that the paper does not show the basic response properties of neurons on both areas across tasks. It is not clear how the neural activity changes between the VP, P, and VPC tasks. It is a change in preferred direction, in the width of the tuning function or on the gain modulation? We strongly suggest providing two 'layers' of information, one based on standards plotting and analysis, to be followed by a second layer of data investigation, addressed to more detailed computational aspects.</p><p>3) Statistics:</p><p>Results of model fitting: The methods mention that the authors compared three models: segregation, fusion and causal inference. However, in the results (Table S1) only two models are presented. The results for segregation should be reported for the sake of completeness. The Ppriori is high, e.g. 0.98 and 0.999. This makes me wonder in how far the R2 and the EPs' can be so much different between causal inference and fusion. Considering e.g. model averaging decision function for BIC, a prior of 0.999 would make fusion and BCI nearly identical. I feel that either the Methods section is lacking important details about the models (See above) or there is either a mistake in the analysis or the reported numbers in the table. The analysis of Bayesian models seems to lack major details, the statistical reporting is below standard (missing effect sizes, degrees of freedom, lack of individual data in figures), the study shows many unjustified parameter choices and key results seem to lack statistical support: not all statements about differences between parietal and premotor cortex seem supported by a direct statistical comparison. Further, while three monkeys contributed data, for only one does the study report data from both brain regions; this makes the claim of a difference between brain regions rather weak and this shortcoming needs to be clearly acknowledged. The actual underlying data (e.g. how single neuron responses are converted to tuning curves; how decoding accuracies vary across neurons) is not shown, which makes it difficult to interpret the robustness of the results. In particular, as the units of analyses vary tremendously between Figures (experimental blocks, neurons, pseudo-epochs, etc).</p><p>The updating of the believe about the sensory causal structure is a central component of this work. The authors present this as well established aspect of the BCI model (l. 170ff). However, most previous studies used a static model that was fit to the aggregate data of an entire experiment without taking the trial history into account (as in the cited Koerding et al. 2007 paper). AS some recent work has incorporated such trial dependencies, it would be important to acknowledge these studies and to explain the novelty of the present work (e.g. Rohe et al. NatComm 2019; Beierholm et al. <italic>eLife</italic> 2020; Badde et al. Cognition 2020 may also be relevant).</p><p>Analysis of recalibration: The analysis of P trials after the VP or VPC task effectively looks at what is known as trial-wise multisensory recalibration (see e.g. Bruns et al. Scientific Reports 2015; Park and Kayser <italic>eLife</italic> 2019; van der Burg et al. J Neurosci 2013; Wozny et al. JNeurosci 2011; Badde et al. Cognition 2020; there is extensive literature on this both in the spatial and the temporal domain!). It seems awkward to investigate this recalibration of uniusensory judgements without alluding to previous work.</p><p>If I understood Figure 3A and the Methods correctly, only in monkey N both brain regions were recorded? If this is correct, the statement that premotor and parietal regions differ in their representations is a result of a mixed within and between subjects analysis. This should be acknowledged explicitly, as it greatly reduces the statistical power of this statement, and the repeated statement about an N of 3 is misleading. CI neurons are defined based on a seemingly arbitrary criterion: exceeding a correlation threshold based on the arbitrary division of the data into 26 bins. Given that neural representations generally span a continuum, I would like to see the distribution of 'causal inference effects' for individual neurons, e.g. in form of a distribution of r2 values (obtained from the correlation; or a regression as I suggest below). The apparent difference between brain regions (Figure 3G) may simply result from the specific choice of statistical cutoff (the criterion of p&lt;0.05 becomes meaningless in the presence of 475+238 tests in total). Seeing the individual-neuron data here seems vital.</p><p>The analysis of population timing suggests that premotor cortex leads (Figure 4F). Is it possible to extract by how much time? Also, the authors focus on the encoding of Pcom, which comprises both the a priori binding tendency and the discrepancy. Why did the authors not decode both the prior and the current multisensory discrepancy separately? This would seem important to differentiate neural signatures of priors from those of current sensory signals.</p><p>Showing the actual data: The key results (e.g. Figure 3C; 4F; 5B; 5F; 6C) would be much stronger rand more convincing if the actual units of analysis were shown in some ways. How does decoding accuracy vary across neurons?</p><p>Other details:</p><p>For most tests there are no measures of effect sizes reported, sometimes the respective test-statistics is missing, and the degrees of freedom remain very unclear. I understand that they wary between analysis, but given that some tests are based on the actually recorded units, some of pseudo-trials or binned data, it would be very important to report for each test the assumed independent units and their number. The false-discovery rate is mentioned frequently, but the precise method is not stated. Most analyses are based on Wilcoxon tests, but figures show mean and SDs. I encourage the authors to use the same nonparametric (or parametric) approaches for figures and stats (e.g. show boxplots and individual data). L. 892: what was precisely compared with the ANOVA? Cluster-based tests: the parameters and the procedures for this test are not reported (l.974)</p><p>To determine whether a neuron confirms to the expectation of causal inference, why is it necessary to bin the data (l 893ff)? Could one not simply derive a regression model for each neuron and visualize the R2 or F-value?</p><p>The authors seem to interpret differences in the significances (e.g. of cluster-based permutation tests) as significant differences between regions and as establishing differences in the relative timing of effects. These are statistical fallacies (e.g. Sassenhagen https://doi.org/10.1111/psyp.13335; and Makin https://doi.org/10.7554/<italic>eLife</italic>.48175).</p><p>For every statement claiming differences between parietal and premotor cortex it is necessary to directly impellent the respective contrast between neurons in each brain region to support such a difference.</p><p>Other methods:</p><p>Spike sorting: I could not find criteria used for spike sorting. Where the analyzed units single units or MUA? More details about spike thresholds, cluster separation etc. should be provided.</p><p>The total number of switches between blocks (e.g. P following VPC) should be reported, as this constitutes the effective degrees of analysis of the block switching analysis (Figure 2C).</p><p>Causal inference models and optimization: The methods leave it unclear how the two alternatives of common and separate sources were combined in the BCI model. Previous work has explored a number of decision functions (e.g. Rohe and Noppeney's work, or Wozny et al. PlosCompBiol 2010) but for the present study it remains unclear which decision function was used. Model fitting: how were likelihoods computed and the posteriors sampled for model fitting? I feel that the procedures are not described in sufficient detail to be reproduced. Over what range of disparities was the model optimized? This is important for the Null model mentioned later on. What is the precise number of data points that entered the BIC calculation?</p><p>Markov analysis: If I understood it correctly, SigmaA and SigmaV are fit to the entire block, and the Pprior derived from the entire block was used as starting value for this parameter? The authors conclude (l 256ff) that to 'maintain a consistency of causal inference, sensory uncertainty … is updated ' as well. However, the Markov model seems to focus only on the updating of the prior.</p><p>Processing of single unit data: In my view the paper would profit from showing actual single neuron PSTH's and how smoothing effected these. The methods (l. 857) mention a 400ms sliding window, but the periods of interest (e.g. target holding) are only minimally longer than this (500ms). This makes we worried that the analyzed data effectively blurs neural representations across epochs and is affected by movement artifacts. When computing the modality contributions to each response, what task epoch was analyzed to derive the tuning curves (l. 869ff)?</p><p>Overall there are many seemingly arbitrary choices in the methods. These include the thresholds to define neurons as 'causal inference', the number of trials required for neurons to be included in the population analysis (l. 964), the duration of smoothing kernels and temporal analysis windows (l. 848 ff), the binning of data for neuro-behavioral correlation (l. 893ff), in the generation of population patterns (l. 912ff), in the cluster-based test (not reported!). It would be good to see a justification for these choices or to learn whether the authors ensured that their main results do not depend on these precise choices.</p><p>4) The authors do not justify why they recorded in the transition between F4 (rostral ventral premotor) and F5 (caudal ventral premotor) with head visual/tactile optic flow signals and grasping signals respectively. The obvious target is F2 (dorsal premotor) since it has strong reaching signals and is highly connected with area 5 of the parietal lobe (Rizzolati, 1990; Mendoza and Merchant, 2014). The authors should provide a more detailed account on the areas studied and the criteria adopted to localize the recording sites. The specification that they were &quot;determined by individual MRI atlas&quot; does not warrant for areal identification, because on natural variability. On this regard, the Figure 3A' insets should be adjusted (for parietal recording sites in L and R hemispheres, the sulci orientation should be different, and for the premotor ones sulci should be reported).</p><p>5) For the discussion and comparison to previous work: The paradigm focuses on visuo-motor paradigm in which the sensory cues are both generated by the subject itself. In contrast, in many classical (e.g. audio-visual; or visual-vestibular)) paradigms both sensory cues are external in nature, and not linked to the subject's action. While in both types of paradigm sensory cues are integrated and can also induce perceptual recalibration, the visuo-motor paradigm still is conceptually distinct and this has implications for the interpretation of the results. The authors should discuss whether they believe that their findings generalize to other paradigms and whether the same or possibly distinct (e.g. parietal) brain regions should be investigated during such paradigms. Such a discussion seems important to place the present work in the context of the plethora of previous work. Indeed, the present study is completely lacking discussion of results with respect to current knowledge on the functional properties of premotor and parietal neurons subtending reaching. The literature on this topic is vast, but the following studies, as examples, could be relevant in this context:</p><p>1. Archambault et al. J Neurosci 2011 (comparison on premotor vs parietal, where premotor activity leads parietal one)</p><p>2. Caminiti et al. eNeuro, 2017 (overall picture of connectivity of fronto-parietal network with updated literature on functional properties of different areas)</p><p>3. Caminiti et al. J Neurosci 1991 (first paper on encoding of reaching in Premotor cortex)</p><p>4. Churchland MM, et al. Nature 2012</p><p>5. Cisek and Kalaska, Neuron 2005 (on premotor activity during reaching)</p><p>6. Gail and Andersen J Neurosci 2006 (on neural dynamics of sensorimotor transformations in parietal cortex)</p><p>7. Jerjian SJ, Sahani M, Kraskov A <italic>ELife</italic> 2020 (on movement representation in premotor cortex)</p><p>8. Jiang X et al. Cell Rep 2020 (onpremotor neural Activity during Observed and Executed Movements)</p><p>9. Mountcastle et al. J Neurophysiol 1975 (first pioneering study on the role of parietal cortex in visuomotor control)</p><p>10. Pezzulo et al. Progr Neurobiol 2022 (on the neural dynamics of premotor neurons during action execution and observation)</p><p>11. Santhanam et al. J Neurophysiol 2009</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Neural dynamics of causal inference in the macaque frontoparietal circuit&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Tirin Moore (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The authors did a good job at answering all the reviewers' comments in the rebuttal, particularly the once regarding analysis and statistically details. However, the consensus of the reviewers is that no real changes in the structure of the paper were carried out to simplify the framing of the manuscript and make it more accessible to a larger audience. In addition, all the reviewers are concerned with the lack of rational regarding the recording locations in the main text.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Although the authors did a good job at answering all the reviewers' comments in the rebuttal, particularly the once regarding analysis and statistically details. However, many of the framing and conceptual comments were not really incorporated in the actual reviewed manuscript. Specifically:</p><p>1) Please start the paper by giving an intuitive example of the key problem addressed in the manuscript.</p><p>2) There is no change in the introduction and the Results sections regarding a simpler and more intuitive framing of the paper (Figure 2A is the same). Again, the reader needs to go quite further into the manuscript to understand the main question and how the authors implemented the experiment.</p><p>3) The paper should refer to the classical notions of sensory motor integration in the parieto-premotor circuit in the discussion.</p><p>4) The authors did not mention why they recorded ventral premotor and a mix of area 5 and 7a in the main text.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In my previous review I pointed out that, although the experimental paradigm was overall well designed and the data analysis technically sophisticated, the manuscript was flawed in several aspects, particularly in relation to the way the paper was written, and the data reported and discussed.</p><p>Despite the extensive point-to-point reply, the revision of the paper remains disappointing, as no substantial changes have been made to consider the criticisms. As a matter of fact, the new version of it is essentially identical to the original one in all its sections (Abstract, Introduction, Results and Discussion). Surprisingly enough, despite the authors' attempt to reply with accuracy to the different issue raised in the reviewing process, no significant improvement of the resubmitted manuscript was achieved, as in most instances all new information was not fully integrated in the revised version.</p><p>The authors were invited to place the present work in the context of the extensive literature on the neurophysiology of the parieto-frontal network, with special attention to its role on reaching movements. In fact, the original manuscript did not adequately discuss the results within the conceptual frame offered by the knowledge accumulated over the last forty years on the dynamic properties of premotor and parietal neurons subtending arm movements. This suggestion was completely ignored, as both Discussion and Introduction remained virtually identical across versions and the authors just added a few references, among those suggested by the reviewers, in a rather superficial fashion, without any emphasis about how they were related to findings and conclusions of the present study.</p><p>Concerning point (1) the authors' action was limited to the mere insertion of new titles at the beginning of some paragraphs. In their response, it is reported that the logic of the manuscript is outlined in the unchanged Fig. 2A, which was already present in the previous version. Therefore, no significant change has been made to take into account this aspect.</p><p>Furthermore, the selected units shown in Fig. 3D to offer an example of neural activity in form of raster plots and mean firing rates (not histograms, as stated) are not indicative of clear response modulation.</p><p>The mentioned Table 1 is neither reported in the main text, nor in Supplementary Material.</p><p>When asked to evaluate the temporal difference between premotor and parietal activity, the authors just replied that &quot;The population decoding of Pcom (Figure 4E) indicated that the premotor cortex leads the parietal by about 300 ms&quot;, but this observation refers to what already shown in the earlier version of the manuscript. Even in this case, in fact, no change was made to the analyses and to the text to take this point into consideration.</p><p>Also concerning the spike sorting technique adopted, despite the reviewer's request, no further details have been provided, relative to what was already reported in the first version of the manuscript.</p><p>Despite the explicit request (see point 4) to provide more details on which premotor area was considered in this study, the authors persist in referring loosely to &quot;premotor&quot; cortex, not specifying exactly which among the different premotor areas is being studied, apart from the details provided graphically in the brain figurine. Given the different functional properties and characteristics in connectivity among different premotor regions, it is inappropriate and misleading from a neurophysiological perspective to simply refer to premotor cortex. Provided that the region of recording mainly encompasses premotor area F2vr (medial to the spur of the arcuate sulcus), a small and medial part of premotor area F5, and part of F4, as evident from Fig. 3A, the main text did not report the rationale underlying the selection of the F2vr, F4/F5 for the present study. In addition, according to the new details provided in the current version of Fig. 3A on recording sites, some of the penetrations (corresponding to about 40-45 collected units) belong to prefrontal area 8 (FEF). These cells should be removed from the premotor database.</p><p>Furthermore, the lack of precise identification of the area/s of neural recording concerns parietal cortex as well. In fact, from a careful inspection of Fig. 3A, most of the penetrations in Monkey N belong to area 7 (which is part of the Inferior Parietal Lobule), and not to area 5 (which extends over the Superior Parietal Lobule instead), as stated throughout the manuscript. Finally, in all four insets of Fig. 3a it is not specified what the dashed grey lines refer to. This uncertainty would require a major revision of the parietal database.</p><p>Beyond the graphical (subjective) representation, no other information is provided about the criteria adopted to identify the recording areas and sites. First, as pointed out by the reviewers, the specification that they were &quot;determined by individual MRI atlas&quot; does not warrant for any areal identification; second is not even vaguely informative on how frontal and parietal areas were identified. The sentence that &quot;The location of the recording chamber on each animal was determined by an individual MRI atlas&quot; remained unchanged in the revised version of paper, without any further details, not even providing the reference on which Atlas has been used.</p><p>Finally, another reason of concern highlighted in the revision process refers to the lack of eye movements data, given that eye position and saccade direction exerts a well-known, although quantitatively different, influence on premotor and parietal neural activity. The authors did not refer to this critical aspect at all, nor did they discuss how eye-related signals might have influenced and eventually contaminated the reported findings.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The authors have addressed most reviewer comments to a sufficient degree. The work is still very dense given the large number of analyses implemented, but I have no specific suggestions for how to change this.</p><p>One remaining shortcoming is that I did not see a specific rationale for the choice of the precise recording locations in the manuscript. In reply to my previous comment, the authors have provided some rather generic text in the rebuttal, but ideally, a clear rationale for the choices should be in the manuscript.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76145.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Main comments:</p><p>The present results are novel and provide important notions. However, we have a series of important concerns.</p><p>1) First, the paper is difficult to follow. The authors should use a simpler and more intuitive framing of the paper to make it more accessible to a general audience. A reader needs to integrate a large part of the results to really have an idea of what is the paper about and how the authors tested their hypothesis. Indeed, the writing in part uses words or phrases that are 'unusual' or imprecise, which makes the text difficult to understand.</p></disp-quote><p>In the revised manuscript, we have divided the behavior results into separate sections and summarized the conclusion of each part. In brief, the logic of the paper is outlined in Figure 2A, in which we decomposed the BCI model into sensory and hidden components, including the posterior probability of common source, the updating of prior belief, and sensory uncertainty, and then examined the behavioral evidence and corresponding neural representations respectively.</p><disp-quote content-type="editor-comment"><p>2) Another important concern is that the paper does not show the basic response properties of neurons on both areas across tasks. It is not clear how the neural activity changes between the VP, P, and VPC tasks. It is a change in preferred direction, in the width of the tuning function or on the gain modulation? We strongly suggest providing two 'layers' of information, one based on standards plotting and analysis, to be followed by a second layer of data investigation, addressed to more detailed computational aspects.</p></disp-quote><p>We have added the results with single-neuron analysis accordingly.</p><p>(i) We first examined whether neurons in the premotor and parietal cortex during the target-holding period were selective to basic task components, including condition (VP or P task), arm location, and visual disparity. For each neuron, we conducted a Two-ANOVA in two datasets. One dataset contains the VP and P tasks (condition (2 levels: VP and P tasks) × arm location (5 levels: [-30°, -20°, 0°, 20°, and 30°]); the response variable is the mean firing rate during the holding period of each neuron). If the main effect of condition (or arm location) in the Two-ANOVA was found (<italic>p</italic> &lt; 0.05), this neuron was classified as a condition (or arm location) selective neuron. The other dataset is the VPC task (the visual disparity (9 levels: [-45°, -35°, -20°, -10°, 0°, 10°, 20°, 35°, 45°]) × target position (5 levels: [-30°, -20°, 0°, 20°, and 30°]); the response variable is the mean firing rate during the holding period of each neuron). If the main effect of visual disparity in the Two-ANOVA was found (<italic>p</italic> &lt; 0.05), this neuron was classified as a disparity selective neuron. We found that in the premotor cortex, 39% (186/475) of neurons were selective to condition, 21% to arm location, and 34% to visual disparity (Figure 3—figure supplement 1A, upper panel). In the parietal cortex, 35% (83/238) of neurons were selective to Condition, 26% to arm location, and 31% to visual disparity (Figure 3—figure supplement 1 lower panel, ANOVA, main effect, <italic>p</italic> &lt; 0.05). Over 25% of neurons in both regions were selective to multiple variables (premotor, 119/475; parietal, 64/238). Figure 3—figure supplement 1B represents example selective neurons in the premotor and parietal cortex, respectively.</p><p>(ii) For the neurons with arm-location selectivity (Figure 3—figure supplement 1A, premotor, n = 100; parietal, n = 63), we further investigated whether the tuning functions of these neurons were modulated in VPC (0°) task relative to the control condition (VP task). We fitted the tuning curve with a reduced von Mises function by using the neuron response in different arm locations (5 levels: [−30°, −20°, 0°, 20°, and 30°]) for these two tasks respectively. Here VPC (0°) task represents the trials in the VPC task where the disparity equals 0. And the fitting function was defined as:<inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>fr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo>∗</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <italic>b</italic> is the spontaneous firing rate of the neuron, <italic>a</italic> is defined as the gain index, and μ is the preferred arm location. <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the firing rate when the arm location is x. In the following, we analyzed the spontaneous firing rate, gain index, and preferred arm location of each neuron between the VP and VPC (0°) tasks in both premotor and parietal cortices. The results show that the preferred arm location was not significantly changed in both regions (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref> right, premotor, <italic>p</italic> = 0.62; parietal, <italic>p</italic> = 0.29, Wilcoxon signed-rank test). The spontaneous firing rates during the VP task were higher than that during the VPC (0°) task in the parietal cortex (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref> left, parietal, Wilcoxon signed-rank test, <italic>p</italic> = 0.012) but not in the premotor cortex (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref> left, premotor, Wilcoxon signed-rank test, <italic>p</italic> = 0.55). The gain index during the VP task was higher than that during the VPC (0°) task in the parietal cortex (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref> middle, parietal, Wilcoxon signed-rank test, <italic>p</italic> = 0.0012) but not in the premotor cortex (<xref ref-type="fig" rid="sa2fig1">Author response image 1A</xref> middle, Wilcoxon signed-rank test, <italic>p</italic> = 0.081). The result is consistent with the conclusion that parietal, but not premotor, neurons update the neural information about arm locations between the VP and VPC (0°) tasks (Figure 6B).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><p>The comparisons of tuning curve parameters between the VP and VPC (0°) tasks. Left: The spontaneous firing rates during VP task were higher than that during VPC (0°) task in parietal cortex (parietal, <italic>W</italic> = 641.0, df = 62, r<sub>rb</sub> = 0.36, <italic>p</italic> = 0.012) but not in the premotor cortex (<italic>W</italic> = 2,350.0, df = 99, r<sub>rb</sub> = 0.069, <italic>p</italic> = 0.55). Middle: The gain index during VP task were higher than that during VPC (0°) task in parietal cortex (parietal, <italic>W</italic> = 535.0, df = 62, r<sub>rb</sub> = 0.47, <italic>p</italic> = 0.0012) but not premotor cortex (<italic>W</italic> = 2,017.0, df = 99, r<sub>rb</sub> = 0.20, <italic>p</italic> = 0.081). Right: The preferred arm location was not significantly changed in the premotor and parietal cortex (premotor, <italic>W</italic> = 2,334.0, df = 99, r<sub>rb</sub> = 0.056, <italic>p</italic> = 0.62; parietal, <italic>W</italic> = 852.0, df = 62, r<sub>rb</sub> = 0.15, <italic>p</italic> = 0.29). Pair-wise comparisons were performed using Wilcoxon signed rank test. Effect sizes (r<sub>rb</sub>) were performed using the rank-biserial correlation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig1-v2.tif"/></fig><p>(iii) Furthermore, we reported raster plots and histograms of two example neurons (integration- and segregation- preferred) that varied their responses to the visual disparity during the target-holding periods (gray zones) in Figure 3D.</p><disp-quote content-type="editor-comment"><p>3) Statistics:</p><p>Results of model fitting: The methods mention that the authors compared three models: segregation, fusion and causal inference. However, in the results (Table S1) only two models are presented. The results for segregation should be reported for the sake of completeness. The Ppriori is high, e.g. 0.98 and 0.999. This makes me wonder in how far the R2 and the EPs' can be so much different between causal inference and fusion. Considering e.g. model averaging decision function for BIC, a prior of 0.999 would make fusion and BCI nearly identical. I feel that either the Methods section is lacking important details about the models (See above) or there is either a mistake in the analysis or the reported numbers in the table. The analysis of Bayesian models seems to lack major details, the statistical reporting is below standard (missing effect sizes, degrees of freedom, lack of individual data in figures), the study shows many unjustified parameter choices and key results seem to lack statistical support: not all statements about differences between parietal and premotor cortex seem supported by a direct statistical comparison. Further, while three monkeys contributed data, for only one does the study report data from both brain regions; this makes the claim of a difference between brain regions rather weak and this shortcoming needs to be clearly acknowledged. The actual underlying data (e.g. how single neuron responses are converted to tuning curves; how decoding accuracies vary across neurons) is not shown, which makes it difficult to interpret the robustness of the results. In particular, as the units of analyses vary tremendously between Figures (experimental blocks, neurons, pseudo-epochs, etc).</p></disp-quote><p>We thank the referee for the suggestions. The details of the model have been added to the revised Methods.</p><p>(i) The full segregation model fitting results are now included in <xref ref-type="table" rid="sa2table1">Author response table 1</xref>.</p><table-wrap id="sa2table1" position="float"><label>Author response table 1.</label><caption><title>Model parameters and fitting evaluations of two models for monkeys.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>subject</th><th>Causal inference (model averaging)</th><th>Forced fusion</th><th/><th/><th/><th/><th/><th/><th/><th/><th/></tr></thead><tbody><tr><td align="left" valign="top"/><td align="left" valign="top">relBIC<sub>group</sub></td><td align="left" valign="top">EP</td><td align="left" valign="top">R<sup>2</sup></td><td align="left" valign="top"><bold>σ</bold><sub>P</sub></td><td align="left" valign="top"><bold>σ</bold><sub>V</sub></td><td align="left" valign="top"><italic>Prior</italic></td><td align="left" valign="top">relBIC<sub>group</sub></td><td align="left" valign="top">EP</td><td align="left" valign="top">R<sup>2</sup></td><td align="left" valign="top"><bold>σ</bold><sub>P</sub></td><td align="left" valign="top"><bold>σ</bold><sub>V</sub></td></tr><tr><td align="left" valign="top">Monkey H</td><td align="left" valign="top">0</td><td align="left" valign="top">1</td><td align="left" valign="top">0.96±0.0017</td><td align="left" valign="top">7.72±0.14</td><td align="left" valign="top">5.83±0.090</td><td align="left" valign="top">0.999±0.0004</td><td align="left" valign="top">329.52</td><td align="left" valign="top">0</td><td align="left" valign="top">0.93±0.0039</td><td align="left" valign="top">9.87±0.24</td><td align="left" valign="top">9.02±0.23</td></tr><tr><td align="left" valign="top">Monkey N</td><td align="left" valign="top">0</td><td align="left" valign="top">1</td><td align="left" valign="top">0.93±0.0080</td><td align="left" valign="top">9.56±0.16</td><td align="left" valign="top">4.93±0.15</td><td align="left" valign="top">0.86±0.026</td><td align="left" valign="top">812.34</td><td align="left" valign="top">0</td><td align="left" valign="top">0.37±0.36</td><td align="left" valign="top">11.34±0.10</td><td align="left" valign="top">10.46±0.13</td></tr><tr><td align="left" valign="top">Monkey S</td><td align="left" valign="top">0</td><td align="left" valign="top">1</td><td align="left" valign="top">0.96±0.0022</td><td align="left" valign="top">8.98±0.14</td><td align="left" valign="top">5.72±0.22</td><td align="left" valign="top">0.98±0.012</td><td align="left" valign="top">290.04</td><td align="left" valign="top">0</td><td align="left" valign="top">0.94±0.0027</td><td align="left" valign="top">10.10±0.14</td><td align="left" valign="top">8.34±0.17</td></tr><tr><td align="left" valign="top">subject</td><td align="left" valign="top">Full segregation (proprioceptive only)</td><td align="left" valign="top">Full segregation (visual only)</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top"/><td align="left" valign="top">relBIC<sub>group</sub></td><td align="left" valign="top">EP</td><td align="left" valign="top">R<sup>2</sup></td><td align="left" valign="top"><bold>σ</bold><sub>P</sub></td><td align="left" valign="top">relBIC<sub>group</sub></td><td align="left" valign="top">EP</td><td align="left" valign="top">R<sup>2</sup></td><td align="left" valign="top"><bold>σ</bold><sub>V</sub></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Monkey H</td><td align="left" valign="top">1876.10</td><td align="left" valign="top">0</td><td align="left" valign="top">0.44±0.026</td><td align="left" valign="top">14.43±0.15</td><td align="left" valign="top">1677.04</td><td align="left" valign="top">0</td><td align="left" valign="top">0.58±0.019</td><td align="left" valign="top">14.51±0.091</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Monkey N</td><td align="left" valign="top">1902.82</td><td align="left" valign="top">0</td><td align="left" valign="top">0.53±0.018</td><td align="left" valign="top">14.76±0.070</td><td align="left" valign="top">1739.40</td><td align="left" valign="top">0</td><td align="left" valign="top">0.51±0.078</td><td align="left" valign="top">14.38±0.095</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Monkey S</td><td align="left" valign="top">1937.32</td><td align="left" valign="top">0</td><td align="left" valign="top">0.39±0.040</td><td align="left" valign="top">14.72±0.066</td><td align="left" valign="top">1440.47</td><td align="left" valign="top">0</td><td align="left" valign="top">0.71±0.016</td><td align="left" valign="top">12.75±0.28</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr></tbody></table></table-wrap><p>The model parameters and <italic>R</italic><sup>2</sup> were averaged across days for monkeys; data are presented as the means ± the standard errors. The relBIC<sub>group</sub> was the summation of all days’ BIC for monkeys.</p><p>Abbreviations: <bold>σ</bold><sub>P</sub>, the standard deviation of the proprioception likelihood; <bold>σ</bold><sub>V</sub>, the standard deviation of the vision likelihood; <italic>P<sub>prior</sub></italic>, the prior probability of a common source; relBIC<sub>group</sub>, Bayesian information criterion at the group level; EP, exceedance probability; <italic>R</italic><sup>2</sup>, coefficient of determination.</p><p>(ii) We agree with the referee that the <italic>P<sub>prior</sub></italic> is high in the monkey H and S, and relatively low in the monkey N. Despite the high <italic>P<sub>prior</sub></italic> in the causal inference model, we would like to mention that there is an essential difference between these two models. In particular, the long-tailed or bimodal distribution of proprioceptive drift under large disparities can only be explained by the causal inference model (in red, Figure 1—figure supplement 2 ) but not by the forced-fusion model (unimodal distributions in green, Figure 1—figure supplement 2 ) (an example behavior session in Figure 1—figure supplement 2 ).</p><p>(iii) For the analysis of Bayesian models, the analysis of the Bayesian model was reported in revised Supplementary file 1-Table 1; see the relBIC<sub>group</sub> (Bayesian information criterion at the group level) and EP (exceedance probability).</p><p>(iv) Rank-biserial correlation was used to estimate the effect size for the nonparametric test (Kerby, Comprehensive Psychology, 2014). The degrees of freedom and effect size were added to the revised figure legend.</p><p>(v) We used the premotor and parietal neurons from the same monkey (Monkey N) to decode <italic>P<sub>com</sub></italic>. The results suggest the same tendency as the results using parietal and premotor neurons across monkeys (Figure 4—figure supplement 2 ).</p><p>(vi) To verify the robustness of the decoding results, we tested the decoding ability by sampling different numbers of neurons for <italic>P<sub>com</sub></italic>, <italic>P<sub>prior</sub></italic>, disparity, and arm locations, respectively (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> and 3). The conclusion holds.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Population decoding of <italic>P<sub>com</sub></italic> (upper panel), <italic>P<sub>prior</sub></italic> (middle panel), and Disparity (bottom panel) with different neuron numbers in the premotor (left panel) and parietal (right panel) cortex, respectively.</title><p>The horizontal dashed black line represents the chance level. The horizontal solid bars at the top represent the time of significant decoding accuracy (cluster-based permutation test, <italic>p</italic> &lt; 0.05). Different color shades represent different numbers of neurons used for decoding.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig2-v2.tif"/></fig><fig id="sa2fig3" position="float"><label>Author response image 3.</label><caption><title>Population decoding of arm locations with different neuron numbers in the premotor and parietal cortex, respectively.</title><p>The horizontal dashed black line represents the chance level. The flat bar at the top represents the time bins in which the decoding accuracy for the VPC (0°) task was significantly lower than that for the VP task (cluster-based permutation test, <italic>p</italic> &lt; 0.05). N: number of neurons used for decoding.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig3-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>The updating of the believe about the sensory causal structure is a central component of this work. The authors present this as well established aspect of the BCI model (l. 170ff). However, most previous studies used a static model that was fit to the aggregate data of an entire experiment without taking the trial history into account (as in the cited Koerding et al. 2007 paper). AS some recent work has incorporated such trial dependencies, it would be important to acknowledge these studies and to explain the novelty of the present work (e.g. Rohe et al. NatComm 2019; Beierholm et al. eLife 2020; Badde et al. Cognition 2020 may also be relevant).</p></disp-quote><p>The discussion about these studies has been added and the references have been cited in the revised manuscript accordingly (L. 596).</p><disp-quote content-type="editor-comment"><p>Analysis of recalibration: The analysis of P trials after the VP or VPC task effectively looks at what is known as trial-wise multisensory recalibration (see e.g. Bruns et al. Scientific Reports 2015; Park and Kayser eLife 2019; van der Burg et al. J Neurosci 2013; Wozny et al. JNeurosci 2011; Badde et al. Cognition 2020; there is extensive literature on this both in the spatial and the temporal domain!). It seems awkward to investigate this recalibration of uniusensory judgements without alluding to previous work.</p></disp-quote><p>Indeed, the references suggested by the referee are relevant. We have cited them accordingly (L. 616). However, it is worth noting that, instead of the multisensory recalibration of the sensory errors [the spatial localization errors (Bruns et al., 2005; Park and Kayser, 2019; Badde et al. Cognition 2020) and temporal shifts (van der Burg et al. J Neurosci 2013)], we focused on sensory uncertainty and its updating.</p><disp-quote content-type="editor-comment"><p>If I understood Figure 3A and the Methods correctly, only in monkey N both brain regions were recorded? If this is correct, the statement that premotor and parietal regions differ in their representations is a result of a mixed within and between subjects analysis. This should be acknowledged explicitly, as it greatly reduces the statistical power of this statement, and the repeated statement about an N of 3 is misleading. CI neurons are defined based on a seemingly arbitrary criterion: exceeding a correlation threshold based on the arbitrary division of the data into 26 bins. Given that neural representations generally span a continuum, I would like to see the distribution of 'causal inference effects' for individual neurons, e.g. in form of a distribution of r2 values (obtained from the correlation; or a regression as I suggest below). The apparent difference between brain regions (Figure 3G) may simply result from the specific choice of statistical cutoff (the criterion of p&lt;0.05 becomes meaningless in the presence of 475+238 tests in total). Seeing the individual-neuron data here seems vital.</p></disp-quote><p>(i) We thank the referee for reminding us of the missing information. We have acknowledged the limitations of our analysis as follows (L. 1130 in the revised manuscript):</p><p>“Note that, due to the limitations of the asynchronous recording (the premotor and parietal neurons were grouped from different individual animals, and only monkey N was recorded in both areas), further studies are required to clarify the dynamics and functional interactions between regions using a simultaneous recording.”</p><p>(ii) Here we show the histogram of the Pearson correlation coefficients between <italic>P<sub>com</sub></italic> and VP weight (<xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>). The correlation coefficients of both regions are significantly larger than 0 (two-sided one-sample t-test, Premotor: <italic>t<sub>474</sub></italic> = 7.88, <italic>p</italic> &lt; 0.001, Cohen’s d = 0.36; Parietal: <italic>t<sub>237</sub></italic> = 3.16, <italic>p</italic> &lt; 0.01, Cohen’s d = 0.21). The Pearson correlation coefficients in premotor neurons are slightly higher than that in the parietal neurons (two-sided two-sample t-test, <italic>t<sub>474, 237</sub></italic> = 1.82, Cohen’s d = 0.10, <italic>p</italic> = 0.069).</p><fig id="sa2fig4" position="float"><label>Author response image 4.</label><caption><title>Histogram of Pearson correlation coefficients between VP weight and <italic>P<sub>com</sub></italic>.</title><p>The correlation coefficients of both regions are significantly larger than 0 (two-sided one-sample t-test, Premotor: t<sub>474</sub> = 7.88, Cohen’s d = 0.36, <italic>p</italic> &lt; 0.001; Parietal: t<sub>237</sub> = 3.16, Cohen’s d = 0.21, <italic>p</italic> &lt; 0.01). ** <italic>p</italic> &lt; 0.01; *** <italic>p</italic> &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig4-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>The analysis of population timing suggests that premotor cortex leads (Figure 4F). Is it possible to extract by how much time? Also, the authors focus on the encoding of Pcom, which comprises both the a priori binding tendency and the discrepancy. Why did the authors not decode both the prior and the current multisensory discrepancy separately? This would seem important to differentiate neural signatures of priors from those of current sensory signals.</p></disp-quote><p>(i) The jPECC analysis only suggested a leading tendency from premotor to parietal. The population decoding of <italic>P<sub>com</sub></italic> (Figure 4E) indicated that the premotor cortex leads the parietal by about 300 ms.</p><p>(ii) We have presented the decoding result of the <italic>P<sub>prior</sub></italic> in Figure 5B and compared it with the <italic>P<sub>com</sub></italic> decoding result in Figure 5F. We found that the premotor neurons encode the <italic>P<sub>prior</sub></italic> (previous trial’s <italic>P<sub>com</sub></italic>) during the baseline period, where there is no sensory input.</p><p>(iii) We have now reported the disparity decoding results in Figure 5—figure supplement 2, and verified it with the number of units (<xref ref-type="fig" rid="sa2fig5">Author response image 5</xref>). The result of population decoding (<xref ref-type="fig" rid="sa2fig5">Author response image 5</xref>) showed a systematic sequential procession of causal inference in the frontoparietal circuit again. At first, the brain encodes the <italic>P<sub>prior</sub></italic> before the multisensory disparity onset (the <italic>P<sub>prior</sub></italic> can be decoded at the baseline period), after that the <italic>P<sub>com</sub></italic> is estimated by combining the <italic>P<sub>prior</sub></italic> and multisensory disparity. Note that the <italic>P<sub>com</sub></italic> and disparity can only be decoded after visual stimulus onset, and the disparity signal emerges earlier than the signal of <italic>P<sub>com</sub></italic>.</p><fig id="sa2fig5" position="float"><label>Author response image 5.</label><caption><title>Population decoding of <italic>P<sub>com</sub></italic> (upper panel), <italic>P<sub>prior</sub></italic> (middle panel), and Disparity (bottom panel) with different neuron numbers of premotor (left panel) and parietal (right panel).</title><p>The horizontal dashed black line represents the chance level. The horizontal solid bars at the top represent the time of significant decoding accuracy (cluster-based permutation test, <italic>p</italic> &lt; 0.05). Different color shades represent different numbers of neurons used for decoding.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig5-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Showing the actual data: The key results (e.g. Figure 3C; 4F; 5B; 5F; 6C) would be much stronger rand more convincing if the actual units of analysis were shown in some ways. How does decoding accuracy vary across neurons?</p></disp-quote><p>We have shown the actual units of analysis for each result (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>; Figure 3C; Author response images 2, 3 and 4). Furthermore, we have tested it with different numbers of units to decode <italic>P<sub>com</sub></italic>, <italic>P<sub>prior</sub></italic>, disparity, and arm locations, respectively (Author response images 2 and 3). The same conclusion holds.</p><disp-quote content-type="editor-comment"><p>Other details:</p><p>For most tests there are no measures of effect sizes reported, sometimes the respective test-statistics is missing, and the degrees of freedom remain very unclear. I understand that they wary between analysis, but given that some tests are based on the actually recorded units, some of pseudo-trials or binned data, it would be very important to report for each test the assumed independent units and their number. The false-discovery rate is mentioned frequently, but the precise method is not stated. Most analyses are based on Wilcoxon tests, but figures show mean and SDs. I encourage the authors to use the same nonparametric (or parametric) approaches for figures and stats (e.g. show boxplots and individual data). L. 892: what was precisely compared with the ANOVA? Cluster-based tests: the parameters and the procedures for this test are not reported (l.974)</p></disp-quote><p>(i) Rank-biserial correlation was used to estimate nonparametric test effect size (Kerby, Comprehensive Psychology, 2014). The degrees of freedom and effect size were added to the revised figure legend, where a nonparametric test was performed.</p><p>(ii) Benjamini-Hochberg procedure was used to correct the false discovery rate (FDR) (see revised Methods, L. 914).</p><p>(iii) We have shown the individual data and used the boxplots (see revised Figures 2B, C, and D and 3C).</p><p>(iv) For L. 892 now L. 982, two-way ANOVA was performed with factors of Condition (2 levels: VP and P) and target location (5 levels).</p><p>(v) The detail of cluster-based permutation tests was added in Methods (L. 1059) as follows:</p><p>“The significant time duration was determined using a cluster-based permutation test for multiple comparisons across time intervals (permutations = 5000; cluster-level statistic: sum of the t values in a cluster; auxiliary cluster defining threshold t = 3) (Gramfort et al., 2013).”</p><disp-quote content-type="editor-comment"><p>To determine whether a neuron confirms to the expectation of causal inference, why is it necessary to bin the data (l 893ff)? Could one not simply derive a regression model for each neuron and visualize the R2 or F-value?</p></disp-quote><p>Since the <italic>P<sub>com</sub></italic> is a hidden variable predicted by the BCI model, it cannot be directly obtained from the drift of one trial. To estimate the <italic>P<sub>com</sub></italic> of each trial, we first simulated 5000 trials under each disparity using the fitted BCI model and grouped these trials into 29 bins. Thus, the <italic>P<sub>com</sub></italic> of each simulated trial can be obtained by the BCI model. The <italic>P<sub>com</sub></italic> of the simulated trials within each bin were averaged to obtain the <italic>P<sub>com</sub></italic> of each bin. Finally, each behavior trial was assigned to one of these bins according to its proprioceptive drift and disparity, and the <italic>P<sub>com</sub></italic> of each trial was assigned according to the bin it belongs to. Thus, the correlation between VP-weight and <italic>P<sub>com</sub></italic> can be calculated across 29 bins.</p><disp-quote content-type="editor-comment"><p>The authors seem to interpret differences in the significances (e.g. of cluster-based permutation tests) as significant differences between regions and as establishing differences in the relative timing of effects. These are statistical fallacies (e.g. Sassenhagen https://doi.org/10.1111/psyp.13335; and Makin https://doi.org/10.7554/eLife.48175).</p></disp-quote><p>We do not rely upon the cluster-based permutation tests to determine differences in the significance as significant differences between regions. Rather, we adopted a randomization test (Panichello and Buschman, bioRxiv, 2020) ( also see Methods, L. 1063):</p><p>“To test whether premotor neurons encode <italic>P<sub>com</sub></italic> earlier than parietal neurons, a randomization test was performed between them. Neurons with more than 50 trials in each <italic>P<sub>com</sub></italic> group were included in this analysis. The corresponding numbers (here, 50 neurons per region) of neurons were randomly exchanged between the paired regions 1,000 times to generate a null distribution (chance level) of time lags. The significance was determined by a permutation test of the true time lag from the original data and the null distribution (Panichello and Buschman, 2020).”</p><disp-quote content-type="editor-comment"><p>For every statement claiming differences between parietal and premotor cortex it is necessary to directly impellent the respective contrast between neurons in each brain region to support such a difference.</p></disp-quote><p>We have added the direct comparison results in the revised manuscript and response letter. In brief, at the single-neuron level, we directly compared the neural information for the arm location (Figures 3C, <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, and Figure 6B) and <italic>P<sub>com</sub></italic> (Figures 3H and <xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>) between the parietal and premotor cortex. At the population level, we conducted population decoding (Figures 4E, 5B, and 6C) and the joint peri-event canonical correlation (jPECC) analysis (Figure 4F) to demonstrate the difference between these regions.</p><disp-quote content-type="editor-comment"><p>Other methods:</p><p>Spike sorting: I could not find criteria used for spike sorting. Where the analyzed units single units or MUA? More details about spike thresholds, cluster separation etc. should be provided.</p></disp-quote><p>All spike data were re-sorted using off-line spike sorting clustering algorithms (Plexon, principal component analysis). With manual adjustments, only well-isolated units were considered for further analysis (signal-to-noise is more significant than 3). As shown in the revised Methods (L. 786):</p><p>“On-line raw neural signals were processed offline to obtain a single unit by Offline Sorter (Plexon Inc, Dallas, TX). All spike data were re-sorted using off-line spike sorting clustering algorithms (Plexon, principal component analysis). The auto sorted neurons were then refined manually, and only well-isolated units were considered for further analysis (signal-to-noise is larger than 3). The sorted files were then exported as MATLAB format for further analysis in MATLAB (Mathworks, Natick, MA, USA) and Python (The Python Software Foundation).”</p><disp-quote content-type="editor-comment"><p>The total number of switches between blocks (e.g. P following VPC) should be reported, as this constitutes the effective degrees of analysis of the block switching analysis (Figure 2C).</p></disp-quote><p>There are 282 P bocks following VPC blocks and 240 P bocks following VP blocks.</p><disp-quote content-type="editor-comment"><p>Causal inference models and optimization: The methods leave it unclear how the two alternatives of common and separate sources were combined in the BCI model. Previous work has explored a number of decision functions (e.g. Rohe and Noppeney's work, or Wozny et al. PlosCompBiol 2010) but for the present study it remains unclear which decision function was used. Model fitting: how were likelihoods computed and the posteriors sampled for model fitting? I feel that the procedures are not described in sufficient detail to be reproduced. Over what range of disparities was the model optimized? This is important for the Null model mentioned later on. What is the precise number of data points that entered the BIC calculation?</p></disp-quote><p>(i) We used the model average decision function to estimate the final arm location. We have revised the methods accordingly in the Methods (L. 836):</p><p>“Here, we used model average decision function to estimate final arm location (Fang et al., 2019):</p><p><inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow/><mml:mo>|</mml:mo><mml:mrow/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow/><mml:mo>|</mml:mo><mml:mrow/><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.”</p><p>(ii) We used maximum likelihood estimation for model fitting, which is described in the Methods (L. 844):</p><p>“To estimate the best-fitting model parameters in the BCI model, for each recording session, an optimization search was implemented that maximized the log likelihood of each model given the monkey’s data under the VPC task. The prior probability of a common source (<italic>P<sub>prior</sub></italic>) and visual and proprioceptive standard deviations, σ<italic><sub>V</sub></italic> and σ<italic><sub>P</sub></italic>, respectively, were set as free parameters to be optimized. Five thousand trials per disparity were simulated for each optimization step to obtain the distribution, and the sum log likelihood of the observations given the model was calculated for each disparity. Then, the parameters were optimized by minimizing the sum log likelihood using a genetic algorithm (ga function in MATLAB).”</p><p>(iii) The total sessions of each monkey that entered the BIC calculation were 68 days (Monkey H), 90 days (Monkey N), and 85 days (Monkey S), respectively.</p><disp-quote content-type="editor-comment"><p>Markov analysis: If I understood it correctly, SigmaA and SigmaV are fit to the entire block, and the Pprior derived from the entire block was used as starting value for this parameter? The authors conclude (l 256ff) that to 'maintain a consistency of causal inference, sensory uncertainty … is updated ' as well. However, the Markov model seems to focus only on the updating of the prior.</p></disp-quote><p>(i) Yes, Σ P, Σ V, and <italic>P<sub>prior</sub></italic> derived from the entire block were used as starting values for the parameter, and only the <italic>P<sub>prior</sub></italic> was free in the Markov model fitting (see Methods L. 887):</p><p>“During the model fitting, we first used the Bayesian causal inference model (as mentioned before) to search the overall <italic>P<sub>prior</sub></italic>, σ<italic><sub>P</sub></italic>, and σ<italic><sub>V</sub></italic> for each session/day, which were used as initial parameter in the subsequent Markov model. The σ<italic><sub>P</sub></italic> and σ<sub><italic>V</italic></sub> were fixed during the model fitting.”</p><p>(ii) The referee raised an interesting question. However, as the sensory uncertainty cannot be estimated at a trial-by-trial level, the Markov model did not include it.</p><disp-quote content-type="editor-comment"><p>Processing of single unit data: In my view the paper would profit from showing actual single neuron PSTH's and how smoothing effected these. The methods (l. 857) mention a 400ms sliding window, but the periods of interest (e.g. target holding) are only minimally longer than this (500ms). This makes we worried that the analyzed data effectively blurs neural representations across epochs and is affected by movement artifacts. When computing the modality contributions to each response, what task epoch was analyzed to derive the tuning curves (l. 869ff)?</p></disp-quote><p>We have verified the decoding with different smoothing windows (200, 300, and 400 ms) (<xref ref-type="fig" rid="sa2fig6">Author response image 6</xref>). The same conclusion holds.</p><fig id="sa2fig6" position="float"><label>Author response image 6.</label><caption><title>Population decoding of <italic>P<sub>com</sub></italic> with various smoothing window sizes (200, 300, and 400 ms), respectively.</title><p>The horizontal dashed black line represents the chance level. The horizontal solid bars at the top represent the time of significant decoding accuracy (cluster-based permutation test, <italic>p</italic> &lt; 0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig6-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Overall there are many seemingly arbitrary choices in the methods. These include the thresholds to define neurons as 'causal inference', the number of trials required for neurons to be included in the population analysis (l. 964), the duration of smoothing kernels and temporal analysis windows (l. 848 ff), the binning of data for neuro-behavioral correlation (l. 893ff), in the generation of population patterns (l. 912ff), in the cluster-based test (not reported!). It would be good to see a justification for these choices or to learn whether the authors ensured that their main results do not depend on these precise choices.</p></disp-quote><p>We are grateful for the referee’s suggestion and have revised the Methods accordingly.</p><p>(i) The definition of ‘causal inference neuron’ follows our previous study (Fang et al., PNAS, 2019). The basic idea here is that the pattern of neuronal activity across 29 bins (see the above response) should correlate with the <italic>P<sub>com</sub></italic> predicted by the BCI model. The significant level of <italic>p</italic> = 0.05 of this correlation was used as the threshold to define the ‘causal inference neuron’.</p><p>(ii) Only neurons with at least 50 trials of each condition were included in the analysis. We have added this information in the revised Methods (L. 1064).</p><p>(iii) The duration of smoothing kernels and temporal analysis windows were often used in previous studies (Fang et al., PNAS, 2019; Gu et al., Cerebral Cortex, 2016; Fried et al., Neuron, 2011).</p><p>(iv) As the neurons in our study did not record simultaneously, we generated a pseudo-simultaneous population by the bootstrap method.</p><p>(v) For the cluster-based test, the details of cluster-based tests were added in the revised Methods (L. 1059) as follows:</p><p>“The significant time duration was determined using a cluster-based permutation test for multiple comparisons across time intervals (permutations = 5000; cluster-level statistic: sum of the t values in a cluster; auxiliary cluster defining threshold t = 3) (Gramfort et al., 2013).”</p><disp-quote content-type="editor-comment"><p>4) The authors do not justify why they recorded in the transition between F4 (rostral ventral premotor) and F5 (caudal ventral premotor) with head visual/tactile optic flow signals and grasping signals respectively. The obvious target is F2 (dorsal premotor) since it has strong reaching signals and is highly connected with area 5 of the parietal lobe (Rizzolati, 1990; Mendoza and Merchant, 2014). The authors should provide a more detailed account on the areas studied and the criteria adopted to localize the recording sites. The specification that they were &quot;determined by individual MRI atlas&quot; does not warrant for areal identification, because on natural variability. On this regard, the Figure 3A' insets should be adjusted (for parietal recording sites in L and R hemispheres, the sulci orientation should be different, and for the premotor ones sulci should be reported).</p></disp-quote><p>(i) The reaching task we used in our study is an adapted task of rubber hand illusion focused on the multisensory representation of the arm. Previous studies have shown that the ventral premotor cortex and parietal area 5 are highly related to body representation and multisensory perception (see reviews (Blanke, Nat Rev Neurosci, 2012; Graziano and Botvinick, Common Mechanisms in Perception and Action, 2002)). More specifically, ventral premotor neurons have been shown to respond to visual stimuli in the space adjacent to the hand or arm (Graziano et al., Science, 1994), and parietal (area 5) neurons have been shown to respond to the seen position of a dummy arm (Graziano et al., Science, 2000). Furthermore, human fMRI studies have shown that the strength of illusion of the dummy arm was highly correlated to the neural response of ventral premotor (Ehrsson et al., Science, 2004).</p><p>(ii) We thank the referee for the suggestion and have adjusted the Figure 3A insets in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>5) For the discussion and comparison to previous work: The paradigm focuses on visuo-motor paradigm in which the sensory cues are both generated by the subject itself. In contrast, in many classical (e.g. audio-visual; or visual-vestibular)) paradigms both sensory cues are external in nature, and not linked to the subject's action. While in both types of paradigm sensory cues are integrated and can also induce perceptual recalibration, the visuo-motor paradigm still is conceptually distinct and this has implications for the interpretation of the results. The authors should discuss whether they believe that their findings generalize to other paradigms and whether the same or possibly distinct (e.g. parietal) brain regions should be investigated during such paradigms. Such a discussion seems important to place the present work in the context of the plethora of previous work. Indeed, the present study is completely lacking discussion of results with respect to current knowledge on the functional properties of premotor and parietal neurons subtending reaching. The literature on this topic is vast, but the following studies, as examples, could be relevant in this context:</p><p>1. Archambault et al. J Neurosci 2011 (comparison on premotor vs parietal, where premotor activity leads parietal one)</p><p>2. Caminiti et al. eNeuro, 2017 (overall picture of connectivity of fronto-parietal network with updated literature on functional properties of different areas)</p><p>3. Caminiti et al. J Neurosci 1991 (first paper on encoding of reaching in Premotor cortex)</p><p>4. Churchland MM, et al. Nature 2012</p><p>5. Cisek and Kalaska, Neuron 2005 (on premotor activity during reaching)</p><p>6. Gail and Andersen J Neurosci 2006 (on neural dynamics of sensorimotor transformations in parietal cortex)</p><p>7. Jerjian SJ, Sahani M, Kraskov A ELife 2020 (on movement representation in premotor cortex)</p><p>8. Jiang X et al. Cell Rep 2020 (onpremotor neural Activity during Observed and Executed Movements)</p><p>9. Mountcastle et al. J Neurophysiol 1975 (first pioneering study on the role of parietal cortex in visuomotor control)</p><p>10. Pezzulo et al. Progr Neurobiol 2022 (on the neural dynamics of premotor neurons during action execution and observation)</p><p>11. Santhanam et al. J Neurophysiol 2009</p></disp-quote><p>We are grateful for the references that the referee suggested. The references now have been cited accordingly (L.596, 638).</p><p>Although a reaching task was used to require monkeys to report their arm locations, the neural signal about <italic>P<sub>com</sub></italic> was observed before reaching. The current study focused on multisensory integration and causal inference, which merely relied on sensory inputs (V, P, and disparity). The reaching behavior in the task could be the action signal guided by the <italic>P<sub>com</sub></italic>.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The authors did a good job at answering all the reviewers' comments in the rebuttal, particularly the once regarding analysis and statistically details. However, the consensus of the reviewers is that no real changes in the structure of the paper were carried out to simplify the framing of the manuscript and make it more accessible to a larger audience. In addition, all the reviewers are concerned with the lack of rational regarding the recording locations in the main text.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>Although the authors did a good job at answering all the reviewers' comments in the rebuttal, particularly the once regarding analysis and statistically details. However, many of the framing and conceptual comments were not really incorporated in the actual reviewed manuscript. Specifically:</p><p>1) Please start the paper by giving an intuitive example of the key problem addressed in the manuscript.</p></disp-quote><p>We have added an intuitive example in the introduction section to highlight the main questions we addressed in this study in the revised manuscript (L. 54-58).</p><disp-quote content-type="editor-comment"><p>2) There is no change in the introduction and the Results sections regarding a simpler and more intuitive framing of the paper (Figure 2A is the same). Again, the reader needs to go quite further into the manuscript to understand the main question and how the authors implemented the experiment.</p></disp-quote><p>As suggested by the referee, we have revised the results and added new paragraphs of the hypotheses in the present study to give a simpler framing of the paper (Line 123-144), which explicitly elaborates Figures 1C-F.</p><disp-quote content-type="editor-comment"><p>3) The paper should refer to the classical notions of sensory motor integration in the parieto-premotor circuit in the discussion.</p></disp-quote><p>As suggested by the referee, we have now added the discussion about the relationship between our study and previous findings of sensorimotor representations in the frontoparietal circuit as follows (L. 682-714; L. 737-745).</p><p>“The frontoparietal circuit, including the premotor and parietal cortices, has long been recognized as a central area in sensorimotor representations (Caminiti et al., 2017; Caminiti et al., 1991). Although the present experiments shared many movement features in the reaching task, the key findings of causal inference processing are unlikely to be explained by the kinematical components. First, previous studies have demonstrated that the neuronal activities in the premotor cortex are related to hand kinematics (e.g., hand position, speed, and direction) in the motor planning and execution (Caminiti et al., 1991; Churchland et al., 2006), which lead the neural activities in the parietal cortex (Archambault et al., 2011). However, in our study, the early activities of <italic>P<sub>com</sub></italic> in the premotor cortex cannot be purely induced by the sequential activities of kinematics in the premotor and parietal cortices. Because the <italic>P<sub>com</sub></italic> is abstract information, and its activity pattern is not correlated with any kinematical components. Expressly, under a given value of <italic>P<sub>com</sub></italic>, the reaching kinematics can be varied (e.g., the hand position can be anywhere on the table according to the target position and disparity in a given trial). Moreover, the neural signals about <italic>P<sub>com</sub></italic> in the premotor cortex were observed before the target onset, where no motor planning was possible during this period. Thus, our results are consistent with the idea that the high-level information, such as abstract and hidden structures, potential probability of multiple motor options, and visual-proprioceptive integration, are encoded in the frontoparietal circuit, which could later integrate with the low-level sensory representations to guide the desired movement (Cisek and Kalaska, 2005; Gail and Andersen, 2006; Limanowski and Blankenburg, 2016).</p><p>Second, the dynamic updating of prior and sensory representation proposed a putative mechanism for multisensory recalibration in sensorimotor tasks. At the behavioral level, our results are in accord with the observations that sensory perception is modulated by a multisensory context with sensory conflicts. The BCI theory thus provides a framework to explain how the multisensory context (e.g., the prior of common source) modulates the sensory representations, such as sensory uncertainty in our study and sensory estimation (e.g., spatial localizations) in previous sensorimotor studies (Badde et al., 2020; Bruns and Roder, 2015; Park and Kayser, 2019; Van der Burg et al., 2013). The results support the notion of dynamic representations of <italic>P<sub>com</sub></italic> in the present study – the top-down signal of common source from the premotor cortex modulates the spatial tuning in the parietal cortex and then guides hand estimation.</p><p>[…]</p><p>Intriguingly, our results seem complementary to previous findings of mirror neuron systems in the premotor and parietal cortices in both humans and monkeys. Typically, a mirror neuron fires both when individual acts and when the individual observes the same action performed by another. That is, the mirror neuron is believed to mediate the understanding of others' behavior (Jerjian et al., 2020; Jiang et al., 2020; Pezzulo et al., 2022). By contrast, the role of causal inference neurons in our study was putatively participating in self-identification and self-other discrimination. Future studies are needed to examine how these two systems work together to identify both self and foreign agents”.</p><disp-quote content-type="editor-comment"><p>4) The authors did not mention why they recorded ventral premotor and a mix of area 5 and 7a in the main text.</p></disp-quote><p>The recording regions were chosen based on previous monkey electrophysiological and human studies. In brief, previous studies showed that the premotor (including dorsal and ventral premotor) and posterior parietal cortices (including areas 5 and 7) are related to visual-somatosensory integration and arm recognition (see the reference below). However, there is no direct evidence showing a specific region for the visual-proprioceptive integration and causal inference processes. As the first study of causal inference integration of visual and proprioceptive signals, we decided to record neurons in both dorsal and ventral premotor and posterior cortices, including areas 5 and 7. In fact, we did not find a significant difference between the sub-regions in the premotor cortex and the sub-regions in the parietal cortex (see <xref ref-type="fig" rid="sa2fig7">Author response image 7</xref>, <italic>p</italic> &gt; 0.05). We have added the reasons why we chose the recorded regions in the revised manuscript as follows (see L. 296-312 in the main text).</p><p>“Previous studies showed that the premotor and parietal cortices were highly involved in body representation and multisensory perception (see reviews (Blanke, 2012; Graziano and Botvinick, 2002)). In monkeys, bimodal neurons with visual and somatosensory receptive fields were found in both premotor (including F2vr in dorsal premotor and F4/F5 in ventral premotor) and posterior parietal cortices (including area 5 and area 7) (Fogassi et al., 1999; Graziano et al., 2000; Graziano and Gross, 1993, 1998; Graziano et al., 1994). Specifically, ventral premotor neurons responded to visual stimuli in the space adjacent to the arm (Graziano and Gross, 1998; Graziano et al., 1994). The bimodal neurons in the parietal cortex (area 5 and area 7) showed to respond to both the real arm position and the seen position of a dummy arm (Graziano et al., 2000), which have a significant projection of the premotor cortex (Graziano and Gross, 1998). Consistently, human fMRI studies found that the posterior parietal and premotor (dorsal and ventral) cortices selectively respond to visual stimulation near the hand (Brozzoli et al., 2011) or the dummy hand near one’s corresponding hand (Blanke et al., 2015; Ehrsson et al., 2004). A human MEG study also revealed that the activities in the prefrontal and intraparietal sulcus were related to the causal inference computation in visual-auditory integration (Cao et al., 2019; Rohe et al., 2019)”.</p><fig id="sa2fig7" position="float"><label>Author response image 7.</label><caption><title>(A) Left and middle: the fraction of selective neurons in the dorsal premotor (PMd) cortex and the ventral premotor (PMv) cortex (ANOVA, main effect, <italic>p</italic> &lt; 0.05).</title><p>There was no significant difference between the fraction of selective neurons in PMd and PMv (Pearson's chi-square test, χ<sup>2</sup> = 0.013, df = 2, <italic>p</italic> = 0.99). Right, the fraction of causal inference neurons in the dorsal premotor cortex and ventral premotor cortex. There was no significant difference between the fraction of causal inference neurons in PMd and PMv (χ<sup>2</sup> = 0.15, df = 1, <italic>p</italic> = 0.70). (B) Left and middle: the fraction of selective neurons in the parietal Area 5 cortex and Area 7 cortices (ANOVA, main effect, <italic>p</italic> &lt; 0.05). There was no significant difference between the fraction of selective neurons in parietal Area 5 and Area 7 cortices (Pearson's chi-square test, χ<sup>2</sup> = 0.047, df = 2, <italic>p</italic> = 0.98). Right, the fraction of causal inference neurons in the parietal Area 5 cortex and the parietal Area 7 cortex. There was no significant difference between the fraction of causal inference neurons in parietal Area 5 and Area 7 (χ<sup>2</sup> = 1.023×10<sup>-30</sup>, df = 1, <italic>p</italic> = 1).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig7-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In my previous review I pointed out that, although the experimental paradigm was overall well designed and the data analysis technically sophisticated, the manuscript was flawed in several aspects, particularly in relation to the way the paper was written, and the data reported and discussed.</p><p>Despite the extensive point-to-point reply, the revision of the paper remains disappointing, as no substantial changes have been made to consider the criticisms. As a matter of fact, the new version of it is essentially identical to the original one in all its sections (Abstract, Introduction, Results and Discussion). Surprisingly enough, despite the authors' attempt to reply with accuracy to the different issue raised in the reviewing process, no significant improvement of the resubmitted manuscript was achieved, as in most instances all new information was not fully integrated in the revised version.</p><p>The authors were invited to place the present work in the context of the extensive literature on the neurophysiology of the parieto-frontal network, with special attention to its role on reaching movements. In fact, the original manuscript did not adequately discuss the results within the conceptual frame offered by the knowledge accumulated over the last forty years on the dynamic properties of premotor and parietal neurons subtending arm movements. This suggestion was completely ignored, as both Discussion and Introduction remained virtually identical across versions and the authors just added a few references, among those suggested by the reviewers, in a rather superficial fashion, without any emphasis about how they were related to findings and conclusions of the present study.</p></disp-quote><p>We thank the referee for reminding us of the changes to the Discussion and Introduction. In the revised manuscript, we first added the hypotheses at the beginning of the results to give a simpler framing of the study, explaining the logic before we showed the results. We then added the reasons we chose to record the ventral and dorsal premotor and parietal (areas 5 and 7) cortices in the results before we showed the neural data. Finally, we modified the discussion to discuss that although our experiments shared many movement features in the reaching task, the key findings of causal inference are unlikely to be explained by the kinematical components during the arm movements. In the following, we list the responses point-by-point:</p><disp-quote content-type="editor-comment"><p>Concerning point (1) the authors' action was limited to the mere insertion of new titles at the beginning of some paragraphs. In their response, it is reported that the logic of the manuscript is outlined in the unchanged Fig. 2A, which was already present in the previous version. Therefore, no significant change has been made to take into account this aspect.</p></disp-quote><p>1) We have added an intuitive example in the introduction section to highlight the main questions we addressed in this study as follows (L. 54-58).</p><p>“For instance, in the ventriloquism illusion, when the audience is presented with a synchronous but spatially discrepant audiovisual stimulus (e.g., a speech sound from the speaker and a visibly moving mouth of the puppet), they usually infer these audiovisual stimuli are coming from a common source and illusive perceive the speech coming from the puppet.”</p><p>2) We have added an overview of the main predictions of this study at the beginning of the results section--“Behavioral paradigm and hierarchical Bayesian causal inference model” in the revised manuscript (L.123-144).</p><disp-quote content-type="editor-comment"><p>Furthermore, the selected units shown in Fig. 3D to offer an example of neural activity in form of raster plots and mean firing rates (not histograms, as stated) are not indicative of clear response modulation.</p></disp-quote><p>To better illustrate the response modulation by the disparity, we plotted the mean responses of the example neuron in Figure 3D during the target-holding period (Figure 3 and Figure 3-Figure supplement 2). Figure 3 shows a “segregation (P) neuron”, which is more active during the P task and exhibited increased activity under the large disparities in the VPC task. By contrast, Figure 3 shows an “integration (VP) neuron”, which is more active during the VP task and exhibited increased activity under the small disparities in the VPC task.</p><disp-quote content-type="editor-comment"><p>The mentioned Table 1 is neither reported in the main text, nor in Supplementary Material.</p></disp-quote><p>We indeed reported Table 1 in our last revision, but it was required to be submitted as an independent document. In this round of revision, we resubmit it as Supplementary file 1. Model parameters and fitting evaluations of two models for monkeys.docx.</p><disp-quote content-type="editor-comment"><p>When asked to evaluate the temporal difference between premotor and parietal activity, the authors just replied that &quot;The population decoding of Pcom (Figure 4E) indicated that the premotor cortex leads the parietal by about 300 ms&quot;, but this observation refers to what already shown in the earlier version of the manuscript. Even in this case, in fact, no change was made to the analyses and to the text to take this point into consideration.</p></disp-quote><p>To further confirm the significance of the temporal difference between the premotor and parietal cortices, a randomization test was performed on the population decoding of <italic>P<sub>com</sub></italic> between these two regions. Neurons with more than 50 trials in each <italic>P<sub>com</sub></italic> group were included in this analysis. The corresponding numbers (200 neurons per region) of neurons were randomly exchanged between the paired regions 1,000 times to generate a null distribution (chance level) of time lags, and the significance was determined by a permutation test of the true time lag from the original data and the null distribution (Panichello &amp; Buschman, 2021). The results again showed that the <italic>P<sub>com</sub></italic> information appeared to occur significantly earlier in the premotor cortex than in the parietal cortex (Figure 4-Figure supplement 3, randomization test, <italic>p</italic> &lt; 0.01). Furthermore, we also confirmed the result using the jPECC analysis in the earlier version of the manuscript (Figure 4F) and discussed the limitation of the asynchronous single-unit recording (L. 1293-1296).</p><disp-quote content-type="editor-comment"><p>Also concerning the spike sorting technique adopted, despite the reviewer's request, no further details have been provided, relative to what was already reported in the first version of the manuscript.</p></disp-quote><p>We have added the details of spike sorting and the reference in the revised Methods (L. 903-910), as follows:</p><p>“All isolated neurons were recorded regardless of their activity during the task, with the recording locations varying from session to session. At each location, the raw extracellular membrane potential was sampled at 40 kHz. Online raw neural signals were processed offline to obtain a single unit by Offline Sorter (Plexon Inc, Dallas, TX). All spike data were re-sorted using offline spike sorting clustering algorithms (Offline Sorter, principal component analysis) (Merchant et al., 2013). With manual adjustments, only well-isolated units were considered for further analysis (signal-to-noise is larger than 3).”</p><disp-quote content-type="editor-comment"><p>Despite the explicit request (see point 4) to provide more details on which premotor area was considered in this study, the authors persist in referring loosely to &quot;premotor&quot; cortex, not specifying exactly which among the different premotor areas is being studied, apart from the details provided graphically in the brain figurine. Given the different functional properties and characteristics in connectivity among different premotor regions, it is inappropriate and misleading from a neurophysiological perspective to simply refer to premotor cortex. Provided that the region of recording mainly encompasses premotor area F2vr (medial to the spur of the arcuate sulcus), a small and medial part of premotor area F5, and part of F4, as evident from Fig. 3A, the main text did not report the rationale underlying the selection of the F2vr, F4/F5 for the present study. In addition, according to the new details provided in the current version of Fig. 3A on recording sites, some of the penetrations (corresponding to about 40-45 collected units) belong to prefrontal area 8 (FEF). These cells should be removed from the premotor database.</p></disp-quote><p>We thank the referee for the acute observation. We have now added the reasons why we chose the recorded regions in the revised manuscript (L. 296-312). In the revised results, we removed the FEF neurons from the neuron database, conducted the same analysis, and updated all the relevant figures accordingly (Figures 3A, 3C, 3G, 3H, 4C, 4D, 4E, 4F, 5A, 5B, 5C, 5E, 5F, 6B, 6C, Figure 3-Figure supplement 1, Figure 4-Figure supplement 1, Figure 4-Figure supplement 2, Figure 5-Figure supplement 1, and Figure 5-Figure supplement 2). Note that the same conclusion holds.</p><p>The recording regions were chosen based on previous monkey electrophysiological and human studies. <bold>Please also see our response to Reviewer 1 (Question 5)</bold>. The recording areas include (1) both ventral and dorsal premotor cortices; (2) both inferior (area 7) and superior (area 5) parietal lobes. Indeed, the previous studies in the parietal cortex also showed that area 7 neurons also responded to the multimodal sensory inputs. For example, Graziano and colleagues showed the visual receptive fields of the visual-somatosensory bimodal neurons in area 7 were fixed to the relevant body part, which was similar to the bimodal neurons in area 5 and premotor cortices (Graziano and Gross, 1993; Graziano et al., 2000). Our further analysis confirmed that there was no significant difference in the percentage of causal inference neurons between the subregions in the premotor and parietal cortices.</p><p>We thank the referee’s suggestions. In the revised manuscript, we have now claimed that the recording regions included both areas 5 and 7 in the parietal cortex (L. 312-315). In Figure 3A, the straight dash grey line roughly separated the dorsal and ventral parts of the premotor cortex in the middle panel. The straight dash grey line roughly indicates the middle between IPS and CS. The circular dash lines indicate the recording chambers. We have added this information in the revised manuscript (L. 338-340).</p><disp-quote content-type="editor-comment"><p>Beyond the graphical (subjective) representation, no other information is provided about the criteria adopted to identify the recording areas and sites. First, as pointed out by the reviewers, the specification that they were &quot;determined by individual MRI atlas&quot; does not warrant for any areal identification; second is not even vaguely informative on how frontal and parietal areas were identified. The sentence that &quot;The location of the recording chamber on each animal was determined by an individual MRI atlas&quot; remained unchanged in the revised version of paper, without any further details, not even providing the reference on which Atlas has been used.</p></disp-quote><p>We collected the structural magnetic resonance images (MRI) of three monkeys (3T, Center for Excellence in Brain Science and Intelligence Technology, Institute of Neuroscience, Chinese Academy of Sciences), while they were in an MRI-compatible Horsley-Clarke stereotaxic apparatus. The location of the recording chamber on each animal was determined by the atlas with the origin at the Ear Bar Zero (Saleem &amp; Logothetis, 2012). The centers of implanting recording chambers were [right: 20.0 mm; forward: 10.0 mm] for the premotor cortex in Monkey N, [left: 21.9 mm; forward: 24.9 mm] for the premotor cortex in Monkey H, [right: 14.7 mm; forward: 1.1 mm] for the parietal cortex in Monkey N, and, [left: 17.0 mm; forward: 3.5 mm] for the parietal cortex in Monkey S (see <xref ref-type="fig" rid="sa2fig8">Author response image 8</xref>). We have added this information in the revised manuscript (L. 891-901).</p><fig id="sa2fig8" position="float"><label>Author response image 8.</label><caption><title>The centers of implanting recording chambers in MRIs for individual monkeys.</title><p>CS, central sulcus; IPS, intraparietal sulcus. Note that the recording locations of monkey N’s premotor and parietal cortices are from different structural magnetic resonance images.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76145-sa2-fig8-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Finally, another reason of concern highlighted in the revision process refers to the lack of eye movements data, given that eye position and saccade direction exerts a well-known, although quantitatively different, influence on premotor and parietal neural activity. The authors did not refer to this critical aspect at all, nor did they discuss how eye-related signals might have influenced and eventually contaminated the reported findings.</p></disp-quote><p>We trained the monkeys to perform the task without their eye fixed, but the eye movement during the recording sessions was recorded. To examine whether the updating of sensory uncertainty was correlated with the uncertainty of eye position between VP and VPC (0°) tasks. We identified the eye fixation position at the target holding period. We examined the divergence of eye fixation position in VP and VPC (0°) tasks and found that there was no significant difference between these tasks (Figure 2-Figure supplement 2, Wilcoxon signed-rank test, <italic>p</italic> = 0.81).</p><p>Moreover, to examine whether the neural activity of <italic>P<sub>com</sub></italic> was correlated with the eye position, we calculated the Pearson correlation coefficients between eye fixation position and VP weight. We found that there was no correlation between the VP weight and the eye fixation position at the population level for both regions (the premotor and parietal cortices) at both horizontal and vertical directions (Figure 3-Figure supplement 3, Wilcoxon signed-rank test, Premotor (horizontal): <italic>p</italic> = 0.11; Premotor (vertical): <italic>p</italic> = 0.86; Parietal (horizontal): <italic>p</italic> = 0.35; Parietal (vertical): <italic>p</italic> = 0.87). Note that the recorded eye movement data used in this analysis included 78 sessions for the premotor cortex and 45 sessions for the parietal cortex. And we added this information in the revised manuscript (L. 369-270, L. 410-415, L. 601-603, and L. 1032-1061).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The authors have addressed most reviewer comments to a sufficient degree. The work is still very dense given the large number of analyses implemented, but I have no specific suggestions for how to change this.</p><p>One remaining shortcoming is that I did not see a specific rationale for the choice of the precise recording locations in the manuscript. In reply to my previous comment, the authors have provided some rather generic text in the rebuttal, but ideally, a clear rationale for the choices should be in the manuscript.</p></disp-quote><p>We have added the reasons why we chose the recorded regions in the revised manuscript (L. 296-312). Please also see our response to Reviewer 1 (Question 5).</p></body></sub-article></article>