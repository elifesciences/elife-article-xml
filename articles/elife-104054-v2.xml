<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">104054</article-id><article-id pub-id-type="doi">10.7554/eLife.104054</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A dynamic scale-mixture model of motion in natural scenes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Salisbury</surname><given-names>Jared M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0004-0215-9717</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6211-6293</contrib-id><email>sepalmer@uchicago.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>Department of Organismal Biology and Anatomy, Department of Physics, Physics Frontier Center for Living Systems, The University of Chicago</institution></institution-wrap><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>09</day><month>12</month><year>2025</year></pub-date><volume>14</volume><elocation-id>e104054</elocation-id><history><date date-type="received" iso-8601-date="2024-10-03"><day>03</day><month>10</month><year>2024</year></date><date date-type="accepted" iso-8601-date="2025-12-08"><day>08</day><month>12</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2023-10-23"><day>23</day><month>10</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.19.563101"/></event></pub-history><permissions><copyright-statement>© 2025, Salisbury and Palmer</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Salisbury and Palmer</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-104054-v2.pdf"/><abstract><p>Some of the most important tasks of visual and motor systems involve estimating the motion of objects and tracking them over time. Such systems evolved to meet the behavioral needs of the organism in its natural environment and may therefore be adapted to the statistics of motion it is likely to encounter. By tracking the movement of individual points in movies of natural scenes, we begin to identify common properties of natural motion across scenes. As expected, objects in natural scenes move in a persistent fashion, with velocity correlations lasting hundreds of milliseconds. More subtly, but crucially, we find that the observed velocity distributions are heavy-tailed and can be modeled as a Gaussian scale-mixture. Extending this model to the time domain leads to a dynamic scale-mixture model, consisting of a Gaussian process multiplied by a positive scalar quantity with its own independent dynamics. Dynamic scaling of velocity arises naturally as a consequence of changes in object distance from the observer and may approximate the effects of changes in other parameters governing the motion in a given scene. This modeling and estimation framework has implications for the neurobiology of sensory and motor systems, which need to cope with these fluctuations in scale in order to represent motion efficiently and drive fast and accurate tracking behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>natural scene statistics</kwd><kwd>visual neuroscience</kwd><kwd>object motion</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>PHY-2317138</award-id><principal-award-recipient><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>PHY-1734030</award-id><principal-award-recipient><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IIS-1652617</award-id><principal-award-recipient><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DMS-2235451</award-id><principal-award-recipient><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cmst727</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>MP-TMPS-00005320</award-id><principal-award-recipient><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EB026943</award-id><principal-award-recipient><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A simple statistical model captures the essential features of object motion in a large database of natural scenes, helping to shed light on the challenges posed to sensory and motor systems.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>One of the great triumphs of theoretical neuroscience has been the success of the efficient coding hypothesis (<xref ref-type="bibr" rid="bib5">Barlow, 1961</xref>), which posits that sensory neural systems are adapted to the statistics of the organism’s natural environment. The importance of this hypothesis lies in its power to explain structural features of the nervous system, such as the shapes of nonlinear response functions (<xref ref-type="bibr" rid="bib49">Laughlin, 1981</xref>) and receptive fields of sensory neurons (<xref ref-type="bibr" rid="bib83">Srinivasan et al., 1982</xref>; <xref ref-type="bibr" rid="bib88">van Hateren, 1992</xref>; <xref ref-type="bibr" rid="bib24">Dan et al., 1996</xref>; <xref ref-type="bibr" rid="bib55">Machens et al., 2004</xref>; <xref ref-type="bibr" rid="bib28">Doi and Lewicki, 2014</xref>), in terms of its function as an efficient information processing device. The success of this theory, particularly in vision, has sparked significant interest in measuring natural scene statistics (for a review, see <xref ref-type="bibr" rid="bib80">Simoncelli and Olshausen, 2001</xref>) and has continued to yield important results, like the ubiquity of non-Gaussian, heavy-tailed statistics and related nonlinear forms of dependency among scene features (<xref ref-type="bibr" rid="bib74">Ruderman and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib75">Schwartz and Simoncelli, 2001</xref>; <xref ref-type="bibr" rid="bib71">Pitkow, 2010</xref>; <xref ref-type="bibr" rid="bib100">Zoran and Weiss, 2012</xref>).</p><p>The observation of heavy-tailed distributions in the natural world connects with the rich structure that the external environment presents to an organism’s sensors, across a variety of sensory modalities. In any of these input streams, the brain has to pick out the relevant features in this rich input space that are most important for the organism’s survival – to select what matters. Adapting to this kind of structure and maintaining an efficient representation of behaviorally relevant features in the world is a common feature of early sensory systems. Understanding how this is achieved, mechanistically, requires more than just the observation and quantification of heavy tails in natural scenes. To be able to understand how the brain represents this structure efficiently, we need to model it to shed light on potential ways the brain compresses this rich structure into an actionable internal signal.</p><p>Organisms are not passive sensory processors; they must also produce adaptive behavior in a complex and dynamic natural environment, where tasks like capturing prey (<xref ref-type="bibr" rid="bib12">Borghuis and Leonardo, 2015</xref>; <xref ref-type="bibr" rid="bib60">Mischiati et al., 2015</xref>; <xref ref-type="bibr" rid="bib97">Yoo et al., 2020</xref>; <xref ref-type="bibr" rid="bib77">Shaw et al., 2023</xref>), fleeing predators (<xref ref-type="bibr" rid="bib34">Gabbiani et al., 2002</xref>; <xref ref-type="bibr" rid="bib19">Card and Dickinson, 2008</xref>; <xref ref-type="bibr" rid="bib63">Muijres et al., 2014</xref>), and navigating obstacles (<xref ref-type="bibr" rid="bib64">Muller et al., 2023</xref>) are all critical to survival. These behaviors inevitably involve prediction (<xref ref-type="bibr" rid="bib7">Berry et al., 1999</xref>; <xref ref-type="bibr" rid="bib82">Spering et al., 2011</xref>; <xref ref-type="bibr" rid="bib6">Ben-Simon et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Leonardo and Meister, 2013</xref>) in order to compensate for substantial sensory and motor delays (<xref ref-type="bibr" rid="bib33">Franklin and Wolpert, 2011</xref>). The basis for such predictive behavior must be statistical regularities in the environment, but little is known about the statistics of the inputs relevant to such behaviors.</p><p>As a step toward characterizing the statistics of behaviorally relevant quantities in natural scenes, we focus on a feature fundamental to many essential sensation-to-action programs – the motion of objects. Object motion relative to the observer drives oculomotor tracking (<xref ref-type="bibr" rid="bib44">Krauzlis and Lisberger, 1994</xref>; <xref ref-type="bibr" rid="bib38">Hayhoe et al., 2012</xref>) and is an essential part of many crucial behaviors, like prey capture (<xref ref-type="bibr" rid="bib9">Bianco et al., 2011</xref>; <xref ref-type="bibr" rid="bib41">Hoy et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Michaiel et al., 2020</xref>). Specialized circuitry as early as the retina distinguishes between object and background motion (<xref ref-type="bibr" rid="bib67">Ölveczky et al., 2003</xref>; <xref ref-type="bibr" rid="bib3">Baccus et al., 2008</xref>), while entire brain regions in the visual cortex of primates specialize in processing motion (<xref ref-type="bibr" rid="bib56">Maunsell and Van Essen, 1983</xref>), with increasing complexity along the dorsal stream (<xref ref-type="bibr" rid="bib59">Mineault et al., 2012</xref>).</p><p>While previous work has characterized motion in certain cases, often focusing on optical flow due to ego-motion (<xref ref-type="bibr" rid="bib16">Calow and Lappe, 2007</xref>; <xref ref-type="bibr" rid="bib72">Roth and Black, 2007</xref>; <xref ref-type="bibr" rid="bib64">Muller et al., 2023</xref>), little is known about the statistics of object motion in the natural world. To address this, we analyze movies from the Chicago Motion Database (<ext-link ext-link-type="uri" xlink:href="https://cmd.rcc.uchicago.edu/">https://cmd.rcc.uchicago.edu/</ext-link>), which were shot and curated for the purposes of statistical analysis and for use as stimuli for neural recordings and visual psychophysics. Rather than trying to track discrete objects (which may be difficult even to define for some movies, like those of flowing water), we simplify the problem by tracking individual points within the image using classic techniques from computer vision (<xref ref-type="bibr" rid="bib53">Lucas and Kanade, 1981</xref>; <xref ref-type="bibr" rid="bib86">Tomasi and Kanade, 1991</xref>).</p><p>Given a point trajectory, the velocity along that trajectory is a spatially local description of an object’s motion through three-dimensional space, projected onto the two-dimensional surface of a sensor array, such as a retina or camera. We find that point velocity is highly correlated on the sub-second timescale we measure, and therefore point trajectories are highly predictable. More subtly, the distributions of velocity along trajectories exhibit heavy tails and nonlinear dependencies, both across horizontal and vertical velocity components and across time. This suggests the presence of an underlying <italic>scale</italic> variable, or local standard deviation, so the local velocity can be modeled as a Gaussian scale-mixture (GSM) (<xref ref-type="bibr" rid="bib2">Andrews and Mallows, 1974</xref>). These models were developed in previous work examining the responses of filters applied to natural images and sounds (<xref ref-type="bibr" rid="bib92">Wainwright et al., 2001</xref>; <xref ref-type="bibr" rid="bib75">Schwartz and Simoncelli, 2001</xref>). We find that the scale fluctuates within individual trajectories on a relatively short timescale, so it is an essential part of our description of natural motion. Despite considerable differences in the velocity statistics across movies, the dynamic scale-mixture structure is remarkably consistent. This has important implications both for the efficient encoding of motion signals by neurons – which must adapt to the fluctuating scale to make full use of their limited dynamic range (<xref ref-type="bibr" rid="bib32">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib68">Olveczky et al., 2007</xref>; <xref ref-type="bibr" rid="bib51">Liu et al., 2016</xref>) – and for behaviors relying on predictive tracking – which must take into account the highly non-Gaussian statistics of natural motion (<xref ref-type="bibr" rid="bib39">Ho and Lee, 1964</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In order to build up a statistical description of motion in natural scenes, we analyze movies from the Chicago Motion Database, which consists of a variety of movies collected for statistical analysis and for use as visual stimuli in experiments. All movies were recorded using a fixed camera, with scenes chosen to contain consistent, dense motion within the field of view for minutes at a time. Scenes include flowing water, plants moving in the wind, and groups of animals such as insects and fish. While natural visual input is dominated by the global optical flow during eye and head movements (<xref ref-type="bibr" rid="bib64">Muller et al., 2023</xref>), object motion warrants specific attention because it is highly behaviorally relevant for essential behaviors like escape or prey capture. Note that these global and local motion signals are approximately additive, so one can combine them to form a more complete description of motion for a given organism. We analyze a total of 15 scenes, with a resolution of 512 × 512 pixels, each 2<sup>14</sup> = 16,384 frames long at a frame rate of 60 Hz (∼4.5 min). The high resolution, frame rate, and lack of compression of these movies are essential for getting precise motion estimates.</p><p>For each scene, we quantify local motion using a standard point tracking algorithm (<xref ref-type="bibr" rid="bib53">Lucas and Kanade, 1981</xref>; <xref ref-type="bibr" rid="bib86">Tomasi and Kanade, 1991</xref>). A set of tracking points is seeded randomly on each frame, then tracked both forward and backward in time to generate trajectories (see <italic>Materials and methods</italic> for details). Early visual and sensorimotor systems operate on a timescale of tens to hundreds of milliseconds, so we restrict our analysis to short trajectories (64 frames, or ∼1 s long) to reduce the amount of inevitable slippage from the point tracking algorithm. The resulting ensembles (2<sup>13</sup> = 8192 trajectories each) sparsely cover most of the moving objects in each movie (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Automated point tracking reveals a diversity of motion statistics across natural scenes.</title><p>(<bold>A</bold>) Natural movie data analyzed via point tracking yields an ensemble of ∼1-s-long point trajectories. (<bold>B–D</bold>) Raw data summaries for three example movies, (<bold>B</bold>) <monospace>bees8-full</monospace>, (<bold>C</bold>) <monospace>trees14-1</monospace>, and (<bold>D</bold>) <monospace>water3</monospace>. (i) Joint and marginal distributions for horizontal (<inline-formula><alternatives><mml:math id="inf1"><mml:mi>u</mml:mi></mml:math><tex-math id="inft1">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula>) and vertical (<inline-formula><alternatives><mml:math id="inf2"><mml:mi>v</mml:mi></mml:math><tex-math id="inft2">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula>) velocity components. Overlaid isoprobability contours for the joint distributions are <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$p(u,v)=10^{-1}, 10^{-2}, {\rm and}\,10^{-3}$\end{document}</tex-math></alternatives></inline-formula> for <bold>B</bold> and <bold>C</bold> and <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>and</mml:mtext><mml:mspace width="thinmathspace"/><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$p(u,v)=10^{-2}, 10^{-3}, \text{and} \,10^{-4}$\end{document}</tex-math></alternatives></inline-formula> for <bold>D</bold>. (ii) Seven example horizontal velocity component time series. (iii) Horizontal and vertical velocity correlation functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig1-v2.tif"/></fig><p>The focus of our analysis is the point velocity or difference in point positions between subsequent frames: a two-dimensional vector quantity measured in raw units of pixels/frame (this is easily converted to degrees of visual angle per unit of time, given a fixed viewing distance). The key advantage of this analysis is that the velocities are associated in time along a given point trajectory, which cannot be achieved by looking at the optical flow (<xref ref-type="bibr" rid="bib40">Horn and Schunck, 1981</xref>) or motion energy (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>) alone. Note that since tracking is a difficult problem, the distribution of velocity constrained to the ensemble of trajectories differs from the overall distribution, leading to underestimation of variance and kurtosis (see <italic>Appendix 2</italic>). This analysis is also distinct from previous work examining the spatiotemporal power spectra of natural scenes (<xref ref-type="bibr" rid="bib29">Dong and Atick, 1995</xref>; <xref ref-type="bibr" rid="bib10">Billock et al., 2001</xref>), since power spectra measure the globally averaged pairwise correlations between pixels.</p><p>Our understanding of motion in natural scenes must be grounded in what is perhaps the first scientific study of motion in a natural setting: the diffusive motion of pollen particles in water observed by <xref ref-type="bibr" rid="bib15">Brown, 1828</xref>, later described theoretically by <xref ref-type="bibr" rid="bib30">Einstein, 1905</xref>, and <xref ref-type="bibr" rid="bib47">Langevin, 1908</xref>. See <italic>Appendix 1</italic> for a discussion of Brownian motion and its relation to our modeling framework. Briefly, Brownian motion is characterized by a Gaussian velocity distribution with an exponential correlation function.</p><p>Natural scenes are, by definition, as richly varied as the natural world itself; each movie we analyze captures a small slice of this immense diversity. Our selection can be divided into three broad categories – animals, plants (animated by wind), and water – and we present summaries of the raw data for a representative movie from each category in <xref ref-type="fig" rid="fig1">Figure 1B–D</xref>. In contrast to the Gaussian velocity distributions expected for Brownian motion, histograms of the raw velocity data tend to be sharply peaked with long tails (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> for a log-scale plot that emphasizes the tail behavior). Furthermore, the velocity time series exhibit correlation functions with diverse shapes, rather than simple exponential decay.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Velocity distributions are jointly heavy-tailed.</title><p>(<bold>A–D</bold>) Velocity distributions for a single example movie, <monospace>bees8-full</monospace>. The marginal distribution for horizontal velocity (<bold>A</bold>) has much heavier tails than a Gaussian with the same variance and is well fit by a Gaussian scale-mixture model. The joint velocity distribution (<bold>B</bold>) is roughly radially symmetric, which differs substantially from the shuffled (and thereby, independent) distribution (<bold>C</bold>) and indicates a nonlinear dependence between the two velocity components. This dependence is alternatively revealed by the conditional distribution of the vertical velocity given the horizontal velocity (<bold>D</bold>), showing a characteristic bow-tie shape.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig2-v2.tif"/></fig><sec id="s2-1"><title>Heavy-tailed statistics of natural motion</title><p>To examine the heavy-tailed structure of the observed point-trajectory velocity distributions, we pool horizontal and vertical velocity components together for an example movie, <monospace>bees8-full</monospace>, and compare this histogram to a Gaussian distribution with the same variance (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Plotted on a log-scale to highlight the tails, the empirical frequency falls off nearly linearly away from zero, while the Gaussian probability falls off quadratically. The same is true for the other movies in our dataset, pooled by category and all together (Figure 4A–C). Velocity distributions from animal and plant movies tend to have heavier tails, while those of water movies are closer to Gaussian.</p><p>When multiple variables are involved, heavy tails may be associated with a nonlinear form of dependency, as observed in the spatial structure of natural images (<xref ref-type="bibr" rid="bib92">Wainwright et al., 2001</xref>). The same is true for the two velocity components in our data. We illustrate this for <monospace>bees8-full</monospace>, but results are similar for all other movies. In <xref ref-type="fig" rid="fig2">Figure 2B</xref>, we show a heat map of the joint histogram of horizontal and vertical velocities, <inline-formula><alternatives><mml:math id="inf5"><mml:mi>u</mml:mi></mml:math><tex-math id="inft5">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf6"><mml:mi>v</mml:mi></mml:math><tex-math id="inft6">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula>. It is nearly radially symmetric. (For other movies with unequal variance in the two components, distributions are elliptic.) The lack of tilt in the histogram indicates that the two velocity components are uncorrelated (in this context, correlation between velocity components would indicate a tendency for objects within a scene to move along some diagonal axis relative to the camera). However, they are far from independent: when we shuffle the data to break any association between <inline-formula><alternatives><mml:math id="inf7"><mml:mi>u</mml:mi></mml:math><tex-math id="inft7">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf8"><mml:mi>v</mml:mi></mml:math><tex-math id="inft8">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula>, the resulting histogram is no longer radially symmetric but is instead diamond-shaped (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This is, in fact, to be expected, since only Gaussian distributions may be both radially symmetric and separable. We demonstrate this dependency more clearly by plotting the conditional distribution of <inline-formula><alternatives><mml:math id="inf9"><mml:mi>v</mml:mi></mml:math><tex-math id="inft9">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula> for each value of <inline-formula><alternatives><mml:math id="inf10"><mml:mi>u</mml:mi></mml:math><tex-math id="inft10">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula>, normalizing by the peak value at each <inline-formula><alternatives><mml:math id="inf11"><mml:mi>u</mml:mi></mml:math><tex-math id="inft11">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula> for visualization purposes (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The resulting ‘bow-tie’ shape (<xref ref-type="bibr" rid="bib75">Schwartz and Simoncelli, 2001</xref>) indicates that the variance of <inline-formula><alternatives><mml:math id="inf12"><mml:mi>v</mml:mi></mml:math><tex-math id="inft12">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula> conditioned on <inline-formula><alternatives><mml:math id="inf13"><mml:mi>u</mml:mi></mml:math><tex-math id="inft13">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula> increases with the magnitude of <inline-formula><alternatives><mml:math id="inf14"><mml:mi>u</mml:mi></mml:math><tex-math id="inft14">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula>. A simple, quantitative indicator of this nonlinear dependence is the Pearson correlation of the component magnitudes (<inline-formula><alternatives><mml:math id="inf15"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.33</mml:mn></mml:math><tex-math id="inft15">\begin{document}$\rho=0.33$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo>≪</mml:mo><mml:mn>0.01</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$p\ll 0.01$\end{document}</tex-math></alternatives></inline-formula>), which will be zero for any pair of independent random variables.</p><p>The form of the velocity distributions observed above suggests that they can be modeled as GSM distributions. As the name suggests, a GSM distribution is obtained by combining (zero-mean) Gaussian distributions of different scales, parameterized by a positive scalar random variable <inline-formula><alternatives><mml:math id="inf17"><mml:mi>S</mml:mi></mml:math><tex-math id="inft17">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula>. Let <inline-formula><alternatives><mml:math id="inf18"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft18">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> be a Gaussian random variable with mean zero and variance <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$\sigma_{Y}^{2}$\end{document}</tex-math></alternatives></inline-formula>. If <inline-formula><alternatives><mml:math id="inf20"><mml:mi>S</mml:mi></mml:math><tex-math id="inft20">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> is a known quantity <inline-formula><alternatives><mml:math id="inf21"><mml:mi>s</mml:mi></mml:math><tex-math id="inft21">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, then <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$X=Ys$\end{document}</tex-math></alternatives></inline-formula> is simply a Gaussian random variable with mean zero and variance <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$s^{2}\sigma_{Y}^{2}$\end{document}</tex-math></alternatives></inline-formula>. The conditional distribution is given by<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle p(x\vert s) = \mathcal{N}\left(x;0,s^2 \sigma_y^2\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>When <inline-formula><alternatives><mml:math id="inf24"><mml:mi>S</mml:mi></mml:math><tex-math id="inft24">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> is unknown, <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mi>S</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$X=YS$\end{document}</tex-math></alternatives></inline-formula> follows a GSM distribution given by<disp-formula id="equ2"><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  p(x)=\int_0^{\infty} p(x|s)p(s)ds,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf26"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft26">\begin{document}$p(s)$\end{document}</tex-math></alternatives></inline-formula> is a distribution with positive support. A convenient choice is to let <inline-formula><alternatives><mml:math id="inf27"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft27">\begin{document}$S=\exp(Z)$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf28"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft28">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> is Gaussian random variable, which we will refer to as the scale generator, with mean zero and variance <inline-formula><alternatives><mml:math id="inf29"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft29">\begin{document}$\sigma_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula>. Then, <inline-formula><alternatives><mml:math id="inf30"><mml:mi>S</mml:mi></mml:math><tex-math id="inft30">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> follows a log-normal distribution, which simplifies the inference problem significantly, despite the fact that the resulting GSM distribution does not have a closed form. The choice of a log-normal distribution can also be justified by a maximum entropy argument (<xref ref-type="bibr" rid="bib87">Van der Straeten and Beck, 2008</xref>). See <xref ref-type="bibr" rid="bib91">Wainwright and Simoncelli, 2000</xref>; <xref ref-type="bibr" rid="bib92">Wainwright et al., 2001</xref>, for a discussion of the GSM model in the context of wavelet analysis of natural images. Parameters were estimated using a variant of the expectation-maximization (EM) algorithm (<xref ref-type="bibr" rid="bib27">Dempster et al., 1977</xref>) (see <italic>Materials and methods</italic>).</p><p>For an individual velocity component as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, the GSM model captures the shape of the distribution well, with only two parameters: <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$\sigma_{Y}$\end{document}</tex-math></alternatives></inline-formula>, controlling the overall scale, and <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$\sigma_{Z}$\end{document}</tex-math></alternatives></inline-formula>, controlling the heaviness of the tail. The variance of <inline-formula><alternatives><mml:math id="inf33"><mml:mi>X</mml:mi></mml:math><tex-math id="inft33">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> is related to these parameters by<disp-formula id="equ3"><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  \sigma_X^2=\sigma_Y^2\exp\left(2\sigma_Z^2\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>The kurtosis, which is the standard measurement of tail heaviness, depends only on <inline-formula><alternatives><mml:math id="inf34"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft34">\begin{document}$\sigma_{Z}$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ4"><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle  \kappa_X=3\exp\left(4\sigma_Z^2\right)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The kurtosis of <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> thus grows exponentially with the variance of <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>Z</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> and matches the Gaussian kurtosis of 3 if and only if <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$\sigma_{Z}=0$\end{document}</tex-math></alternatives></inline-formula>.</p><p>To model the joint distribution, as in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, clearly we cannot use independent GSM models for each component, since this corresponds to the shuffled distribution in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. Instead, we consider a model in which two independent Gaussian random variables, <inline-formula><alternatives><mml:math id="inf38"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft38">\begin{document}$Y_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf39"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft39">\begin{document}$Y_{2}$\end{document}</tex-math></alternatives></inline-formula>, are multiplied by a shared scale variable <inline-formula><alternatives><mml:math id="inf40"><mml:mi>S</mml:mi></mml:math><tex-math id="inft40">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ5"><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>S</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  X_1=Y_1S\,,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ6"><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>S</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  X_2=Y_2S\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Note that we will maintain the general notation for the model for clarity. Applied to the velocity data, we have<disp-formula id="equ7"><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  (X_1,X_2)=(U,V)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>and <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$(Y_1, Y_2)$\end{document}</tex-math></alternatives></inline-formula> are the corresponding scale-normalized velocity components. The model is depicted in <xref ref-type="fig" rid="fig3">Figure 3</xref>, and it captures the radially symmetric (or more generally, when the variances are not equal, elliptic) shape of the joint distribution. Note that this model reveals why the Pearson correlation of the component magnitudes captures the nonlinear dependence between <italic>U</italic> and <italic>V</italic>. We have, for the covariance (the numerator of the correlation coefficient),<disp-formula id="equ8"><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>S</mml:mi><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle E\left[(|X_1| - E[|X_1|]) (|X_2| - E[|X_2|])\right]= E[|Y_1|]\, E[|Y_2|]\, \left(E[S^2] - E[S]^2\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>which is positive whenever <inline-formula><alternatives><mml:math id="inf42"><mml:mi>S</mml:mi></mml:math><tex-math id="inft42">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> has nonzero variance.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Schematic of the two-dimensional Gaussian scale-mixture model.</title><p>(<bold>A–D</bold>) A Gaussian random variable <inline-formula><alternatives><mml:math id="inf43"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft43">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> (<bold>C</bold>) is passed through an exponential nonlinearity to yield a log-normal scale variable <inline-formula><alternatives><mml:math id="inf44"><mml:mi>S</mml:mi></mml:math><tex-math id="inft44">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> (<bold>B</bold>). The scale multiplies both components of an underlying Gaussian distribution (<bold>A</bold>) to produce radially symmetric heavy tails (<bold>D</bold>). For the joint distributions, probabilities less than 10<sup>−3</sup> were set to zero to facilitate comparison with empirical histograms. (<bold>E</bold>) Dependency graph for the variables in the model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig3-v2.tif"/></fig><p>This model has only three parameters: <inline-formula><alternatives><mml:math id="inf45"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft45">\begin{document}$\sigma_{Y_{1}}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$\sigma_{Y_{2}}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf47"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft47">\begin{document}$\sigma_{Z}$\end{document}</tex-math></alternatives></inline-formula>. (A more complicated model could have two correlated scale variables with different standard deviations. This does not appear to be necessary since the scale generator standard deviations fit independently to each component are nearly identical for most movies, and the elliptic shapes of the distributions indicate that the scale correlations across components are near one.) We observe a wide range of scale generator standard deviations <inline-formula><alternatives><mml:math id="inf48"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft48">\begin{document}$\sigma_{Z}$\end{document}</tex-math></alternatives></inline-formula> both within and across categories (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). The trend across categories – namely that animal and plant movies tend to have higher scale standard deviations than water movies – agrees with the relative heaviness of the tails for the pooled data (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). On the other hand, <inline-formula><alternatives><mml:math id="inf49"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft49">\begin{document}$\sigma_{Y_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf50"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft50">\begin{document}$\sigma_{Y_{2}}$\end{document}</tex-math></alternatives></inline-formula> need not be similar, and many movies had a larger standard deviation of motion on the horizontal axis than vertical (the ratio <inline-formula><alternatives><mml:math id="inf51"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$\sigma_{Y_{2}}/\sigma_{Y_{1}}$\end{document}</tex-math></alternatives></inline-formula> tended to be less than one, <xref ref-type="fig" rid="fig4">Figure 4E</xref>). The AIC calculated for the two-dimensional GSM model with a common scale variable indicates that it is a better fit to the data than a two-dimensional, independent Gaussian model (<xref ref-type="fig" rid="fig4">Figure 4F</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Quantifying heavy tails across scenes and categories (<bold>A</bold>).</title><p>Marginal distributions for vertical and horizontal velocity components, grouped by category. (<bold>B</bold>) Legend of individual movie names for A and all subsequent plots. (<bold>C</bold>) Marginal distributions for the combined data across categories. Each velocity component of each movie was normalized by its standard deviation before combining. (<bold>D</bold>) Estimated standard deviations for the scale generator variable, <inline-formula><alternatives><mml:math id="inf52"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft52">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula>, varied across movies, corresponding to different amounts of kurtosis. (<bold>E</bold>) The ratio of estimated standard deviations of the underlying Gaussian variables, <inline-formula><alternatives><mml:math id="inf53"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft53">\begin{document}$Y_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf54"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft54">\begin{document}$Y_{2}$\end{document}</tex-math></alternatives></inline-formula>, showing the degree of anisotropy. (<bold>F</bold>) Akaike information criterion (AIC) values for the two-dimensional, shared scale Gaussian scale-mixture (GSM) model versus the two-dimensional, independent Gaussian model. (<bold>G</bold>) Coding efficiency as a function of signal-to-noise ratio for different values of <inline-formula><alternatives><mml:math id="inf55"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft55">\begin{document}$\sigma_{Z}$\end{document}</tex-math></alternatives></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig4-v2.tif"/></fig><sec id="s2-1-1"><title>Coding implications of heavy tails</title><p>Heavy-tailed velocity distributions pose a particular challenge for efficient coding via sensory neurons. Consider the classic information theoretic problem of coding a random variable <inline-formula><alternatives><mml:math id="inf56"><mml:mi>X</mml:mi></mml:math><tex-math id="inft56">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> with an additive white Gaussian noise (AWGN) channel (<xref ref-type="bibr" rid="bib23">Cover and Thomas, 2006</xref>) (in a more biologically realistic setting, one could consider, e.g., a population of Poisson neurons tuned to different motion directions, but the AWGN channel suffices for developing intuition). In this problem, the input is encoded with some function <inline-formula><alternatives><mml:math id="inf57"><mml:mi>f</mml:mi></mml:math><tex-math id="inft57">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula>, Gaussian noise <inline-formula><alternatives><mml:math id="inf58"><mml:mi>N</mml:mi></mml:math><tex-math id="inft58">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> is added by the channel, and the noise-corrupted signal is decoded by some function <inline-formula><alternatives><mml:math id="inf59"><mml:mi>g</mml:mi></mml:math><tex-math id="inft59">\begin{document}$g$\end{document}</tex-math></alternatives></inline-formula> to form an estimate of the input, <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$\hat{X}=g\left[f(X)+N\right]$\end{document}</tex-math></alternatives></inline-formula>. The channel capacity, <inline-formula><alternatives><mml:math id="inf61"><mml:mi>C</mml:mi></mml:math><tex-math id="inft61">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula>, is a function of the signal-to-noise ratio (SNR),<disp-formula id="equ9"><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  C=\frac{1}{2}\log(1+\mathrm{SNR}),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$\mathrm{SNR}=\sigma_{f(X)}^{2}/\sigma_{N}^{2}$\end{document}</tex-math></alternatives></inline-formula> is the ratio of the encoded signal variance to the noise variance. The mutual information <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$I(X,\hat{X})$\end{document}</tex-math></alternatives></inline-formula> is equal to <inline-formula><alternatives><mml:math id="inf64"><mml:mi>C</mml:mi></mml:math><tex-math id="inft64">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> if and only if <inline-formula><alternatives><mml:math id="inf65"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft65">\begin{document}$f(X)$\end{document}</tex-math></alternatives></inline-formula> is Gaussian. Otherwise, <inline-formula><alternatives><mml:math id="inf66"><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>C</mml:mi></mml:math><tex-math id="inft66">\begin{document}$I \lt C$\end{document}</tex-math></alternatives></inline-formula>, and the coding efficiency <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$E=I/C$\end{document}</tex-math></alternatives></inline-formula> is less than one. It is commonly assumed that biological systems must expend energy to achieve a desired SNR, either by amplifying the signal or suppressing the noise, so that coding efficiency corresponds directly to energy efficiency.</p><p>When the encoding function is identity, <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>X</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$f(X)=X$\end{document}</tex-math></alternatives></inline-formula>, heavy tails in <inline-formula><alternatives><mml:math id="inf69"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft69">\begin{document}$p(x)$\end{document}</tex-math></alternatives></inline-formula> lead to a loss of efficiency. We calculate the coding efficiency as a function of the parameters of the GSM model for <inline-formula><alternatives><mml:math id="inf70"><mml:mi>X</mml:mi></mml:math><tex-math id="inft70">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> and the noise level <inline-formula><alternatives><mml:math id="inf71"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft71">\begin{document}$\sigma_{N}^{2}$\end{document}</tex-math></alternatives></inline-formula>. We have<disp-formula id="equ10"><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle  I(\hat{X},X)=H(\hat{X})-H(\hat{X}|X)=H(\hat{X})-H(N)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ11"><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle  H(\hat{X})=-\int_{-\infty}^\infty p(\hat{x})\log p(\hat{x}) d\hat{x}\,,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ12"><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle  p(\hat{x})=\int_{0}^\infty p(\hat{x}|s)p(s) ds,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ13"><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle  p(\hat{x}|s)=\mathcal{N}\left(\hat{x};0,s^2\sigma_Y^2+\sigma_N^2\right)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>and<disp-formula id="equ14"><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle  H(N)=\frac{1}{2}\log\left(2\pi e \sigma_N^2\right)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>To see the effect of heavy tails, we vary <inline-formula><alternatives><mml:math id="inf72"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$\sigma_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula> and the SNR, keeping either <inline-formula><alternatives><mml:math id="inf73"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$\sigma_{Y}^{2}$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$\sigma_{N}^{2}$\end{document}</tex-math></alternatives></inline-formula> fixed, and calculate the above mutual information numerically (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). Since <inline-formula><alternatives><mml:math id="inf75"><mml:mi>I</mml:mi></mml:math><tex-math id="inft75">\begin{document}$I$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf76"><mml:mi>C</mml:mi></mml:math><tex-math id="inft76">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> have the same scaling behavior with SNR, <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>E</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$E\to 1$\end{document}</tex-math></alternatives></inline-formula> as <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:mo>±</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$\log(\mathrm{SNR})\to\pm\infty$\end{document}</tex-math></alternatives></inline-formula>, the heavy tails have no effect at very high or low SNR. At intermediate SNR, the coding efficiency decreases monotonically as <inline-formula><alternatives><mml:math id="inf79"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft79">\begin{document}$\sigma_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula> increases. The efficiency reaches a minimum at <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$\log\mathrm{SNR}=3/2$\end{document}</tex-math></alternatives></inline-formula> for all <inline-formula><alternatives><mml:math id="inf81"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft81">\begin{document}$\sigma_{Z}^{2} \gt 0$\end{document}</tex-math></alternatives></inline-formula>.</p><p>How can one encode <inline-formula><alternatives><mml:math id="inf82"><mml:mi>X</mml:mi></mml:math><tex-math id="inft82">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> for better efficiency? The most straightforward way is to ‘Gaussianize’ <inline-formula><alternatives><mml:math id="inf83"><mml:mi>X</mml:mi></mml:math><tex-math id="inft83">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> so that <inline-formula><alternatives><mml:math id="inf84"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft84">\begin{document}$f(X)$\end{document}</tex-math></alternatives></inline-formula> is Gaussian (<xref ref-type="bibr" rid="bib20">Chen and Gopinath, 2000</xref>), thereby achieving perfect efficiency. This can be done generically by choosing <inline-formula><alternatives><mml:math id="inf85"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft85">\begin{document}$f(X)=\Phi^{-1}\left[F(X)\right]$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf86"><mml:mi>F</mml:mi></mml:math><tex-math id="inft86">\begin{document}$F$\end{document}</tex-math></alternatives></inline-formula> is the cumulative distribution function of <inline-formula><alternatives><mml:math id="inf87"><mml:mi>X</mml:mi></mml:math><tex-math id="inft87">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf88"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math><tex-math id="inft88">\begin{document}$\Phi$\end{document}</tex-math></alternatives></inline-formula> is the cumulative distribution function of the standard Gaussian distribution. Neurons have been shown to implement this kind of efficient coding by matching their response nonlinearities to natural scene statistics (<xref ref-type="bibr" rid="bib49">Laughlin, 1981</xref>), although the mapping is to a uniform distribution over a fixed interval rather than a Gaussian. (The uniform distribution is the maximum entropy distribution on an interval – here, the range of firing rates from zero to some upper limit – just as the Gaussian is the maximum entropy distribution on the real line with fixed variance. The following argument still applies in this setting.) In principle, this method can be applied to each time step and channel (velocity component) independently; however, if <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$X_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf90"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft90">\begin{document}$X_{2}$\end{document}</tex-math></alternatives></inline-formula> are not independent, as demonstrated for velocity components above, the joint distribution of <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}$f_{1}(X_{1})$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf92"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft92">\begin{document}$f_{2}(X_{2})$\end{document}</tex-math></alternatives></inline-formula> will not be Gaussian, so that efficiency is still not perfect in terms of the two-dimensional generalization of the channel capacity. More complex encoding methods may yield approximately jointly Gaussian distributions (<xref ref-type="bibr" rid="bib20">Chen and Gopinath, 2000</xref>; <xref ref-type="bibr" rid="bib48">Laparra et al., 2011</xref>), but these are not biologically plausible. Another downside of this strategy is that it introduces signal-dependent noise to the decoded variable, since <inline-formula><alternatives><mml:math id="inf93"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>N</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft93">\begin{document}$\hat{X}=f^{-1}\left[f(X)+N\right]=X+\hat{N}(X)$\end{document}</tex-math></alternatives></inline-formula>. In particular, the velocity values with highest magnitude, which may be the most relevant for behavior, will have the highest noise, since <inline-formula><alternatives><mml:math id="inf94"><mml:mi>f</mml:mi></mml:math><tex-math id="inft94">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula> must be compressive at the tails and therefore <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$f^{-1}$\end{document}</tex-math></alternatives></inline-formula> must be expansive.</p><p>Another strategy is to demodulate or normalize <inline-formula><alternatives><mml:math id="inf96"><mml:mi>X</mml:mi></mml:math><tex-math id="inft96">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> by estimating <inline-formula><alternatives><mml:math id="inf97"><mml:mi>S</mml:mi></mml:math><tex-math id="inft97">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> and dividing <inline-formula><alternatives><mml:math id="inf98"><mml:mi>X</mml:mi></mml:math><tex-math id="inft98">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> by it. If the estimate is accurate, then <inline-formula><alternatives><mml:math id="inf99"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>X</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>≈</mml:mo><mml:mi>Y</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft99">\begin{document}$X/\hat{S}=\hat{Y}\approx Y$\end{document}</tex-math></alternatives></inline-formula>, a Gaussian, and channel efficiency for <inline-formula><alternatives><mml:math id="inf100"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft100">\begin{document}$\hat{Y}$\end{document}</tex-math></alternatives></inline-formula> will be high. In order to recover <inline-formula><alternatives><mml:math id="inf101"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft101">\begin{document}$\hat{X}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf102"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft102">\begin{document}$\hat{Z}=\log\hat{S}$\end{document}</tex-math></alternatives></inline-formula> will need to be encoded in another channel, and the two sources of additive noise will result in multiplicative noise and heavy-tailed additive noise in the estimate:<disp-formula id="equ15"><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle  \hat{X}=(\hat{Y}+N_Y)\exp(\hat{Z}+N_Z)=X\exp(N_Z)+N_Y\exp(\hat{Z}+N_Z)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Of course, this strategy fails for a single variable <inline-formula><alternatives><mml:math id="inf103"><mml:mi>X</mml:mi></mml:math><tex-math id="inft103">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> since the only reasonable estimate for the scale is <inline-formula><alternatives><mml:math id="inf104"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><tex-math id="inft104">\begin{document}$\hat{S}=|X|$\end{document}</tex-math></alternatives></inline-formula>, so that <inline-formula><alternatives><mml:math id="inf105"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft105">\begin{document}$\hat{Y}=\pm 1$\end{document}</tex-math></alternatives></inline-formula>. However, since the two velocity components share a common scale variable, the estimate can be improved by making use of both components, and these can be made approximately jointly Gaussian, unlike in the marginal Gaussianization strategy described above. Furthermore, since the scale may be correlated in time in natural scenes, as shown in the next section, the history of <inline-formula><alternatives><mml:math id="inf106"><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft106">\begin{document}$X(t)$\end{document}</tex-math></alternatives></inline-formula> can also be used to further improve the estimate <inline-formula><alternatives><mml:math id="inf107"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft107">\begin{document}$\hat{S}$\end{document}</tex-math></alternatives></inline-formula>. Calculating the overall cost of this strategy is complicated by the fact that it introduces a new channel for the scale variable with its own associated SNR-dependent energy consumption.</p></sec></sec><sec id="s2-2"><title>The dynamics of natural motion</title><p>While the time-independent statistics of velocity are important, a full description of how objects move must include how the velocity evolves over time <italic>along</italic> point trajectories. (This description should ideally also include how multiple points on the same object evolve over time, allowing us to capture rotations, contractions, and expansions; we do not attempt this more ambitious analysis here and limit our discussion to local translations.) This motivates our point tracking analysis, which provides information that cannot be gleaned from motion estimates at fixed locations alone. From the raw data, we know the velocity is highly correlated at short time lags, but it is not clear how the scale variable enters into play. We again inspect the joint velocity distribution for an example movie, now across neighboring time points for one velocity component (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). The tilt indicates strong linear correlation across time in the velocity, as expected, and we note that the overall shape is elliptic despite having heavy-tailed marginals, as in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. In <xref ref-type="fig" rid="fig5">Figure 5B</xref>, we condition on the velocity at one time step and observe a tilted version of the same bow-tie shape as in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. With the addition of an overall tilt, these are precisely the same observations that led us to the GSM model above. Thus, two forms of dependence – linear correlation and the nonlinear dependence due to the scale variable – coexist in the time domain.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Temporal correlations in velocity and scale.</title><p>(<bold>A–B</bold>) Joint (<bold>A</bold>) and conditional (<bold>B</bold>) histograms for horizontal velocity across two adjacent frames for an example movie (<monospace>bees8-full</monospace>). The tilt indicates a strong linear correlation, while the elliptic shape in (<bold>A</bold>) and bow-tie shape in (<bold>B</bold>) indicate the coexistence of a nonlinear dependence due to an underlying scale variable. (<bold>C</bold>) The correlation coefficient between velocity component magnitudes, offset in time by <inline-formula><alternatives><mml:math id="inf108"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:math><tex-math id="inft108">\begin{document}$\Delta t$\end{document}</tex-math></alternatives></inline-formula>, decays as a function of <inline-formula><alternatives><mml:math id="inf109"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:math><tex-math id="inft109">\begin{document}$\Delta t$\end{document}</tex-math></alternatives></inline-formula>, indicating that the shared scale variable fluctuates in time. (<bold>D</bold>) The joint distributions of the two components separated by <inline-formula><alternatives><mml:math id="inf110"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft110">\begin{document}$\Delta t$\end{document}</tex-math></alternatives></inline-formula> show a gradual transformation from the original radially symmetric shape (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) toward the diamond shape of the shuffled distribution (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) (red curves are isoprobability contours at p=0.01).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig5-v2.tif"/></fig><p>We next ask whether this scale variable is constant in time (varying only from trajectory to trajectory) or dynamic (varying in time within a given trajectory). If it is constant, the joint distribution of the two components will not depend on the alignment of the two components in time, so long as they are from the same trajectory. In <xref ref-type="fig" rid="fig5">Figure 5D</xref>, we examine these joint distributions after shifting one component relative to the other by a time lag, for a range of lags. The distributions gradually shift from the radially symmetric zero-lag distribution to a diamond shape similar to the shuffled distribution. Similarly, the correlation between velocity component magnitudes, an indicator of nonlinear dependence discussed above, decreases as the lag increases (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). In other words, at zero lag, the two velocity components are governed by a single scale variable; at nonzero lags, they are governed by two correlated scale variables, and this correlation decreases with lag. This indicates that the underlying scale variable fluctuates over time. Notably, we can detect these fluctuations within the ∼1-s-long trajectories to which we limit our analysis. This would not be the case if the scale were to change only on a very long timescale or only across different point trajectories within a scene.</p><p>We would like to capture this dynamic scale variable in our model of natural motion. It is straightforward to make the GSM model dynamic by replacing each Gaussian variable with an autoregressive Gaussian process, and we call this new model the ARGSM model. We illustrate it schematically in <xref ref-type="fig" rid="fig6">Figure 6</xref> by generating example traces for one Gaussian velocity component <inline-formula><alternatives><mml:math id="inf111"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft111">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> and the scale generator <inline-formula><alternatives><mml:math id="inf112"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft112">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula>. Note that the autoregressive scale generator variable is the temporal equivalent to the spatial Markov random fields explored in the image domain (<xref ref-type="bibr" rid="bib92">Wainwright et al., 2001</xref>; <xref ref-type="bibr" rid="bib73">Roth and Black, 2009</xref>; <xref ref-type="bibr" rid="bib54">Lyu and Simoncelli, 2009</xref>). Given this model, we perform estimation of the parameters using a stochastic approximation variant of the EM algorithm (see <italic>Materials and methods</italic>). (Code for fitting the ARGSM model is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sepalmer/motion-scale-mixture">https://github.com/sepalmer/motion-scale-mixture</ext-link>, <xref ref-type="bibr" rid="bib70">Palmer and Salisbury, 2026</xref>) Example traces illustrating the results of this model are shown in <xref ref-type="fig" rid="fig7">Figure 7A–C</xref>. The estimated autoregression coefficients determine the correlation functions of the underlying Gaussian velocity and scale generator processes, which we plot for each movie in <xref ref-type="fig" rid="fig7">Figure 7D and E</xref>, respectively. The fact that some velocity correlation functions and many scale generator correlation functions do not go to zero over the length of the trajectories could indicate a nonzero mean component that varies from trajectory to trajectory, but this is beyond the scope of the present analysis. Average correlation functions across categories are shown in <xref ref-type="fig" rid="fig7">Figure 7F–G</xref>. We also report the time to 0.5 correlation for each movie for the velocity in <xref ref-type="fig" rid="fig7">Figure 7H–I</xref>. Akaike information criterion (AIC) scores (<xref ref-type="fig" rid="fig7">Figure 7K</xref>) indicate that the full ARGSM is a better fit to the data compared to the AR model. It is also a better fit compared to the ARGSM model with a static <inline-formula><alternatives><mml:math id="inf113"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft113">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> value for each trajectory, indicating that a dynamic scale variable is essential for describing the data. In the context of visual tracking of moving objects, the timescales of these correlation functions are extremely important. On one hand, the velocity correlation time determines how far into the future motion can be extrapolated. On the other hand, the scale correlation time determines the timescale on which adaptation must take place in order to efficiently process motion signals with limited dynamic range.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Schematic of the dynamic Gaussian scale-mixture model.</title><p>(<bold>A–D</bold>) Both <inline-formula><alternatives><mml:math id="inf114"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft114">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> (<bold>A</bold>) and <inline-formula><alternatives><mml:math id="inf115"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft115">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> (<bold>C</bold>) are modeled by high-order autoregressive processes to capture arbitrary correlation functions. (Only AR(1) processes are depicted graphically and used to simulate data.) The scale process <inline-formula><alternatives><mml:math id="inf116"><mml:mi>S</mml:mi></mml:math><tex-math id="inft116">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> (<bold>B</bold>) is generated by passing <inline-formula><alternatives><mml:math id="inf117"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>Z</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft117">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> through an element-wise exponential nonlinearity. It then multiplies the underlying Gaussian process <inline-formula><alternatives><mml:math id="inf118"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft118">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> element-wise to yield the observed process with fluctuating scale (<bold>D</bold>). Only one component is depicted. In the full model, two independent Gaussian processes share a common scale process. (<bold>E</bold>) Dependency graph for the variables in the one-dimensional model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig6-v2.tif"/></fig><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Quantifying velocity and scale correlations.</title><p>(<bold>A–C</bold>) Example traces of the raw velocity (<bold>A</bold>), scale-normalized velocity (<bold>B</bold>), and estimated scale variable (<bold>C</bold>). (<bold>D</bold>) Temporal correlation functions for the underlying Gaussian processes of each movie, grouped by category. Horizontal and vertical components were averaged before normalizing (equivalently, each component was weighted by its variance). (<bold>E</bold>) As in <bold>D</bold>, for the scale-generating Gaussian process, <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>Z</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula>. (<bold>F</bold>) The Gaussian process correlation functions in <bold>D</bold> averaged within categories. (<bold>G</bold>) As in F, for the scale-generating Gaussian process correlation functions in <bold>E</bold>. (<bold>H</bold>) Lag time to reach a correlation of 0.5 for the underlying velocity Gaussian processes for each movie (components were weighted by variance as in <bold>D</bold>). (<bold>I</bold>) As in <bold>H</bold>, for the scale-generating Gaussian process. (<bold>J</bold>) Predictive variance explained for each movie. Variances were averaged across horizontal and vertical components before calculating <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$R^{2}$\end{document}</tex-math></alternatives></inline-formula>. (<bold>K</bold>) Akaike information criterion (AIC) values for different models for each movie. Lower values indicate better model fit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig7-v2.tif"/></fig><p>Finally, we ask whether the full ARGSM model is necessary to carry out scale normalization in practice for our trajectory data. Our model fitting provides an estimate of the scale at each time point, which we use to normalize the raw data. To quantify normalization performance, we calculate the kurtosis, or fourth standardized moment, which measures how heavy-tailed a distribution is. The standard reference is a Gaussian random variable, which has a kurtosis of 3. In <xref ref-type="fig" rid="fig8">Figure 8</xref>, we compare the kurtosis of the velocity before and after dividing by a point estimate of the scale under three models of increasing complexity. If normalization is successful, the distribution of the resulting normalized velocity should be approximately Gaussian. Under the time-independent model, the normalized velocity consistently has kurtosis less than 3, indicating that the scale tends to be overestimated (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). In contrast, for a model with correlated velocity and constant scale for each trajectory, the kurtosis is consistently larger than 3, indicating that the scale tends to be underestimated (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). Only the full model, with correlated velocity and a dynamic, correlated scale variable, yields a kurtosis around 3 for each movie, even with highly kurtotic data (<xref ref-type="fig" rid="fig8">Figure 8C</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>A dynamic scale-mixture model is necessary for effective normalization.</title><p>(<bold>A</bold>) Kurtosis of the velocity before and after dividing by a point estimate of the scale (<italic>bottom</italic>) under the time-independent model (<italic>top</italic>). Kurtosis was computed by pooling the two components after normalizing by each standard deviation, so that differences in the variance across components do not contribute additional kurtosis. A Gaussian distribution has a kurtosis of 3 (dashed lines). (<bold>B</bold>) As in A, but for a model with autocorrelated Gaussian processes and a constant scale for each trajectory. (<bold>C</bold>) As in A, but for the fully dynamic model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-fig8-v2.tif"/></fig><p>This exercise of using the ARGSM model to estimate the scale at each time point, then dividing the velocity by this scale, serves as a proxy for what the nervous system can achieve through adaptation mechanisms. An important caveat is that the model has access to the full trajectory, while the nervous system must operate in an online, causal setting.</p></sec><sec id="s2-3"><title>Implications for prediction</title><p>Prediction is an important problem both for compression via predictive coding and for overcoming sensory and motor delays during behavior. Prediction is built into the ARGSM framework since the regression coefficients of the AR models are optimal for predicting the next time step of <inline-formula><alternatives><mml:math id="inf121"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft121">\begin{document}$Y_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf122"><mml:msub><mml:mi>Z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft122">\begin{document}$Z_{t}$\end{document}</tex-math></alternatives></inline-formula> given their past values. Let <inline-formula><alternatives><mml:math id="inf123"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft123">\begin{document}$\hat{Y}_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf124"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft124">\begin{document}$\hat{Z}_{t}$\end{document}</tex-math></alternatives></inline-formula> denote the predicted values and <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>υ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$\upsilon_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf126"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft126">\begin{document}$\zeta_{t}$\end{document}</tex-math></alternatives></inline-formula> the respective errors:<disp-formula id="equ16"><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>υ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle  Y_t=\hat{Y}_t+\upsilon_t,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ17"><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ζ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle  Z_t=\hat{Z}_t+\zeta_t.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The variance explained by a prediction <inline-formula><alternatives><mml:math id="inf127"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft127">\begin{document}$\hat{X}_{t}$\end{document}</tex-math></alternatives></inline-formula> is given by<disp-formula id="equ18"><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle  R^2=\frac{E\left[X_t^2\right]-E\left[\left(X_t-\hat{X}_t\right)^2\right]}{E\left[X_t^2\right]}\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>For the Gaussian process <inline-formula><alternatives><mml:math id="inf128"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft128">\begin{document}$Y_{t}$\end{document}</tex-math></alternatives></inline-formula>, this simplifies to<disp-formula id="equ19"><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>υ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle  R_Y^2=\frac{\sigma_{\hat{Y}}^2}{\sigma_Y^2}=1-\frac{\sigma_\upsilon^2}{\sigma_Y^2}\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Assuming knowledge of the histories of both <inline-formula><alternatives><mml:math id="inf129"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft129">\begin{document}$X_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf130"><mml:msub><mml:mi>Z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft130">\begin{document}$Z_{t}$\end{document}</tex-math></alternatives></inline-formula>, the prediction for <inline-formula><alternatives><mml:math id="inf131"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft131">\begin{document}$X_{t}$\end{document}</tex-math></alternatives></inline-formula> is<disp-formula id="equ20"><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle  \hat{X}_t=\hat{Y}_t\exp(\hat{Z}_t),$$\end{document}</tex-math></alternatives></disp-formula></p><p>The associated variance explained is<disp-formula id="equ21"><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ζ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ζ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle  R_{X|Z}^2=R_Y^2\left(2\exp\frac{1}{2}\sigma_\zeta^2-\exp2\sigma_\zeta^2\right)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>This is an upper bound on the performance of any predictor with access only to <inline-formula><alternatives><mml:math id="inf132"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft132">\begin{document}$X_{t}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Notably, <inline-formula><alternatives><mml:math id="inf133"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$R_{X|Z}^{2}$\end{document}</tex-math></alternatives></inline-formula> is independent of <inline-formula><alternatives><mml:math id="inf134"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft134">\begin{document}$\sigma_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula>: the variance explained under the ARGSM model for <inline-formula><alternatives><mml:math id="inf135"><mml:mi>X</mml:mi></mml:math><tex-math id="inft135">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> is equal to the variance explained for <inline-formula><alternatives><mml:math id="inf136"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft136">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula>, multiplied by a function of the variance of the driving noise for <inline-formula><alternatives><mml:math id="inf137"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft137">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf138"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft138">\begin{document}$\sigma_{\zeta}^{2}$\end{document}</tex-math></alternatives></inline-formula>, that slowly decreases from one to zero. Since <inline-formula><alternatives><mml:math id="inf139"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft139">\begin{document}$\sigma_{\zeta}^{2}$\end{document}</tex-math></alternatives></inline-formula> is small for the estimated models, indicating that the scale fluctuations are highly predictable at the level of single time steps, we expect this term to have little effect. In <xref ref-type="fig" rid="fig7">Figure 7J</xref>, we compare the variance explained by applying naive autoregression to <inline-formula><alternatives><mml:math id="inf140"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft140">\begin{document}$X_{t}$\end{document}</tex-math></alternatives></inline-formula> (denoted <inline-formula><alternatives><mml:math id="inf141"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft141">\begin{document}$R_{X}^{2}$\end{document}</tex-math></alternatives></inline-formula>) to <inline-formula><alternatives><mml:math id="inf142"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft142">\begin{document}$R_{Y}^{2}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf143"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft143">\begin{document}$R_{X|Z}^{2}$\end{document}</tex-math></alternatives></inline-formula> using the estimated model parameters. The variance explained is close to one for all movies except three depicting insects (a consequence of their very short velocity correlation times). Values of <inline-formula><alternatives><mml:math id="inf144"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft144">\begin{document}$R_{X}^{2}$\end{document}</tex-math></alternatives></inline-formula> tend to be only slightly smaller than <inline-formula><alternatives><mml:math id="inf145"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft145">\begin{document}$R_{X|Z}^{2}$\end{document}</tex-math></alternatives></inline-formula> (except in two instances where it was slightly larger). We conclude that heavy-tailed statistics have little effect on the predictability of natural motion, although scale estimation is necessary for estimating the variance associated with the prediction, i.e., the variance of the posterior distribution <inline-formula><alternatives><mml:math id="inf146"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>←</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft146">\begin{document}$p\left(x_{t}|\overleftarrow{\mathbf{x}}_{t-1}\right)\approx\mathcal{N}\left(x_ {t};\hat{x}_{t},\sigma_{\upsilon}^{2}\exp\left(2\hat{z_{t}}\right)\right)$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Long correlation times and high values of <inline-formula><alternatives><mml:math id="inf147"><mml:msup><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft147">\begin{document}$R^{2}$\end{document}</tex-math></alternatives></inline-formula> indicate that the velocity time series of natural motion are highly predictable. One way to make use of this predictability is through predictive coding, in which only prediction errors (with variance <inline-formula><alternatives><mml:math id="inf148"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft148">\begin{document}$\sigma_{X}^{2}-\sigma_{\hat{X}}^{2}$\end{document}</tex-math></alternatives></inline-formula>, as opposed to <inline-formula><alternatives><mml:math id="inf149"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft149">\begin{document}$\sigma_{X}^{2}$\end{document}</tex-math></alternatives></inline-formula> for the original signal) are sent through a channel. However, this may be a challenge for the visual system, since motion is encoded in spatial arrays of neurons rather than individual channels. A second use is actually carrying out the prediction to compensate for delays in perception or to drive motor output. (Note that since the position <inline-formula><alternatives><mml:math id="inf150"><mml:mi>q</mml:mi></mml:math><tex-math id="inft150">\begin{document}$q$\end{document}</tex-math></alternatives></inline-formula> of a point is the integral of its velocity, the prediction of position by means of correlations in the velocity is given by <inline-formula><alternatives><mml:math id="inf151"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>τ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft151">\begin{document}$\hat{q}(t+\alpha)=q(t)+\int_{0}^{\alpha}\hat{v}(t+\tau)d\tau$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf152"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft152">\begin{document}$\hat{v}(t+\tau)$\end{document}</tex-math></alternatives></inline-formula> is the prediction of the velocity at time <inline-formula><alternatives><mml:math id="inf153"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:math><tex-math id="inft153">\begin{document}$t+\tau$\end{document}</tex-math></alternatives></inline-formula> given its history up to time <inline-formula><alternatives><mml:math id="inf154"><mml:mi>t</mml:mi></mml:math><tex-math id="inft154">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>.) The results of this section indicate that this is feasible with or without taking the fluctuating scale into account.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The observed pattern of heavy-tailed velocity distributions in natural movies, with a scale parameter that is shared across velocity components and fluctuates in time, is remarkably consistent across scenes and categories, despite substantial variation in the content of those scenes, velocity correlation functions, and the overall velocity variance. Together with previous results showing similar statistics in natural images and sounds (<xref ref-type="bibr" rid="bib75">Schwartz and Simoncelli, 2001</xref>), this suggests that scale-mixing is a fundamental property of natural stimuli, with deep implications for both neural coding and behavior.</p><p>In the context of object motion, scale mixing may arise from two distinct mechanisms, as outlined in our discussion of Brownian motion (see <italic>Appendix 1</italic>). First, objects may appear at a variety of distances from the observer, and those distances may change over time. The velocity of a point on an object, as it appears to an image-forming device, like a camera or the eye, is an angular velocity, which can be calculated as the tangential component of the physical velocity divided by the distance. A fluctuating distance thus scales the overall angular velocity over time: even an isolated point moving with Gaussian velocity statistics in three-dimensional space will have a heavy-tailed angular velocity distribution from the perspective of the observer. Second, the scale of the driving forces (either internal or external) may fluctuate over time. In our scenes, this corresponds to changes in the behavioral states of animals or to the turbulent nature of water and the wind driving plant motion. Since heavy-tailed distributions and scale fluctuations are observed in scenes with very little variance in depth, such as <monospace>bees8-full</monospace>, we emphasize that this mechanism is also at play in natural scenes.</p><p>Regardless of the source of scale-mixing, strategies for encoding and behaviorally compensating for it should be similar. On the encoding side, the presence of local scale variables suggests that sensory systems should adapt their response properties to local statistics in order to maximize information transmission. Given a fixed distribution of external input values, the optimal neural response function is the one that produces a uniform distribution over the neuron’s dynamic range (<xref ref-type="bibr" rid="bib49">Laughlin, 1981</xref>). The logarithmic speed tuning observed in MT (<xref ref-type="bibr" rid="bib66">Nover et al., 2005</xref>) is consistent with this kind of static efficient coding. Here, we demonstrate that the scale of the distribution changes over time, so the gain of the response function should also change to match it (<xref ref-type="bibr" rid="bib90">Wainwright, 1999</xref>; <xref ref-type="bibr" rid="bib14">Brenner et al., 2000</xref>; <xref ref-type="bibr" rid="bib95">Wark et al., 2009</xref>). Such adaptation or gain control is observed throughout the nervous system (see <xref ref-type="bibr" rid="bib96">Weber et al., 2019</xref>, for a recent review), including in systems relevant to object motion encoding (<xref ref-type="bibr" rid="bib32">Fairhall et al., 2001</xref>; <xref ref-type="bibr" rid="bib4">Bair and Movshon, 2004</xref>; <xref ref-type="bibr" rid="bib68">Olveczky et al., 2007</xref>; <xref ref-type="bibr" rid="bib51">Liu et al., 2016</xref>). This adaptation could be the result of subcellular mechanisms, such as the molecular kinematics of synaptic vesicle release (<xref ref-type="bibr" rid="bib69">Ozuysal and Baccus, 2012</xref>), or nonlinear circuit-level mechanisms (<xref ref-type="bibr" rid="bib13">Borst et al., 2005</xref>; <xref ref-type="bibr" rid="bib8">Bharioke and Chklovskii, 2015</xref>). By measuring the timescale on which the scale variable fluctuates in natural movie scenes, we have determined the timescale on which adaptation mechanisms in the brain should operate. Although the range is considerable, for most movies the time to 0.5 correlation for the scale generator is less than 1 s (<xref ref-type="fig" rid="fig7">Figure 7I</xref>). Future experiments could be targeted at probing adaptation timescales in the retina and cortex of various model organisms that occupy different environments. Our prediction is that these adaptation variables will tightly match the motion statistics in the organism’s ecological niche.</p><p>Beyond single-cell adaptation, our results are also relevant to a population-level adaptation mechanisms known as divisive normalization (<xref ref-type="bibr" rid="bib17">Carandini and Heeger, 1994</xref>; <xref ref-type="bibr" rid="bib18">Carandini and Heeger, 2012</xref>), in which neighboring neurons in a population are mutually inhibitory in a divisive fashion. In many systems, motion is represented by a local population of neurons, each tuned to a narrow band of directions. Our results show that the fluctuating scale is shared between horizontal and vertical velocity components, and, hence, adaptation should ideally be distributed throughout the local population. Divisive normalization is a prime candidate for the implementation of this population-level adaptation, as has been suggested for GSM models of filter responses (<xref ref-type="bibr" rid="bib75">Schwartz and Simoncelli, 2001</xref>; <xref ref-type="bibr" rid="bib93">Wainwright et al., 2002</xref>; <xref ref-type="bibr" rid="bib76">Schwartz et al., 2006</xref>; <xref ref-type="bibr" rid="bib21">Coen-Cagli et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Coen-Cagli et al., 2015</xref>; <xref ref-type="bibr" rid="bib81">Snow et al., 2016</xref>). Most models of divisive normalization only capture steady-state responses to static or constant velocity stimuli, although some work has been done to describe the dynamics of divisive normalization during change detection and decision-making (<xref ref-type="bibr" rid="bib52">Louie et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Ernst et al., 2021</xref>). Again, these dynamics should be tuned to the timescale of the scale fluctuations measured here.</p><p>These data suggest a previously unexplored challenge for adaptation mechanisms in the context of object motion: an object may travel an appreciable distance before local mechanisms have a chance to take effect. A solution is to pool from a larger neighborhood, or, more intriguingly, for a local population to receive an adaptation signal selectively from those neurons in nearby populations whose preferred directions point to it. To our knowledge, these hypotheses have not yet been explored, either theoretically or experimentally.</p><p>In terms of behavior, our results help refine our understanding of the object tracking problems animals must solve in natural environments, which are crucial to survival. A commonly invoked framework for tracking is sequential Bayesian inference under a state-space model (<xref ref-type="bibr" rid="bib39">Ho and Lee, 1964</xref>). In this framework, the brain has a probabilistic representation of the state of the object (i.e. a probability distribution over its position and velocity). An internal model of object motion is used to evolve this distribution forward in time, and this prediction is combined with incoming measurements to update the estimated state distribution. Under Gaussian assumptions, this yields the famous Kalman filter solution (<xref ref-type="bibr" rid="bib43">Kalman, 1960</xref>). Our work has two important implications for the state-space model framework of object tracking. First, the velocity distributions we observe are typically non-Gaussian, so the Kalman filter solution is not strictly applicable. While heavy tails have little impact on prediction, they have a large effect on the uncertainty of the posterior estimate. Second, state-space models usually model the velocity as either an AR(1) or (discrete) diffusion process (i.e. a nonstationary AR(1) process with coefficient equal to one). The AR models we fit for the underlying Gaussian components generally have more than one large coefficient. The ARGSM model could naturally serve as a predictive state-space model that incorporates these empirical observations by including the recent history of the velocity and scale in the state description (note that the scale does not have a corresponding direct measurement, but it can be estimated from the incoming velocity measurements). Flexible Bayesian methods like the particle filter (<xref ref-type="bibr" rid="bib25">Del Moral, 1997</xref>) can be used to implement such a model. The merging of the sort of adaptation mechanisms described above with neuromorphic particle filtering (<xref ref-type="bibr" rid="bib46">Kutschireiter et al., 2017</xref>) is an intriguing avenue for future research.</p><p>Motion estimation itself can be framed as a Bayesian inference problem, and the tracking algorithm we use corresponds to a Gaussian prior (<xref ref-type="bibr" rid="bib79">Simoncelli et al., 1991</xref>). The ARGSM model could thus serve as a better prior, motivating new motion estimation algorithms based on natural scene statistics. Speed perception in humans and animals can also be viewed through the lens of Bayesian inference, and experimental results are consistent with a heavy-tailed prior, specifically, a power law (<xref ref-type="bibr" rid="bib84">Stocker and Simoncelli, 2006</xref>; <xref ref-type="bibr" rid="bib99">Zhang and Stocker, 2022</xref>). The GSM model yields a heavy-tailed distribution for speed compared to the Rayleigh distribution expected under Gaussian assumptions, but it is not a true power law. Since power laws are an idealization and are always subject to some cutoff, the GSM model may be considered a more realistic (if less tractable) alternative. The correlated scale fluctuations also suggest that optimal Bayesian inference should be history-dependent, which could be assessed psychophysically using, e.g., a trial structure that is correlated in time.</p><p>Finally, the significant diversity of velocity and scale correlation functions and variances across scenes has implications both for efficient coding and for tracking. Namely, an encoder or tracker which is optimized for the statistics of one scene will be suboptimal for others. Indeed, there is a general trade-off in adaptation to global versus local statistics (<xref ref-type="bibr" rid="bib61">Młynarski and Hermundstad, 2018</xref>; <xref ref-type="bibr" rid="bib62">Młynarski and Hermundstad, 2021</xref>). The original efficient coding work posited adaptation on evolutionary timescales to natural scene statistics. Here, we emphasize the subsecond timescale of scale fluctuations in natural motion. Neural systems should also have the flexibility to adapt on intermediate timescales to changes in the environment or behavioral context (<xref ref-type="bibr" rid="bib85">Teşileanu et al., 2022</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Point tracking</title><p>We compute short trajectories using the <monospace>PointTracker</monospace> function in MATLAB’s Computer Vision toolbox. The function employs a Kanade-Lucas-Tomasi (<xref ref-type="bibr" rid="bib53">Lucas and Kanade, 1981</xref>; <xref ref-type="bibr" rid="bib86">Tomasi and Kanade, 1991</xref>) feature tracking algorithm, which uses multi-scale image registration under a translational motion model to track individual points from frame to frame. Briefly, given an image patch <inline-formula><alternatives><mml:math id="inf155"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft155">\begin{document}$I(x,y,t)$\end{document}</tex-math></alternatives></inline-formula> centered on some seeded initial position, the algorithm finds the displacement <inline-formula><alternatives><mml:math id="inf156"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft156">\begin{document}$(\Delta x,\Delta y)$\end{document}</tex-math></alternatives></inline-formula> that minimizes the squared error,<disp-formula id="equ22"><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>∬</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle  \epsilon^2=\iint\left[I(x,y,t)-I(x+\Delta x,y+\Delta y,t+\Delta t)\right]^2dxdy \,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>and updates the seed position on the next frame. Our strategy is to collect as many high-quality, short trajectories (64 frames) as possible from each movie, then subsample these down to a reasonable number of trajectories (8192) for statistical analysis. Initial points are seeded using the <monospace>detectMinEigenFeatures</monospace> function, which detects image features that can be tracked well under the motion model (<xref ref-type="bibr" rid="bib78">Shi and Tomasi, 1994</xref>). From the initial seeds, we run the tracking algorithm forward and backward for 32 frames each, rather than running it in one direction for 64 frames. This increases the chances of capturing short-lived trajectories bounded by occlusion or image boundaries. Points are seeded on each frame, so the resulting set of trajectories is highly overlapping in time. On most movies, we employ the built-in forward-backward error checking method (<xref ref-type="bibr" rid="bib42">Kalal et al., 2010</xref>), with a threshold of 0.25 pixels, to automatically detect tracking errors. The exceptions are three movies depicting water (water3, water5, and water6) where the small threshold leads to rejecting most trajectories, so we use a threshold of 8 pixels. In these cases, there are not well-defined objects, so relaxing this strict criterion is justified. The algorithm uses a multi-resolution pyramid and computes gradients within a neighborhood at each level. We use the default values of 3 pyramid levels and a neighborhood size of 31 by 31 pixels for all movies except the 3 water movies, where we find we can decrease the amount of erroneously large jumps in trajectories by increasing the neighborhood size to 129 by 129 pixels and using only 1 pyramid level (at a cost of greater computation time).</p><p>This method automatically tracks the stationary background points, which may be erroneously ‘picked up’ by a moving object as it traverses that location. To ensure that the trajectories we analyze are full of motion, we define a speed (velocity magnitude) threshold of 0.1 pix/frame and discard trajectories in which 16 or more time steps are below this threshold. We validated our method on a simple synthetic movie animated by an ARGSM process (<italic>Appendix 2</italic>).</p><p>The velocity time series is simply the first difference of the point positions along each trajectory. Within each ensemble, we subtract the ensemble mean from each velocity component (this is typically very close to zero, except for some water movies with a persistent flow). We then slightly rotate the horizontal and vertical velocity components to remove small correlations between them (these arise if, for example, objects tended to move along a slight diagonal relative to the camera’s sensor). All visualizations and calculations are carried out after these minor preprocessing steps. Note that we do not subtract the average velocity within each trajectory, as this introduces an artificial anticorrelation at long lags.</p></sec><sec id="s4-2"><title>GSM models</title><p>The basic one-dimensional GSM model is described in the main text. Note that for some choices of the distribution for <inline-formula><alternatives><mml:math id="inf157"><mml:mi>S</mml:mi></mml:math><tex-math id="inft157">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula>, the distribution for <inline-formula><alternatives><mml:math id="inf158"><mml:mi>X</mml:mi></mml:math><tex-math id="inft158">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> has a closed-form solution. For example, the well-known Student’s <inline-formula><alternatives><mml:math id="inf159"><mml:mi>t</mml:mi></mml:math><tex-math id="inft159">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>-distribution is formed when <inline-formula><alternatives><mml:math id="inf160"><mml:mi>S</mml:mi></mml:math><tex-math id="inft160">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> follows an inverse <inline-formula><alternatives><mml:math id="inf161"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>χ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft161">\begin{document}$\chi$\end{document}</tex-math></alternatives></inline-formula>-distribution, and the Laplace distribution is formed when <inline-formula><alternatives><mml:math id="inf162"><mml:mi>S</mml:mi></mml:math><tex-math id="inft162">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> follows a Rayleigh distribution. In this work, we assume <inline-formula><alternatives><mml:math id="inf163"><mml:mi>S</mml:mi></mml:math><tex-math id="inft163">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> follows a log-normal distribution, which does not yield a closed-form distribution for <inline-formula><alternatives><mml:math id="inf164"><mml:mi>X</mml:mi></mml:math><tex-math id="inft164">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula>. This choice makes modeling correlations straightforward, as will be made clear below. In practice, the lack of a closed-form <inline-formula><alternatives><mml:math id="inf165"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft165">\begin{document}$p(x)$\end{document}</tex-math></alternatives></inline-formula> is not a drawback, since we do not need to normalize the posterior distribution, <inline-formula><alternatives><mml:math id="inf166"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft166">\begin{document}$p(s|x)\propto p(x|s)p(s)$\end{document}</tex-math></alternatives></inline-formula>, in order to sample from it.</p><p>When considering multiple variables, a shared scale variable introduces a nonlinear form of dependence between them. Suppose <inline-formula><alternatives><mml:math id="inf167"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>S</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft167">\begin{document}$X_{1}=Y_{1}S$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf168"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>S</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft168">\begin{document}$X_{2}=Y_{2}S$\end{document}</tex-math></alternatives></inline-formula>. If <inline-formula><alternatives><mml:math id="inf169"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft169">\begin{document}$Y_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf170"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft170">\begin{document}$Y_{2}$\end{document}</tex-math></alternatives></inline-formula> are uncorrelated, then <inline-formula><alternatives><mml:math id="inf171"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft171">\begin{document}$X_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf172"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft172">\begin{document}$X_{2}$\end{document}</tex-math></alternatives></inline-formula> are conditionally independent given <inline-formula><alternatives><mml:math id="inf173"><mml:mi>S</mml:mi></mml:math><tex-math id="inft173">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ23"><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle  p(x_1,x_2|s)=p(x_1|s)p(x_2|s).$$\end{document}</tex-math></alternatives></disp-formula></p><p>However, <inline-formula><alternatives><mml:math id="inf174"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft174">\begin{document}$X_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf175"><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft175">\begin{document}$X_{2}$\end{document}</tex-math></alternatives></inline-formula> are not, in general, independent:<disp-formula id="equ24"><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>≠</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle  p(x_1,x_2)=\int_0^\infty p(x_1|s)p(x_2|s)p(s)ds\neq p(x_1)p(x_2)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>This nonlinear dependence manifests itself in the elliptic level sets of <inline-formula><alternatives><mml:math id="inf176"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft176">\begin{document}$p(x_{1},x_{2})$\end{document}</tex-math></alternatives></inline-formula>, in contrast to the diamond-shaped level sets of <inline-formula><alternatives><mml:math id="inf177"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft177">\begin{document}$p(x_{1})p(x_{2})$\end{document}</tex-math></alternatives></inline-formula>. Note that this nonlinear dependence can coincide with the usual linear dependence if <inline-formula><alternatives><mml:math id="inf178"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft178">\begin{document}$Y_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf179"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft179">\begin{document}$Y_{2}$\end{document}</tex-math></alternatives></inline-formula> are correlated, and that a weaker form of nonlinear dependence may be present if <inline-formula><alternatives><mml:math id="inf180"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft180">\begin{document}$X_{1}=Y_{1}S_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf181"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft181">\begin{document}$X_{2}=Y_{2}S_{2}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf182"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft182">\begin{document}$S_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf183"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft183">\begin{document}$S_{2}$\end{document}</tex-math></alternatives></inline-formula> are not independent.</p></sec><sec id="s4-3"><title>Autoregressive models</title><p>Autoregressive models (<xref ref-type="bibr" rid="bib98">Yule, 1927</xref>; <xref ref-type="bibr" rid="bib94">Walker, 1931</xref>) are a well-established and flexible way to capture correlations in time series data by supposing a linear relationship between the current value of a random variable with its previous values. Given a time series, <inline-formula><alternatives><mml:math id="inf184"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math><tex-math id="inft184">\begin{document}$\{X_{1},\dots,X_{T}\}$\end{document}</tex-math></alternatives></inline-formula>, the <italic>k</italic>th order autoregressive, or AR(<inline-formula><alternatives><mml:math id="inf185"><mml:mi>k</mml:mi></mml:math><tex-math id="inft185">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>), model is given by<disp-formula id="equ25"><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle  X_t=\sum_{i=1}^k\phi_iX_{t-i}+\xi_t$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf186"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math><tex-math id="inft186">\begin{document}$\{\phi_{1},\dots,\phi_{k}\}$\end{document}</tex-math></alternatives></inline-formula> are regression coefficients and <inline-formula><alternatives><mml:math id="inf187"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft187">\begin{document}$\xi_{t}$\end{document}</tex-math></alternatives></inline-formula> is Gaussian noise with variance <inline-formula><alternatives><mml:math id="inf188"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft188">\begin{document}$\sigma^{2}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>The order <inline-formula><alternatives><mml:math id="inf189"><mml:mi>k</mml:mi></mml:math><tex-math id="inft189">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> is typically chosen by cross-validation to avoid overfitting. This makes sense from the standpoint of finding a model that generalizes well to new data. However, our primary aim here is simply to measure the autocovariances of the hidden variables, since their timescales are relevant to prediction and adaptation in the nervous system. For this reason, we choose <inline-formula><alternatives><mml:math id="inf190"><mml:mi>k</mml:mi></mml:math><tex-math id="inft190">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> to be as high as possible: <inline-formula><alternatives><mml:math id="inf191"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>31</mml:mn></mml:math><tex-math id="inft191">\begin{document}$k=31$\end{document}</tex-math></alternatives></inline-formula> time steps, since <inline-formula><alternatives><mml:math id="inf192"><mml:mi>k</mml:mi></mml:math><tex-math id="inft192">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> must be less than <inline-formula><alternatives><mml:math id="inf193"><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:math><tex-math id="inft193">\begin{document}$T/2$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Typically, the model parameters are fit by standard linear regression (after organizing the data appropriately) (<xref ref-type="bibr" rid="bib37">Hamilton, 2020</xref>). However, this method gives maximum likelihood estimates only if the initial <inline-formula><alternatives><mml:math id="inf194"><mml:mi>k</mml:mi></mml:math><tex-math id="inft194">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> time steps are considered fixed. If the initial data are assumed to be drawn from the stationary distribution defined by the parameters, the problem becomes nonlinear. The EM algorithm described below requires parameter estimates to be maximum likelihood, and since we would like the initial <inline-formula><alternatives><mml:math id="inf195"><mml:mi>k</mml:mi></mml:math><tex-math id="inft195">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> time steps (where <inline-formula><alternatives><mml:math id="inf196"><mml:mi>k</mml:mi></mml:math><tex-math id="inft196">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> is large) to be modeled by the stationary distribution, we must pursue this more difficult course. We calculate the maximum likelihood estimates numerically, following <xref ref-type="bibr" rid="bib58">Miller, 1995</xref>. See <italic>Appendix 3</italic> for a full description of this method.</p></sec><sec id="s4-4"><title>The ARGSM model</title><p>The dynamic scale-mixture model generalizes the two-dimensional, shared scale variable GSM model described above to time series, assuming the underlying Gaussian random variables, <inline-formula><alternatives><mml:math id="inf197"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft197">\begin{document}$Y_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf198"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft198">\begin{document}$Y_{2}$\end{document}</tex-math></alternatives></inline-formula>, and the generator, <inline-formula><alternatives><mml:math id="inf199"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft199">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula>, of the scale variable are all AR(<inline-formula><alternatives><mml:math id="inf200"><mml:mi>k</mml:mi></mml:math><tex-math id="inft200">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>) processes. Specifically, let <inline-formula><alternatives><mml:math id="inf201"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft201">\begin{document}$X_{1,t}=Y_{1,t}S_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf202"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft202">\begin{document}$X_{2,t}=Y_{2,t}S_{t}$\end{document}</tex-math></alternatives></inline-formula>. Written as <inline-formula><alternatives><mml:math id="inf203"><mml:mi>T</mml:mi></mml:math><tex-math id="inft203">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>-dimensional vectors, we have <inline-formula><alternatives><mml:math id="inf204"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft204">\begin{document}$\mathbf{X}_{1}=\mathbf{Y}_{1}\odot\mathbf{S}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf205"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft205">\begin{document}$\mathbf{X}_{2}=\mathbf{Y}_{2}\odot\mathbf{S}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf206"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>⊙</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft206">\begin{document}$\odot$\end{document}</tex-math></alternatives></inline-formula> is element-wise multiplication. The AR process assumptions imply<disp-formula id="equ26"><alternatives><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t26">\begin{document}$$\displaystyle p(\mathbf{y}_1)=\mathcal{N}\left(\mathbf{y}_1;\mathbf{0},\mathbf{\Sigma}_{Y_1}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ27"><alternatives><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t27">\begin{document}$$\displaystyle p(\mathbf{y}_2)=\mathcal{N}\left(\mathbf{y}_2;\mathbf{0},\mathbf{\Sigma}_{Y_2}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ28"><alternatives><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t28">\begin{document}$$\displaystyle p(\mathbf{z})=\mathcal{N}\left(\mathbf{z};\mathbf{0},\mathbf{\Sigma}_Z\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the covariance matrices are determined by the parameters of independent AR(<inline-formula><alternatives><mml:math id="inf207"><mml:mi>k</mml:mi></mml:math><tex-math id="inft207">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>) models as described above. <inline-formula><alternatives><mml:math id="inf208"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft208">\begin{document}$\mathbf{S}$\end{document}</tex-math></alternatives></inline-formula> is related to <inline-formula><alternatives><mml:math id="inf209"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft209">\begin{document}$\mathbf{Z}$\end{document}</tex-math></alternatives></inline-formula> by element-wise application of the exponential function, <inline-formula><alternatives><mml:math id="inf210"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft210">\begin{document}$\mathbf{S}=\exp(\mathbf{Z})$\end{document}</tex-math></alternatives></inline-formula>. When <inline-formula><alternatives><mml:math id="inf211"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft211">\begin{document}$\mathbf{Z}$\end{document}</tex-math></alternatives></inline-formula> is known, we have<disp-formula id="equ29"><alternatives><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t29">\begin{document}$$\displaystyle p(\mathbf{x}_1|\mathbf{z})=\mathcal{N}\left(\mathbf{x}_1;\mathbf{0},\mathbf{D}_\mathbf{s}\mathbf{\Sigma}_{Y_1}\mathbf{D}_\mathbf{s}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ30"><alternatives><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t30">\begin{document}$$\displaystyle p(\mathbf{x}_2|\mathbf{z})=\mathcal{N}\left(\mathbf{x}_2;\mathbf{0},\mathbf{D}_\mathbf{s}\mathbf{\Sigma}_{Y_2}\mathbf{D}_\mathbf{s}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf212"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft212">\begin{document}$\mathbf{D}_{\mathbf{s}}$\end{document}</tex-math></alternatives></inline-formula> is the matrix with the elements of <inline-formula><alternatives><mml:math id="inf213"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:math><tex-math id="inft213">\begin{document}$\mathbf{s}$\end{document}</tex-math></alternatives></inline-formula> along the diagonal and zeros elsewhere.</p></sec><sec id="s4-5"><title>EM and stochastic approximation</title><p>The classic EM algorithm is a useful tool for finding (local) maximum likelihood estimates of parameters of hidden variable models (<xref ref-type="bibr" rid="bib27">Dempster et al., 1977</xref>). Let <inline-formula><alternatives><mml:math id="inf214"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft214">\begin{document}$\theta=\left\{\boldsymbol{\phi}_{Y_{1}},\boldsymbol{\phi}_{Y_{2}},\boldsymbol{ \phi}_{Z},\sigma_{Y_{1}}^{2},\sigma_{Y_{2}}^{2},\sigma_{Z}^{2}\right\}$\end{document}</tex-math></alternatives></inline-formula> be the collection of parameters of the ARGSM model, where each <inline-formula><alternatives><mml:math id="inf215"><mml:mi mathvariant="bold-italic">ϕ</mml:mi></mml:math><tex-math id="inft215">\begin{document}$\boldsymbol{\phi}$\end{document}</tex-math></alternatives></inline-formula> is the vector of AR coefficients and each <inline-formula><alternatives><mml:math id="inf216"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft216">\begin{document}$\sigma^{2}$\end{document}</tex-math></alternatives></inline-formula> is the driving noise variance for each variable. The observed data, <inline-formula><alternatives><mml:math id="inf217"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>n</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft217">\begin{document}$\mathcal{D}=\{\mathbf{x}_{1,n},\mathbf{x}_{2,n}\},1\leq n\leq N$\end{document}</tex-math></alternatives></inline-formula>, are the <inline-formula><alternatives><mml:math id="inf218"><mml:mi>N</mml:mi></mml:math><tex-math id="inft218">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> pairs of <inline-formula><alternatives><mml:math id="inf219"><mml:mi>T</mml:mi></mml:math><tex-math id="inft219">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>-dimensional vectors corresponding here to the horizontal and vertical velocity along each trajectory. The hidden variables, <inline-formula><alternatives><mml:math id="inf220"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi class="mathcal" mathvariant="script">H</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>n</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft220">\begin{document}$\mathcal{H}=\{\mathbf{z}_{n}\},1\leq n\leq N$\end{document}</tex-math></alternatives></inline-formula>, are the Gaussian generators of the time-varying scale associated with each trajectory. The likelihood<disp-formula id="equ31"><alternatives><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>L</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t31">\begin{document}$$\displaystyle  \begin{array}{ll}L&amp;=\log p(\mathcal{D}|\theta)\\ &amp;=\sum_{n=1}^N\log\int_{\mathbb{R}^T}p(\mathbf{x}_{1,n},\mathbf{x}_{2,n}|\mathbf{z},\theta)p(\mathbf{z}|\theta)D\mathbf{z}\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>is intractable to maximize due to the high-dimensional integral. The EM algorithm finds a local maximum iteratively. Starting with an initial guess for the parameters, <inline-formula><alternatives><mml:math id="inf221"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft221">\begin{document}$\theta_{0}$\end{document}</tex-math></alternatives></inline-formula>, at each step, one computes the expectation with respect to the probability distribution of the hidden variables given the data and the current parameter estimate <inline-formula><alternatives><mml:math id="inf222"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft222">\begin{document}$\theta_{t}$\end{document}</tex-math></alternatives></inline-formula> of the complete data log-likelihood,<disp-formula id="equ32"><alternatives><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">H</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">H</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:math><tex-math id="t32">\begin{document}$$\displaystyle  Q(\theta|\theta_t)=E_{p(\mathcal{H}|\mathcal{D},\theta_t)}\left[\log p(\mathcal{D},\mathcal{H}|\theta)\right]\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>then updates the parameters to maximize this function,<disp-formula id="equ33"><alternatives><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>θ</mml:mi></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t33">\begin{document}$$\displaystyle  \theta_{t+1}=\arg\max_\theta Q(\theta|\theta_t)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>In our setting, we have<disp-formula id="equ34"><alternatives><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mi>n</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t34">\begin{document}$$\displaystyle  \begin{array}{ll}Q(\theta|\theta_t)&amp;=-\frac{1}{2}\sum_{n=1}^NE\left[\mathbf{y}_{1,n}^\top\mathbf{\Sigma}_{Y_1}^{-1}\mathbf{y}_{1,n}+\mathbf{y}_{2,n}^\top\mathbf{\Sigma}_{Y_2}^{-1}\mathbf{y}_{2,n}+\mathbf{z}_n^\top\mathbf{\Sigma}_Z^{-1}\mathbf{z}_n\right]+K \\ &amp;=-\frac{1}{2}\mathrm{tr}\left(\mathbf{C}_{Y_1}\mathbf{\Sigma}_{Y_1}^{-1}+\mathbf{C}_{Y_2}\mathbf{\Sigma}_{Y_2}^{-1}+\mathbf{C}_Z\mathbf{\Sigma}_Z^{-1}\right)+K\,, \end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ35"><alternatives><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mi>n</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t35">\begin{document}$$\displaystyle  \mathbf{C}_Z=\sum_{n=1}^NE\left[\mathbf{z}_n\mathbf{z}_n^\top\right]$$\end{document}</tex-math></alternatives></disp-formula></p><p>and similarly for <inline-formula><alternatives><mml:math id="inf223"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft223">\begin{document}$\mathbf{C}_{Y_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf224"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft224">\begin{document}$\mathbf{C}_{Y_{2}}$\end{document}</tex-math></alternatives></inline-formula>. Note that in this context, the <inline-formula><alternatives><mml:math id="inf225"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math><tex-math id="inft225">\begin{document}$\mathbf{y}$\end{document}</tex-math></alternatives></inline-formula> variables are merely shorthand for <inline-formula><alternatives><mml:math id="inf226"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⊘</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft226">\begin{document}$\mathbf{y}_{1,n}=\mathbf{x}_{1,n}\oslash\mathbf{s}_{n}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf227"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>⊘</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft227">\begin{document}$\mathbf{y}_{2,n}=\mathbf{x}_{2,n}\oslash\mathbf{s}_{n}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf228"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>⊘</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft228">\begin{document}$\oslash$\end{document}</tex-math></alternatives></inline-formula> is element-wise division. Given the <inline-formula><alternatives><mml:math id="inf229"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:math><tex-math id="inft229">\begin{document}$\mathbf{C}$\end{document}</tex-math></alternatives></inline-formula> matrices, the maximum likelihood estimates of the AR parameters can be calculated as described in <italic>Appendix 3</italic>.</p><p>Unfortunately, the expectation values in the <inline-formula><alternatives><mml:math id="inf230"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:math><tex-math id="inft230">\begin{document}$\mathbf{C}$\end{document}</tex-math></alternatives></inline-formula> matrices are also intractable, but they can be approximated through sampling methods. A variant of the EM algorithm, called stochastic approximation EM, was developed to address this problem (<xref ref-type="bibr" rid="bib26">Delyon et al., 1999</xref>). Given a sample from the distribution <inline-formula><alternatives><mml:math id="inf231"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">H</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">D</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft231">\begin{document}$p(\mathcal{H}|\mathcal{D},\theta_{t})$\end{document}</tex-math></alternatives></inline-formula>, one calculates the sample matrices <inline-formula><alternatives><mml:math id="inf232"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft232">\begin{document}$\hat{\mathbf{C}}$\end{document}</tex-math></alternatives></inline-formula>, then updates the stochastic approximations as<disp-formula id="equ36"><alternatives><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t36">\begin{document}$$\displaystyle  \mathbf{C}_t=\mathbf{C}_{t-1}+\eta_t\left(\hat{\mathbf{C}}_t-\mathbf{C}_{t-1}\right)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The sequence of parameters <inline-formula><alternatives><mml:math id="inf233"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft233">\begin{document}$\eta_{t}$\end{document}</tex-math></alternatives></inline-formula> is given by<disp-formula id="equ37"><alternatives><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>α</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>α</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t37">\begin{document}$$\displaystyle  \eta_t=\begin{cases}1&amp;1\leq t\leq\alpha \\(t-\alpha)^{-\beta}&amp;t \gt \alpha\end{cases}\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>We choose <italic>α</italic> = 2500 or 5000, so that the algorithm runs in a fully stochastic mode until the parameter estimates are nearly stationary, and <inline-formula><alternatives><mml:math id="inf234"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft234">\begin{document}$\beta=1$\end{document}</tex-math></alternatives></inline-formula>, so that after this initial period, the algorithm converges by simply taking a running average of the samples of the <inline-formula><alternatives><mml:math id="inf235"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft235">\begin{document}$\hat{\mathbf{C}}$\end{document}</tex-math></alternatives></inline-formula> matrices. Importantly, the samples do not need to be independent across iterations for the algorithm to converge (<xref ref-type="bibr" rid="bib45">Kuhn and Lavielle, 2004</xref>). This means that, when performing the Gibbs sampling described below, we only need to update each hidden variable element once for each iteration, rather than updating many times and throwing out samples to achieve independence. Since each M-step (the AR model MLE algorithm described above) is much faster than each E-step (calculating the <inline-formula><alternatives><mml:math id="inf236"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft236">\begin{document}$\mathbf{C}$\end{document}</tex-math></alternatives></inline-formula> matrices through sampling), this results in a more sample-efficient algorithm (<xref ref-type="bibr" rid="bib65">Neal and Hinton, 1998</xref>).</p><p>We also estimate the expectation of the hidden variables <inline-formula><alternatives><mml:math id="inf237"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math><tex-math id="inft237">\begin{document}$\{\mathbf{z}_{n}\}$\end{document}</tex-math></alternatives></inline-formula> in an identical fashion. This is equivalent to a Bayesian point estimate where the estimated parameters form a forward model and prior. These estimates are then used to remove the scale from the velocity, in order to examine the kurtosis under different model assumptions (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>The EM algorithm, and its stochastic approximation variant, converges to a local maximum of the likelihood function that depends on the initial conditions. We find that, in practice, it is important to introduce the scale variable gradually to the model. We initialize the model with AR parameters fit to the raw data for the <inline-formula><alternatives><mml:math id="inf238"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft238">\begin{document}$Y_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf239"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft239">\begin{document}$Y_{2}$\end{document}</tex-math></alternatives></inline-formula> components, and let <inline-formula><alternatives><mml:math id="inf240"><mml:mi>Z</mml:mi></mml:math><tex-math id="inft240">\begin{document}$Z$\end{document}</tex-math></alternatives></inline-formula> be uncorrelated with very small variance (regression coefficients <inline-formula><alternatives><mml:math id="inf241"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft241">\begin{document}$\boldsymbol{\phi}_{Z}=\mathbf{0}$\end{document}</tex-math></alternatives></inline-formula> and driving noise variance <inline-formula><alternatives><mml:math id="inf242"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mn>0.05</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft242">\begin{document}$\sigma_{Z}^{2}=0.05^{2}(\gamma_{Y_{1},0}+\gamma_{Y_{2},0})/2$\end{document}</tex-math></alternatives></inline-formula>).</p></sec><sec id="s4-6"><title>Sampling methods</title><p>We use a combination of Gibbs and rejection sampling to sample from the posterior of the hidden variables given the data and the current parameter estimates (<xref ref-type="bibr" rid="bib11">Bishop, 2006</xref>). In Gibbs sampling, an initial vector <inline-formula><alternatives><mml:math id="inf243"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:math><tex-math id="inft243">\begin{document}$\mathbf{z}$\end{document}</tex-math></alternatives></inline-formula> is used to generate a new sample by sampling each element individually, conditioned on the remaining elements. Since the conditional distribution is intractable, we use rejection sampling, which allows us to sample from an arbitrary, unnormalized distribution by sampling from a proposal distribution (in this case a Gaussian with parameters chosen to envelope the conditional distribution) and rejecting some draws in order to shape it into the target distribution. See <italic>Appendix 3</italic> for a detailed description of the sampling algorithm.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-104054-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All videos analyzed are part of the Chicago Motion Database, located at <ext-link ext-link-type="uri" xlink:href="https://cmd.rcc.uchicago.edu">https://cmd.rcc.uchicago.edu</ext-link>. Code for fitting the ARGSM model is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sepalmer/motion-scale-mixture">https://github.com/sepalmer/motion-scale-mixture</ext-link> (copy archived at <xref ref-type="bibr" rid="bib70">Palmer and Salisbury, 2026</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the National Science Foundation through the Physics Frontier Center for Living Systems (PHY-2317138), the Center for the Physics of Biological Function (PHY-1734030), and a CAREER award to SEP (IIS-1652617); by the NSF-Simons National Institute for Theory and Mathematics in Biology, awards DMS-2235451 (NSF) and MP-TMPS-00005320 (Simons Foundation); and by the National Institutes of Health BRAIN Initiative (R01EB026943). We thank Siwei Wang and Benjamin Hoshal for useful comments on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><elocation-id>284</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname><given-names>DF</given-names></name><name><surname>Mallows</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Scale mixtures of normal distributions</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>36</volume><fpage>99</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1974.tb00989.x</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Manu</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A retinal circuit that computes object motion</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6807</fpage><lpage>6817</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4206-07.2008</pub-id><pub-id pub-id-type="pmid">18596156</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bair</surname><given-names>W</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Adaptive temporal integration of motion in direction-selective neurons in macaque visual cortex</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>7305</fpage><lpage>7323</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0554-04.2004</pub-id><pub-id pub-id-type="pmid">15317857</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible principles underlying the transformations of sensory messages</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group><source>Sensory Communication</source><publisher-name>MIT Press</publisher-name><fpage>217</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Simon</surname><given-names>A</given-names></name><name><surname>Ben-Shahar</surname><given-names>O</given-names></name><name><surname>Vasserman</surname><given-names>G</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive saccade in the absence of smooth pursuit: Interception of moving targets in the archer fish</article-title><source>The Journal of Experimental Biology</source><volume>215</volume><fpage>4248</fpage><lpage>4254</lpage><pub-id pub-id-type="doi">10.1242/jeb.076018</pub-id><pub-id pub-id-type="pmid">22972882</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Brivanlou</surname><given-names>IH</given-names></name><name><surname>Jordan</surname><given-names>TA</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Anticipation of moving stimuli by the retina</article-title><source>Nature</source><volume>398</volume><fpage>334</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/18678</pub-id><pub-id pub-id-type="pmid">10192333</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharioke</surname><given-names>A</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatic adaptation to fast input changes in a time-invariant neural circuit</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004315</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004315</pub-id><pub-id pub-id-type="pmid">26247884</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>IH</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prey capture behavior evoked by simple visual stimuli in larval zebrafish</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>101</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00101</pub-id><pub-id pub-id-type="pmid">22203793</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billock</surname><given-names>VA</given-names></name><name><surname>de Guzman</surname><given-names>GC</given-names></name><name><surname>Scott Kelso</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Fractal time and 1/f spectra in dynamic images and human vision</article-title><source>Physica D</source><volume>148</volume><fpage>136</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/S0167-2789(00)00174-3</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghuis</surname><given-names>BG</given-names></name><name><surname>Leonardo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The role of motion extrapolation in amphibian prey capture</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>15430</fpage><lpage>15441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3189-15.2015</pub-id><pub-id pub-id-type="pmid">26586829</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Flanagin</surname><given-names>VL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptation without parameter change: Dynamic gain control in motion detection</article-title><source>PNAS</source><volume>102</volume><fpage>6172</fpage><lpage>6176</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500491102</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname><given-names>N</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Adaptive rescaling maximizes information transmission</article-title><source>Neuron</source><volume>26</volume><fpage>695</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81205-2</pub-id><pub-id pub-id-type="pmid">10896164</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1828">1828</year><article-title>XXVII. A brief account of microscopical observations made in the months of June, July and August 1827, on the particles contained in the pollen of plants; and on the general existence of active molecules in organic and inorganic bodies</article-title><source>The Philosophical Magazine</source><volume>4</volume><fpage>161</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1080/14786442808674769</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calow</surname><given-names>D</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Local statistics of retinal optic flow for self-motion through natural sceneries</article-title><source>Network: Computation in Neural Systems</source><volume>18</volume><fpage>343</fpage><lpage>374</lpage><pub-id pub-id-type="pmid">18360939</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Summation and division by neurons in primate visual cortex</article-title><source>Science</source><volume>264</volume><fpage>1333</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1126/science.8191289</pub-id><pub-id pub-id-type="pmid">8191289</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Card</surname><given-names>G</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visually mediated motor planning in the escape response of <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>18</volume><fpage>1300</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.07.094</pub-id><pub-id pub-id-type="pmid">18760606</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Gopinath</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Gaussianization</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical surround interactions and perceptual salience via natural scene statistics</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002405</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002405</pub-id><pub-id pub-id-type="pmid">22396635</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Flexible gating of contextual influences in natural vision</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1648</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1038/nn.4128</pub-id><pub-id pub-id-type="pmid">26436902</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name><name><surname>Thomas</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Elements of Information Theory</source><publisher-name>Wiley-Interscience</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Efficient coding of natural scenes in the lateral geniculate nucleus: Experimental test of a computational theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>3351</fpage><lpage>3362</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-10-03351.1996</pub-id><pub-id pub-id-type="pmid">8627371</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Moral</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Nonlinear filtering: Interacting particle resolution</article-title><source>Comptes Rendus de l’Académie Des Sciences - Series I - Mathematics</source><volume>325</volume><fpage>653</fpage><lpage>658</lpage><pub-id pub-id-type="doi">10.1016/S0764-4442(97)84778-7</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delyon</surname><given-names>B</given-names></name><name><surname>Lavielle</surname><given-names>M</given-names></name><name><surname>Moulines</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Convergence of a stochastic approximation version of the EM algorithm</article-title><source>The Annals of Statistics</source><volume>27</volume><elocation-id>8031103</elocation-id><pub-id pub-id-type="doi">10.1214/aos/1018031103</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dempster</surname><given-names>AP</given-names></name><name><surname>Laird</surname><given-names>NM</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>39</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1977.tb01600.x</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doi</surname><given-names>E</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A simple model of optimal population coding for sensory systems</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003761</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003761</pub-id><pub-id pub-id-type="pmid">25121492</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>DW</given-names></name><name><surname>Atick</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Temporal decorrelation: A theory of lagged and nonlagged responses in the lateral geniculate nucleus</article-title><source>Network</source><volume>6</volume><fpage>159</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_6_2_003</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Einstein</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1905">1905</year><article-title>Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen</article-title><source>Annalen Der Physik</source><volume>322</volume><fpage>549</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1002/andp.19053220806</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>UA</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Bohnenkamp</surname><given-names>L</given-names></name><name><surname>Galashan</surname><given-names>FO</given-names></name><name><surname>Wegener</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dynamic divisive normalization circuits explain and predict change detection in monkey area MT</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009595</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009595</pub-id><pub-id pub-id-type="pmid">34767547</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1038/35090500</pub-id><pub-id pub-id-type="pmid">11518957</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franklin</surname><given-names>DW</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Computational mechanisms of sensorimotor control</article-title><source>Neuron</source><volume>72</volume><fpage>425</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.10.006</pub-id><pub-id pub-id-type="pmid">22078503</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabbiani</surname><given-names>F</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Multiplicative computation in a visual neuron sensitive to looming</article-title><source>Nature</source><volume>420</volume><fpage>320</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1038/nature01190</pub-id><pub-id pub-id-type="pmid">12447440</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="1996">1996a</year><article-title>Exact numerical simulation of the Ornstein-Uhlenbeck process and its integral</article-title><source>Physical Review E</source><volume>54</volume><fpage>2084</fpage><lpage>2091</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.54.2084</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="1996">1996b</year><article-title>The mathematics of Brownian motion and Johnson noise</article-title><source>American Journal of Physics</source><volume>64</volume><fpage>225</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1119/1.18210</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Time Series Analysis</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>MM</given-names></name><name><surname>McKinney</surname><given-names>T</given-names></name><name><surname>Chajka</surname><given-names>K</given-names></name><name><surname>Pelz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive eye movements in natural vision</article-title><source>Experimental Brain Research</source><volume>217</volume><fpage>125</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1007/s00221-011-2979-2</pub-id><pub-id pub-id-type="pmid">22183755</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>A Bayesian approach to problems in stochastic estimation and control</article-title><source>IEEE Transactions on Automatic Control</source><volume>9</volume><fpage>333</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1109/TAC.1964.1105763</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horn</surname><given-names>BKP</given-names></name><name><surname>Schunck</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Determining optical flow</article-title><source>Artificial Intelligence</source><volume>17</volume><fpage>185</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(81)90024-2</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Yavorska</surname><given-names>I</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vision drives accurate approach behavior during prey capture in laboratory mice</article-title><source>Current Biology</source><volume>26</volume><fpage>3046</fpage><lpage>3052</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.09.009</pub-id><pub-id pub-id-type="pmid">27773567</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kalal</surname><given-names>Z</given-names></name><name><surname>Mikolajczyk</surname><given-names>K</given-names></name><name><surname>Matas</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Forward-backward error: Automatic detection of tracking failures</article-title><conf-name>20th International Conference on Pattern Recognition IEEE</conf-name><fpage>2756</fpage><lpage>2759</lpage><pub-id pub-id-type="doi">10.1109/ICPR.2010.675</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauzlis</surname><given-names>RJ</given-names></name><name><surname>Lisberger</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Temporal properties of visual motion signals for the initiation of smooth pursuit eye movements in monkeys</article-title><source>Journal of Neurophysiology</source><volume>72</volume><fpage>150</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.72.1.150</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>E</given-names></name><name><surname>Lavielle</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Coupling a stochastic approximation version of EM with an MCMC procedure</article-title><source>ESAIM</source><volume>8</volume><fpage>115</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1051/ps:2004007</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutschireiter</surname><given-names>A</given-names></name><name><surname>Surace</surname><given-names>SC</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Pfister</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Nonlinear Bayesian filtering and learning: A neuronal dynamics for perception</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>8722</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-06519-y</pub-id><pub-id pub-id-type="pmid">28821729</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langevin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1908">1908</year><article-title>Sur la théorie du mouvement brownien</article-title><source>Comptes-Rendus de l’Académie Des Sciences</source><volume>146</volume><fpage>530</fpage><lpage>533</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laparra</surname><given-names>V</given-names></name><name><surname>Camps-Valls</surname><given-names>G</given-names></name><name><surname>Malo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Iterative Gaussianization: From ICA to random rotations</article-title><source>IEEE Transactions on Neural Networks</source><volume>22</volume><fpage>537</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1109/TNN.2011.2106511</pub-id><pub-id pub-id-type="pmid">21349790</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A simple coding procedure enhances a neuron’s information capacity</article-title><source>Zeitschrift Für Naturforschung C</source><volume>36</volume><fpage>910</fpage><lpage>912</lpage><pub-id pub-id-type="doi">10.1515/znc-1981-9-1040</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonardo</surname><given-names>A</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Nonlinear dynamics support a linear population code in a retinal target-tracking circuit</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>16971</fpage><lpage>16982</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2257-13.2013</pub-id><pub-id pub-id-type="pmid">24155302</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Macellaio</surname><given-names>MV</given-names></name><name><surname>Osborne</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient sensory cortical coding optimizes pursuit eye movements</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12759</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12759</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>LoFaro</surname><given-names>T</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic divisive normalization predicts time-varying value coding in decision-related circuits</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>16046</fpage><lpage>16057</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2851-14.2014</pub-id><pub-id pub-id-type="pmid">25429145</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>BD</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An iterative image registration technique with an application to stereo vision</article-title><conf-name>In Proceedings of the 7th International Joint Conference on Artificial Intelligence-Volume 2</conf-name><fpage>674</fpage><lpage>679</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyu</surname><given-names>S</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Nonlinear extraction of independent components of natural images using radial Gaussianization</article-title><source>Neural Computation</source><volume>21</volume><fpage>1485</fpage><lpage>1519</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.04-08-773</pub-id><pub-id pub-id-type="pmid">19191599</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Wehr</surname><given-names>MS</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Linearity of cortical receptive fields measured with natural sounds</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>1089</fpage><lpage>1100</lpage><pub-id pub-id-type="pmid">14762127</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Functional properties of neurons in middle temporal visual area of the macaque monkey. I. Selectivity for stimulus direction, speed, and orientation</article-title><source>Journal of Neurophysiology</source><volume>49</volume><fpage>1127</fpage><lpage>1147</lpage><pub-id pub-id-type="doi">10.1152/jn.1983.49.5.1127</pub-id><pub-id pub-id-type="pmid">6864242</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaiel</surname><given-names>AM</given-names></name><name><surname>Abe</surname><given-names>ET</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamics of gaze control during prey capture in freely moving mice</article-title><source>eLife</source><volume>9</volume><elocation-id>e57458</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57458</pub-id><pub-id pub-id-type="pmid">32706335</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Exact maximum likelihood estimation in autoregressive processes</article-title><source>Journal of Time Series Analysis</source><volume>16</volume><fpage>607</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9892.1995.tb00258.x</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mineault</surname><given-names>PJ</given-names></name><name><surname>Khawaja</surname><given-names>FA</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Hierarchical processing of complex motion along the primate dorsal visual pathway</article-title><source>PNAS</source><volume>109</volume><fpage>E972</fpage><lpage>E980</lpage><pub-id pub-id-type="doi">10.1073/pnas.1115685109</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mischiati</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>HT</given-names></name><name><surname>Herold</surname><given-names>P</given-names></name><name><surname>Imler</surname><given-names>E</given-names></name><name><surname>Olberg</surname><given-names>R</given-names></name><name><surname>Leonardo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Internal models direct dragonfly interception steering</article-title><source>Nature</source><volume>517</volume><fpage>333</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/nature14045</pub-id><pub-id pub-id-type="pmid">25487153</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname><given-names>WF</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adaptive coding for dynamic sensory inference</article-title><source>eLife</source><volume>7</volume><elocation-id>e32055</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32055</pub-id><pub-id pub-id-type="pmid">29988020</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Młynarski</surname><given-names>WF</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Efficient and adaptive sensory codes</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>998</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00846-0</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muijres</surname><given-names>FT</given-names></name><name><surname>Elzinga</surname><given-names>MJ</given-names></name><name><surname>Melis</surname><given-names>JM</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flies evade looming targets by executing rapid visually directed banked turns</article-title><source>Science</source><volume>344</volume><fpage>172</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1126/science.1248955</pub-id><pub-id pub-id-type="pmid">24723606</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>KS</given-names></name><name><surname>Matthis</surname><given-names>J</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Cormack</surname><given-names>LK</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Retinal motion statistics during natural locomotion</article-title><source>eLife</source><volume>12</volume><elocation-id>e82410</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.82410</pub-id><pub-id pub-id-type="pmid">37133442</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>RM</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>A view of the EM algorithm that justifies incremental, sparse, and other variants</chapter-title><person-group person-group-type="editor"><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><source>Learning in Graphical Models</source><publisher-name>Springer</publisher-name><fpage>355</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1007/978-94-011-5014-9_12</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nover</surname><given-names>H</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A Logarithmic, scale-invariant representation of speed in macaque middle temporal area accounts for speed discrimination performance</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>10049</fpage><lpage>10060</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1661-05.2005</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ölveczky</surname><given-names>BP</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Segregation of object and background motion in the retina</article-title><source>Nature</source><volume>423</volume><fpage>401</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1038/nature01652</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Retinal adaptation to object motion</article-title><source>Neuron</source><volume>56</volume><fpage>689</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.030</pub-id><pub-id pub-id-type="pmid">18031685</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozuysal</surname><given-names>Y</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Linking the computational structure of variance adaptation to biophysical mechanisms</article-title><source>Neuron</source><volume>73</volume><fpage>1002</fpage><lpage>1015</lpage><pub-id pub-id-type="pmid">22405209</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>SE</given-names></name><name><surname>Salisbury</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2026">2026</year><data-title>motion-scale-mixture</data-title><version designator="swh:1:rev:a021f0ce3b974cd745261faa2e50f4eef8e58ccd">swh:1:rev:a021f0ce3b974cd745261faa2e50f4eef8e58ccd</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:84d03fc6562e0e9dbfb7164200d464852e2daf9f;origin=https://github.com/sepalmer/motion-scale-mixture;visit=swh:1:snp:5d43ee7206a2139080f48024c625e498660a5211;anchor=swh:1:rev:a021f0ce3b974cd745261faa2e50f4eef8e58ccd">https://archive.softwareheritage.org/swh:1:dir:84d03fc6562e0e9dbfb7164200d464852e2daf9f;origin=https://github.com/sepalmer/motion-scale-mixture;visit=swh:1:snp:5d43ee7206a2139080f48024c625e498660a5211;anchor=swh:1:rev:a021f0ce3b974cd745261faa2e50f4eef8e58ccd</ext-link></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Exact feature probabilities in images with occlusion</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>42</elocation-id><pub-id pub-id-type="doi">10.1167/10.14.42</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>S</given-names></name><name><surname>Black</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>On the spatial statistics of optical flow</article-title><source>International Journal of Computer Vision</source><volume>74</volume><fpage>33</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/s11263-006-0016-x</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>S</given-names></name><name><surname>Black</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fields of Experts</article-title><source>International Journal of Computer Vision</source><volume>82</volume><fpage>205</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1007/s11263-008-0197-6</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruderman</surname><given-names>DL</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistics of natural images: Scaling in the woods</article-title><source>Physical Review Letters</source><volume>73</volume><fpage>814</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.73.814</pub-id><pub-id pub-id-type="pmid">10057546</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural signal statistics and sensory gain control</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1038/90526</pub-id><pub-id pub-id-type="pmid">11477428</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Soft mixer assignment in a hierarchical generative model of natural scene statistics</article-title><source>Neural Computation</source><volume>18</volume><fpage>2680</fpage><lpage>2718</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.11.2680</pub-id><pub-id pub-id-type="pmid">16999575</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaw</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>KH</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Fast prediction in marmoset reach-to-grasp movements for dynamic prey</article-title><source>Current Biology</source><volume>33</volume><fpage>2557</fpage><lpage>2565</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.05.032</pub-id><pub-id pub-id-type="pmid">37279754</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Tomasi</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Good features to track</article-title><conf-name>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.1994.323794</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Probability distributions of optical flow</article-title><conf-name>1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><conf-loc>Maui, HI, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.1991.139707</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snow</surname><given-names>M</given-names></name><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Specificity and timescales of cortical adaptation as inferences about natural movie statistics</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/16.13.1</pub-id><pub-id pub-id-type="pmid">27699416</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spering</surname><given-names>M</given-names></name><name><surname>Schütz</surname><given-names>AC</given-names></name><name><surname>Braun</surname><given-names>DI</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Keep your eyes on the ball: smooth pursuit eye movements enhance prediction of visual motion</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>1756</fpage><lpage>1767</lpage><pub-id pub-id-type="doi">10.1152/jn.00344.2010</pub-id><pub-id pub-id-type="pmid">21289135</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Dubs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Predictive coding: A fresh view of inhibition in the retina</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>216</volume><fpage>427</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1098/rspb.1982.0085</pub-id><pub-id pub-id-type="pmid">6129637</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Noise characteristics and prior expectations in human visual speed perception</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>578</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/nn1669</pub-id><pub-id pub-id-type="pmid">16547513</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teşileanu</surname><given-names>T</given-names></name><name><surname>Golkar</surname><given-names>S</given-names></name><name><surname>Nasiri</surname><given-names>S</given-names></name><name><surname>Sengupta</surname><given-names>AM</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural circuits for dynamics-based segmentation of time series</article-title><source>Neural Computation</source><volume>34</volume><fpage>891</fpage><lpage>938</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01476</pub-id><pub-id pub-id-type="pmid">35026035</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Tomasi</surname><given-names>C</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Detection and tracking of point features</source><publisher-name>International Journal of Computer Vision</publisher-name></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Straeten</surname><given-names>E</given-names></name><name><surname>Beck</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Superstatistical distributions from a maximum entropy principle</article-title><source>Physical Review E</source><volume>78</volume><elocation-id>051101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.78.051101</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A theory of maximizing sensory information</article-title><source>Biological Cybernetics</source><volume>68</volume><fpage>23</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1007/BF00203134</pub-id><pub-id pub-id-type="pmid">1486129</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verbyla</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>A note on the inverse covariance matrix of the autoregressive process</article-title><source>Australian Journal of Statistics</source><volume>27</volume><fpage>221</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1111/j.1467-842X.1985.tb00564.x</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Visual adaptation as optimal information transmission</article-title><source>Vision Research</source><volume>39</volume><fpage>3960</fpage><lpage>3974</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(99)00101-7</pub-id><pub-id pub-id-type="pmid">10748928</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Scale mixtures of Gaussians and the statistics of natural images</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Willsky</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Random cascades on wavelet trees and their use in analyzing and modeling natural images</article-title><source>Applied and Computational Harmonic Analysis</source><volume>11</volume><fpage>89</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1006/acha.2000.0350</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Natural image statistics and divisive normalization</chapter-title><person-group person-group-type="editor"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Olshausen</surname><given-names>B</given-names></name><name><surname>Lewicki</surname><given-names>M</given-names></name></person-group><source>Probabilistic Models of the Brain: Perception and Neural Function</source><publisher-name>MIT Press</publisher-name><fpage>203</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.7551/mitpress/5583.003.0015</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>GT</given-names></name></person-group><year iso-8601-date="1931">1931</year><article-title>On periodicity in series of related terms</article-title><source>Monthly Weather Review</source><volume>59</volume><fpage>277</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1175/1520-0493(1931)59&lt;277:OPISOR&gt;2.0.CO;2</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wark</surname><given-names>B</given-names></name><name><surname>Fairhall</surname><given-names>A</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Timescales of inference in visual adaptation</article-title><source>Neuron</source><volume>61</volume><fpage>750</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.019</pub-id><pub-id pub-id-type="pmid">19285471</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>AI</given-names></name><name><surname>Krishnamurthy</surname><given-names>K</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coding principles in adaptation</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>427</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014818</pub-id><pub-id pub-id-type="pmid">31283447</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>SBM</given-names></name><name><surname>Tu</surname><given-names>JC</given-names></name><name><surname>Piantadosi</surname><given-names>ST</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neural basis of predictive pursuit</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>252</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0561-6</pub-id><pub-id pub-id-type="pmid">31907436</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yule</surname><given-names>GU</given-names></name></person-group><year iso-8601-date="1927">1927</year><article-title>VII. On a method of investigating periodicities disturbed series, with special reference to Wolfer’s sunspot numbers</article-title><source>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of A Mathematical or Physical Character</source><volume>226</volume><fpage>267</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1098/rsta.1927.0007</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>LQ</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Prior expectations in visual speed perception predict encoding characteristics of neurons in area MT</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>2951</fpage><lpage>2962</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1920-21.2022</pub-id><pub-id pub-id-type="pmid">35169018</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zoran</surname><given-names>D</given-names></name><name><surname>Weiss</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Natural images, Gaussian mixtures and dead leaves</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>A motivating example: Brownian motion</title><p>Here, we introduce a canonical example of natural motion to guide our intuition about departures from the simplest case: Brownian motion. Brown discovered the motion that bears his name by observing small particles released by a grain of pollen floating in water under a microscope (<xref ref-type="bibr" rid="bib15">Brown, 1828</xref>) – a highly controlled setting, but similar in spirit to our own. Each particle is subjected to tiny molecular forces from individual water molecules colliding with it at random. As a result, it moves in a random fashion across the surface of the water, limited only by the boundary of the dish. Its motion is not completely unpredictable, however; since the particle has some mass, albeit small, it has some inertia, or tendency to continue moving with the same velocity. Statistically, this means the velocity is correlated in time.</p><p>Based on this observation, Einstein derived his famous diffusion equation, describing how a density of diffusing particles changes over time, by considering the case of particles with infinitesimal mass (<xref ref-type="bibr" rid="bib30">Einstein, 1905</xref>). For describing the motion of an individual, massive particle, it is more useful to look to Langevin’s description, an early application of stochastic differential equations (<xref ref-type="bibr" rid="bib47">Langevin, 1908</xref>). Using Newton’s law and assuming the force can be divided into the sum of a drag force, proportional to the velocity by a factor of <inline-formula><alternatives><mml:math id="inf244"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft244">\begin{document}$-\gamma$\end{document}</tex-math></alternatives></inline-formula>, and a randomly fluctuating force due to the collision of water molecules, <inline-formula><alternatives><mml:math id="inf245"><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft245">\begin{document}$F(t)$\end{document}</tex-math></alternatives></inline-formula>, we have the following differential equation for the velocity, <inline-formula><alternatives><mml:math id="inf246"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft246">\begin{document}$V(t)$\end{document}</tex-math></alternatives></inline-formula>, of a particle with mass <inline-formula><alternatives><mml:math id="inf247"><mml:mi>m</mml:mi></mml:math><tex-math id="inft247">\begin{document}$m$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ38"><alternatives><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t38">\begin{document}$$\displaystyle  m\frac{d}{dt}V(t)=-\gamma V(t)+F(t)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>When <inline-formula><alternatives><mml:math id="inf248"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft248">\begin{document}$F(t)$\end{document}</tex-math></alternatives></inline-formula> is an uncorrelated Gaussian process, <inline-formula><alternatives><mml:math id="inf249"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft249">\begin{document}$V(t)$\end{document}</tex-math></alternatives></inline-formula> is an Ornstein-Uhlenbeck process, which has an exponential correlation function (see <xref ref-type="bibr" rid="bib35">Gillespie, 1996a</xref>; <xref ref-type="bibr" rid="bib36">Gillespie, 1996b</xref> for a pedagogical review). The variance of the velocity, <inline-formula><alternatives><mml:math id="inf250"><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft250">\begin{document}$\sigma^{2}$\end{document}</tex-math></alternatives></inline-formula>, is related to the temperature, <inline-formula><alternatives><mml:math id="inf251"><mml:mi>T</mml:mi></mml:math><tex-math id="inft251">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>, by <inline-formula><alternatives><mml:math id="inf252"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft252">\begin{document}$\sigma^{2}=kT/m$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf253"><mml:mi>k</mml:mi></mml:math><tex-math id="inft253">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> is Boltzmann’s constant.</p><p>Numerically and experimentally, we must always discretize time. The discrete-time approximation to the Ornstein-Uhlenbeck process with time-step <inline-formula><alternatives><mml:math id="inf254"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:math><tex-math id="inft254">\begin{document}$\Delta t$\end{document}</tex-math></alternatives></inline-formula> is given by the difference equation<disp-formula id="equ39"><alternatives><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t39">\begin{document}$$\displaystyle  V_{t+\Delta t}=\phi V_t+\xi_t ,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ40"><alternatives><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t40">\begin{document}$$\displaystyle  p(\xi_t)=\mathcal{N}\left(\xi_t;0,\sigma_\xi^2\right) \,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>which is a first-order autoregressive, or AR(1), process (<xref ref-type="bibr" rid="bib98">Yule, 1927</xref>; <xref ref-type="bibr" rid="bib94">Walker, 1931</xref>). Like the Ornstein-Uhlenbeck process, it has an exponential autocovariance function, given by<disp-formula id="equ41"><alternatives><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t41">\begin{document}$$\displaystyle  E\left[V_tV_{t+k\Delta t}\right]=\sigma^2\exp\left(-\frac{k\Delta t}{\tau}\right)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>with variance<disp-formula id="equ42"><alternatives><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t42">\begin{document}$$\displaystyle  \sigma^2=\frac{\sigma_\xi^2}{1-\phi^2}$$\end{document}</tex-math></alternatives></disp-formula></p><p>and time constant<disp-formula id="equ43"><alternatives><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t43">\begin{document}$$\displaystyle  \tau=-\frac{\Delta t}{\ln\phi} \,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Let us consider how to incorporate a fluctuating variance in the AR(1) process. There are two possibilities: the variance of the driving noise <inline-formula><alternatives><mml:math id="inf255"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft255">\begin{document}$\xi_{t}$\end{document}</tex-math></alternatives></inline-formula> could fluctuate, as in Brownian motion with a fluctuating temperature. Alternatively, the entire process could be scaled by a fluctuating positive variable; in the Brownian motion experiment, this corresponds to changes in the magnification level of the microscope. This also corresponds to changes in inverse distance if the velocity is an angular velocity relative to an observer. The former, additive model corresponds to<disp-formula id="equ44"><alternatives><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>ξ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t44">\begin{document}$$\displaystyle  V^+_{t+\Delta t}=\phi V^+_t+S_t\xi_t \,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The latter, multiplicative model corresponds to<disp-formula id="equ45"><alternatives><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>t</mml:mi><mml:mo>×</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t45">\begin{document}$$\displaystyle  V^\times_t=S_t V_t \,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf256"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft256">\begin{document}$S_{t}$\end{document}</tex-math></alternatives></inline-formula> is the fluctuating scale. Rewriting<disp-formula id="equ46"><alternatives><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>×</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfrac><mml:mi>ϕ</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>t</mml:mi><mml:mo>×</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ξ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t46">\begin{document}$$\displaystyle  V^\times_{t+\Delta t}=\frac{S_{t+\Delta t}}{S_t}\phi V^\times_t+S_{t+\Delta t}\xi_t \,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>we see that <inline-formula><alternatives><mml:math id="inf257"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math><tex-math id="inft257">\begin{document}$V^{+}_{t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf258"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>×</mml:mo></mml:mrow></mml:msubsup></mml:math><tex-math id="inft258">\begin{document}$V^{\times}_{t}$\end{document}</tex-math></alternatives></inline-formula> are identical when <inline-formula><alternatives><mml:math id="inf259"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft259">\begin{document}$S_{t}$\end{document}</tex-math></alternatives></inline-formula> is constant in time and varies only across the ensemble. However, they are not equivalent when <inline-formula><alternatives><mml:math id="inf260"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft260">\begin{document}$S_{t}$\end{document}</tex-math></alternatives></inline-formula> changes over time. In the main text, we adopt the multiplicative model, since it factors into a product of two independent stochastic processes (one for normalized velocity and one for scale), which simplifies the inference problem.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Tracking algorithm details</title><sec sec-type="appendix" id="s9-1"><title>Validation</title><p>We validated the tracking algorithm by generating a synthetic movie, computing trajectories, and comparing them to ground truth. The synthetic movie consists of a single frame from <monospace>bees8-full</monospace> with a grid of 16 disks (of radius 48 pixels) copied from their initial positions, overlaid on the original image, and animated according to an ARGSM process. The underlying Gaussian velocity time series were generated by simulating critically damped harmonic oscillators (independently for each component) with velocity standard deviation <inline-formula><alternatives><mml:math id="inf261"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>Y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math><tex-math id="inft261">\begin{document}$\sigma_{Y_{1}}=\sigma_{Y_{2}}=0.5$\end{document}</tex-math></alternatives></inline-formula> and time constant <inline-formula><alternatives><mml:math id="inf262"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><tex-math id="inft262">\begin{document}$\tau=100$\end{document}</tex-math></alternatives></inline-formula> ms. The scale time series were generated by simulating an AR(1) process with standard deviation <inline-formula><alternatives><mml:math id="inf263"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math><tex-math id="inft263">\begin{document}$\sigma_{Z}=0.5$\end{document}</tex-math></alternatives></inline-formula> and coefficient <inline-formula><alternatives><mml:math id="inf264"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:math><tex-math id="inft264">\begin{document}$\phi_{Z}=0.75$\end{document}</tex-math></alternatives></inline-formula>. We multiplied each pair of velocity components by the scale and calculated the cumulative sum to determine the position of the disks on each frame relative to their grid positions. The disks occasionally occlude one another but are largely isolated; we expect that adding more disks and occlusions decreases tracking performance accordingly.</p><p>We apply the tracking algorithm to generate 8192 trajectories of length 65. After applying the tracking algorithm, we associate each tracking trajectory with a given ground truth trajectory if its initial position is within a given radius of the center of the corresponding disk; trajectories which are not associated with any ground truth are discarded. We calculate the mean squared error of the velocity along these trajectories relative to the ground truth velocity at the corresponding times. Looking at the fraction that remains (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1A</xref>), we find that many trajectories are seeded at the perimeter of the disk, since there tends to be a distinct edge there. The presence of trajectories with initial positions outside of any disk indicates that the algorithm detects some spurious motion in the vicinity of the actual motion. We calculate the mean squared error of the velocity along these trajectories relative to the ground truth velocity at the corresponding times. The SNR (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1B</xref>), or ratio of the variance of the ground truth velocity to the mean squared error, drops off quickly when the radius is large enough to include the spurious trajectories. However, the statistics of the ensemble are very close to the ground truth when all of the tracking trajectories are included. The distribution of a single velocity component (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1C</xref>) captures the overall shape and tails of the ground truth distribution but is more sharply peaked. The autocorrelation (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1D</xref>) is underestimated only slightly.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Tracking performance.</title><p>(<bold>A</bold>) Fraction of trajectories within a given radius of the center of the moving disks. (<bold>B</bold>) Tracking signal-to-noise ratio for the subset of trajectories within a given radius. (<bold>C</bold>) Horizontal velocity histogram for the tracking data and ground truth. (<bold>D</bold>) Horizontal velocity autocorrelations for the tracking data and ground truth.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-app2-fig1-v2.tif"/></fig></sec><sec sec-type="appendix" id="s9-2"><title>Effect of restricting velocity to trajectories</title><p>Since tracking is a difficult problem that involves associating velocity values in time subject to our criteria for trajectory quality, the set of velocity values along trajectories is only a subset of all velocity values in a given scene. To understand the difference, we apply the same algorithm with the trajectory length set to two, so there is only a single velocity per trajectory, with no forward-backward error thresholding. For <monospace>bees8-full</monospace>, we find a substantially broader distribution for the unrestricted velocity (see <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref>). The sample standard deviation and kurtosis for the trajectory-restricted velocity are 1.2 pix<sup>2</sup> and 10.2, respectively, compared to 1.9 pix<sup>2</sup> and 24.5 for the unrestricted velocity. We thus expect the standard deviation and kurtosis values reported here to be underestimates compared to theoretically perfect tracking.</p><fig id="app2fig2" position="float"><label>Appendix 2—figure 2.</label><caption><title>Velocity histograms.</title><p>Histograms for horizontal velocity with and without restriction to trajectories.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-104054-app2-fig2-v2.tif"/></fig></sec></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Numerical details</title><sec sec-type="appendix" id="s10-1"><title>Maximum likelihood estimation for autoregressive models</title><p>Following <xref ref-type="bibr" rid="bib58">Miller, 1995</xref>, we consider the time series as a <inline-formula><alternatives><mml:math id="inf265"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft265">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>-dimensional vector, <inline-formula><alternatives><mml:math id="inf266"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft266">\begin{document}$\mathbf{X}=[X_{1}\dots X_{T}]^{\top}$\end{document}</tex-math></alternatives></inline-formula>. Then,<disp-formula id="equ47"><alternatives><mml:math id="m47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t47">\begin{document}$$\displaystyle  p(\mathbf{x})=\mathcal{N}\left(\mathbf{x};\mathbf{0},\mathbf{\Sigma}\right)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ48"><alternatives><mml:math id="m48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t48">\begin{document}$$\displaystyle  \mathbf{\Sigma}=E\left[\mathbf{X}\mathbf{X}^\top\right]=\mathrm{Toeplitz}(\gamma_0,\dots,\gamma_{T-1})\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The autocovariance series <inline-formula><alternatives><mml:math id="inf267"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft267">\begin{document}$\{\gamma_{0},\dots,\gamma_{T-1}\}$\end{document}</tex-math></alternatives></inline-formula> is related to the regression coefficients and driving noise variance by<disp-formula id="equ49"><alternatives><mml:math id="m49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t49">\begin{document}$$\displaystyle  \gamma_0=\sum_{i=1}^k\phi_i\gamma_i+\sigma^2$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ50"><alternatives><mml:math id="m50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mi>j</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t50">\begin{document}$$\displaystyle  \gamma_j=\sum_{i=1}^k\phi_{i}\gamma_{i-j}\quad\quad j\geq1 \,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Given a collection of <inline-formula><alternatives><mml:math id="inf268"><mml:mi>N</mml:mi></mml:math><tex-math id="inft268">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> samples of length <inline-formula><alternatives><mml:math id="inf269"><mml:mi>T</mml:mi></mml:math><tex-math id="inft269">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> from the time series, we calculate the matrix <inline-formula><alternatives><mml:math id="inf270"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:math><tex-math id="inft270">\begin{document}$\mathbf{R}$\end{document}</tex-math></alternatives></inline-formula> defined by<disp-formula id="equ51"><alternatives><mml:math id="m51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t51">\begin{document}$$\displaystyle  R_{ij}=R_{ji}=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=i+1}^{T-j}x_{n,t}x_{n,t+i-j}\quad\quad 0\leq i\leq j\leq k \,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the index <inline-formula><alternatives><mml:math id="inf271"><mml:mi>n</mml:mi></mml:math><tex-math id="inft271">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> ranges over samples and <inline-formula><alternatives><mml:math id="inf272"><mml:mi>t</mml:mi></mml:math><tex-math id="inft272">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> ranges over time steps. Then, the maximum likelihood estimate of the parameters satisfies a nonlinear system of <inline-formula><alternatives><mml:math id="inf273"><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft273">\begin{document}$2(k+1)$\end{document}</tex-math></alternatives></inline-formula> equations, consisting of the <inline-formula><alternatives><mml:math id="inf274"><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft274">\begin{document}$k+1$\end{document}</tex-math></alternatives></inline-formula> autocovariance relations above, up to <inline-formula><alternatives><mml:math id="inf275"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft275">\begin{document}$\gamma_{k}$\end{document}</tex-math></alternatives></inline-formula>, together with the following:<disp-formula id="equ52"><alternatives><mml:math id="m52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t52">\begin{document}$$\displaystyle  R_{00}=\sum_{i=1}^k\phi_i\left(R_{0i}+i\gamma_i\right)+T\sigma^2$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ53"><alternatives><mml:math id="m53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mspace width="1em"/><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t53">\begin{document}$$\displaystyle  R_{j0}=\sum_{i=1}^k\phi_i\left(R_{ij}+i\gamma_{i-j}\right)\quad\quad 1\leq j\leq k\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>We solve this system numerically using MATLAB’s <monospace>fsolve</monospace>.</p><p>While the covariance matrix is easily calculated from the AR model parameters, it is not well appreciated that the inverse covariance matrix can be calculated exactly from the parameters as well (<xref ref-type="bibr" rid="bib89">Verbyla, 1985</xref>). This is useful because it is the inverse that is needed for the sampling methods described below, and inverting large, nearly singular matrices is numerically unstable. The inverse of <inline-formula><alternatives><mml:math id="inf276"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:math><tex-math id="inft276">\begin{document}$\mathbf{\Sigma}$\end{document}</tex-math></alternatives></inline-formula> is given by<disp-formula id="equ54"><alternatives><mml:math id="m54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t54">\begin{document}$$\displaystyle  \mathbf{\Sigma}^{-1}=\frac{1}{\sigma^2}\left(\mathbf{I}+\sum_{i=1}^k\phi_i^2\mathbf{E}_i-\sum_{i=1}^k\phi_i\mathbf{F}_i+\sum_{i=1}^{k-1}\sum_{j=1}^{p-i}\phi_i\phi_{i+j}\mathbf{G}_{j,i+j}\right)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf277"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:math><tex-math id="inft277">\begin{document}$\mathbf{I}$\end{document}</tex-math></alternatives></inline-formula> is the identity matrix, <inline-formula><alternatives><mml:math id="inf278"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft278">\begin{document}$\mathbf{E}_{i}$\end{document}</tex-math></alternatives></inline-formula> is the identity matrix with the first and last <inline-formula><alternatives><mml:math id="inf279"><mml:mi>i</mml:mi></mml:math><tex-math id="inft279">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> diagonal elements set to zero, <inline-formula><alternatives><mml:math id="inf280"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft280">\begin{document}$\mathbf{F}_{i}$\end{document}</tex-math></alternatives></inline-formula> is a matrix with ones along the <italic>i</italic>th upper and lower diagonals and zeros elsewhere, and <inline-formula><alternatives><mml:math id="inf281"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">G</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft281">\begin{document}$\mathbf{G}_{j,i+j}=\mathbf{E}_{i}\mathbf{F}_{j}\mathbf{E}_{i}$\end{document}</tex-math></alternatives></inline-formula> (note that we have corrected a typo in <xref ref-type="bibr" rid="bib89">Verbyla, 1985</xref>, in the indexing of the last term of the equation).</p></sec><sec sec-type="appendix" id="s10-2"><title>Sampling methods</title><p>Gibbs sampling works by starting with an initial vector <inline-formula><alternatives><mml:math id="inf282"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:math><tex-math id="inft282">\begin{document}$\mathbf{z}$\end{document}</tex-math></alternatives></inline-formula> and updating each element <inline-formula><alternatives><mml:math id="inf283"><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft283">\begin{document}$z_{t}$\end{document}</tex-math></alternatives></inline-formula> (in a random order) conditioned on the remaining elements <inline-formula><alternatives><mml:math id="inf284"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft284">\begin{document}$\mathbf{z}_{\setminus t}$\end{document}</tex-math></alternatives></inline-formula>, where the notation <inline-formula><alternatives><mml:math id="inf285"><mml:mo class="MJX-variant">∖</mml:mo><mml:mi>t</mml:mi></mml:math><tex-math id="inft285">\begin{document}$\setminus t$\end{document}</tex-math></alternatives></inline-formula> indicates all indices in the set <inline-formula><alternatives><mml:math id="inf286"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft286">\begin{document}$\{1,\dots,T\}\setminus t$\end{document}</tex-math></alternatives></inline-formula>. The posterior distribution is given by<disp-formula id="equ55"><alternatives><mml:math id="m55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t55">\begin{document}$$\displaystyle  \begin{array}{ll}p(z_t|\mathbf{x}_1,\mathbf{x}_2,\mathbf{z}_{\setminus t})&amp;\propto p(x_{1,t},x_{2,t}|\mathbf{x}_{1,\setminus t},\mathbf{x}_{2,\setminus t},\mathbf{z})p(z_t|\mathbf{x}_{1,\setminus t},\mathbf{x}_{2,\setminus t},\mathbf{z}_{\setminus t})\\ &amp;=p(x_{1,t}|\mathbf{x}_{1,\setminus t},\mathbf{z})p(x_{2,t}|\mathbf{x}_{2,\setminus t},\mathbf{z})p(z_t|\mathbf{z}_{\setminus t})\,.\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Each distribution in the product is a Gaussian given by<disp-formula id="equ56"><alternatives><mml:math id="m56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t56">\begin{document}$$\displaystyle  p(x_{1,t}|\mathbf{x}_{1,\setminus t},\mathbf{z})=\mathcal{N}\left(x_{1,t},\hat{\mu}_{Y_1}s_t,\hat{\sigma}_{Y_1}^2s_t^2\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ57"><alternatives><mml:math id="m57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t57">\begin{document}$$\displaystyle  p(x_{2,t}|\mathbf{x}_{2,\setminus t},\mathbf{z})=\mathcal{N}\left(x_{2,t},\hat{\mu}_{Y_2}s_t,\hat{\sigma}_{Y_2}^2s_t^2\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ58"><alternatives><mml:math id="m58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t58">\begin{document}$$\displaystyle  p(z_t|\mathbf{z}_{\setminus t})=\mathcal{N}\left(\hat{\mu}_Z,\hat{\sigma}_Z^2\right)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ59"><alternatives><mml:math id="m59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t59">\begin{document}$$\displaystyle  \hat{\sigma}^2_{Y_1}=\left(\mathbf{\Sigma}_{Y_1}^{-1}\right)_{tt}^{-1}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ60"><alternatives><mml:math id="m60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t60">\begin{document}$$\displaystyle  \hat{\sigma}^2_{Y_2}=\left(\mathbf{\Sigma}_{Y_2}^{-1}\right)_{tt}^{-1}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ61"><alternatives><mml:math id="m61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t61">\begin{document}$$\displaystyle  \hat{\sigma}^2_Z=\left(\mathbf{\Sigma}_Z^{-1}\right)_{tt}^{-1}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ62"><alternatives><mml:math id="m62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t62">\begin{document}$$\displaystyle  \hat{\mu}_{Y_1}=-\hat{\sigma}_{Y_1}^2\left(\mathbf{\Sigma}_{Y_1}^{-1}\right)_{t,\setminus t}\mathbf{y}_{1,\setminus t}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ63"><alternatives><mml:math id="m63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t63">\begin{document}$$\displaystyle  \hat{\mu}_{Y_2}=-\hat{\sigma}_{Y_2}^2\left(\mathbf{\Sigma}_{Y_2}^{-1}\right)_{t,\setminus t}\mathbf{y}_{2,\setminus t}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ64"><alternatives><mml:math id="m64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>∖</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t64">\begin{document}$$\displaystyle  \hat{\mu}_Z=-\hat{\sigma}_Z^2\left(\mathbf{\Sigma}_Z^{-1}\right)_{t,\setminus t}\mathbf{z}_{\setminus t}\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Rejection sampling is used to draw a sample of each individual <inline-formula><alternatives><mml:math id="inf287"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft287">\begin{document}$z_{t}$\end{document}</tex-math></alternatives></inline-formula>. In this framework, a proposal distribution that is easy to sample from, <inline-formula><alternatives><mml:math id="inf288"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft288">\begin{document}$q(z)$\end{document}</tex-math></alternatives></inline-formula>, is chosen as an envelope (after an appropriate scaling) of a target distribution, <inline-formula><alternatives><mml:math id="inf289"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft289">\begin{document}$\hat{p}(z)$\end{document}</tex-math></alternatives></inline-formula>, from which we would like to sample, i.e., <inline-formula><alternatives><mml:math id="inf290"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>M</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft290">\begin{document}$\hat{p}(z)\leq Mq(z)$\end{document}</tex-math></alternatives></inline-formula> for some positive scaling factor <inline-formula><alternatives><mml:math id="inf291"><mml:mi>M</mml:mi></mml:math><tex-math id="inft291">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula>. The proposal distribution is sampled from, followed by sampling another variable <inline-formula><alternatives><mml:math id="inf292"><mml:mi>u</mml:mi></mml:math><tex-math id="inft292">\begin{document}$u$\end{document}</tex-math></alternatives></inline-formula> from a uniform distribution. If <inline-formula><alternatives><mml:math id="inf293"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>u</mml:mi><mml:mo>≤</mml:mo><mml:mi>M</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft293">\begin{document}$u\leq Mq(z)/\hat{p}(z)$\end{document}</tex-math></alternatives></inline-formula>, the sample is accepted; otherwise, it is rejected and the sampling is repeated. The method produces samples from the target distribution exactly, even if it is unnormalized, but requires the envelope to be tight to avoid rejecting too many samples. Here, the target distribution is the unnormalized posterior (dropping time indices for clarity),<disp-formula id="equ65"><alternatives><mml:math id="m65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t65">\begin{document}$$\displaystyle  \hat{p}(z)=\mathcal{N}\left(x_1;\hat{\mu}_{Y_1}s,\hat{\sigma}_{Y_1}^2s^2\right)\mathcal{N}\left(x_1;\hat{\mu}_{Y_2}s,\hat{\sigma}_{Y_2}^2s^2\right)\mathcal{N}\left(z;\hat{\mu}_Z,\hat{\sigma}_Z^2\right)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>We let the proposal distribution be a Gaussian,<disp-formula id="equ66"><alternatives><mml:math id="m66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t66">\begin{document}$$\displaystyle  q(z)=\mathcal{N}\left(z;\mu_q,\sigma_q^2\right)\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>whose mean and variance are optimized heuristically to form a tight envelope of the target distribution. We center this Gaussian over the peak of the target distribution,<disp-formula id="equ67"><alternatives><mml:math id="m67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>z</mml:mi></mml:munder><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t67">\begin{document}$$\displaystyle  \mu_q=\arg\max_z\hat{p}(z)\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>and compute the scale factor <inline-formula><alternatives><mml:math id="inf294"><mml:mi>M</mml:mi></mml:math><tex-math id="inft294">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> to match the two distributions at the peak, <inline-formula><alternatives><mml:math id="inf295"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft295">\begin{document}$M=\hat{p}(\mu_{q})/q(\mu_{q})$\end{document}</tex-math></alternatives></inline-formula>. To optimize the variance, note that we can write<disp-formula id="equ68"><alternatives><mml:math id="m68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t68">\begin{document}$$\displaystyle  \begin{array}{ll}\log\hat{p}(z)=&amp;-\frac{1}{2}\frac{\left[x_1\exp(-z)-\hat{\mu}_{Y_1}\right]^2}{\hat{\sigma}_{Y_1}^2}-\frac{1}{2}\frac{\left[x_2\exp(-z)-\hat{\mu}_{Y_2}\right]^2}{\hat{\sigma}_{Y_2}^2}\\ &amp;-\frac{1}{2}\frac{\left[z-\left(\hat{\mu}_Z-2\hat{\sigma}_Z^2\right)\right]^2}{\hat{\sigma}_Z^2}-2\left(\hat{\mu}_Z-\hat{\sigma}_Z^2\right)+K\,.\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>The first two terms rapidly approach constants to the right and diverge negatively to the left. Thus, the right tail of <inline-formula><alternatives><mml:math id="inf296"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft296">\begin{document}$\hat{p}(z)$\end{document}</tex-math></alternatives></inline-formula> behaves as <inline-formula><alternatives><mml:math id="inf297"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft297">\begin{document}$\mathcal{N}(z;\hat{\mu}_{Z}-2\hat{\sigma}_{Z}^{2},\hat{\sigma}_{Z}^{2})$\end{document}</tex-math></alternatives></inline-formula>, and the left tail falls off extremely rapidly. A variance of <inline-formula><alternatives><mml:math id="inf298"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>q</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft298">\begin{document}$\sigma_{q}^{2}=\hat{\sigma}_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula> is necessary and sufficient to cover the two tails when the peak of the proposal distribution is to the right of the peak of this Gaussian distribution, i.e., when <inline-formula><alternatives><mml:math id="inf299"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft299">\begin{document}$\mu_{q}\geq\hat{\mu}_{Z}-2\hat{\sigma}_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula>. A smaller variance will not fully envelop the right tail regardless of the value of <inline-formula><alternatives><mml:math id="inf300"><mml:msub><mml:mi>μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft300">\begin{document}$\mu_{q}$\end{document}</tex-math></alternatives></inline-formula>. We require a slightly larger variance when <inline-formula><alternatives><mml:math id="inf301"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft301">\begin{document}$\mu_{q} \lt \hat{\mu}_{Z}-2\hat{\sigma}_{Z}^{2}$\end{document}</tex-math></alternatives></inline-formula>, which we calculate by assuming <inline-formula><alternatives><mml:math id="inf302"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft302">\begin{document}$Mq(z)$\end{document}</tex-math></alternatives></inline-formula> makes exactly one other point of contact with <inline-formula><alternatives><mml:math id="inf303"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft303">\begin{document}$\hat{p}(z)$\end{document}</tex-math></alternatives></inline-formula>. Let<disp-formula id="equ69"><alternatives><mml:math id="m69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t69">\begin{document}$$\displaystyle  f(z)=\log\hat{p}(z)$$\end{document}</tex-math></alternatives></disp-formula></p><p>and<disp-formula id="equ70"><alternatives><mml:math id="m70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>M</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t70">\begin{document}$$\displaystyle  \begin{array}{ll}g(z)&amp;=\log[Mq(z)] \\ &amp;=-\frac{1}{2}\frac{(z-\mu_q)^2}{\sigma_q^2}+f(\mu_q)\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Let <inline-formula><alternatives><mml:math id="inf304"><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft304">\begin{document}$z_{0}$\end{document}</tex-math></alternatives></inline-formula> be the point of contact. Then, <inline-formula><alternatives><mml:math id="inf305"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft305">\begin{document}$f(z_{0})=g(z_{0})$\end{document}</tex-math></alternatives></inline-formula> implies<disp-formula id="equ71"><alternatives><mml:math id="m71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t71">\begin{document}$$\displaystyle  \sigma_q^2=\frac{1}{2}\frac{(z_0-\mu_q)^2}{f(\mu_q)-f(z_0)}\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The two curves must also be tangent at <inline-formula><alternatives><mml:math id="inf306"><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft306">\begin{document}$z_{0}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf307"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft307">\begin{document}$f^{\prime}(z_{0})=g^{\prime}(z_{0})$\end{document}</tex-math></alternatives></inline-formula>. We have<disp-formula id="equ72"><alternatives><mml:math id="m72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>g</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t72">\begin{document}$$\displaystyle  g'(z_0)=-\frac{z_0-\mu_q}{\sigma_q^2}=-2\frac{f(\mu_q)-f(z_0)}{z_0-\mu_q}\,.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The tangency condition implies <inline-formula><alternatives><mml:math id="inf308"><mml:msub><mml:mi>z</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft308">\begin{document}$z_{0}$\end{document}</tex-math></alternatives></inline-formula> is the solution to<disp-formula id="equ73"><alternatives><mml:math id="m73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t73">\begin{document}$$\displaystyle  f'(z_0)+2\frac{f(\mu_q)-f(z_0)}{z_0-\mu_q}=0\,,$$\end{document}</tex-math></alternatives></disp-formula></p><p>which we solve numerically using MATLAB’s <monospace>fzero</monospace> function, and use this solution to calculate <inline-formula><alternatives><mml:math id="inf309"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>q</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft309">\begin{document}$\sigma_{q}^{2}$\end{document}</tex-math></alternatives></inline-formula>. Finally, if the above procedure fails due to numerical issues, we simply set <inline-formula><alternatives><mml:math id="inf310"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>q</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1.1</mml:mn><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft310">\begin{document}$\sigma_{q}^{2}=(1.1\hat{\sigma}_{z})^{2}$\end{document}</tex-math></alternatives></inline-formula>, which is large enough to form an envelope in practice.</p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104054.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>This paper tackles an important problem: the statistics of natural motion. The statistics of natural stimuli are in general highly structured, and the properties of that structure has guided understanding of sensory coding. This paper extends this analysis to natural motion. The authors first characterize the non-Gaussian properties of natural motion, and then introduce a simple Gaussian scale-mixture model that captures that behavior. The model is developed in a clear and convincing way and the results are compelling.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104054.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A dynamic scale-mixture model of motion in natural scenes&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>The reviewers all appreciated the importance of the topic and the general approach taken. The main concerns regarded the clarity of explanations of several key results and assumptions for some of the analyses. Those are detailed in the reviewer comments below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Strengths:</p><p>A systematic treatment of the properties of natural motion is needed. The point tracking approach used here is a nice approach to resolve many of the challenges facing other approaches to this problem. The evidence that motion statistics are non-Gaussian is clear, and the introduction of the nonlinear scaling parameter to capture that non-Gaussian behavior provides a quite useful characterization of that non-Gaussian behavior.</p><p>Weaknesses:</p><p>1. Intuitively, it seems expected that horizontal and vertical velocities should be correlated given that most motion will occur in a direction that is not purely along one of those axes. That intuitive description seems to capture the key features illustrated in Figure 2. Can this physical aspect of the problem be incorporated into the analysis? At a minimum more could be done to develop this intuition, and then use that to intuition to help interpret the more quantitative analyses.</p><p>2. Some of the assumptions in the &quot;Coding implications of heavy tails&quot; section were not made sufficiently clear. First, I think that a nonlinearity that mapped the distribution to a Gaussian would introduce signal dependent noise only if the noise occurs prior to the nonlinearity. If that is correct, it should be stated as part of that analysis. Second, doesn't the &quot;demodulation&quot; approach introduced on line 219 also introduce signal dependent noise if the noise is introduced prior to the normalization step? This would seem to violate the conditions under which the simple Gaussian channel argument holds (line 199-201). Because of these concerns it was not clear what to take away from this section.</p><p>3. Figure 5A and B are described (first paragraph of the dynamics section) as providing evidence for a contribution of a nonlinear scale factor to motion correlations. I am not clear on why that is needed. Both of these would seem to be true for simple linear correlations. I may well be missing something, but if so the argument in the text about the need for the nonlinear scaling in this case should be made more clearly. This concern made it difficult to evaluate the rest of that section, as it was not clear why the scaling was needed. This figure and the associated analysis could be described more clearly, perhaps showing the bow-tie structures from earlier figures.</p><p>Line 206: I believe the reference here should be to Figure 4G</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Summary of work:</p><p>The paper introduces a model of motion in video sequences from natural scenes. The main contribution of the paper is to show that the statistics of the velocity of the motion of key points in natural scenes are well captured by Gaussian scale mixtures (GSM) models. Two hypotheses for this are that: (1) the distance of the moving objects to the camera control the range of angular motion; and (2) the scale of the driving forces that create motion may fluctuate over time. The proposed model introduces a GSM process which is a combination of an underlying Gaussian process and a scaling process.</p><p>Strengths:</p><p>- The proposed model elegantly captures the statistics of the velocity distribution for vertical and horizontal direction in the video sequence.</p><p>- The authors provide an intuitive information theoretic justification for the normalization process that involves reversing the scaling operation to recover the underlying Gaussian latent factor of their model.</p><p>- The justification of the model's complexity and the gains in predictive accuracy using the Akaike Information Criterion (AIC) add strength to the argument.</p><p>- The choice of the distribution of the scaling variable as a $\log$ normal random variable provides an easy way to track model in terms of relevant statistics like variance and kurtosis that apply to testing fits and justifying the coding efficiency arguments.</p><p>- The introduction of a dynamic GSM using Gaussian processes for both the underlying Gaussian variable and the $\log$ of the scaling factors is also well justified with comparison to other alternative models with static scaling or time independence.</p><p>Weaknesses:</p><p>- While the model using $\log$ normal scaling variables provides tractable statistics it may not be the most accurate and also not fully justified from the underlying phenomenon (at least this point is not made clear in the paper)</p><p>- The proposed model discusses a way to represent velocities as a multiplicative mix. It would benefit from making more explicit connection between the GSM model's representation and areas in the brain that may represent velocities. If the work in this area is lacking, does the proposed model hint at areas that could be encoding velocities?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This is one of the most enjoyable manuscripts I have reviewed in a while! The topic of object motion estimation and tracking is central to visual and visuomotor processing. While this is a very broad topic, the manuscript focuses on a small, well-defined part of it and reports a study with exemplary execution. The main finding is that the statistics of motion of points (a more tractable proxy for &quot;objects&quot;) in natural videos are heavy-tailed, highly predictable, and well modeled with Gaussian Scale Mixtures and their new dynamic (auto-regressive) extensions. The main implications are that these regularities should be exploited by the visual system to predict object motion and by neurons to adapt to object motion statistics across different environments, leading to several experimental predictions.</p><p>The authors propose to characterize the statistics of local motion over time in natural videos, and to develop generative models that capture those statistics. To do so, they choose a database of natural videos with moderate complexity, including no camera motion and closeup scenes of uninterrupted dense motion (e.g. water flowing or swarms of insects). This allows them to use effectively a standard point-tracking algorithm to define the point motion trajectories for analysis, and to restrict their analysis to 1-second-long clips of uninterrupted but substantially variable motion. They explain well the importance of studying trajectories (which are dynamic motion features) rather than simply motion energy or optic flow, which are in a sense instantaneous motion features.</p><p>Their analysis provides compelling evidence that both the instantaneous and dynamic statistic of point velocities deviate from Gaussian, have long tails, and may result from a Gaussian-distributed velocities multiplied by a global scaling factor, leading them to formulate Gaussian Scale Mixture models (GSM). GSMs have been used to model statistics of images (some references cited) and instantaneous statistics of movies (some references missing), but not in the context of object/point motion and not considering entire trajectories, as they do here. The analysis carefully quantifies the improvement in capturing motion statistics with their models over simpler alternatives.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;A dynamic scale-mixture model of motion in natural scenes&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Timothy Behrens (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some small remaining suggestions that should be considered, as detailed below:</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This is a revision of a paper describing the statistics of natural motion. This is an issue that, to my knowledge, has not been described in detail and the work in the paper fills this gap nicely. The revisions have improved the paper substantially. I have a few remaining suggestions.</p><p>Line 117: define &quot;good trajectories&quot;</p><p>Line 130-132: the heavy tails are clear in Figure 2 but not in Figure 1. Can you refer ahead here to help a reader out?</p><p>Line 207 (and below): doesn't this strategy often also increase noise since it must be based on an estimate of X (with noise) and the estimate of S could be small in some instances?</p><p>Line 286: define &quot;innovation noise&quot; – particularly why it is given that name</p><p>Line 287: why is the innovation noise small?</p><p>Lines 291-295: What is going on with the insect movies?</p><p>Lines 291-295: How can the variance explained by the naive model R^2_X be higher than that for the more complete model?</p><p>Lines 302-304: the prediction section ends on a technical note. It would be helpful to have a summary of the main take home point from that section.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>After reading the revised version of the manuscript, I can see that the concerns raised by all reviewers where either incorporated into the manuscript or clarified in the author responses. The section on the coding implications of heavy tails presents the argument better as it also considers some of the limitations of possible encoding decoding schemes. Another section that has improved greatly is the explanation of the reasoning for introducing a dynamic scale model. We appreciate the notation changes to denote expected values with the more commonly used notation in statistics. Also, the added magnitude correlation helps explaining the shift to independence. is there any way this can be linked to footnote 3?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The authors have replied and addressed my (minor) suggestions. The main one was an issue in Figure 5, which has been addressed with a new quantification and plot that I find helpful. The revisions address also the comments from other reviewers, as far as I can tell, and improve the clarity of the paper.</p><p><italic>I have no remaining concerns.</italic></p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.104054.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Strengths:</p><p>A systematic treatment of the properties of natural motion is needed. The point tracking approach used here is a nice approach to resolve many of the challenges facing other approaches to this problem. The evidence that motion statistics are non-Gaussian is clear, and the introduction of the nonlinear scaling parameter to capture that non-Gaussian behavior provides a quite useful characterization of that non-Gaussian behavior.</p><p>Weaknesses:</p><p>1. Intuitively, it seems expected that horizontal and vertical velocities should be correlated given that most motion will occur in a direction that is not purely along one of those axes. That intuitive description seems to capture the key features illustrated in Figure 2. Can this physical aspect of the problem be incorporated into the analysis? At a minimum more could be done to develop this intuition, and then use that to intuition to help interpret the more quantitative analyses.</p></disp-quote><p>While it is true that most motion samples are not purely along the horizontal or vertical axes in our data, this does not imply an overall correlation between the horizontal and vertical components of the velocity. Correlation between velocity components means that there is some dominant axis of the motion distribution that is not aligned with the horizontal or vertical axes. Some scenes did indeed have small correlations between velocity components, as indicated by a tilt in their raw histograms (not shown). These small correlations were removed as described in the last paragraph of the “Point tracking” section of “Materials and methods.” Thus, horizontal and vertical velocity are linearly uncorrelated in all scenes.</p><p>It seems possible the reviewer is referring to the horizontal and vertical speed, that is, the absolute values of the velocity components, and the fact that the mean of the product of the two speeds will be positive. This mean absolute product is not a meaningful measure of correlation, since it will be positive even if the two velocities are independent. After subtracting off the mean speeds to properly calculate the correlation, it will be zero if the velocity components are independent.</p><p>This brings up the interesting point that the component speed correlations are, in fact, nonzero for our data, while the component velocity correlations are zero. This is a simple way of demonstrating the nonlinear dependence between velocity components. We have added a simple calculation to the text when first describing heavy-tailed joint distributions (second paragraph of “Heavy-tailed statistics of natural motion”) and describe how this arises in a model with a shared scale parameter (footnote 3 in the same section). We have also added this calculation as a function of time lag in Figure 5C, since it very clearly demonstrates that the nonlinear dependency decays with time lag, which (as other reviewers point out) is not so easily appreciated from the raw data in Figure 5D.</p><p>We have also added the following text to avoid confusion about what is meant by velocity and correlation:</p><p>“The focus of our analysis is the point velocity, or difference in point positions between subsequent frames: a two-dimensional vector quantity measured in raw units of pixels/frame.” (3rd paragraph of “Results”)</p><p>“The lack of tilt in the histogram indicates that the two velocity components are uncorrelated (in this context, correlation between velocity components would indicate a tendency for objects within a scene to move along some diagonal axis relative to the camera).” (2nd paragraph of “Heavy-tailed statistics of natural motion”)</p><disp-quote content-type="editor-comment"><p>2. Some of the assumptions in the &quot;Coding implications of heavy tails&quot; section were not made sufficiently clear. First, I think that a nonlinearity that mapped the distribution to a Gaussian would introduce signal dependent noise only if the noise occurs prior to the nonlinearity. If that is correct, it should be stated as part of that analysis. Second, doesn't the &quot;demodulation&quot; approach introduced on line 219 also introduce signal dependent noise if the noise is introduced prior to the normalization step? This would seem to violate the conditions under which the simple Gaussian channel argument holds (line 199-201). Because of these concerns it was not clear what to take away from this section.</p></disp-quote><p>We have expanded upon the motivations and assumptions for the Gaussian channel argument, which is intended as a minimal, intuitive explanation of the information theoretic implications of our findings. A Gaussian channel introduces additive noise to an encoded input signal, placing the noise after the encoding. The output of the Gaussian channel is then decoded, and the effects of the added noise must be ameliorated by the choice of the input encoding function. In the “Gaussianization” example, the encoding and decoding are done by the nonlinear function f and its inverse, respectively. We have clarified that it is the noise associated with the decoded variable that is signal-dependent to avoid confusion with the channel noise, which is, of course, independent. In the “demodulation” example, the decoded variable is also corrupted by signal-dependent noise.</p><disp-quote content-type="editor-comment"><p>3. Figure 5A and B are described (first paragraph of the dynamics section) as providing evidence for a contribution of a nonlinear scale factor to motion correlations. I am not clear on why that is needed. Both of these would seem to be true for simple linear correlations. I may well be missing something, but if so the argument in the text about the need for the nonlinear scaling in this case should be made more clearly. This concern made it difficult to evaluate the rest of that section, as it was not clear why the scaling was needed. This figure and the associated analysis could be described more clearly, perhaps showing the bow-tie structures from earlier figures.</p></disp-quote><p>We have revised the first paragraph of this section to make it clearer, with reference to Figures 2B and 2D, that these are essentially the same observations as before, with the addition of a tilt due to linear correlation, and hence the same conclusions hold. The two forms of dependency coexist. The scale variable is “needed” for the same reason as in the static case, namely to model the heavy tails. The subtle distinction, which we build up to, is that there is not a single scale variable modulating the velocity at different time points, but two correlated scale variables. These empirical observations all serve to motivate the full ARGSM model.</p><disp-quote content-type="editor-comment"><p>Line 206: I believe the reference here should be to Figure 4G</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Summary of work:</p><p>The paper introduces a model of motion in video sequences from natural scenes. The main contribution of the paper is to show that the statistics of the velocity of the motion of key points in natural scenes are well captured by Gaussian scale mixtures (GSM) models. Two hypotheses for this are that: (1) the distance of the moving objects to the camera control the range of angular motion; and (2) the scale of the driving forces that create motion may fluctuate over time. The proposed model introduces a GSM process which is a combination of an underlying Gaussian process and a scaling process.</p><p>Strengths:</p><p>- The proposed model elegantly captures the statistics of the velocity distribution for vertical and horizontal direction in the video sequence.</p><p>- The authors provide an intuitive information theoretic justification for the normalization process that involves reversing the scaling operation to recover the underlying Gaussian latent factor of their model.</p><p>- The justification of the model's complexity and the gains in predictive accuracy using the Akaike Information Criterion (AIC) add strength to the argument.</p><p>- The choice of the distribution of the scaling variable as a $\log$ normal random variable provides an easy way to track model in terms of relevant statistics like variance and kurtosis that apply to testing fits and justifying the coding efficiency arguments.</p><p>- The introduction of a dynamic GSM using Gaussian processes for both the underlying Gaussian variable and the $\log$ of the scaling factors is also well justified with comparison to other alternative models with static scaling or time independence.</p><p>Weaknesses:</p><p>- While the model using $\log$ normal scaling variables provides tractable statistics it may not be the most accurate and also not fully justified from the underlying phenomenon (at least this point is not made clear in the paper)</p></disp-quote><p>This is a valid point. Different authors in the GSM literature use different scale variable distributions, with little consensus on what distribution is best. Vanderstraeten and Beck (2008) (referenced in the main text) argue that a log-normal distribution arises from a maximum entropy argument in some systems in statistical physics (along with γ and inverse γ, depending on the problem). However, we are operating quite far from such a first principles argument in our own work. The log-normal GSM empirically fits the marginal distributions quite well. To capture the time-dependency of the scale, it is very helpful to model temporal correlations with an underlying Gaussian random variable, then pass this through a rectifying nonlinearity to get a positive scale variable. Other nonlinearities could be used, but the exponential function is a simple and natural choice. Thus, the log-normal distribution both fits the data well and is simple to work with. Other distributions have the advantage of giving closed-form expressions for the resulting GSM distribution, but this turns out not to be useful for estimating the multivariate and time-varying models.</p><disp-quote content-type="editor-comment"><p>- The proposed model discusses a way to represent velocities as a multiplicative mix. It would benefit from making more explicit connection between the GSM model's representation and areas in the brain that may represent velocities. If the work in this area is lacking, does the proposed model hint at areas that could be encoding velocities?</p></disp-quote><p>Many parts of the brain encode velocity, from simple direction-selective cells in the retinas of many mammals to highly refined representation in area MT of the primate cortex. We do not believe that our GSM model of motion points to particular brain regions, but rather highlights the problem of an underlying fluctuating scale that should apply to all velocity-encoding cells. Mechanisms for dealing with this problem range from single-cell adaptation mechanisms to circuit-level normalization, as mentioned in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This is one of the most enjoyable manuscripts I have reviewed in a while! The topic of object motion estimation and tracking is central to visual and visuomotor processing. While this is a very broad topic, the manuscript focuses on a small, well-defined part of it and reports a study with exemplary execution. The main finding is that the statistics of motion of points (a more tractable proxy for &quot;objects&quot;) in natural videos are heavy-tailed, highly predictable, and well modeled with Gaussian Scale Mixtures and their new dynamic (auto-regressive) extensions. The main implications are that these regularities should be exploited by the visual system to predict object motion and by neurons to adapt to object motion statistics across different environments, leading to several experimental predictions.</p><p>The authors propose to characterize the statistics of local motion over time in natural videos, and to develop generative models that capture those statistics. To do so, they choose a database of natural videos with moderate complexity, including no camera motion and closeup scenes of uninterrupted dense motion (e.g. water flowing or swarms of insects). This allows them to use effectively a standard point-tracking algorithm to define the point motion trajectories for analysis, and to restrict their analysis to 1-second-long clips of uninterrupted but substantially variable motion. They explain well the importance of studying trajectories (which are dynamic motion features) rather than simply motion energy or optic flow, which are in a sense instantaneous motion features.</p><p>Their analysis provides compelling evidence that both the instantaneous and dynamic statistic of point velocities deviate from Gaussian, have long tails, and may result from a Gaussian-distributed velocities multiplied by a global scaling factor, leading them to formulate Gaussian Scale Mixture models (GSM). GSMs have been used to model statistics of images (some references cited) and instantaneous statistics of movies (some references missing), but not in the context of object/point motion and not considering entire trajectories, as they do here. The analysis carefully quantifies the improvement in capturing motion statistics with their models over simpler alternatives.</p></disp-quote><p>We thank this reviewer for their enthusiasm and for making a number of helpful recommendations.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>This is a revision of a paper describing the statistics of natural motion. This is an issue that, to my knowledge, has not been described in detail and the work in the paper fills this gap nicely. The revisions have improved the paper substantially. I have a few remaining suggestions.</p></disp-quote><p>We appreciate your attention to detail! Here are our changes:</p><disp-quote content-type="editor-comment"><p>Line 117: define &quot;good trajectories&quot;</p></disp-quote><p>We have changed this to “the ensemble of trajectories” to avoid subjective language.</p><disp-quote content-type="editor-comment"><p>Line 130-132: the heavy tails are clear in Figure 2 but not in Figure 1. Can you refer ahead here to help a reader out?</p></disp-quote><p>We have added the parenthetical “(see \FIG{Figure 2}A for a log scale plot that emphasizes the tail behavior).”</p><disp-quote content-type="editor-comment"><p>Line 207 (and below): doesn't this strategy often also increase noise since it must be based on an estimate of X (with noise) and the estimate of S could be small in some instances?</p></disp-quote><p>In the limit that no channel noise is added to the estimate of <italic>S</italic>, the scaling can be inverted perfectly regardless of its value, so errors in estimating <italic>S</italic> do not increase noise. They do, however, make the distribution of the estimate of <italic>Y</italic> less Gaussian and therefore decrease channel efficiency.</p><disp-quote content-type="editor-comment"><p>Line 286: define &quot;innovation noise&quot; – particularly why it is given that name</p></disp-quote><p>This is jargon from the signal processing literature. We have replaced all instances with “driving noise,” which should be more intuitive for a broad audience.</p><disp-quote content-type="editor-comment"><p>Line 287: why is the innovation noise small?</p></disp-quote><p>This is an empirical observation. We have replaced the relevant sentence with, “Since $\σ_\zeta^2$ is small for the estimated models, indicating that the scale fluctuations are highly predictable at the level of single time steps, we expect this term to have little effect.”</p><disp-quote content-type="editor-comment"><p>Lines 291-295: What is going on with the insect movies?</p></disp-quote><p>These do show a faster timescale and motivate recording with a higher framerate camera (outside the scope of this paper, though). We have added the parenthetical “(a consequence of their very short velocity correlation times).”</p><disp-quote content-type="editor-comment"><p>Lines 291-295: How can the variance explained by the naive model R^2_X be higher than that for the more complete model?</p></disp-quote><p>We have double-checked the numerics and this appears to be just some quirk of the model fitting. It is unexpected but not mathematically impossible, since the ARGSM model has a more complicated likelihood function and fitting procedure. The AIC demonstrates that ARGSM is a better fit than AR in terms of likelihood, and it is likelihood that we are optimizing, not R^2. We have added the word “Predictive” to the caption for Figure 7J so that this is less easily confused with a measure of model fit. Since our conclusion is that AR and ARGSM are quite similar in terms of R^2, we don’t feel it is necessary to highlight this beyond pointing it out parenthetically as we do now.</p><disp-quote content-type="editor-comment"><p>Lines 302-304: the prediction section ends on a technical note. It would be helpful to have a summary of the main take home point from that section.</p></disp-quote><p>We have made this technical sentence a footnote and added the concluding sentence, “The results of this section indicate that this is feasible with or without taking the fluctuating scale into account.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>After reading the revised version of the manuscript, I can see that the concerns raised by all reviewers where either incorporated into the manuscript or clarified in the author responses. The section on the coding implications of heavy tails presents the argument better as it also considers some of the limitations of possible encoding decoding schemes. Another section that has improved greatly is the explanation of the reasoning for introducing a dynamic scale model. We appreciate the notation changes to denote expected values with the more commonly used notation in statistics. Also, the added magnitude correlation helps explaining the shift to independence. is there any way this can be linked to footnote 3?</p></disp-quote><p>Thank you for your response and helpful suggestion. We have added the phrase “discussed above (see footnote 3)” to the text introducing Figure 5C.</p></body></sub-article></article>