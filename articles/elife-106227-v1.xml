<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106227</article-id><article-id pub-id-type="doi">10.7554/eLife.106227</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106227.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Developmental Biology</subject></subj-group></article-categories><title-group><article-title>MorphoNet 2.0: An innovative approach for qualitative assessment and segmentation curation of large-scale 3D time-lapse imaging datasets</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Gallean</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Laurent</surname><given-names>Tao</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Biasuz</surname><given-names>Kilian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0240-2754</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Clement</surname><given-names>Ange</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Faraj</surname><given-names>Noura</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lemaire</surname><given-names>Patrick</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4925-2009</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Faure</surname><given-names>Emmanuel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2787-0885</contrib-id><email>emmanuel.faure@lirmm.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013yean28</institution-id><institution>Laboratoire d'informatique, de robotique et de microélectronique de Montpellier, LIRMM, Université de Montpellier, CNRS</institution></institution-wrap><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01xpc6869</institution-id><institution>Centre de Recherche de Biologie cellulaire de Montpellier, CRBM, Université de Montpellier, CNRS</institution></institution-wrap><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/051escj72</institution-id><institution>Montpellier Ressources Imagerie, Biocampus, Université de Montpellier, CNRS, INSERM</institution></institution-wrap><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Graña</surname><given-names>Martin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dpm2z73</institution-id><institution>Institut Pasteur de Montevideo</institution></institution-wrap><country>Uruguay</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution>CNRS</institution><country>France</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>12</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP106227</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-02-21"><day>21</day><month>02</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-03-01"><day>01</day><month>03</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.21.639560"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-05-12"><day>12</day><month>05</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106227.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-11-04"><day>04</day><month>11</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106227.2"/></event></pub-history><permissions><copyright-statement>© 2025, Gallean, Laurent et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Gallean, Laurent et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106227-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-106227-figures-v1.pdf"/><abstract><p>Thanks to recent promising advances in AI, automated segmentation of imaging datasets has made significant strides. However, the evaluation and curation of 3D and 3D+t datasets remain extremely challenging and highly resource-intensive. We present MorphoNet 2.0, a major conceptual and technical evolution designed to facilitate the segmentation, self-evaluation, and correction of 3D images. The application is accessible to non-programming biologists through user-friendly graphical interfaces and works on all major operating systems. We showcase its power in enhancing segmentation accuracy and boosting interpretability across five previously published segmented datasets. This new approach is crucial for producing ground-truth datasets of discovery-level scientific quality, critical for training and benchmarking advanced AI-driven segmentation tools, as well as for competitive challenges.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd><italic>Drosophila melanogaster</italic></kwd><kwd>ascidian</kwd><kwd><italic>Platynereis dumerilii</italic></kwd><kwd>tribolium castaneum</kwd><kwd>starfish</kwd><kwd><italic>Caenorhabditis elegans</italic></kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>A. thaliana</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Occitanie Region</institution></institution-wrap></funding-source><award-id>ESR-PREMAT-213</award-id><principal-award-recipient><name><surname>Faure</surname><given-names>Emmanuel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rbzpz17</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-INBS-04</award-id><principal-award-recipient><name><surname>Faure</surname><given-names>Emmanuel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rbzpz17</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-19-CE13-0020</award-id><principal-award-recipient><name><surname>Lemaire</surname><given-names>Patrick</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rbzpz17</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-21-CE13-0046</award-id><principal-award-recipient><name><surname>Lemaire</surname><given-names>Patrick</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>EpiGenMed Labex</institution></institution-wrap></funding-source><award-id>ProjetIA-10-LABX-0012</award-id><principal-award-recipient><name><surname>Lemaire</surname><given-names>Patrick</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04w6kn183</institution-id><institution>Fondation pour la Recherche Médicale</institution></institution-wrap></funding-source><award-id>EQU202303016262</award-id><principal-award-recipient><name><surname>Lemaire</surname><given-names>Patrick</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A collaborative open-source platform enabling interactive 3D visualization, quantitative evaluation, and expert curation of complex biological morphologies to enhance the quality and reproducibility of large-scale imaging studies.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Recent advances in optical time-lapse microscopy now enable the 3D capture of life’s temporal dynamics, from molecular assemblies to entire organisms (<xref ref-type="bibr" rid="bib24">McDole et al., 2018</xref>), in cells, embryos, and aggregates. These technologies have thus become crucial for cell and developmental biology. The automated segmentation and tracking of these complex datasets is, however, necessary for their interpretation and quantification (<xref ref-type="bibr" rid="bib24">McDole et al., 2018</xref>).</p><p>Recent progress in the 3D segmentation of animal and plant cells (<xref ref-type="bibr" rid="bib29">Pachitariu and Stringer, 2022</xref>), or organelles (<xref ref-type="bibr" rid="bib10">Cutler et al., 2022</xref>) has made this process highly efficient, despite the persistence of algorithm-dependent systematic errors (<xref ref-type="bibr" rid="bib21">Liu et al., 2023</xref>). The processing of time-lapse datasets, however, remains challenging. The accumulation over hundreds of time points of even a few residual segmentation errors can strongly affect data interpretation and notably disrupt long-term cell tracking. Following the excitement generated by this new generation of imaging systems, which offers the ability to image living systems at unprecedented spatial and temporal scales, a glass ceiling has now been reached due to the quality of these reconstructed data, which fails to scale to real research exploitations.</p><p>Beyond the reconstruction requirements of individual research projects, proper metrics are essential for evaluating the performance of bioimage analysis algorithms (<xref ref-type="bibr" rid="bib28">Nature Methods, 2024</xref>). However, the training and benchmarking of next-generation AI-based segmentation tools fundamentally depend on the availability of accurate ground truth data (<xref ref-type="bibr" rid="bib23">Maška et al., 2023</xref>). The critical bottleneck is the lack of a massive amount of 3D high-quality reconstructed data to train these systems.</p><p>We have decided to take on this challenge and develop a tool to produce high-precision and high-quality 3D datasets for AI. Two major barriers must be overcome to reach this milestone. The first concerns the <bold>evaluation</bold> of dataset reconstructions: how can we measure the accuracy of segmentations obtained by automated algorithms for these complex 3D and 3D+t datasets? The second focuses on the <bold>curation</bold> of reconstructed data: how can we correct and significantly enhance segmentations to achieve reliable reconstructions that are essential for major scientific breakthroughs?</p><p>2D imaging has long been a cornerstone of biological research, driving significant scientific discoveries with visual validation being largely adequate, as it could be performed directly on a 2D screen. The traditional method involves visually comparing the reconstruction by overlaying it with the acquired data, as seen in classic software like ImageJ (<xref ref-type="bibr" rid="bib34">Schneider et al., 2012</xref>), Napari (<xref ref-type="bibr" rid="bib36">Sofroniew, 2022</xref>), or Ilastik (<xref ref-type="bibr" rid="bib4">Berg et al., 2019</xref>). When discrepancies arise, experts can perform curation and manually adjust the pixel values to align with the desired class (e.g. a cell).</p><p>In the context of 3D and 3D+t data, the technique encounters significant obstacles that hinder reliable scientific analysis. The 3D structure of images, inherently unsuited to the 2D digital environment, demands numerous compromises that restrict their interpretability. Projecting 3D objects onto a 2D screen causes data masking, requiring experts to perform extensive manipulations to verify accuracy. Additionally, the high number and density of 3D objects further complicate the process, as visual interference from overlapping objects increases during these adjustments. As a result, voxel-level (3D pixel) curation becomes almost unfeasible using conventional methods, compelling experts to revert to working in 2D. This approach becomes very time-consuming and, therefore, does not allow to obtain a satisfactory reconstruction quality for scientific use. A final obstacle is caused by the much larger size of 3D datasets, which significantly lengthens processing time for each curation action. This computational burden becomes daunting for 3D+t time-lapse datasets, as error propagation between consecutive time points rapidly increases the number of segmentation and object tracking errors needing correction. Restricting the application of advanced image processing tasks to subsets of the image may alleviate this issue.</p><p>An unsupervised objective assessment can be derived from a priori knowledge of expected spatial (e.g. smoothness of contours, shape regularity, volume…) or temporal (e.g. stability and smooth evolution, lifetime of objects…) features of the objects (<xref ref-type="bibr" rid="bib9">Correia and Pereira, 2002</xref>). Features of the object boundaries (e.g. contrast between inside and outside of an object) can also be computed (<xref ref-type="bibr" rid="bib9">Correia and Pereira, 2002</xref>). More sophisticated strategies have been proposed when no statistically relevant a priori knowledge can be drawn (<xref ref-type="bibr" rid="bib40">Valindria et al., 2017</xref>). Since curation is typically carried out by expert biologists with limited programming skills, the computation of image features and the projection of relevant metrics onto individual segmented objects should be accessible through user-friendly interfaces. An alternative approach for interacting with 3D segmented images is, therefore, necessary.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We previously developed a novel concept of morphogenetic web browser, MorphoNet (<xref ref-type="bibr" rid="bib20">Leggio et al., 2019</xref>), to <bold>visualize</bold> and <bold>interact</bold> with 3D+t segmented datasets through their meshed representations. This user-friendly web solution required no installation and was suited for datasets of moderate size (up to a thousand cells over a few hundred time points). Since its release, this platform has been used in a variety of morphological studies in plant and animal systems (<xref ref-type="bibr" rid="bib22">Manni et al., 2022</xref>; <xref ref-type="bibr" rid="bib8">Chung et al., 2023</xref>) and has been instrumental to interpret the relationship between gene expression and complex evolving cell shapes (<xref ref-type="bibr" rid="bib33">Refahi et al., 2021</xref>; <xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>; <xref ref-type="bibr" rid="bib11">Dardaillon et al., 2020</xref>). This web version was restricted to visualizing and interacting with segmented datasets through their precomputed 3D dual meshes. This approach offered significant benefits in terms of online 3D rendering efficiency and data management but it also came with limitations. For example, meshes are simplified representations of the segmented object that may not capture fine details of the original volumetric data and that cannot be validated without comparison to the original raw intensity images. Meshed representations can also make it challenging to perform the precise computations required for detailed geometric and topological analyses (<xref ref-type="bibr" rid="bib37">Sophie et al., 2024</xref>). Exploration of larger datasets was nonetheless constrained by the computational resource limitations of internet browsers.</p><p>We present here MorphoNet 2.0, a major conceptual evolution of the platform, designed to offer powerful tools for the <bold>reconstruction</bold>, <bold>evaluation</bold>, and <bold>curation</bold> of 3D and 3D+t datasets. This innovative approach leverages the richness and redundancy of information embedded within these complex datasets. It addresses the two previously identified challenges: <bold>evaluating</bold> 3D segmented data without relying on manual ground truth by fully harnessing the data’s richness, and enabling semi-automated <bold>curation</bold> of 3D data, now achievable with just a few clicks. This enables us to achieve data quality at a level suitable for scientific discovery. We showcase the impact of these advancements by revisiting and enhancing five published animal and plant datasets previously regarded as ground truth (<xref ref-type="bibr" rid="bib23">Maška et al., 2023</xref>).</p><p>We feature a new standalone application running on all major operating systems that exploits the resources of the user’s local machine to explore, segment, assess, and manually curate very large 3D and 3D+t datasets. Like the web version, the MorphoNet application uses the power and versatility of the Unity game engine and includes its main features (<xref ref-type="bibr" rid="bib20">Leggio et al., 2019</xref>). By overcoming web-based limitations, the application can handle more complex datasets, including heavier segmented voxel images and up to several tens of thousands of objects on a research laptop (<xref ref-type="fig" rid="fig1">Figure 1h</xref>, and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). The standalone application allows users to directly explore private data stored on their own computer, without need for an upload to the MorphoNet server. Subsequent data sharing with other researchers or a wider public in an open science process is facilitated by the MorphoNet server upload functions of the standalone.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Illustration of the visualization of datasets of various complexity and nature in the MorphoNet standalone application.</title><p>(<bold>a–f</bold>) Visualization of a 64 cell stage Phallusia mammillata embryo with labeled cell nuclei and cell membranes. (<bold>a-c</bold>) Intensity images showing the nuclei (<bold>a</bold>), the membranes (<bold>b</bold>) or both (<bold>c</bold>). (<bold>d</bold>) Same as <bold>c</bold> with additional nuclei segmentation obtained with the Binarize plugin (see Materials and methods for the full description of curation plugins). (<bold>e</bold>) Same as <bold>b</bold> with additional membrane segmentation obtained with the Cellpose 3D plugin. (<bold>f</bold>) Same as <bold>c</bold> with a combination of several rendering possibilities of cell and nuclei segmentations. (<bold>g</bold>) Multi-colored shaders allow the simultaneous visualization of the expression patterns of multiple genes extracted from the ANISEED (<xref ref-type="bibr" rid="bib11">Dardaillon et al., 2020</xref>) database and of tissue fate information. Ascidian embryo (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>) at stage 15 (mid neurula); cells with a single color are colored with larval tissue fate; multi-colored cells are colored with both larval tissue fate and the expression of selected genes. (<bold>h</bold>) Visualization of a 6 days post-fertilization <italic>Platynereis dumerilii</italic> embryo (<xref ref-type="bibr" rid="bib42">Vergara et al., 2021</xref>) imaged by whole-body serial block face scanning electron microscopy followed by the automated whole-cell segmentation of 16,000 cells. (<bold>i-k</bold>) Visualization of a cell cycle 14 <italic>Drosophila melanogaster</italic> embryo imaged with SiMView microscopy and segmented with RACE (<xref ref-type="bibr" rid="bib38">Stegmaier et al., 2016</xref>). (<bold>i</bold>) Projection on each segmented cell of the mean image intensity. (<bold>j</bold>) Projection on each segmented cell of the ratio between the length of the major and the minor axes of the ellipse that has the same normalized second central moments as the segmented cell. (<bold>k</bold>) Projection of the cell volume.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig1-v1.tif"/></fig><p>MorphoNet is designed with a strong emphasis on user-friendliness, making its use highly intuitive for experimental biologists with no coding experience (See Videos in Materials and Methods). Every feature and interface element has been crafted to ensure a smooth, straightforward user experience, allowing both beginners and experts to utilize its capabilities efficiently. Additionally, the solution is fully open-source allowing bio-image analysis to develop their own features (See <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>One of the key features of this new standalone application is the automatic integration of a duality between the 3D segmented images and their corresponding meshes (<xref ref-type="fig" rid="fig2">Figure 2</xref>). This feature is achieved by linking Python, dedicated to high-performance image processing, with the Unity Game Engine, fully optimized for seamless interaction with the dual meshes. This integration enables access to state-of-the-art reconstruction (<xref ref-type="bibr" rid="bib29">Pachitariu and Stringer, 2022</xref>; <xref ref-type="bibr" rid="bib43">Weigert et al., 2020</xref>) tools, including those leveraging AI libraries, while also addressing the challenges of interacting with 3D images by harnessing the powerful interactive capabilities of game engines.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>MorphoNet Standalone Schema.</title><p>From the local data (loaded from the green box), the MorphoNet Standalone application first computes the dual meshes for each of the segmented objects (in the module python in the yellow box). Then, using the 3D viewer (in the blue box), users identify detection, segmentation, or tracking issues using, if necessary, the cell lineage information, the raw images, and/or properties computed on the segmented objects. Errors are then corrected by choosing and executing the appropriate image processing plugin from the curation menu. Finally, new meshes are computed from the result of the plugin execution to update the visualization.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig2-v1.tif"/></fig><p>To ensure users can consistently assess segmentation quality, raw intensity images are also available by superimposition. This provides access to the entire workflow, from raw image acquisition to meshed segmentation, allowing simultaneous exploration and visualization of both meshed and voxel images (<xref ref-type="fig" rid="fig1">Figure 1a–f</xref>).</p><p>We also leveraged Unity’s scene management tools to implement simultaneous visualization of multiple scenes. This feature enables the display of interactive cell lineages in a dedicated, fully connected window, offering essential insights into cellular trajectories within 3D+t datasets.</p><sec id="s2-1"><title>Automatic error detection</title><p>Efficient dataset curation requires the rapid identification of segmentation errors in large 3D or time-lapse datasets containing thousands of segmented objects. MorphoNet 2.0 tackles this critical challenge by introducing unsupervised metrics for objective, automated assessment, eliminating the subjectivity, inefficiency, and inconsistency inherent in manual visual inspections. These metrics leverage prior knowledge of generic data properties, such as shape regularity, contour smoothness, and temporal stability of shapes and volumes - to deliver scalable and consistent analysis. Beyond improving curation efficiency, these metrics offer quantitative scores that enable systematic comparisons across datasets, experiments, and tools. Integrated into MorphoNet, they support benchmarking, algorithm refinement, and reproducibility. Using the scikit-image library (<xref ref-type="bibr" rid="bib41">van der Walt et al., 2014</xref>), we automatically compute object properties from both segmented and intensity images. These properties quickly help identify outliers, which often correspond to segmentation errors. While the traditional approach uses some metrics to evaluate global distributions, it is crucial for curation purposes to apply these properties at the level of each individual segmented object. Thus, these properties, including shape metrics like volume, convexity, and elongation, and intensity metrics, such as mean voxel intensity within or around segmented objects, can be easily projected onto meshed objects for visualization (<xref ref-type="fig" rid="fig1">Figure 1i–k</xref>).</p><p>Segmentation quality assessment is further enhanced by calculating three categories of properties for each dataset: (1) Morphological features – including volume, convexity, roughness, and elongation, (2) Intensity-based measurements – within and around each object in the original acquisition images, such as mean intensity, homogeneity, or deviation at segmentation borders; (3) Temporal features, such as object lifetime or cell lineage distances (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>).</p><p>By projecting these values onto segmented object meshes, outliers are readily identified as prime candidates for curation. These values can also be exported and plotted to reveal distributions, providing an unsupervised assessment of overall segmentation quality. This process is exemplified in various use cases described below.</p><p>To evaluate the relevance of these unsupervised metrics, we compared their distributions in the published and curated datasets against a manually annotated gold standard in Use Case 1. The curated segmentations exhibited higher Intersection over Union (IoU) scores (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Although these unsupervised metrics are not flawless predictors of segmentation accuracy, they show a strong correlation with standard quality indicators, such as IoU. This confirms their practical utility as reliable proxies for segmentation quality and valuable tools to guide efficient manual curation.</p><p>A key limitation of relying solely on unsupervised metrics is the risk of circular logic: corrections or model training based on masks deemed ‘good’ by these metrics may promote homogeneity without improving biological accuracy. This can lead to segmentations optimized for metric conformity rather than real quality. To avoid this, MorphoNet’s metrics are intended as guidance tools for flagging candidate errors, which should then be validated through expert curation, ground truth comparisons, or biological plausibility.</p></sec><sec id="s2-2"><title>Biocuration</title><p>The execution time of algorithms in 3D image processing limits the optimization of their parameters. Additionally, the inherent heterogeneity within 3D images frequently prevents achieving consistently high-quality results across the entire dataset. To tackle these challenges, we defined in Morphonet the notion of multi-type representation layers, able to combine and interoperate 3D image intensity layers with mesh-based layers. This dual representation enables seamless interaction with meshes, facilitating the rapid identification and precise selection of objects requiring curation. By focusing processing efforts on these selected regions rather than the entire image, MorphoNet significantly reduces the time and effort required, streamlining the curation workflow for complex 3D+t datasets.</p><p>The meshed versions of the segmentations are used for 3D rendering, for the identification and selection of objects needing curation, and to launch the needed curation algorithms, which are performed on the segmented images (<xref ref-type="fig" rid="fig2">Figure 2</xref>). A significant acceleration of operations on 3D images is achieved by targeting the processing activities on a subpart of the image, for instance, on the bounding box of an object or of a set of objects. This approach enables results to be generated in just seconds for most tasks, even with highly complex 3D data. Given the heterogeneity within 3D images, it is challenging to find a single set of parameters that works effectively across the entire image. This approach makes it easy to run image processing algorithms with different parameters in different subregions of the image.</p><p>Editions of objects necessitating curation can then proceed using dedicated plugins. The curation module of MorphoNet has an expandable open-source Python plugin architecture and provides user-friendly graphical interfaces accessible to experimental biologists with limited programming skills. To support advanced image processing techniques powered by deep learning, which have an increasing impact on image analysis, the default installation package includes training libraries like Scikit-learn, PyTorch, and TensorFlow. Plugins can take the raw intensity image into account to perform 3D image processing tasks, as exemplified by the integration of popular segmentation tools, such as Stardist (<xref ref-type="bibr" rid="bib43">Weigert et al., 2020</xref>) and Cellpose (<xref ref-type="bibr" rid="bib29">Pachitariu and Stringer, 2022</xref>). The full list of plugins can be found in the Materials and Methods. Users and developers can expand the current plugin list by creating their own through an easy-to-use Python template (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>To help the user identify suitable plug-ins, we grouped them into seven categories. The first five are used to create or edit segmentations of 3D datasets, the last two are dedicated to the temporal propagation of corrections in 3D+t datasets. All categories will be exemplified in the five use cases below. Figure 8 in Materials and methods illustrates how the majority of segmentation and tracking errors can be corrected using plugins, single-handedly or in combination. For example, an under-segmentation can be corrected by running locally the <italic>Cellpose</italic> plugin as in use case 2 or the Temporal Propagation plugin (<italic>Prope</italic>), as in use case 5.</p><list list-type="order" id="list1"><list-item><p><italic>De novo Segmentation</italic> plugins<italic>:</italic> these plugins create a de novo segmentation from the original intensity images (or from a specified region of interest of the intensity images). This category includes segmentation tools, such as Cellpose 3D or Stardist, as well as simpler intensity thresholding. As described below, MorphoNet plugins allow to specifically target the application of these tools to error-rich regions.</p></list-item><list-item><p><italic>De novo Seed</italic> plugins: these plugins automatically create seeds (i.e. 3D points in the 3D images), which can then be used to initiate segmentation tasks. Additionally, seeds can also be added manually using the interface.</p></list-item><list-item><p><italic>Segmentation from Seeds</italic> plugins: these plugins are used to perform local segmentation using predefined seeds, for example, by using a watershed algorithm.</p></list-item><list-item><p><italic>Segmentation correction</italic> plugins<italic>:</italic> these plugins are used to correct the main classes of segmentation errors on selected segmented objects. This includes the fusion of over-segmented cells, the resolution of small artifactual cells, or the splitting of under-segmented cells.</p></list-item><list-item><p><italic>Shape transform</italic> plugins: these plugins are used to edit the shape of selected segmented objects. This includes classical morphological operators (such as dilate, erode, etc.) or the manual deformation of a cell’s shape.</p></list-item><list-item><p><italic>Propagate Segmentation</italic> plugins<italic>:</italic> these plugins are used to propagate an accurate segmentation at a specific time point to erroneous segmentations at later or earlier time points. Examples: Propagate eroded cell masks to correct under-segmentation errors.</p></list-item><list-item><p><italic>Edit Temporal Links</italic> plugins<italic>:</italic> these plugins are used to curate cell lineage trees by creating or editing temporal information. Examples: Create temporal links using the spatial overlap between homologous objects at successive time points.</p></list-item></list></sec><sec id="s2-3"><title>Use cases</title><p>To highlight the efficiency of MorphoNet plugins in detecting and correcting the main types of errors across a broad range of organisms, we present five examples from previously published fluorescent live imaging datasets, representing various levels of quality and complexity. These examples collectively showcase MorphoNet’s powerful segmentation quality assessment and error detection capabilities in different contexts, ranging from low-quality nuclear segmentations to high-quality whole-cell segmentations. While these examples frequently involve the use of Cellpose, the current state-of-the-art tool for 3D dataset segmentation, the focus is not on emphasizing the tool’s power but on demonstrating how advanced tools can be enhanced through integration into MorphoNet’s sophisticated graphical plugin interfaces.</p><p>The first use-case introduces two unsupervised metrics for assessing dataset quality in the absence of ground truth on a time-lapse light-sheet microscopy movie of a developing <italic>Tribolium castaneum</italic> embryo. Using these metrics, we demonstrate the power of an iterative segmentation and training strategy. By applying a Cellpose model, trained on a curated subpopulation, to the entire dataset, we improve segmentation quality.</p><p>The second use-case presents unsupervised whole-cell morphological metrics to assess the quality of several independent segmentations of a time-lapse confocal movie of a developing starfish embryo, up to the 512 cell stage, with fluorescently labeled cell plasma membranes. Originally, a bio-image analyst segmented the movie using a complex workflow. We demonstrate that running an imported, custom-trained Cellpose 3D model through MorphoNet’s graphical interfaces greatly increases the number of cells that can be successfully segmented.</p><p>The third use-case focuses on detecting and resolving regions of under-segmented cells in a confocal 3D dataset of <italic>Arabidopsis thaliana</italic> plant apical shoot meristem, with fluorescent plasma membrane and whole-cell segmentation of 1800 cells. It demonstrates the identification of a large region with heavily under-segmented cells, which is successfully addressed by targeting Cellpose to the specific region of interest.</p><p>The fourth use-case addresses the correction of low-quality automated cell nuclei segmentation in a time-lapse confocal <italic>Caenorhabditis elegans</italic> embryo with poor-quality nuclei labeling up to the 350 cell stage, despite accurate manual tracking. It demonstrates how object edition plugins can be used to correct individual nuclei segmentations and propagate these corrections over time.</p><p>The final use-case illustrates the efficient detection and correction in the cell lineage of rare residual errors that escaped previous scrutiny in a published time-lapse multiview light-sheet imaging dataset of a developing <italic>Phallusia mammillata</italic> embryo. The dataset features labeled cell membranes and high-quality whole-cell segmentation and tracking. This polishing work is crucial for generating ground truths for studies of natural variation.</p><p>All the evaluation and curation steps for the use cases are detailed in Materials and methods.</p><sec id="s2-3-1"><title>Use-case 1: Assessing and improving segmentation quality of <italic>Tribolium castaneum</italic> embryos using unsupervised nuclei intensity metrics and iterative training on partial annotation</title><p>This use-case introduces two unsupervised metrics for assessing dataset quality without ground truth, demonstrating their application in evaluating segmentation performance. It also highlights the effectiveness of an iterative segmentation and training approach, where applying a Cellpose model trained on a curated subset significantly improves overall segmentation quality.</p><sec id="s2-3-1-1"><title>Dataset description</title><p>This dataset features a 59-time-step light-sheet microscopy acquisition of labeled cell nuclei from a developing <italic>Tribolium castaneum</italic> embryo, containing over 7000 cells per time step. It includes two types of ground truth: a <italic>gold truth</italic> (GT), which is expert manual tracking of 181 nuclei over all time steps, and a <italic>silver truth</italic> (ST), generated through a complex pipeline combining the outputs of up to 16 top-performing algorithms selected from 39 submissions. The ST provides automated segmentation masks using a modified label fusion approach for improved accuracy in dense datasets. The GT and ST are independent and do not share common elements.</p></sec><sec id="s2-3-1-2"><title>Type of errors</title><p>This dataset serves as a benchmark in the fifth Cell Tracking Challenge (<xref ref-type="bibr" rid="bib23">Maška et al., 2023</xref>) to evaluate and compare AI-based image segmentation methods and is, therefore, regarded as a reliable ground truth. The manual tracking GT is of high quality (expert annotation), but the automated ST is of lesser quality. To evaluate the segmentation quality of the dataset at first time point, the intensity image and the two ground truths (ST and GT) were simultaneously displayed in separate MorphoNet channels (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Systematic visual inspection identified 12/181 GT points without corresponding nucleus in the ST data; eight misplaced segmentations where the GT point is not inside the segmentation; 56 over segmentations and many suspicious nuclear shapes.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Unsupervised quality assessment and curation of the <italic>Tribolium castaneum</italic> embryo.</title><p>(<bold>a</bold>) The five steps of the curation pipeline. (<bold>b</bold>) View of the original intensity images (in green) of the first time step of the published data (<xref ref-type="bibr" rid="bib23">Maška et al., 2023</xref>). Both Ground Truth channel (GT, red point) and Silver Truth (ST, white nucleus segmentation) are shown at the top. Blue segmentation corresponds to the match between ST and GT. Bottom, zoom at the GT region (ROI) without the intensity images. (<bold>c</bold>) Projection for the ST of the distance between the gravity center of the intensity inside the segmentation and the centroid of the segmentation. Color bar at the bottom of <bold>d</bold>. (<bold>d</bold>) Same as <bold>c</bold> for the curated pipeline. (<bold>e</bold>) Projection for the ST of the deviation of the intensity at the border of the segmentation. Color bar at the bottom of <bold>f</bold>. (<bold>f</bold>) Same as <bold>e</bold> for the curated pipeline. (<bold>g</bold>) Comparative histogram of the intensity_offset property distribution between the ST, the Step 1 and the Step 5 for the 181 curated nuclei (left) and the whole image (right). (<bold>h</bold>) Same as <bold>g</bold> for the distribution of the intensity_border_variation property. (<bold>i</bold>) Same as <bold>g</bold> for the distribution of the nuclei volume property.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Illustration of properties <italic>intensity_border_variation</italic> and <italic>intensity_offset for 2 nuclei</italic>.</title><p>2D slice of the intensity image of 2 distinct nuclei superimposed with their corresponding segmentation border (in yellow). The blue cross is the corresponding gravity center of the intensity images inside the segmentation. The blue cross corresponds to the geometrical center of the segmentation shape. The intensity offset is the Euclidean distance between the geometric center of the segmented object and the center of mass of the signal intensity. The intensity_border_variation is the standard deviation of the intensity images only at the border of the segmentation (in yellow). The left nucleus, representing a well-segmented example, exhibits low values for both properties (intensity_offset = 0.18 and intensity_border_variation = 1.84). The right nucleus, where the segmentation is misaligned, shows high values for both properties (intensity_offset = 2.06 and intensity_border_variation = 10.73).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Analysis of two automatically computed intensity properties.</title><p>(<bold>a</bold>) Published data, Silver Truth (ST) segmentation corresponding to the ROI of the Gold truth (GT). Color code: green: exact matching between a segmentation in the ST and the GT; red: identified over-segmentation in the ST close to the GT; blue: missing cell of the GT with no correspondence in the ST. (<bold>b</bold>) Projection of the property intensity_border_variation onto the nuclei segmentation selected in <bold>a</bold>. (<bold>c</bold>) Projection of the property intensity_offset onto the same nuclei segmentation. (<bold>d</bold>) Double projection of both property intensity_offset and intensity_border_variation. Colormap same as in <bold>b</bold> and <bold>c</bold>. (<bold>e</bold>) 2D plot of the values of both properties (intensity_offset in X-axis and intensity_border_variation in Y-axis) of the same selected nuclei segmentation. Color code is equivalent as in <bold>a</bold>. The blue line in each axis represents the last quartile of the distribution. (<bold>f</bold>) Double projection of both properties (same as in <bold>d</bold>) on the whole ST embryo. (<bold>g</bold>) Same as in <bold>f</bold> for the curated data obtained using the five steps pipeline (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). (<bold>h</bold>) 2D plot of 40 ground truth cells and their corresponding segmentations in both the published and curated datasets, showing intensity_offset property on the X-axis and Intersection over Union (IoU) on the Y-axis. (<bold>i</bold>) Same as in <bold>h</bold> with the intensity_border_variation property on the X-axis. (<bold>j</bold>) Comparison of IoU values for 40 ground truth cells and their corresponding segmentations in the published and curated datasets. Points above the red line correspond to cells with improved IoU values in the curated dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Distribution of three properties for the <italic>Tribolium</italic> reconstruction.</title><p>Comparative histogram of a specific property distribution between the silver truth (ST), the Step 1, and the Step 5 for the 181 curated nuclei (left) and the whole image (right). (<bold>a</bold>) Comparative histogram of the distribution of the intensity offset property. (<bold>b</bold>) Comparative histogram of the distribution of the intensity border deviation property. (<bold>c</bold>) Comparative histogram of the nuclei volume distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig3-figsupp3-v1.tif"/></fig></fig-group></sec><sec id="s2-3-1-3"><title>Error identification</title><p>Using the <italic>Match</italic> plugin, each GT position was automatically associated with a corresponding nucleus in the ST, leaving 11 out of 181 positions unassigned (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), indicating approximately 6% of missed nuclei in the ST. To identify inaccurately shaped segmentations, we used two signal intensity-based MorphoNet metrics. The first metric measures the distance between the geometric center of a segmented object and the center of mass of the signal intensity (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and Materials and methods for the full properties description), with 32 of the 56 over-segmented nuclei falling into the upper quartile of this metric’s distribution (<xref ref-type="fig" rid="fig3">Figure 3c and d</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The second metric evaluates deviations in the signal intensity along the segmentation boundary, with 29 out of 56 over-segmented nuclei identified in the upper quartile (<xref ref-type="fig" rid="fig3">Figure 3c–f</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). These errors were visualized in 3D by projecting both metrics onto each nucleus, enabling spatial identification of potential segmentation issues (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p></sec><sec id="s2-3-1-4"><title>Error correction</title><p>Due to the high number of errors in the published ST, a de novo nucleus segmentation pipeline was created in MorphoNet with minimal manual intervention. The five-step process (as detailed in Materials and methods UC1 Curated pipeline, <xref ref-type="video" rid="video1">Video 1</xref>) included initial segmentation using the standard Cellpose nuclei model, manual curation of 181 GT nuclei, iterative training of a custom Cellpose model, and refinement of segmentation using geometric properties. Key steps involved correcting segmentation errors with various plugins, extending the Cellpose nuclei model using curated data, and refining non-convex shapes. The pipeline significantly improved segmentation quality compared to the published ST and standard Cellpose model (<xref ref-type="fig" rid="fig3">Figure 3i</xref>). Analysis of the curated nuclei revealed a more heterogeneous volume distribution, fewer over-segmentations, and better alignment of intensity-based metrics (<xref ref-type="fig" rid="fig3">Figure 3g and h</xref>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>), showing substantial improvement over the original segmentation pipeline. We also created a new set of manually segmented ground truth cells (see Materials and methods UC1 ground truth protocol), distributed across the ranges of both <italic>intensity_offset</italic> and <italic>intensity_border_variation</italic>. Using this reference, we evaluated the published and curated segmentations (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2h–j</xref>), confirming that our pipeline both reduces segmentation errors and better matches expert annotations. Moreover, the analysis revealed a clear correlation between the unsupervised metrics and segmentation accuracy. These results validate the utility of our metrics and demonstrate that the iterative pipeline yields higher-quality segmentations than the published dataset.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106227-video1.mp4" id="video1"><label>Video 1.</label><caption><title><italic>Tribolium castaneum</italic>.</title><p>The movie shows how to train a Cellpose model using a curated sub-part of a dataset, and how to fine-tune models with successive training on specific nuclei using image properties.</p></caption></media></sec></sec></sec><sec id="s2-4"><title>Use case 2: Evaluating membrane segmentation quality using <italic>smoothness</italic> metrics and simplifying a segmentation workflow for <italic>Patiria miniata</italic> starfish embryo</title><p>This use case thus introduces new unsupervised morphological metrics (the <italic>smoothness</italic> property) to assess segmentation quality and demonstrates that using a custom-trained 3D Cellpose model via graphical interfaces allows extending segmentation to areas of the dataset with lower intensity/noise ratio. Visualization of metrics, such as smoothness and cell volumes simplify the identification of cells that need further manual curation.</p><sec id="s2-4-1"><title>Dataset description</title><p>This dataset comprises 300 3D stacks from time-lapse, two-channel confocal imaging of twelve <italic>Patiria miniata</italic> wild-type and compressed embryos, captured between the 128- and 512 cell stages (<xref ref-type="bibr" rid="bib3">Barone et al., 2024</xref>). Fluorescent labeling highlights cell membranes and nuclei, though imaging was limited to about half of each embryo due to poor penetration and the use of low magnification air objectives. The published whole-cell segmentation (see Materials and methods UC2 Original workflow) relied on a complex, multi-step workflow adapted from the CartoCel (<xref ref-type="bibr" rid="bib2">Andrés-San Román et al., 2023</xref>) requiring advanced bioanalysis expertise. For this use case, a particularly challenging dataset - a compressed wild-type embryo at the 512 cell stage - was selected for evaluation and curation.</p></sec><sec id="s2-4-2"><title>MorphoNet segmentation with the advanced cellpose plugin</title><p>We explored cellpose’s potential to improve segmentation quality in challenging regions of a 3D dataset. Initial segmentation using the Cyto2 model identified 1187 cells, with a bimodal size distribution and 939 cells smaller than 1000 voxels (<xref ref-type="fig" rid="fig4">Figure 4i</xref>). The Deli plugin, which eliminates these small cells (<xref ref-type="fig" rid="fig4">Figure 4j</xref>), brought the number of cells down to 211 (<xref ref-type="fig" rid="fig4">Figure 4j</xref>); however, analysis with the <italic>smoothness</italic> property (<xref ref-type="fig" rid="fig4">Figure 4g</xref>) revealed rough cell surfaces. To address this, we further extend the training of the <italic>Cyto2</italic> model on the published 3D database (<xref ref-type="bibr" rid="bib3">Barone et al., 2024</xref>) (see Materials and methods UC2 Training Protocol workflow, <xref ref-type="video" rid="video2">Video 2</xref>). The retrained model produced 284 cells with, again, a bimodal size distribution (<xref ref-type="fig" rid="fig4">Figure 4i</xref>) which was corrected with the <italic>Deli</italic> plugin. This significantly simpler pipeline produced smooth cell segmentations (<xref ref-type="fig" rid="fig4">Figure 4h</xref>), with a size distribution comparable to the published ground truth and a high Intersection over Union (IoU) score (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). While 2 cells were missing, and 11 under-segmented compared to the published data (<xref ref-type="fig" rid="fig4">Figure 4f</xref>), this new approach recovered 48 additional cells (<xref ref-type="fig" rid="fig4">Figure 4e</xref>) while simplifying the segmentation process compared to the original pipeline.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Starfish whole-cell segmentation using Cellpose.</title><p>(<bold>a</bold>) Animal and Lateral view of the maximum intensity projection from the published dataset (<xref ref-type="bibr" rid="bib3">Barone et al., 2024</xref>). (<bold>b</bold>) Segmented ground truth image. (<bold>c</bold>) Result of the segmentation using Cellpose Cyto2 model followed by the removal of small cells (&lt;1000 voxels) with the Deli plugin (<bold>d</bold>) Result of a Cellpose segmentation with a model trained on <italic>P. miniata</italic> dataset followed by the Deli plugin. (<bold>e</bold>) New cells created (different between <bold>b</bold> and<bold> d</bold>) (<bold>f</bold>) Under segmentation and missing cells generated by <bold>d</bold> compared to <bold>b</bold>. (<bold>g</bold>) Vegetal view of <bold>c</bold> with smoothness representation. (<bold>h</bold>) Opposite view of <bold>d</bold> with smoothness representation. (<bold>i</bold>) Cell size distribution in the published segmentation (<bold>b</bold>), Cellpose cyto2 model (<bold>c</bold>), and Cellpose with <italic>P. miniata</italic> model (<bold>d</bold>). (<bold>j</bold>) Identical as <bold>i</bold> after application of the Deli plugin. Colors in <bold>b</bold>-<bold>f</bold> represent cell volume in µm<sup>3</sup>. Colors in <bold>g</bold>, <bold>h</bold> represent Smoothness.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Evaluation of Segmentation in <italic>Patiria miniata</italic> starfish embryo.</title><p>(<bold>a</bold>) Histogram of Intersection over Union (IoU) values comparing the published dataset (gray) with the Cellpose cyto2+Deli pipeline (orange) and the Cellpose <italic>P. miniata</italic> model + Deli pipeline (green).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig4-figsupp1-v1.tif"/></fig></fig-group><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106227-video2.mp4" id="video2"><label>Video 2.</label><caption><title><italic>Patiria miniata</italic>.</title><p>The movie shows how to create a dataset, how to use an already existing custom Cellpose model and how to easily correct usual Cellpose segmentation errors.</p></caption></media></sec></sec><sec id="s2-5"><title>Use case 3: Interactive targeted segmentation using cellpose for resolving under-segmentation in <italic>Arabidopsis thaliana</italic> shoot apical meristem whole-cell segmentation</title><p>This use case demonstrates the power of interactive selection with targeted segmentation methods that can resolve under-segmentation issues in complex datasets.</p><sec id="s2-5-1"><title>Dataset description</title><p>This dataset (<xref ref-type="bibr" rid="bib44">Willis et al., 2016</xref>) consists of a 19-time steps time-lapse confocal acquisition of a live <italic>Arabidopsis thaliana</italic> shoot apical meristem with fluorescently-labeled cell membranes. Cell numbers ranged from 600 cells at the first time point to around 1800 cells at the last (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). We use the last time step (19) with the higher number of cells to illustrate the curation procedure.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Curation of a segmented shoot apical meristem of <italic>Arabidopsis thaliana</italic>.</title><p>Visualization of the time step n°19 for several types of curations using MorphoNet. (<bold>a</bold>) 3D view of an <italic>Arabidopsis thaliana</italic> shoot apical meristem (<xref ref-type="bibr" rid="bib44">Willis et al., 2016</xref>). (<bold>b</bold>) Published 3D intensity images. (<bold>c</bold>) Published 3D segmentation of an <italic>Arabidopsis thaliana</italic> shoot apical meristem (<xref ref-type="bibr" rid="bib44">Willis et al., 2016</xref>) obtained using MARS-ALT (<xref ref-type="bibr" rid="bib12">Fernandez et al., 2010</xref>). (<bold>d</bold>) Comparative histogram based on the cell volume between the published segmentation (<bold>c</bold>), the result of the cyto2 prediction (<bold>e</bold>), and the final curated version (<bold>i</bold>) + Deli plugin for cells &lt;1000 voxels. X and Y axes are in log scale. (<bold>e</bold>) Result of the Cellpose (<xref ref-type="bibr" rid="bib39">Stringer et al., 2021</xref>) 3D MorphoNet plugin using the pretrained cyto2 model. (<bold>f</bold>) Result using the Cellpose 3D MorphoNet plugin using the model trained over the first 10 time steps with the Cellpose training plugin with the XY planes. (<bold>g</bold>) Result of the Cellpose 3D MorphoNet plugin using the model trained over the first 10 time steps with the Cellpose training plugin with each plane of the 3D images. (<bold>h</bold>) Selected masks larger than 300 µm<sup>3</sup> in the published dataset (<bold>c</bold>). (<bold>i</bold>) Result of the Cellpose 3D MorphoNet on the selected masks (<bold>h</bold>) using the model trained over the ten first time steps with the Cellpose training plugin with each plane of the 3D images.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Comparison of the Cellpose models for the reconstruction shoot apical meristem of <italic>Arabidopsis thaliana</italic>.</title><p>(<bold>a</bold>) Result of the Cellpose prediction on the full image using the model cyto. Color bar on the right represents the cell volume in µm<sup>3</sup>. (<bold>b</bold>) Same as <bold>a</bold> with cyto2 Cellpose model. (<bold>c</bold>) Same as <bold>a </bold>with cyto3 Cellpose model. (<bold>d</bold>) Result of the Cellpose prediction using the model cyto3 with the option disconnect non-connected component activated. (<bold>e</bold>) Identical as <bold>d </bold>with the visualization for cells with a volume &lt;10 µm<sup>3</sup>. (<bold>f</bold>) Result of the Cellpose prediction using the model cyto3 with the option disconnect non-connected component activated and the option to reassign objects under a volume &lt;10 µm<sup>3</sup>. (<bold>g</bold>) Distribution of the segmented cell volume (µm<sup>3</sup>) for the time step 19. Left: comparison of the three default models of Cellpose (cyto,cyto2, and cyto3). Cellpose train 3D: the model was trained using XY, YZ, and XZ planes and the prediction is then applied on the full 3D image. Cellpose train 3D selected masks: the model was trained using XY, YZ, and XZ planes and the prediction is then applied only on the selected masks with volumes &gt;300 µm<sup>3</sup>. (<bold>h</bold>) 3D view of the masks larger than 300  µm³ in the published dataset. Same orientation as in <xref ref-type="fig" rid="fig5">Figure 5a</xref>. (<bold>i</bold>) Same view of the corresponding masks after curation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig5-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-5-2"><title>Error detection</title><p>The analysis of the published segmentation uncovered multiple under- and over-segmentation errors in the deep cell layers, caused by poor image quality in the inner regions of the meristem (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). These errors hindered accurate cell tracking over time. Observing the size homogeneity of shoot apical meristem cells (<xref ref-type="fig" rid="fig5">Figure 5d</xref>), we hypothesized that larger-than-expected cells might indicate under-segmentation errors, while smaller cells could result from over-segmentation. By projecting the automatically computed cell volumes as a color map onto the segmentations, we identified a large under-segmented region containing 20 cells in the deep meristem layer (highlighted in red in <xref ref-type="fig" rid="fig5">Figure 5c</xref>), along with a few small over-segmented cells (<xref ref-type="fig" rid="fig5">Figure 5i</xref>).</p></sec><sec id="s2-5-3"><title>Error correction</title><p>The large number of fused cells in the under-segmented region prompted us to test whether our interactive Cellpose plugin could outperform the original MARS/ALT software (<xref ref-type="bibr" rid="bib12">Fernandez et al., 2010</xref>). The standard pretrained <italic>cyto2</italic> model of Cellpose (<xref ref-type="bibr" rid="bib29">Pachitariu and Stringer, 2022</xref>) produced numerous very small over-segmented cells (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), suggesting it was not well-suited for this dataset (<xref ref-type="bibr" rid="bib16">Kar et al., 2022</xref>). To improve performance, we used the high-quality MARS/ALT segmentations from the first ten time points and employed our Cellpose train plugin to extend the <italic>cyto2</italic> model training. We tested two training modalities: ‘2D’ training, using only the XY planes, and ‘3D’ training, incorporating XY, XZ, and YZ planes. The segmentations after additional 2D training still contained many over-segmented cells (<xref ref-type="fig" rid="fig5">Figure 5f</xref>), a problem that was significantly reduced with 3D training (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). Despite this, the heterogeneity of signal intensities still caused over-segmentation in more superficial regions of the meristem. We thus targeted the Cellpose plugin only to the large under-segmented region (<xref ref-type="fig" rid="fig5">Figure 5h</xref>) and removed small over-segmented cells using the <italic>Deli</italic> segmentation correction plugin (<xref ref-type="fig" rid="fig5">Figure 5i</xref>). This targeted approach was fast and accurate (<xref ref-type="fig" rid="fig5">Figure 5d</xref>) and allowed us to preserve the accurate segmentations from the original published data while curating the identified errors. Notably, the curated result restored a unimodal and symmetric volume distribution (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1g</xref>), consistent with the expected homogeneity of the shoot apical meristem (<xref ref-type="bibr" rid="bib35">Shapiro et al., 2015</xref>), which was further confirmed by visual inspection (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1h–i</xref>). With this approach, we successfully curated 20 under-segmented regions and generated 98 new cells (<xref ref-type="video" rid="video3">Video 3</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106227-video3.mp4" id="video3"><label>Video 3.</label><caption><title><italic>Arabidopsis thaliana</italic>.</title><p>The movie shows how to train a Cellpose model on several time steps of a 3D+t dataset, and how to use it to predict under-segmentations on a specific part of a dataset.</p></caption></media></sec></sec><sec id="s2-6"><title>Use-case 4: Improving segmentation quality using object editing plugins for <italic>Caenorhabditis elegans</italic> cell lineage</title><p>This use case illustrates how object editing plugins can be employed to improve segmentation quality in challenging datasets despite poor-quality nuclei labeling.</p><sec id="s2-6-1"><title>Dataset description</title><p>This dataset (<xref ref-type="bibr" rid="bib26">Murray et al., 2008</xref>) consists of a live 3D time-lapse confocal acquisition of a developing <italic>Caenorhabditis elegans</italic> embryo with fluorescently labeled cell nuclei (<xref ref-type="fig" rid="fig6">Figure 6a</xref>), over 195 time steps between the 4- and 362 cell stages. This dataset was included in the Cell Tracking Challenge (<xref ref-type="bibr" rid="bib23">Maška et al., 2023</xref>) (see Materials and methods UC4 for the original workflow).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Curation of a <italic>Caenorhabditis elegans</italic> embryo dataset.</title><p>(<bold>a</bold>) Cell lineage viewer of all time steps colored by clonal cells of the published segmented dataset (<xref ref-type="bibr" rid="bib26">Murray et al., 2008</xref>). (<bold>b</bold>) Comparative histogram based on the axis ratio between the published data and the curation. (<bold>c</bold>) 3D View of the intensity images at t=150. (<bold>d</bold>) 3D View of the published segmented dataset at t=150. Colors represent the ratio of the longest axis on the shortest axis of the shape. (<bold>e</bold>) Automatic selection (in gray) of the nuclei with an axis ratio &gt;2.9. (<bold>f</bold>) Result of the Fusoc plugin applied on all selected nuclei. Colors represent the nuclei volume in µm<sup>3</sup>. (<bold>g</bold>) Same as <bold>f</bold> but only previously selected nuclei are shown. (<bold>h</bold>) Result of the Gaumi plugin applied independently on regions fused with four, three or two nuclei. Colors represent the axis ratio as in <bold>d</bold>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Manual Validation of Curation for the <italic>Caenorhabditis elegans</italic> Embryo.</title><p>(<bold>a</bold>) 2D XZ view at Y=306 of the intensity image at time point t=150 using Napari. (<bold>b</bold>) Same view showing the published segmentation, where vertically elongated segmentation artifacts are visually apparent. (<bold>c</bold>) Same view after curation, where these artifacts have been corrected.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig6-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-6-2"><title>Error detection</title><p>Although the manually curated published cell lineage appeared accurate (<xref ref-type="fig" rid="fig6">Figure 6a</xref>), some touching nuclei were elongated with flat vertical contact interfaces, likely due to poor imaging quality. To identify these segmentation artifacts systematically, we calculated the axis ratio for each segmented object and visualized these values directly on their surface meshes (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). The distribution revealed two peaks: a sharp one around 2, containing most nuclei, and a broader one above 3. Nuclei with an axis ratio greater than 2.9 were flagged for potential correction.</p></sec><sec id="s2-6-3"><title>Error correction</title><p>We curated these errors for a specific time point by first applying a fusion correction plugin (<xref ref-type="fig" rid="fig6">Figure 6f and g</xref>) to all selected nuclei pairs with an axis ratio greater than 2.9 (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). This was followed by a separation plugin (<xref ref-type="fig" rid="fig6">Figure 6h</xref>) that divides fused objects into distinct nuclei using a Gaussian mixture applied to intensity values. We initially addressed the more complex cases manually, which included one group of six fused nuclei, one group of four fused nuclei, and two groups of three. Next, we automatically curated the remaining nine simpler cases of two fused nuclei by selecting nuclei with a volume greater than 80 µm³. As expected, the axis ratio distribution of the curated set became unimodal and centered around 2 (<xref ref-type="fig" rid="fig6">Figure 6b</xref>), reflecting more regular nuclear shapes—an improvement confirmed by visual inspection (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). The entire time step at t=150 was fully curated with just 5 plugin actions (<xref ref-type="fig" rid="fig6">Figure 6h</xref>, <xref ref-type="video" rid="video4">Video 4</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106227-video4.mp4" id="video4"><label>Video 4.</label><caption><title><italic>Caenorhabditis elegans</italic>.</title><p>The movie shows how to use the lineage and image properties to easily detect segmentation issues, and how to fix them in batches for a fast curation of dense datasets.</p></caption></media></sec></sec><sec id="s2-7"><title>Use-case 5: Enhancing cell lineage accuracy by detecting segmentation errors in <italic>Phallusia mammillata</italic> embryos</title><p>This use case showcases how MorphoNet can efficiently detect and correct segmentation errors in complex 3D+t datasets of developing ascidian embryos, enhancing the accuracy of cell lineage reconstructions and analysis of cell division timing variations.</p><sec id="s2-7-1"><title>Dataset description</title><p>This dataset (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>) comprises 64 time steps of multi-view light-sheet microscopy of developing ascidian embryos (64–298 cell stages), segmented using the ASTEC algorithm (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>). Despite the high-quality segmentation and tracking, residual errors, such as multi-component cells, over- and under-segmentations, delayed, or missed divisions, and shape inaccuracies remained, with no tools available at the time of the publication to systematically validate or correct the 10,000 cell snapshots.</p></sec><sec id="s2-7-2"><title>Error detection</title><p>MorphoNet was used to polish the cell lineage to a sufficient level to study natural variation in cell division timing. The main challenge was identifying rare errors within a cell lineage that appeared accurate by visual inspection. To address common lineage residual errors, such as missing or delayed divisions and broken lineage links, we developed a set of segmentation error identification metrics. These metrics were visualized by projecting their results onto the lineage using an interactive viewer linked to the 3D dataset (<xref ref-type="fig" rid="fig7">Figure 7a and b</xref>). All identified errors stemmed from segmentation issues rather than tracking, and their correction was streamlined by the bidirectional connection between the lineage and embryo representation windows, allowing direct navigation between the two.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Curation of a <italic>Phallusia mammillata</italic> ascidian embryo named Astec-Pm9 in the publication (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>).</title><p>(<bold>a</bold>) Cell lineage of the cell b7.1 after the execution of the Disco and Deli plugins on the published data. Projection of the volume property, the colormap between <bold>b</bold> and <bold>c</bold> represents the cell volume in µm<sup>3</sup>. (<bold>b</bold>) Scatter view of the corresponding segmented embryo described in <bold>a</bold>. at t=28 colored by cell volume. Colormap identical as <bold>a</bold>. (<bold>c</bold>) Same view as<bold> b</bold> with the activation of the ‘highlight’ mode which focuses on the selected cell and shows other cells in transparent colors. (<bold>d</bold>) Several snapshots of the cell b8.21 at different time points with its associated cell lineage. Colormap represents the cell volume in µm<sup>3</sup> which points to a missing division. (<bold>e</bold>) Cell lineage of the bilateral cells b7.11 and b7.11*. Color bar shows the lineage distance between the bilateral symmetrical cells. Black region represents snapshots with no matches between bilateral symmetrical cells. (<bold>f</bold>) Cell lineage of the bilateral cells a7.5 and b8.6. Color bar shows the compactness property. The property highlights that the delay of division between A7.5* and A7.5 is due to an under-segmented error of A7.5*. B8.6 and B8.6* have expected behavior. (<bold>g</bold>) Result of the Fuse plugin applied on an over-segmented cell. (<bold>h</bold>) Top line: Several snapshots of B7.7* cell under-segmented (between time point 31 and 33) from the original segmented embryo. Bottom: result of the Propa plugin applied backward from time t=34 where both cells are well separated. (<bold>i</bold>) Example of the result of the Wata plugin from a manual added seed (red dot) in the empty space. (<bold>j</bold>) Result of the Copy-Paste plugin from the selected cell (in gray) on the left side of the embryo to the right side (the new cell appears with a mesh shader in blue). Colormap represents cell volume in µm<sup>3</sup>. (<bold>k</bold>) Comparison of the lifetime of bilateral symmetrical cells. The X-axis shows the number of in-time points separating the division of bilateral cell pairs. The Y-axis corresponds to the number of cells (in log).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Cell lineage following curation of a <italic>Phallusia mammillata</italic> embryo.</title><p>(<bold>a</bold>) Cell lineage of the cell b7.1 after curation. Projection of the volume property as in <xref ref-type="fig" rid="fig7">Figure 7a</xref>. (<bold>b</bold>) Cell Lineage of the cell b8.21 after curation. Colormap represents the cell volume in µm<sup>3</sup> as in <xref ref-type="fig" rid="fig7">Figure 7d</xref>. (<bold>c</bold>) Cell lineage of the bilateral cells b7.11 and b7.11* after curation. Color bar shows the lineage distance between the bilateral symmetrical cells as in <xref ref-type="fig" rid="fig7">Figure 7e</xref>. (<bold>d</bold>) Cell lineage of the bilateral cells 7.5 after curation. Color bar shows the compactness property as in <xref ref-type="fig" rid="fig7">Figure 7f</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig7-figsupp1-v1.tif"/></fig></fig-group><p>State-of-the-art segmentation methods often produce systematic errors, such as very small segmented objects, which we identified using geometric properties like cell volume (e.g. cell a7.1, <xref ref-type="fig" rid="fig7">Figure 7a</xref>). Property projection, combined with scatter visualization, facilitates the identification of cells even in dense, interior regions (<xref ref-type="fig" rid="fig7">Figure 7b</xref>), while the highlight mode enhances this process by isolating the cell of interest (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). Occasional missing or false divisions disrupt accurate cell histories. During early ascidian embryogenesis, stable cell volumes allowed volume projections onto the lineage to identify rapid variations, revealing missed divisions (<xref ref-type="fig" rid="fig7">Figure 7d</xref>).</p><p>Ascidian embryos exhibit bilateral symmetry, with homologous cells dividing almost synchronously. A l<italic>ineage distance</italic> property (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>) revealed missed divisions (e.g. b7.11* in <xref ref-type="fig" rid="fig7">Figure 7e</xref>). To distinguish segmentation errors from natural variability, a <italic>compactness</italic> property was used to detect inaccuracies in division timing by tracking cell rounding during mitosis (e.g. <xref ref-type="fig" rid="fig7">Figure 7f</xref>).</p><p>Using MorphoNet, an expert identified approximately 20 isolated cell lineage errors in a dataset of 185 cell snapshots in just 2 hr.</p></sec><sec id="s2-7-3"><title>Error correction</title><p>The 41 multi-component labels were separated using the <italic>Disco</italic> plugin, identifying seven potential cells via the <italic>volume</italic> property analysis (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). The others, considered as small artifacts, were merged with neighboring cells sharing the largest surface area using the <italic>Deli</italic> plugin. Division timing issues often stem from over-segmentation, under-segmentation, or missing cells. Over-segmentations were resolved with the <italic>Fuse</italic> plugin (<xref ref-type="fig" rid="fig7">Figure 7g</xref>), while under-segmentations were corrected using <italic>Propagate segmentation</italic> plugins by tracing back from the first accurate segmentation of sister cells to their mother’s division point (<xref ref-type="fig" rid="fig7">Figure 7h</xref>). For missing cells, seeds were added with seed generator plugins and segmented using local seeded watershed algorithms (<xref ref-type="fig" rid="fig7">Figure 7i</xref>). If imaging quality was too poor, the bilaterally symmetrical cell served as a mirror-image proxy, using the Copy-Paste plugin to replicate, rotate, and scale the cell to the missing position (<xref ref-type="fig" rid="fig7">Figure 7j</xref>). A total of 185 errors were corrected in approximately eight hours using 264 MorphoNet plugin actions, resulting in a more accurate estimation of natural variability in cell division timing (<xref ref-type="fig" rid="fig7">Figure 7k</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). This duration includes the full workflow performed by a single user: loading the dataset, exploring the cell lineage, identifying segmentation or division errors, and applying corrective plugins. The reported actions include both exploratory attempts and validated corrections, combining user interaction with computation time (<xref ref-type="video" rid="video5">Video 5</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106227-video5.mp4" id="video5"><label>Video 5.</label><caption><title><italic>Phallusia mammillata</italic>.</title><p>This movie shows how to identify and fix several issues on a segmented dataset, using the lineage viewer and a large array of plugins. It shows how to fix large curation errors using a couple of actions only.</p></caption></media></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Recent advancements in optical time-lapse microscopy allow for the 3D capture of dynamic biological processes but face challenges in automating the segmentation and tracking of large, complex datasets. Residual segmentation errors in time-lapse datasets disrupt data interpretation and hinder long-term cell tracking. Moreover, accurate, high-quality 3D data is critical for training next-generation AI-based segmentation tools, yet the availability of such datasets remains limited.</p><p>To address these challenges, <bold>MorphoNet 2.0</bold> was developed as a standalone application for reconstructing, evaluating, and curating 3D and 3D+t datasets. Building on its previous web-based version, MorphoNet 2.0 integrates voxel images with meshed representations, leveraging both Unity Game Engine and Python for enhanced interactivity and processing power. Key features include tools for assessing segmentation quality through unsupervised metrics (e.g., volume, smoothness, and temporal stability), automated error detection, and visualization of segmented objects alongside raw intensity images.</p><p>Conventional image curation tools struggle with the complexities of interacting with 3D voxel images. In contrast, MorphoNet 2.0 introduces a novel approach using dual representation layers, enabling efficient biocuration by isolating problem areas and significantly accelerating dataset refinement. MorphoNet 2.0 is user-friendly, open-source, and supports both scientific discovery and AI training by producing high-precision 3D datasets and enabling reproducible, scalable data analysis.</p><p>By showcasing five use-cases of fluorescence datasets in which cell membranes or nuclei were labeled, we demonstrated that the tool’s high versatility and user-friendliness enables biologists without programming skills to efficiently and intuitively detect and handle a broad range of errors (under-segmentation, over-segmentation, missing objects, lineage errors).</p><p>MorphoNet has a high potential to adapt to evolving datasets and segmentation challenges. First, its open-source Python plugin architecture fosters community-driven improvements. These could target cellular datasets as exemplified by the five use-cases presented. They could also open MorphoNet to other imaging modalities, including multi-modal datasets combining, for instance, fluorescence and electron microscopy. Additional plugins could accommodate new AI tools or automate the training of segmentation models through data augmentation, feature selection, or hyperparameter optimization. Plugins for widely used platforms, such as Napari or Fiji will also broaden the user-base and interoperability of the tool, as would also the development of flexible export options to integrate MorphoNet outputs with other analytical pipelines or visualization software. The MorphoNet platform could be further extended through the creation of a centralized repository for community-developed plugins, the organization of MorphoNet-based bio-image analysis challenges to stimulate community engagement, and the provision of curated datasets to serve as benchmarks for testing and validating new segmentation algorithms. To support these developments over time, we rely on institutional support from our host laboratories and research organizations, and we will seek additional funding through dedicated research grants. We also aim to foster open-source contributions, develop training materials, and provide user support to ensure long-term adoption and sustainability.</p><p>Curation could also be improved by the introduction of cloud-based, multi-user capabilities to enable experts to work on the same dataset simultaneously. For now, web browsers impose constraints on the use of computing resources, which could be lifted in the near future, for example, through the development of WebGPU.</p><p>Curation capabilities will likely also be enhanced by the implementation of adaptive machine learning models that integrate past user corrections to suggest or automate future edits. In the context of 3D segmentation tasks, manual expertise and curation for voxel-wise segmentation is labor-intensive and expensive. Full annotation of large datasets may thus not be feasible. Partial annotations allow datasets to be created with less time and resources while still providing valuable information. Including algorithms, such as Sketchpose (<xref ref-type="bibr" rid="bib6">Cazorla et al., 2025</xref>) could leverage weakly-supervised learning to generalize from the partially annotated data and infer segmentation patterns in the unlabeled portions of the dataset, a strategy we initiated with the Tribolium dataset. This will make it possible to train models on diverse datasets without the burden of full annotations.</p><p>By addressing major challenges in 3D and 3D+t dataset assessment and curation, MorphoNet 2.0 provides a versatile platform for improving segmentation quality and generating reliable ground truths. Its user-friendliness, adaptability, and extensibility position it as a valuable tool for advancing quantitative bio-image analysis, with significant potential for enhancement and broader application in the future.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Tools comparison</title><p>MorphoNet (MN) is a platform specifically designed for end users, requiring no programming skills. Its standalone version is a portable, code-free application, similar in spirit to Fiji (FJ).</p><p>MN provides interactive visualization of segmentations and supports both segmentation creation (via a range of plugins) and curation through a broad set of image processing tools. Unlike FJ and Napari (NP), which typically operate at the voxel level, MN performs curation exclusively at the object level. This object-centric approach enables faster and more intuitive editing of complex 3D segmentations.</p><p>MN also supports cell tracking and lineage visualization. It includes plugins for both automatic lineage generation and manual curation, similar to FJ’s TrackMate plugin and NP’s btrack (the latter requiring some coding knowledge).</p><p>MN offers built-in tools for visualizing and exporting morphological properties of segmented objects. In contrast to FJ, MN can directly map these properties onto segmentations using color-coded overlays, and can also project them onto lineage trees in its dedicated viewer.</p><p>Deep learning-based tools like Cellpose and StarDist are natively embedded in MN’s standalone application, supporting both inference and model training with no additional installation. In comparison, FJ includes only a StarDist plugin, while NP supports both tools but typically requires command-line installation and environment setup.</p><p>Finally, all MN plugins are bundled and maintained by the development team, ensuring integration and stability. In contrast, plugin ecosystems in FJ and NP are open, allowing users to freely develop, install, and share new tools with the community.</p></sec><sec id="s4-2"><title>Curated and detailed pipeline for use cases</title><sec id="s4-2-1"><title>UC 1: <italic>Tribolium castaneum</italic> embryo cell nuclei segmentation</title><sec id="s4-2-1-1"><title>Curated pipeline</title><p>Due to the high number of errors in the published ST, we decided to generate de novo nucleus segmentation using MorphoNet. We defined a pipeline in five steps, which required only a few manual curations. Step 1- We started by launching the <italic>Cellpose prediction</italic> plugin using the pretrained standard <italic>nuclei</italic> model. Step 2- The nuclei of this segmentation corresponding to the published GT (181 nuclei) were curated with various plugins. Eight segmentations with wrong shape were recomputed using the <italic>Binarize</italic> plugin. One missing nucleus was added with the <italic>BinCha</italic> plugin which creates a new segmentation from the selected GT using a binary threshold on intensity image. Twelve segmentation errors (which were wrong segmentation boundaries between adjacent nuclei, and over-segmentations) could be corrected by using combinations of the <italic>Fuse</italic>, <italic>Gaumi,</italic> and <italic>Delete</italic> plugins. Also, using the intensity properties, we visually identified 248 over-segmented nuclei outside of the GT, that were easily corrected with the <italic>Fuse</italic> plugin. Step 3.We developed a <italic>Cellpose Train</italic> plugin with a user-friendly interface, which we used to extend pretrained Cellpose models (here <italic>nuclei</italic>) using XY, XZ, and YZ plane information. We added an option to train a model on selected objects within a region of interest of the image. We extended the <italic>nuclei</italic> model by an additional training step restricted to the 181 curated GT cells and used this new model to predict a new segmentation on the entire image. Step 4- Finally, based on this new segmentation, we automatically select nuclei with a volume between 1000 and 10,000 voxels and perform a new Cellpose train using this selection only. Step 5- To achieve a perfect shape for each GT-associated nuclei, we identified 35 nuclei with non-convex shapes using the <italic>convexity</italic> property, which we then refined using the <italic>Shape Transform</italic> plugin family (See Plugin list).</p></sec><sec id="s4-2-1-2"><title>Manual segmentation protocol</title><p>To sample a representative set of segmentation quality levels, we used the histograms of the intensity_border_variation and intensity_offset properties computed from the original Cell Tracking Challenge dataset. For each metric, one cell was randomly selected from each of the 20 histogram bins, yielding a total of 40 cells. For each selected cell, a dilated bounding box was extracted from the original intensity image, and manual ground truth segmentation was performed using Napari’s label editing tool. To detect over-segmentation, any segmented object other than the matched one that overlapped the manual ground truth by at least 5% of its volume was classified as an over-segmentation.</p></sec></sec></sec><sec id="s4-3"><title>UC 2: Whole membrane segmentation of a <italic>Patiria miniata</italic> starfish embryo</title><sec id="s4-3-1"><title>Original workflow</title><p>The published workflow started with the construction of a 3D Voronoi diagram based on cell centroids calculated from the nuclei channel acquisition, which was used to train a 3D ResU-Net (<xref ref-type="bibr" rid="bib13">Franco-Barranco et al., 2022</xref>) on 14 consecutive stacks from a single embryo at the 256 cell stage. The predictions obtained with this trained model on wild-type and mutant embryos from the 128- to 512 cell stages were used as input for PlantSeg (<xref ref-type="bibr" rid="bib45">Wolny et al., 2020</xref>), a watershed-based algorithm. Segmentation errors were corrected using custom Matlab scripts and the resulting segmentations were used to train a second 3D ResU-Net model, which was applied to the whole dataset. In compressed embryos, the authors choose to segment only 196 cells from a 512 cell stage embryo (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), the signal intensity in the embryos’ center and external circumferences being considered too low for precise and objective segmentation.</p></sec><sec id="s4-3-2"><title>Training Protocol for MorphoNet segmentation with the advanced Cellpose plugin</title><p>To train Cellpose 3D on the 300 <italic>Patiria miniata</italic> starfish embryos kindly provided by the authors of the paper (<xref ref-type="bibr" rid="bib3">Barone et al., 2024</xref>), we split it into three subgroups: a training set corresponding to 80% of the database, a testing set (10%) and a validation set (10%). We then split 3D images into 2D slides in each direction (X, Y, Z). Cellpose has to preload data in memory to convert them in the native format before training. Due to inherent memory limitations, we had to split the training process into several trials. Each trial contains 5000 random images of the training set. We run 50 trials with each of them containing 50 epochs. We started the first trial using the <italic>cyto2</italic> model and then each successive trial was continued to train based on the model obtained in the previous one. We then evaluate each trial on the testing set and keep the one which give the best accurate segmentation.</p></sec></sec><sec id="s4-4"><title>UC 4: <italic>Caenorhabditis elegans</italic> embryo cell nuclei segmentation</title><sec id="s4-4-1"><title>Original workflow</title><p>The dataset contains 3D stacks of intensity images and two types of ground truth annotations. A silver truth (ST) automated segmentation of each nucleus using StarryNite (<xref ref-type="bibr" rid="bib25">Murray et al., 2006</xref>). A gold truth (GT) manual annotation of the position of each nucleus over time, whose tracking was manually corrected with AceTree (<xref ref-type="bibr" rid="bib5">Boyle et al., 2006</xref>).</p></sec></sec><sec id="s4-5"><title>UC 5: <italic>Phallusia mammillata</italic> embryo whole-cell segmentation</title><sec id="s4-5-1"><title>Error correction</title><p>The 41 labels (which should be cells) consisting of more than one connected component were disconnected using the <italic>Disco</italic> plugin. Visual inspection using the <italic>volume</italic> property of these objects identified seven potential cells. The others, considered as small artifacts, were removed by fusion to the cell neighbor they share the most surface with using the <italic>Deli</italic> plugin. This plugin requires the threshold value for the minimum cell volume, which can be manually found by specifically highlighting all small cells (<xref ref-type="fig" rid="fig7">Figure 7b</xref>).</p><p>Division timing issues typically arise from over-segmentation, under-segmentation or missed cells. Over-segmentations were easily corrected using the <italic>Fuse</italic> plugin (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). Using the <italic>Propagate segmentation</italic> family plugins, under-segmentations were corrected by backwards propagation from the first accurate segmentation of the two sister cells in the dataset up to the precise moment of division of their mother (<xref ref-type="fig" rid="fig7">Figure 7h</xref>). Missing cells can be difficult to correct when signal intensity quality is poor. One correction option is to add a missing seed with the seed generator plugins. The expert can then apply one of several local seeded watershed algorithms to generate the missing cell segmentations (<xref ref-type="fig" rid="fig7">Figure 7i</xref>). When the imaging quality is too low for this strategy to succeed, we reasoned that the bilaterally symmetrical cell may be a good mirror-image approximation of the missing cell. We thus used the <italic>Copy-Paste</italic> plugin to copy a cell from her symmetric and to paste it, after appropriate rotation and scaling, where a cell is missing (<xref ref-type="fig" rid="fig7">Figure 7j</xref>).</p><p>Using a combination of using 264 MorphoNet plugin actions, 185 errors were corrected in 8 hr actions, leading to a better estimation of the natural variability in cell division timing (<xref ref-type="fig" rid="fig7">Figure 7k</xref>).</p></sec></sec><sec id="s4-6"><title>Cellpose plugins</title><sec id="s4-6-1"><title>Cellpose train plugin</title><p>The current version of Cellpose has limitations for 3D model training. It requires command-line or Python API usage, which is inaccessible for most experimental biologists. Additionally, Cellpose only trains on XY planes of 3D datasets, making it inefficient for anisotropic datasets with lower axial resolution, like the <italic>Arabidopsis</italic> dataset, resulting in over-segmented cells. These issues were addressed by developing a MorphoNet Cellpose Train plugin with a user-friendly interface that leverages information from the XY, XZ, and YZ planes, improving segmentation by fine-tuning pretrained models using 3D data. We added an option to make the image isotropic (by lineage interpolation) before launching the training which increases the model accuracy. Users have the possibility to train their own model on several time steps. Finally, in order to be able to use partial annotation, this plugin can be run only on selected masks, such as in the Tribolium dataset.</p></sec><sec id="s4-6-2"><title>Cellpose predict plugin</title><p>The Cellpose Predict plugin in MorphoNet is dedicated to the prediction of 3D segmented images. We implemented in MorphoNet an option to run each algorithm only on selected masks only (e.g., on cells containing segmented errors). Thus, this plugin can be run only on a subpart of the dataset (such as in <italic>Arabidopsis</italic> dataset). Cellpose generates multiple unconnected components with the same label which can be removed using the Disconnected Components option (Identical as the Disco Plugin). Applying the <italic>cyto3</italic> model on the <italic>Arabidopsis</italic> dataset generates 13293 objects with disconnected components (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d and e</xref>). We also add the possibility to automatically remove these small artifacts generated by Cellpose using an option to fuse or delete the objects below a given size (It will remove these objects if they are small artefacts in the background) (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1f</xref>). This targeted approach is fastest, gives the best results and has the additional advantage of preserving the accurate cell segmentations.</p></sec></sec><sec id="s4-7"><title>Properties</title><p>The majority of the region properties are computed from the scikit-image Python package <ext-link ext-link-type="uri" xlink:href="https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops">as described here</ext-link>. Properties marked with a (v) are computed on the original image, whereas others are computed on an image re-scaled by its voxel size, to give appropriate physical measurements.</p><list list-type="bullet" id="list2"><list-item><p>volume (v): Corresponds to the <italic>area</italic> property of scikit-image. Area of the region, i.e., number of voxels in the region.</p></list-item><list-item><p>volume-real: Volume property but scaled by the voxel-size to give a real physical volume <bold>volume-bbox</bold>: Corresponds to the <italic>area_bbox</italic> property of scikit-image. Area of the bounding box i.e., number of voxels of bounding box scaled by voxel-area.</p></list-item><list-item><p>volume-filled: Corresponds to the <italic>area_filled</italic> property of scikit-image. Area of the region with all holes filled in.</p></list-item><list-item><p>axis-major-length: The length of the major axis of the ellipse that has the same normalized second central moments as the region.</p></list-item><list-item><p>axis-minor-length: The length of the minor axis of the ellipse that has the same normalized second central moments as the region.</p></list-item><list-item><p>axis-ratio: Ratio of the longest axis over the smallest axis of the label (see <italic>axis-major-length</italic> and <italic>axis-minor-length</italic>).</p></list-item><list-item><p>diameter (v): The mean diameter of the region. It is the mean of axis-major-length and axis-minor-length. IMPORTANT: this value is expressed in voxels, not physical size, so it can be of use for plugins that use voxel measurements, such as Cellpose, for instance. <bold>Equivalent-diameter-area</bold>: The diameter of a circle with the same area as the region.</p></list-item><list-item><p>euler-number: Euler characteristic of the set of non-zero pixels. Computed as the number of connected components plus the number of holes, subtracted by the number of tunnels.</p></list-item><list-item><p>extent: Ratio of pixels in the region to pixels in the total bounding box. Computed as <italic>volume</italic> divided by number of rows multiplicated by number of columns in bounding box <bold>connected-neighbors</bold>: number of connected other labels.</p></list-item><list-item><p>convexity: distance to convexity. Computed as volume of the convex hull (smallest convex polygon that encloses the region) divided by the volume of the region.</p></list-item><list-item><p>roughness: mean of the absolute values of the region minus the closing (dilation followed by erosion) of the region.</p></list-item><list-item><p>compactness: computed as <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>36</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mi>π</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$\frac{region\, surface\, area^{3}}{36\, \times \pi \, *\, volume^{2}}$\end{document}</tex-math></alternatives></inline-formula></p></list-item><list-item><p>smoothness: computed as <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$\frac{region\, surface\, area}{volume^{\frac{2}{3}}}$\end{document}</tex-math></alternatives></inline-formula></p></list-item><list-item><p>Intensity-max (v): Value with the greatest intensity in the region.</p></list-item><list-item><p>Intensity mean (v): Value with the mean intensity in the region.</p></list-item><list-item><p>Intensity-min (v): Value with the least intensity in the region.</p></list-item><list-item><p>Intensity-border variation (v): the standard deviation of the intensity images only at the border of the segmentation.</p></list-item><list-item><p>Intensity-offset (v): the Euclidean distance between the gravity center of the intensity images and the geometrical center of the segmentation.</p></list-item><list-item><p>Lineage-distance: Requires a lineage property, as well as the <italic>cell_name</italic> property, containing the Conklin (<xref ref-type="bibr" rid="bib7">Child, 1906</xref>) naming of cells. Compute the tree-edit distance between lineage trees (<xref ref-type="bibr" rid="bib14">Guignard et al., 2020</xref>) of symmetrical cells.</p></list-item><list-item><p>Bbox (v): bounding box of the region (tuple).</p></list-item></list></sec><sec id="s4-8"><title>Curation</title><p>The MorphoNet standalone application allows users to perform the 3D images curation using a new paradigm. The concept is based on a duality between the segmented image and the construction of 3D mesh objects for each individual label. While the meshes object are extremely powerful for 3D visualization and interaction, the 3D segmented image remains the most standard data type for image labels. The curation is performed in four steps:</p><list list-type="order" id="list3"><list-item><p>Users identify their issues to curate using the viewer based on the interaction with the messages (using Lineage or Intensity Images)</p></list-item><list-item><p>Users identify and launch the most appropriate plugin to solve their issue</p></list-item><list-item><p>The plugin performs the curation directly inside the segmented image</p></list-item><list-item><p>MorphoNet automatically recomputes the modification of the meshes of the labeled which have been modified by the plugin and finally refresh the window</p></list-item></list><p>Each plugin performs a modification of the 3D segmented images. The modification is done locally on the backup of the segmented images stored inside the MorphoNet application, not on the original data. Finally, users can export their curation as new segmented images. Meshes can also be exported in standard formats, including OBJ, STL, and PLY.</p><p>MorphoNet includes a list of various 3D plugins described below. We also add the possibility for any bio-image analysis to simply develop, test, and add their own plugins (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). The Python environment already contains several libraries, such as NUMpy (<xref ref-type="bibr" rid="bib15">Harris et al., 2020</xref>), scikit-image (<xref ref-type="bibr" rid="bib41">van der Walt et al., 2014</xref>), scikit-learn (<xref ref-type="bibr" rid="bib31">Pedregosa, 2025</xref>), TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi, 2016</xref>), and pytorch (<xref ref-type="bibr" rid="bib30">Paszke, 2019</xref>).</p></sec><sec id="s4-9"><title>Curation plugins</title><p>MorphoNet already included a list of various 3D plugins which are classified by family types: De Novo Segmentation, De Novo Seed, Segmentation from Seeds, Segmentation Correction, Propagate Segmentation, Edit Temporal Links, and Shape Transform (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Representation of the MorphoNet Plugins organized by the corresponding functionalities according to segmentation issues.</title><p>Color code represents the plugin family. Description of each plugin functionalities is accessible on the help web page: <ext-link ext-link-type="uri" xlink:href="https://morphonet.org/help_curation#plugin_list">https://morphonet.org/help_curation#plugin_list</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106227-fig8-v1.tif"/></fig><sec id="s4-9-1"><title>De Novo segmentation</title><p>These plugins function without any previous segmentation and directly create a segmentation from the 3D intensity images. Alternatively, they can override any segmented image (or a part of) when used on already segmented images (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.DeNovoSegmentation.html">Full online documentation</ext-link>).</p><sec id="s4-9-1-1"><title>Stardist-predict: Perform nuclei segmentation on intensity images</title><p>This plugin uses intensity images of the nucleus from a local dataset to compute segmentations of the nucleus, using the 3D Stardist deep learning algorithm (<xref ref-type="bibr" rid="bib43">Weigert et al., 2020</xref>). The default demo model of Stardist can be used, as well as custom models (which can be created by the Stardist-Train plugin).</p></sec><sec id="s4-9-1-2"><title>Stardist-train: Train the Stardist model for nuclei on your own data</title><p>This plugin allows users to train their own Stardist model of their 3D datasets. Using the 3D intensity image(s) with the corresponding segmentation(s), users can train their own models. The models can subsequently be used in the Stardist-Predict plugin to perform nuclei segmentation prediction on 3d intensity images (<xref ref-type="bibr" rid="bib43">Weigert et al., 2020</xref>).</p></sec><sec id="s4-9-1-3"><title>Cellpose-predict: Perform membrane segmentation on intensity images</title><p>This plugin uses an intensity image of the membranes from a local dataset at a specific time point to compute a segmentation of the membranes, using the 3D Cellpose deep learning algorithm. Users can apply Cellpose on a 3D selected Mask to apply it to a part of a segmentation. By default, the plugin also disconnects all non-connex labels in the generated segmentation, and deletes all segmentations below a certain volume (in voxels), the same way it does in the <italic>Disco</italic> and <italic>Deli</italic> plugins. Each of these operations can be disabled with the plugin parameters.</p></sec><sec id="s4-9-1-4"><title>Cellpose-train: Perform membrane segmentation on intensity images</title><p>This plugin allows users to train their own model (from the models provided by Cellpose) on their own 3D datasets. With an intensity image and the corresponding segmentation, users can re-train a Cellpose model to obtain their own model, trained on their images. The model will be trained by using 2D images, which are the image stacks on the XY, XZ and YZ planes of the 3D images. Users can apply the plugin on a 3D selected Mask to train on a sub-part of a segmentation.</p><p>The model you output from this plugin can then be used in the Cellpose-Predict plugin, by inputting it in the pretrained_model parameter.</p></sec><sec id="s4-9-1-5"><title>Mars: Perform a seeded watershed segmentation on intensity images</title><p>This plugin uses an intensity image from a local dataset at a specific time point to perform a segmentation using a seeded watershed algorithm (<xref ref-type="bibr" rid="bib32">Pinidiyaarachchi and Wählby, 2006</xref>).</p></sec><sec id="s4-9-1-6"><title>Binarize: Apply a threshold to intensity image</title><p>This plugin applies a threshold to the intensity image and then creates labels on each connected component above the threshold. This binarization can be applied on a given mask to run it on a sub-part of the image.</p></sec><sec id="s4-9-1-7"><title>BinCha: Apply a binary threshold on the other channel from selected objects</title><p>This plugin creates a new segmentation from each mask of the selected objects using a binary threshold on intensity image in the desired channel. Alternatively, you can also do the thresholding on the centroid of each object, with a bounding box of input radius.</p></sec><sec id="s4-9-1-8"><title>BinBox: Binarize intensity images and label each object inside a bounding box</title><p>This plugin applies a threshold to intensity image on a bounding box and creates new labels on each connected component above the threshold.</p></sec></sec></sec><sec id="s4-10"><title>De Novo seed</title><p>These plugins automatically create seeds (e.g. 3D points within the 3D images), which can be then used by the plugins in the Segmentation from seeds family (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.DeNovoSeed.html">Full online documentation</ext-link>).</p><sec id="s4-10-1"><title>Seedio: Create seeds from minimum local intensity images on the selected objects</title><p>This plugin generates seeds that can be used in other plugins (mainly watershed segmentation). Users have to select objects (or label them) to generate seeds. Seeds are generated at the minima inside the selected object.</p></sec><sec id="s4-10-2"><title>Seedin: Create seeds from minimum local intensity images (without selected objects)</title><p>This plugin generates seeds that can be used in other plugins (mainly watershed segmentation). Seeds are generated at the minimum intensity where no segmentation labels are found (in the background).</p></sec><sec id="s4-10-3"><title>Seedis: Create seeds from the maximum distance to the border of the selected objects (without intensity images)</title><p>This plugin generates seeds that can be used in other plugins (mainly watershed segmentation). It computes the distance to the border of the selected objects and then extracts the maxima. N seeds are generated at the maximal distance inside the selected object (N being the number of seeds to generate), if the distance (between seeds) is above the threshold given by the min_distance parameter (in voxels, not physical size).</p></sec><sec id="s4-10-4"><title>Seedax: Create Seeds on the long axis of the selected objects (without intensity images)</title><p>This plugin generates seeds that can be used in other plugins (mainly watershed segmentation). The longest axis of the segmentation shape is computed, and then split in N segments (N being the number of seeds in parameter). Seeds are generated at the contact points of the segments. It requires the user to select or label objects on MorphoNet.</p></sec><sec id="s4-10-5"><title>Seedero: Create seeds from the erosion of selected objects (without intensity images)</title><p>This plugin generates seeds that can be used in other (mainly segmentation) algorithms. This plugin applies multiple erosion steps of each selected object, until objects can be separated into multiple unconnected parts. Then a seed is placed at the barycenter of each individual sub-part of the segmentation.</p></sec></sec><sec id="s4-11"><title>Segmentation from seeds</title><p>These plugins can generate segmentations using seeds (mostly with a watershed algorithm <xref ref-type="bibr" rid="bib27">Najman and Schmitt, 1994</xref>). Seeds can be either added by a plugin or manually on the interface (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.SegmentationFromSeeds.html">Full online documentation</ext-link>).</p><sec id="s4-11-1"><title>Watio: Perform a watershed segmentation on intensity images on selected objects</title><p>This plugin creates new objects using a watershed algorithm from seed generated using a plugin or manually placed in the MorphoNet Viewer inside selected objects.</p><p>The watershed algorithm generates new objects based on the intensity image and replaces the selected objects. If the new generated objects are under the volume threshold defined by the user, the object is not created.</p></sec><sec id="s4-11-2"><title>Wati: Perform a watershed segmentation on intensity images (without selected objects)</title><p>This plugin creates new objects using a watershed algorithm from seed(s) generated or placed in the MorphoNet Viewer. The watershed algorithm generates new objects using the intensity image for each seed that is in the background. If the new generated objects are under a volume threshold defined by the user, the object is not created.</p></sec><sec id="s4-11-3"><title>Wato: Perform a watershed segmentation on selected objects (without intensity images)</title><p>This plugin creates new objects using a watershed algorithm from seed(s) generated using a plugin or placed in the MorphoNet Viewer inside selected objects. The watershed algorithm generates new objects using the segmentation image for each seed and replaces the selected objects. If the new generated objects are under the volume threshold defined by the user, the object is not created.</p></sec><sec id="s4-11-4"><title>Wata: Perform a watershed segmentation (without intensity images and without selected objects)</title><p>This plugin creates new objects using a watershed algorithm from seed(s) generated using a plugin or placed in the MorphoNet Viewer. The watershed algorithm generates new objects using the segmentation image for each seed inside a box that is not in another object. If the new generated objects are under the volume threshold defined by the user, the object is not created.</p></sec></sec><sec id="s4-12"><title>Segmentation correction</title><p>A various list of plugins to perform actions at the level of the selected objects (fusion, deletion, split, copy-paste, etc.) (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.SegmentationCorrection.html">Full online documentation</ext-link>).</p><sec id="s4-12-1"><title>Fuse: Fuse the selected objects</title><p>This plugin performs the fusion of selected objects into a single one. Multiple selected objects will be fused together at the current time point. If objects are labelled it will apply a fusion between all objects sharing the same label (for each individual time points).</p></sec><sec id="s4-12-2"><title>Gaumi: Split the selected objects using probability distribution</title><p>This plugin calculates the Gaussian mixture model probability distribution on selected objects in order to split them into several objects which will replace the selected ones.</p></sec><sec id="s4-12-3"><title>Disco: Split the selected objects for unconnected objects</title><p>This plugin can be used to split any object made up of several unconnected sub-objects.</p></sec><sec id="s4-12-4"><title>Splax: Split the selected objects in the middle of a given axis</title><p>This plugin splits any selected objects into two new objects in the middle of one of the given image axes.</p></sec><sec id="s4-12-5"><title>lete: Delete the selected objects</title><p>This plugin removes any selected objects from the segmented images. Users can select objects at the current time point, or label any objects to delete at several time points. The background value (usually 0) will replace the object voxels inside the segmented image.</p></sec><sec id="s4-12-6"><title>Deli: Delete any objects below a certain size</title><p>This plugin removes all objects that are under a certain volume (in voxel count) from the segmented images.</p></sec><sec id="s4-12-7"><title>Copy-paste: Copy a selected object and apply a transformation to the copy</title><p>This plugin gives the possibility to copy an object (a segmented cell, for example) and paste it at another time step and/or another location. The object can be moved, rotated, and scaled on the MorphoNet interface.</p></sec><sec id="s4-12-8"><title>Match: Matching objects across multiple channels</title><p>This plugin allows you to match the elements of the same object across several channels. It will give the selected objects a matching label in the segmented image. Can be used in batch by labeling objects together with label groups.</p></sec></sec><sec id="s4-13"><title>Shape transform</title><p>These plugins allow the user to change the shape of objects, with Morphological operators (Erode, Dilate, etc.) and even manually (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.ShapeTransform.html">Full online documentation</ext-link>).</p><sec id="s4-13-1"><title>Dilate: Dilate the selected objects</title><p>This plugin performs the dilation morphological operator on each individual selected object.</p></sec><sec id="s4-13-2"><title>Erode: Erode the selected objects</title><p>This plugin performs the erosion morphological operator on each individual selected object.</p></sec><sec id="s4-13-3"><title>Open: Open the selected objects</title><p>This plugin performs the opening morphological operator on each individual selected object.</p></sec><sec id="s4-13-4"><title>Close: Close the selected objects</title><p>This plugin performs the closing morphological operator on each individual selected object.</p></sec><sec id="s4-13-5"><title>Convex: Make selected objects convex</title><p>This plugin computes the convex voxel hull of each individual selected object.</p></sec><sec id="s4-13-6"><title>Deform: Apply a manual deformation on the selected object</title><p>This plugin allows the user to manually deform a selected object using mesh deformation. This plugin must be used with the mesh morphing menu and its various tools. The mesh morphing menu allows the user to manually deform a selected object of your dataset by applying various transformations (move vertices, extrude, hollow,...) with the mouse pointer. Once the user is satisfied with the deformation, this plugin computes the transformation(s) applied to the mesh to the segmented image in the dataset, and regenerates the mesh object using this segmented data.</p></sec></sec><sec id="s4-14"><title>Propagate segmentation</title><p>These plugins allow the user to propagate a good segmentation at a given time to other time points (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.PropagateSegmentation.html">Full online documentation</ext-link>).</p><sec id="s4-14-1"><title>Propa: Propagate selected eroded objects through time on the next object (with or without intensity images)</title><p>This plugin propagates labeled objects at the current time point through time. It requires applying a specific label to the objects to propagate at the current time (named the source objects) And then labeling the corresponding objects on which to propagate at the next or previous time points (named the destination objects). The plugin can be executed forward (source objects are at the beginning of the time range) or backward (source objects are at the end of the time range). The source objects are eroded until they fit into destination objects at the next time point, and then a watershed is computed. The watershed algorithm can be computed using an intensity image by checking the ‘use intensity’ box. The new objects created by a watershed will replace the destination objects.</p><p>This plugin propagates labeled objects at the current time point through time, filling empty space. The selected object(s) (named the source objects) will be propagated forward in time or backward in time by choosing the appropriate time_direction parameter. The source objects are then copied to the segmentation of the next/previous time (the intersection with the rest of the segmentation is removed), and finally a watershed algorithm is applied, using the corresponding intensity image. New objects are created in the background of the segmentation.</p></sec></sec><sec id="s4-15"><title>Edit temporal links</title><p>These plugins can create or modify temporal information in order, for example, to curate a cell lineage tree (<ext-link ext-link-type="uri" xlink:href="https://morphonet.org/helpfiles/API/morphonet.plugins.EditTemporalLinks.html">Full online documentation</ext-link>).</p><sec id="s4-15-1"><title>Addlink: Create temporal links between labeled objects</title><p>This plugin creates temporal links at several time points on objects sharing the same label. After the execution, the lineage property is updated.</p></sec><sec id="s4-15-2"><title>Delink: Delete temporal links on labeled objects</title><p>This plugin deletes temporal links at several time points on objects sharing the same label. After the execution, the lineage property is updated.</p></sec><sec id="s4-15-3"><title>Tracko: Create temporal links using the overlap between all objects</title><p>This plugin creates a complete object lineage using the maximum of overlap between objects. The overlap is calculated between the bounding box enveloping each object. After the execution, the lineage property is updated.</p></sec></sec><sec id="s4-16"><title>Create a new plugin</title><p>It is possible for any user to develop their own curation plugins in Python, and integrate them in their version of the MorphoNet standalone application. Users who wish to develop their own plugins can do so by following a tutorial on the MorphoNet website help page: <ext-link ext-link-type="uri" xlink:href="https://morphonet.org/help_api?menu=morphonetplot#plugins">https://morphonet.org/help_api?menu=morphonetplot#plugins</ext-link>.</p></sec><sec id="s4-17"><title>Code availability</title><p>The MorphoNet platform is composed of several Open-Source repositories:</p><list list-type="bullet" id="list4"><list-item><p>The frontend 3D Viewer code and Unity3D project is available here: (<ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/MorphoNet/morphonet_unity">https://gitlab.inria.fr/MorphoNet/morphonet_unity</ext-link>, <xref ref-type="bibr" rid="bib17">Laurent, 2025a</xref>).</p></list-item><list-item><p>The python API code, which contains the python backend of the standalone application is available here: (<ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/MorphoNet/morphonet_api">https://gitlab.inria.fr/MorphoNet/morphonet_api</ext-link>, <xref ref-type="bibr" rid="bib18">Laurent, 2025b</xref>).</p></list-item><list-item><p>The lineage viewer code and Unity3D project is available here: (<ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/MorphoNet/morphonet_lineage">https://gitlab.inria.fr/MorphoNet/morphonet_lineage</ext-link>, <xref ref-type="bibr" rid="bib19">Laurent, 2025c</xref>).</p></list-item></list></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Visualization</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Visualization</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Validation, Visualization</p></fn><fn fn-type="con" id="con4"><p>Software</p></fn><fn fn-type="con" id="con5"><p>Software</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-106227-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Summary of the benchmarking datasets and device performance evaluation for the MorphoNet standalone application.</title></caption><media xlink:href="elife-106227-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Overview of the MorphoNet documentation hub, organizing help resources by user profile and intended use cases.</title></caption><media xlink:href="elife-106227-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The MorphoNet_Data repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.30529745.v1">https://doi.org/10.6084/m9.figshare.30529745.v1</ext-link>) provides all datasets and resources associated with the MorphoNet 2.0 publication. It serves as an open archive supporting the reproducibility and reuse of the study's results. The repository includes: DATASETS - Original and curated imaging datasets used in MorphoNet 2.0. BENCHMARKING_DATASETS - Reference datasets employed for performance evaluation, as detailed in the benchmark documentation and supplementary materials. MODELS - Pre-trained Cellpose models specifically used for segmentation curation in each dataset. MOVIES - Demonstration videos illustrating the use cases of each dataset.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Gallean</surname><given-names>B</given-names></name><name><surname>Laurent</surname><given-names>T</given-names></name><name><surname>Biasuz</surname><given-names>K</given-names></name><name><surname>Clement</surname><given-names>A</given-names></name><name><surname>Faraj</surname><given-names>N</given-names></name><name><surname>Lemaire</surname><given-names>P</given-names></name><name><surname>Faure</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>MorphoNet 2.0: An innovative approach for qualitative assessment and segmentation curation of large-scale 3D time-lapse imaging datasets</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.30529745.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by grants of the Occitanie Region (ESR-PREMAT-213) and by the French National Infrastructure France BioImaging (ANR-10-INBS-04) to EF, by the Cell-Whisper (ANR-19-CE13-0020), the scEmbryo-Mech (ANR-21-CE13-0046) ANR projects and the Fondation pour la Recherche Médicale (EQU202303016262) coordinated by PL. EF and PL were CNRS staff scientists. NF was an assistant professor at UM. KB was a UM Phd Student supported by the EpiGenMed Labex (ProjetIA-10-LABX-0012) and a post-doc funded by the scEmbryo-Mech project. BG,TL were contract CNRS engineers. AC was a UM Master student. We thank Christophe Godin, Vanessa Barone, Thibault de Villèle, and Volker Baecker for their valuable feedback and advice.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>TensorFlow: A system for large-scale machine learning</article-title><conf-name>12th USENIX1468 Symposium on Operating Systems Design and Implementation (OSDI 16)</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrés-San Román</surname><given-names>JA</given-names></name><name><surname>Gordillo-Vázquez</surname><given-names>C</given-names></name><name><surname>Franco-Barranco</surname><given-names>D</given-names></name><name><surname>Morato</surname><given-names>L</given-names></name><name><surname>Fernández-Espartero</surname><given-names>CH</given-names></name><name><surname>Baonza</surname><given-names>G</given-names></name><name><surname>Tagua</surname><given-names>A</given-names></name><name><surname>Vicente-Munuera</surname><given-names>P</given-names></name><name><surname>Palacios</surname><given-names>AM</given-names></name><name><surname>Gavilán</surname><given-names>MP</given-names></name><name><surname>Martín-Belmonte</surname><given-names>F</given-names></name><name><surname>Annese</surname><given-names>V</given-names></name><name><surname>Gómez-Gálvez</surname><given-names>P</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Escudero</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>CartoCell, a high-content pipeline for 3D image analysis, unveils cell morphology patterns in epithelia</article-title><source>Cell Reports Methods</source><volume>3</volume><elocation-id>100597</elocation-id><pub-id pub-id-type="doi">10.1016/j.crmeth.2023.100597</pub-id><pub-id pub-id-type="pmid">37751739</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barone</surname><given-names>V</given-names></name><name><surname>Tagua</surname><given-names>A</given-names></name><name><surname>Román</surname><given-names>J</given-names></name><name><surname>Hamdoun</surname><given-names>A</given-names></name><name><surname>Garrido-García</surname><given-names>J</given-names></name><name><surname>Lyons</surname><given-names>DC</given-names></name><name><surname>Escudero</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Local and global changes in cell density induce reorganisation of 3D packing in a proliferating epithelium</article-title><source>Development</source><volume>151</volume><elocation-id>dev202362</elocation-id><pub-id pub-id-type="doi">10.1242/dev.202362</pub-id><pub-id pub-id-type="pmid">38619327</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Kutra</surname><given-names>D</given-names></name><name><surname>Kroeger</surname><given-names>T</given-names></name><name><surname>Straehle</surname><given-names>CN</given-names></name><name><surname>Kausler</surname><given-names>BX</given-names></name><name><surname>Haubold</surname><given-names>C</given-names></name><name><surname>Schiegg</surname><given-names>M</given-names></name><name><surname>Ales</surname><given-names>J</given-names></name><name><surname>Beier</surname><given-names>T</given-names></name><name><surname>Rudy</surname><given-names>M</given-names></name><name><surname>Eren</surname><given-names>K</given-names></name><name><surname>Cervantes</surname><given-names>JI</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Beuttenmueller</surname><given-names>F</given-names></name><name><surname>Wolny</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Koethe</surname><given-names>U</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ilastik: interactive machine learning for (bio)image analysis</article-title><source>Nature Methods</source><volume>16</volume><fpage>1226</fpage><lpage>1232</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id><pub-id pub-id-type="pmid">31570887</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>TJ</given-names></name><name><surname>Bao</surname><given-names>Z</given-names></name><name><surname>Murray</surname><given-names>JI</given-names></name><name><surname>Araya</surname><given-names>CL</given-names></name><name><surname>Waterston</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>AceTree: a tool for visual analysis of <italic>Caenorhabditis elegans</italic> embryogenesis</article-title><source>BMC Bioinformatics</source><volume>7</volume><elocation-id>275</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2105-7-275</pub-id><pub-id pub-id-type="pmid">16740163</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cazorla</surname><given-names>C</given-names></name><name><surname>Munier</surname><given-names>N</given-names></name><name><surname>Morin</surname><given-names>R</given-names></name><name><surname>Weiss</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Sketchpose: learning to segment cells with partial annotations</article-title><source>Machine Learning for Biomedical Imaging</source><volume>3</volume><fpage>367</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.59275/j.melba.2025-f7b3</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Child</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1906">1906</year><article-title>The organization and cell-lineage of the ascidian egg</article-title><source>Science</source><volume>23</volume><fpage>340</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1126/science.23.583.340</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>J</given-names></name><name><surname>Newman-Smith</surname><given-names>E</given-names></name><name><surname>Kourakis</surname><given-names>MJ</given-names></name><name><surname>Miao</surname><given-names>Y</given-names></name><name><surname>Borba</surname><given-names>C</given-names></name><name><surname>Medina</surname><given-names>J</given-names></name><name><surname>Laurent</surname><given-names>T</given-names></name><name><surname>Gallean</surname><given-names>B</given-names></name><name><surname>Faure</surname><given-names>E</given-names></name><name><surname>Smith</surname><given-names>WC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A single oscillating proto-hypothalamic neuron gates taxis behavior in the primitive chordate Ciona</article-title><source>Current Biology</source><volume>33</volume><fpage>3360</fpage><lpage>3370</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.06.080</pub-id><pub-id pub-id-type="pmid">37490920</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Correia</surname><given-names>PL</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Stand-alone objective segmentation quality evaluation</article-title><source>EURASIP Journal on Advances in Signal Processing</source><volume>2002</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1155/S1110865702000707</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutler</surname><given-names>KJ</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Lo</surname><given-names>TW</given-names></name><name><surname>Rappez</surname><given-names>L</given-names></name><name><surname>Stroustrup</surname><given-names>N</given-names></name><name><surname>Brook Peterson</surname><given-names>S</given-names></name><name><surname>Wiggins</surname><given-names>PA</given-names></name><name><surname>Mougous</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation</article-title><source>Nature Methods</source><volume>19</volume><fpage>1438</fpage><lpage>1448</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01639-4</pub-id><pub-id pub-id-type="pmid">36253643</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dardaillon</surname><given-names>J</given-names></name><name><surname>Dauga</surname><given-names>D</given-names></name><name><surname>Simion</surname><given-names>P</given-names></name><name><surname>Faure</surname><given-names>E</given-names></name><name><surname>Onuma</surname><given-names>TA</given-names></name><name><surname>DeBiasse</surname><given-names>MB</given-names></name><name><surname>Louis</surname><given-names>A</given-names></name><name><surname>Nitta</surname><given-names>KR</given-names></name><name><surname>Naville</surname><given-names>M</given-names></name><name><surname>Besnardeau</surname><given-names>L</given-names></name><name><surname>Reeves</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Fagotto</surname><given-names>M</given-names></name><name><surname>Guéroult-Bellone</surname><given-names>M</given-names></name><name><surname>Fujiwara</surname><given-names>S</given-names></name><name><surname>Dumollard</surname><given-names>R</given-names></name><name><surname>Veeman</surname><given-names>M</given-names></name><name><surname>Volff</surname><given-names>J-N</given-names></name><name><surname>Roest Crollius</surname><given-names>H</given-names></name><name><surname>Douzery</surname><given-names>E</given-names></name><name><surname>Ryan</surname><given-names>JF</given-names></name><name><surname>Davidson</surname><given-names>B</given-names></name><name><surname>Nishida</surname><given-names>H</given-names></name><name><surname>Dantec</surname><given-names>C</given-names></name><name><surname>Lemaire</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>ANISEED 2019: 4D exploration of genetic data for an extended range of tunicates</article-title><source>Nucleic Acids Research</source><volume>48</volume><fpage>D668</fpage><lpage>D675</lpage><pub-id pub-id-type="doi">10.1093/nar/gkz955</pub-id><pub-id pub-id-type="pmid">31680137</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez</surname><given-names>R</given-names></name><name><surname>Das</surname><given-names>P</given-names></name><name><surname>Mirabet</surname><given-names>V</given-names></name><name><surname>Moscardi</surname><given-names>E</given-names></name><name><surname>Traas</surname><given-names>J</given-names></name><name><surname>Verdeil</surname><given-names>J-L</given-names></name><name><surname>Malandain</surname><given-names>G</given-names></name><name><surname>Godin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Imaging plant growth in 4D: robust tissue reconstruction and lineaging at cell resolution</article-title><source>Nature Methods</source><volume>7</volume><fpage>547</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1472</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franco-Barranco</surname><given-names>D</given-names></name><name><surname>Pastor-Tronch</surname><given-names>J</given-names></name><name><surname>González-Marfil</surname><given-names>A</given-names></name><name><surname>Muñoz-Barrutia</surname><given-names>A</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep learning based domain adaptation for mitochondria segmentation on EM volumes</article-title><source>Computer Methods and Programs in Biomedicine</source><volume>222</volume><elocation-id>106949</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2022.106949</pub-id><pub-id pub-id-type="pmid">35753105</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guignard</surname><given-names>L</given-names></name><name><surname>Fiúza</surname><given-names>U-M</given-names></name><name><surname>Leggio</surname><given-names>B</given-names></name><name><surname>Laussu</surname><given-names>J</given-names></name><name><surname>Faure</surname><given-names>E</given-names></name><name><surname>Michelin</surname><given-names>G</given-names></name><name><surname>Biasuz</surname><given-names>K</given-names></name><name><surname>Hufnagel</surname><given-names>L</given-names></name><name><surname>Malandain</surname><given-names>G</given-names></name><name><surname>Godin</surname><given-names>C</given-names></name><name><surname>Lemaire</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Contact area-dependent cell communication and the morphological invariance of ascidian embryogenesis</article-title><source>Science</source><volume>369</volume><elocation-id>eaar5663</elocation-id><pub-id pub-id-type="doi">10.1126/science.aar5663</pub-id><pub-id pub-id-type="pmid">32646972</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>A</given-names></name><name><surname>Petit</surname><given-names>M</given-names></name><name><surname>Refahi</surname><given-names>Y</given-names></name><name><surname>Cerutti</surname><given-names>G</given-names></name><name><surname>Godin</surname><given-names>C</given-names></name><name><surname>Traas</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Benchmarking of deep learning algorithms for 3D instance segmentation of confocal image datasets</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009879</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009879</pub-id><pub-id pub-id-type="pmid">35421081</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Laurent</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2025">2025a</year><data-title>MorphoNet_Unity</data-title><version designator="61010d9f">61010d9f</version><source>GitLab</source><ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/MorphoNet/morphonet_unity">https://gitlab.inria.fr/MorphoNet/morphonet_unity</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Laurent</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2025">2025b</year><data-title>MorphoNet_API</data-title><version designator="a1f72212">a1f72212</version><source>GitLab</source><ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/MorphoNet/morphonet_api">https://gitlab.inria.fr/MorphoNet/morphonet_api</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Laurent</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2025">2025c</year><data-title>MorphoNet_Lineage</data-title><version designator="b784523c">b784523c</version><source>GitLab</source><ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/MorphoNet/morphonet_lineage">https://gitlab.inria.fr/MorphoNet/morphonet_lineage</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leggio</surname><given-names>B</given-names></name><name><surname>Laussu</surname><given-names>J</given-names></name><name><surname>Carlier</surname><given-names>A</given-names></name><name><surname>Godin</surname><given-names>C</given-names></name><name><surname>Lemaire</surname><given-names>P</given-names></name><name><surname>Faure</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MorphoNet: an interactive online morphological browser to explore complex multi-scale data</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2812</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10668-1</pub-id><pub-id pub-id-type="pmid">31249294</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Jin</surname><given-names>Y</given-names></name><name><surname>Azizi</surname><given-names>E</given-names></name><name><surname>Blumberg</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Cellstitch: 3D cellular anisotropic image segmentation via optimal transport</article-title><source>BMC Bioinformatics</source><volume>24</volume><elocation-id>480</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-023-05608-2</pub-id><pub-id pub-id-type="pmid">38102537</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manni</surname><given-names>L</given-names></name><name><surname>Caicci</surname><given-names>F</given-names></name><name><surname>Anselmi</surname><given-names>C</given-names></name><name><surname>Vanni</surname><given-names>V</given-names></name><name><surname>Mercurio</surname><given-names>S</given-names></name><name><surname>Pennati</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Morphological study and 3D reconstruction of the larva of the ascidian halocynthia roretzi</article-title><source>Journal of Marine Science and Engineering</source><volume>10</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3390/jmse10010011</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maška</surname><given-names>M</given-names></name><name><surname>Ulman</surname><given-names>V</given-names></name><name><surname>Delgado-Rodriguez</surname><given-names>P</given-names></name><name><surname>Gómez-de-Mariscal</surname><given-names>E</given-names></name><name><surname>Nečasová</surname><given-names>T</given-names></name><name><surname>Guerrero Peña</surname><given-names>FA</given-names></name><name><surname>Ren</surname><given-names>TI</given-names></name><name><surname>Meyerowitz</surname><given-names>EM</given-names></name><name><surname>Scherr</surname><given-names>T</given-names></name><name><surname>Löffler</surname><given-names>K</given-names></name><name><surname>Mikut</surname><given-names>R</given-names></name><name><surname>Guo</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Allebach</surname><given-names>JP</given-names></name><name><surname>Bao</surname><given-names>R</given-names></name><name><surname>Al-Shakarji</surname><given-names>NM</given-names></name><name><surname>Rahmon</surname><given-names>G</given-names></name><name><surname>Toubal</surname><given-names>IE</given-names></name><name><surname>Palaniappan</surname><given-names>K</given-names></name><name><surname>Lux</surname><given-names>F</given-names></name><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Sugawara</surname><given-names>K</given-names></name><name><surname>Magnusson</surname><given-names>KEG</given-names></name><name><surname>Aho</surname><given-names>L</given-names></name><name><surname>Cohen</surname><given-names>AR</given-names></name><name><surname>Arbelle</surname><given-names>A</given-names></name><name><surname>Ben-Haim</surname><given-names>T</given-names></name><name><surname>Raviv</surname><given-names>TR</given-names></name><name><surname>Isensee</surname><given-names>F</given-names></name><name><surname>Jäger</surname><given-names>PF</given-names></name><name><surname>Maier-Hein</surname><given-names>KH</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Ederra</surname><given-names>C</given-names></name><name><surname>Urbiola</surname><given-names>A</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name><name><surname>Cunha</surname><given-names>A</given-names></name><name><surname>Muñoz-Barrutia</surname><given-names>A</given-names></name><name><surname>Kozubek</surname><given-names>M</given-names></name><name><surname>Ortiz-de-Solórzano</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The cell tracking challenge: 10 years of objective benchmarking</article-title><source>Nature Methods</source><volume>20</volume><fpage>1010</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1038/s41592-023-01879-y</pub-id><pub-id pub-id-type="pmid">37202537</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDole</surname><given-names>K</given-names></name><name><surname>Guignard</surname><given-names>L</given-names></name><name><surname>Amat</surname><given-names>F</given-names></name><name><surname>Berger</surname><given-names>A</given-names></name><name><surname>Malandain</surname><given-names>G</given-names></name><name><surname>Royer</surname><given-names>LA</given-names></name><name><surname>Turaga</surname><given-names>SC</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>In toto imaging and reconstruction of post-implantation mouse development at the single-cell level</article-title><source>Cell</source><volume>175</volume><fpage>859</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.09.031</pub-id><pub-id pub-id-type="pmid">30318151</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JI</given-names></name><name><surname>Bao</surname><given-names>Z</given-names></name><name><surname>Boyle</surname><given-names>TJ</given-names></name><name><surname>Waterston</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The lineaging of fluorescently-labeled <italic>Caenorhabditis elegans</italic> embryos with StarryNite and AceTree</article-title><source>Nature Protocols</source><volume>1</volume><fpage>1468</fpage><lpage>1476</lpage><pub-id pub-id-type="doi">10.1038/nprot.2006.222</pub-id><pub-id pub-id-type="pmid">17406437</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JI</given-names></name><name><surname>Bao</surname><given-names>Z</given-names></name><name><surname>Boyle</surname><given-names>TJ</given-names></name><name><surname>Boeck</surname><given-names>ME</given-names></name><name><surname>Mericle</surname><given-names>BL</given-names></name><name><surname>Nicholas</surname><given-names>TJ</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Sandel</surname><given-names>MJ</given-names></name><name><surname>Waterston</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Automated analysis of embryonic gene expression with cellular resolution in <italic>C. elegans</italic></article-title><source>Nature Methods</source><volume>5</volume><fpage>703</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1228</pub-id><pub-id pub-id-type="pmid">18587405</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najman</surname><given-names>L</given-names></name><name><surname>Schmitt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Watershed of a continuous function</article-title><source>Signal Processing</source><volume>38</volume><fpage>99</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/0165-1684(94)90059-0</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Nature Methods</collab></person-group><year iso-8601-date="2024">2024</year><article-title>Where imaging and metrics meet</article-title><source>Nature Methods</source><volume>21</volume><elocation-id>151</elocation-id><pub-id pub-id-type="doi">10.1038/s41592-024-02187-9</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cellpose 2.0: how to train your own model</article-title><source>Nature Methods</source><volume>19</volume><fpage>1634</fpage><lpage>1641</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01663-4</pub-id><pub-id pub-id-type="pmid">36344832</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: an imperative style, high-performance deep learning library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Scikit-learn: machine learning in python</data-title><source>Mach Learn PYTHON</source></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pinidiyaarachchi</surname><given-names>A</given-names></name><name><surname>Wählby</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>ICIAP</chapter-title><person-group person-group-type="editor"><name><surname>Roli</surname><given-names>F</given-names></name><name><surname>Vitulano</surname><given-names>S</given-names></name></person-group><source>Seeded Watersheds for Combined Segmentation and Tracking of Cells</source><publisher-name>Springer</publisher-name><fpage>336</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1007/11553595_41</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Refahi</surname><given-names>Y</given-names></name><name><surname>Zardilis</surname><given-names>A</given-names></name><name><surname>Michelin</surname><given-names>G</given-names></name><name><surname>Wightman</surname><given-names>R</given-names></name><name><surname>Leggio</surname><given-names>B</given-names></name><name><surname>Legrand</surname><given-names>J</given-names></name><name><surname>Faure</surname><given-names>E</given-names></name><name><surname>Vachez</surname><given-names>L</given-names></name><name><surname>Armezzani</surname><given-names>A</given-names></name><name><surname>Risson</surname><given-names>AE</given-names></name><name><surname>Zhao</surname><given-names>F</given-names></name><name><surname>Das</surname><given-names>P</given-names></name><name><surname>Prunet</surname><given-names>N</given-names></name><name><surname>Meyerowitz</surname><given-names>EM</given-names></name><name><surname>Godin</surname><given-names>C</given-names></name><name><surname>Malandain</surname><given-names>G</given-names></name><name><surname>Jönsson</surname><given-names>H</given-names></name><name><surname>Traas</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A multiscale analysis of early flower development in Arabidopsis provides an integrated view of molecular regulation and growth control</article-title><source>Developmental Cell</source><volume>56</volume><fpage>540</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1016/j.devcel.2021.01.019</pub-id><pub-id pub-id-type="pmid">33621494</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>CA</given-names></name><name><surname>Rasband</surname><given-names>WS</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>NIH Image to ImageJ: 25 years of image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>671</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2089</pub-id><pub-id pub-id-type="pmid">22930834</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>BE</given-names></name><name><surname>Tobin</surname><given-names>C</given-names></name><name><surname>Mjolsness</surname><given-names>E</given-names></name><name><surname>Meyerowitz</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Analysis of cell division patterns in the Arabidopsis shoot apical meristem</article-title><source>PNAS</source><volume>112</volume><fpage>4815</fpage><lpage>4820</lpage><pub-id pub-id-type="doi">10.1073/pnas.1502588112</pub-id><pub-id pub-id-type="pmid">25825722</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Napari: a multi-dimensional image viewer for python</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7276432">https://doi.org/10.5281/zenodo.7276432</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sophie</surname><given-names>T</given-names></name><name><surname>Mendieta-Serrano</surname><given-names>MA</given-names></name><name><surname>Chapa-y-Lazo, Juliet Chen</surname><given-names>B</given-names></name><name><surname>Saunders</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>CellMet: extracting 3D shape metrics from cells and tissues</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.10.11.617843v1</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stegmaier</surname><given-names>J</given-names></name><name><surname>Amat</surname><given-names>F</given-names></name><name><surname>Lemon</surname><given-names>WC</given-names></name><name><surname>McDole</surname><given-names>K</given-names></name><name><surname>Wan</surname><given-names>Y</given-names></name><name><surname>Teodoro</surname><given-names>G</given-names></name><name><surname>Mikut</surname><given-names>R</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Real-time three-dimensional cell segmentation in large-scale microscopy data of developing embryos</article-title><source>Developmental Cell</source><volume>36</volume><fpage>225</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.devcel.2015.12.028</pub-id><pub-id pub-id-type="pmid">26812020</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title><source>Nature Methods</source><volume>18</volume><fpage>100</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id><pub-id pub-id-type="pmid">33318659</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valindria</surname><given-names>VV</given-names></name><name><surname>Lavdas</surname><given-names>I</given-names></name><name><surname>Bai</surname><given-names>W</given-names></name><name><surname>Kamnitsas</surname><given-names>K</given-names></name><name><surname>Aboagye</surname><given-names>EO</given-names></name><name><surname>Rockall</surname><given-names>AG</given-names></name><name><surname>Rueckert</surname><given-names>D</given-names></name><name><surname>Glocker</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reverse classification accuracy: predicting segmentation performance in the absence of ground truth</article-title><source>IEEE Transactions on Medical Imaging</source><volume>36</volume><fpage>1597</fpage><lpage>1606</lpage><pub-id pub-id-type="doi">10.1109/TMI.2017.2665165</pub-id><pub-id pub-id-type="pmid">28436849</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergara</surname><given-names>HM</given-names></name><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Meechan</surname><given-names>KI</given-names></name><name><surname>Zinchenko</surname><given-names>V</given-names></name><name><surname>Genoud</surname><given-names>C</given-names></name><name><surname>Wanner</surname><given-names>AA</given-names></name><name><surname>Mutemi</surname><given-names>KN</given-names></name><name><surname>Titze</surname><given-names>B</given-names></name><name><surname>Templin</surname><given-names>RM</given-names></name><name><surname>Bertucci</surname><given-names>PY</given-names></name><name><surname>Simakov</surname><given-names>O</given-names></name><name><surname>Dürichen</surname><given-names>W</given-names></name><name><surname>Machado</surname><given-names>P</given-names></name><name><surname>Savage</surname><given-names>EL</given-names></name><name><surname>Schermelleh</surname><given-names>L</given-names></name><name><surname>Schwab</surname><given-names>Y</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name><name><surname>Tischer</surname><given-names>C</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Whole-body integration of gene expression and single-cell morphology</article-title><source>Cell</source><volume>184</volume><fpage>4819</fpage><lpage>4837</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2021.07.017</pub-id><pub-id pub-id-type="pmid">34380046</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Schmidt</surname><given-names>U</given-names></name><name><surname>Haase</surname><given-names>R</given-names></name><name><surname>Sugawara</surname><given-names>K</given-names></name><name><surname>Myers</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy</article-title><conf-name>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name><conf-loc>Snowmass Village, CO, USA</conf-loc><pub-id pub-id-type="doi">10.1109/WACV45572.2020.9093435</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willis</surname><given-names>L</given-names></name><name><surname>Refahi</surname><given-names>Y</given-names></name><name><surname>Wightman</surname><given-names>R</given-names></name><name><surname>Landrein</surname><given-names>B</given-names></name><name><surname>Teles</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>KC</given-names></name><name><surname>Meyerowitz</surname><given-names>EM</given-names></name><name><surname>Jönsson</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cell size and growth regulation in the <italic>Arabidopsis thaliana</italic> apical stem cell niche</article-title><source>PNAS</source><volume>113</volume><fpage>E8238</fpage><lpage>E8246</lpage><pub-id pub-id-type="doi">10.1073/pnas.1616768113</pub-id><pub-id pub-id-type="pmid">27930326</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolny</surname><given-names>A</given-names></name><name><surname>Cerrone</surname><given-names>L</given-names></name><name><surname>Vijayan</surname><given-names>A</given-names></name><name><surname>Tofanelli</surname><given-names>R</given-names></name><name><surname>Barro</surname><given-names>AV</given-names></name><name><surname>Louveaux</surname><given-names>M</given-names></name><name><surname>Wenzl</surname><given-names>C</given-names></name><name><surname>Strauss</surname><given-names>S</given-names></name><name><surname>Wilson-Sánchez</surname><given-names>D</given-names></name><name><surname>Lymbouridou</surname><given-names>R</given-names></name><name><surname>Steigleder</surname><given-names>SS</given-names></name><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Bailoni</surname><given-names>A</given-names></name><name><surname>Duran-Nebreda</surname><given-names>S</given-names></name><name><surname>Bassel</surname><given-names>GW</given-names></name><name><surname>Lohmann</surname><given-names>JU</given-names></name><name><surname>Tsiantis</surname><given-names>M</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name><name><surname>Schneitz</surname><given-names>K</given-names></name><name><surname>Maizel</surname><given-names>A</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Accurate and versatile 3D segmentation of plant tissues at cellular resolution</article-title><source>eLife</source><volume>9</volume><elocation-id>e57613</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57613</pub-id><pub-id pub-id-type="pmid">32723478</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106227.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Graña</surname><given-names>Martin</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Institut Pasteur de Montevideo</institution><country>Uruguay</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> work presents technical and conceptual advances with the release of MorphoNet 2.0, a versatile and accessible platform for 3D+T segmentation and analysis. The authors provide <bold>compelling</bold> evidence across diverse datasets, and the clarity of the manuscript together with the software's usability broadens its impact. Although the strength of some improvements is hard to fully gauge given sample complexity, the tool is a significant step forward that will likely impact many biological imaging fields.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106227.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This article presents Morphonet 2.0, a software designed to visualise and curate segmentations of 3D and 3D+t data. The authors demonstrate its capabilities on five published datasets, showcasing how even small segmentation errors can be automatically detected, easily assessed and corrected by the user. This allows for more reliable ground truths which will in turn be very much valuable for analysis and training deep learning models. Morphonet 2.0 offers intuitive 3D inspection and functionalities accessible to a non-coding audience, thereby broadening its impact.</p><p>Strengths:</p><p>The work proposed in this article is expected to be of great interest for the community, by enabling easy visualisation and correction of complex 3D(+t) datasets. Moreover, the article is clear and well written making MorphoNet more likely to be used. The goals are clearly defined, addressing an undeniable need in the bioimage analysis community. The authors use a diverse range of datasets, successfully demonstrating the versatility of the software.</p><p>We would also like to highlight the great effort that was made to clearly explain which type of computer configurations are necessary to run the different dataset and how to find the appropriate documentation according to your needs. The authors clearly carefully thought about these two important problems and came up with very satisfactory solutions.</p><p>Weaknesses:</p><p>Sometimes, it can be a bit difficult to assess the strength of the improvements made by the proposed methods, but this is not something the authors could easily address, given the great complexity of the samples</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106227.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gallean</surname><given-names>Benjamin</given-names></name><role specific-use="author">Author</role><aff><institution>University of Montpellier, CNRS</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Laurent</surname><given-names>Tao</given-names></name><role specific-use="author">Author</role><aff><institution>University of Montpellier, CNRS</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Biasuz</surname><given-names>Kilian</given-names></name><role specific-use="author">Author</role><aff><institution>University of Montpellier, CNRS</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Clement</surname><given-names>Ange</given-names></name><role specific-use="author">Author</role><aff><institution>University of Montpellier, CNRS</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Faraj</surname><given-names>Noura</given-names></name><role specific-use="author">Author</role><aff><institution>University of Montpellier, CNRS</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Lemaire</surname><given-names>Patrick</given-names></name><role specific-use="author">Author</role><aff><institution>CNRS-Université de Montpellier</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Faure</surname><given-names>Emmanuel</given-names></name><role specific-use="author">Author</role><aff><institution>Laboratoire d'Informatique, de Robotique et de Microélectronique de Montpellier</institution><addr-line><named-content content-type="city">Montpellier</named-content></addr-line><country>France</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>The authors present a substantial improvement to their existing tool, MorphoNet, intended to facilitate assessment of 3D+t cell segmentation and tracking results, and curation of high-quality analysis for scientific discovery and data sharing. These tools are provided through a user-friendly GUI, making them accessible to biologists who are not experienced coders. Further, the authors have re-developed this tool to be a locally installed piece of software instead of a web interface, making the analysis and rendering of large 3D+t datasets more computationally efficient. The authors evidence the value of this tool with a series of use cases, in which they apply different features of the software to existing datasets and show the improvement to the segmentation and tracking achieved.</p><p>While the computational tools packaged in this software are familiar to readers (e.g., cellpose), the novel contribution of this work is the focus on error correction. The MorphoNet 2.0 software helps users identify where their candidate segmentation and/or tracking may be incorrect. The authors then provide existing tools in a single user-friendly package, lowering the threshold of skill required for users to get maximal value from these existing tools. To help users apply these tools effectively, the authors introduce a number of unsupervised quality metrics that can be applied to a segmentation candidate to identify masks and regions where the segmentation results are noticeably different from the majority of the image.</p><p>This work is valuable to researchers who are working with cell microscopy data that requires high-quality segmentation and tracking, particularly if their data are 3D time-lapse and thus challenging to segment and assess. The MorphoNet 2.0 tool that the authors present is intended to make the iterative process of segmentation, quality assessment, and re-processing easier and more streamlined, combining commonly used tools into a single user interface.</p></disp-quote><p>We sincerely thank the reviewer for their thorough and encouraging evaluation of our work. We are grateful that they highlighted both the technical improvements of MorphoNet 2.0 and its potential impact for the broader community working with complex 3D+t microscopy datasets. We particularly appreciate the recognition of our efforts to make advanced segmentation and tracking tools accessible to non-expert users through a user-friendly and locally installable interface, and for pointing out the importance of error detection and correction in the iterative analysis workflow. The reviewer’s appreciation of the value of integrating unsupervised quality metrics to support this process is especially meaningful to us, as this was a central motivation behind the development of MorphoNet 2.0. We hope the tool will indeed facilitate more rigorous and reproducible analyses, and we are encouraged by the reviewer’s positive assessment of its utility for the community.</p><disp-quote content-type="editor-comment"><p>One of the key contributions of the work is the unsupervised metrics that MorphoNet 2.0 offers for segmentation quality assessment. These metrics are used in the use cases to identify low-quality instances of segmentation in the provided datasets, so that they can be improved with plugins directly in MorphoNet 2.0. However, not enough consideration is given to demonstrating that optimizing these metrics leads to an improvement in segmentation quality. For example, in Use Case 1, the authors report their metrics of interest (Intensity offset, Intensity border variation, and Nuclei volume) for the uncurated silver truth, the partially curated and fully curated datasets, but this does not evidence an improvement in the results. Additional plotting of the distribution of these metrics on the Gold Truth data could help confirm that the distribution of these metrics now better matches the expected distribution.</p><p>Similarly, in Use Case 2, visual inspection leads us to believe that the segmentation generated by the Cellpose + Deli pipeline (shown in Figure 4d) is an improvement, but a direct comparison of agreement between segmented masks and masks in the published data (where the segmentations overlap) would further evidence this.</p></disp-quote><p>We agree that demonstrating the correlation between metric optimization and real segmentation improvement is essential. We have added new analysis comparing the distributions of the unsupervised metrics with the gold truth data before and after curation. Additionally, we provided overlap scores where ground truth annotations are available, confirming the improvement. We also explicitly discussed the limitation of relying solely on unsupervised metrics without complementary validation.</p><disp-quote content-type="editor-comment"><p>We would appreciate the authors addressing the risk of decreasing the quality of the segmentations by applying circular logic with their tool; MorphoNet 2.0 uses unsupervised metrics to identify masks that do not fit the typical distribution. A model such as StarDist can be trained on the &quot;good&quot; masks to generate more masks that match the most common type. This leads to a more homogeneous segmentation quality, without consideration for whether these metrics actually optimize the segmentation</p></disp-quote><p>We thank the reviewer for this important and insightful comment. It raises a crucial point regarding the risk of circular logic in our segmentation pipeline. Indeed, relying on unsupervised metrics to select “good” masks and using them to train a model like StarDist could lead to reinforcing a particular distribution of shapes or sizes, potentially filtering out biologically relevant variability. This homogenization may improve consistency with the chosen metrics, but not necessarily with the true underlying structures.</p><p>We fully agree that this is a key limitation to be aware of. We have revised the manuscript to explicitly discuss this risk, emphasizing that while our approach may help improve segmentation quality according to specific criteria, it should be complemented with biological validation and, when possible, expert input to ensure that important but rare phenotypes are not excluded.</p><disp-quote content-type="editor-comment"><p>In Use case 5, the authors include details that the errors were corrected by &quot;264 MorphoNet plugin actions ... in 8 hours actions [sic]&quot;. The work would benefit from explaining whether this is 8 hours of human work, trying plugins and iteratively improving, or 8 hours of compute time to apply the selected plugins.</p></disp-quote><p>We clarified that the “8 hours” refer to human interaction time, including exploration, testing, and iterative correction using plugins.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>This article presents Morphonet 2.0, a software designed to visualise and curate segmentations of 3D and 3D+t data. The authors demonstrate their capabilities on five published datasets, showcasing how even small segmentation errors can be automatically detected, easily assessed, and corrected by the user. This allows for more reliable ground truths, which will in turn be very much valuable for analysis and training deep learning models. Morphonet 2.0 offers intuitive 3D inspection and functionalities accessible to a non-coding audience, thereby broadening its impact.</p><p>Strengths:</p><p>The work proposed in this article is expected to be of great interest to the community by enabling easy visualisation and correction of complex 3D(+t) datasets. Moreover, the article is clear and well written, making MorphoNet more likely to be used. The goals are clearly defined, addressing an undeniable need in the bioimage analysis community. The authors use a diverse range of datasets, successfully demonstrating the versatility of the software.</p><p>We would also like to highlight the great effort that was made to clearly explain which type of computer configurations are necessary to run the different datasets and how to find the appropriate documentation according to your needs. The authors clearly carefully thought about these two important problems and came up with very satisfactory solutions.</p></disp-quote><p>We would like to sincerely thank the reviewer for their positive and thoughtful feedback. We are especially grateful that they acknowledged the clarity of the manuscript and the potential value of MorphoNet 2.0 for the community, particularly in facilitating the visualization and correction of complex 3D(+t) datasets. We also appreciate the reviewer’s recognition of our efforts to provide detailed guidance on hardware requirements and access to documentation—two aspects we consider crucial to ensuring the tool is both usable and widely adopted. Their comments are very encouraging and reinforce our commitment to making MorphoNet 2.0 as accessible and practical as possible for a broad range of users in the bioimage analysis community.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>There is still one concern: the quantification of the improvement of the segmentations in the use cases and, therefore, the quantification of the potential impact of the software. While it appears hard to quantify the quality of the correction, the proposed work would be significantly improved if such metrics could be provided.</p><p>The authors show some distributions of metrics before and after segmentations to highlight the changes. This is a great start, but there seem to be two shortcomings: first, the comparison and interpretation of the different distributions does not appear to be trivial. It is therefore difficult to judge the quality of the improvement from these. Maybe an explanation in the text of how to interpret the differences between the distributions could help. A second shortcoming is that the before/after metrics displayed are the metrics used to guide the correction, so, by design, the scores will improve, but does that accurately represent the improvement of the segmentation? It seems to be the case, but it would be nice to maybe have a better assessment of the improvement of the quality.</p></disp-quote><p>We thank the reviewer for this constructive and important comment. We fully agreed that assessing the true quality improvement of segmentation after correction is a central and challenging issue. While we initially focused on changes in the unsupervised quality metrics to illustrate the effect of the correction, we acknowledged that interpreting these distributions was not always straightforward, and that relying solely on the metrics used to guide the correction introduced an inherent bias in the evaluation.</p><p>To address the first point, we revised the manuscript to provide clearer guidance on how to interpret the changes in metric distributions before and after correction, with additional examples to make this interpretation more intuitive.</p><p>Regarding the second point, we agreed that using independent, external validation was necessary to confirm that the segmentation had genuinely improved. To this end, we included additional assessments using complementary evaluation strategies on selected datasets where ground truth was accessible, to compare pre- and post-correction segmentations with an independent reference. These results reinforced the idea that the corrections guided by unsupervised metrics generally led to more accurate segmentations, but we also emphasized their limitations and the need for biological validation in real-world cases.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>A very thorough technical report of a new standalone, open-source software for microscopy image processing and analysis (MorphoNet 2.0), with a particular emphasis on automated segmentation and its curation to obtain accurate results even with very complex 3D stacks, including timelapse experiments.</p><p>Strengths:</p><p>The authors did a good job of explaining the advantages of MorphoNet 2.0, as compared to its previous web-based version and to other software with similar capabilities. What I particularly found more useful to actually envisage these claimed advantages is the five examples used to illustrate the power of the software (based on a combination of Python scripting and the 3D game engine Unity). These examples, from published research, are very varied in both types of information and image quality, and all have their complexities, making them inherently difficult to segment. I strongly recommend the readers to carefully watch the accompanying videos, which show (although not thoroughly) how the software is actually used in these examples.</p></disp-quote><p>We sincerely thanked the reviewer for their thoughtful and encouraging feedback. We were particularly pleased that the reviewer appreciated the comparative analysis of MorphoNet 2.0 with both its earlier version and existing tools, as well as the relevance of the five diverse and complex use cases we had selected. Demonstrating the software’s versatility and robustness across a variety of challenging datasets was a key goal of this work, and we were glad that this aspect came through clearly. We also appreciated the reviewer’s recommendation to watch the accompanying videos, which we had designed to provide a practical sense of how the tool was used in real-world scenarios. Their positive assessment was highly motivating and reinforced the value of combining scripting flexibility with an interactive 3D interface.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>Being a technical article, the only possible comments are on how methods are presented, which is generally adequate, as mentioned above. In this regard, and in spite of the presented examples (chosen by the authors, who clearly gave them a deep thought before showing them), the only way in which the presented software will prove valuable is through its use by as many researchers as possible. This is not a weakness per se, of course, but just what is usual in this sort of report. Hence, I encourage readers to download the software and give it time to test it on their own data (which I will also do myself).</p></disp-quote><p>We fully agreed that the true value of MorphoNet 2.0 would be demonstrated through its practical use by a wide range of researchers working with complex 3D and 3D+t datasets. In this regard, we improved the user documentation and provided a set of example datasets to help new users quickly familiarize themselves with the platform. We were also committed to maintaining and updating MorphoNet 2.0 based on user feedback to further support its usability and impact.</p><disp-quote content-type="editor-comment"><p>In conclusion, I believe that this report is fundamental because it will be the major way of initially promoting the use of MorphoNet 2.0 by the objective public. The software itself holds the promise of being very impactful for the microscopists' community.</p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) In Use Case 1, when referring to Figure 3a, they describe features of 3b?</p></disp-quote><p>We corrected the mismatch between Figure 3a and 3b descriptions.</p><disp-quote content-type="editor-comment"><p>(2) In Figure 3g-I, columns for Curated Nuclei and All Nuclei appear to be incorrectly labelled, and should be the other way around.</p></disp-quote><p>We corrected the label swapped between “Curated Nuclei” and “All Nuclei.”</p><disp-quote content-type="editor-comment"><p>(3) Some mention of how this will be supported in the future would be of interest.</p></disp-quote><p>We added a note on long-term support plans</p><disp-quote content-type="editor-comment"><p>(4) Could Morphonet be rolled into something like napari and integrated into its environment with access to its plugins and tools?</p></disp-quote><p>We thank the reviewer for this pertinent suggestion. We fully recognize the growing importance of interoperability within the bioimage analysis community, and we have been working on establishing a bridge between MorphoNet and napari to enable data exchange and complementary use of the two tools. As a platform, all new developments are first evaluated by our beta testers before being officially released to the user community and subsequently documented. The interoperability component is still under active development and will be announced shortly in a beta-testing phase. For this reason, we were not able to include it in the present manuscript, but we plan to document it in a future release.</p><disp-quote content-type="editor-comment"><p>(5) Can meshes be extracted/saved in another format?</p></disp-quote><p>We agreed that the ability to extract and save meshes in standard formats was highly useful for interoperability with other tools. We implemented this feature in the new version of MorphoNet, allowing users to export meshes in commonly used formats such as OBJ or STL. Response: We thank the reviewer for this pertinent suggestion. We fully recognize the growing importance of interoperability within the bioimage analysis community, and we have been working on establishing a bridge between MorphoNet and napari to enable data exchange and complementary use of the two tools. As a platform, all new developments are first evaluated by our beta testers before being officially released to the user community and subsequently documented. The interoperability component is still under active development and will be announced shortly in a beta-testing phase. For this reason, we were not able to include it in the present manuscript, but we plan to document it in a future release.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>As a comment, since the authors mentioned the recent progress in 3D segmentation of various biological components, including organelles, it could be interesting to have examples of Morphonet applied to investigate subcellular structures. These present different challenges in visualization and quantification due to their smaller scale.</p></disp-quote><p>We thank the reviewer for this insightful suggestion. We fully agree that applying MorphoNet 2.0 to the analysis of sub-cellular structures is a promising direction, particularly given the specific challenges these datasets present in terms of resolution, visualization, and quantification. While our current use cases focus on cellular and tissue-level segmentation, we are actively interested in extending the applicability of the tool to finer scales. We are currently exploring plugins for spot detection and curation in single-molecule FISH data. However, this requires more time to properly validate relevant use cases, and we plan to include this functionality in the next release.</p><disp-quote content-type="editor-comment"><p>Another comment is that the authors briefly mention two other state-of-the-art softwares (namely FIJI and napari) but do not really position MorphoNet against them. The text would likely benefit from such a comparison so the users can better decide which one to use or not.</p></disp-quote><p>We agreed that providing a clearer comparison between MorphoNet 2.0 and other widely used tools such as FIJI and Napari would greatly benefit readers and potential users. In response, we included a new paragraph in the supplementary materials of the revised manuscript, highlighting the main features, strengths, and limitations of each tool in the context of 3D+t segmentation, visualization, and correction workflows. This addition helped users better understand the positioning of MorphoNet 2.0 and make informed choices based on their specific needs.</p><disp-quote content-type="editor-comment"><p>Minor comments:</p><p>L 439: The Deli plugin is mentioned but not introduced in the main text; it could be helpful to have an idea of what it is without having to dive into the supplementary material.</p></disp-quote><p>We included a brief description in the main text and thoroughly revise the help pages to improve clarity</p><disp-quote content-type="editor-comment"><p>Figure 4: It is not clear how the potential holes created by the removal of objects are handled. Are the empty areas filled by neighboring cells, for example, are they left empty?</p></disp-quote><p>We clarified in the figure legend of Figure 4.</p><disp-quote content-type="editor-comment"><p>Please remove from the supplementary the use cases that are already in the main text.</p></disp-quote><p>We cleaned up redundant use case descriptions.</p><disp-quote content-type="editor-comment"><p>Typos:</p><p>L 22: the end of the sentence is missing.</p><p>L 51: There are two &quot;.&quot;</p><p>L 370: replace 'et' with 'and'.</p><p>L 407-408, Figure 3: panels g-i, the columns 'curated nuclei' and 'all nuclei' seem to be inverted.</p><p>L 549: &quot;four 4&quot;.</p><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>Dear Authors, what follows are &quot;minor comments&quot; (the only sort of comment I have for this nice report):</p><p>Minor issues:</p><p>(1) Not being a user of MorphoNet, I found that reading the manuscript was a bit hard due to the several names of plugins or tools that are mentioned, many times without a clear explanation of what they do. One way of improving this could be to add a table, a sort of glossary, with those names, a brief explanation of what they are, and a link to their &quot;help&quot; page on the web.</p></disp-quote><p>We understood that the manuscript might be difficult to follow for readers unfamiliar with MorphoNet, especially due to the numerous plugin and tool names referenced. To address this, we carried out a complete overhaul of the help pages to make them clearer, more structured, and easier to navigate.</p><disp-quote content-type="editor-comment"><p>(2) Figure 4d, orthogonal view: It is claimed that this segmentation is correct according to the original intensity image, but it is not clear why some cells in the border actually appear a lot bigger than other cells in the embryo. It does look like an incomplete segmentation due to the poor image quality at the border. Whether this is the case or if the authors consider the contrary, it should be somehow explained/discussed in the figure legend or the main text.</p></disp-quote><p>We revised the figure legend and main text to acknowledge the challenge of segmenting peripheral regions with low signal-to-noise ratios and discussed how this affects segmentation.</p><disp-quote content-type="editor-comment"><p>Small writing issues I could spot:</p><p>Line 247: there is a double point after &quot;Sup. Mat..&quot;.</p><p>Line 329: probably a diagrammation error of the pdf I use to review, there is a loose sentence apparently related to a figure: &quot;Vegetal view ofwith smoothness&quot;.</p><p>Line 393 (and many other places): avoid using numbers when it is not a parameter you are talking about, and the number is smaller than 10. In this case, it should be: &quot;The five steps...&quot;.</p><p>Line 459: Is &quot;opposite&quot; referring to &quot;Vegetal&quot;, like in g? In addition, it starts with lower lowercase.</p><p>Lines 540-541: Check if redaction is correct in &quot;...projected the values onto the meshed dual of the object...&quot; (it sounds obscure to me).</p><p>Lines 548-549: Same thing for &quot;...included two groups of four 4 nuclei and one group of 3 fused nuclei.&quot;.</p><p>Line 637: Should it be &quot;Same view as b&quot;?</p><p>Line 646: &quot;The property highlights...&quot;?</p><p>Line 651: In the text, I have seen a &quot;propagation plugin&quot; named as &quot;Prope&quot;, &quot;Propa&quot;, and now &quot;Propi&quot;. Are they all different? Is it a mistake? Please, see my first &quot;Minor issue&quot;, which might help readers navigate through this sort of confusing nomenclature.</p><p>Line 702: I personally find the use of the term &quot;eco-system&quot; inappropriate in this context. We scientists know what an ecosystem is, and the fact that it has now become a fashionable word for politicians does not make it correct in any context.</p></disp-quote><p>We thank the reviewer for their careful reading of the manuscript and for pointing out these writing and typographic issues. We corrected all the mentioned points in the revised version, including punctuation, sentence clarity, consistent naming of tools (e.g., the propagation plugin), and appropriate use of terms such as “ecosystem.” We also appreciated the suggestion to avoid numerals for numbers under ten when not referring to parameters, and we ensured consistency throughout the text. These corrections improved the clarity and readability of the manuscript, and we were grateful for the reviewer’s attention to detail.</p></body></sub-article></article>