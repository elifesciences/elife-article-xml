<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">25784</article-id><article-id pub-id-type="doi">10.7554/eLife.25784</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dynamic representation of partially occluded objects in primate prefrontal and visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-82297"><name><surname>Fyall</surname><given-names>Amber M</given-names></name><email>fyalla@uw.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-82298"><name><surname>El-Shamayleh</surname><given-names>Yasmine</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5396-2823</contrib-id><email>yasmine1@uw.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-82299"><name><surname>Choi</surname><given-names>Hannah</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8192-1121</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-95683"><name><surname>Shea-Brown</surname><given-names>Eric</given-names></name><email>estb@uw.edu</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-69859"><name><surname>Pasupathy</surname><given-names>Anitha</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3808-8063</contrib-id><email>pasupat@uw.edu</email><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of BIological Structure, Washington National Primate Research Center</institution><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Physiology and Biophysics, Washington National Primate Research Center</institution><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Applied Mathematics, University of Washington Institute for Neuroengineering</institution><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Applied Mathematics</institution><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of BIological Structure</institution><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-12465"><name><surname>Rust</surname><given-names>Nicole</given-names></name><role>Reviewing Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>19</day><month>09</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e25784</elocation-id><history><date date-type="received" iso-8601-date="2017-02-19"><day>19</day><month>02</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2017-08-24"><day>24</day><month>08</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Fyall et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Fyall et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-25784-v1.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e25784v1"><date date-type="v1" iso-8601-date="2017-09-19"><day>19</day><month>09</month><year>2017</year></date></related-object><abstract><object-id pub-id-type="doi">10.7554/eLife.25784.001</object-id><p>Successful recognition of partially occluded objects is presumed to involve dynamic interactions between brain areas responsible for vision and cognition, but neurophysiological evidence for the involvement of feedback signals is lacking. Here, we demonstrate that neurons in the ventrolateral prefrontal cortex (vlPFC) of monkeys performing a shape discrimination task respond more strongly to occluded than unoccluded stimuli. In contrast, neurons in visual area V4 respond more strongly to unoccluded stimuli. Analyses of V4 response dynamics reveal that many neurons exhibit two transient response peaks, the second of which emerges after vlPFC response onset and displays stronger selectivity for occluded shapes. We replicate these findings using a model of V4/vlPFC interactions in which occlusion-sensitive vlPFC neurons feed back to shape-selective V4 neurons, thereby enhancing V4 responses and selectivity to occluded shapes. These results reveal how signals from frontal and visual cortex could interact to facilitate object recognition under occlusion.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>neurophysiology</kwd><kwd>object representation and recognition</kwd><kwd>feedback signals</kwd><kwd>prefrontal cortex</kwd><kwd>visual area V4</kwd><kwd>partial occlusion</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001906</institution-id><institution>Washington Research Foundation</institution></institution-wrap></funding-source><award-id>Innovation Postdoctoral Fellowship in Neuroengineering</award-id><principal-award-recipient><name><surname>Choi</surname><given-names>Hannah</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DMS-1056125</award-id><principal-award-recipient><name><surname>Shea-Brown</surname><given-names>Eric</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01EY018839</award-id><principal-award-recipient><name><surname>Pasupathy</surname><given-names>Anitha</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>OD010425</award-id><principal-award-recipient><name><surname>Pasupathy</surname><given-names>Anitha</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>P30EY01730</award-id><principal-award-recipient><name><surname>Pasupathy</surname><given-names>Anitha</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Complementary neural codes in frontal and visual cortex support a role for feedback signals in the representation and recognition of partially occluded objects.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When an object is partially occluded, relevant sensory evidence available to the visual system is diminished, making the process of object recognition challenging. Nevertheless, primates are remarkably adept at recognizing partially occluded objects –a common occurrence in the natural world. The neural mechanisms that mediate this perceptual capacity are largely unknown and are the focus of this study.</p><p>Biologically inspired models of object recognition are often implemented as hierarchical, feedforward architectures (<xref ref-type="bibr" rid="bib32">Perrett and Oram, 1993</xref>; <xref ref-type="bibr" rid="bib44">Wallis and Rolls, 1997</xref>; <xref ref-type="bibr" rid="bib34">Riesenhuber and Poggio, 1999</xref>) despite extensive evidence for the role of feedback signaling in visual processing (<xref ref-type="bibr" rid="bib19">Lamme et al., 1998</xref>; <xref ref-type="bibr" rid="bib10">Gilbert and Li, 2013</xref>). These feedforward models, and even more elaborate schemes such as artificial convolutional neural networks, remain incapable of successfully recognizing partially occluded objects, although they can perform other object recognition tasks well (<xref ref-type="bibr" rid="bib47">Wyatte et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">Pepik et al., 2015</xref>). The failure of these models has been attributed to the exclusion of critical computations mediated by feedback signals (<xref ref-type="bibr" rid="bib48">Yuille and Kersten, 2006</xref>; <xref ref-type="bibr" rid="bib17">Kriegeskorte, 2015</xref>; <xref ref-type="bibr" rid="bib37">Rust and Stocker, 2010</xref>; <xref ref-type="bibr" rid="bib41">Tang and Kreiman, 2017</xref>). Indeed, more recent models that incorporate feedback signals show improved recognition performance for occluded objects (<xref ref-type="bibr" rid="bib29">O'Reilly et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Tang et al., 2014b</xref>). However, little is known about where the relevant feedback signals originate in higher cortex, where they terminate in visual cortex and how they contribute to the recognition of occluded objects. To provide new insights, we investigated the role of the prefrontal cortex in the representation of occluded objects, focusing on how responses in this area compare to responses in the visual cortex, and how frontal and visual cortical areas might interact to facilitate recognition performance.</p><p>The prefrontal cortex (PFC) plays an important role in cognitive control—the orchestration of thought and action in accordance with internal goals (<xref ref-type="bibr" rid="bib26">Miller and Cohen, 2001</xref>). Given its high-level function, it may seem unlikely that PFC would contribute to low-level visual representations and mediate the perception and recognition of occluded objects. However, anatomical studies demonstrate that a sub-region of PFC, the ventrolateral PFC (vlPFC), receives direct projections from visual cortical areas involved in higher form processing, that is V4 and inferotemporal cortex (IT) (<xref ref-type="bibr" rid="bib1">Barbas and Mesulam, 1985</xref>; <xref ref-type="bibr" rid="bib42">Ungerleider et al., 2008</xref>). The vlPFC also sends projections back to these visual areas (<xref ref-type="bibr" rid="bib28">Ninomiya et al., 2012</xref>). The existence of functional interactions between these areas is also supported by the demonstration of synchronous neural activity in the theta frequency range between lateral PFC and V4 during perceptual discrimination of visual stimuli (<xref ref-type="bibr" rid="bib22">Liebe et al., 2012</xref>) and by the engagement of PFC in perceptual processing under conditions of greater task difficulty (<xref ref-type="bibr" rid="bib13">Jiang and Kanwisher, 2003</xref>). Given the anatomical and physiological evidence for interactions between vlPFC and visual cortical areas, we hypothesized that vlPFC responses could contribute to the representation and recognition of objects when perceptual judgments are made more difficult by partial occlusion.</p><p>To test this hypothesis, we conducted neurophysiological recordings in rhesus monkeys while they discriminated partially occluded shapes. Based on these neuronal data, we addressed three questions. First, how do vlPFC neurons respond to partially occluded shapes compared to neurons in visual area V4? Second, are the response dynamics and tuning properties of V4 neurons consistent with the arrival of feedback signals from vlPFC? Third, is V4 neuronal discriminability for occluded shapes enhanced after the putative arrival of feedback signals from vlPFC?</p></sec><sec id="s2" sec-type="results"><title>Results</title><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.002</object-id><label>Figure 1.</label><caption><title>Behavioral task and monkey performance.</title><p>(<bold>A</bold>) On each trial, monkeys viewed two sequentially presented stimuli: an unoccluded shape (reference) and either an unoccluded or a partially occluded shape (test). Monkeys reported whether the two shapes were the same or different by making a saccade to the right or left choice target, respectively. (<bold>B</bold>) Monkey performance on example individual sessions (top) and average performance across all sessions (bottom). Performance was near 100% for unoccluded stimuli (100% visible area) and declined gradually for higher occlusion levels (<italic>i.e.</italic> &lt;100% visible area).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig1-v1"/></fig><sec id="s2-1"><title>Responses to partially occluded shapes in ventrolateral prefrontal cortex</title><p>To determine how vlPFC contributes to the representation and recognition of partially occluded objects, we studied single neuronal responses in monkeys performing a sequential shape discrimination task. In this task, monkeys reported whether two sequentially presented shapes, the ‘reference’ and ‘test’, were the same or different by making a saccade to one of two choice targets (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To test discrimination under occlusion, the test stimulus was partially occluded with a field of randomly positioned dots. The level of occlusion was titrated by varying dot diameter and was quantified as the percentage of the shape area that remained visible (% visible area). In each session, two shapes were chosen from a standard stimulus set (<xref ref-type="bibr" rid="bib30">Pasupathy and Connor, 2001</xref>; <xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>) to serve as the discriminanda. For both monkeys, task performance was high for unoccluded stimuli (100% visible area) and decreased gradually as the % visible area decreased (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) — that is as occlusion increased (gray arrow).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.003</object-id><label>Figure 2.</label><caption><title>Responses of example vlPFC neurons.</title><p>(<bold>A–B</bold>) Responses of one neuron to its preferred and non-preferred shape (compare <bold>A</bold> and <bold>B</bold>) at different occlusion levels (colors). Responses to unoccluded stimuli were weak (black; gray lines show standard error of the mean) whereas responses to occluded stimuli were stronger (colors). For PSTHs, σ was 10 ms. (<bold>C</bold>) Neuronal shape selectivity across time for unoccluded (black) and occluded (red) stimuli quantified by a sliding window ROC curve analysis (see Materials and methods). (<bold>D–E</bold>) Responses of a second vlPFC neuron, showing comparable responses to both shapes (compare <bold>D</bold> and <bold>E</bold>). Responses to unoccluded stimuli were weak whereas responses to occluded stimuli were stronger. The test stimulus was extinguished at 600 ms and the elevation in response beyond this time point was due to the saccadic response. (<bold>F</bold>) Neuronal shape selectivity across time (same format as in <bold>C</bold>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig2-v1"/></fig><p>We analyzed the responses of vlPFC neurons during the test stimulus epoch in which occlusion level was varied. Many neurons responded strongly to occluded stimuli and weakly to unoccluded stimuli. Data from two example neurons are shown (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The responses of the first example neuron demonstrate a preference for one of the two shapes used (compare <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>). For both the preferred and non-preferred shape, responses were stronger when the shapes were occluded (colored lines) than unoccluded (black lines). These responses were also more discriminable when the shapes were occluded: shape selectivity was stronger for occluded than unoccluded stimuli (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; see Materials and methods). The responses of the second example neuron were also stronger when the shapes were occluded than unoccluded (<xref ref-type="fig" rid="fig2">Figure 2D–F</xref>). However, this neuron showed no preference for either of the two shapes used; shape selectivity was therefore weak for occluded and unoccluded stimuli (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). The responses of this second example neuron are consistent with sensitivity for the total area or circumference of the occluding dots. In contrast, the responses of the first example neuron are inconsistent with this interpretation because the stronger responses to occluded stimuli were also accompanied by stronger shape selectivity.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.004</object-id><label>Figure 3.</label><caption><title>Population results for vlPFC neurons.</title><p>(<bold>A</bold>) Normalized average responses (80–230 ms) to preferred shapes across occlusion level (columns). Along the abscissa, % visible area is rank ordered; so each column could represent a different occlusion level for each neuron. From left to right, occlusion level decreases and % visible area increases, with 100% representing the unoccluded case. Neurons were tested with five or six occlusion levels (62/98 and 36/98 neurons, respectively), including the unoccluded case. Responses were normalized by the maximum response for each neuron, separately. For most neurons, responses were weak (blue) for unoccluded stimuli and stronger (yellow) for higher occlusion levels. Responses to the unoccluded stimuli were strongest for only 17/98 neurons. (<bold>B</bold>) Distribution of linear regression slopes fit to the responses in <bold>A</bold>. Most neurons (71/98) had negative slopes, indicating stronger responses to occluded stimuli; the median slope was −0.12. (<bold>C</bold>) Normalized average responses derived from the responses of occlusion-sensitive neurons to the preferred shapes at different occlusion levels. vlPFC neurons were studied at 5–6 oc clusion levels chosen from a set of 9 possible values (100, 99, 95, 90, 82, 72, 59, 45% and 27% visible area). The occlusion levels presented to each neuron was different, and not all neurons were studied at all the occlusion levels listed. The numbers of neurons contributing to each curve are listed in brackets (see Materials and methods). Data for the highest occlusion levels tested (45% and 27% visible area) are not shown because too few neurons contributed to the population average and the curves for these occlusion levels were highly variable. Across the population, responses were weakest to unoccluded stimuli (black) and stronger for intermediate and high occlusion levels. (<bold>D</bold>) Average shape selectivity as a function of occlusion level. Shape selectivity was weak for unoccluded stimuli (black) at all time points whereas it gradually increased for occluded shapes (colors), reaching maximal values between ~100–300 ms. Also see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.005</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Shape selectivity as a function of occlusion for different groups of vlPFC neurons.</title><p>(<bold>A</bold>) Neurons that are shape-selective but not occlusion-sensitive. (<bold>B</bold>) All shape-selective neurons. (<bold>C</bold>) All shape-selective and occlusion-sensitive neurons, that is neurons that carry any task-relevant information. (<bold>D</bold>) All visually-responsive neurons. Shape and occlusion sensitivity was assessed with a 2-way ANOVA (see Materials and methods for details). Shape selectivity was stronger for shape-selective than occlusion-sensitive neurons (compare panels <bold>A–B</bold> with <xref ref-type="fig" rid="fig3">Figure 3D</xref>) but across all neuronal groups, including neurons not significantly influenced by occlusion (in <bold>A</bold>), shape selectivity was strongest at intermediate occlusion levels.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig3-figsupp1-v1"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.006</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Shape selectivity for two groups of vlPFC neurons.</title><p>(<bold>A</bold>) Neurons that responded preferentially to stimuli at lower occlusion levels (higher % visible area). (<bold>B</bold>) Neurons that responded preferentially to stimuli at higher occlusion levels (lower % visible area). Neurons with a positive slope in <xref ref-type="fig" rid="fig3">Figure 3B</xref> contributed to the data in (<bold>A</bold>) whereas neurons with a negative slope contributed to the data in (<bold>B</bold>). Two patterns are evident. First, even among vlPFC neurons that responded more strongly to unoccluded stimuli, shape selectivity was not stronger for unoccluded stimuli compared to occluded stimuli. Second, vlPFC neurons that preferred higher occlusion levels also carried more information about the occluded shape.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig3-figsupp2-v1"/></fig></fig-group><p>Most of the vlPFC neurons we recorded responded more strongly to occluded stimuli (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Of 216 neurons that were visually responsive during the test stimulus epoch (see Materials and methods), 98 neurons (45%) were significantly modulated by occlusion level (2-way ANOVA, p&lt;0.05). The responses of most of these occlusion-sensitive neurons were stronger for higher occlusion levels (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For individual neurons, this observation manifested as a negative linear regression slope between % visible area and the average responses during the test epoch (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Of the 98 occlusion-sensitive neurons, 71 had a negative regression slope; 59 neurons had a slope that was significantly less than zero (p&lt;0.05). For the subset of occlusion-sensitive neurons, normalized responses were also stronger at higher occlusion levels (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The results were also qualitatively similar when all visually responsive vlPFC neurons were included.</p><p>Shape selectivity was stronger for occluded than unoccluded stimuli across the population of vlPFC neurons. For the subset of occlusion-sensitive vlPFC neurons, shape selectivity was strongest for occluded stimuli at intermediate occlusion levels (blue/green) and weakest for unoccluded stimuli (black) (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). This observation also held for the subset of shape-selective vlPFC neurons (N = 66; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>) and for all visually responsive neurons (N = 216; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D</xref>). Even for the small subset of vlPFC neurons that responded more strongly or equally well to unoccluded stimuli (27/98 neurons had a positive regression slope; 17/98 neurons had a slope significantly greater than zero, p&lt;0.05), shape selectivity was not stronger for unoccluded than occluded stimuli (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). Thus, the vlPFC neuronal population has stronger, more shape-selective responses to occluded than unoccluded stimuli.</p><p>In addition to showing a preference for occluded stimuli during the test epoch, the responses of many vlPFC neurons during this epoch also signaled whether the test and reference shapes were a match/nonmatch. Of 216 neurons, the responses of 42 had a significant main effect of match/nonmatch condition, and the responses of 65 other neurons had a significant interaction between shape and match/nonmatch condition (two-way ANOVA, p&lt;0.05).</p><p>The vlPFC results presented thus far differ markedly from what we and others have reported in the visual cortex regarding the representation of occluded and unoccluded objects. In monkey cortical areas V4 (<xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>) and IT (<xref ref-type="bibr" rid="bib16">Kovács et al., 1995</xref>) and in human occipitotemporal cortex (<xref ref-type="bibr" rid="bib39">Tang et al., 2014a</xref>), neuronal responses are strongest for unoccluded objects and neuronal shape selectivity declines gradually with increasing occlusion level. Thus, the strong responses and shape selectivity of vlPFC neurons for occluded stimuli cannot be inherited directly from visual cortex. Next, we examine how the signals in vlPFC compare to those in the visual cortex by analyzing neuronal response dynamics in V4 datasets collected previously (<xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>), recorded in the same monkeys while they performed the same behavioral task used in the vlPFC testing sessions.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.007</object-id><label>Figure 4.</label><caption><title>Responses of example V4 neurons.</title><p>(<bold>A</bold>–<bold>B</bold>) Responses (rasters and PSTHs) of one neuron to the preferred shape at different occlusion levels (colors). This neuron had only one transient response peak. Responses were strongest to the unoccluded stimulus (black; gray lines show standard error of the mean) and declined gradually with increasing occlusion level (i.e. lower % visible area). (<bold>C</bold>–<bold>D</bold>) Responses of another V4 neuron. This neuron had two transient response peaks (black and red horizontal bars in (<bold>D</bold>) with different dependence on occlusion level.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig4-v1"/></fig></sec><sec id="s2-2"><title>Responses to partially occluded shapes in visual area V4</title><p>If feedback signals originating in vlPFC contribute to V4 responses, their influence on V4 would be evident after the onset of vlPFC responses. Additionally, their influence would manifest in V4 as stronger responses for occluded than unoccluded stimuli, consistent with the vlPFC response properties described earlier. Many V4 neurons in our dataset did <italic>not</italic> show evidence of feedback modulation in their temporal response profiles. The responses of one such example neuron (<xref ref-type="fig" rid="fig4">Figure 4A–B</xref>) had a temporal response profile with a single transient response phase (i.e. peak) followed by a sustained response phase. During both the transient and sustained phases, responses were stronger for unoccluded than occluded stimuli, unlike what we observed in vlPFC. However, many other V4 neurons showed a different temporal profile with two transient response peaks – one early and one late – each of which showed a different dependency on occlusion level. The responses of one such example neuron (<xref ref-type="fig" rid="fig4">Figure 4C–D</xref>) had two transient response peaks: the first ~82 ms and the second ~150 ms after test stimulus onset. The neuron’s responses during the first peak (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, black bar) were strongest for the unoccluded stimulus and declined gradually with increasing occlusion level. In contrast, the neuron’s responses during the second peak (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, red bar) were strongest for intermediate occlusion levels.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.008</object-id><label>Figure 5.</label><caption><title>Responses of two additional example V4 neurons with two transient response peaks.</title><p>(<bold>A</bold>–<bold>B</bold>) PSTHs of responses to the preferred shape at different occlusion levels (colors; same format as in <xref ref-type="fig" rid="fig4">Figure 4B, D</xref>). Also see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig5-v1"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.009</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Additional examples of V4 neurons with two peaks.</title><p>For each neuron, there was a statistically significant increase in response at the time of the second peak relative to the preceding trough (t Test, p&lt;0.05, Bonferroni corrected).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig5-figsupp1-v1"/></fig></fig-group><p>The responses of two other V4 neurons with two peaks are shown (<xref ref-type="fig" rid="fig5">Figure 5</xref>). For one neuron (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), the first and second peaks occurred ~63 ms and ~191 ms after stimulus onset. For the other neuron (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), the first and second peak occurred ~66 ms and ~218 ms after stimulus onset. Additional examples of V4 neurons with two peaks are provided (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.010</object-id><label>Figure 6.</label><caption><title>Population results for V4 and vlPFC neurons.</title><p>(<bold>A</bold>–<bold>B</bold>) Population-level, normalized PSTHs for 30/85 V4 neurons that showed two transient response peaks (<bold>A</bold>) and for V4 neurons without two peaks (<bold>B</bold>), identified using an <italic>ad hoc</italic> algorithm (see Materials and methods). The numbers of neurons contributing to each curve are listed in brackets. (<bold>C</bold>) Distribution of the first (black) and second (red) response peak times for V4 neurons with two peaks. (<bold>D</bold>) Distribution of the latency of peak responses for occlusion-sensitive vlPFC neurons; median latency was 157 ms (diamond). The first and second response peaks in V4 typically occurred before and after the peak response of vlPFC neurons, respectively. Also see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig6-v1"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.011</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Schematic of the <italic>ad hoc</italic> peak finding algorithm used.</title><p>(<bold>A</bold>) We demonstrate this procedure for the responses of an example V4 neuron with two peaks (same data as in <xref ref-type="fig" rid="fig4">Figure 4B</xref>). (<bold>B</bold>) We constructed an average PSTH from the individual PSTHs to each occlusion level (% visible area &lt;100) that evoked strong responses. We then used a zero-crossing algorithm to identify local peaks (filled circles) and an intervening trough (open circles) in the average PSTH, and within 300 ms of test stimulus onset. Small peaks (&lt;50% of the initial transient response; that is y/z &lt; 0.5) and small trough-to-peak modulations (&lt;15% of the local peak magnitude, that is x/y &lt; 0.15) were rejected. For each putative second peak that met these criteria, we asked whether there was a statistically significant increase in response relative to the preceding trough. To do this, we conducted a paired t Test between the average activity in a 30 ms window centered on the putative second peak and on the preceding trough (p&lt;0.05, Bonferroni corrected). For this example neuron, there was one peak (labeled peak 2) that passed the criteria and t Test. (<bold>D</bold>) Example of neuronal responses where the local maxima in the average PSTH were rejected as candidates for second response peak because the candidate peak was too small relative to the first peak (top, y/z &lt; 0.5) or because the trough-to-peak modulation was too small (bottom, x/y &lt; 0.15). See <bold>B</bold> for schematic representations of x, y, and z.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig6-figsupp1-v1"/></fig></fig-group><p>We developed an <italic>ad hoc</italic> peak finding algorithm to identify V4 neurons with and without two transient response peaks (see Materials and methods; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). The algorithm detected the occurrence of two robust transient peaks separated by a sizeable intervening trough, and the results were vetted using statistical tests. Of 85 neurons, 30 neurons (35%; 14 neurons recorded in Monkey O and 16 neurons recorded in Monkey M) were classified as having two peaks (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and 55 neurons were classified as not having two peaks (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The second response peak was less striking when the responses were averaged across neurons (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) due to variability in second peak times for individual neurons (see also <xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref>). Across all V4 neurons with two peaks, the timing of the first and second peaks had a broad range, with a median of 84 ms and 214 ms, respectively (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In comparison, across all occlusion-sensitive vlPFC neurons, the peak response occurred later than in V4, 93–581 ms after test stimulus onset, with a median of 157 ms (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Thus, the median peak time in vlPFC straddled the median peak times of the first and second response peaks in V4.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.012</object-id><label>Figure 7.</label><caption><title>Comparison of V4 responses during the first and second peaks.</title><p>(<bold>A</bold>) Data from one example V4 neuron with two peaks (same neuron as in <xref ref-type="fig" rid="fig4">Figure 4D</xref>). Average responses (left ordinate) were measured within a 30 ms window centered on the first and second peaks, relative to the preceding baseline (85–115 ms before test stimulus onset). Difference in responses (right ordinate) between the first and second peaks (gray line) is shown. During the first peak (black), responses were strongest for the unoccluded shape (100% visible area) and declined for higher occlusion levels (&lt;100% visible area). During the second peak (red), responses were strongest for intermediate levels of occlusion. Response difference was an inverted U-shaped curve, with a negative value for the unoccluded stimulus and positive for all levels of occlusion. (<bold>B–C</bold>) Data for two other V4 neurons (same neurons as in <xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, respectively). (<bold>D</bold>) Population average of the difference in responses between first and second peak for neurons with (dark gray) and without (light gray) two peaks. Responses of each neuron were first normalized by the maximum across time and occlusion levels. Then, for each neuron and at each occlusion level, we measured the difference in response between two time points: during the initial transient (69–99 ms after test stimulus onset) and at a later time in the sustained response phase (199–229 ms after test stimulus onset). These time points are centered around the median peak time of the first and second peaks, respectively. For neurons with two peaks, the average response difference between first and second peaks followed a U-shaped curve, with positive values for intermediate levels of occlusion. For neurons without two peaks, the difference curve was flat and negative for all levels of occlusion. Asterisks mark occlusion levels for which the average response difference curve was significantly greater for neurons with two peaks (t Test, p&lt;0.05).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig7-v1"/></fig><p>If feedback signals from vlPFC contribute to V4 activity during the second response peak, we expect that V4 responses would differ in their dependence on occlusion level over time. We therefore assessed neuronal sensitivity to occlusion during the first and second response peaks in V4. Data from three example V4 neurons are shown (<xref ref-type="fig" rid="fig7">Figure 7A–C</xref>). For the first neuron (<xref ref-type="fig" rid="fig7">Figure 7A</xref>; same neuron as in <xref ref-type="fig" rid="fig4">Figure 4C–D</xref>), responses during the first peak (black) declined gradually as occlusion level increased. In contrast, responses during the second peak were strongest at intermediate occlusion levels. Thus, the difference in responses between the first and second peak (gray) was largest at intermediate occlusion levels. The two other example neurons showed similar results (<xref ref-type="fig" rid="fig7">Figure 7B–C</xref>; same neurons as in <xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, respectively): the difference in responses between the first and second peak was larger for occluded stimuli than unoccluded stimuli for both neurons.</p><p>We compared the first and second peak responses for V4 neurons with and without two peaks. For both groups of neurons, responses during the first peak (69–99 ms) declined gradually as occlusion level increased. There was no significant difference between neurons with and without two peaks in their responses during the first peak at any occlusion level (t Test, p&gt;0.1). However, later in the test stimulus epoch (199–229 ms), around the time of the second peak, the responses of the two groups of neurons had different trends. For neurons with two peaks, the difference in responses between the first and second peak was largest for intermediate occlusion levels and small for unoccluded stimuli and high occlusion levels (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, dark gray). Thus, the response difference curve had an inverted U shape, as seen for the example neurons (compare dark gray curves, <xref ref-type="fig" rid="fig7">Figure 7A–D</xref>). In contrast, for neurons without two peaks, the difference in responses between the two time points was small and similar in magnitude across all occlusion levels (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, light gray). The difference in responses between the first and second peaks was significantly greater for neurons with two peaks than other neurons at intermediate occlusion levels (compare curves in 7D; t Test, p&lt;0.05, asterisks). This finding is explained by the observation that V4 neurons without two peaks had responses that declined gradually over time, for all occlusion levels (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). In contrast, neurons with two peaks showed a relative increase in responses to occluded stimuli during the second peak (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), a pattern that mirrored the responses of occlusion-sensitive neurons in vlPFC.</p><fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.013</object-id><label>Figure 8.</label><caption><title>Dynamics of neuronal shape selectivity in V4.</title><p>(<bold>A</bold>–<bold>B</bold>) Time course of average shape selectivity for unoccluded (black) and occluded (colors) stimuli for V4 neurons with two peaks (<bold>A</bold>) and V4 neurons without two peaks (<bold>B</bold>). For unoccluded stimuli (black), the time course and magnitude of shape selectivity were similar for the two neuronal groups. For stimuli at intermediate occlusion levels (72–95% visible area), shape selectivity was stronger and reached a maximal value later for neurons with two peaks (compare corresponding colors in <bold>A</bold> and <bold>B</bold>, also see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). (<bold>C</bold>) For most neurons with two peaks, shape selectivity at the time of the second peak (ordinate) was stronger than during the first peak (abscissa). Shape selectivity for occluded stimuli was significantly higher during the second peak than during the first peak (t Test, p&lt;0.01). We computed shape selectivity using ROC analysis across all occlusion trials (i.e. % visible area &lt;100) based on spike counts in a 30 ms window centered around the times of the first and second peaks, respectively (also see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). (<bold>D–E</bold>) For neurons with two peaks, the time of the second peak (gray) is compared to the time of maximal shape selectivity (black and red), separately for unoccluded (black, <bold>D</bold>) and occluded stimuli (red, <bold>E</bold>). Shape selectivity for unoccluded stimuli reached a maximal value significantly earlier than the second peak (t Test, p&lt;0.01); this was not the case for occluded stimuli (t Test, p=0.98). Also see <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplements 3</xref>–<xref ref-type="fig" rid="fig8s6">6</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-v1"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.014</object-id><label>Figure 8—figure supplement 1.</label><caption><title>Comparison of shape selectivity under occlusion for V4 neurons with and without two peaks.</title><p>Each panel shows shape selectivity as a function of time for unoccluded stimuli (black) and for stimuli at different occlusion levels (colors). Data are shown separately for neurons with (solid) and without (dashed) two peaks. At intermediate occlusion levels (blue to yellow), shape selectivity was stronger and reached a maximal value later for V4 neurons with two peaks, close to the median of second peak times (~214 ms). These data are as in <xref ref-type="fig" rid="fig8">Figure 8A–B</xref>, and replotted to facilitate direct comparison between neuronal groups.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-figsupp1-v1"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.015</object-id><label>Figure 8—figure supplement 2.</label><caption><title>Relative response amplitude versus change in shape selectivity during the second peak for V4 neurons with two peaks.</title><p>(<bold>A</bold>) Change in shape selectivity (second peak – first peak, ordinate) is plotted against the relative response amplitude (second peak/first peak, abscissa; same as y/z, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>). For each peak, shape selectivity and response amplitude were measured during a 30 ms window centered on the second peak. Most neurons (24/30; black) showed an increase in shape selectivity during the second peak; this enhanced selectivity was observed even for neurons that responded more weakly during the second peak than the first, that is the relative amplitude of the second peak, y/z &lt; 1.0. (<bold>B</bold>) Relative amplitude during the second peak for preferred (abscissa, as in <bold>A</bold>) and non-preferred stimuli (ordinate). For most neurons with increased shape selectivity during the second peak (black), the relative amplitude of the second peak was larger for the preferred than non-preferred stimuli. Neurons with decreased shape selectivity during the second peak (gray) lie close to, or above the diagonal. Thus, shape selectivity increased during the second peak, in large part because the size of the second peak relative to the first peak was smaller for non-preferred than preferred stimuli.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-figsupp2-v1"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.016</object-id><label>Figure 8—figure supplement 3.</label><caption><title>Distribution of parameter values for the peak-finding algorithm.</title><p>The amplitude of the putative second peak relative to the initial transient (y/z, see inset) is plotted against the size of the trough-to-peak modulation (x/y) for 42 neurons that had a second peak with a sizable amplitude and trough-to-peak modulation (i.e. for neurons with y/z &gt; 0.5 and x/y &gt; 0.15; see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref> for details). Thirty neurons that also passed the statistical test are shown in black. Dotted lines indicate alternative threshold criteria for the peak amplitude and modulation parameters used in <xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref> to classify neurons as having two peaks.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-figsupp3-v1"/></fig><fig id="fig8s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.017</object-id><label>Figure 8—figure supplement 4.</label><caption><title>Dependence of observed V4 results on the peak-finding algorithm parameters.</title><p>Population PSTHs (<bold>A, C, E, G</bold>) and average shape selectivity plots (<bold>B, D, F, H</bold>) for neurons with and without two peaks based on alternative parameter choices. Threshold criteria were: y/z &gt; 0.75 and x/y &gt; 0.25 for (<bold>A</bold>–<bold>D</bold>) and y/z &gt; 1.0 and x/y &gt; 0.25 for (<bold>E</bold>–<bold>H</bold>) (see <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). In both cases, the second peak had to show a statistically significant increase in responses relative to the preceding trough (p&lt;0.05, Bonferroni corrected). In (<bold>A</bold>–<bold>D</bold>) and (<bold>E</bold>–<bold>H</bold>), 26/85 and 16/85 neurons were classified as having two peaks. In both cases, results were similar to those shown in <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig8">8</xref>. The median peak times for the second peak were 201 ms (<bold>A</bold>) and 196 ms (<bold>E</bold>). Shape selectivity was stronger and more delayed for neurons with two peaks.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-figsupp4-v1"/></fig><fig id="fig8s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.018</object-id><label>Figure 8—figure supplement 5.</label><caption><title>Analysis of V4 response dynamics across occlusion level.</title><p>(<bold>A</bold>) Responses of an example neuron with two peaks (same data as in <xref ref-type="fig" rid="fig4">Figure 4B</xref>). (<bold>B</bold>) Responses of an example neuron without two peaks (same data as in <xref ref-type="fig" rid="fig4">Figure 4D</xref>). (<bold>C</bold>–<bold>D</bold>) Response predictions of a linear model (dotted lines) and the residuals (solid lines), derived from the responses in (<bold>A</bold>–<bold>B</bold>), as described below. Responses to stimuli at different occlusion levels were modeled as gain modulations of responses to unoccluded stimuli (black lines) using the following equation. <inline-formula><mml:math id="inf1"><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi> <mml:mi/><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo> <mml:mi/><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo> <mml:mi/><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi> <mml:mi/><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> where <italic>r</italic> is the response as a function of occlusion level <italic>u</italic> and time <italic>t. k</italic> is a scalar value that specifies the response gain. For each occlusion level, we estimated <italic>k</italic> using a linear regression between the responses to occluded and unoccluded stimuli in the interval 65–150 ms. For the V4 neuron whose responses are shown in <bold>A</bold>, the predicted responses provide a good match to the observed responses and the residuals are low. This implies that the neuron’s temporal response profile is similar across occlusion levels up to a multiplicative scale factor, and that the dynamical response has a stereotypic trajectory with a single input parameter <italic>k(u)</italic>. This result suggests that the dependence of this neuron’s responses on occlusion level is governed by its earliest input signals and that this relationship is stationary during the test stimulus epoch. For the V4 neuron whose responses are shown in (<bold>B</bold>), the predicted responses do not provide a good match to the observed data. The residuals are large and positive: they reach a maximal value around 150 ms, and are strongest for intermediate occlusion levels. Thus, a good minimal description of the dynamical response trajectories can be obtained by including an additive term that is stronger for occluded than unoccluded stimuli. Specifically. <inline-formula><mml:math id="inf2"><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi> <mml:mi/><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo> <mml:mi/><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo> <mml:mi/><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi> <mml:mi/><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo> <mml:mi/><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi> <mml:mi/><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> where <italic>f(u,t)</italic> is given by the residual (in <bold>D</bold>). This result is consistent with the possibility that the additive input given by <italic>f(u,t)</italic> is derived from vlPFC inputs because <italic>f(u,t)</italic> emerges (~150 ms) after response onset in roughly half the PFC neurons, and that <italic>f(u,t)</italic> is stronger for responses to occluded than unoccluded stimuli as in vlPFC.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-figsupp5-v1"/></fig><fig id="fig8s6" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.019</object-id><label>Figure 8—figure supplement 6.</label><caption><title>Population results for neurons with stronger responses to occluded stimuli than expected from linear scaling (see <xref ref-type="fig" rid="fig8s5">Figure 8—figure supplement 5</xref>).</title><p>(<bold>A</bold>) Scatter plot of the sum of residuals (ordinate) derived from the predictions of the gain-modulation model versus the relative amplitude of the second peak (y/z, same as abscissa in <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>) for neurons with (black) and without (gray) two peaks. Neuronal classifications were based on the peak-finding algorithm. Of the neurons classified as having one transient response peak (gray), 43 neurons did not have a candidate second peak with a sizable amplitude and trough-to-peak modulation and are therefore assigned y/z = 0; 12 neurons that did have a candidate second peak with y/z &gt; 0.5, did not pass the statistical criterion. We fit the linear gain-modulation model as described in <xref ref-type="fig" rid="fig8s5">Figure 8—figure supplement 5</xref> and summed the residuals in the time window 45–600 ms after test stimulus onset, allowing for V4 response latency. For 43 neurons that had no candidate second peak in their responses, y/z = 0. For most neurons with two peaks (black dots), the sum of the residuals was &gt;0. The average sum of residuals (diamonds) was significantly greater (t Test, p&lt;0.05) for neurons with two peaks. (<bold>B</bold>) We examined the time course of the residuals for 25 neurons with the highest summed residuals, that is those neurons above the dashed line in (<bold>A</bold>). These are neurons with stronger responses to occluded stimuli than expected based on multiplicative scaling of the initial transient in response to unoccluded shapes. Of these 25 neurons, 17 were classified as having two peaks (by the peak-finding algorithm), a significantly greater proportion than expected by chance (Binomial test, p&lt;0.01). The average residual across these 25 neurons (white line) reached a maximal value at 208 ms, close to the median time of the second peak (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). (<bold>C</bold>–<bold>F</bold>) Population PSTHs and shape selectivity for the 25 neurons in <bold>B</bold>, showing similar results to those in <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig8">8</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig8-figsupp6-v1"/></fig></fig-group><p>To quantify how shape selectivity evolves during the test stimulus epoch, we examined average neuronal shape selectivity across time for unoccluded and occluded stimuli for V4 neurons with two peaks (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) and those without (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). For unoccluded stimuli (black lines), shape selectivity was similar in magnitude and time course for the two groups, reaching a maximum value at ~120 ms. For occluded stimuli (colored lines), shape selectivity was similar for the two groups (t Test, p=0.6) early in the test stimulus epoch, around the time of the first peak (69–99 ms). However, shape selectivity for neurons with two peaks was significantly stronger (t Test, p&lt;0.01) later in the test stimulus epoch, around the time of the second peak (199–229 ms). This is because for neurons with two peaks, shape selectivity for occluded stimuli increased over time and reached a maximal value closer to the time of the second peak (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>).</p><p>We demonstrate this enhanced shape selectivity for occluded stimuli later in the test stimulus epoch in two ways. First, we compared the magnitude of shape selectivity at different periods (<xref ref-type="fig" rid="fig8">Figure 8C</xref>, early and late). Second, we compared the timing of peak selectivity for occluded and unoccluded stimuli (<xref ref-type="fig" rid="fig8">Figure 8D–E</xref>). For neurons with two peaks, shape selectivity around the time of the second peak was significantly stronger than around the time of the first peak (t Test, p&lt;0.01; <xref ref-type="fig" rid="fig8">Figure 8C</xref>). This observation did not hold for neurons without two peaks (p=0.92) or for unoccluded stimuli for either group of neurons (p&gt;0.5). The timing of maximal shape selectivity for unoccluded stimuli occurred significantly earlier than the second peak (<xref ref-type="fig" rid="fig8">Figure 8D</xref>, median 131 vs. 214 ms, respectively; t Test, p&lt;0.01). In contrast, the timing of maximal shape selectivity for occluded stimuli occurred around the time of the second peak (<xref ref-type="fig" rid="fig8">Figure 8E</xref>, median 188 vs. 214 ms, respectively; t Test, p&gt;0.98),</p><p>The enhanced shape selectivity for occluded stimuli around the time of the second peak occurred even for neurons that had stronger responses during the first peak than the second peak (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2A</xref>). This finding suggests that response magnitude during the second peak does not fully account for the strength of shape selectivity. However, for neurons that had stronger shape selectivity during the second peak, the relative magnitude of responses during the second peak was larger for the preferred than non-preferred shapes (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2B</xref>). This differential enhancement of responses to preferred shapes serves to amplify shape selectivity during the second peak.</p><p>Given that we classified neurons based on an <italic>ad hoc</italic> algorithm with customized parameters (see Materials and methods and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), we sought to ensure that the findings did not depend on the choice of parameters used and that the algorithm did not yield false-positives. To address these concerns, we examined population results for neurons with and without two peaks using different choices of threshold parameters (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplements 3</xref>–<xref ref-type="fig" rid="fig8s4">4</xref>). Additionally, we developed a model-based procedure that was independent of the <italic>ad hoc</italic> peak finding algorithm to identify neurons whose responses to occluded stimuli were stronger than expected from a linear scaling of responses to unoccluded stimuli (<xref ref-type="fig" rid="fig8s5">Figure 8—figure supplements 5</xref>–<xref ref-type="fig" rid="fig8s6">6</xref>). We found good correspondence between the model-based and algorithm-based approaches in terms of the neurons identified as having two peaks. Population results generated using different parameter choices for the <italic>ad hoc</italic> algorithm and using the model-based procedure were remarkably similar to those presented earlier (<xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig8">8</xref>).</p><p>Collectively, these results support the hypothesis that occlusion-sensitive signals in vlPFC are relayed to V4 and that these feedback signals contribute to V4 responses during the second peak, enhancing neuronal selectivity for occluded shapes. These putative feedback signals may be well suited to enhance perceptual discriminability of partially occluded objects.</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.020</object-id><label>Figure 9.</label><caption><title>Model of V4–vlPFC interactions.</title><p>The two-layer dynamical network model includes two V4 units (1 and 2) and two vlPFC units (1 and 2). The V4 units prefer two different shapes. Each vlPFC unit receives inputs from both V4 units; the shape preference of a vlPFC unit is determined by the preference of the V4 unit that provides the stronger input. V4 units receive feedforward input from upstream visual areas denoted by <inline-formula><mml:math id="inf3"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and feedback from both vlPFC units denoted by <inline-formula><mml:math id="inf4"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></inline-formula>In this notation, <italic>i</italic> indexes the recipient unit and <italic>j</italic> indexes the sender unit. Feedback inputs are passed through a rectifying nonlinearity prior to arrival in V4. Each vlPFC unit receives feedforward inputs from both V4 units, denoted by (<inline-formula><mml:math id="inf5"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>), as well as a modulatory signal,<inline-formula><mml:math id="inf6"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>, that depend on occlusion level (for modeling details, see Materials and methods).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig9-v1"/></fig></sec><sec id="s2-3"><title>A model of V4–vlPFC interactions</title><p>To demonstrate the plausibility of feedback signals from vlPFC contributing to V4 responses to occluded stimuli, we constructed a two-layer dynamical model of V4 and vlPFC interactions (<xref ref-type="fig" rid="fig9">Figure 9</xref>; see Materials and methods). In this model, shape-selective V4 units send feedforward inputs to vlPFC units (<xref ref-type="fig" rid="fig9">Figure 9</xref>, light gray arrows). The shape preference of each vlPFC unit is inherited from the V4 unit which provides the strongest input. vlPFC units also receive a gain modulation signal that increases with increasing occlusion level (dashed box), imparting a preference for occluded stimuli that is not observed in the feedforward V4 inputs to vlPFC. Additionally, vlPFC units send feedback inputs onto V4 units (medium gray arrows) with connection strengths that are proportional to the feedforward signals from each V4 unit. Importantly, feedback signals from vlPFC first pass through a rectifying nonlinearity prior to their arrival in V4 (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, Materials and methods). The vlPFC feedback signals contribute to two key response features of the V4 units: a second transient response peak and a dynamic preference for occlusion level over the test stimulus epoch.</p><fig-group><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.021</object-id><label>Figure 10.</label><caption><title>Example model results.</title><p>(<bold>A</bold>) Input to a V4 unit, <inline-formula><mml:math id="inf7"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, when its preferred stimulus was presented at different occlusion levels (colors). The input was strongest for the unoccluded stimulus (black) and declined gradually with increasing occlusion level. (<bold>B</bold>) Responses of a vlPFC unit. Responses were weak for the unoccluded stimulus and stronger for occluded stimuli. (<bold>C</bold>) Responses of a V4 unit receiving feedback from the vlPFC unit (in <bold>B</bold>) showing two transient peaks. Also see <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplements 1</xref>–<xref ref-type="fig" rid="fig10s6">6</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-v1"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.022</object-id><label>Figure 10—figure supplement 1.</label><caption><title>Demonstrating the necessity of feedback, synaptic adaptation and gain mechanisms in the model.</title><p>(<bold>A</bold>) To demonstrate the necessity of feedback from vlPFC to V4, we set <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:math></inline-formula> for all <italic>i</italic> and <italic>j</italic> (see <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> in Methods and materials). With only feedforward connections from V4 to vlPFC, the responses of the V4 unit (left) did not show a second transient peak. (<bold>B</bold>) To demonstrate the necessity of synaptic adaptation on the feedforward connections from V4 to vlPFC, the feedforward connection weights from V4 to vlPFC (<inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>and <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> were set to be constant over time (see <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, Materials and methods). This resulted in ‘ringing’ and ‘blow up’ of V4 (left) and vlPFC (right) responses. (<bold>C</bold>) To demonstrate the necessity of gain-modulation, we set, <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, Materials and methods). In this case, vlPFC unit responses and V4 unit responses during the second peak resembled those during the first peak in V4: responses were strongest for unoccluded stimuli and declined with increasing occlusion level.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-figsupp1-v1"/></fig><fig id="fig10s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.023</object-id><label>Figure 10—figure supplement 2.</label><caption><title>Demonstrating the necessity of half-wave rectification in the model.</title><p>(<bold>A</bold>) When we excluded the thresholding and half-wave rectification on the feedback from vlPFC to V4 (see <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, Materials and methods), our simulations produced a large second peak in V4 unit responses both to preferred (left) and to non-preferred stimuli (middle). Thus, without rectification, the model fails to reproduce the enhanced shape selectivity for occluded stimuli during the second peak (right). (<bold>B</bold>) When thresholding and half-wave rectification are included, a large second peak is observed only in V4 unit responses to the preferred shape (left), and shape selectivity for the occluded stimuli is enhanced at the time of the second peak (right). To compute shape selectivity, we first generated Poisson spikes on 1000 trials, within a 30 ms window centered on the first and the second peaks for responses to both the preferred and non-preferred shapes. We then constructed spike-count distributions and used the area under the ROC curve to compute shape selectivity, as was done for the neuronal data (see Materials and methods).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-figsupp2-v1"/></fig><fig id="fig10s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.024</object-id><label>Figure 10—figure supplement 3.</label><caption><title>Effect of delaying the arrival of gain modulation signals to vlPFC in the model.</title><p>To consider the possibility that gain modulation signals carrying information about occlusion level (<inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig9">Figure 9</xref>) may be communicated via feedforward signals from IT cortex to vlPFC, we tested the effect of delays of 10 ms (<bold>A</bold>), 20 ms (<bold>B</bold>) and 50 ms (<bold>C</bold>) on the arrival of gain modulation signals relative to the arrival of feedforward shape-selective signals from V4. By increasing this delay, the peak time of vlPFC responses and the second peak in V4 were slightly delayed, but in all three cases, our simulations produced robust second peak in the V4 unit.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-figsupp3-v1"/></fig><fig id="fig10s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.025</object-id><label>Figure 10—figure supplement 4.</label><caption><title>Relationship between the strength of shape selectivity in vlPFC in the model and the magnitude of V4 second peak responses produced.</title><p>We varied the shape selectivity of vlPFC neurons by modifying the relative strengths of feedforward connections from the two V4 units (see parameters listed in <bold>A–C</bold>). For all three sets of parameters, V4 unit shape selectivity during the first peak (black) was strongest for the unoccluded shape and declined gradually for higher occlusions, as observed in the neuronal data. vlPFC shape selectivity (gray) was strongest for intermediate levels of occlusion, consistent with the neuronal data. vlPFC shape selectivity was weaker overall when the weights for the two V4 units were comparable (in <bold>C</bold>) and this resulted in a smaller second response peak in the V4 unit. However, in all three cases (<bold>A–C</bold>), V4 shape selectivity for the occluded shape was enhanced during the second peak. These simulations confirm that the model produces similar results for a range of vlPFC shape selectivity indices. In all cases, feedback connection strengths were proportional to feedforward connection strengths.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-figsupp4-v1"/></fig><fig id="fig10s5" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.026</object-id><label>Figure 10—figure supplement 5.</label><caption><title>Heterogeneity of V4 second peak response properties reproduced by the model.</title><p>(<bold>A</bold>) Varying feedback connection strength produces second response peaks of different magnitude; stronger feedback produced larger peaks. (<bold>B</bold>) Varying synaptic delay produces second response peaks at different delays relative to the first peak. These simulations confirm that our model can generate the diversity of responses observed in the neuronal data.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-figsupp5-v1"/></fig><fig id="fig10s6" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.25784.027</object-id><label>Figure 10—figure supplement 6.</label><caption><title>Population average of 258 V4 model units with two peaks.</title><p>We generated sets of parameters with varied feedback delays (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>-</mml:mo><mml:mn>80</mml:mn></mml:math></inline-formula>ms), feedforward synaptic strengths between V4 and PFC units sharing the same shape preferences (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn><mml:mo>-</mml:mo><mml:mn>0.6</mml:mn></mml:math></inline-formula>) and different shape preferences (<inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula>), and feedback synaptic strengths (<inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>-</mml:mo><mml:mn>1.3</mml:mn></mml:math></inline-formula>;<inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). Varying these parameters produced V4 model unit responses with heterogeneous peak magnitudes and peak times; averaging across these units produced population results that resemble the neuronal population data in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. Specifically, the second response peak was less striking than in the responses of individual V4 units (<xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref>) due to variability in second peak times for individual neurons consistent with neuronal data.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-25784-fig10-figsupp6-v1"/></fig></fig-group><p>To demonstrate model performance, we present data for a simulated V4 and vlPFC unit (<xref ref-type="fig" rid="fig10">Figure 10</xref>). In the model, feedforward input to the V4 unit is modulated both by shape and occlusion level (<xref ref-type="fig" rid="fig10">Figure 10A</xref>): it is strongest when the preferred shape is unoccluded and is progressively weaker for higher occlusion levels. This pattern is consistent with our V4 neuronal data and captures responses during the first peak for V4 neurons with two peaks, as well as the responses of V4 neurons without two peaks. Occlusion-dependent gain modulation of feedforward input from V4 produced vlPFC unit responses that were weak to unoccluded stimuli and stronger to occluded stimuli (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Furthermore, the V4 unit receiving feedback signals from vlPFC had two transient response peaks: one earlier and one later than the response peak of the vlPFC unit (<xref ref-type="fig" rid="fig10">Figure 10C</xref>). The V4 unit’s responses during the first peak were strongest for the unoccluded stimulus and declined with increasing occlusion level. In contrast, the V4 unit’s responses during the second peak were strongest at intermediate occlusion levels. Thus, the response dynamics generated by this V4–vlPFC interaction model successfully recapitulated the dynamics observed in our neuronal recordings (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig4">4</xref>).</p></sec><sec id="s2-4"><title>Parsimonious model construction</title><p>Our model was constructed to include only the minimal set of mechanisms that were needed to account for the main features of the neurophysiological data. We started with the simplest feedforward model composed of two V4 units and two vlPFC units, and we included four mechanisms to achieve the desired dynamics in V4 and vlPFC responses: (1) feedback from vlPFC to V4; (2) synaptic adaptation in the feedforward inputs from V4 to vlPFC; (3) half-wave rectification of feedback signals from vlPFC to V4; (4) occlusion-dependent gain modulation input to vlPFC. We included feedback from vlPFC to V4 because a network with only feedforward connections from V4 to vlPFC units cannot generate the second response peak in V4 units (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1A</xref>). Thus, in our model, feedback from vlPFC to V4 is necessary to reproduce the response dynamics observed in V4. Second, without synaptic adaptation on the feedforward connections from V4 to vlPFC (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, Materials and methods), the feedforward-feedback loop reinforces activity in V4 and PFC units positively (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1B</xref>). The resulting ‘ringing’ and ‘blow up’ in simulated model responses are inconsistent with the data, thus arguing for the inclusion of an adaptation mechanism. Indeed, such an adaptation mechanism is often used in models to soften positive feedback loops (e.g. <xref ref-type="bibr" rid="bib46">Wei and Wang, 2016</xref>). Third, without half-wave rectification of the feedback input from vlPFC to V4 units, the model produces a large second response peak even to presentation of non-preferred stimuli (<xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref>), which conflicts with the data (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2B</xref>). Thus, without half-wave rectification, the enhanced shape selectivity during the second peak in the V4 neuronal data (<xref ref-type="fig" rid="fig8">Figure 8C</xref>) is not reproduced by the model. Given that rectifying nonlinearities can occur when synaptic inputs are transformed into output spikes, the model suggests that feedback from vlPFC may arrive in V4 after passing through a synapse. Indeed, anatomical observations of disynaptic feedback connections between V4 and vlPFC exist (<xref ref-type="bibr" rid="bib28">Ninomiya et al., 2012</xref>). Fourth, when all other mechanisms are in place but the gain modulation is removed, vlPFC responses decrease with increasing occlusion level (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1C</xref>). Consequently, V4 shape selectivity under occlusion is not enhanced during the second peak. To consider the possibility that gain modulation of vlPFC may be mediated by signals from V4 or IT cortex, we verified that our simulations were unaffected by delays of up to ~50 ms in the arrival of gain modulation relative to the arrival of shape selective signals (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref>).</p></sec><sec id="s2-5"><title>Heterogeneity in model V4 and vlPFC responses</title><p>To generate the simulated responses shown (<xref ref-type="fig" rid="fig10">Figure 10</xref>), we chose model parameters that reproduced the response dynamics of example neurons (<xref ref-type="fig" rid="fig2">Figures 2A</xref>, <xref ref-type="fig" rid="fig4">4</xref> and <xref ref-type="fig" rid="fig5">5</xref>). However, V4 neurons show substantial diversity in the magnitude and timing of the second response peak (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). vlPFC neurons also show diversity in terms of their shape selectivity and the dependence of their responses on occlusion level. Therefore, we systematically varied the model parameters governing synaptic strengths and delays to generate diverse simulated response dynamics and patterns in V4 and vlPFC model units. We varied the relative strengths of the feedforward input from the two V4 units to vlPFC, and we verified that a second response peak was observed in the V4 unit responses even when vlPFC units are only weakly shape-selective (<xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref>). By varying the feedback connection strengths, and the synaptic delays between the two areas, we were able to generate a range of second peak magnitudes (<xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5A</xref>) and second peak times (<xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5B</xref>). Finally, the population average response across model V4 units (<xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref>) resembles the V4 population data (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>To determine the contributions of prefrontal cortex to the representation and recognition of partially occluded objects, we compared the response dynamics of vlPFC and V4 neurons in monkeys discriminating shapes in the presence and absence of occluders. Our study provides three new insights. First, neuronal responses in vlPFC are strongest for occluded stimuli and weaker for unoccluded stimuli, in contrast to neuronal responses in visual areas V4 and IT (<xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Kovács et al., 1995</xref>; <xref ref-type="bibr" rid="bib39">Tang et al., 2014a</xref>). Second, the responses of many V4 neurons have two transient peaks, the second of which emerges after the onset of vlPFC responses and shows a stronger preference for occluded stimuli. Third, neuronal shape selectivity for occluded stimuli in V4 is enhanced during the second transient peak. Our results support the hypothesis that feedback signals from vlPFC mediate V4 responses during the second transient peak and that these signals facilitate object recognition under occlusion.</p><sec id="s3-1"><title>Representation of occluded stimuli in vlPFC</title><p>Our results demonstrate that visual representations in vlPFC do not always mirror representations in visual cortex and suggest that vlPFC may play an important role in representing objects. We used different experimental approaches for the V4 and vlPFC recordings, but these methodological differences cannot account for differences in how V4 and vlPFC neurons represent occluded and unoccluded stimuli. In V4 recording sessions, but not in vlPFC sessions, we tailored the stimulus color and shape to the preferences of the neuron; this may explain the preponderance of V4 neurons that responded preferentially to unoccluded shapes in our dataset. Without tailoring stimuli we would expect a roughly equal proportion of neurons showing responses that increased and decreased with increasing occlusion level. We found, however, that 72% of vlPFC neurons responded preferentially to occluded shapes, a proportion that deviates significantly from the null hypothesis (binomial test, p&lt;0.01). Furthermore, because the visible difference between any two shapes declines with increasing occlusion level, we expect shape selectivity to decline regardless of whether we tailored visual stimuli. The enhanced shape selectivity we observed in vlPFC under occlusion defies this expectation.</p><p>The stronger responses to occluded stimuli in vlPFC cannot be attributed to neuronal preferences for the color of the occluding dots. We verified in control experiments that the preference for occluded stimuli was independent of dot color (data not shown). Given that many vlPFC neurons are selective for the shape of the occluded stimulus, it is unlikely that vlPFC responses <bold>solely</bold> reflect task difficulty level or attentional demands. If difficulty or attention could fully explain vlPFC responses, occlusion-sensitive neurons would not be shape-selective (i.e. the PSTHs in <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig2">Figure 2B</xref> would be identical). We also verified in control experiments that vlPFC neuronal responses were weaker when the occluding dots were in the same color as the background – an observation suggesting that the vlPFC responses we observed rely on explicit occlusion-related signals.</p><p>The dependence of vlPFC responses on occlusion level varied across neurons. The responses of some neurons increased gradually with increasing occlusion level whereas the responses of other neurons increased abruptly, even at the lowest occlusion levels. Further experiments are needed to determine whether neuronal sensitivity to occlusion is determined by feedforward inputs, by gating in vlPFC or by the difficulty of the perceptual discrimination.</p><p>We propose that vlPFC responses arise from the modulation of occlusion-dependent, shape-selective feedforward signals from V4 by another feedforward signal that is dependent only on the occlusion level. In our simple behavioral task, where the occluding dots have a different color than the occluded shapes, a neuron sensitive to the color and area of the occluding dots could signal the level of occlusion. Indeed, in one monkey performing the same behavioral task used in the current study, we found that the responses of many IT neurons were consistent with encoding the total area of the occluders (<xref ref-type="bibr" rid="bib27">Namima and Pasupathy, 2016</xref>). However, in the natural world, where there are multiple objects and the attributes of the occluders are not known <italic>a priori</italic>, identifying which object is occluded, and by how much, could be challenging. Extending our simple model to tackle more complex, naturalistic cases would likely require the incorporation of attention and memory processes.</p></sec><sec id="s3-2"><title>Implications for decision-making and recognition</title><p>To perform the sequential shape discrimination task used in the current study, the reference stimulus held in memory must be compared to the test stimulus on the screen. Given its role in working memory, the PFC is a plausible neural locus for this comparison (<xref ref-type="bibr" rid="bib9">Fuster, 1989</xref>; <xref ref-type="bibr" rid="bib14">Kim and Shadlen, 1999</xref>; <xref ref-type="bibr" rid="bib36">Romo and de Lafuente, 2013</xref>). Our results suggest, however, that the comparison of reference and test stimuli is unlikely to be implemented in vlPFC. We found stronger neuronal selectivity in vlPFC for occluded than unoccluded test stimuli. Thus, if behavioral performance depended on comparisons implemented in vlPFC, discriminability would be higher for occluded stimuli and lower for unoccluded stimuli – the opposite of the performance we observed (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The weak neuronal responses in vlPFC to unoccluded stimuli are consistent with a report of weak neuronal selectivity in this area for stimulus color in monkeys performing a color change detection task (<xref ref-type="bibr" rid="bib21">Lara and Wallis, 2014</xref>). Together, these findings challenge the notion that vlPFC activity mediates perceptual discriminations of form and color directly. The comparison of sensory representations could be implemented in other parts of the PFC or in sensory cortex, where signals correlated with monkeys’ behavioral decisions have been reported (<xref ref-type="bibr" rid="bib14">Kim and Shadlen, 1999</xref>; <xref ref-type="bibr" rid="bib8">Eskandar et al., 1992</xref>; <xref ref-type="bibr" rid="bib25">Miller and Desimone, 1994</xref>; <xref ref-type="bibr" rid="bib45">Wallis and Miller, 2003</xref>; <xref ref-type="bibr" rid="bib35">Romo and Salinas, 2003</xref>; <xref ref-type="bibr" rid="bib49">Zaksas and Pasternak, 2006</xref>; <xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>). The evidence for functional connectivity between V4 and lateral PFC during memory maintenance also supports the implementation of decision computations in visual cortex (<xref ref-type="bibr" rid="bib22">Liebe et al., 2012</xref>).</p><p>The finding that neuronal responses in vlPFC are stronger for occluded stimuli is consistent with two possibilities—a role for this area in decision-making and a role in the recognition of occluded objects. Given that the monkeys were required to report their perceptual judgments, it is possible that the vlPFC responses we recorded reflect this area’s engagement in facilitating decisions under limited sensory evidence. When shape selectivity in visual cortex is weakened by the presence of occlusion (<xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Kovács et al., 1995</xref>; <xref ref-type="bibr" rid="bib39">Tang et al., 2014a</xref>), decision-making becomes difficult. In this case, vlPFC feedback might serve to amplify weak signals expressly to facilitate decisions. In this regard, our results are consistent with vlPFC’s engagement in tasks of greater difficulty or cognitive demand (<xref ref-type="bibr" rid="bib6">Crittenden and Duncan, 2014</xref>). Importantly, when the task becomes difficult, rather than reflecting task difficulty per se, we propose that vlPFC responses amplify behaviorally relevant signals to facilitate perceptual decisions.</p><p>An alternative possibility is that the preference of vlPFC neurons for occluded stimuli may be related to the specific engagement of vlPFC in the recognition of occluded objects. Previous work has argued that the processing of complex visual scenes containing clutter and occlusions may be guided by higher cognitive, memory processes (<xref ref-type="bibr" rid="bib3">Cavanagh, 1991</xref>; <xref ref-type="bibr" rid="bib18">Kveraga et al., 2007</xref>). For example, image representations in early and mid-level areas of the ventral visual pathway may be relayed to higher processing stages where they are compared to stored representations of object prototypes, leading to the recognition of objects in the scene (<xref ref-type="bibr" rid="bib18">Kveraga et al., 2007</xref>). This recognition process may then guide the grouping of appropriate contours and regions, thereby facilitating object segmentation and scene understanding (<xref ref-type="bibr" rid="bib24">McDermott, 2004</xref>; <xref ref-type="bibr" rid="bib18">Kveraga et al., 2007</xref>). Our results are also broadly consistent with the possibility that vlPFC activity embodies a recognition signal that is fed back to V4 to refine object representations. Further experiments are needed to differentiate between the two alternative roles for vlPFC.</p></sec><sec id="s3-3"><title>Response dynamics in V4</title><p>Studies of object representation and recognition often consider spiking activity only within the first hundred milliseconds after stimulus onset (e.g. <xref ref-type="bibr" rid="bib12">Hung et al., 2005</xref>). The rationale for choosing this early temporal epoch for analysis is based on the argument that successful categorization can be achieved by feedforward processes alone (<xref ref-type="bibr" rid="bib43">VanRullen and Thorpe, 2001</xref>; <xref ref-type="bibr" rid="bib38">Serre et al., 2007</xref>). However, neuronal responses to visual stimuli depend not only on signals carried by feedforward connections but also by feedback and horizontal connections. Feedback and horizontal connections may modulate neuronal responses based on stimulus context and behavioral goals, and confer selectivity for more complex visual stimuli (<xref ref-type="bibr" rid="bib20">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib10">Gilbert and Li, 2013</xref>).</p><p>The relative contributions of feedforward, feedback and horizontal connections to neuronal responses are hard to disentangle experimentally, but examining the response dynamics provides useful insights. For example, recent studies comparing V4 and V1 response dynamics during contour grouping and scene segmentation tasks suggest that feedback from V4 to V1 enhances the representation of figures and suppresses the representation of backgrounds in V1 (<xref ref-type="bibr" rid="bib4">Chen et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Poort et al., 2012</xref>). Similarly, our results suggest that feedback from vlPFC to V4 enhances the representation of behaviorally relevant, occluded shapes in V4.</p><p>Our model simulations suggest that the enhancement of V4 shape selectivity could be mediated even by weakly tuned vlPFC neurons. This may be because we used only two discriminanda in each experimental session, thereby simplifying the object recognition problem. In this case, any given vlPFC neuron receiving differential input from the two subsets of V4 neurons that signal the two shape discriminanda could contribute to enhanced V4 shape selectivity. We cannot rule out the possibility that IT responses also contribute to V4 responses during the second transient peak. However, our IT recordings suggest this is unlikely because, as in V4, shape selectivity in IT is stronger for unoccluded than occluded stimuli (<xref ref-type="bibr" rid="bib27">Namima and Pasupathy, 2016</xref>). Thus, putative feedback from IT may not be well-suited for enhancing V4 shape selectivity at intermediate occlusion levels.</p><p>The V4 neurons in our data set showed a broad range of second peak times and peak magnitudes. The diversity in timing of the second peak is likely because V4 and vlPFC are connected via feedforward and feedback pathways that are direct and indirect, and these different pathways are expected to have different conduction times. It is also possible that feedback signals from vlPFC to V4 are carried by a sparse projection and then distributed more broadly across V4 via horizontal connections, resulting in longer delays (than expected for disynaptic transmission) between the peak of vlPFC responses and the second peak of V4 responses. The strength of connections between V4 and vlPFC is likely heterogeneous, which could explain the range of second peak magnitudes we documented. Our simulations demonstrate that even weak functional interactions between the two areas could result in a small, second response peak in V4 that may be undetectable in highly variable responses. Overall, the heterogeneous properties of the second response peak support the possibility that V4 neurons with and without two peaks lie along a continuum.</p><p>While our model simulations successfully reproduced the diversity of neuronal dynamics observed in V4 in terms of the amplitude and timing of the second transient response peak, this result was achieved by tweaking the parameters of response timing and connection strengths in different instantiations of the two-layer V4–vlPFC interaction model. We do not know whether a large set of interconnected neurons, each with a large number of incoming inputs, could exhibit diverse response dynamics despite differences in response timing and connection strengths. In such a network, ‘averaging’ across the many inputs each neuron receives could dampen diversity in the response dynamics. It is also possible that in the limiting case where the neural population is large in size, but the number of active incoming connections per neuron is relatively small, substantial variation in response dynamics could persist across neurons. A detailed study of the relationships between network connectivity, network size and neuronal dynamics would be useful to validate the proposed model.</p><p>We studied vlPFC and V4 neuronal responses in the same monkeys and using the same behavioral paradigm, thereby facilitating direct comparisons of measurements made in the two cortical areas. Nevertheless, our approach to studying vlPFC contributions to V4 had several limitations. First, we conducted V4 and vlPFC recordings in separate sessions so we cannot compare V4 and vlPFC activity on individual trials. Second, we used only two discriminanda in each behavioral session, so we do not know whether vlPFC neurons are sufficiently sensitive to shape information to mediate the recognition of occluded objects. Third, we did not instruct monkeys to report their behavioral decisions as soon as possible, so we cannot infer the precise epoch of V4 neural activity that mediates perceptual judgments. Specifically, we do not know whether V4 neuronal responses during the second transient peak, which have stronger shape selectivity, contributed to the monkeys’ perceptual decisions. Fourth, we do not know whether the engagement of vlPFC neurons in our study was contingent on the monkeys reporting their perceptual decisions, or whether these same neurons would also be engaged in the recognition of partially occluded objects in natural viewing conditions. We hope that future studies will answer these outstanding questions and probe the causal link between V4 and vlPFC activity using perturbations of neuronal activity.</p><p>Partial occlusions pose a major challenge to the successful recognition of visual objects because they reduce the evidence available to the brain. Recognizing partially occluded objects could require solving an ill-posed inverse problem (<xref ref-type="bibr" rid="bib11">Helmholtz, 1910</xref>; <xref ref-type="bibr" rid="bib48">Yuille and Kersten, 2006</xref>), one that lacks a unique solution because the retinal image of an occluded object is often compatible with multiple interpretations (e.g. see Bregman's B illusion, <xref ref-type="bibr" rid="bib2">Bregman, 1981</xref>). As a result, recognition must rely not only on information about the physical object but also on information about the occlusion, scene context and perceptual experience. Our results provide support for the hypothesis that feedback signals from vlPFC, which carry information about occlusions, contribute to object representation in V4 and to object recognition under occlusion. Other brain regions, for example IT cortex, are likely to be involved and should be studied. Future experiments are needed to reveal the detailed algorithms used by neurons and circuits to solve object recognition under occlusion.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and Methods</title><sec id="s4-1"><title>Experimental subjects</title><p>Two adult male rhesus macaques (<italic>Macaca mulatta</italic>) were prepared for neurophysiological recordings using sterile surgical procedures. For experiments in prefrontal cortex, recording chambers were centered over the principal sulcus and targeted the ventrolateral prefrontal cortex (vlPFC), located ventral to and along the caudal third of the principal sulcus. The stereotaxic, central coordinates of prefrontal recording chambers were derived based on structural MRI images for each animal, and were ~21 mm anterior of interaural zero and ~19 mm lateral to the midline. For experiments in visual cortex, recording chambers were centered on the dorsal surface along the prelunate gyrus and targeted area V4, extending between the lunate sulcus and the superior temporal sulcus. Recordings from the two areas were carried out serially in the same monkeys, starting with V4 then vlPFC. All animal procedures conformed to NIH guidelines and were approved by the Institutional Animal Care and Use Committee at the University of Washington.</p></sec><sec id="s4-2"><title>Neurophysiology</title><p>Extracellular recordings were performed using epoxy-coated tungsten microelectrodes (250 µm, FHC) lowered into cortex through an acute microdrive system (Gray Matter Research, 8-channel). Voltage signals were amplified and band-pass filtered (0.1–8 kHz) using a recording system (Plexon Systems, 16-channel). The waveforms of single units were isolated manually using spike-sorting software (Plexon Systems, Offline Sorter). The results reported in the current study are based on 381 vlPFC neurons (260 and 121 from Monkey M and Monkey O, respectively) and on 85 V4 neurons (41 from Monkey M and 44 from Monkey O, respectively). A subset of the V4 neurons (62 of 85 neurons) contributed to a previous study (see <xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>).</p></sec><sec id="s4-3"><title>Visual stimuli</title><p>Visual stimuli were presented on a calibrated CRT monitor (1600 x 1200 pixels; 97 Hz frame rate; 57 cm in front of the monkey). Stimuli were presented against an achromatic gray background of mean luminance 5.4 cd/m<sup>2</sup>. Stimulus onset and offset times were based on photodiode detection of synchronized pulses in one corner of the monitor. Stimulus presentation and behavioral events were controlled by custom software written in Python (Pype, originally developed by Jack Gallant and James Mazer; <xref ref-type="bibr" rid="bib23">Mazer, 2013</xref>). Eye position was monitored using a 1 kHz infrared eye-tracking system (Eyelink 1000; SR Research).</p></sec><sec id="s4-4"><title>Behavioral task</title><p>Monkeys performed a sequential shape discrimination task in the presence and absence of occluders (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each trial began with the presentation of a central point (0.1°), which the monkey had to fixate within a circular window of radius 0.75°. After acquiring fixation, two stimuli were presented: a ‘reference’ stimulus, followed by a ‘test’ stimulus. The reference stimulus was always an unoccluded, 2D shape. The test stimulus was a 2D shape that was unoccluded or partially occluded by a field of randomly positioned dots. Occlusion level was quantified as the percentage of the shape area that remained visible (‘% visible area’) and was titrated by varying the diameter of the occluding dots (for details, see <xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>). Each stimulus was presented for 600 ms, with an inter-stimulus interval of 200 ms between the reference and the test stimuli. Following a 50 ms delay, the fixation point was extinguished and two peripheral choice targets appeared (left and right dots, 6° eccentric; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The monkey reported whether the two shapes presented were the same or different via a saccade to the right or left target, respectively, and within 500 ms of target onset. The monkey received liquid reward for correct performance. In cases where the monkey broke fixation or failed to respond, the trial was repeated later in the session. Behavioral trials were separated by an inter-trial interval of 2 s.</p></sec><sec id="s4-5"><title>Approach to data collection</title><p>V4 data were collected by studying one neuron at a time and tailoring the shapes, occluding dots, colors and position of the test stimulus to the preferences of the neuron recorded. Based on preliminary characterizations (for details, see <xref ref-type="bibr" rid="bib15">Kosai et al., 2014</xref>) we chose two shapes as the discriminanda: one preferred and one non-preferred. The two shapes were presented in the neuron’s preferred color whereas the occluding dots were presented in a contrasting, non-preferred color. The reference stimulus was presented at central fixation whereas the test stimulus was presented at the center of the neuron’s RF. The vlPFC data were collected by studying several neurons simultaneously, an approach that precluded tailoring the shapes and occluding dots to the preferences of individual neurons. To equate behavioral task difficulty across V4 and vlPFC recording sessions, we chose, at random, stimulus parameters for each vlPFC recording session from among those used for V4 recording sessions.</p><p>Two shapes were used in each behavioral session, yielding four trial conditions per occlusion level (2 shapes x two behavioral outcomes). We studied each neuron’s responses to the two shapes under four or more occlusion levels, including the unoccluded case. In V4 recordings, we sampled 4–9 oc clusion levels (median = 6). In vlPFC recordings, we sampled 5–6 oc clusion levels (median = 5). We only included data from neurons tested with at least seven repeated presentations of each trial condition and of each occlusion level tested. The median number of repeats was 24 for V4 recordings and 15 for vlPFC recordings.</p></sec><sec id="s4-6"><title>Data analysis</title><sec id="s4-6-1"><title>Visually responsive and occlusion-sensitive vlPFC neurons</title><p>We identified visually responsive vlPFC neurons by comparing the firing rate during a 150 ms window, beginning 80 ms after test stimulus onset, to the firing rate during the fixation epoch before reference stimulus onset. Neuronal responses to the test stimulus were often transient (see <xref ref-type="fig" rid="fig2">Figure 2</xref>), motivating us to calculate firing rates in a 150 ms window rather than the full duration of stimulus presentation. The 80 ms offset was introduced to account for the visual response latency of neurons. Among 381 vlPFC neurons, 216 (142/260 in Monkey M and 74/121 in Monkey O) were significantly responsive during the test stimulus epoch (t Test, p&lt;0.01). All further data analyses were restricted to these visually responsive neurons (57% of vlPFC neurons recorded).</p><p>To assess whether neuronal responses to the test stimulus were modulated by shape and/or occlusion level, we conducted a 2-way ANOVA on activity during the same response window defined above, with stimulus shape and occlusion level as factors. Among the 216 visually responsive neurons, the responses of 98 neurons (71/260 in Monkey M and 27/121 in Monkey O) showed a significant dependence on occlusion level (p&lt;0.05) and the responses of 66 neurons showed a significant dependence on stimulus shape (p&lt;0.05).</p></sec><sec id="s4-6-2"><title>Shape selectivity</title><p>To examine the dynamics of neuronal shape selectivity, we performed a sliding-window Receiver Operating Characteristic (ROC) curve analysis on responses to the preferred and non-preferred shapes at each occlusion level. For V4 neurons, the preferred shape was that which evoked the largest average response across all occlusion levels. For vlPFC, because many neurons did not respond to unoccluded shapes, we computed the average response for each shape across all occlusion levels (i.e. visible area &lt;100%) during the test epoch, and identified the preferred shape as that which evoked the largest average response. At every time point (1 ms steps), we counted spikes in a centered window of duration 75 ms (for V4) or 150 ms (for vlPFC). We then assessed shape selectivity by computing the area under the ROC curve derived from the spike count distributions of responses to preferred and non-preferred shapes. Shape selectivity values ranged from 0.5 (unselective) to 1.0 (very selective). To identify the time of maximal shape selectivity for occluded stimuli (<xref ref-type="fig" rid="fig8">Figure 8D</xref>, red bars), we also computed shape selectivity as described above, pooling across all levels of occlusion tested for each neuron.</p></sec><sec id="s4-6-3"><title>Population response histograms</title><p>To generate population response histograms (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig6">6</xref>), we normalized the responses of each neuron to the maximum across all occlusion levels then averaged the data at each occlusion level for all neurons. We did not test all neurons at the same occlusion levels (for each neuron, we tested 4–9 occlusion levels), so the number of neurons contributing to the average histograms varied for each occlusion level; these numbers are listed in the figures. For both cortical areas, population response histograms for the occlusion conditions of 44% and 27% visible area were based on only a few neurons and were therefore excluded.</p></sec><sec id="s4-6-4"><title>Peak latency</title><p>To find the time to peak response for each neuron, we first constructed an average response histogram from the Gaussian-smoothed (σ = 10 ms) PSTHs across all occlusion levels. We then identified the time of maximal response between 50–600 ms after test stimulus onset. This temporal window allowed us to identify peaks associated with responses to the test stimulus rather than responses related to the preceding reference stimulus, memory delay or saccades that followed the test stimulus.</p></sec><sec id="s4-6-5"><title>Peak finding algorithm</title><p>To identify V4 neurons with two transient peaks in their responses to occluded shapes, we devised an <italic>ad hoc</italic> algorithm, described below. This procedure was designed to identify neurons with a robust second transient response peak that could not be attributed to small, noisy ripples in the response. For each neuron, we first constructed an average PSTH of its responses to the preferred shape at different occlusion levels, smoothed with a Gaussian function (σ = 10 ms). We only included occlusion levels that evoked a response that was at least 33% of the maximal response to the unoccluded preferred shape. We then used a zero-crossing algorithm to identify local peaks within 300 ms of stimulus onset. Small peaks (&lt;50% of the first transient peak) and small trough-to-peak modulation ratios (&lt;15% of local peak magnitude) were rejected as false positives (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for a schematic of the procedure and examples of rejection cases). For each putative peak that met the peak amplitude and modulation criteria, we asked whether there was a statistically significant response increase relative to the preceding trough. To assess statistical significance, we conducted a paired t-Test between single trial spike counts within a 30 ms window centered at the peak and at the preceding trough (p&lt;0.05, Bonferroni corrected). Of 85 V4 neurons, 43 had no robust peaks beyond the first transient that passed the peak amplitude and modulation criteria. Of the remaining 42 neurons, 30 had a second peak that showed a statistically significant response increase relative to the preceding trough; these neurons were classified as having two peaks. Specifically, 29/30 neuons had exactly one peak that qualified as the second response peak.</p></sec></sec><sec id="s4-7"><title>Dynamic network model</title><p>To evaluate whether interactions between vlPFC and V4 could account for the observed response dynamics, we constructed a network model (<xref ref-type="fig" rid="fig9">Figure 9</xref>). The model includes two stages of cortical processing that are intended to map onto areas vlPFC and V4. The V4 stage comprises two units (<inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), each selective for one of the shapes used in a testing session. The vlPFC stage also comprises two units (<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) that receive excitatory feedforward input from V4 units. Each vlPFC unit inherits a preference for stimulus shape from the V4 unit that provides the strongest input (e.g. <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> receives the strongest feedforward input from <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). In the model simulations presented here, feedforward and feedback connections strengths are proportional. However, we have verified that the results hold for a broad range of connection strengths, as long as each vlPFC unit sends stronger feedback input to the V4 unit which provides its dominant feedforward input.</p><p>All model parameters (listed in <xref ref-type="table" rid="table1">Table 1</xref>) were chosen to reproduce the response dynamics observed in the experimental data. In addition, we also varied synaptic weights and delays over a range of values (see <xref ref-type="table" rid="table2">Table 2</xref>) to compare the heterogeneity of model units to observed data (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplements 3</xref>–<xref ref-type="fig" rid="fig10s5">5</xref>).</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.028</object-id><label>Table 1.</label><caption><title>Parameters used for the model (fixed)</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Parameters</th><th valign="top">Values</th></tr></thead><tbody><tr><td valign="top"><inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td><td valign="top">50</td></tr><tr><td valign="top"><inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td><td valign="top">20</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula></p></td><td valign="top">20</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula></p></td><td valign="top">0</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf28"><mml:mi mathvariant="bold-italic">σ</mml:mi></mml:math></inline-formula></p></td><td valign="top">90</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf29"><mml:mi mathvariant="bold-italic">N</mml:mi></mml:math></inline-formula></p></td><td valign="top">2</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></p></td><td valign="top">100</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></p></td><td valign="top">30</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></p></td><td valign="top">10</td></tr><tr><td valign="top"><inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td><td valign="top">30</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.25784.029</object-id><label>Table 2.</label><caption><title>Parameters used for the model (varied)</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Parameters</th><th valign="top"><xref ref-type="fig" rid="fig10">Figure 10</xref>, <break/><xref ref-type="fig" rid="fig10s2">Figure 10—figure supplements 1,2</xref></th><th colspan="3" valign="top"><xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref></th><th valign="top"><xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref></th><th valign="top"><xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref></th></tr></thead><tbody><tr><td valign="top"><inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td><td valign="top">40</td><td colspan="3" valign="top">40</td><td valign="top">40</td><td valign="top">40</td></tr><tr><td valign="top"><inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="bold-italic"/><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td><td valign="top">40</td><td colspan="3" valign="top">40</td><td valign="top">20, 40, 80</td><td valign="top">20–80</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></p></td><td valign="top">0.5</td><td valign="top">0.7</td><td valign="top">0.6</td><td valign="top">0.5</td><td valign="top">0.5</td><td valign="top">0.4–0.6</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></p></td><td valign="top">0.2</td><td valign="top">0</td><td valign="top">0.1</td><td valign="top">0.2</td><td valign="top">0.2</td><td valign="top">0–0.2</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></p></td><td valign="top">1.3</td><td colspan="3" valign="top">1</td><td valign="top">0.2, 0.8, 1.3, 1.5</td><td valign="top">0.1–1.3</td></tr><tr><td valign="top"><p><inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></p></td><td colspan="6" valign="top"><p><inline-formula><mml:math id="inf40"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></p></td></tr></tbody></table></table-wrap><p>We modeled the dynamical firing rate response of each model V4 unit, <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>and the firing rate response of each model vlPFC unit, <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, as:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p> where <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote the time constants of the responses; <italic>t</italic> denotes time; <inline-formula><mml:math id="inf45"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>is a nonlinear function of the Naka Rushton form given by:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p> where <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the firing rate thresholds, and <inline-formula><mml:math id="inf48"><mml:mi>η</mml:mi></mml:math></inline-formula> is a Gaussian white noise term with a standard deviation of <inline-formula><mml:math id="inf49"><mml:mn>300</mml:mn><mml:mi>*</mml:mi><mml:msqrt><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msqrt></mml:math></inline-formula> and a timestep <inline-formula><mml:math id="inf50"><mml:mi>d</mml:mi><mml:mi>t</mml:mi> <mml:mi/><mml:mo>=</mml:mo> <mml:mi/><mml:mn>0.01</mml:mn> <mml:mi/><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. We omitted the noise term for some simulations (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplements 1</xref>–<xref ref-type="fig" rid="fig10s6">6</xref>). Note that the precise form of the nonlinear function <inline-formula><mml:math id="inf51"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is not critical; any monotonically increasing nonlinear function with saturation and threshold, along with the dynamics of firing rates defined in <xref ref-type="disp-formula" rid="equ1 equ2">Equations (1) and (2)</xref> provide a standard firing rate model (<xref ref-type="bibr" rid="bib7">Dayan and Abbott, 2005</xref>).</p><p>For V4 model units, the input <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>was the sum of two sources: (i) excitatory feedforward input from upstream visual areas, <inline-formula><mml:math id="inf53"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, and (ii) excitatory feedback inputs from vlPFC, <inline-formula><mml:math id="inf54"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The feedforward input <inline-formula><mml:math id="inf56"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> confers shape selectivity to the V4 model units and a dependence of their responses on occlusion level (<xref ref-type="fig" rid="fig9">Figure 9</xref>). For the preferred stimulus, this input is strong and declines gradually with increasing occlusion level. For the non-preferred shape, this input is weak, as is the modulatory influence of occlusion level. The feedforward input, <inline-formula><mml:math id="inf57"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, was constructed by first convolving a difference of Gaussian filter <inline-formula><mml:math id="inf58"><mml:mi>k</mml:mi></mml:math></inline-formula> (the standard kernel normalized difference of <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>⁡</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>30</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>800</mml:mn></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>⁡</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>50</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>800</mml:mn></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:math></inline-formula>)) with a <inline-formula><mml:math id="inf61"><mml:mn>500</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula>-long ramp <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> followed by cubing, normalization and half-wave rectification:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The ramp function (R), defined separately for the preferred (i = 1) and nonpreferred (i = 2) shapes, increases monotonically with the percentage of visible area (c) and declines over time with a support of 500 ms, that is<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2.5</mml:mn><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mn>20</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.05</mml:mn><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>12</mml:mn><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>120</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>0.05</mml:mn><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>when <inline-formula><mml:math id="inf63"><mml:mn>30</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mn>530</mml:mn></mml:math></inline-formula>. <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>is <inline-formula><mml:math id="inf65"><mml:mn>0</mml:mn></mml:math></inline-formula> otherwise.</p><p><xref ref-type="disp-formula" rid="equ4 equ5">Equations 4 and 5</xref> were designed to simulate the input to V4 units (e.g. <xref ref-type="fig" rid="fig10">Figure 10A</xref>) with an onset latency of 30 ms, a strong initial transient response, a gradually declining sustained response, collectively lasting ~500 ms. Note that the precise function defining <inline-formula><mml:math id="inf66"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is not critical as long as it produces strong input signals for the preferred shape that decrease with increasing occlusion level, thus capturing the observed V4 neuronal response properties.</p><p>For the vlPFC units, the input, <inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the excitatory feedforward inputs from both V4 units, <inline-formula><mml:math id="inf68"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. In addition, the vlPFC units receive a gain modulation signal, <inline-formula><mml:math id="inf70"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, that is proportional to the occlusion level. We modeled <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as a nonlinear, cubic function of the % visible area,<inline-formula><mml:math id="inf72"><mml:mi>c</mml:mi></mml:math></inline-formula>. The function’s output was lowest for the unoccluded shape (<inline-formula><mml:math id="inf73"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mi>%</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and increased for higher occlusion levels (<xref ref-type="fig" rid="fig9">Figure 9</xref>, <inline-formula><mml:math id="inf74"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>). The coefficients were fit so that the model responses closely resembled the neuronal data, but the qualitative results were independent of the coefficient values used:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.0017</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>0.39</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>29.6</mml:mn><mml:mo>⋅</mml:mo><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mn>806</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Inputs between model units were modulated by connection weights: <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>for the stronger feedforward inputs from V4 units to vlPFC units of the same shape preference (e.g. V4 unit 1→ vlPFC unit 1), <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the corresponding feedback inputs (e.g. vlPFC unit 1→ V4 unit 1), <inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the weaker feedforward inputs from V4 units to vlPFC units of a different shape preference (e.g. V4 unit 1→ vlPFC unit 2), and <inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the corresponding feedback inputs (e.g. vlPFC unit 1→ V4 unit 2). Thus, the feedback input from vlPFC unit <inline-formula><mml:math id="inf79"><mml:mi>j</mml:mi></mml:math></inline-formula> onto V4 unit <inline-formula><mml:math id="inf80"><mml:mi>i</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf81"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, was implemented as follows:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the responses of vlPFC units were thresholded <inline-formula><mml:math id="inf82"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and half-wave rectified. This threshold on vlPFC firing rates was introduced to reduce the magnitude of the second transient peak in V4 unit responses to the non-preferred shape (see <xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref>).</p><p>The feedforward excitatory input from V4 unit <inline-formula><mml:math id="inf83"><mml:mi>j</mml:mi></mml:math></inline-formula> to vlPFC unit <inline-formula><mml:math id="inf84"><mml:mi>i</mml:mi></mml:math></inline-formula> was implemented as:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The feedforward and feedback temporal delays between vlPFC and V4 unit responses, <inline-formula><mml:math id="inf85"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf86"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were chosen to be consistent with the difference in time between the vlPFC and V4 response peaks observed in our neuronal data.</p><p>To prevent the second response peak of V4 units from inducing a second response peak in vlPFC units (see <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1B</xref>), the feedforward connections from V4 to vlPFC included an adaptation term, as follows:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo stretchy="false">←</mml:mo><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p> where the weight <inline-formula><mml:math id="inf87"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of connections from V4 to vlPFC represents both <inline-formula><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and evolves with time scale <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. When vlPFC activity exceeds the value of <inline-formula><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (10 spk/sec, see <xref ref-type="table" rid="table1">Table 1</xref>), the steady state feedforward connection from V4 to vlPFC, <inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, goes to 0, and any subsequent input from V4 will fail to activate vlPFC. The feedback connectivity weight was time-independent and set to steady state values: <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>The set of differential equations, with stochastic noise term <inline-formula><mml:math id="inf95"><mml:mi>η</mml:mi><mml:mo>,</mml:mo></mml:math></inline-formula>was solved using the Forward Euler Method in MATLAB. The initial firing rate values for <inline-formula><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were set to 0 spikes per second. The initial connectivity weights were equivalent to the steady-state weights <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf99"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; these and other parameters are given in <xref ref-type="table" rid="table1">Tables 1</xref> and <xref ref-type="table" rid="table2">2</xref>.</p><p>The code for the full model is available on Github (<xref ref-type="bibr" rid="bib5">Choi, 2017</xref>). A copy is archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/V4-PFC-dynamics">https://github.com/elifesciences-publications/V4-PFC-dynamics</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Wyeth Bair, Gregory Horwitz and Dina Popovkina for helpful discussions and comments on the manuscript, and Yoshito Kosai for assistance with animal training and V4 data collection. Technical support was provided by the Bioengineering group at the Washington National Primate Research Center. This work was funded by NEI grant R01EY018839 to A Pasupathy, Vision Core grant P30EY01730 to the University of Washington, P51 grant OD010425 to the Washington National Primate Research Center, NSF grant DMS-1056125 to E Shea-Brown, and Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering to H Choi.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All animal procedures conformed to NIH guidelines and were approved by the Institutional Animal Care and Use Committee at the University of Washington (IACUC Protocol #4133-01).</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbas</surname> <given-names>H</given-names></name><name><surname>Mesulam</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Cortical afferent input to the principalis region of the rhesus monkey</article-title><source>Neuroscience</source><volume>15</volume><fpage>619</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(85)90064-8</pub-id><pub-id pub-id-type="pmid">4069349</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bregman</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="1981">1981</year><chapter-title>Asking the “What-For” questions in auditory perception</chapter-title><person-group person-group-type="editor"><name><surname>Kuy</surname> <given-names>M</given-names></name><name><surname>Pomerantz</surname> <given-names>J</given-names></name></person-group><source>Perceptual Organisation</source><publisher-name>Erlbaum</publisher-name></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1991">1991</year><chapter-title>What's up in top-down processing?</chapter-title><person-group person-group-type="editor"><name><surname>Gorea</surname> <given-names>A</given-names></name></person-group><source>Representation of Vision: Trends and Tacit Assumptions in Vision Research</source><fpage>295</fpage><lpage>304</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Yan</surname> <given-names>Y</given-names></name><name><surname>Gong</surname> <given-names>X</given-names></name><name><surname>Gilbert</surname> <given-names>CD</given-names></name><name><surname>Liang</surname> <given-names>H</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Incremental integration of global contours through interplay between visual cortical areas</article-title><source>Neuron</source><volume>82</volume><fpage>682</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.03.023</pub-id><pub-id pub-id-type="pmid">24811385</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Choi</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>V4-PFC-dynamics</article-title><ext-link ext-link-type="uri" xlink:href="https://github.com/myhannahchoi/V4-PFC-dynamics">https://github.com/myhannahchoi/V4-PFC-dynamics</ext-link><date-in-citation>ac02e4b</date-in-citation></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crittenden</surname> <given-names>BM</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task difficulty manipulation reveals multiple demand activity but no frontal lobe hierarchy</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>532</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs333</pub-id><pub-id pub-id-type="pmid">23131804</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eskandar</surname> <given-names>EN</given-names></name><name><surname>Richmond</surname> <given-names>BJ</given-names></name><name><surname>Optican</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Role of inferior temporal neurons in visual memory. I. Temporal encoding of information about visual images, recalled images, and behavioral context</article-title><source>Journal of Neurophysiology</source><volume>68</volume><fpage>1277</fpage><lpage>1295</lpage><pub-id pub-id-type="pmid">1432084</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fuster</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>The Prefrontal Cortex</source><publisher-loc>New York</publisher-loc><publisher-name>Raven</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname> <given-names>CD</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Top-down influences on visual processing</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>350</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id><pub-id pub-id-type="pmid">23595013</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Helmholtz</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1910">1910</year><source>Treatise on Physiological Optics</source><publisher-loc>New York</publisher-loc><publisher-name>Dover</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname> <given-names>CP</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title><source>Science</source><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1126/science.1117593</pub-id><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>Y</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Common neural mechanisms for response selection and perceptual processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>1095</fpage><lpage>1110</lpage><pub-id pub-id-type="doi">10.1162/089892903322598076</pub-id><pub-id pub-id-type="pmid">14709229</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>JN</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neural correlates of a decision in the dorsolateral prefrontal cortex of the macaque</article-title><source>Nature neuroscience</source><volume>2</volume><fpage>176</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/5739</pub-id><pub-id pub-id-type="pmid">10195203</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosai</surname> <given-names>Y</given-names></name><name><surname>El-Shamayleh</surname> <given-names>Y</given-names></name><name><surname>Fyall</surname> <given-names>AM</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The role of visual area V4 in the discrimination of partially occluded shapes</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>8570</fpage><lpage>8584</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1375-14.2014</pub-id><pub-id pub-id-type="pmid">24948811</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovács</surname> <given-names>G</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Selectivity of macaque inferior temporal neurons for partially occluded shapes</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>15</volume><fpage>1984</fpage><lpage>1997</lpage><pub-id pub-id-type="pmid">7891146</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kveraga</surname> <given-names>K</given-names></name><name><surname>Ghuman</surname> <given-names>AS</given-names></name><name><surname>Bar</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Top-down predictions in the cognitive brain</article-title><source>Brain and Cognition</source><volume>65</volume><fpage>145</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2007.06.007</pub-id><pub-id pub-id-type="pmid">17923222</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Supèr</surname> <given-names>H</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80042-1</pub-id><pub-id pub-id-type="pmid">9751656</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01657-X</pub-id><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lara</surname> <given-names>AH</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Executive control processes underlying multi-item working memory</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>876</fpage><lpage>883</lpage><pub-id pub-id-type="doi">10.1038/nn.3702</pub-id><pub-id pub-id-type="pmid">24747574</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebe</surname> <given-names>S</given-names></name><name><surname>Hoerzer</surname> <given-names>GM</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Rainer</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Theta coupling between V4 and prefrontal cortex predicts visual short-term memory performance</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>456</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3038</pub-id><pub-id pub-id-type="pmid">22286175</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Mazer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>pype3</article-title><ext-link ext-link-type="uri" xlink:href="https://github.com/mazerj/pype3">https://github.com/mazerj/pype3</ext-link><date-in-citation>5e3bd9a</date-in-citation></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Psychophysics with junctions in real images</article-title><source>Perception</source><volume>33</volume><fpage>1101</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1068/p5265</pub-id><pub-id pub-id-type="pmid">15560510</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Parallel neuronal mechanisms for short-term memory</article-title><source>Science</source><volume>263</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1126/science.8290960</pub-id><pub-id pub-id-type="pmid">8290960</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Namima</surname> <given-names>T</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural responses in the inferior temporal cortex to partially occluded and occluding stimuli</article-title><source>Society for Neuroscience Abstracts</source></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ninomiya</surname> <given-names>T</given-names></name><name><surname>Sawamura</surname> <given-names>H</given-names></name><name><surname>Inoue</surname> <given-names>K</given-names></name><name><surname>Takada</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Segregated pathways carrying frontally derived top-down signals to visual areas MT and V4 in macaques</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>6851</fpage><lpage>6858</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6295-11.2012</pub-id><pub-id pub-id-type="pmid">22593054</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname> <given-names>RC</given-names></name><name><surname>Wyatte</surname> <given-names>D</given-names></name><name><surname>Herd</surname> <given-names>S</given-names></name><name><surname>Mingus</surname> <given-names>B</given-names></name><name><surname>Jilk</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Recurrent Processing during Object Recognition</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00124</pub-id><pub-id pub-id-type="pmid">23554596</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Shape representation in area V4: position-specific tuning for boundary conformation</article-title><source>Journal of neurophysiology</source><volume>86</volume><fpage>2505</fpage><lpage>2519</lpage><pub-id pub-id-type="pmid">11698538</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pepik</surname> <given-names>B</given-names></name><name><surname>Benenson</surname> <given-names>R</given-names></name><name><surname>Ritschel</surname> <given-names>T</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What is holding back convnets for detection?</article-title><source>Lecture Notes in Computer Science</source><fpage>517</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24947-6_43</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname> <given-names>DI</given-names></name><name><surname>Oram</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Neurophysiology of shape processing</article-title><source>Image and Vision Computing</source><volume>11</volume><fpage>317</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1016/0262-8856(93)90011-5</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poort</surname> <given-names>J</given-names></name><name><surname>Raudies</surname> <given-names>F</given-names></name><name><surname>Wannig</surname> <given-names>A</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Neumann</surname> <given-names>H</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of attention in figure-ground segregation in areas V1 and V4 of the visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.032</pub-id><pub-id pub-id-type="pmid">22794268</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Salinas</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Flutter discrimination: neural codes, perception, memory and decision making</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1038/nrn1058</pub-id><pub-id pub-id-type="pmid">12612633</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>de Lafuente</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Conversion of sensory signals into perceptual decisions</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>41</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.03.007</pub-id><pub-id pub-id-type="pmid">22472964</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname> <given-names>NC</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ambiguity and invariance: two fundamental challenges for visual processing</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>382</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.04.013</pub-id><pub-id pub-id-type="pmid">20545020</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id><pub-id pub-id-type="pmid">17404214</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>H</given-names></name><name><surname>Buia</surname> <given-names>C</given-names></name><name><surname>Madhavan</surname> <given-names>R</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Madsen</surname> <given-names>JR</given-names></name><name><surname>Anderson</surname> <given-names>WS</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Spatiotemporal dynamics underlying object completion in human ventral visual cortex</article-title><source>Neuron</source><volume>83</volume><fpage>736</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.06.017</pub-id><pub-id pub-id-type="pmid">25043420</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>H</given-names></name><name><surname>Buia</surname> <given-names>C</given-names></name><name><surname>Madsen</surname> <given-names>J</given-names></name><name><surname>Anderson</surname> <given-names>WS</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>A role for recurrent processing in object completion: neurophysiological, psychophysical and computational evidence</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.2942">1409.2942</ext-link><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.2942">https://arxiv.org/abs/1409.2942</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>H</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Recognition of occluded objects</chapter-title><person-group person-group-type="editor"><name><surname>Zhao</surname> <given-names>Q</given-names></name></person-group><source>Computational and Cognitive Neuroscience of Vision</source><publisher-loc>Singapore</publisher-loc><publisher-name>Springer-Verlag</publisher-name><pub-id pub-id-type="doi">10.1007/978-981-10-0213-7_3</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Galkin</surname> <given-names>TW</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Gattass</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical connections of area V4 in the macaque</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>477</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm061</pub-id><pub-id pub-id-type="pmid">17548798</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname> <given-names>R</given-names></name><name><surname>Thorpe</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Is it a bird? Is it a plane? Ultra-rapid visual categorisation of natural and artifactual objects</article-title><source>Perception</source><volume>30</volume><fpage>655</fpage><lpage>668</lpage><pub-id pub-id-type="doi">10.1068/p3029</pub-id><pub-id pub-id-type="pmid">11464555</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>G</given-names></name><name><surname>Rolls</surname> <given-names>ET</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Invariant face and object recognition in the visual system</article-title><source>Progress in Neurobiology</source><volume>51</volume><fpage>167</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/S0301-0082(96)00054-8</pub-id><pub-id pub-id-type="pmid">9247963</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>From rule to response: neuronal processes in the premotor and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>1790</fpage><lpage>1806</lpage><pub-id pub-id-type="doi">10.1152/jn.00086.2003</pub-id><pub-id pub-id-type="pmid">12736235</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>W</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inhibitory Control in the Cortico-Basal Ganglia-Thalamocortical Loop: Complex Regulation and Interplay with Memory and Decision Processes</article-title><source>Neuron</source><volume>92</volume><fpage>1093</fpage><lpage>1105</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.031</pub-id><pub-id pub-id-type="pmid">27866799</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyatte</surname> <given-names>D</given-names></name><name><surname>Curran</surname> <given-names>T</given-names></name><name><surname>O'Reilly</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The limits of feedforward vision: recurrent processing promotes robust object recognition when objects are degraded</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>2248</fpage><lpage>2261</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00282</pub-id><pub-id pub-id-type="pmid">22905822</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuille</surname> <given-names>A</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Vision as Bayesian inference: analysis by synthesis?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>301</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.002</pub-id><pub-id pub-id-type="pmid">16784882</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaksas</surname> <given-names>D</given-names></name><name><surname>Pasternak</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Directional signals in the prefrontal cortex and in area MT during a working memory for visual motion task</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>11726</fpage><lpage>11742</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3420-06.2006</pub-id><pub-id pub-id-type="pmid">17093094</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.25784.030</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rust</surname><given-names>Nicole</given-names></name><role>Reviewing Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Dynamic representation of partially occluded objects in primate prefrontal and visual cortex&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by David Van Essen (Senior Editor) and three reviewers, one of whom is a member of our Board of Reviewing Editors. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this letter to crystallize our concerns going forward. We feel the work is important and interesting but key issues remain unresolved that must be addressed satisfactorily to produce an acceptable manuscript.</p><p>At this point we are unable to render a binding recommendation and require a response from you indicating the feasibility of your completing the essential tasks in a reasonable period of time – around 2 months. The Board member and reviewers will consider your response and provide a binding decision.</p><p>General assessment:</p><p>This paper characterizes responses of V4 and vlPFC neurons to partially occluded visual stimuli and suggests that feedback from vlPFC to V4 boosts V4 responses to occluded shapes and helps resolve stimulus identity. The authors recorded vlPFC and V4 neurons from the same monkeys performing the same task, although in separate experimental sessions. They demonstrate that vlPFC neurons respond more strongly and more selectively to occluded stimuli, unlike V4 neurons, which commonly respond most strongly and selectively to unoccluded images. The authors suggest that a subset of V4 neurons respond to occluded images with two distinct peaks (but not all reviewers are convinced that the distinction between subpopulations with one and two peaks are real). The second peak follows the vlPFC response peak and shows similar characteristics to vlPFC. Based on these observations, the authors construct a two-layer neural network in which V4 and vlPFC model units are reciprocally connected and vlPFC responses shape the second peak of V4 units.</p><p>The reviewers find the proposal that PFC interacts with V4 to resolve the challenge of solving shape discrimination in the presence of occlusion to be timely and of significant interest. At the same time, the reviewers identified problematic issues with the data analysis that must be resolved before this work could be considered for publication.</p><p>Summary:</p><p>General concerns about the experimental aspects of the paper include the reproducibility of the main result and the impact of using different approaches when recording from the two brain areas that are compared. General concerns about the model include that it may be unnecessarily complex for the current illustrations provided, and that, even with this complexity, the current illustrations do not reflect population average effects.</p><p>Essential revisions:</p><p>1) The main claims of the paper rest on the assertion that V4 shape selectivity increases as a function of time. The concerns about this claim are two-fold:</p><p>1A) The current illustration is made via an argument that there are two subpopulations of neurons, those that have a second, shape selective peak and those that do not. The reviewers are concerned that the existence of these two subpopulations will not be reproducible. This includes some confusion about the methods that were employed to identify the two peaks, as well as the suspicion that the specific parameters used for this identification were overfit to this particular data set.</p><p>The way that the two peaks are identified (subsection “Peak finding algorithm”) is hard to understand: &quot;were within ± 12 ms of at least one third of the peaks identified based on the PSTHs for individual occlusion levels&quot;.</p><p>How sensitive are these findings to the parameters of the ad hoc peak finding algorithm? Similarly, one may think that the algorithm for detecting double-peak V4 cells has many false positives and the true frequency of cells with two peaks is lower than that suggested in the paper. Can you clarify?</p><p>In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, how can the &quot;Time to peak&quot; for V4 neurons be greater than 300 ms, if the second peak was required to be before 300 ms (&quot;The second peak was constrained to be no later than 300 ms after test stimulus onset&quot;)?</p><p>The manuscript states: &quot;However, shape selectivity for occluded shapes, particularly at intermediate occlusion levels (visible area 72- 95%), was different for the two groups: neurons with two peaks had significantly greater shape selectivity than neurons without two peaks in the time interval ~200-260 ms after test stimulus onset (t-Test, p &lt; 0.05).&quot; The supporting figure is <xref ref-type="fig" rid="fig8">Figure 8A</xref> vs. B. What this is stating exactly? Is the significance tested for each occlusion level separately and it was significant for visible area conditions of 72, 82, 90, and 95%? If so, was Bonferroni correction or another correction applied? How was this time period selected (~200-260 ms)? Was there a correction for testing multiple time periods? Do the results hold in different time periods?</p><p>To examine the tuning of the &quot;second peak&quot; the authors use as baseline the activity in an earlier phase of the visual response. Hence, if the neurons respond strongly early on, their response will decrease more strongly during the second peak. In other words, there is suspicion that the results presented in <xref ref-type="fig" rid="fig7">Figure 7</xref> where activity of the second peak decreases if the visible area is large is simply an artifact of this erroneous choice of a time window to compute the baseline. Instead the authors should use the pre-stimulus activity as baseline for all epochs. This choice implies in the subsection “Response change”, in the first equation that <italic>b</italic> depends on <italic>i</italic>, in incoming input. In <xref ref-type="fig" rid="fig7">Figure 7D, E</xref> the y-axis is in units of &quot;normalized response change&quot;. How was the normalization?</p><p>The average PSTH of V4 subpopulation with two response peaks is quite distinct from the single cell examples. For the population, the second peak is not obvious and responses to unoccluded stimuli stay above the occluded shapes, unlike the single cells in <xref ref-type="fig" rid="fig4">Figures 4</xref> and 5. What causes the discrepancy? Is it the variability of the time of the second peak in V4 neurons? It will be helpful to have a supplementary figure with more single cell examples.</p><p>1B) More generally, the claim is that V4 shape selectivity changes as a function of time, and missing is the more direct comparison of shape selectivity for the same neurons early versus later in the response.</p><p>2) The broad tuning of neurons in vlPFC seems qualitatively inconsistent with it playing a role in enhancing V4 shape selectivity. The model does not currently resolve this nor is an explanation provided.</p><p>3) There are concerns that the functional differences between the responses in PFC and V4 may follow from differences in the experimental paradigm. These include:</p><p>3A) What was the effect of tailoring stimuli for neurons in one area and not doing so in another area? Can the latency of responses or their tolerance to occlusion be influenced by tailoring stimuli for neurons?</p><p>3B) It is unclear why the authors focus on the vlPFC neurons that are influenced by occlusion. There are also many neurons that do not care about the occlusion and we wonder whether these cells include some neurons that are tuned to the shape of the stimuli. If yes, is their tuning better or worse than that of the neurons that are influence by the occlusion?</p><p>What would <xref ref-type="fig" rid="fig3">Figure 3D</xref> look like if you included all of the 216 vlPFC neurons that were responsive during the test epoch?</p><p>Some cells have a stronger response if the visible area increases. Is it possible that these neurons are better tuned to the shapes than those neurons that are most active for high levels of occlusion?</p><p>Related to this last point: is it conceivable that the neurons that increase their response if there is more occlusion are tuned to some aspect of the occluders, e.g. the total surface area or the total perimeter of the occluding dots?</p><p>This latter possibility seems to be supported by the finding that the vlPFC preference for occlusion decreased when the occluders had the same color as the background (as is stated in the first paragraph of the subsection “Representation of occluded stimuli in vlPFC”).</p><p>There were 381 vlPFC neurons, 98 were significantly modulated by occlusion. How many of these 98 neurons are in monkey M vs. O?</p><p>4) Can the model replicate all aspects of the data? Including:</p><p>The shape selectivity index in vlPFC is considerably lower than in V4. Can the model in <xref ref-type="fig" rid="fig9">figure 9</xref> work with reduced shape selectivity?</p><p>V4 cells are divided into two subpopulations, one with two peaks and another with a single peak. How can the feedback model explain that many V4 cells do not show two peaks in their responses to occluded stimuli? Are the authors assuming inhomogeneous feedback from vlPFC to V4? Alternatively, they may be suggesting that V4 population includes a continuum of responses that vary from single peak to double peaks and include in-between responses.</p><p>Can the model be adjusted to replicate population data in <xref ref-type="fig" rid="fig6">Figure 6</xref>? In its current form the model seems to best explain single cell examples. It is unclear how well these examples represent the population.</p><p>5) The gain mechanism that the authors propose in their model may be envisioned as PFC receiving two signals – an intertwined shape and occlusion signal and an occlusion-only signal, and then correcting the intertwined estimate with occlusion information. This seems like a bit of a chicken-and-egg problem: how and why would the brain extract occlusion level from occluded stimuli but not shape (preceding the locus at which it disambiguates shape)? Where could this v4-independent but occlusion dependent gain modulation be coming from? The authors suggest IT as a potential source, but IT units receive input from V4 and also feedback to V4. Does the model assume that gain modulation arrives at vlPFC with the same latency as the V4 inputs? Why can't feedback from IT to V4 be the source of the second V4 response peak? The authors cite their SfN abstract for occlusion selective signals in IT. It will be useful to provide more explanation about the results, especially for readers who did not stop by the SfN poster. What if there are two visible stimuli: one that is occluded and one that is not? Would the gain modulation then be different for the two stimuli? What if one stimulus occludes another stimulus?</p><p>6) Can the model be simplified? The proposed dynamic model has multiple degrees of freedom and several nonlinearities. Is all this complexity necessary? In the current version of the paper, it is difficult to gain intuition about the model based on the text. Simplifying the model can shine light on which components and nonlinearities are indispensable and therefore reasonable targets for follow-up studies. For example, the adaptation term for V4 projections to vlPFC seems a little arbitrary. Why such an adaptation does not happen for other connections in the model? A similar question can be asked about the half-rectified feedback from vlPFC to V4. Why other connections in the model are not half-rectified? More generally, we encourage the authors to explore the model space a little more extensively for simpler model architectures. As it stands, the network seems like a high-parameter model that one can tweak to get many different types of outputs. Establishing the necessity of the proposed architecture and parameterization requires a little more work.</p><p>7) On the interpretation of the effects in V4:</p><p>In the first paragraph of the subsection “Representation of occluded stimuli in vlPFC” the authors argue that the responses of some of the vlPFC neurons that are stronger if occlusion is stronger does not depend on the increased task difficulty or attentional demands, but this reasoning is unclear. Furthermore, the authors seem to have changed their mind in the fourth paragraph of the subsection 2Representation of occluded stimuli in vlPFC”, where they argue that vlPFC may amplify weak signals and that vlPFC is engaged in tasks of greater difficulty or cognitive demand.</p><p>In the first paragraph of the subsection “Representation of occluded stimuli in vlPFC” the authors argue that because many vlPFC neurons were tuned to shape, that these neurons therefore cannot reflect task difficulty or attentional demands. This argument does not hold because neurons may well be tuned to multiple aspects of a task.</p><p>8) How did you know whether penetrations were indeed in vlPFC? Was histology performed?</p><p>9) [Additional comment sent to the authors in response to authors’ plan for revision]: The authors should clarify if there is a real dichotomy between neurons with one versus two peaks, as well as how broad the distribution of the timing for the second peak is. Can they really convince the reader that they are not simply amplifying noise with their analysis? Furthermore, they now make the point that the tuning is stronger during the second peak. We would like to know if this is not simply predicted by the presence of extra spikes – i.e. the presence of a peak implies some extra spikes at a certain point in time.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Dynamic representation of partially occluded objects in primate prefrontal and visual cortex&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by David Van Essen (Senior Editor), a Reviewing Editor, and two reviewers.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as described below by reviewer #3. We envision that these revisions will be straightforward to carry out and that verification can be handled by the Reviewing Editor.</p><p><italic>Reviewer #2:</italic> </p><p>The authors have addressed the most critical comments. There are structural weaknesses in the dataset that keep alternative interpretations plausible. However, I believe the authors' interpretation is strengthened by the new analyses and the paper passes threshold for publication.</p><p><italic>Reviewer #3:</italic> </p><p>In their revision, Fyall et al. have addressed many of my concerns satisfactorily. They have made it clearer that some of the V4 neurons have a second peak that is more prominent in case of occlusion (<xref ref-type="fig" rid="fig4">Figure 4C</xref> represents a compelling example). Also, the peak detection method is now more convincing and it is also better documented. It remains unclear whether activity in vlPFC indeed contributes to late V4 activity and it is therefore conceivable that there are additional areas that could contribute to late V4 activity. Yet, I do realize that demonstrating the causal link between dlPFC and V4 would require a different approach, which would be beyond the scope of the present contribution. However, establishing such a causal link might be an important topic for future research, and the authors could mention this point, which could be added to the paragraph of suggested future work (subsection “Response dynamics in V4”, fifth paragraph).</p><p>Remaining points:</p><p>1) I find it difficult to understand why the vlPFC neurons do not respond so well when the occluders have the same color of the background (subsection “Representation of occluded stimuli in vlPFC”, second paragraph). I would suspect that the processes for shape recognition would remain the same. Or did the monkeys' performance show signs that this was not the case?</p><p>2) Quite some p-values are lacking, three examples:</p><p>&quot;The responses of most of these occlusion-sensitive 155 neurons (71/98) increased with increasing occlusion level&quot;.</p><p>&quot;Even for the small subset of vlPFC neurons that responded more strongly to unoccluded stimuli (27/98), shape selectivity was not stronger for unoccluded than occluded stimuli (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>).&quot;</p><p>&quot;Shape selectivity for occluded shapes was significantly higher during the second peak than during the first peak.&quot;</p><p>3) &quot;Of 85 neurons, 30 neurons (~35%) were classified as having two peaks&quot;. How were these cells distributed across the two monkeys?</p><p>4) The model with interactions between vlPFC and V4 seems still somewhat simplistic as there are only a few neurons and the variation (in effect size and timing) across neurons shown in the figures is actually a variation across neurons in different models rather than a variation of neurons within the same model. In networks with many units and reciprocal connections, the network dynamics might actually work against variation across neurons. The authors should discuss this. It would be great if it would be possible to show the same range of differences between neurons within a same model, but I will not insist on such a demonstration given that making such a larger model might require a substantial investment of time.</p><p>5) &quot;We cannot rule out the possibility that IT responses also contribute to V4 responses during the second transient peak. However, our IT recordings suggest this is unlikely because, as in V4, shape selectivity in IT is stronger for unoccluded than occluded stimuli&quot;. Is it conceivable that some IT neurons also have two phases in their response where the second phase is more pronounced in the presence of occlusion? It would be great if the authors could look for this possibility in the previous data set by Namima and Pasupathy, 2016? If the two phases are there it would strengthen the paper, but it would also be interesting if that is not the case.</p><p>6) Equations 4/5: I failed to see the logic of these equations, would it be possible to clarify this? Equation 9: what is <italic><sub>thr2</sub></italic>?</p><p>7) I found <xref ref-type="fig" rid="fig8s6">Figure 8—figure supplement 6A</xref> confusing: how do you compute y/z for neurons with one peak?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.25784.031</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>1) The main claims of the paper rest on the assertion that V4 shape selectivity increases as a function of time. The concerns about this claim are two-fold:</italic> </p><p><italic>1A) The current illustration is made via an argument that there are two subpopulations of neurons, those that have a second, shape selective peak and those that do not. The reviewers are concerned that the existence of these two subpopulations will not be reproducible. This includes some confusion about the methods that were employed to identify the two peaks, as well as the suspicion that the specific parameters used for this identification were overfit to this particular data set.</italic> </p><p><italic>The way that the two peaks are identified (subsection “Peak finding algorithm”) is hard to understand: &quot;were within ± 12 ms of at least one third of the peaks identified based on the PSTHs for individual occlusion levels&quot;.</italic> </p><p><italic>How sensitive are these findings to the parameters of the ad hoc peak finding algorithm? Similarly, one may think that the algorithm for detecting double-peak V4 cells has many false positives and the true frequency of cells with two peaks is lower than that suggested in the paper. Can you clarify?</italic> </p><p>We appreciate the reviewers’ general concern regarding the reproducibility of our finding that some V4 neurons have two transient response peaks. We address this concern in our revision in several ways:</p><p>i) We have revised and simplified the algorithm employed for peak finding, and we have added a new supplementary figure to explain this procedure schematically (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Specifically, the steps identified as confusing above have now been removed and replaced with a statistical test (see below).</p><p>ii) We have repeated our analyses using different parameter choices, and have added two supplementary figures (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref> and <xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref>) demonstrating the robustness of the main findings described in the manuscript (<xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="fig" rid="fig8">Figure 8</xref>) across different parameters.</p><p>iii) To mitigate concerns about false positives, we have added a statistical criterion to our peak finding algorithm. Only those neurons with a statistically significant (paired t-Test, p &lt; 0.05, Bonferroni corrected) increase in response at the time of the putative second peak are categorized as having two peaks.</p><p>iv) We include an independent, model-based procedure for identifying neurons with two transient response peaks that does not rely on the ad hoc peak-finding algorithm. Briefly, this model-based procedure detects neurons for which the response PSTHs for occluded stimuli cannot be modeled as a linear scaling of the response PSTH for unoccluded stimuli. We present two new supplementary figures (<xref ref-type="fig" rid="fig8s5">Figure 8—figure supplement 5</xref> and <xref ref-type="fig" rid="fig8s6">Figure 8—figure supplement 6</xref>) demonstrating a good correspondence between the results of this model-based procedure and those of the ad hoc peak-finding procedure used previously.</p><p><italic>In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, how can the &quot;Time to peak&quot; for V4 neurons be greater than 300 ms, if the second peak was required to be before 300 ms (&quot;The second peak was constrained to be no later than 300 ms after test stimulus onset&quot;)?</italic> </p><p>We have corrected this plotting error.</p><p><italic>The manuscript states: &quot;However, shape selectivity for occluded shapes, particularly at intermediate occlusion levels (visible area 72- 95%), was different for the two groups: neurons with two peaks had significantly greater shape selectivity than neurons without two peaks in the time interval ~200-260 ms after test stimulus onset (t-Test, p &lt; 0.05).&quot; The supporting figure is <xref ref-type="fig" rid="fig8">Figure 8A</xref> vs. B. What this is stating exactly? Is the significance tested for each occlusion level separately and it was significant for visible area conditions of 72, 82, 90, and 95%? If so, was Bonferroni correction or another correction applied? How was this time period selected (~200-260 ms)? Was there a correction for testing multiple time periods? Do the results hold in different time periods?</italic> </p><p>For improved clarity, we have revised this analysis and we present the new results in the revised manuscript. We now use two t Tests to ask whether shape selectivity was significantly stronger for neurons with two peaks than neurons without two peaks at two time points: one centered around the first peak (69–99 ms) and another centered around the second peak (199–229 ms) of V4 responses. For occluded stimuli, shape selectivity was significantly stronger for neurons with two peaks during the second peak but not during the first peak. For unoccluded stimuli, shape selectivity was not significantly different between the two neuronal subsets during either time point.</p><p><italic>To examine the tuning of the &quot;second peak&quot; the authors use as baseline the activity in an earlier phase of the visual response. Hence, if the neurons respond strongly early on, their response will decrease more strongly during the second peak. In other words, there is suspicion that the results presented in <xref ref-type="fig" rid="fig7">Figure 7</xref> where activity of the second peak decreases if the visible area is large is simply an artifact of this erroneous choice of a time window to compute the baseline. Instead the authors should use the pre-stimulus activity as baseline for all epochs. This choice implies in the subsection “Response change”, in the first equation that b depends on i, in incoming input. In <xref ref-type="fig" rid="fig7">Figure 7D, E</xref> the y-axis is in units of &quot;normalized response change&quot;. How was the normalization?</italic> </p><p>As requested by the reviewers, we present average activity plots with respect to a single prestimulus baseline for both epochs in the revised manuscript (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The results show clearly that V4 neuronal sensitivity to occlusion level changes over time. We hope that this revised analysis of the data mitigates the reviewers’ concerns regarding the time window over which baseline activity is computed. Additionally, we clarify how the response normalization was implemented in the Materials and methods section and in the figure legend. Briefly, prior to constructing the population average, each neuron’s PSTHs were normalized to the maximum value across time and occlusion level. This strategy ensured that differences in peak firing rates between neurons did not impact the population averages.</p><p><italic>The average PSTH of V4 subpopulation with two response peaks is quite distinct from the single cell examples. For the population, the second peak is not obvious and responses to unoccluded stimuli stay above the occluded shapes, unlike the single cells in <xref ref-type="fig" rid="fig4">Figures 4</xref> and 5. What causes the discrepancy? Is it the variability of the time of the second peak in V4 neurons? It will be helpful to have a supplementary figure with more single cell examples.</italic> </p><p>We clarify this point in the revised Results section. We agree with the reviewers that the second response peak is less obvious in the population response averages than in the responses of individual example neurons. Our new model simulations (<xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref>) verify that this difference could arise due to variability in the timing of the second peak across V4 neurons. As requested by the reviewers, we include a new supplementary figure with additional example V4 neuronal responses from six neurons with two transient response peaks (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We would like to note that for many neurons with two peaks, the responses to occluded stimuli do not exceed responses to unoccluded stimuli during the second peak (e.g. see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A, B, D</xref>). Rather, our results suggest that the difference in responses to occluded stimuli is larger than that for unoccluded stimuli during the second peak (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><p><italic>1B) More generally, the claim is that V4 shape selectivity changes as a function of time, and missing is the more direct comparison of shape selectivity for the same neurons early versus later in the response.</italic> </p><p>As requested by the reviewers, we have added a new scatter plot providing a direct comparison of shape selectivity for occluded stimuli during the first and second peak for V4 neurons with two peaks (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). We describe this analysis in the revised Results section. Shape selectivity for occluded stimuli was significantly stronger during the second peak than the first peak. In contrast, we observed no significant difference in shape selectivity for V4 neurons without two peaks. We also observed no significant difference in shape selectivity for unoccluded stimuli for V4 neurons with or without two peaks.</p><p><italic>2) The broad tuning of neurons in vlPFC seems qualitatively inconsistent with it playing a role in enhancing V4 shape selectivity. The model does not currently resolve this nor is an explanation provided.</italic> </p><p>We address this point in the revised Discussion section. Our new model simulations demonstrate that feedback signals from weakly shape-selective vlPFC neurons could enhance shape selectivity in V4 (<xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref>). In the behavioral task used, the occluded shape used in each experimental session is restricted to one of two choices, thus simplifying the object recognition problem. In this case, any given vlPFC neuron receiving differential input from V4 neurons that signal the two shape discriminanda could contribute to enhanced V4 shape selectivity. We would need additional experiments to determine how vlPFC and V4 might interact in more general situations. It is likely that recognition and memory-related mechanisms would contribute.</p><p><italic>3) There are concerns that the functional differences between the responses in PFC and V4 may follow from differences in the experimental paradigm. These include:</italic> </p><p><italic>3A) What was the effect of tailoring stimuli for neurons in one area and not doing so in another area? Can the latency of responses or their tolerance to occlusion be influenced by tailoring stimuli for neurons?</italic> </p><p>The lack of stimulus tailoring cannot explain away our two major vlPFC findings: i) the large fraction of vlPFC neurons that respond preferentially to occluded stimuli, ii) the stronger shape selectivity under occlusion. We address this point extensively in the revised Discussion section.</p><p>Tailoring stimuli in V4 may explain the preponderance of neurons in our V4 dataset that responded preferentially to unoccluded shapes. Without tailoring stimuli in vlPFC, we would expect a roughly equal proportion of neurons showing responses that increased and decreased with increasing occlusion level. However, we found that 72% of vlPFC neurons responded preferentially to occluded stimuli, a proportion that deviates significantly from the null hypothesis (binomial test, p &lt; 0.01).</p><p>Furthermore, because the visible difference between any two shapes declines with increasing occlusion, we expect shape selectivity to decline with increasing occlusion regardless of whether we tailored the stimuli to each neuron. Stronger shape selectivity in the vlPFC under occlusion defies this expectation. We do not expect that stimulus tailoring (or lack thereof) influenced neuronal response latency for two reasons. First, in vlPFC, we did not find a statistically significant difference in response latency between neurons that were shape-selective (N=66) and neurons that were visually responsive but not shape-selective (N=150) (t Test, p = 0.55). Second, as part of an ongoing study in the lab, we find that V4 response latencies are similar for preferred and non-preferred stimuli (Zamarashkina et al., VSS Abstracts 2017).</p><p><italic>3B) It is unclear why the authors focus on the vlPFC neurons that are influenced by occlusion. There are also many neurons that do not care about the occlusion and we wonder whether these cells include some neurons that are tuned to the shape of the stimuli. If yes, is their tuning better or worse than that of the neurons that are influence by the occlusion?</italic> </p><p>We address this point with a new supplementary figure (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), and revised text in the Results section. Briefly, of 216 visually-responsive vlPFC neurons, 66 were shape selective and, of these, 41 were occlusion-sensitive (two-way ANOVA, p &lt; 0.05). Thus, few neurons (25/216) were classified as shape-selective but not occlusion-sensitive by our statistical tests. We now show average shape selectivity for four different vlPFC neuronal subsets (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>): (1) shape-selective but not occlusion-sensitive neurons (N=25), (2) all shape-selective neurons (N=66), (3) neurons that carry task relevant information, i.e. either shape-selective or occlusion sensitive (N=123), (4) all visually-responsive neurons (N=216). Shape selectivity was stronger for shape selective neurons than occlusion-sensitive neurons. Notably, however, shape selectivity was strongest at intermediate occlusion levels, for both types of neurons.</p><p><italic>What would <xref ref-type="fig" rid="fig3">Figure 3D</xref> look like if you included all of the 216 vlPFC neurons that were responsive during the test epoch?</italic> </p><p>As requested by the reviewers, we present these data in a new supplementary figure (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D</xref>). Compared to <xref ref-type="fig" rid="fig3">Figure 3D</xref>, the overall magnitude of shape selectivity is reduced due to the inclusion of more neurons, but otherwise the trends are similar. Specifically, shape selectivity is strongest for stimuli at intermediate occlusion levels in both figures.</p><p><italic>Some cells have a stronger response if the visible area increases. Is it possible that these neurons are better tuned to the shapes than those neurons that are most active for high levels of occlusion?</italic> </p><p>We address this point with a new supplementary figure (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) and revised text in the Results section. This figure shows shape selectivity across occlusion level separately for vlPFC neurons that prefer lower occlusion levels (higher% visible area, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>) and for neurons that prefer higher occlusion levels (lower% visible area, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). The data are more variable in A due to the inclusion of fewer neurons. Nevertheless, two patterns are evident: (1) even for neurons that respond more strongly to unoccluded stimuli (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>), shape selectivity is not stronger for unoccluded than occluded stimuli (compare black and colored lines); (2) neurons that prefer higher occlusion levels are more shape-selective under occlusion (compare colors in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref> versus <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>).</p><p><italic>Related to this last point: is it conceivable that the neurons that increase their response if there is more occlusion are tuned to some aspect of the occluders, e.g. the total surface area or the total perimeter of the occluding dots?</italic> </p><p><italic>This latter possibility seems to be supported by the finding that the vlPFC preference for occlusion decreased when the occluders had the same color as the background (as is stated in the first paragraph of the subsection “Representation of occluded stimuli in vlPFC”).</italic> </p><p>We now clarify this point in the revised Discussion section. vlPFC neurons that prefer higher occlusion levels may be sensitive to the total area of the occluding dots. This possibility is consistent with the finding that neuronal preference for the occluding dots is reduced when these dots are rendered in the same color as the background. However, it is important to note that many occlusion-sensitive vlPFC neurons are also selective for the occluded shape, so their responses reflect the characteristics of both the occluding dots and the occluded shape.</p><p><italic>There were 381 vlPFC neurons, 98 were significantly modulated by occlusion. How many of these 98 neurons are in monkey M vs. O?</italic> </p><p>We include these numbers in the revised Materials and methods section. In monkey M, 71/260 neurons (27%) were occlusion-sensitive. In monkey O, 27/121 neurons (22%) were occlusion-sensitive.</p><p><italic>4) Can the model replicate all aspects of the data? Including:</italic> </p><p><italic>The shape selectivity index in vlPFC is considerably lower than in V4. Can the model in <xref ref-type="fig" rid="fig9">figure 9</xref> work with reduced shape selectivity?</italic> </p><p>We have revised our model in response to this question and verified that the model can produce vlPFC unit responses with different magnitudes of shape selectivity. Specifically, each vlPFC model unit now receives input from both V4 units and the relative connection strengths from V4 to vlPFC units govern the strength of shape selectivity in the vlPFC unit. <xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref> demonstrates that our model captures the overall trends observed in the neuronal data (i.e. two transient response peaks in V4, and a preference for occluded stimuli in vlPFC) under a wide range of vlPFC shape selectivity magnitudes.</p><p><italic>V4 cells are divided into two subpopulations, one with two peaks and another with a single peak. How can the feedback model explain that many V4 cells do not show two peaks in their responses to occluded stimuli? Are the authors assuming inhomogeneous feedback from vlPFC to V4? Alternatively, they may be suggesting that V4 population includes a continuum of responses that vary from single peak to double peaks and include in-between responses.</italic> </p><p>We address this question with a supplementary figure (<xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref>) and new text in the Discussion section. <xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref> demonstrates that, by varying the strength of feedback connections from V4 to vlPFC, the model can produce V4 unit response dynamics that vary along a continuum, with some units having one peak and others having two peaks. Together with the broad range of second peak magnitudes observed in our neuronal data, these model simulations support the hypothesis that V4 neurons with and without two peaks form a continuum. It is also possible that projections from vlPFC to V4 are inhomogeneous. Anatomical estimates of the proportion of V4 neurons that receive projections from vlPFC and quantification of the diversity and strength of these projections would be needed to fully resolve this question.</p><p><italic>Can the model be adjusted to replicate population data in <xref ref-type="fig" rid="fig6">Figure 6</xref>? In its current form the model seems to best explain single cell examples. It is unclear how well these examples represent the population.</italic> </p><p>We address this question in two supplementary figures (<xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref> and <xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref>). <xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref> demonstrates that, by varying the synaptic delays and strengths in the feedback from vlPFC to V4, the model can produce a variety of V4 responses, with second response peaks that have a range of magnitudes and peak times. <xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref> demonstrates that a simulated population average of 258 V4 model units produced a population-level PSTH that resembled the neuronal data.</p><p><italic>5) The gain mechanism that the authors propose in their model may be envisioned as PFC receiving two signals – an intertwined shape and occlusion signal and an occlusion-only signal, and then correcting the intertwined estimate with occlusion information. This seems like a bit of a chicken-and-egg problem: how and why would the brain extract occlusion level from occluded stimuli but not shape (preceding the locus at which it disambiguates shape)?</italic> </p><p>We expand on this point in the revised Discussion section. In our behavioral task, the occluding dots and occluded shapes have different colors. In this simple case, extracting information about occlusion level is easy, and can be mediated by a neuron sensitive to the total area and color of the occluding dots. In the more complex case of natural vision, extracting information about occlusion level could be substantially harder. Future experiments in which features of occluding objects (e.g. color, shape and size) are varied across trials will be needed to extend our findings to more naturalistic cases.</p><p>Extracting information about the occluded shape is hard even for our simple task because occlusion makes parts of the shape inaccessible to the visual system. So, a neuron sensitive to boundary form or to the area rendered in a specific color will show reduced responses under occlusion and thus can only provide an intertwined shape and occlusion signal.</p><p><italic>Where could this v4-independent but occlusion dependent gain modulation be coming from? The authors suggest IT as a potential source, but IT units receive input from V4 and also feedback to V4. Does the model assume that gain modulation arrives at vlPFC with the same latency as the V4 inputs? Why can't feedback from IT to V4 be the source of the second V4 response peak?</italic> </p><p>We address these questions with a supplementary figure (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref>) and new text in the revised Results and Discussion sections. It is possible that occlusion-only signals arise first at the level of V4 or, alternatively, IT. Future experiments comparing the preponderance and latency of occlusion-only signals in the two cortical areas are needed to uncover their likely origin. We have now implemented changes to the model to allow for the occlusion signals to arrive either from V4 or from IT cortex (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref>) by the varying the delay in the time of arrival of the gain modulation signal relative to shape sensitive signals.</p><p>We cannot rule out the possibility that IT responses contribute to V4 responses during the second transient peak. However, our IT recordings (Namima and Pasupathy, 2016) suggest this is unlikely because, as in V4, shape selectivity in IT is stronger for unoccluded than occluded stimuli. Thus, putative feedback from IT may not be well-suited for enhancing V4 shape selectivity at intermediate occlusion levels.</p><p><italic>The authors cite their SfN abstract for occlusion selective signals in IT. It will be useful to provide more explanation about the results, especially for readers who did not stop by the SfN poster.</italic> </p><p>As requested by the reviewers, we expand on this point in the revised Discussion section. Briefly, recordings performed in IT cortex of one monkey performing the same behavioral task used in the current study show that the responses of many IT neurons are consistent with encoding the total area of the occluding dots (Namima and Pasupathy, 2016).</p><p><italic>What if there are two visible stimuli: one that is occluded and one that is not? Would the gain modulation then be different for the two stimuli? What if one stimulus occludes another stimulus?</italic> </p><p>We speculate on these points briefly in the revised Discussion section, although these questions are best addressed with new experiments. When multiple stimuli are presented within a V4 neuron’s receptive field, past studies (e.g. Moran and Desimone, 1985) suggest that neuronal responses are dictated largely by the attended stimulus. Extending our results to the case of two stimuli, one occluded and one unoccluded, we expect gain modulation to be low or high depending on whether the unoccluded or the occluded stimulus is the attended, behaviorally-relevant stimulus.</p><p>When one stimulus occludes another and the attributes of the occluders are not known a priori, identifying which object is occluded, and by how much, could be challenging. To extend our simple model to tackle more complex, naturalistic cases would likely require the incorporation of attention and memory processes.</p><p><italic>6) Can the model be simplified? The proposed dynamic model has multiple degrees of freedom and several nonlinearities. Is all this complexity necessary? In the current version of the paper, it is difficult to gain intuition about the model based on the text. Simplifying the model can shine light on which components and nonlinearities are indispensable and therefore reasonable targets for follow-up studies. For example, the adaptation term for V4 projections to vlPFC seems a little arbitrary. Why such an adaptation does not happen for other connections in the model? A similar question can be asked about the half-rectified feedback from vlPFC to V4. Why other connections in the model are not half-rectified? More generally, we encourage the authors to explore the model space a little more extensively for simpler model architectures. As it stands, the network seems like a high-parameter model that one can tweak to get many different types of outputs. Establishing the necessity of the proposed architecture and parameterization requires a little more work.</italic> </p><p>We address this question in the revised manuscript with six supplementary figures (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>–<xref ref-type="fig" rid="fig10s6">6</xref>) and two new Results sections titled, “Parsimonious model construction” and “Heterogeneity in V4 and vlPFC responses”.</p><p>We demonstrate the necessity of four mechanisms included in the model: i) feedback from vlPFC to V4, ii) synaptic adaptation in the feedforward inputs from V4 to vlPFC, iii) a half-wave rectifying nonlinearity on feedback signals from vlPFC to V4, and, iv) gain modulation signals on vlPFC. First, a network with only feedforward connections from V4 to vlPFC units fails to generate a second response peak in V4 units (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1A</xref>). Second, without synaptic adaptation on the feedforward connections from V4 to vlPFC, the feedforward-feedback loop positively reinforces neuronal activity in V4 and PFC units (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1B</xref>), resulting in “ringing” and “blow up” of responses, which is inconsistent with the neuronal data. Third, without half-wave rectification on feedback signals from vlPFC units to V4 units, the model produces a large second peak even in V4 responses to non-preferred stimuli (<xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref>), which is inconsistent with the neuronal data (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). Given that such nonlinearities occur at synapses, our model suggests that feedback from vlPFC may arrive in V4 di-synaptically. Finally, we also illustrate the necessity of the occlusion-dependent gain modulation on vlPFC, without which vlPFC unit responses decrease with increasing occlusion (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1C</xref>). We also repeated our model simulations for a range of parameter values associated with V4–vlPFC feedforward connection strengths (<xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref>), feedback synaptic strengths and delays (<xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref> and <xref ref-type="fig" rid="fig10s6">Figure 10—figure supplement 6</xref>) and the timing of gain modulation (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref>). We hope these results validate the necessity of the proposed model architecture and parameter regimes.</p><p><italic>7) On the interpretation of the effects in V4:</italic> </p><p><italic>In the first paragraph of the subsection “Representation of occluded stimuli in vlPFC” the authors argue that the responses of some of the vlPFC neurons that are stronger if occlusion is stronger does not depend on the increased task difficulty or attentional demands, but this reasoning is unclear. Furthermore, the authors seem to have changed their mind in the fourth paragraph of the subsection 2Representation of occluded stimuli in vlPFC”, where they argue that vlPFC may amplify weak signals and that vlPFC is engaged in tasks of greater difficulty or cognitive demand.</italic> </p><p>We have revised relevant portions of the text to clarify our points:</p><p>“Given that many vlPFC neurons are selective for the shape of the occluded stimulus, it is unlikely that the observed vlPFC responses solely reflect task difficulty level or attentional demands. If difficulty or attention could fully explain vlPFC responses, then occlusion sensitive neurons would not be shape-selective (i.e. the PSTHs in <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig2">Figure 2B</xref> would be identical).”</p><p>“Our results are consistent with vlPFC’s engagement in tasks of greater difficulty or cognitive demand (Crittenden and Duncan, 2014). Importantly, when the task becomes difficult, rather than reflecting difficulty per se, we propose that vlPFC responses amplify behaviorally relevant signals to facilitate perceptual decisions.”</p><p><italic>In the first paragraph of the subsection “Representation of occluded stimuli in vlPFC” the authors argue that because many vlPFC neurons were tuned to shape, that these neurons therefore cannot reflect task difficulty or attentional demands. This argument does not hold because neurons may well be tuned to multiple aspects of a task.</italic> </p><p>We have revised the text to clarify our point (see our responses to the previous point). The reviewers are correct in that vlPFC neurons can be tuned to multiple aspects of a task. Our assertion is simply that the responses of vlPFC neurons in our dataset cannot solely reflect attentional demands or task difficulty because these responses also signal the identity of the occluded shape.</p><p><italic>8) How did you know whether penetrations were indeed in vlPFC? Was histology performed?</italic> </p><p>We have added text to the Materials and methods section to address this question. Briefly, we conduced structural MRIs for each monkey and localized the principal sulcus and arcuate sulcus based on these data. We then positioned vlPFC recording chambers based on these stereotactic coordinates, centered roughly at 21 mm anterior of interaural zero and 19 mm lateral to the midline.</p><p>With regards to histological confirmation, the monkeys are currently contributing to neurophysiological studies of IT, thus precluding histological analysis.</p><p><italic>9) [Additional comment sent to the authors in response to authors’ plan for revision]: The authors should clarify if there is a real dichotomy between neurons with one versus two peaks, as well as how broad the distribution of the timing for the second peak is. Can they really convince the reader that they are not simply amplifying noise with their analysis?</italic> </p><p>We address this concern with several new analyses and supplementary figures (<xref ref-type="fig" rid="fig8s5">Figure 8—figure supplement 5</xref> and <xref ref-type="fig" rid="fig8s6">Figure 8—figure supplement 6</xref>). We summarize our approach to these manuscript revisions below.</p><p>i) Our revised peak-finding algorithm now includes a statistical test. Only those neurons that show a statistically significant increase in response around the time of a putative second peak are classified as having two peaks.</p><p>ii) To further guard against false positives, we present a new, model-based procedure for identifying neurons with two peaks that is independent of the ad hoc peak-finding algorithm. We show that this model-based procedure works well for example neurons (<xref ref-type="fig" rid="fig8s5">Figure 8—figure supplement 5</xref>) and that the results generated show good correspondence with the results from the revised peak-finding algorithm (<xref ref-type="fig" rid="fig8s6">Figure 8—figure supplement 6</xref>).</p><p>iii) Rather than a complete dichotomy, we propose a continuum of functional response properties, akin to the continuum of V1 simple and complex cells. The strength of connections between V4 and vlPFC are likely to be heterogeneous, and our model simulations demonstrate that even weak functional interactions between the two areas could result in a small, second response peak in V4 that may be undetectable in highly variable responses (<xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref> and <xref ref-type="fig" rid="fig10s5">Figure 10—figure supplement 5</xref>).</p><p>iv) With regards the timing of the second peak, we designed our peak-detection algorithm to identify response transients within a 300 ms window from test stimulus onset. This choice of temporal window was motivated by the fact that a large fraction of vlPFC neurons responded within 150ms of test stimulus onset. No additional neurons passed the criterion for significance when we extended this window to 500 ms (to exclude responses associated with the offset of the test stimulus at 600 ms). Thus, the distribution of second peak times shown (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) cannot be attributed to our choice of temporal window.</p><p><italic>Furthermore, they now make the point that the tuning is stronger during the second peak. We would like to know if this is not simply predicted by the presence of extra spikes – i.e. the presence of a peak implies some extra spikes at a certain point in time.</italic></p><p>We address this concern with a new analysis and a supplementary figure (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>).Shape selectivity for occluded stimuli was stronger at the time of the second peak even for neurons with two peaks whose responses to the preferred shape were stronger during the first peak than the second peak (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2A</xref>). Thus, extra spikes at the time of the second peak cannot entirely predict stronger shape selectivity during this epoch. For neurons that showed stronger shape selectivity during the second peak, the magnitude of the second peak relative to the first was larger for the preferred shape than the non-preferred shape (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2B</xref>). We argue that this differential enhancement of responses to the preferred shape serves to amplify shape selectivity.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p><italic>Reviewer #3:</italic> </p><p><italic>In their revision, Fyall et al. have addressed many of my concerns satisfactorily. They have made it clearer that some of the V4 neurons have a second peak that is more prominent in case of occlusion (<xref ref-type="fig" rid="fig4">Figure 4C</xref> represents a compelling example). Also, the peak detection method is now more convincing and it is also better documented. It remains unclear whether activity in vlPFC indeed contributes to late V4 activity and it is therefore conceivable that there are additional areas that could contribute to late V4 activity. Yet, I do realize that demonstrating the causal link between dlPFC and V4 would require a different approach, which would be beyond the scope of the present contribution. However, establishing such a causal link might be an important topic for future research, and the authors could mention this point, which could be added to the paragraph of suggested future work (subsection “Response dynamics in V4”, fifth paragraph).</italic> </p><p>We thank the reviewer for raising this point, which we have now incorporated into the Discussion.</p><p><italic>Remaining points:</italic> </p><p><italic>1) I find it difficult to understand why the vlPFC neurons do not respond so well when the occluders have the same color of the background (subsection “Representation of occluded stimuli in vlPFC”, second paragraph). I would suspect that the processes for shape recognition would remain the same. Or did the monkeys' performance show signs that this was not the case?</italic> </p><p>We did not observe a difference in behavioral performance (% correct) when the occluders were in the same color as the background. However, given that we used a fixed duration task design, we do not know if behavioral reaction times might have been affected by the color of the occluders. The stronger vlPFC responses we recorded when the occluders were in a contrasting color supports our hypothesis that vlPFC responses arise from the modulation of occlusion-dependent, shape-selective feedforward signals from V4 by another feedforward signal that is dependent only on occlusion level. The latter signal may be weaker when the occluders were in the same color as the background, thus yielding weaker vlPFC responses.</p><p><italic>2) Quite some p-values are lacking, three examples:</italic> </p><p><italic>&quot;The responses of most of these occlusion-sensitive 155 neurons (71/98) increased with increasing occlusion level&quot;.</italic> </p><p><italic>&quot;Even for the small subset of vlPFC neurons that responded more strongly to unoccluded stimuli (27/98), shape selectivity was not stronger for unoccluded than occluded stimuli (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>).&quot;</italic> </p><p>We have re-written the relevant text to clarify this analysis, and to provide the results of significance testing for these two instances pointed out by the reviewer.</p><p>Neurons that were classified as occlusion-sensitive based on a two-way ANOVA (p &lt; 0.05) were divided into two groups based on the sign of the regression slope between occlusion level and responses: 71 neurons had a negative slope, indicating that their responses were stronger for occluded stimuli, whereas 27 neurons had a positive or zero slope, indicating that their responses were strongest or comparable for unoccluded stimuli. Of the 98 occlusion sensitive neurons, 59 neurons had a slope that was significantly less than zero whereas 17 neurons had a slope that was significantly greater than zero (p &lt; 0.05).</p><p><italic>&quot;Shape selectivity for occluded shapes was significantly higher during the second peak than during the first peak.&quot;</italic> </p><p>We have added the significance level to the end of the sentence (t Test, p &lt; 0.01).</p><p><italic>3) &quot;Of 85 neurons, 30 neurons (~35%) were classified as having two peaks&quot;. How were these cells distributed across the two monkeys?</italic> </p><p>Of the 30 neurons classified as having two peaks, 14 were recorded in monkey O and 16 were recorded in monkey M. We include this information in the revised manuscript.</p><p><italic>4) The model with interactions between vlPFC and V4 seems still somewhat simplistic as there are only a few neurons and the variation (in effect size and timing) across neurons shown in the figures is actually a variation across neurons in different models rather than a variation of neurons within the same model. In networks with many units and reciprocal connections, the network dynamics might actually work against variation across neurons. The authors should discuss this. It would be great if it would be possible to show the same range of differences between neurons within a same model, but I will not insist on such a demonstration given that making such a larger model might require a substantial investment of time.</italic> </p><p>We agree with the reviewer that in a model with many neurons and many incoming connections per neuron, dynamics across neurons may be similar due to “averaging” across all the inputs converging onto each neuron. However, in the limiting case where the neural population is large in size, but the number of active incoming connections per neuron is relatively small, substantial variation in inputs and in the response dynamics of individual neurons could persist. A detailed study of the precise conditions of connectivity and active network size needed to account for the observed data would be an interesting future direction for validating the model presented in this manuscript. We now mention this point in Discussion.</p><p><italic>5) &quot;We cannot rule out the possibility that IT responses also contribute to V4 responses during the second transient peak. However, our IT recordings suggest this is unlikely because, as in V4, shape selectivity in IT is stronger for unoccluded than occluded stimuli&quot;. Is it conceivable that some IT neurons also have two phases in their response where the second phase is more pronounced in the presence of occlusion? It would be great if the authors could look for this possibility in the previous data set by Namima and Pasupathy, 2016? If the two phases are there it would strengthen the paper, but it would also be interesting if that is not the case.</italic> </p><p>Our preliminary analyses reveal that only a few IT neurons (8/102) exhibit a second transient response peak that is generally smaller in amplitude than what we have observed in V4. Pending further analyses of the IT dataset, our current thinking is that the second transient response peak is much more prevalent and prominent in V4 than in IT cortex.</p><p><italic>6) Equations 4/5: I failed to see the logic of these equations, would it be possible to clarify this? Equation 9: what is <sub>thr2</sub>?</italic> </p><p>We have revised the Methods section to clarify the logic behind Equations 4 and 5, and the termrthr2.</p><p>Equations 4 and 5 were designed to simulate the input to V4 units (e.g. <xref ref-type="fig" rid="fig10">Figure 10A</xref>) with an onset latency of 30 ms, a strong initial transient response, and a gradually declining sustained response, collectively lasting ~500 ms. Specifically, a difference of Gaussian filter (k) was convolved with the ramp (R), then cubed, normalized and half-wave rectified (Equation 4) to produce the desired input. The ramp function (R), defined separately for the preferred (i = 1) and nonpreferred (i = 2) stimuli (Equation 5), increases monotonically with the% visible area (c) and declines over time with a support of 500 ms (30 &lt; t &lt; 530ms). Note that the precise function defininguFFi is not critical as long as it produces strong input signals for the preferred shape that decrease with increasing occlusion level, thus capturing the observed V4 neuronal response properties.</p><p>In Equation 9, rthr2 represents a threshold value on vlPFC activity. When vlPFC activity exceeds rthr2 (10 spk/sec, see <xref ref-type="table" rid="table1">Table 1</xref>), the steady state feedforward connection from V4 to vlPFC, w∞,ff, goes to 0, and any subsequent input from V4 will fail to activate vlPFC. This synaptic adaptation term was introduced to prevent the second response peak of V4 units from inducing a second response peak in vlPFC units (see <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1B</xref>).</p><p><italic>7) I found <xref ref-type="fig" rid="fig8s6">Figure 8—figure supplement 6A</xref> confusing: how do you compute y/z for neurons with one peak?</italic> </p><p>We now provide additional clarification in the legend of this figure supplement.</p><p>V4 neurons were classified as having one response peak if: i) their responses did not include a candidate second peak with a sizable amplitude and trough-to-peak modulation, or, ii) if their responses did include a candidate second peak that did not pass the statistical criterion. For 43 V4 neurons without a candidate second peak, we cannot compute y/z and these neurons are assigned y/z = 0 in <xref ref-type="fig" rid="fig8s6">Figure 8—figure supplement 6A</xref>. For 12 neurons with a candidate second peak that did not pass the statistical criterion we can compute y/z and the values are as shown.</p></body></sub-article></article>