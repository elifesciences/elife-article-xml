<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">94191</article-id><article-id pub-id-type="doi">10.7554/eLife.94191</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94191.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Visual working memories are abstractions of percepts</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-342854"><name><surname>Duan</surname><given-names>Ziyi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7567-4120</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-73078"><name><surname>Curtis</surname><given-names>Clayton E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0702-1499</contrib-id><email>clayton.curtis@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Psychology, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>31</day><month>05</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP94191</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-24"><day>24</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-12-03"><day>03</day><month>12</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.01.569634"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-02"><day>02</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94191.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-17"><day>17</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94191.2"/></event></pub-history><permissions><copyright-statement>© 2024, Duan and Curtis</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Duan and Curtis</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-94191-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-94191-figures-v1.pdf"/><abstract><p>During perception, decoding the orientation of gratings depends on complex interactions between the orientation of the grating, aperture edges, and topographic structure of the visual map. Here, we aimed to test how aperture biases described during perception affect working memory (WM) decoding. For memoranda, we used gratings multiplied by radial and angular modulators to generate orthogonal aperture biases for identical orientations. Therefore, if WM representations are simply maintained sensory representations, they would have similar aperture biases. If they are abstractions of sensory features, they would be unbiased and the modulator would have no effect on orientation decoding. Neural patterns of delay period activity while maintaining the orientation of gratings with one modulator (e.g. radial) were interchangeable with patterns while maintaining gratings with the other modulator (e.g. angular) in visual and parietal cortex, suggesting that WM representations are insensitive to aperture biases during perception. Then, we visualized memory abstractions of stimuli using models of visual field map properties. Regardless of aperture biases, WM representations of both modulated gratings were recoded into a single oriented line. These results provide strong evidence that visual WM representations are abstractions of percepts, immune to perceptual aperture biases, and compel revisions of WM theory.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>working memory</kwd><kwd>vision</kwd><kwd>visual cortex</kwd><kwd>decoding</kwd><kwd>orientation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>EY016407</award-id><principal-award-recipient><name><surname>Curtis</surname><given-names>Clayton E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>EY033925</award-id><principal-award-recipient><name><surname>Curtis</surname><given-names>Clayton E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Working memory representations are not hard copies of sensory information, even in early visual cortex, but can be recoded into more abstract and goal-directed formats aligned to behavioral goals.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Following now classic studies demonstrating that fMRI patterns of voxel activity in human early visual cortex can be used to decode the contents of visual working memory (WM; <xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib42">Serences et al., 2009</xref>), decoding WM content from visual cortex has been a workhorse for neuroimaging studies testing aspects of the sensory recruitment hypothesis of WM. This incredibly influential hypothesis posits that visual WM storage utilizes the encoding machinery in the visual cortex, assuming that memory and perception utilize similar mechanisms (<xref ref-type="bibr" rid="bib32">Postle, 2006</xref>; <xref ref-type="bibr" rid="bib6">Curtis and D’Esposito, 2003</xref>; <xref ref-type="bibr" rid="bib8">D’Esposito and Postle, 2015</xref>; <xref ref-type="bibr" rid="bib43">Serences, 2016</xref>).</p><p>Research has produced evidence for and against this hypothesis. On the one hand, WM representations can be decoded from the activity patterns as early as primary visual cortex (V1; <xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib42">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Riggall and Postle, 2012</xref>; <xref ref-type="bibr" rid="bib45">Sprague et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Rahmati et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Curtis and Sprague, 2021</xref>). There is even some evidence that classifiers trained on data collected from early visual cortex while participants are simply viewing stimuli (e.g. oriented gratings) can be used to decode the contents of WM (<xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib34">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Albers et al., 2013</xref>). The assumption here is that if sensory representations generated via bottom-up processing are interchangeable with WM representations, then the representation itself is perceptual in nature (although see <xref ref-type="bibr" rid="bib25">Lee et al., 2012</xref>). Finally, the degree to which WM representations in early visual cortex are epiphenomenal or only support memory under impoverished laboratory conditions remains controversial. Some evidence suggests, however, that the neural circuitry in early visual cortex can simultaneously maintain WM representations while encoding incoming and potentially distracting percepts (<xref ref-type="bibr" rid="bib15">Hallenbeck et al., 2021</xref>; <xref ref-type="bibr" rid="bib34">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Lorenc et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Iamshchinina et al., 2021</xref>). Moreover, trialwise variations in these decoded WM representations predict key behavioral factors like errors (<xref ref-type="bibr" rid="bib10">Ester et al., 2013</xref>) and uncertainty of memory (<xref ref-type="bibr" rid="bib27">Li et al., 2021</xref>). Distractor-induced distortions in WM representations also predict the direction and degree of distractor-induced memory errors (<xref ref-type="bibr" rid="bib15">Hallenbeck et al., 2021</xref>). Together, it appears as if memory-guided behaviors depend on a readout of these representations in early visual cortex.</p><p>On the other hand, several pieces of evidence are at odds with the sensory recruitment hypothesis of WM. With perhaps the exception of spatial WM (<xref ref-type="bibr" rid="bib40">Saber et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Hallenbeck et al., 2021</xref>; <xref ref-type="bibr" rid="bib28">Li and Curtis, 2023</xref>; <xref ref-type="bibr" rid="bib47">van Kerkoerle et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Supèr et al., 2001</xref>), persistent activity, the most conclusive neural mechanism of WM, is not characteristic of V1 neurons (<xref ref-type="bibr" rid="bib24">Leavitt et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Curtis and Sprague, 2021</xref>). As mentioned above, fMRI patterns during perception can be used to predict WM content. However, decoding is usually worse compared to when WM data are used to train decoders (<xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib34">Rademaker et al., 2019</xref>), especially in parietal cortex (<xref ref-type="bibr" rid="bib1">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Rademaker et al., 2019</xref>). WM representations in early visual cortex also appear to change over time from when encoding the memoranda to its maintenance throughout the retention interval. These changes appear to reflect reformatting of the representation from one that is more sensory-like to one during WM that is more connected to the demands of the memory-guided behavior (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib28">Li and Curtis, 2023</xref>; <xref ref-type="bibr" rid="bib18">Henderson et al., 2022</xref>) and may explain how WM representations in V1 survive distraction (<xref ref-type="bibr" rid="bib34">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Hallenbeck et al., 2021</xref>).</p><p>Most of these studies that provide evidence for and against the sensory recruitment hypothesis of WM relied on decoding the orientation of gratings with fMRI patterns of voxel acitivity. The general linking hypothesis, therefore, assumes that successful orientation decoding depends on the unique patterns of activity originating from inhomogeneous sampling of orientation columns at fine scales across voxels (<xref ref-type="bibr" rid="bib3">Boynton, 2005</xref>; <xref ref-type="bibr" rid="bib17">Haynes and Rees, 2005</xref>; <xref ref-type="bibr" rid="bib20">Kamitani and Tong, 2005</xref>). However, recent research suggests that coarse, not fine, scale biases at the retinotopic map level, such as a global preference for cardinal and radial orientations (<xref ref-type="bibr" rid="bib13">Freeman et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Mannion et al., 2010</xref>; <xref ref-type="bibr" rid="bib39">Roth et al., 2022</xref>) underlie decoding of orientation during perception. Rather than just a reflection of fine-scale sampling of orientation tuned neurons, orientation decoding also relies on complex interactions between the stimulus’s orientation, its bounding aperture, and topographic inhomogeneities across the visual field map (<xref ref-type="bibr" rid="bib4">Carlson, 2014</xref>; <xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). Despite changes to the hypothesis linking successful decoding of perceived orientation to its underlying causes, it remains unknown how WM decoding might be affected by these coarse-scale biases.</p><p>Here, we directly address this gap by testing how aperture biases affect WM decoding, as well as leveraging these carefully manipulated stimulus properties of gratings to test how sensory-like are WM codes. In order to disambiguate the contributions to orientation decoding, we used as memoranda stimuli with aperture biases that were either aligned with or orthogonal to a grating’s orientation (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). Previewing our results, we found that WM but not perceptual representations in early visual cortex were immune to aperture biases. Using models of V1 (<xref ref-type="bibr" rid="bib44">Simoncelli et al., 1992</xref>) and techniques to visualize the spatial patterns associated with seeing and remembering oriented gratings (<xref ref-type="bibr" rid="bib22">Kok and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib49">Yoo et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Zhou et al., 2022</xref>; <xref ref-type="bibr" rid="bib12">Favila et al., 2022</xref>), WM representations were recoded into line-like patterns across retinotopic cortex. Together, these findings provide strong evidence that visual WM representations are not sensory-like in nature. They are abstractions of percepts and provide evidence that compels revisions to the sensory recruitment hypothesis of WM.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Angular and radial modulators impact orientation decoding during perception but not memory</title><p>We measured fMRI blood-oxygen-level-dependent (BOLD) activity in retinotopic visual field maps (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) in humans when participants performed a delayed orientation WM task using gratings with two types of modulators (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; WM). Participants also performed a separate perceptual control experiment using the same type of stimuli, but without a WM delay (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; perception). Stimuli were created by multiplying oriented sinusoidal gratings (the carrier) with an angular or a radial polar grating (the modulator) to generate orthogonal aperture biases despite having the same orientation (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). Specifically, the radial modulator evokes a coarse-scale bias aligned with the carrier orientation, while the angular modulator evokes a coarse-scale bias orthogonal to the carrier orientation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We predicted that if the format of the memorized orientation is sensory-like in nature, decoding would conform with the aperture bias.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Population receptive field mapping, trial design, and stimuli generation schema.</title><p>(<bold>A</bold>) A separate retinotopic mapping session was used to estimate voxel receptive field parameters for defining visual field maps in visual, parietal, and frontal cortices. Example participant’s left hemisphere is shown.White lines denote the boundaries at the upper vertical meridian (UVM) and black lines denote the lower vertical meridian (LVM). (<bold>B</bold>) For the WM task (left), participants maintained the oriented stimuli over a 12 s retention interval and rotated a recall probe to match their memory. More points were awarded for less errors. For the perceptual control task (right), participants viewed the stimuli twice in a row with a short ISI and asked to decide which one has a higher contrast; it places no demand on remembering orientation. Colors denote different epoch of interests, green denotes stimulus epoch while red denotes delay epoch. (<bold>C</bold>) Each of the stimuli was created by multiplying a vertical or horizontal grating by a radial or angular modulator. These stimuli were used as input to the model. For radial modulated gratings (left in magenta), the model exhibits a radial preference: larger responses to vertical gratings along the vertical meridian and larger responses to horizontal gratings along the horizontal meridian. However, for angular modulated gratings (right in blue), the orientation preference is tangential: larger responses to vertical gratings along the horizontal meridian and larger responses to horizontal gratings along the vertical meridian. Here, we demonstrate the stimulus and aperture bias using vertical and horizontal carrier orientations. In the experiment, the carrier orientations were 15°, 75°, and 135° clockwise from vertical with random jitter (&lt;7°).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig1-v1.tif"/></fig><p>We first aimed to demonstrate that during a simple perception task without WM the radial and angular modulators induce different aperture biases that impact orientation decoding. As predicted, we replicated (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>) that classifiers trained to decode the orientation of gratings altered by one type of modulator could only decode the orientation of gratings altered by the same type of modulator (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; within). Classifiers could not cross-decode orientation gratings altered by the other type of modulator (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; cross) presumably because classification depends on aperture biases that are orthogonal for radial and angular modulated gratings. Note that these effects were limited to visual field maps in early visual cortex (V1-V3).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Decoding orientation during WM and perception.</title><p>(<bold>A</bold>) Orientations could be decoded only within each kind of modulator, but not across different modulators in visual cortex, indicating the influence of the aperture bias on the stimulus in the perceptual task. (<bold>B</bold>) Orientations could be decoded both within and cross modulators in both visual and parietal cortices, suggesting a shared format during the late delay epoch in the WM task. (<bold>C</bold>) When training the classifier based on the neural pattern of the radial modulator (magenta) in the perceptual task, orientations of both radial (within) and angular (cross) modulators could be decoded during the WM late delay epoch in the visual cortex. However, training the classifier based on the angular modulator (blue) could not be generalized, except for V3AB. Results suggest that neural patterns during WM late delay are only similar to perceptual representations when their aperture bias aligns with the orientation bias (radial modulator) in early visual cortex (V1–V3). *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, n.s. Not significant. Error bars represent ±1 SEM. Small circles for each bar represent individual data (n=16). Dashed horizontal line denotes theoretical chance level (1/3), but results are based on non-parametric permutation tests. Results for all ROIs can be seen in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s3">3</xref>. Statistical results can be seen in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a-c</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Within and cross-modulator decoding results by using the stimulus period of the perceptual control task for all ROIs.</title><p>*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, n.s. Not significant. Error bars represent ±1 SEM. Small circles for each bar represent individual data (n=16). Dashed horizontal line denotes theoretical chance level (1/3), but results are based on non-parametric permutation tests.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Within and cross-modulator decoding results by using the late delay period of the WM task for all ROIs.</title><p>*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, n.s. Not significant. Error bars represent ±1 SEM. Small circles for each bar represent individual data (n=16). Dashed horizontal line denotes theoretical chance level (1/3), but results are based on non-parametric permutation tests.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Within and cross-modulator decoding results by training classifiers based on the stimulus period of the control task and testing them on the late delay period of the WM task for all ROIs.</title><p>*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, n.s. Not significant. Error bars represent ±1 SEM. Small circles for each bar represent individual data (n=16). Dashed horizontal line denotes theoretical chance level (1/3), but results are based on non-parametric permutation tests.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig2-figsupp3-v1.tif"/></fig></fig-group><p>Next, we focused on the patterns of late delay period activity during the WM task (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) when the signals were temporally separated from those evoked during visual stimulation. We used this epoch of data for both training classifiers and testing decoding success. We first validated our methods by replicating successful orientation decoding in visual and parietal cortex separately for each type of modulator (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>; within; <xref ref-type="bibr" rid="bib9">Emrich et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib36">Riggall and Postle, 2012</xref>; <xref ref-type="bibr" rid="bib41">Sarma et al., 2016</xref>; <xref ref-type="bibr" rid="bib42">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Yu and Shim, 2017</xref>). Turning to the critical test, we asked if a classifier trained on oriented gratings with one type of modulator (e.g. radial) could be used to successfully cross-decode gratings with the other type of modulator (e.g. angular). Indeed, we found that despite the orthogonal aperture biases induced by the two modulators, their patterns during WM maintenance were interchangeable. Within visual field maps in early and mid visual cortex (V1, V2, V3, V3AB), parietal cortex (IPS0/1, IPS2/3), and frontal cortex (sPCS), classifiers trained on different modulators could cross-decode the orientation of gratings (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>; cross). These results indicate that WM representations of orientation are immune to the aperture biases we demonstrated during perception.</p><p>To further test if WM representations are similar to perception, we next trained classifiers using data from the perceptual control task and measured the extent to which these classifiers can decode orientation during WM, and what effect the modulators have on decoding. In early visual cortex (V1-V3), we found that classifiers trained during perception can be used to decode orientation information in WM, but only when the aperture bias is aligned with the orientation of the grating (i.e. radial modulator; <xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>; within). Similarly, we only observed significant WM decoding across modulator types in early visual cortex when classifiers were trained during perception of the radial (aligned with orientation) but not angular (orthogonal to orientation) modulated grating (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>; cross). These results indicate that WM representations of orientation, which are not biased by aperture, are only similar to perceptual representations when they happen to align with the aperture biases induced during perception. Note that V3AB was a notable exception in that orientation could be decoded regardless of the type of modulator used for training or testing.</p></sec><sec id="s2-2"><title>WM representations are recoded into abstractions of percepts</title><p>The results thus far imply that WM representations in early visual cortex are distinct from perceptual representations. Moreover, WM representations are immune to the aperture biases during perception perhaps because they have been recoded into another format during memory. Next, we aimed to visualize changes in format during perception and WM for oriented gratings with orthogonal aperture biases. We hypothesized that participants recoded in WM the carrier orientation of gratings, regardless of the type of modulator, into line-like images encoded in the spatial distribution of response amplitudes across topographic maps (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib27">Li et al., 2021</xref>). Again using the late delay period activity during the WM task, we constructed the spatial profile of neural activity within visual field maps (<xref ref-type="bibr" rid="bib22">Kok and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib49">Yoo et al., 2022</xref>) for both radial and angular modulated orientation gratings (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Specifically, for each voxel, we weighted its receptive field (the exponent of a Gaussian distribution) by the delay period amplitude and then summed across all voxels within an ROI (see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in Methods). Then, we rotated the reconstruction map for each orientation such that they were all centered at zero degrees (vertical meridian) and averaged across all orientation conditions. Clearly, the visualization technique confirmed our hypothesis and revealed a line encoded in the amplitudes of voxel activity at the angle matching the target orientation during the WM delay in V1-V3AB and IPS0/1 (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), but not other ROIs (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for details). Critically, these line-like representations were matched to the carrier orientation and not the aperture biases induced by the modulator. We statistically confirmed these effects by quantifying the fidelity of reconstructions of the carrier orientation (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Visualizing WM and perception of radial and angular modulated oriented gratings.</title><p>(<bold>A</bold>) Line-like patterns emerged across maps of visual space matching the memorized orientation of carrier gratings regardless of the type of modulator (radial - magenta; angular - blue) during the late delay period of the WM task. Spatial maps were rotated such that all orientations were aligned at 0° (top). The warmer colors correspond to increased amplitude of BOLD activity in voxels with receptive fields corresponding to that portion of the visual field. Best fitting lines (black lines) and the size of the stimulus (black circles) are overlaid. (<bold>B</bold>) Quantitative analysis confirmed the line-like patterns being aligned with the carrier orientation in the WM task. Filtered responses (top row) represent the sum of pixel values within the area of a line-shaped mask (12° length) oriented –90°–90°, where 0° represents the true orientation. Fidelity values (bottom row) are the result of projecting the filtered responses to 0° (see Methods), where higher fidelity values indicate stronger stimulus orientation representations. (<bold>C</bold>) Unlike the WM task, during the perception task the angle of the line-like patterns depended on the type of modulator in early visual areas (V1 and V2), where the line matched the orientation of the aperture bias, not the carrier. Note how the line is orthogonal to the angular modulated carrier in early visual cortex (V1 and V2) but not in later visual field maps (e.g. V3A/B). (<bold>D</bold>) During the perception task, the line-like representations in early visual cortex for radial but not angular modulated orientations result in strong filtered responses and fidelities. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001. Error bars represent±1SEM (n=16). Results for all ROIs can be seen in Figure 3, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref> and <xref ref-type="fig" rid="fig3s2">2</xref>. Statistical results can be seen in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1d-e</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Spatial reconstruction results for the WM task across all ROIs.</title><p>(<bold>A</bold>) Line-like patterns emerged across maps of visual space matching the memorized orientation of carrier gratings regardless of the type of modulator (radial - magenta; angular - blue) during the late delay period of the WM task. Spatial maps were rotated such that all orientations were aligned at 0° (top). The warmer colors correspond to increased amplitude of BOLD activity in voxels with receptive fields corresponding to that portion of the visual field. Best fitting lines (black lines) and the size of the stimulus (black circles) are overlaid. (<bold>B</bold>) Filtered responses (top row) represent the sum of pixel values within the area of a line-shaped mask (12° length) oriented –90°–90°, where 0° represents the true orientation. Fidelity values (bottom row) are the result of projecting the filtered responses to 0° (see Methods), where higher fidelity values indicate stronger stimulus orientation representations. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001. Error bars represent±1SEM (n=16).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Spatial reconstruction results for the perceptual control task across all ROIs.</title><p>(<bold>A</bold>) Line-like patterns emerged across maps of visual space matching the memorized orientation of carrier gratings regardless of the type of modulator (radial - magenta; angular - blue) during the delay period of the perceptual task. Spatial maps were rotated such that all orientations were aligned at 0° (top). The warmer colors correspond to increased amplitude of BOLD activity in voxels with receptive fields corresponding to that portion of the visual field. Best fitting lines (black lines) and the size of the stimulus (black circles) are overlaid. (<bold>B</bold>) Filtered responses (top row) represent the sum of pixel values within the area of a line-shaped mask (12° length) oriented –90°–90°, where 0° represents the true orientation. Fidelity values (bottom row) are the result of projecting the filtered responses to 0° (see Methods), where higher fidelity values indicate stronger stimulus orientation representations. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001. Error bars represent±1SEM (n=16).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Next, we performed the same analyses using the data from the perception control experiment. The spatial maps in V1 and V2 revealed line-like representations of the gratings; however, they were aligned with the carrier orientation only when it was radial modulated (<xref ref-type="fig" rid="fig3">Figure 3C/D</xref>, for other ROIs see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>); angular modulated gratings produced line-like representations that were orthogonal carrier orientation reflecting the influence of stimulus vignetting (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). We compared those reconstructed spatial maps with the simulated responses from an image-computable model based on the properties of V1 (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Simoncelli et al., 1992</xref>). We simulated model outputs of both types of modulated gratings as well as line-like images at angles matching the orientation of the carrier grating (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The spatial maps based on model responses matched those in the perception control task. They showed a clear orthogonal orientation bias induced by the stimulus aperture, while the results for line-like images matched the spatial profile of neural activity during the WM delay.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Modeling and reconstructing spatial maps of perceptual and mnemonic representations in V1.</title><p>At the left, we illustrate the output of the model of V1 depicting the aperture biases aligned and orthogonal to the carrier orientation for radial and angular modulators, respectively. Using these modeled responses as inputs, we visualized the population code employing the measured pRF parameters from V1 (see Methods). In the modeled stimulus spatial map, line-like representations match the aperture biases, which in turn matches the observed data from V1 during the perception task. Critically, during WM storage, the line-like representations are aligned with the memorized carrier orientation in V1, regardless of modulator type. At the right and using the same model of V1, we visualize a WM representation in V1 assuming that participants are maintaining in WM a simple line that matches the carrier orientation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-fig4-v1.tif"/></fig><p>Overall, these results provide solid evidence that mnemonic representations are flexibly recoded into a spatial topographic format that is line-like in nature with angles matching the target orientation. WM appears immune to the aperture biases because its format is an abstraction of the perceptual features underlying the biases.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In attempts to adjudicate conflicting results between monkey and human studies of the role of the PFC in WM, <xref ref-type="bibr" rid="bib6">Curtis and D’Esposito, 2003</xref> hypothesized that the PFC might be the source of top-down control signals that target neurons in sensory areas where WM representations are stored (see also <xref ref-type="bibr" rid="bib32">Postle, 2006</xref>; <xref ref-type="bibr" rid="bib7">Curtis and Sprague, 2021</xref>). Although just a speculation at the time, a few years later key evidence emerged. The orientations of memorized gratings could be decoded from the patterns of voxel activity during WM delays in primary visual cortex (<xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib42">Serences et al., 2009</xref>), supporting the prediction that WM representations could be stored in sensory cortex. What became known as the <italic>sensory recruitment hypothesis</italic> of WM emerged shortly after (<xref ref-type="bibr" rid="bib32">Postle, 2006</xref>; <xref ref-type="bibr" rid="bib6">Curtis and D’Esposito, 2003</xref>; <xref ref-type="bibr" rid="bib8">D’Esposito and Postle, 2015</xref>; <xref ref-type="bibr" rid="bib43">Serences, 2016</xref>), which simply stated that the same neural encoding mechanisms used for perception are also utilized to store WM representations. The findings from the current study have two major and direct implications for this highly influential theory of WM. As we detail next, they provide conclusive evidence that neural representations of percepts are not the same as neural representations of memory, even in early visual cortex. Instead, our evidence indicates that WM representations are reformatted abstractions of percepts.</p><sec id="s3-1"><title>Orientation decoding during perception and memory depends on distinct mechanisms</title><p>First, we situate our results within existing evidence that the neural mechanisms that support perception and WM are shared. The patterns of fMRI voxel activity in early visual cortex during perception of an oriented grating can be used to predict the orientation of a grating stored in WM (<xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib34">Rademaker et al., 2019</xref>). Data such as these have been used to support the idea that the representation of WM features in early visual cortex are sensory-like in nature, presumably because orientation decoding during perception and WM both depend on the activities of neurons with orientation tuning (<xref ref-type="bibr" rid="bib10">Ester et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib42">Serences et al., 2009</xref>). However, the reason why patterns of voxel activity in early visual cortex can be used to decode orientation in simple perception studies has come under scrutiny. Initially, orientation decoding was thought to reflect random voxel sampling of the fine-scale columnar distributions of neurons with orientation tuning (<xref ref-type="bibr" rid="bib3">Boynton, 2005</xref>; <xref ref-type="bibr" rid="bib17">Haynes and Rees, 2005</xref>; <xref ref-type="bibr" rid="bib20">Kamitani and Tong, 2005</xref>). Theoretical (<xref ref-type="bibr" rid="bib4">Carlson, 2014</xref>) and empirical work (<xref ref-type="bibr" rid="bib13">Freeman et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>) argued that decoding depended instead on coarse-scale factors. Specifically, it appears that orientation decoding relies to some degree on the complex interaction between a grating’s orientation, its bounding aperture, and the non-isotropic distribution of orientation tuned neurons across the topographic map of V1. (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>), using the same modulated orientation gratings we used, demonstrated that fMRI decoding of orientation depends on the coarse-scale aperture biases the modulators evoke. Here, we leveraged the precise control of these aperture biases evoked by the modulators to test if WM representations also depend on these aperture biases. First, we replicated the aperture biases in early visual cortex during perception reported by <xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>; <xref ref-type="fig" rid="fig2">Figure 2A</xref>. Second and remarkably, we found that decoding orientation from patterns of activity in early visual cortex during WM delays were immune to the aperture biases noted during perception (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Third, when training classifiers based on the perceptual task, we could only decode orientation during WM when the aperture bias was aligned with the orientation of the carrier grating (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Together, these results provide strong and direct evidence that the patterns of neural activity during perception of an oriented grating are distinct from the patterns during WM for the same grating.</p></sec><sec id="s3-2"><title>Seeing is believing: WM representations are abstractions of percepts</title><p>Next, we addressed <italic>how</italic> WM representations of orientation are different from those during perception. To do so, we first visualized the spatial pattern of population activity within visual field maps by projecting voxel activity from cortex into spatial maps of activity in the coordinates of the physical screen within which stimuli were presented (<xref ref-type="bibr" rid="bib22">Kok and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib49">Yoo et al., 2022</xref>; <xref ref-type="bibr" rid="bib28">Li and Curtis, 2023</xref>; <xref ref-type="bibr" rid="bib12">Favila et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Zhou et al., 2022</xref>). We found line-like representations of WM across many of the visual field maps in the dorsal stream whose angle matched the orientation of memorized gratings regardless of the modulator (radial, angular) and thus, regardless of the alignment between the carrier orientation and induced aperture bias (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). During the perception control task, the line-like patterns were also present in the population response, but the angles of these lines matched the axis of the aperture bias rather than the grating’s orientation (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), again confirming differences between perception and memory. Finally, we used a computational model of V1 that simulated the aperture biases induced by the modulators (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Simoncelli et al., 1992</xref>). Consistent with our empirical data, we found line-like stripes across retinotopic V1 aligned to the aperture bias and not the carrier orientation, providing a plausible explanation for why WM decoding depends on factors other than orientation. Instead, we propose that WM for oriented gratings, no matter what the aperture is, are reformatted into simple spatial codes, like a line (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib28">Li and Curtis, 2023</xref>). These line-like patterns are remarkably similar to simulations of a physical line input to the model of V1 (<xref ref-type="fig" rid="fig4">Figure 4</xref>), suggesting people are storing a simplified abstraction of the physical stimulus. These results also may explain why WM representations do not appear to undergo normalization like perceptual representations (<xref ref-type="bibr" rid="bib2">Bloem et al., 2018</xref>).</p></sec><sec id="s3-3"><title>Concluding remarks</title><p>In summary, we found the WM decoding of orientation is immune to the aperture biases that drive decoding during perceptual studies of orientation. Moreover, WM representations are reformatted into efficient abstractions of percepts such that they most closely support memory guided behavior. Although our previous study also found evidence that oriented gratings were recoded into line-like representations (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>), here we demonstrate that those representations are not driven by aperture biases, but instead reflect abstract line-like representations. These results together necessitate revisions to the sensory recruitment hypothesis of WM because the same stimulus is supported by distinct, and not as predicted interchangeable, patterns of neural activity during perception and memory. If seeing and remembering a stimulus depends on the same encoding mechanisms then one would predict an interchangeable pattern. At the very least, the sensory recruitment hypothesis must be modified to take into account both how WM representations differ from perceptual representations, and how WM representations can morph into different formats that likely depend on the goal of the memory-guided behavior.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Subjects</title><p>Sixteen neurologically healthy volunteers (including the two authors; five females; 20–54 years old) with normal or corrected-to-normal vision participated in this study. Each participant completed three experimental sessions (two for the WM task and one for the control task, ~1 hr 30 min each) and one to two sessions of retinotopic mapping and anatomical scans (~2 hr). The experiments were conducted with the informed consent of each participant. The experimental protocols were approved by the University Committee on Activities involving Human Subjects at New York University associated with IRB-FY2017-1024.</p></sec><sec id="s4-2"><title>Stimuli</title><p>Stimuli were created by multiplying two gratings (a carrier and a modulator; <xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). The carrier grating consisted of a large, oriented sinusoidal Cartesian grating (contrast = 0.8, spatial frequency = 1 cycle/°) presented within an annulus (inner diameter: 1.2°; outer diameter: 12°). The spatial phase of the carrier grating was either 0 or π, counterbalanced within each run. We generated 180 orientations for the carrier grating to cover the whole orientation space during the continuous report task. A gray circular aperture with a diameter of 24.8° (equal to the height of the screen) was presented as the background throughout the experiment.</p><p>The modulator grating was polar-transformed and square wave with hard edges, so that when multiplying with the carrier, it alternates the phase of the carrier and creates apertures. On half of the runs, the modulator produced a set of rings starting from the fovea (radial modulator, scaled with eccentricity). While on the other half of the runs, the modulator produced a set of inward-pointing wedges encircling the fovea (angular modulator). Importantly, the image-computable model of V1 (described below; <xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Simoncelli et al., 1992</xref>) predicted a radial preference for the radial modulated gratings, but a tangential preference for the angular modulated gratings. Therefore, the radial modulator induced a bias that is consistent with the carrier orientation while the angular modulator induced a bias that is orthogonal to the carrier orientation. The modulator grating was either sine phase or cosine phase, counterbalanced within each run and orthogonal to the carrier’s phase. The example of modulators in <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows one kind of the phase conditions.</p><p>Importantly, when changing the orientation of the stimuli for each trial, it only changed the orientation of the carrier grating but not the modulator grating. Therefore, any fMRI activity measured could be attributed to either the orientation of the carrier grating, or an interaction between the orientation of the carrier grating and the static modulator grating.</p></sec><sec id="s4-3"><title>Apparatus setup</title><p>All stimuli were generated by using PsychToolBox in Matlab 2021b and presented by an LCD (VPixx ProPix) projector. The projected image spanned 36.2 cm in height and 64.4 cm in width. The spatial resolution is 1920 × 1080 for all tasks. The refresh rate is 120 Hz for the two tasks in the current study and 60 Hz for the retinotopic mapping tasks.</p></sec><sec id="s4-4"><title>fMRI task</title><p>Each participant completed two sessions for the WM task and one session for the control task on separate days. The two sessions for the WM task were acquired in 2 continuous days while having several days intervals between the WM task and the control task to minimize the task confusion. The sequence of the two tasks was randomly assigned (10 subjects did the WM task first). For both tasks, each session consisted of 10 runs, which cost 1.5–2 hr. Each run had 12 trials for the WM task and 24 trials for the control task. Thus, participants completed 240 trials in total for both tasks. For each run, the target orientation (i.e. the carrier’s orientation) were 15°, 75°, and 135° clockwise from vertical with random jitter (&lt;7°).</p></sec><sec id="s4-5"><title>The WM task</title><p>Participants performed a delayed-estimation WM task where they need to report the remembered orientation for the target stimulus. Each trial began with 0.75 s of central fixation (subtended 0.8° diameters) followed by a target stimulus for 1.5 s. The stimulus was either radial modulated or angular modulated grating, presented in blocked designs and in interleaved order. After a 12 s delay period, participants were asked to rotate a recall probe with a dial to match the remembered orientation within a 4.5 s response window. To avoid visual afterimage, we inserted a 0.6 s noise mask at the beginning of the delay. The recall probe was the same type as the target stimulus to avoid forcing participants to represent the two stimulus types in an abstract manner. Again, when changing the orientation of the recall probe, only the carrier grating but not the modulator was changing. Participants were provided with feedback on the error they made and the points earned based on the error for each trial (100 points for 0°, no points for ≥ 50°, 2 points for each degree). The feedback was displayed for 1.5 s and followed by an inter-trial-interval (ITI) of 6, 9, or 12 s.</p></sec><sec id="s4-6"><title>The perceptual control task</title><p>To better compare mnemonic formats with sensory representations, we asked participants to do an additional control task. Instead of asking participants to remember the orientation of the target stimulus, we presented it twice (1.25 s for each) with a short inter-stimulus-interval (ISI, 0.5 s) and asked participants to discriminate their contrast. The feedback was displayed for 0.5 s and followed by an inter-trial-interval (ITI) of 6, 9, or 12 s. Thus, the two target stimuli were exactly the same except for their contrast. The contrast for each stimulus was generated from a predefined set of 20 contrasts uniformly distributed between 0.5 and 1.0 (0.025 step size). We created 19 levels of task difficulty based on the contrast distance between the two stimuli. Thus, the difficulty ranged from choosing contrast pairs with the largest difference (0.5, easiest) to contrast pairs with the smallest difference (0.025, hardest). Task difficulty level changed based on an adaptive, 1-up-2-down staircase procedure (<xref ref-type="bibr" rid="bib26">Levitt, 1971</xref>) to maintain performance at approximately 70% correct.</p></sec><sec id="s4-7"><title>Retinotopic mapping task</title><p>Each participant was scanned for a separate retinotopic mapping session (8–12 runs) to identify region-of-interest (ROI) and model each voxel’s population receptive field (pRF). Participants ran in either type of attention-demanding tasks: random dot kinematogram (RDK) motion direction discrimination task (2 participants; <xref ref-type="bibr" rid="bib30">Mackey et al., 2017</xref>) or an object image rapid serial visual presentation (RSVP) task (14 participants).</p><p>In the RDK motion discrimination task, participants maintained fixation at the center of the screen while covertly tracking a bar sweeping slowly but discreetly across the screen in four directions (left-to-right, right-to-left, bottom-to-up, up-to-bottom). The bar was divided into three rectangular patches (one central patch and two flanking patches). The dot motion in one of the flanking patches matched the one in the central patch, while the other is the opposite. Participants were asked to discriminate which one is matched. The coherence of dot motions was 100% in the central patch, while the coherence in the flanking patches was staircase by using 2-up-1-down procedure to keep the task difficulty at about 75% accuracy (<xref ref-type="bibr" rid="bib26">Levitt, 1971</xref>).</p><p>In the object image RSVP task, the moving bar that participants need to track consisted of six different object images. In each sweep, participants were asked to report whether the target object image existed among the six images by pressing a button. The target image was pseudo-randomly chosen for each run and was shown at the start of each run to help participants get familiar with it. The presentation duration of object bars was adjusted based on participants’ accuracy in a staircase procedure.</p></sec><sec id="s4-8"><title>MRI data acquisition</title><p>MRI data were acquired on a Siemens Prisma 3T scanner with a 64-channel head/neck coil. For the WM task and the control task, BOLD contrast images were acquired using multiband (MB) 2D GE-EPI (MB factor of 4, 44 slices, 2.5x2.5 x 2.5mm voxel size, FoV 200x200 mm, TE/TR of 30/750ms, P → A phase encoding). Intermittently throughout each scanning session, we also acquired distortion mapping scans to measure field inhomogeneities with both forward and reverse phase encoding using a 2D SE-EPI readout and the number of slices matching that of the GE-EPI (TE/TR: 45.6/3537ms, 3 volumes per phase encode direction). BOLD contrast images for the retinotopic mapping task were acquired in a separate session with a higher resolution (MB factor of 4, 56 slices, 2x2 x 2mm voxel size, FoV 208x208 mm, TE/TR: 42/1300ms, P → A phase encoding). Similarly, we collected distortion mapping scans to measure field inhomogeneities with both forward and reverse phase encoding using a 2D SE-EPI readout and the number of slices matching that of the GE-EPI (TE/TR: 71.8/6690ms). Moreover, we also collected 2 or 3 T1 weighted (192 slices, 0.8x0.8 x 0.8mm voxel size, FoV 256x240 mm, TE/TR: 2.24/2400ms) and 1 or 2 T2 weighted (224 slices, 0.8x0.8 x 0.8mm voxel size, FoV 256x240 mm, TE/TR: 564/3200ms) whole-brain anatomical scans using the Siemens product MPRAGE for each participant.</p></sec><sec id="s4-9"><title>MRI data preprocessing</title><p>We used intensity-normalized high-resolution anatomical scans as input to Freesurfer’s recon-all script (version 6.0) to identify pial and white matter surfaces, which were converted to the SUMA format. This anatomical image processed for each subject was the alignment target for all functional images. For functional preprocessing, we divided each functional session into two to six sub-sessions consisting of two to five task runs split by distortion runs (a pair of spin-echo images acquired in opposite phase encoding directions) and applied all preprocessing steps described below to each sub-session independently.</p><p>First, we corrected functional images for intensity inhomogeneity induced by the high-density receive coil by dividing all images by a smoothed bias field (15 mm FWHM), which was computed as the ratio of signal acquired with the head coil to that of the body coil. Then, to improve co-registration of functional data to the target T1 anatomical image, transformation matrices between functional and anatomical images were computed using distortion-corrected and averaged spin-echo images (distortion scans used to compute distortion fields restricted to the phase-encoding direction). Then, we used the distortion-correction procedure to undistort and motion-correct functional images. The next step was rendering functional data from native acquisition space into un-warped, motion-corrected, and co-registered anatomical space for each participant at the same voxel size as data acquisition (2.5 mm iso-tropic voxel). This volume-space data was projected onto the reconstructed cortical surface, which was projected back into the volume space for all analyses. Finally, we linearly detrended activation values from each voxel from each run. These values were then converted to percent signal change by dividing by the mean of the voxel’s activation values over each run.</p></sec><sec id="s4-10"><title>Retinotopic mapping and region of interest (ROI) definition</title><p>Since the retinotopic mapping scans were acquired with a higher resolution than the experimental scans, we projected the retinotopic time series data onto the surface from its original space (2 mm), then from the surface to volume space at the task voxel resolution (2.5 mm). This ensured that estimates of variance-explained faithfully reflected the goodness of fit and were not impacted by smoothing incurred from transforming fit parameter values between different voxel grids.</p><p>We fitted a population receptive field (pRF) model with compressive spatial summation to the averaged time series across all retinotopy runs for each participant after smoothing on the surface with 5 mm FWHM Gaussian kernel (<xref ref-type="bibr" rid="bib21">Kay et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Wandell et al., 2007</xref>). Then, we projected the best-fit polar angle and eccentricity parameters onto each participant’s inflated brain surface map via AFNI and SUMA. ROIs were drawn on the surface based on established criteria for polar angle reversals and foveal representations (<xref ref-type="bibr" rid="bib30">Mackey et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Wandell et al., 2007</xref>). We set a threshold to only include voxels with greater than 10% variance explained by the pRF model. We defined bilateral visual ROIs, V1, V2, V3, V3AB, IPS0, IPS1, IPS2, IPS3, iPCS, and sPCS.</p></sec><sec id="s4-11"><title>fMRI data analysis: decoding accuracy</title><p>All decoding analyses were performed using the multinomial logistic regression with custom code based on the Princeton MVPA toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/princetonuniversity/princeton-mvpa-toolbox">https://github.com/princetonuniversity/princeton-mvpa-toolbox</ext-link>; <xref ref-type="bibr" rid="bib33">PrincetonUniversity, 2016</xref>). We used Softmax and cross entropy as the activation and performance functions, which are suitable for multi-class linear classification problems (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>). The scaled conjugate gradient method was used to fit the weights and bias parameters.</p></sec><sec id="s4-12"><title>WM task decoding analysis</title><p>For the main task, we focused on the delay epoch to test the abstract representational format in WM. First, we performed within-modulator decoding from the same modulator type to verify the reliable orientation information during the WM delay epoch. Then, we conducted cross-modulator decoding from different modulator types (e.g. training on the angular modulator and testing on the radial modulator). We were mostly interested in the cross-modulator decoding results to examine whether WM forms an abstract representation across different modulator types.</p><p>Decoding analysis was performed on the beta coefficients acquired from running a voxel-wise general linear model (GLM) using AFNI 3dDeconvolve. For each participant, we used GLM to estimate the responses of each voxel to the stimulus encoding, delay, and response epochs. Note that, to better separate data from delay epoch from encoding epoch, we modeled the second half of the whole delay period (late delay). Using the whole delay did not change any of the results we reported here. Each epoch was modeled by the convolution of a canonical model of the hemodynamic impulse response function with a square wave (boxcar regressor) whose duration was equal to the duration of the corresponding epoch. Importantly, we estimated beta coefficients for every trial independently for the late delay epoch in performing the decoding analysis. Other epochs were estimated using a common regressor for all trials (<xref ref-type="bibr" rid="bib37">Rissman et al., 2004</xref>). This method was used to capitalize on the trial-by-trial variability of the epoch of interest while preventing the trial-by-trial variability of other epochs from soaking up a large portion of variance which could potentially be explained by the epoch of interest. Six motion regressors were included to account for movement during the scan. Each voxel’s beta coefficients were z-scored within each run independently before the decoding analysis.</p><p>We performed a three-way classification to decode the three target orientation conditions, which were 15°, 75°, and 135° clockwise from vertical. For within-modulator decoding, we used leave-one-run-out cross-validation procedure, in which all trials in one run were left out on each iteration to test the performance of the classifier trained on the data from all other runs. For cross-stimulus decoding, the classifier was trained on beta coefficients of all trials in one modulator condition and tested on all trials in the other modulator condition.</p></sec><sec id="s4-13"><title>Control task decoding analysis</title><p>To get better control and verify the existence of the stimulus vignetting effect (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>), we conducted a purely perceptual task and performed the same analysis on the stimulus epoch data from this task. Based on previous findings, we expected to find reliable above-chance decoding performance for within-modulator decoding, but not for cross-modulator decoding.</p></sec><sec id="s4-14"><title>Cross-task decoding analysis</title><p>We also performed cross-task decoding to test how neural representational formats change for different task goals. For each modulator type (e.g. angular modulator), we trained the classifier based on the stimulus epoch data in the control task and tested it on both the stimulus epoch and the delay epoch data in the WM task for both modulator types (i.e. angular and radial modulators). We were mainly interested in testing the classifier on the late delay epoch data in the WM task. If the WM representations changed to a common format for both modulator types to match the orientation bias, we expected to find a reliable above-chance decoding when training the classifier based on the radial modulator but not the angular modular type. This is because the radial modulator induces a bias that is consistent with the carrier orientation, while the angular modulator induces an orthogonal bias compared to the carrier orientation.</p></sec><sec id="s4-15"><title>fMRI data analysis: spatial reconstruction</title><p>To visualize the spatial profile of neural activity during the epoch of interest, we projected voxel amplitudes onto the 2D visual field space for each orientation condition and each ROI across all participants. Specifically, we first averaged the beta coefficients (β) from GLM for all trials in each orientation condition. Then, for each voxel, we weighted its receptive field (the exponent of a Gaussian distribution) by the averaged β. Finally, we summed the weighted receptive fields across all voxels within a certain ROI for each orientation condition. To account for the individual differences in the pRF structure, we normalized the spatial profile for each participant and then got the averaged spatial profile across all participants. For generating all the spatial reconstruction maps, we downsampled the resolution of the visual field space such that each pixel corresponded to 0.1 of visual angle. Only voxels whose pRF eccentricities were within 20 degrees of visual angle were included in the reconstruction (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>).</p><p>For each orientation condition <inline-formula><mml:math id="inf1"><mml:mi>i</mml:mi></mml:math></inline-formula>, the sum <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of all voxels’ weighted receptive fields (assuming the number of voxels in a certain ROI is <inline-formula><mml:math id="inf3"><mml:mi>m</mml:mi></mml:math></inline-formula>) could be computed as <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, where <inline-formula><mml:math id="inf4"><mml:mi>j</mml:mi></mml:math></inline-formula> is the index of each voxel; <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and <inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the center and width of the voxel’s receptive field. x and y are the positions in the reconstruction map at which the receptive fields were evaluated.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To better visualize the line format, we fitted a first-degree polynomial to the reconstructed map in <xref ref-type="fig" rid="fig3">Figure 3A and C</xref> (black lines). Specifically, we selected pixels within the stimulus size with the top 10% image intensity and fit these pixels’ coordinates to a first-degree polynomial with a constraint that the fitted polynomial passed through the center. To account for the difference in image intensity between different pixels, we conducted a weighted fit, in which the weight corresponds to the voxel’s rank in terms of its image intensity. We were mainly interested in comparing the spatial reconstruction maps between the delay epoch in the WM task and the stimulus epoch in the control task. The visualization provided us with an intuitive understanding of how representational formats changed from perception to WM, and what drove the different decoding results.</p></sec><sec id="s4-16"><title>Model simulation: image-computable model of V1</title><p>We used an image-computable model to predict fMRI responses of V1 to different types of stimuli for visual perception (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). We first simulated model outputs to different modulator types and then predicted fMRI responses by using pRF sampling analysis. To better visualize the model predictions, we conducted the same spatial reconstruction based on the simulated fMRI responses.</p></sec><sec id="s4-17"><title>Simulate model outputs</title><p>The image-computable model was based on the steerable pyramid model of V1 (<xref ref-type="bibr" rid="bib44">Simoncelli et al., 1992</xref>), a subband image transform that decomposes an image into orientation and spatial frequency channels. Responses of many linear receptive fields (RFs) were simulated, each of which computed a weighted sum of the stimulus image. The weights determined the spatial frequency and orientation tuning of the linear RFs, which were hypothetical basis sets of spatial frequency and orientation tuning curves of V1. RFs with the same orientation and spatial frequency tuning but different location preferences were channels. In the model, the number of spatial frequency channels, orientation channels, and orientation bandwidth were adjustable. For the model simulation, we used six orientation bands (bandwidth = 180°/6=30°) and a spatial frequency bandwidth of 0.5 octaves as in previous studies (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>; <xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>). Using four or more bands with correspondingly broader or narrower tuning curves yielded similar results supporting the same conclusions. The number of spatial frequency channels was determined by the size of the input image and the spatial frequency bandwidth. We used images that were 1920 × 1080 pixels, which resulted in 16 levels/scales for the model. The input images had the same configurations (size of fixation, inner aperture, outer aperture, etc) as the stimuli in both the WM task and the control task. The model outputs were images of the same resolution as the input images, in which each pixel can be thought of as a simulated neuron in the retinotopic map of V1. Importantly, we summed the model responses across all orientation channels, which resulted in a model without any orientation tuning.</p><p>For both types of stimuli, we used three target orientations (15°, 75°, and 135° clockwise from vertical), two phases for the carrier (0 or π), and two phases for the modulator (sine or cosine phase). We first generated the model responses to each phase condition separately, then averaged them across all phases for each orientation condition. This yielded three sets of simulated voxel maps, within which we had 16 maps for all subbands. For the final predicted responses, we chose the subband with maximal responses (the 9th level), which corresponds to the spatial frequency of the stimulus (<xref ref-type="bibr" rid="bib38">Roth et al., 2018</xref>).</p></sec><sec id="s4-18"><title>pRF sampling analysis</title><p>To simulate an fMRI voxel’s response to the stimuli, each participant’s pRF Gaussian parameters of V1 were used to weight the model outputs, which resulted in a weighted sum of neural responses corresponding to pRFs. For each orientation condition <inline-formula><mml:math id="inf8"><mml:mi>i</mml:mi></mml:math></inline-formula>, the sampled fMRI BOLD signal (<inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) for voxel <inline-formula><mml:math id="inf10"><mml:mi>j</mml:mi></mml:math></inline-formula> with a pRF centered at <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and standard deviation of <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , is computed as the dot product between the pRF and the model output (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) as in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. x and y are the positions in the model outputs at which the receptive fields were evaluated.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Finally, we performed the same spatial reconstruction analysis on these simulated BOLD signals after normalizing (z-score) across the three orientation conditions. To account for the individual differences in the pRF structure, we normalized the spatial profile for each participant’s simulation and then got the averaged spatial profile across all participants. This was done separately for each of the two modulators.</p></sec><sec id="s4-19"><title>Eye-tracking setup and analyses</title><p>For all imaging sessions, we measured eye position using an EyeLink 1000 Plus infrared video-based eye tracker (SR Research) mounted beneath the screen inside the scanner bore operating at 500 Hz. The camera always tracked the participant’s right eye, and we calibrated using either a 9-point (WM task and perceptual control task) or 5-point (retinotopic mapping task) calibration routine at the beginning of the session and as necessary between runs. We monitored gaze data and adjusted pupil/ corneal reflection detection parameters as necessary during and/or between each run.</p><p>We preprocessed raw gaze data using fully-automated procedures implemented within iEye_ts (<ext-link ext-link-type="uri" xlink:href="https://github.com/clayspacelab/iEye">https://github.com/clayspacelab/iEye</ext-link>, copy archived at <xref ref-type="bibr" rid="bib5">clayspacelab, 2024</xref>). Eye positions were not monitored for S04 during the first and for S16 during both of the two WM task sessions due to technical issues. Overall, 97.72% (radial) and 96.79% (angular) of the total number of eye position sample points during the delay epoch of the WM task across all subjects were within 2° eccentricity from the center (the fixation and the stimulus subtended 0.8° and 12° diameter, respectively). The circular correlation between the polar angle of the target orientation and the polar angle of the eye positions was not significant for both the radial (mean = 0.030, s.d.=0.103, <italic>t</italic>(14)=1.141, p=0.273) and the angular (mean = 0.001, s.d.=0.099, <italic>t</italic>(14)=0.056, p=0.956) modulator, suggesting that the eye movements could not account for our findings.</p></sec><sec id="s4-20"><title>Quantification and statistical analysis</title><p>No data were excluded for analysis. All statistical results reported here were based on permutation tests over 1000 iterations. To test whether decoding accuracy was significantly greater than chance level (1/3), we generated permuted null distributions of decoding accuracy values for each participant, ROI, decoding type (within/cross), modulator type (angular/radial), and each time point for the temporal decoding analysis. On each iteration, we shuffled the training data matrix (voxels x trials) for both dimensions so that both voxel information and orientation labels were shuffled. Then, we performed the decoding analysis based on the shuffled data. This procedure was conducted for each of the 16 participants, resulting in 16 null distributions of decoding accuracy. Combining the null decoding accuracy across all participants resulted in one t-statistic per permutation. To test across-participants decoding accuracy against chance level (1/3), we compared the t-statistic calculated from the intact data against the permuted null distribution of t-statistic for each condition and ROI. The p-value was calculated as the proportion of permuted t-statistics that were greater than or equal to the t-statistic using the intact data.</p><p>For the spatial reconstruction analysis, we computed reconstruction fidelity to quantify the amount of orientation information in each reconstruction map. Specifically, we first created line filters, whose length was equal to the stimulus’s diameter (12°), with orientations evenly spaced between –90° and 90° in steps of 1°. Then, we created masks around these line filters based on two rules. First, coordinates formed an acute angle to the oriented line filter (dot product &gt;0). Second, to constrain the width of line filters, the projected distance squared was less than 1000 (<xref ref-type="bibr" rid="bib23">Kwak and Curtis, 2022</xref>), using different thresholds did not change the results. We chose pixels within these masked areas and summed up the intensities. After z-scoring the summed intensities within each orientation condition, we rotated the response function so that the center is the target orientation. The final tuning curve-like response function was averaged across all three orientation conditions. To compute fidelity, we projected the filtered responses at each orientation filter onto a vector centered on the true orientation (0°) and took the mean of all the projected vectors. Conceptually, this metric measured whether and how strongly reconstruction on average points in the correct direction.</p><p>The same procedure for statistical analysis was used for the reconstruction fidelity, with the exception that the null hypothesis for the t-statistic was 0. Specifically, the data-derived fidelity value was compared against the distribution of null fidelity values from shuffled data. To generate the null distribution, the matrix of beta coefficients was shuffled across both the voxel and orientation condition label dimensions, and the shuffled beta coefficients were used to weight the voxels’ pRF parameters.</p><p>To test whether there were differences in decoding accuracy (and reconstruction fidelity value) between the decoding type and modulator type within each ROI, we used permutation-based two-way repeated-measures analysis of variance (ANOVA). For each permutation, we shuffled the condition labels (decoding type and modulator type) per participant and calculated the null F-statistic. We repeated this procedure 1000 times and got the null distribution of the F-statistic. We compared the F-statistic derived from the intact data with the null distribution to get the p-value. Significant effects were followed up with post-hoc paired-sample t-tests, and the p-value was calculated by comparing the t-statistic derived by the intact data against a permuted null distribution of t-statistics generated by shuffling condition labels. The p-value was corrected by using a false-discovery rate (FDR) procedure for multiple comparisons.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Validation, Investigation, Visualization, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experiments were conducted with the informed consent of each participant. The experimental protocols were approved by the University Committee on Activities involving Human Subjects at New York University.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Decoding accuracy tables with statistical results.</title><p>(<bold>a</bold>) Decoding accuracy for the stimulus-presenting epoch in the perceptual control task. (<bold>b</bold>) Decoding accuracy for the late delay epoch in the WM task. (<bold>c</bold>) Decoding accuracy for the cross-task decoding by training the classifier in the perceptual control task and testing it in the WM task. (<bold>d</bold>) Reconstruction fidelity values for the late delay epoch in the WM task. (<bold>e</bold>) Reconstruction fidelity values for the stimulus-presenting epoch in the perceptual control task.</p></caption><media xlink:href="elife-94191-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-94191-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The processed fMRI data and raw behavioral data generated in this study have been deposited in the Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/KWS9B">https://doi.org/10.17605/OSF.IO/KWS9B</ext-link>. Processed fMRI data contains extracted time series from each voxel of each ROI. We also make publicly available all code that was used to analyze the fMRI data, implement the theoretic model of V1, and generate the stimuli.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>Z</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data and code for &quot;Visual working memories are abstractions of percepts&quot;</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/KWS9B</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by National Institutes of Health Grants R01 EY016407 and EY033925 to CEC. We thank Jonathan Winawer for helpful comments on earlier versions of the manuscript, and NYU’s Center for Brain Imaging for support.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloem</surname><given-names>IM</given-names></name><name><surname>Watanabe</surname><given-names>YL</given-names></name><name><surname>Kibbe</surname><given-names>MM</given-names></name><name><surname>Ling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual Memories Bypass Normalization</article-title><source>Psychological Science</source><volume>29</volume><fpage>845</fpage><lpage>856</lpage><pub-id pub-id-type="doi">10.1177/0956797617747091</pub-id><pub-id pub-id-type="pmid">29596038</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Imaging orientation selectivity: decoding conscious perception in V1</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>541</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1038/nn0505-541</pub-id><pub-id pub-id-type="pmid">15856054</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orientation decoding in human visual cortex: new insights from an unbiased perspective</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>8373</fpage><lpage>8383</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0548-14.2014</pub-id><pub-id pub-id-type="pmid">24920640</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><collab>clayspacelab</collab></person-group><year iso-8601-date="2024">2024</year><data-title>iEye</data-title><version designator="swh:1:rev:ba2f9cf75e476d9f03d2d943f5ea2be3701e970f">swh:1:rev:ba2f9cf75e476d9f03d2d943f5ea2be3701e970f</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:1fc6a2ba0f8bd61cf3b3aea2ceab7f5c397d4fb5;origin=https://github.com/clayspacelab/iEye;visit=swh:1:snp:36b4954a7291e959c3ee0e73568dc59914bdd918;anchor=swh:1:rev:ba2f9cf75e476d9f03d2d943f5ea2be3701e970f">https://archive.softwareheritage.org/swh:1:dir:1fc6a2ba0f8bd61cf3b3aea2ceab7f5c397d4fb5;origin=https://github.com/clayspacelab/iEye;visit=swh:1:snp:36b4954a7291e959c3ee0e73568dc59914bdd918;anchor=swh:1:rev:ba2f9cf75e476d9f03d2d943f5ea2be3701e970f</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Persistent activity in the prefrontal cortex during working memory</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>415</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00197-9</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Persistent Activity During Working Memory From Front to Back</article-title><source>Frontiers in Neural Circuits</source><volume>15</volume><elocation-id>696060</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2021.696060</pub-id><pub-id pub-id-type="pmid">34366794</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Esposito</surname><given-names>M</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cognitive neuroscience of working memory</article-title><source>Annual Review of Psychology</source><volume>66</volume><fpage>115</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010814-015031</pub-id><pub-id pub-id-type="pmid">25251486</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emrich</surname><given-names>SM</given-names></name><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Larocque</surname><given-names>JJ</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distributed patterns of activity in sensory cortex reflect the precision of multiple items maintained in visual short-term memory</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>6516</fpage><lpage>6523</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5732-12.2013</pub-id><pub-id pub-id-type="pmid">23575849</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Anderson</surname><given-names>DE</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A neural measure of precision in visual working memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>754</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00357</pub-id><pub-id pub-id-type="pmid">23469889</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Parietal and Frontal Cortex Encode Stimulus-Specific Mnemonic Representations during Visual Working Memory</article-title><source>Neuron</source><volume>87</volume><fpage>893</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.013</pub-id><pub-id pub-id-type="pmid">26257053</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Perception and memory have distinct spatial tuning properties in human visual cortex</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>5864</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-33161-8</pub-id><pub-id pub-id-type="pmid">36257949</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Brouwer</surname><given-names>GJ</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Orientation decoding depends on maps, not columns</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>4792</fpage><lpage>4804</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5160-10.2011</pub-id><pub-id pub-id-type="pmid">21451017</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Coarse-scale biases for spirals and orientation in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19695</fpage><lpage>19703</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0889-13.2013</pub-id><pub-id pub-id-type="pmid">24336733</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallenbeck</surname><given-names>GE</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Rahmati</surname><given-names>M</given-names></name><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Working memory representations in visual cortex mediate distraction effects</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>4714</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-24973-1</pub-id><pub-id pub-id-type="pmid">34354071</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Predicting the orientation of invisible stimuli from activity in human primary visual cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>686</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1038/nn1445</pub-id><pub-id pub-id-type="pmid">15852013</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>MM</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible utilization of spatial- and motor-based codes for the storage of visuo-spatial information</article-title><source>eLife</source><volume>11</volume><elocation-id>e75688</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.75688</pub-id><pub-id pub-id-type="pmid">35522567</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Essential considerations for exploring visual working memory storage in the human brain</article-title><source>Visual Cognition</source><volume>29</volume><fpage>425</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1080/13506285.2021.1915902</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Decoding the visual and subjective contents of the human brain</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>679</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1038/nn1444</pub-id><pub-id pub-id-type="pmid">15852014</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Compressive spatial summation in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>481</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1152/jn.00105.2013</pub-id><pub-id pub-id-type="pmid">23615546</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Shape perception simultaneously up- and downregulates neural activity in the primary visual cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>1531</fpage><lpage>1535</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.042</pub-id><pub-id pub-id-type="pmid">24980501</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwak</surname><given-names>Y</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unveiling the abstract format of mnemonic representations</article-title><source>Neuron</source><volume>110</volume><fpage>1822</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.016</pub-id><pub-id pub-id-type="pmid">35395195</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leavitt</surname><given-names>ML</given-names></name><name><surname>Mendoza-Halliday</surname><given-names>D</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sustained Activity Encoding Working Memories: Not Fully Distributed</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>328</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.04.004</pub-id><pub-id pub-id-type="pmid">28515011</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Disentangling visual imagery and perception of real-world objects</article-title><source>NeuroImage</source><volume>59</volume><fpage>4064</fpage><lpage>4073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id><pub-id pub-id-type="pmid">22040738</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Transformed up-down methods in psychoacoustics</article-title><source>The Journal of the Acoustical Society of America</source><volume>49</volume><fpage>467</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1121/1.1912375</pub-id><pub-id pub-id-type="pmid">5541744</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HH</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Yoo</surname><given-names>AH</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Joint representation of working memory and uncertainty in human cortex</article-title><source>Neuron</source><volume>109</volume><fpage>3699</fpage><lpage>3712</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.08.022</pub-id><pub-id pub-id-type="pmid">34525327</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HH</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural population dynamics of human working memory</article-title><source>Current Biology</source><volume>33</volume><fpage>3775</fpage><lpage>3784</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.07.067</pub-id><pub-id pub-id-type="pmid">37595590</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenc</surname><given-names>ES</given-names></name><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Nee</surname><given-names>DE</given-names></name><name><surname>Vandenbroucke</surname><given-names>ARE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible Coding of Visual Working Memory Representations during Distraction</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5267</fpage><lpage>5276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3061-17.2018</pub-id><pub-id pub-id-type="pmid">29739867</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackey</surname><given-names>WE</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual field map clusters in human frontoparietal cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22974</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22974</pub-id><pub-id pub-id-type="pmid">28628004</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mannion</surname><given-names>DJ</given-names></name><name><surname>McDonald</surname><given-names>JS</given-names></name><name><surname>Clifford</surname><given-names>CWG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Orientation anisotropies in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>3465</fpage><lpage>3471</lpage><pub-id pub-id-type="doi">10.1152/jn.00190.2010</pub-id><pub-id pub-id-type="pmid">20410358</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Working memory as an emergent property of the mind and brain</article-title><source>Neuroscience</source><volume>139</volume><fpage>23</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2005.06.005</pub-id><pub-id pub-id-type="pmid">16324795</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><collab>PrincetonUniversity</collab></person-group><year iso-8601-date="2016">2016</year><data-title>Princeton-Mvpa-Toolbox</data-title><version designator="214b50d">214b50d</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/princetonuniversity/princeton-mvpa-toolbox">https://github.com/princetonuniversity/princeton-mvpa-toolbox</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id><pub-id pub-id-type="pmid">31263205</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahmati</surname><given-names>M</given-names></name><name><surname>Saber</surname><given-names>GT</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Population Dynamics of Early Visual Cortex during Working Memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>219</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01196</pub-id><pub-id pub-id-type="pmid">28984524</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12990</fpage><lpage>12998</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1892-12.2012</pub-id><pub-id pub-id-type="pmid">22993416</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rissman</surname><given-names>J</given-names></name><name><surname>Gazzaley</surname><given-names>A</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Measuring functional connectivity during distinct stages of a cognitive task</article-title><source>NeuroImage</source><volume>23</volume><fpage>752</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.06.035</pub-id><pub-id pub-id-type="pmid">15488425</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>ZN</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stimulus vignetting and orientation selectivity in human visual cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e37241</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37241</pub-id><pub-id pub-id-type="pmid">30106372</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>ZN</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Natural scene sampling reveals reliable coarse-scale orientation tuning in human V1</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>6469</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-34134-7</pub-id><pub-id pub-id-type="pmid">36309512</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saber</surname><given-names>GT</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Saccade planning evokes topographically specific activity in the dorsal and ventral streams</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>245</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1687-14.2015</pub-id><pub-id pub-id-type="pmid">25568118</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarma</surname><given-names>A</given-names></name><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Task-specific versus generalized mnemonic representations in parietal and prefrontal cortices</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>143</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1038/nn.4168</pub-id><pub-id pub-id-type="pmid">26595652</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus-specific delay activity in human primary visual cortex</article-title><source>Psychological Science</source><volume>20</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02276.x</pub-id><pub-id pub-id-type="pmid">19170936</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural mechanisms of information storage in visual short-term memory</article-title><source>Vision Research</source><volume>128</volume><fpage>53</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.09.010</pub-id><pub-id pub-id-type="pmid">27668990</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Freeman</surname><given-names>WT</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Shiftable multiscale transforms</article-title><source>IEEE Transactions on Information Theory</source><volume>38</volume><fpage>587</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1109/18.119725</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reconstructions of information in visual spatial working memory degrade with memory load</article-title><source>Current Biology</source><volume>24</volume><fpage>2174</fpage><lpage>2180</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.07.066</pub-id><pub-id pub-id-type="pmid">25201683</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name><name><surname>Lamme</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A neural correlate of working memory in the monkey primary visual cortex</article-title><source>Science</source><volume>293</volume><fpage>120</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1126/science.1060496</pub-id><pub-id pub-id-type="pmid">11441187</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13804</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13804</pub-id><pub-id pub-id-type="pmid">28054544</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Brewer</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.012</pub-id><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>AH</given-names></name><name><surname>Bolaños</surname><given-names>A</given-names></name><name><surname>Hallenbeck</surname><given-names>GE</given-names></name><name><surname>Rahmati</surname><given-names>M</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Behavioral Prioritization Enhances Working Memory Precision and Neural Population Gain</article-title><source>Journal of Cognitive Neuroscience</source><volume>34</volume><fpage>365</fpage><lpage>379</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01804</pub-id><pub-id pub-id-type="pmid">34942647</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Shim</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Occipital, parietal, and frontal cortices selectively maintain task-relevant features of multi-feature objects in visual working memory</article-title><source>NeuroImage</source><volume>157</volume><fpage>97</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.05.055</pub-id><pub-id pub-id-type="pmid">28559190</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Fougnie</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Common Neural Mechanisms Control Attention and Working Memory</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>7110</fpage><lpage>7120</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0443-22.2022</pub-id><pub-id pub-id-type="pmid">35927036</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94191.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Radboud University Nijmegen</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This paper provides <bold>valuable</bold> insights into the neural substrates of human working memory. Through clever experimental design and rigorous analyses, the paper provides <bold>compelling</bold> evidence that the working memory representation of stimulus orientation is a reformatted version of the presented stimulus, though more work is needed to establish more generally that visual working memories are abstractions of percepts. This work will be of broad interest to cognitive neuroscientists working on the neural bases of visual perception and memory.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94191.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors aim to test the sensory recruitment theory of visual memory, which assumes that visual sensory areas are recruited for working memory and that these sensory areas represent visual memories in a similar fashion to how perceptual inputs are represented. To test the overlap between working memory (WM) and perception, the authors use coarse stimulus (aperture) biases that are known to account for (some) orientation decoding in visual cortex (i.e., stimulus energy is higher for parts of an image where a grating orientation is perpendicular to an aperture edge, and stimulus energy drives decoding). Specifically, the authors show gratings (with a given &quot;carrier&quot; orientation) behind two different apertures: One is a radial modulator (with maximal energy aligned with the carrier orientation) and the other an angular modulator (with maximal energy orthogonal to the carrier orientation). When subject detect contrast changes in these stimuli (the perceptual task), orientation decoding only works when training and testing within each modulator, but not across modulators, showing the impact of stimulus energy on decoding performance. Instead, when subjects remember the orientation over a 12s delay, orientation decoding works irrespective of the modulator used. The authors conclude that representations during WM are therefore not &quot;sensory-like&quot;, given that they are immune to aperture biases. This invalidates the sensory recruitment hypothesis, or at least the part assuming that when sensory areas that are recruited during WM, they are recruited in a manner that resembles how these areas are used during perception.</p><p>Strengths:</p><p>Duan and Curtis very convincingly show that aperture effects that are present during perception, do not appear to be present during the working memory delay. Especially when the debate about &quot;why can we decode orientations from human visual cortex&quot; was in full swing, many may have quietly assumed this to be true (e.g., &quot;the memory delay has no stimuli, and ergo no stimulus aperture effects&quot;), but it is definitely not self-evident and nobody ever thought to test it directly until now. In addition to the clear absence of aperture effects during the delay, Duan and Curtis also show that when stimulus energy aligns with the carrier orientation, cross-generalization between perception and memory does work (which could explain why perception-to-memory cross decoding also works). All in all, this is a clever manipulation, and I'm glad someone did it, and did it well.</p><p>Weaknesses:</p><p>There seems to be a major possible confound that prohibits strong conclusions about &quot;abstractions&quot; into &quot;line-like&quot; representation, which is spatial attention. What if subjects simply attend the end points of the carrier grating, or attend to the edge of the screen where the carrier orientation &quot;intersects&quot; in order to do the task? This may also result in reconstructions that have higher bold at areas close to the stimulus/screen edges along the carrier orientation. The question then would be if this is truly an &quot;abstracted representation&quot;, or if subjects are merely using spatial attention to do the task.</p><p>Alternatively (and this reaches back to the &quot;fine vs coarse&quot; debate), another argument could be that during memory, what we are decoding is indeed fine-scale inhomogenous sampling of orientation preferences across many voxels. This is clearly not the most convincing argument, as the spatial reconstructions (e.g., Figure 3A and C) show higher BOLD for voxels with receptive fields that are aligned to the remembered orientation (which is in itself a form of coarse scale bias), but could still play a role.</p><p>To conclude that the spatial reconstruction from the data indeed comes from a line-like representation, you'd need to generate modeled reconstructions of all possible stimuli and representations. Yes, Figure 4 shows that a line results in a modeled spatial map that resembles the WM data, but many other stimuli might too, and some may better match the data. For example, the alternative hypothesis (attention to grating endpoints) may very well lead to a very comparable model output to the one from a line. But testing this would not suffice, as there may be an inherent inverse problem (with multiple stimuli that can lead to the same visual field model).</p><p>The main conclusion, and title of the paper, that visual working memories are abstractions of percepts, is therefore not supported. Subjects could be using spatial attention, for example. Furthermore, even if it is true that gratings are abstracted into lines, this form of abstraction would not generalize to any non-spatial feature (e.g., color cannot become a line, contrast cannot become a line, etc.), which means it has limited explanatory power.</p><p>Additional context:</p><p>The working memory and perception tasks are rather different. In this case, the perception task does not require the subject to process the carrier orientation (which is largely occluded, and possibly not that obvious without paying attention to it), but attention is paid to contrast. In this scenario, stimulus energy may dominate the signal. In the WM task, subjects have to work out what orientation is shown to do the task. Given that the sensory stimulus in both tasks is brief (1.5s during memory encoding, and 2.5s total in the perceptual task), it would be interesting to look at decoding (and reconstructions) for the WM stimulus epoch. If abstraction (into a line) happens in working memory, then this perceptual part of the task should still be susceptible to aperture biases. It allows the authors to show that it is indeed during memory (and not merely the task or attentional state of the subject) that abstraction occurs.</p><p>What's also interesting is what happens in the passive perceptual condition, and the fact that spatial reconstructions for areas beyond V1 and V2 (i.e., V3, V3AB, and IPS0-1) align with (implied) grating endpoints, even when an angular modulator is used (Figure 3C). Are these areas also &quot;abstracting&quot; the stimulus (in a line-like format)?</p><p>Review after revision:</p><p>(1) It's nice of the authors to simulate how a dot stimulus affects the image computable model, but this does not entirely address my concern about attention to endpoints. The assumption that attention can be used in the same manner as a physical stimulus to calculate stimulus energy is questionable. (also, why would a dot at 15º lead to high stimulus energy tangential to that orientation?). This simulation also does not at all address my concern about model mimicry (many possible inputs can lead to a line-like output).</p><p>(2) It's also nice that the authors agree that much more work needs to be done, and these results may not generalize to all forms of memory. Given this agreement, and until that &quot;more work&quot; is done, I strongly believe we should refrain from making hyperbolic claims that might preemptively imply all visual working memories are abstractions of percepts. Time (and much more work) will likely show things to be much more subtle and complex.</p><p>The work presented in this paper is cool, but it uses a specific case: spatial stimuli (gratings) with the task to remember orientation. This limits possible conclusions for several reasons (1) These results are specific to EVC, as visual maps are a prerequisite meaning that these results will not hold up in other, non-retinotopic areas. (2) The fact that subjects are &quot;focusing&quot; along the main stimulus axis (attention or not) can simply be a strategy employed by the majority of (but not all) subjects - a strategy that may not be necessary to do the task, and therefore not a canonical method of Abstraction. It may be a &quot;shared preferred strategy&quot; or something. (3) If subjects had to (for example) remember contrast, and not orientation, results may have been entirely different (I would hypothesize there is no line-like abstraction in this case). Vice versa, if the perceptual task would have been on orientation (instead of contrast), the authors admit that &quot;participants would reformat the grating into a line-like representation to make the judgments&quot; (quote from author's response under &quot;Additional context&quot;). Thus, the results may be entirely about the task/ cognitive state, and not about how perceptual information is abstracted into memory.</p><p>Instead of unveiling *the* working memory Abstraction, this work (very nicely) shows a specific instance of possible abstraction. A more correct (but admittedly, less &quot;sexy&quot;) conclusion may be &quot;Visual working memories of orientation can be abstracted into a line in early visual cortex&quot;. As it stands, the authors still do not acknowledge any of the alternatives that myself (see above) and the other reviewers have put forth, nor do they acknowledge recent work by Chunharas et al. (2023, BioRxiv), that directly applies principles of efficient coding to address the exact same question of working memory abstraction. The link between a &quot;line-like&quot; representation and efficient coding implied by the authors (in their response) is merely tentative to me, but it would be great if the authors could explain this further.</p><p>These were, and remain, the major weaknesses in the original submission, that in my view have not been adequately addressed by the authors, as many overly broad conclusions about abstractions are currently still present in the manuscript (in for example the title).</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94191.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work, Duan and Curtis addressed an important issue related to the nature of working memory representations. This work is motivated by findings illustrating that orientation decoding performance for perceptual representations can be biased by the stimulus aperture (modulator). Here, the authors examined whether the decoding performance for working memory representations is similarly influenced by these aperture biases. The results provide convincing evidence that working memory representations have a different representational structure, as the decoding performance was not influenced by the type of stimulus aperture.</p><p>Strengths:</p><p>The strength of this work lies in the direct comparison of decoding performance for perceptual representations with working memory representations. The authors take well-motivated approach and illustrate that perceptual and working memory representations do not share a similar representational structure. The authors test a clear question, with a rigorous approach and provide compelling evidence. First, the presented oriented stimuli are carefully manipulated to create orthogonal biases introduced by the stimulus aperture (radial or angular modulator), regardless of the stimulus carrier orientation. Second, the authors implement advanced methods to decode the orientation information, in visual and parietal cortical regions, when directly perceiving or holding an oriented stimulus in memory. The data illustrates that working memory decoding is not influenced by the type of aperture, while this is the case in perception. In sum, the main claims are important and shed light on the nature of working memory representations.</p><p>Weaknesses:</p><p>After the authors revised the original manuscript, a few of my initial concerns remain.</p><p>(1) Theoretical framing in the introduction. The introduction proposes that decoding of orientation information during perception does not reflect orientation selectivity, and it is instead driven by coarse scale biases. This is an overstatement. Recent work shows that orientation decoding is indeed influenced by coarse biases, but also reflects orientation selectivity (Roth, Kay &amp; Merriam, 2022).</p><p>(2) The description of the image computable V1 model remains incomplete. The steerable pyramid is a model that simulates the responses of V1 neurons. To do so, it incorporates a set of linear receptive fields with varying orientation and spatial frequency tuning. However, the information that is lacking in the Methods is whether the implemented pyramid also included two quadrature phase pairs (odd and even phase Gabor filters making the output phase invariant). The sum of the squares of the responses to these offset phase filters computes the stimulus energy within each orientation and spatial frequency channel. Without this description, it is unclear what the model output represents.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94191.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Duan</surname><given-names>Ziyi</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Curtis</surname><given-names>Clayton E</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1:</bold></p><p>Summary:</p><p>The authors aim to test the sensory recruitment theory of visual memory, which assumes that visual sensory areas are recruited for working memory, and that these sensory areas represent visual memories in a similar fashion to how perceptual inputs are represented. To test the overlap between working memory (WM) and perception, the authors use coarse stimulus (aperture) biases that are known to account for (some) orientation decoding in the visual cortex (i.e., stimulus energy is higher for parts of an image where a grating orientation is perpendicular to an aperture edge, and stimulus energy drives decoding). Specifically, the authors show gratings (with a given &quot;carrier&quot; orientation) behind two different apertures: one is a radial modulator (with maximal energy aligned with the carrier orientation) and the other an angular modulator (with maximal energy orthogonal to the carrier orientation). When the subject detects contrast changes in these stimuli (the perceptual task), orientation decoding only works when training and testing within each modulator, but not across modulators, showing the impact of stimulus energy on decoding performance. Instead, when subjects remember the orientation over a 12s delay, orientation decoding works irrespective of the modulator used. The authors conclude that representations during WM are therefore not &quot;sensory-like&quot;, given that they are immune to aperture biases. This invalidates the sensory recruitment hypothesis, or at least the part assuming that when sensory areas are recruited during WM, they are recruited in a manner that resembles how these areas are used during perception.</p><p>Strengths:</p><p>Duan and Curtis very convincingly show that aperture effects that are present during perception, do not appear to be present during the working memory delay. Especially when the debate about &quot;why can we decode orientations from human visual cortex&quot; was in full swing, many may have quietly assumed this to be true (e.g., &quot;the memory delay has no stimuli, and ergo no stimulus aperture effects&quot;), but it is definitely not self-evident and nobody ever thought to test it directly until now. In addition to the clear absence of aperture effects during the delay, Duan and Curtis also show that when stimulus energy aligns with the carrier orientation, cross-generalization between perception and memory does work (which could explain why perception-to-memory cross-decoding also works). All in all, this is a clever manipulation, and I'm glad someone did it, and did it well.</p><p>Weaknesses:</p><p>There seems to be a major possible confound that prohibits strong conclusions about &quot;abstractions&quot; into &quot;line-like&quot; representation, which is spatial attention. What if subjects simply attend the endpoints of the carrier grating, or attend to the edge of the screen where the carrier orientation &quot;intersects&quot; in order to do the task? This may also result in reconstructions that have higher bold at areas close to the stimulus/screen edges along the carrier orientation. The question then would be if this is truly an &quot;abstracted representation&quot;, or if subjects are merely using spatial attention to do the task.</p><p>Alternatively (and this reaches back to the &quot;fine vs coarse&quot; debate), another argument could be that during memory, what we are decoding is indeed fine-scale inhomogenous sampling of orientation preferences across many voxels. This is clearly not the most convincing argument, as the spatial reconstructions (e.g., Figure 3A and C) show higher BOLD for voxels with receptive fields that are aligned to the remembered orientation (which is in itself a form of coarse-scale bias), but could still play a role.</p><p>To conclude that the spatial reconstruction from the data indeed comes from a line-like representation, you'd need to generate modeled reconstructions of all possible stimuli and representations. Yes, Figure 4 shows that line results in a modeled spatial map that resembles the WM data, but many other stimuli might too, and some may better match the data. For example, the alternative hypothesis (attention to grating endpoints) may very well lead to a very comparable model output to the one from a line. However testing this would not suffice, as there may be an inherent inverse problem (with multiple stimuli that can lead to the same visual field model).</p><p>The main conclusion, and title of the paper, that visual working memories are abstractions of percepts, is therefore not supported. Subjects could be using spatial attention, for example. Furthermore, even if it is true that gratings are abstracted into lines, this form of abstraction would not generalize to any non-spatial feature (e.g., color cannot become a line, contrast cannot become a line, etc.), which means it has limited explanatory power.</p></disp-quote><p>We thank the reviewer for bringing up these excellent questions.</p><p>First, to test the alternative hypothesis of spatial attention, we fed a dot image into the image-computable model. We placed the dot where we suspect one might place their spatial attention, namely, at the edge of the stimulus that is tangent to the orientation of the grating. We generated the model response for three orientations and their combination by rotating and averaging. From Author response image 1 below, one can see that this model does not match the line-like representation we reported. Nonetheless, we would like to avoid making the argument that attention does not play a role. We strongly suspect that if one was attending to multiple places along a path that makes up a line, it would produce the results we observed. But there begins a circularity in the logic, where one cannot distinguish between attention to a line-like representation and a line of attention being the line-like representation.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Reconstruction maps for the dot image at the edge of 15°, 75°, 135°, and the combined across three orientation conditions.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94191-sa3-fig1-v1.tif"/></fig><p>Second, we remain agnostic to the question of whether fine-scale inhomogenous sampling of orientation selective neurons may drive some of the decoding results we report here. It is possible that our line-like representations are driven by neurons tuned to the sample orientation that have receptive fields that lie along the line. Here, we instead focus on testing the idea that WM decoding does not depend on aperture biases.</p><p>Finally, we agree with the reviewer that there is much more work to be done in this area. Our working hypothesis, that WM representations are abstractions of percepts, is admittedly based on Occam's razor and an appeal to efficient coding principles. We also agree that these results may not generalize to all forms of WM (eg, color). As always, there is a tradeoff between interpretability (visual spatial formats in retinotopically organized maps) and generalizability. Frankly, we have no idea how one might be able to test these ideas when subjects might be using the most common type of memory reformatting - linguistic representations, which are incredibly efficient.</p><disp-quote content-type="editor-comment"><p>Additional context:</p><p>The working memory and perception tasks are rather different. In this case, the perception task does not require the subject to process the carrier orientation (which is largely occluded, and possibly not that obvious without paying attention to it), but attention is paid to contrast. In this scenario, stimulus energy may dominate the signal. In the WM task, subjects have to work out what orientation is shown to do the task. Given that the sensory stimulus in both tasks is brief (1.5s during memory encoding, and 2.5s total in the perceptual task), it would be interesting to look at decoding (and reconstructions) for the WM stimulus epoch. If abstraction (into a line) happens in working memory, then this perceptual part of the task should still be susceptible to aperture biases. It allows the authors to show that it is indeed during memory (and not merely the task or attentional state of the subject) that abstraction occurs.</p></disp-quote><p>Again, this is an excellent question. We used a separate perceptual task instead of the stimulus epoch as control mainly for two reasons. First, we used a control task in which participants had to process the contrast, not orientation, of the grating because we were concerned that participants would reformat the grating into a line-like representation to make the judgments. To avoid this, we used a task similar to the one used when previous researchers first found the stimulus vignetting effect (Roth et al., 2018). Again, our main goal was to try to focus on the bottom-up visual features. Second, because of the sluggishness of the BOLD response, combined with our task design (ie, memory delay always followed the target stimulus), we cannot disentangle the visual and memory responses that co-exist at this epoch. Any result could be misleading.</p><disp-quote content-type="editor-comment"><p>What's also interesting is what happens in the passive perceptual condition, and the fact that spatial reconstructions for areas beyond V1 and V2 (i.e., V3, V3AB, and IPS0-1) align with (implied) grating endpoints, even when an angular modulator is used (Figure 3C). Are these areas also &quot;abstracting&quot; the stimulus (in a line-like format)?</p></disp-quote><p>We agree these findings are interesting and replicate what we found in our previous paper (Kwak &amp; Curtis, Neuron, 2022). We believe that these results do imply that these areas indeed store a reformatted line-like WM representation that is not biased by vignetting. We would like to extend a note of caution, however, because the decoding results in the higher order areas(V3AB, IPS0-1, etc) are somewhat poor (especially in comparison to V1, V2, V3) (see Figure 2).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>Summary:</p><p>According to the sensory recruitment model, the contents of working memory (WM) are maintained by activity in the same sensory cortical regions responsible for processing perceptual inputs. A strong version of the sensory recruitment model predicts that stimulus-specific activity patterns measured in sensory brain areas during WM storage should be identical to those measured during perceptual processing. Previous research casts doubt on this hypothesis, but little is known about how stimulus-specific activity patterns during perception and memory differ. Through clever experimental design and rigorous analyses, Duan &amp; Curtis convincingly demonstrate that stimulus-specific representations of remembered items are highly abstracted versions of representations measured during perceptual processing and that these abstracted representations are immune to aperture biases that contribute to fMRI feature decoding. The paper provides converging evidence that neural states responsible for representing information during perception and WM are fundamentally different, and provides a potential explanation for this difference.</p><p>Strengths:</p><p>(1) The generation of stimuli with matching vs. orthogonal orientations and aperture biases is clever and sets up a straightforward test regarding whether and how aperture biases contribute to orientation decoding during perception and WM. The demonstration that orientation decoding during perception is driven primarily by aperture bias while during WM it is driven primarily by orientation is compelling.</p><p>(2) The paper suggests a reason why orientation decoding during WM might be immune to aperture biases: by weighting multivoxel patterns measured during WM storage by spatial population receptive field estimates from a different task the authors show that remembered but not actively viewed - orientations form &quot;line-like&quot; patterns in retinotopic cortical space.</p></disp-quote><p>We thank the reviewer for noting the strengths in our work.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) The paper tests a strong version of the sensory recruitment model, where neural states representing information during WM are presumed to be identical to neural states representing the same information during perceptual processing. As the paper acknowledges, there is already ample reason to doubt this prediction (see, e.g., earlier work by Kok &amp; de Lange, Curr Biol 2014; Bloem et al., Psych Sci, 2018; Rademaker et al., Nat Neurosci, 2019; among others). Still, the demonstration that orientation decoding during WM is immune to aperture biases known to drive orientation decoding during perception makes for a compelling demonstration.</p></disp-quote><p>We agree with the reviewer, and would add that the main problem with the sensory recruitment model of WM is that it remains underspecified. The work cited above and in our paper, and the results in this report is only the beginning of efforts to fully detail what it means to recruit sensory mechanisms for memory.</p><disp-quote content-type="editor-comment"><p>(2) Earlier work by the same group has reported line-like representations of orientations during memory storage but not during perception (e.g., Kwak &amp; Curtis, Neuron, 2022). It's nice to see that result replicated during explicit perceptual and WM tasks in the current study, but I question whether the findings provide fundamental new insights into the neural bases of WM. That would require a model or explanation describing how stimulus-specific activation patterns measured during perception are transformed into the &quot;line-like&quot; patterns seen during WM, which the authors acknowledge is an important goal for future research.</p></disp-quote><p>We agree with the reviewer that perhaps some might see the current results as an incremental step given our previous paper. However, we would point out that researchers have been decoding memorized orientation from the early visual cortex for 15 years, and not one of those highly impactful studies had ever done what we did here, which was to test if decoded WM representations are the product of aperture biases. Not only do our results indicate that decoding memorized orientation is immune to these biases, but they critically suggest a reason why one can decode orientation during WM.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Summary:</p><p>In this work, Duan and Curtis addressed an important issue related to the nature of working memory representations. This work is motivated by findings illustrating that orientation decoding performance for perceptual representations can be biased by the stimulus aperture (modulator). Here, the authors examined whether the decoding performance for working memory representations is similarly influenced by these aperture biases. The results provide convincing evidence that working memory representations have a different representational structure, as the decoding performance was not influenced by the type of stimulus aperture.</p><p>Strengths:</p><p>The strength of this work lies in the direct comparison of decoding performance for perceptual representations with working memory representations. The authors take a well-motivated approach and illustrate that perceptual and working memory representations do not share a similar representational structure. The authors test a clear question, with a rigorous approach and provide convincing evidence. First, the presented oriented stimuli are carefully manipulated to create orthogonal biases introduced by the stimulus aperture (radial or angular modulator), regardless of the stimulus carrier orientation. Second, the authors implement advanced methods to decode the orientation information present, in visual and parietal cortical regions, when directly perceiving or holding an oriented stimulus in memory. The data illustrates that working memory decoding is not influenced by the type of aperture, while this is the case in perception. In sum, the main claims are important and shed light on the nature of working memory representations.</p></disp-quote><p>We thank the reviewer for noting the strengths in our work.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>I have a few minor concerns that, although they don't affect the main conclusion of the paper, should still be addressed.</p><p>(1) Theoretical framing in the introduction: Recent work has shown that decoding of orientation during perception does reflect orientation selectivity, and it is not only driven by the stimulus aperture (Roth, Kay &amp; Merriam, 2022).</p></disp-quote><p>Excellent point, and similar to the point made by Reviewer 1. We now adjust our text and cite the paper in the Introduction.</p><p>Below, we paste our response to Reviewer 1:</p><p>“Second, we remain agnostic to the question of whether fine-scale inhomogenous sampling of orientation selective neurons may drive some of the decoding we report here. It is possible that our line-like representations are driven by neurons tuned to the sample orientation that have receptive fields that lie along the line. Here, we instead focus on testing the idea that WM decoding does not depend on aperture biases.”</p><disp-quote content-type="editor-comment"><p>(2) Figure 1C illustrates the principle of how the radial and angular modulators bias the contrast energy extracted by the V1 model, which in turn would influence orientation decoding. It would be informative if the carrier orientations used in the experiment were shown in this figure, or at a minimum it would be mentioned in the legend that the experiment used 3 carrier orientations (15{degree sign}, 75{degree sign}, 135{degree sign}) clockwise from vertical. Related, when trying to find more information regarding the carrier orientation, the 'Stimuli' section of the Methods incorrectly mentions that 180 orientations are used as the carrier orientation.</p></disp-quote><p>We apologize for not clearly indicating the stimulus features in the figure. Now, we added the information about the target orientations in Figure 1C legend. Also, we now corrected in the Methods section the mistakes about the carrier orientation and the details of the task. Briefly, participants were asked to use a continuous report over 180 orientations. We now clarify that “We generated 180 orientations for the carrier grating to cover the whole orientation space during the continuous report task.”</p><disp-quote content-type="editor-comment"><p>(3) The description of the image computable V1 model in the Methods is incomplete, and at times inaccurate. (i) The model implements 6 orientation channels, which is inaccurately referred to as a bandwidth of 60{degree sign} (should be 180/6=30). (ii) The steerable pyramid combines information across phase pairs to obtain a measure of contrast energy for a given stimulus. Here, it is only mentioned that the model contains different orientation and spatial scale channels. I assume there were also 2 phase pairs, and they were combined in some manner (squared and summed to create contrast energy). Currently, it is unclear what the model output represents. (iii) The spatial scale channel with the maximal response differences between the 2 modulators was chosen as the final model output. What spatial frequency does this channel refer to, and how does this spatial frequency relate to the stimulus?</p></disp-quote><p>(i) First, we thank the reviewer for pointing out this mistake since the range of orientations should be 180deg instead of 360deg. We corrected this in the revised version.</p><p>(ii) Second, we apologize for not being clear. In the second paragraph of the “Simulate model outputs” section, we wrote,</p><p>“For both types of stimuli, we used three target orientations (15°, 75°, and 135° clockwise from vertical), which had two kinds of phases for both the carriers and the modulators. We first generated the model’s responses to each target image separately, then averaged the model responses across all phases for each orientation condition.”</p><p>We have corrected this text by now writing,</p><p>from vertical), two phases for the carrier (0 or π), and two phases for the modulator (sine “For both types of stimuli, we used three target orientations (15°, 75°, and 135° clockwise from vertical), two phases for the carrier (0 or π), and two phases for the modulator (sine or cosine phase). We first generated the model responses to each phase condition separately, then averaged them across all phases for each orientation condition.”</p><p>(iii) Third and again we apologize for the misunderstanding. Since both modulated gratings have the same spatial frequency, the channel with the largest response should be equal to the spatial frequency of the stimulus. We corrected this by now writing,</p><p>“For the final predicted responses, we chose the subband with maximal responses (the 9th level), which corresponds to the spatial frequency of the stimulus (Roth, Heeger, and Merriam 2018).”</p><disp-quote content-type="editor-comment"><p>(4) It is not clear from the Methods how the difficulty in the perceptual control task was controlled. How were the levels of task difficulty created?</p></disp-quote><p>Apologies for not being clear. The task difficulty was created by setting the contrast differences between the two stimuli. The easiest level is choosing the first and the last contrast as pairs, while the hardest level is choosing the continuous two contrasts. We added these sentences</p><p>“The contrast for each stimulus was generated from a predefined set of 20 contrasts uniformly distributed between 0.5 and 1.0 (0.025 step size). We created 19 levels of task difficulty based on the contrast distance between the two stimuli. Thus, the difficulty ranged from choosing contrast pairs with the largest difference (0.5, easiest) to contrast pairs with the smallest difference (0.025, hardest). Task difficulty level changed based on an adaptive, 1-up-2-down staircase procedure (Levitt 1971) to maintain performance at approximately 70% correct.”</p><disp-quote content-type="editor-comment"><p><bold>Recommendations For The Authors</bold></p><p><bold>(Reviewer #1):</bold></p><p>(1) If the black circle (Fig 3A &amp; C) is the stimulus size, and the stimulus (12º) is roughly half the size of the entire screen (24.8º), then how are spatial reconstructions generated for parts of the visual field that fall outside of the screen? I am asking because in Figure 3 the area over which spatial reconstructions are plotted has a diameter at least 3 times the diameter of that black circle (the stimulus). I'm guessing this is maybe possible when using a very liberal fitting approach to prf's, where the center of a prf can be outside of the screen (so you'd fit a circle to an elongated blob, assuming that blob is the edge of a circle, or something). Can you really reliably estimate that far out into visual space/ extrapolate prf's that exist in a part of the space you did not fully map (because it's outside of the screen)?</p></disp-quote><p>We thank the reviewer for pointing out this confusing issue.</p><p>First, the spatial construction map has a diameter 3 times the diameter of the stimulus because we included voxels whose pRF eccentricities were within 20º in the reconstruction, the same as Kwak &amp; Curtis, 2022. There are reasons for doing so. First, while the height of the screen is 24.8º, the width of the screen is 44º. Thus, it is possible to have voxels whose pRF eccentricities are &gt;20º. Second, for areas outside the height boundaries, there might not be pRF centers, but the whole pRF Gaussian distributions might still cover the area. Moreover, when creating the final map combined across three orientation conditions, we rotated them to be centered vertically, which then required a 20x20º square. Finally, inspecting the reconstruction maps, we noticed that the area that was twice the stimulus size (black circle) made very little contributions to the reconstructions. Therefore, the results depicted in Figure 3A&amp;C are justified, but see the next comment and our response.</p><disp-quote content-type="editor-comment"><p>(2) Is the quantification in 3B/C justified? The filter line uses a huge part of visual space outside of the stimulus (and even the screen). For the angular modulator in the &quot;perception&quot; condition, this means that there is no peak at -90/90 degree. But if you were to only use a line that is about the size of the stimulus (a reasonable assumption), it would have a peak at -90/90 degree.</p></disp-quote><p>This is an excellent question. We completely agree that it is more reasonable to use filter lines that have the same size (12º) as the stimulus instead of the whole map size (40º). Based on the feedback from the Reviewer, we redid the spatial reconstruction analyses and now include the following changes to Figure 3.</p><p>(1) We fitted the lines using pixels only within the stimulus. In Figure 3A and Figure 3C, we now replaced the reconstruction maps.</p><p>(2) We added the color bar in Figure 3A.</p><p>(3) We regenerated the filtered responses and calculated the fidelity results by using line filters with the stimulus size. We replaced the filtered responses and fidelity results in Figure 3B and Figure 3D. With the new analysis, as anticipated by the Reviewer, we now found peaks at -90/90 degrees for the angular modulated gratings in the perceptual control task in V1 and V2. Thank you Reviewer 1!!!!</p><p>(4) We also made corresponding changes in the Supplementary Figure S4 and S5, as well as the statistical results in Table S4 and S5.</p><p>(5) In the “Methods” section, we added “within the stimulus size” for both “fMRI data analysis: Spatial reconstruction” and “Quantification and statistical analysis” subsections.</p><disp-quote content-type="editor-comment"><p>(3) Figure 4 is nice, but not exactly quantitative. It does not address that the reconstructions from the perceptual task are hugging the stimulus edges much more closely compared to the modeled map. Conversely, the yellow parts of the reconstructions from the delay fan out much further than those of the model. The model also does not seem to dissociate radial/angular stimuli, while in the perceptual data the magnitude of perceptual reconstruction is clearly much weaker for angular compared to radial modulator.</p></disp-quote><p>We thank the reviewer for this question. First, we admit that Figure 4 is more qualitative than quantitative. However, we see no alternative that better depicts the similarity in the model prediction and the fMRI results for the perceptual control and WM tasks. The figure clearly shows the orthogonal aperture bias. Second, we agree that aspects of the observed fMRI results are not perfectly captured by the model. This could be caused by many reasons, including fMRI noise, individual differences, etc. Importantly, different modulators induce orthogonal aperture bias in the perceptual but not the WM task, and therefore does not have a major impact on the conclusions.</p><disp-quote content-type="editor-comment"><p>(4) The working memory and perception tasks are rather different. In this case, the perception task does not require the subject to process the carrier orientation (which is largely occluded, and possibly not that obvious without paying attention to it), but attention is paid to contrast. In this scenario, stimulus energy may dominate the signal. In the WM task, subjects have to work out what orientation is shown to do the task. Given that the sensory stimulus in both tasks is brief (1.5s during memory encoding, and 2.5s total in the perceptual task), it would be interesting to look at decoding (and reconstructions) for the WM stimulus epoch. If abstraction (into a line) happens in working memory, then this perceptual part of the task should still be susceptible to aperture biases. It allows the authors to show that it is indeed during memory (and not merely the task or attentional state of the subject) that abstraction occurs.</p></disp-quote><p>We addressed the same point in the response for Reviewer 1, “additional context” section.</p><disp-quote content-type="editor-comment"><p>Recommendations for improving the writing:</p><p>(1) The main text had too little information about the Methods. Of course, some things need not be there, but others are crucial to understanding the basics of what is being shown. For example, the main text does not describe how many orientations are used (well... actually the caption to Figure 1 says there are 2: horizontal and vertical, which is confusing), and I had to deduce from the chance level (1/3) that there must have been 3 orientations. Also, given how important the orthogonality of the carrier and modulator are, it would be good to have this explicit (I would even want an analysis showing that indeed the two are independent). A final example is the use of beta weights, and for delay period decoding only the last 6s (of the 12s delay) are modeled and used for decoding.</p></disp-quote><p>We thank the reviewer for identifying aspects of the manuscript that were confusing. We made several changes to the paper to clarify these details.</p><p>First, we added the information about the orientations we used in the caption for Figure 1 and made it clear that Figure 1C is just an illustration using vertical/horizontal orientations. Second, the carrier and the modulator are different in many ways. For example, the carrier is a grating with orientation and contrast information, while the modulator is the aperture that bounds the grating without these features. Their phases are orthogonal, and we added this in the second paragraph of the “Stimuli” section. Last, in the main text and the captions, we now denote “late delay” when writing about our procedures.</p><disp-quote content-type="editor-comment"><p>(2) Right under Figure 3, the text reads &quot;angular modulated gratings produced line-like representations that were orthogonal carrier orientation reflecting the influence of stimulus vignetting&quot;, but the quantification (Figure 3D) does not support this (there is no orthogonal &quot;bump&quot; in the filtered responses from V1-V3, and one aligned with the carrier orientation in higher areas).</p></disp-quote><p>This point was addressed in the “recommendations for the authors (Reviewer 1), point 2” above.</p><disp-quote content-type="editor-comment"><p>Minor corrections to text and figures:</p><p>(1) Abstract: &quot;are WM codes&quot; should probably be &quot;WM codes are&quot;.</p></disp-quote><p>We prefer to keep “are WM codes” as it is grammatically correct.</p><disp-quote content-type="editor-comment"><p>(2) Introduction: Second sentence 2nd paragraph: representations can be used to decode representations? Or rather voxel patterns can be used...</p></disp-quote><p>Changed to “On the one hand, WM representations can be decoded from the activity patterns as early as primary visual cortex (V1)...”</p><disp-quote content-type="editor-comment"><p>(3) Same paragraph: might be good to add more references to support the correlation between V1 decoding and behavior. There's an Ester paper, and Iamchinina et al. 2021. These are not trial-wise, but trial-wise can also be driven by fluctuating arousal effects, so across-subject correlations help fortify this point.</p></disp-quote><p>We added these two papers as references.</p><disp-quote content-type="editor-comment"><p>(4) Last paragraph: &quot;are WM codes&quot; should probably be &quot;WM codes are&quot;.</p></disp-quote><p>See (1) above.</p><disp-quote content-type="editor-comment"><p>(5) Figure 1B &amp; 2A caption: &quot;stimulus presenting epoch&quot; should probably be &quot;stimulus presentation epoch&quot;.</p></disp-quote><p>Changed to “stimulus epoch”.</p><disp-quote content-type="editor-comment"><p>(6) Figure 1C: So this is very unclear, to say stimuli are created using vertical and horizontal gratings (when none of the stimuli used in the experiment are either).</p></disp-quote><p>We solved and answered this point in response to Reviewer 3, point 2.</p><disp-quote content-type="editor-comment"><p>(7) Figure 2B caption &quot;cross&quot; should probably be &quot;across&quot;.</p></disp-quote><p>We believe “cross” is fine since cross here means cross-decoding.</p><disp-quote content-type="editor-comment"><p>(8) Figure 3A and C are missing a color bar, so it's unclear how these images are generated (are they scaled, or not) and what the BOLD values are in each pixel.</p></disp-quote><p>All values in the map were scaled to be within -1 to 1. We added the color bar in both Figure 3 and Figure 4.</p><disp-quote content-type="editor-comment"><p>(9) Figure 3B and D (bottom row) are missing individual subject data.</p></disp-quote><p>We use SEM to indicate the variance across subjects.</p><disp-quote content-type="editor-comment"><p>(10) Figure D caption: &quot;early (V1 and V2)&quot; should probably be &quot;early areas (V1 and V2)&quot;.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>(11) Methods, stimuli says &quot;We generated 180 orientations for the carrier grating to cover the whole orientation space.&quot; But it looks like only 3 orientations were generated, so this is confusing.</p></disp-quote><p>We solved and answered this point in response to Reviewer 3, point 2.</p><disp-quote content-type="editor-comment"><p>(12) Further down (fMRI task) &quot;random jitters&quot; is probably &quot;random jitter&quot;</p></disp-quote><p>Corrected.</p></body></sub-article></article>