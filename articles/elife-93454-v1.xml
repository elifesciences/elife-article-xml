<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93454</article-id><article-id pub-id-type="doi">10.7554/eLife.93454</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93454.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Visual routines for detecting causal interactions are tuned to motion direction</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Ohl</surname><given-names>Sven</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0292-2151</contrib-id><email>sven.ohl@hu-berlin.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rolfs</surname><given-names>Martin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8214-8556</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt-Universität zu Berlin, Rudower Chaussee</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Berlin School of Mind and Brain</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kok</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>03</day><month>04</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP93454</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-02"><day>02</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-10-10"><day>10</day><month>10</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.22.554237"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-10"><day>10</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93454.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-03-21"><day>21</day><month>03</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93454.2"/></event></pub-history><permissions><copyright-statement>© 2024, Ohl and Rolfs</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Ohl and Rolfs</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93454-v1.pdf"/><abstract><p>Detecting causal relations structures our perception of events in the world. Here, we determined for visual interactions whether generalized (i.e. feature-invariant) or specialized (i.e. feature-selective) visual routines underlie the perception of causality. To this end, we applied a visual adaptation protocol to assess the adaptability of specific features in classical launching events of simple geometric shapes. We asked observers to report whether they observed a launch or a pass in ambiguous test events (i.e. the overlap between two discs varied from trial to trial). After prolonged exposure to causal launch events (the adaptor) defined by a particular set of features (i.e. a particular motion direction, motion speed, or feature conjunction), observers were less likely to see causal launches in subsequent ambiguous test events than before adaptation. Crucially, adaptation was contingent on the causal impression in launches as demonstrated by a lack of adaptation in non-causal control events. We assessed whether this negative aftereffect transfers to test events with a new set of feature values that were not presented during adaptation. Processing in specialized (as opposed to generalized) visual routines predicts that the transfer of visual adaptation depends on the feature similarity of the adaptor and the test event. We show that the negative aftereffects do not transfer to unadapted launch directions but do transfer to launch events of different speeds. Finally, we used colored discs to assign distinct feature-based identities to the launching and the launched stimulus. We found that the adaptation transferred across colors if the test event had the same motion direction as the adaptor. In summary, visual adaptation allowed us to carve out a visual feature space underlying the perception of causality and revealed specialized visual routines that are tuned to a launch’s motion direction.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>perception</kwd><kwd>causality</kwd><kwd>adaptation</kwd><kwd>direction selectivity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>OH 274/4-1</award-id><principal-award-recipient><name><surname>Ohl</surname><given-names>Sven</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>OH 274/5-1</award-id><principal-award-recipient><name><surname>Ohl</surname><given-names>Sven</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>RO3579/8-1</award-id><principal-award-recipient><name><surname>Rolfs</surname><given-names>Martin</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>RO3579/12-1</award-id><principal-award-recipient><name><surname>Rolfs</surname><given-names>Martin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Visual adaptation in humans revealed that the perception of causality was tuned to the direction of motion while the adaptation transferred across different motion speeds and colors.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When two objects collide, the objects’ future path can be inferred based on physical laws, allowing us, for instance, to purposely change the motion path of an asteroid by steering an uncrewed spacecraft into a collision with the asteroid (see NASA’s Double Asteroid Redirection Test; <xref ref-type="fig" rid="fig1">Figure 1a</xref>). In contrast, how humans detect such causal interactions in their visual environment is less clear. The proposed theories regarding our understanding of causal sensory interactions vary considerably, proposing purely perceptual (<xref ref-type="bibr" rid="bib26">Michotte, 1963</xref>; <xref ref-type="bibr" rid="bib36">Scholl and Tremoulet, 2000</xref>), abstract probabilistic (<xref ref-type="bibr" rid="bib34">Sanborn et al., 2013</xref>), or cognitive mechanisms (<xref ref-type="bibr" rid="bib30">Rips, 2011</xref>; <xref ref-type="bibr" rid="bib42">Weir, 1978</xref>; <xref ref-type="bibr" rid="bib43">White, 2006</xref>). These are neither mutually exclusive nor do they make distinct predictions and they are, therefore, hard to distinguish (<xref ref-type="bibr" rid="bib30">Rips, 2011</xref>). Here, we will study the mechanisms underlying the computations of causal interactions in the visual domain by capitalizing on visual adaptation of causality (<xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>; <xref ref-type="bibr" rid="bib31">Rolfs et al., 2013</xref>). Adaptation is a powerful behavioral tool for discovering and dissecting a visual mechanism (<xref ref-type="bibr" rid="bib18">Kohn, 2007</xref>; <xref ref-type="bibr" rid="bib41">Webster, 2015</xref>) that provides an intriguing testing ground for the perceptual roots of causality. Perceptual accounts of causal understanding posit the existence of visual routines—local, semi-independent operations that can engage mid- and higher-level processes (<xref ref-type="bibr" rid="bib40">Ullman, 1987</xref>; <xref ref-type="bibr" rid="bib10">Cavanagh et al., 2001</xref>). The precise nature of visual routines that detect causal relations, however, remains rather hazy. In its purest form, such a visual routine could constitute a generalized mechanism that responds to all kinds of causal visual interactions. Alternatively, there could be many different specialized visual routines for the detection of causality that are tuned to the features of a particular causal interaction (e.g. the direction, kinematics, or object identity; <xref ref-type="fig" rid="fig1">Figure 1b</xref>; see <xref ref-type="bibr" rid="bib30">Rips, 2011</xref> for a discussion). Here, we will use visual adaptation to determine whether the computation of causality occurs in generalized (i.e. feature-invariant) or in specialized (i.e. feature-selective) visual routines. Specifically, we will determine whether an adaptor is most effective for test events that match the features of the adaptor.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The perception of causality.</title><p>(<bold>a</bold>) A person perceiving an upcoming causal interaction between an uncrewed spacecraft and an asteroid. (<bold>b</bold>) Features of such launching events are the direction and speed of the colliding objects as well as their object identities (i.e. a specific set of features). Assessing the adaptation’s transfer between features allows us to determine whether the perception of causality arises in specialized visual routines that are tuned to a particular visual feature. (<bold>c</bold>) Trial sequence of a test event. A peripheral disc moved towards a stationary disc and stopped with some degree of overlap (ranging from 0–100% overlap in seven equidistant steps) between the two discs. The second disc then immediately started to move in the same direction, with the same speed as the first disc. In adaptation blocks, 320 launches were presented before the first test event of a block, and 16 top-up adaptation events before each subsequent test event.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93454-fig1-v1.tif"/></fig><p>To this end, we took advantage of the phenomenon that observers perceive causality even in simple kinematic displays—a moving disc that stops next to another disc appears to launch the second disc into motion (i.e. launching effect, <xref ref-type="fig" rid="fig1">Figure 1c</xref>; <xref ref-type="bibr" rid="bib26">Michotte, 1963</xref>; see <xref ref-type="bibr" rid="bib30">Rips, 2011</xref>; <xref ref-type="bibr" rid="bib36">Scholl and Tremoulet, 2000</xref>; <xref ref-type="bibr" rid="bib43">White, 2006</xref>, <xref ref-type="bibr" rid="bib44">White, 2017</xref> for reviews). The prolonged viewing of such launching events in an adaptation protocol strongly alters the perception of causality by reducing the proportion of reported launches in subsequent test events (<xref ref-type="bibr" rid="bib31">Rolfs et al., 2013</xref>). The adaptation of causality is spatially specific to the retinotopic coordinates of the adapting stimulus (<xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>; <xref ref-type="bibr" rid="bib31">Rolfs et al., 2013</xref>; for an object-centered elasticity aftereffect using a related stimulus on a circular motion path, see <xref ref-type="bibr" rid="bib1">Arnold et al., 2015</xref>), suggesting that the detection of causal interactions is implemented locally in visual space. Here, we selected adaptors from a feature space—while keeping their spatial locations constant—and determined whether adaptation transfers to test events with a different set of features in that feature space (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). If the strength of the adaptation’s perceptual consequences depends on the similarity of the adaptor and the test event, it would reveal the existence of specialized visual routines for the detection of causal interactions that are tuned to that feature (i.e. there would be multiple visual routines within a feature dimension, each tuned to a different preferred feature value). Alternatively, if we observe adaptation transfer from one feature to another, this will support the notion of a generalized, feature-invariant, visual routine.</p><p>In three experiments, we assessed whether adaptation to launches of a particular motion direction, motion speed, or feature identity (i.e. a conjunction of two features) transfers to other values of that feature space or, alternatively, whether the consequences of adaptation are restricted to the feature values of the adaptor. Our results provide compelling evidence for visual routines that are specialized for processing launches of a particular motion direction.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Adaptation of causality is selective for motion direction</title><p>In <bold>Experiment 1</bold>, we tested whether causality is computed separately within visual routines specialized for different motion directions. To this end, we presented observers with brief test events in one of two possible horizontal directions (from left to right, or right to left) in which a peripheral disc moved swiftly towards a stationary one. The first disc stopped moving as soon as the two discs partially overlapped (the amount of overlap was manipulated in seven steps from zero to full overlap, <xref ref-type="fig" rid="fig1">Figure 1c</xref>). Simultaneously, the second disc started to move along the same trajectory, and we asked observers to report whether they perceived that the first disc passed the second one (i.e. a pass event; commonly reported at full overlap), or rather launched it into motion (i.e. a launch event; common at zero overlap). We quantified the point of subjective equality (PSE) between perceived launches and passes by modeling psychometric functions for the perception of causality both <italic>before</italic> and <italic>after</italic> adaptation to causal launches (note that we presented the adaptor in a range of ±30° around one of the two possible directions). Moreover, we compared the visual adaptation to launches to a (non-causal) control condition in which we presented slip events as adaptor. In a slip event, the initially moving disc passes completely over the stationary disc, stops immediately on the other side, and then the initially stationary disc begins to move in the same direction without delay. Thus, the two movements are presented consecutively without a temporal gap. This stimulus typically produces the impression of two independent (non-causal) movements.</p><p>Visual adaptation to launches successfully affected the perception of causality in this task. We observed a strong negative aftereffect when test events matched the direction of the launch adaptor. That is, observers were less likely to report a launch after adapting to launches in the same direction as the test events (<xref ref-type="fig" rid="fig2">Figure 2a–c</xref>). This observation was corroborated by a two-way (two event types: launch vs. slip; two directions: congruent vs. incongruent) repeated measures analysis of variance (rmANOVA) in which we assessed the magnitude of adaptation by subtracting the PSE before adaptation (PSE<sub>before</sub> = 0.60, <italic>CI<sub>95%</sub></italic> = [0.59 0.62]) from the PSEs obtained in each of the four different adaptation conditions (PSE<sub>cong_launch</sub> = 0.40, <italic>CI<sub>95%</sub></italic> = [0.36 0.43]; PSE<sub>incong_launch</sub> = 0.55, <italic>CI<sub>95%</sub></italic> = [0.54 0.56]; PSE<sub>cong_slip</sub> = 0.55, <italic>CI<sub>95%</sub></italic> = [0.54 0.57]; PSE<sub>incong_slip</sub> = 0.61, <italic>CI<sub>95%</sub></italic> = [0.59 0.64]). The analysis revealed a significant main effect of event type (<italic>F</italic> (1, 7)=62.73, p&lt;0.001; <xref ref-type="fig" rid="fig2">Figure 2a–c</xref>) demonstrating stronger adaptation for launching than slip events (ΔPSE<sub>event</sub> = –0.11, <italic>CI<sub>95%</sub></italic> = [–0.14–0.08]). Moreover, adaptors in a congruent direction as the test event resulted in stronger adaptation than in incongruent directions (<italic>F</italic> (1, 7)=41.62, p&lt;0.001; ΔPSE<sub>direction</sub> = –0.11, <italic>CI<sub>95%</sub></italic> = [–0.15–0.07]). Critically, we observed a significant interaction (<italic>F</italic> (1, 7)=12.23, p=0.01), revealing that the influence of direction-congruency on the magnitude of adaptation was significantly stronger for launches than for slip events (Post-hoc t-test: <italic>t</italic>(7) = 3.50, p=0.01; ΔPSE<sub>eventXdirection</sub> = –0.09, <italic>CI<sub>95%</sub></italic> = [–0.16–0.03]) supporting the hypothesis that specialized visual routines are tuned to the motion direction of launching events.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Results of Experiment 1, 2, and 3.</title><p>(<bold>a</bold>) Experimental conditions of Experiment 1. (<bold>b</bold>) Mean proportion of causal reports as a function of disc overlap in Experiment 1. Visualization of psychometric curves is based on the mean parameters averaged across observers before adaptation (pink), and after adaptation with direction-congruent launches (blue), direction-incongruent launches (in purple), direction-congruent slip events (red) and direction-incongruent slip events (in orange). (<bold>c</bold>) PSEs for each individual observers (circles, n=8) and the mean across observers (square). (<bold>d</bold>) Experimental conditions of Experiment 2. (<bold>e</bold>) Mean proportion of causal reports as a function of disc overlap in Experiment 2. Visualization of psychometric curves is based on the mean parameters averaged across observers before adaptation (pink), and after adaptation with speed-congruent launches (blue), speed-incongruent launches (in purple), speed-congruent slip events (red) and speed-incongruent slip events (in orange). (<bold>f</bold>) PSEs for each individual observers (circles, n=8) and the mean across observers (square). (<bold>g</bold>) Experimental conditions of Experiment 3. (<bold>h</bold>) Mean proportion of causal reports as a function of disc overlap in Experiment 3. Visualization of psychometric curves is based on the mean parameters averaged across observers before adaptation (pink), and after adaptation with adaptors that are identical with the test event (blue), share the same direction but different colors (in purple), same colors but different direction (red) and completely different test events (in orange). (<bold>i</bold>) PSEs for each individual observers (circles, n=8) and the mean across observers (square). All error bars are ±1 SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93454-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Adaptation of causality transfers across motion speed</title><p>In <bold>Experiment 2</bold>, we determined whether the perception of causality is also selective for other feature dimensions that are relevant to causal interactions. Causal events in our visual environment encompass interactions of various motion speeds. Here, we examined whether adapting the perception of causality transfers between events that either had different speeds (same as in <bold>Experiment 1</bold> or half that). Both speed-congruent (i.e. test events had the same speed as the adaptor) and speed-incongruent launches (i.e. test events and adaptor differed by a factor of 2) resulted in adaptation, demonstrating a transfer of adaptation across motion speeds (<xref ref-type="fig" rid="fig2">Figure 2d–f</xref>). As in <bold>Experiment 1</bold>, we corroborated this observation by a two-way rmANOVA in which we assessed differences in the magnitude of adaptation (i.e. the difference in PSEs before and after adaptation; PSE<sub>before</sub> = 0.59, <italic>CI<sub>95%</sub></italic> = [0.57 0.61]; PSE<sub>cong_launch</sub> = 0.43, <italic>CI<sub>95%</sub></italic> = [0.40 0.46]; PSE<sub>incong_launch</sub> = 0.43, <italic>CI<sub>95%</sub></italic> = [0.39 0.47]; PSE<sub>cong_slip</sub> = 0.60, <italic>CI<sub>95%</sub></italic> = [0.57 0.63]; PSE<sub>incong_slip</sub> = 0.54, <italic>CI<sub>95%</sub></italic> = [0.50 0.59]) as a function of speed congruency (congruent vs. incongruent speed), event type (launch vs. slip event) and their interaction. The analysis revealed a significant main effect of event type (<italic>F</italic> (1, 7)=23.39, p=0.002; <xref ref-type="fig" rid="fig2">Figure 2d–f</xref>) demonstrating stronger adaptation for launching than slip events (ΔPSE<sub>event</sub> = –0.14, <italic>CI<sub>95%</sub></italic> = [–0.21–0.07]). Importantly, the magnitude of adaptation was not significantly different for congruent and incongruent speed between adaptor and test event (<italic>F</italic> (1, 7)=3.43, p=0.107; ΔPSE<sub>speed</sub> = 0.03 <italic>CI<sub>95%</sub></italic> = [–0.01 0.07]). Moreover, there was no significant interaction between event type and speed (<italic>F</italic> (1, 7)=3.48, p=0.105; ΔPSE<sub>eventXspeed</sub> = –0.06, <italic>CI<sub>95%</sub></italic> = [–0.14 0.02]). These findings demonstrate that the perception of causality is not tuned to motion speed—or, at least, that the tuning is very broad.</p></sec><sec id="s2-3"><title>Adaptation of causality using feature-conjunctions</title><p>In <bold>Experiment 3</bold>, we determined whether the identities of the two objects involved in the causal interaction can break the observed dominance of the direction-specific computation underlying the perception of causality. For instance, if two visually distinct objects are involved in a causal interaction with one type of object always launching a second, visually distinct object, it is unclear whether the influence of the adaptor is confined to events with the same feature conjunction or, alternatively, whether motion direction is the only relevant feature dimension for determining the presence of causal interactions. To distinguish between these alternatives, the two discs in the presented test events had two different colors (i.e. they were either red or green). Moreover, the adaptor in a session displayed only one particular feature conjunction (e.g. the adaptor in one session was always a red disc on the left launching a green disc on the right into rightward motion; adaptors varied across sessions and each subject saw each combination of feature conjunction across the multiple sessions). In contrast to the adaptor’s fixed feature conjunction in a session, both motion direction and color identities in the test events varied randomly from trial to trial. Again, we observed strong adaptation when test events and adaptors were identical. However, when one of the features in the feature conjunction differed, we observed adaptation only for test events in the same direction as the adaptor, irrespective of the object’s color. Thus, color does not constitute a critical feature of the visual routine for detecting causal interactions (<xref ref-type="fig" rid="fig2">Figure 2g–i</xref>). Again, we corroborated this observation by a two-way rmANOVA in which we assessed differences in the magnitude of adaptation (i.e. the difference in PSEs before and after adaptation; PSE<sub>before</sub> = 0.68, <italic>CI<sub>95%</sub></italic> = [0.63 0.73]; PSE<sub>same_stimulus</sub> = 0.50, <italic>CI<sub>95%</sub></italic> = [0.47 0.53]; PSE<sub>same_direction</sub> = 0.55, <italic>CI<sub>95%</sub></italic> = [0.50 0.59]; PSE<sub>same_color</sub> = 0.68, <italic>CI<sub>95%</sub></italic> = [0.65 0.72]; PSE<sub>different_stimulus</sub> = 0.65, <italic>CI<sub>95%</sub></italic> = [0.61 0.69]) as a function of motion direction (same vs. different), color assignment (same vs. different) and their interaction. The analysis revealed a significant main effect of motion direction (<italic>F</italic> (1, 7)=21.07, p=0.003) demonstrating stronger adaptation for launches in the same as compared to the opposite direction as the adaptor (ΔPSE<sub>direction</sub> = –0.14, <italic>CI<sub>95%</sub></italic> = [–0.22–0.07]). Adaptation following adaptors with the same color vs. different color assignments were not significantly different (<italic>F</italic> (1, 7)=0.071, p=0.798; ΔPSE<sub>color</sub> = –0.004, <italic>CI<sub>95%</sub></italic> = [–0.03 0.03]). However, we observed a marginally significant interaction between color and motion direction (<italic>F</italic> (1, 7)=4.59, p=0.07; ΔPSE<sub>directionXcolor</sub> = –0.07, <italic>CI<sub>95%</sub></italic> = [–0.16 0.01]). Post-hoc paired t-tests showed that color had a marginally significant influence on the magnitude of adaptation when adaptor and test events were in same direction (<italic>t</italic> (7)=1.97, p=0.090; ΔPSE<sub>color_samedir</sub> = –0.04, <italic>CI<sub>95%</sub></italic> = [–0.09 0.01]). However, color had no significant influence on the adaptation of causality when adaptor and test events were in opposite direction (<italic>t</italic> (7)=1.97, p=0.090; ΔPSE<sub>color_opp_dir</sub> = 0.03, <italic>CI<sub>95%</sub></italic> = [–0.02 0.09]).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We provide new evidence for the fundamental role of perceptual processes in detecting cause and effect. Using visual adaptation, we revealed that visual routines underlie the perception of causality that are specialized for a launch’s motion direction but invariant across a range of motion speeds. The tuning of causal perception to motion direction reveals a mechanism that is operating locally in feature space and complements previous reports of a spatially specific mechanism residing in retinotopic space (<xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>; <xref ref-type="bibr" rid="bib31">Rolfs et al., 2013</xref>).</p><p>The observed adaptation is specific to launching events that evoked a phenomenological impression of causality. This implies that adapting simply to the direction of a moving stimulus will only change the input to the visual routine but not the functioning of the routine itself. We controlled for such potential non-causal adaptation of low-level features (e.g. number of stimuli, time of contact, speed, overlap, or simply motion) using carefully designed control events (i.e. slip events) that did not result in comparable aftereffects.</p><p>Motion direction constitutes a basic computational unit in vision and can be computed as early as in retinal circuits (e.g. in retinal ganglion cells about 2–3 synapses downstream of photoreceptors in rabbits, <xref ref-type="bibr" rid="bib4">Barlow and Hill, 1963</xref>). While a retinal origin of launch detection is unlikely, the specialization of visual routines for the perception of causality at the level of individual motion directions raises the possibility that this function is located surprisingly early in the visual system as opposed to a higher-level visual computation. A similar adaptation-based rationale can be applied to study whether different causal interactions can be distinguished from each other by assessing the transfer of adaptation between different causal interactions (<xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>). While Michotte speculated about three separate causal impressions (i.e. launching, entraining, triggering; see <xref ref-type="bibr" rid="bib44">White, 2017</xref> for an extended catalog of causal interactions) adaptation helped to refine these categories as an adaptation to triggering events transferred to launching events, but adaptation to entraining had no such influence (<xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>). Thus, adaptation at the category level of the causal interaction allows to distinguish specialized detectors for a particular causal interaction. Our findings break down this specialization to a more fundamental level, showing that a specialized visual routine for launching events exists even within separate motion direction channels. While the present study demonstrates direction-selectivity for the detection of launches, previous adaptation protocols demonstrated successful adaptation using adaptors with random motion direction (<xref ref-type="bibr" rid="bib31">Rolfs et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>). These results, therefore, suggest independent direction-specific routines, in which adaptation to launches in one direction does not counteract an adaptation to launches in the opposite direction (as for example in opponent color coding).</p><p>Habituation studies in 6-mo-old infants also demonstrated that the reversal of a launch resulted in a recovery from habituation to launches (while a non-causal control condition of delayed launches did not; <xref ref-type="bibr" rid="bib24">Leslie and Keeble, 1987</xref>). In their study, the reversal of motion direction was accompanied by a reversal of the color assignment to the cause-effect-relationship. In contrast, our findings suggest, that in adults color does not play a major role in the detection of a launch. Future studies should further delineate similarities and differences obtained from adaptation studies in adults and habituation studies in children (e.g. <xref ref-type="bibr" rid="bib19">Kominsky et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>).</p><p>The adaptation transferred across different speeds and colors as long as the test events shared the same motion direction as the adaptor. For instance, we demonstrated a transfer of adaptation across speed with symmetrical speed ratios. This result complements a previous finding that reported that the adaptation to triggering events (with a speed ratio of 1:3) resulted in significant retinotopic adaptation of ambiguous (launching) test events of different speed ratios (i.e. test events with a speed ratio of 1:1 and of 1:3; <xref ref-type="bibr" rid="bib20">Kominsky and Scholl, 2020</xref>). Moreover, perceiving a red disc that launched a green disc into motion is also affected by the adaptation of a green disc launching a red disc (i.e. the opposite color assignment) into motion as long as the two events share the same motion direction. This finding rules out an alternative cognitive mechanism, in which the causing disc is first identified (e.g. the red disc is identified to launch other discs into motion) and the criterion for detecting a causal interaction would be selectively adjusted only for this particular causing disc in a top-down manner. Indeed, our results show that the appearance of the objects in the launching event plays only a minor role in adapting the perception of causality (if any), suggesting that the visual system can calibrate the detection of causal interactions independent of the involved object identities. Our findings, therefore, provide additional support for the claim that an event’s spatiotemporal parameters mediate the perception of causality (<xref ref-type="bibr" rid="bib26">Michotte, 1963</xref>; <xref ref-type="bibr" rid="bib23">Leslie, 1984</xref>; <xref ref-type="bibr" rid="bib36">Scholl and Tremoulet, 2000</xref>).</p><p>We suggest that at least two functional benefits result from a specialized visual routine for detecting causality. First, a direction-selective detection of launches allows adaptation to occur separately for each direction. That means that the visual system can automatically calibrate the sensitivity of these visual routines in response to real-world statistics. For instance, while falling objects drop vertically toward the ground, causal relations such as launches are common in horizontal directions moving along a stable ground. Second, we think that causal visual events are action-relevant, and the faster we can detect such causal interactions, the faster we can react to them. Direction-selective motion signals are available very early on in the visual system. Visual routines that are based on these direction-selective motion signals may enable faster detection. While our present findings demonstrate direction-selectivity, they do not pinpoint where exactly that visual routine is located. It is possible that the visual routine is located higher up in the visual system (or distributed across multiple levels), relying on a direction-selective population response as input.</p><p>The existence of specialized visual routines computing causality, however, does not rule out the possibility of a more complex hierarchical architecture that is composed of both specialized and generalized routines. The output of local, specialized detectors within that architecture could feed into a global, generalized detector. Thus, a weaker response from adapted specialized cause detectors would elicit a weaker response in unadapted global detectors (or super-detectors; <xref ref-type="bibr" rid="bib30">Rips, 2011</xref>). Indeed, visual adaptation to other low-level visual features (e.g. contrast sensitivity) can yield weaker responses during early visual processing which are then passed on to other downstream areas (<xref ref-type="bibr" rid="bib18">Kohn, 2007</xref>).</p><p>We used visual adaptation to carve out a bottom-up visual routine for detecting causal interactions in form of launching events. However, we know that more complex behaviors of perceiving causal relations can result from integrating information across space (e.g. in causal capture; <xref ref-type="bibr" rid="bib37">Scholl and Nakayama, 2002</xref>), across time (postdictive influence; <xref ref-type="bibr" rid="bib10">Cavanagh et al., 2001</xref>), and across sensory modalities (<xref ref-type="bibr" rid="bib38">Sekuler et al., 1997</xref>). Bayesian causal inference has been particularly successful as a normative framework to account for multisensory integration (<xref ref-type="bibr" rid="bib21">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib39">Shams and Beierholm, 2022</xref>). In that framework, the evidence for a common-cause hypothesis competes with the evidence for an independent-causes hypothesis (<xref ref-type="bibr" rid="bib39">Shams and Beierholm, 2022</xref>). The task in our experiments could be similarly formulated as two competing hypotheses for the second disc’s movement (i.e. the movement was caused by the first disc vs. the second disc did not move). This framework also emphasizes the distributed nature of the neural implementation for solving such inferences, showing the contributions of parietal and frontal areas in addition to sensory processing (for review see <xref ref-type="bibr" rid="bib39">Shams and Beierholm, 2022</xref>). Moreover, even visual adaptation to contrast in mouse primary visual cortex is influenced by top-down factors such as behavioral relevance—suggesting a complex implementation of the observed adaptation results (<xref ref-type="bibr" rid="bib16">Keller et al., 2017</xref>). The present experiments, however, presented purely visual events that do not require an integration across processing domains. Thus, the outcome of our suggested visual routine can provide initial evidence from within the visual system for a causal relation in the environment that may then be integrated with signals from other domains (e.g. auditory signals). Determining exactly how the perception of causality relates to mechanisms of causal inference and the neural implementation thereof is an exciting avenue for future research. Note, however, that perceived causality can be distinguished from judged causality: Even when participants are aware that a third variable (e.g. a color change) is the best predictor of the movement of the second disc in launching events, they still perceive the first disc as causing the movement of the second disc (<xref ref-type="bibr" rid="bib35">Schlottmann and Shanks, 1992</xref>).</p><p>Neurophysiological studies support the view of distributed neural processing underlying sensory causal interactions with the visual system playing a major role. Imaging studies in particular revealed a network for the perception of causality that is also involved in action observation (<xref ref-type="bibr" rid="bib6">Blakemore et al., 2003</xref>; <xref ref-type="bibr" rid="bib13">Fonlupt, 2003</xref>; <xref ref-type="bibr" rid="bib14">Fugelsang et al., 2005</xref>; <xref ref-type="bibr" rid="bib33">Roser et al., 2005</xref>). The fact that visual adaptation of causality occurs in a retinotopic reference frame emphasizes the role of retinotopically organized areas within that network (e.g. V5 and the superior temporal sulcus). Interestingly, single cell recordings in area F5 of the primate brain revealed that motor areas contribute to the perception of causality (<xref ref-type="bibr" rid="bib8">Caggiano et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Rolfs, 2016</xref>), emphasizing the distributed nature of the computations underlying causal interactions. This finding also stresses that the detection, and the prediction, of causality is essential for processes outside purely sensory systems (e.g. for understanding other’s actions, for navigating, and for avoiding collisions). The neurophysiology subserving in causal inference further extends the candidate cortical areas that might contribute to the detection of causal relations, emphasizing the role of the frontal cortex for the flexible integration of multisensory representations (<xref ref-type="bibr" rid="bib9">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Coen et al., 2023</xref>).</p><p>Visual adaptation of causality constitutes a powerful tool that allows us to reveal how specialized visual routines are tuned to the motion direction of a launching event. Visual adaptation in general is thought to be one of the signatures of a perceptual process (<xref ref-type="bibr" rid="bib15">Hafri and Firestone, 2021</xref>) and, therefore, is informative for identifying the visual origin of a mechanism as opposed to, for instance, cognitive accounts (e.g. cognitive schema; see <xref ref-type="bibr" rid="bib30">Rips, 2011</xref> for a review). The detection of causal interactions features many hallmarks of perception: An impression of causality emerges fast and automatically for briefly presented stimuli (<xref ref-type="bibr" rid="bib26">Michotte, 1963</xref>) and causal events reach awareness faster than non-causal events (<xref ref-type="bibr" rid="bib27">Moors et al., 2017</xref>). The implementation of visual routines for the perception of causality in separate channels of a basic visual feature such as motion direction fortifies the view that our understanding of causal interactions indeed starts as a low-level perceptual routine.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>In each of the three experiments, we tested eight human observers (<bold>Experiment 1</bold>: ages 21–32 y; five female; three male; eight right-handed; five right-eye dominant: <bold>Experiment 2</bold>: ages 21–30 y; five female; three male; seven right-handed; six right-eye dominant; <bold>Experiment 3</bold>: ages 23–30 y; six female; one non-binary; seven right-handed; six right-eye dominant) in five sessions (one training session without adaptation, four test sessions with adaptation). We determined the final sample size by computing 90% power contours as a function of sample size and trial number (<xref ref-type="bibr" rid="bib3">Baker et al., 2021</xref>) and defining the additional criterion of a minimum of eight observers per experiment. Based on the data obtained in the session without adaptation (i.e. the first session), we determined whether participants do distinguish between passes and launches in the basic experiment by determining whether the proportion of reported passes increases with increasing disc overlap. We excluded observers who did not distinguish between passes and launches after the first session and replaced them by new observers (resulting in the replacement of one observer in each experiment). Data obtained in the first session (i.e. without adaptation) did not enter the final analyses. In <bold>Experiment 1</bold> and <bold>2</bold>, we paid observers 8€ per session as compensation for participation and a bonus of 4€ after successful completion of all sessions. In <bold>Experiment 3</bold>, we paid observers 10€ per session. In all experiments, we obtained observers’ written informed consent before the first session. All observers had normal or corrected-to-normal vision and color vision as assessed using the color test <italic>Tafeln zur Prüfung des Farbensinnes/Farbensehens</italic> (<xref ref-type="bibr" rid="bib22">Kuchenbecker and Broschmann, 2016</xref>). The study was approved by the ethics committee of the Psychology Department of the Humboldt-Universität zu Berlin and it followed the guidelines of the World Medical Association (2008). Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Material</title><p>Observers sat in a sound-shielded, dimly lit room putting their head on a chin and forehead rest. We controlled for observers’ eye position by tracking their dominant eye using an Eyelink 1000 Desktop Mount eye tracker (SR Research, Ottawa, ON, Canada) with a sampling rate of 1000 Hz. We displayed visual stimuli on a video-projection screen (Celexon HomeCinema, Tharston, Norwich, UK) using a PROPixx DLP projector (VPixx Technologies Inc, Saint Bruno, QC, Canada) at a spatial resolution of 1920×1080 pixels and a refresh rate of 120 Hz. The screen was mounted on a wall at 180 cm away from the observer. The experiment was running on a DELL Precision T7810 (Debian GNU Linux 8) and implemented in Matlab 2016b (Mathworks, Natick, MA, USA) using the Psychophysics toolbox 3 (<xref ref-type="bibr" rid="bib7">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib17">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="bib29">Pelli, 1997</xref>) for stimulus presentation and the Eyelink toolbox (<xref ref-type="bibr" rid="bib12">Cornelissen et al., 2002</xref>) for control of the eye tracker. Behavioral responses were collected by pressing one of the two possible keys on a standard keyboard.</p></sec><sec id="s4-3"><title>General procedure</title><p>In our experiments, we asked observers to report whether they perceived a launch or a pass in test events composed of two discs. In the test events, a peripheral disc approached a stationary second disc and stopped at varying disc overlaps across trials (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Immediately after that, the second disc started moving in the same direction and with the same speed as the first disc. Before the first trial, observers read the instruction for the experiment that was displayed on the screen. As part of the instructions, we presented short demo trials of test events with 0% overlap that are typically perceived as launches (see <xref ref-type="video" rid="video1">Video 1</xref>), and test events with full disc overlap, that are typically perceived as a pass (see <xref ref-type="video" rid="video2">Video 2</xref>). Observers had the opportunity to inspect these two events as often as they wanted. Following the instructions, observers ran a short training session of 14 trials with varying disc overlaps ranging from 0–100% overlap.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-93454-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Example of the launch stimulus.</title><p>The stimulus is slowed down by a factor of 2.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-93454-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Example of the pass stimulus.</title><p>The stimulus is slowed down by a factor of 2.</p></caption></media><p>At the beginning of a trial, we asked observers to fixate a gray fixation point (diameter of 1.5 dva) in the center of the screen on a black background and the trial only started after observers successfully fixated the fixation symbol for at least 200ms. We presented the test events at 3 dva below the fixation symbol. In these test events, the first moving disc (gray; diameter of 1.5 dva) located either left or right from the vertical meridian started moving towards a stationary disc (gray; diameter of 1.5 dva) located 3 dva below the fixation point. The first disc stopped moving at one of seven possible distances away from the stationary disc, resulting in seven different disc overlaps ranging from 0 to 100%. The entire duration of the test event was 175ms. At the end of the trial, observers reported whether they perceived a launch or a pass in the test event by pressing either the arrow up key for launches or arrow down key for passes. In our study, we assessed causal perception by asking observers to report whether they observed a launch or a pass in events of varying ambiguity. This method assumes that launches and passes can be mapped onto a dimension that ranges from causal to non-causal impressions. It has been questioned whether pass events are a natural representative of non-causal events: Observers often report high impressions of causality upon first exposure to pass events, which then decrease after seeing a canonical launch (<xref ref-type="bibr" rid="bib5">Bechlivanidis et al., 2019</xref>). In our study, therefore, participants completed a separate session that included canonical launches before starting the main experiment.</p><p>In addition, the first session served the purpose to determine whether observers perceive launches and passes as a function of disc overlap in our behavioral task. To this end, observers completed 10 blocks with the direction of the test event (2 conditions: left vs. right) as well as the amount of disc overlap (7 conditions: ranging from 0–100% overlap) presented in a randomized order in a block. Each combination of these manipulations was repeated two times in a block, resulting in 28 trials in each block and a total of 280 trials in the first session. In sessions 2–5 we presented additional adaptors that varied between the experiments.</p><p>In all experiments, we tracked observers’ dominant eye to ensure proper fixation behavior during presentation of the test events and presentation of the adaptors. More specifically, we tracked the dominant eye’s current position at a sampling rate of 1000 Hz and determined online the eyes’ distance to the screen center. We aborted a trial, whenever the distance between eye position and screen center exceeded 2 dva. Observers repeated these trials at the end of a block in randomized order. During presentation of the adaptors, we presented a short message (at the fixation point) asking observers to please fixate in the center of the screen once observers gaze exceeded 2 dva away from the screen center.</p></sec><sec id="s4-4"><title>Adaptation in Experiment 1</title><p>In <bold>Experiment 1</bold>, each observer ran in 15 blocks in each of sessions 2–5. The first 5 blocks in a session were again without adaptation to measure an observer’s perception of causality before adaptation. As in the first session, we presented test events in two possible horizontal directions and varied the disc overlap resulting in 28 trials in each block. In blocks 6–15, we presented an adaptor before the first trial of a block. The adaptor was chosen from one of four possible adaptors. In two of the sessions, we used launches in a particular direction as the adaptor (the adaptor’s direction was fixed in a session). Before the first trial of a block, we presented 320 launching events at the same location as the test events described above. The exact direction of these launching events was randomly chosen from a narrow uniform distribution around the direction on the horizontal meridian (±30 degrees). The long adaptation phase was complemented by a top-up adaptation of 16 launching events in the same direction as the adaptor (again drawn from the same narrow uniform distribution around that main direction) before each trial to maintain an effective adaptation across the entire block.</p><p>In two additional sessions, we presented slip events as adaptors to control that the adaptation was specific for the impression of causality in the launching events (see <xref ref-type="video" rid="video3">Video 3</xref>). Slip events are designed to match the launching events in as many physical properties as possible while producing a very different, non-causal phenomenology. In slip events, the first peripheral disc also moves towards a stationary disc. In contrast to launching events, however, the first disc passes the stationary disc and stops only when it is adjacent to the opposite edge of the stationary disc. While slip events do not elicit a causal impression, they have the same number of objects and motion onsets, the same motion direction and speed, as well as the same spatial area of the event as launches. As for the launch adaptor, we displayed slip adaptors in a narrow uniform range of directions around one of the two possible horizontal directions (from left-to-right or from right-to-left being fixed in a session). This experimental design resulted in four sessions and the order of the type of adaptor (launches vs. slip adaptors; movement direction of the adaptor) was randomly determined for each participant. Overall, each observer ran in a total of 1,680 trials in <bold>Experiment 1</bold>.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-93454-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Example of the slip stimulus.</title><p>The stimulus is slowed down by a factor of 2.</p></caption></media></sec><sec id="s4-5"><title>Adaptation variations in Experiment 2</title><p>In <bold>Experiment 2</bold>, we used a very similar design as in <bold>Experiment 1</bold> with one notable difference. Adaptors and test events varied in motion speed either being the same as in <bold>Experiment 1</bold> or half that speed. Correspondingly, in slow events, the test event duration was twice as long (i.e. event duration of 350 ms) as for test events displayed at fast motion speed (i.e. event duration of 175 ms). We presented all test events (and adaptors) always in the same direction (from left to right). Similarly, we also displayed slip events as adaptors in slow and fast motion speeds. In sessions with adaptation, the first five blocks were without adaptation while blocks 6–15 were with adaptors, each block consisting of 28 trials. Each observer ran in a total of 1680 trials in <bold>Experiment 2</bold>.</p></sec><sec id="s4-6"><title>Adaptation variations in Experiment 3</title><p>In <bold>Experiment 3</bold>, we determined the transfer of adaptation for feature conjunctions (motion direction x color). The two discs in the test events and during adaptation had different colors (i.e. red and green) and could move in both horizontal directions. The red and green discs were not matched for luminance. Measurements obtained after the experiments yielded a luminance of 21 cd/m<sup>2</sup> for the green disc and 6 cd/m<sup>2</sup> for the red disc. In contrast to <bold>Experiments 1 and 2</bold>, slip events were not used as adaptors. This design resulted in four different adaptors that were presented in separate sessions. Again, the first five blocks of a session were without adaptation while blocks 6–15 were with adaptors, each block consisting of 28 trials. Each observer ran in a total of 1,680 trials in <bold>Experiment 3</bold>.</p></sec><sec id="s4-7"><title>Data analysis</title><p>For the statistical analyses and estimation of the psychometric functions, we used the <italic>quickpsy</italic> package (<xref ref-type="bibr" rid="bib25">Linares and López-Moliner, 2016</xref>) and the R environment (<xref ref-type="bibr" rid="bib29">Pelli, 1997</xref>). We related disc overlap to the proportion of reported launches using logistic functions with four parameters for the intercept, slope, as well as upper and lower asymptotes. We fitted these functions separately for each observer and condition and obtained the points of subjective equality (PSE; the amount of overlap between the two discs in the test events that result in the same proportion of reported launches and passes). For model fitting, we constrained the range of possible estimates for each parameter of the logistic model. The lower asymptote for the proportion of reported launches was constrained to be in the range of 0–0.75, and the upper asymptote in the range of 0.25–1. The intercept of the logistic model was constrained to be in the range 1–15, and the slope was constrained to be in the range –20 to –1.</p><p>For inferential statistics, we analyzed PSEs using repeated-measures analyses of variance (rmANOVA). Error bars indicate ±1 within-subject standard error of the mean (SEM; <xref ref-type="bibr" rid="bib2">Baguley, 2012</xref>; <xref ref-type="bibr" rid="bib28">Morey, 2008</xref>). A significant interaction in the rmANOVA was complemented by running post-hoc paired t-tests. Please note, one observer in <bold>Experiment 3</bold> showed such a strong negative aftereffect when test event and adaptor were identical, such that the upper asymptote was below a proportion of 0.5 (i.e. the point when launches and passes are equally often reported). Instead of a non-identifiable PSE for that observer in that particular condition, we computed inflection points of the psychometric functions in all conditions for that observer which then entered the statistical analyses. Neither of the studies reported in this article was preregistered. The data and analysis code has been deposited in the Open Science Framework and is publicly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/x947m/">https://osf.io/x947m/</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Methodology, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>In all experiments, we obtained observers' written informed consent before the first session. The study was approved by the ethics committee of the Psychology Department of the Humboldt-Universität zu Berlin and it followed the guidelines of the Declaration of Helsinki (2008).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93454-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and analysis code has been deposited at the Open Science Framework and is publicly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/x947m/">https://osf.io/x947m/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ohl</surname><given-names>S</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Visual routines for detecting causal interactions are tuned to motion direction</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/x947m/">x947m</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by a DFG research grant to SO (OH 274/4–1) as well as funding from the Heisenberg Programme of the DFG to SO (OH 274/5–1) and MR (grants RO3579/8-1 and RO3579/12-1). We acknowledge support by the Open Access Publication Fund of Humboldt-Universität zu Berlin. The authors declare no competing financial interests.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname><given-names>DH</given-names></name><name><surname>Petrie</surname><given-names>K</given-names></name><name><surname>Gallagher</surname><given-names>R</given-names></name><name><surname>Yarrow</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An object-centered aftereffect of A latent material property: A squishiness visual aftereffect, not causality adaptation</article-title><source>Journal of Vision</source><volume>15</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1167/15.9.4</pub-id><pub-id pub-id-type="pmid">26161633</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baguley</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Calculating and graphing within-subject confidence intervals for ANOVA</article-title><source>Behavior Research Methods</source><volume>44</volume><fpage>158</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.3758/s13428-011-0123-7</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>DH</given-names></name><name><surname>Vilidaite</surname><given-names>G</given-names></name><name><surname>Lygo</surname><given-names>FA</given-names></name><name><surname>Smith</surname><given-names>AK</given-names></name><name><surname>Flack</surname><given-names>TR</given-names></name><name><surname>Gouws</surname><given-names>AD</given-names></name><name><surname>Andrews</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Power contours: Optimising sample size and precision in experimental psychology and human neuroscience</article-title><source>Psychological Methods</source><volume>26</volume><fpage>295</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1037/met0000337</pub-id><pub-id pub-id-type="pmid">32673043</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name><name><surname>Hill</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Selective sensitivity to direction of movement in ganglion cells of the rabbit retina</article-title><source>Science</source><volume>139</volume><fpage>412</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1126/science.139.3553.412</pub-id><pub-id pub-id-type="pmid">13966712</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bechlivanidis</surname><given-names>C</given-names></name><name><surname>Schlottmann</surname><given-names>A</given-names></name><name><surname>Lagnado</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causation without realism</article-title><source>Journal of Experimental Psychology. General</source><volume>148</volume><fpage>785</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1037/xge0000602</pub-id><pub-id pub-id-type="pmid">30958016</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blakemore</surname><given-names>SJ</given-names></name><name><surname>Boyer</surname><given-names>P</given-names></name><name><surname>Pachot-Clouard</surname><given-names>M</given-names></name><name><surname>Meltzoff</surname><given-names>A</given-names></name><name><surname>Segebarth</surname><given-names>C</given-names></name><name><surname>Decety</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The detection of contingency and animacy from simple animations in the human brain</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>837</fpage><lpage>844</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.8.837</pub-id><pub-id pub-id-type="pmid">12853370</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caggiano</surname><given-names>V</given-names></name><name><surname>Fleischer</surname><given-names>F</given-names></name><name><surname>Pomper</surname><given-names>JK</given-names></name><name><surname>Giese</surname><given-names>MA</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mirror neurons in monkey premotor area F5 show tuning for critical features of visual causality perception</article-title><source>Current Biology</source><volume>26</volume><fpage>3077</fpage><lpage>3082</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.10.007</pub-id><pub-id pub-id-type="pmid">27818177</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference in the multisensory brain</article-title><source>Neuron</source><volume>102</volume><fpage>1076</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.03.043</pub-id><pub-id pub-id-type="pmid">31047778</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>P</given-names></name><name><surname>Labianca</surname><given-names>AT</given-names></name><name><surname>Thornton</surname><given-names>IM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Attention-based visual routines: sprites</article-title><source>Cognition</source><volume>80</volume><fpage>47</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(00)00153-0</pub-id><pub-id pub-id-type="pmid">11245839</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Sit</surname><given-names>TPH</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mouse frontal cortex mediates additive multisensory decisions</article-title><source>Neuron</source><volume>111</volume><fpage>2432</fpage><lpage>2447</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.05.008</pub-id><pub-id pub-id-type="pmid">37295419</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornelissen</surname><given-names>FW</given-names></name><name><surname>Peters</surname><given-names>EM</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The eyelink toolbox: eye tracking with MATLAB and the psychophysics toolbox</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>34</volume><fpage>613</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.3758/BF03195489</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonlupt</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Perception and judgement of physical causality involve different brain structures</article-title><source>Brain Research. Cognitive Brain Research</source><volume>17</volume><fpage>248</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(03)00112-5</pub-id><pub-id pub-id-type="pmid">12880896</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fugelsang</surname><given-names>JA</given-names></name><name><surname>Roser</surname><given-names>ME</given-names></name><name><surname>Corballis</surname><given-names>PM</given-names></name><name><surname>Gazzaniga</surname><given-names>MS</given-names></name><name><surname>Dunbar</surname><given-names>KN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Brain mechanisms underlying perceptual causality</article-title><source>Brain Research. Cognitive Brain Research</source><volume>24</volume><fpage>41</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.12.001</pub-id><pub-id pub-id-type="pmid">15922156</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafri</surname><given-names>A</given-names></name><name><surname>Firestone</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The perception of relations</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>475</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.01.006</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>AJ</given-names></name><name><surname>Houlton</surname><given-names>R</given-names></name><name><surname>Kampa</surname><given-names>BM</given-names></name><name><surname>Lesica</surname><given-names>NA</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stimulus relevance modulates contrast adaptation in visual cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e21589</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21589</pub-id><pub-id pub-id-type="pmid">28130922</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Pelli</surname><given-names>DG</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual adaptation: physiology, mechanisms, and functional benefits</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>3155</fpage><lpage>3164</lpage><pub-id pub-id-type="doi">10.1152/jn.00086.2007</pub-id><pub-id pub-id-type="pmid">17344377</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kominsky</surname><given-names>JF</given-names></name><name><surname>Strickland</surname><given-names>B</given-names></name><name><surname>Wertz</surname><given-names>AE</given-names></name><name><surname>Elsner</surname><given-names>C</given-names></name><name><surname>Wynn</surname><given-names>K</given-names></name><name><surname>Keil</surname><given-names>FC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Categories and constraints in causal perception</article-title><source>Psychological Science</source><volume>28</volume><fpage>1649</fpage><lpage>1662</lpage><pub-id pub-id-type="doi">10.1177/0956797617719930</pub-id><pub-id pub-id-type="pmid">28956971</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kominsky</surname><given-names>JF</given-names></name><name><surname>Scholl</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Retinotopic adaptation reveals distinct categories of causal perception</article-title><source>Cognition</source><volume>203</volume><elocation-id>104339</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104339</pub-id><pub-id pub-id-type="pmid">32711120</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuchenbecker</surname><given-names>J</given-names></name><name><surname>Broschmann</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Tafeln Zur Prüfung Des Farbensinnes/Farbensehens</source><publisher-name>Georg Thieme Verlag</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leslie</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Spatiotemporal continuity and the perception of causality in infants</article-title><source>Perception</source><volume>13</volume><fpage>287</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1068/p130287</pub-id><pub-id pub-id-type="pmid">6514514</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leslie</surname><given-names>AM</given-names></name><name><surname>Keeble</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Do six-month-old infants perceive causality?</article-title><source>Cognition</source><volume>25</volume><fpage>265</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(87)80006-9</pub-id><pub-id pub-id-type="pmid">3581732</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linares</surname><given-names>D</given-names></name><name><surname>López-Moliner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>quickpsy: An R package to fit psychometric functions for multiple groups</article-title><source>The R Journal</source><volume>8</volume><fpage>122</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.32614/RJ-2016-008</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Michotte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1963">1963</year><source>The Perception of Causality</source><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moors</surname><given-names>P</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>de-Wit</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Causal events enter awareness faster than non-causal events</article-title><source>PeerJ</source><volume>5</volume><elocation-id>e2932</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.2932</pub-id><pub-id pub-id-type="pmid">28149698</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morey</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Confidence intervals from normalized data: a correction to cousineau (2005)</article-title><source>Tutorials in Quantitative Methods for Psychology</source><volume>4</volume><fpage>61</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.20982/tqmp.04.2.p061</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rips</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Causation from perception</article-title><source>Perspectives on Psychological Science</source><volume>6</volume><fpage>77</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1177/1745691610393525</pub-id><pub-id pub-id-type="pmid">26162117</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Dambacher</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual adaptation of the perception of causality</article-title><source>Current Biology</source><volume>23</volume><fpage>250</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.12.017</pub-id><pub-id pub-id-type="pmid">23313360</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual neuroscience: seeing causality with the motor system?</article-title><source>Current Biology</source><volume>26</volume><fpage>R1183</fpage><lpage>R1185</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.09.046</pub-id><pub-id pub-id-type="pmid">27875696</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roser</surname><given-names>ME</given-names></name><name><surname>Fugelsang</surname><given-names>JA</given-names></name><name><surname>Dunbar</surname><given-names>KN</given-names></name><name><surname>Corballis</surname><given-names>PM</given-names></name><name><surname>Gazzaniga</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dissociating processes supporting causal perception and causal inference in the brain</article-title><source>Neuropsychology</source><volume>19</volume><fpage>591</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1037/0894-4105.19.5.591</pub-id><pub-id pub-id-type="pmid">16187877</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanborn</surname><given-names>AN</given-names></name><name><surname>Mansinghka</surname><given-names>VK</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reconciling intuitive physics and Newtonian mechanics for colliding objects</article-title><source>Psychological Review</source><volume>120</volume><fpage>411</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1037/a0031912</pub-id><pub-id pub-id-type="pmid">23458084</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlottmann</surname><given-names>A</given-names></name><name><surname>Shanks</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Evidence for a distinction between judged and perceived causality</article-title><source>The Quarterly Journal of Experimental Psychology Section A</source><volume>44</volume><fpage>321</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1080/02724989243000055</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname><given-names>BJ</given-names></name><name><surname>Tremoulet</surname><given-names>PD</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Perceptual causality and animacy</article-title><source>Trends in Cognitive Sciences</source><volume>4</volume><fpage>299</fpage><lpage>309</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01506-0</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname><given-names>BJ</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Causal capture: contextual effects on the perception of collision events</article-title><source>Psychological Science</source><volume>13</volume><fpage>493</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00487</pub-id><pub-id pub-id-type="pmid">12430831</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekuler</surname><given-names>R</given-names></name><name><surname>Sekuler</surname><given-names>AB</given-names></name><name><surname>Lau</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sound changes perception of visual motion</article-title><source>Nature</source><volume>384</volume><fpage>308</fpage><lpage>309</lpage><pub-id pub-id-type="doi">10.1038/385308a0</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Bayesian causal inference: A unifying neuroscience theory</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>137</volume><elocation-id>104619</elocation-id><pub-id pub-id-type="doi">10.1016/j.neubiorev.2022.104619</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><chapter-title>Visual routines</chapter-title><person-group person-group-type="editor"><name><surname>Ullman</surname><given-names>S</given-names></name></person-group><source>Readings in Computer Vision</source><publisher-name>ScienceDirect</publisher-name><fpage>298</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/B978-0-08-051581-6.50035-0</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual Adaptation</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>547</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035509</pub-id><pub-id pub-id-type="pmid">26858985</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weir</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The percepton of motion: Michotte revisited</article-title><source>Perception</source><volume>7</volume><fpage>247</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1068/p070247</pub-id><pub-id pub-id-type="pmid">358131</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The causal asymmetry</article-title><source>Psychological Review</source><volume>113</volume><fpage>132</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.1.132</pub-id><pub-id pub-id-type="pmid">16478304</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>White</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title><italic>visual impressions of causality</italic></chapter-title><person-group person-group-type="editor"><name><surname>White</surname><given-names>PA</given-names></name></person-group><source>The Oxford Handbook of Causal Reasoning</source><publisher-name>Oxford University Press</publisher-name><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199399550.013.17</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93454.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kok</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This study provides a <bold>valuable</bold> contribution to our understanding of causal inference in visual perception. The evidence provided through multiple well-designed psychophysical experiments is <bold>convincing</bold>. The current study targets very specific visual features of launch events, future work will be able to build on this to study the implementation of causal inference in general.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93454.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors investigated causal inference in the visual domain through a set of carefully designed experiments, and sound statistical analysis. They suggest the early visual system has a crucial contribution to computations supporting causal inference.</p><p>Strengths:</p><p>(1) I believe the authors target an important problem (causal inference) with carefully chosen tools and methods. Their analysis rightly implies the specialization of visual routines for causal inference and the crucial contribution of early visual systems to perform this computation. I believe this is a novel contribution and their data and analysis are in the right direction.</p><p>(2) Authors sufficiently discuss the alternative perspective to causal inference.</p><p>(3) The authors also expand the discussions beyond pure psychophysics and also include neural aspects.</p><p>Weaknesses:</p><p>I would not call them weaknesses, perhaps a different perspective:</p><p>(1) Authors arguing pro a mere bottom-up contribution of early sensory areas for causal inference. Certainly, as the authors suggested, early sensory areas have a crucial contribution, and the authors expand it to other possibilities in their discussion (but more for more complex scenario). It would say, even in simple cases, we can still consider the effect of top down processes. This particularly makes sense in light of recent studies. These studies progressively suggest perception as an active process that also weighs in strongly, the top-down cognitive contributions. For instance, the most simple cases of perception have been conceptualized along this line (Martin, Solms, and Sterzer 2021) and even some visual illusions (Safavi and Dayan 2022), and other extensions (Kay et al. 2023). Thus, I believe it would be helpful to extend the discussion on the top-down and cognitive contributions of causal inference (of course that can also be hinted at, based on recent developments). Even adaptation, which is central in this study, can be influenced by top-down factors (Keller et al. 2017).</p><p>Lastly, I hope the authors find this review helpful. I generally want to try to end all of my reviews with areas of the paper I liked because I think this should be part of the feedback. Certainly, there were many in this manuscript as well (clever questions, experimental design and statistical analysis) that I had to highlight further. I congratulate the authors again on their manuscript and hope they will find it helpful.</p><p>Bibliography</p><p>Aller, Mate, and Uta Noppeney. 2018. &quot;To Integrate or Not to Integrate: Temporal Dynamics of Bayesian Causal Inference.&quot; Biorxiv, December, 504118. .</p><p>Cao, Yinan, Christopher Summerfield, Hame Park, Bruno Lucio Giordano, and Christoph Kayser. 2019. &quot;Causal Inference in the Multisensory Brain.&quot; Neuron 102 (5): 1076-87.e8. .</p><p>Coen, Philip, Timothy P. H. Sit, Miles J. Wells, Matteo Carandini, and Kenneth D. Harris. 2021. &quot;The Role of Frontal Cortex in Multisensory Decisions.&quot; Biorxiv, April. Cold Spring Harbor Laboratory, 2021.04.26.441250. .</p><p>Kay, Kendrick, Kathryn Bonnen, Rachel N. Denison, Mike J. Arcaro, and David L. Barack. 2023. &quot;Tasks and Their Role in Visual Neuroscience.&quot; Neuron 111 (11). Elsevier: 1697-1713. .</p><p>Keller, Andreas J, Rachael Houlton, Björn M Kampa, Nicholas A Lesica, Thomas D Mrsic-Flogel, Georg B Keller, and Fritjof Helmchen. 2017. &quot;Stimulus Relevance Modulates Contrast Adaptation in Visual Cortex.&quot; Elife 6. eLife Sciences Publications, Ltd: e21589.</p><p>Kording, K. P., U. Beierholm, W. J. Ma, S. Quartz, J. B. Tenenbaum, and L. Shams. 2007. &quot;Causal Inference in Multisensory Perception.&quot; PloS One 2: e943. .</p><p>Martin, Joshua M., Mark Solms, and Philipp Sterzer. 2021. &quot;Useful Misrepresentation: Perception as Embodied Proactive Inference.&quot; Trends Neurosci. 44 (8): 619-28. .</p><p>Safavi, Shervin, and Peter Dayan. 2022. &quot;Multistability, Perceptual Value, and Internal Foraging.&quot; Neuron, August. .</p><p>Shams, L. 2012. &quot;Early Integration and Bayesian Causal Inference in Multisensory Perception.&quot; In The Neural Bases of Multisensory Processes, edited by M. M. Murray and M. T. Wallace. Frontiers in Neuroscience. Boca Raton (FL).</p><p>Shams, Ladan, and Ulrik Beierholm. 2022. &quot;Bayesian Causal Inference: A Unifying Neuroscience Theory.&quot; Neuroscience &amp; Biobehavioral Reviews 137 (June): 104619.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93454.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This paper seeks to determine whether the human visual system's sensitivity to causal interactions is tuned to specific parameters of a causal launching event, using visual adaptation methods. The three parameters the author investigates in this paper are the direction of motion in the event, the speed of the objects in the event, and surface features or identity of the objects in the event (in particular, having two objects of different color).</p><p>The key method, visual adaptation to causal launching, has now been demonstrated by at least three separate groups and seems to be a robust phenomenon. Adaptation is a strong indicator of a visual process that is tuned to a specific feature of the environment, in this case launching interactions. Whereas other studies have focused on retinotopically-specific adaptation (i.e., whether the adaptation effect is restricted to the same test location on the retina as the adaptation stream was presented to), this one focuses on feature-specificity.</p><p>The first experiment replicates the adaptation effect for launching events as well as the lack of adaptation event for a minimally different non-causal 'slip' event. However, it also finds that the adaptation effect does not work for launching events that do not have a direction of motion more than 30 degrees from the direction of the test event. The interpretation is that the system that is being adapted is sensitive to the direction of this event, which is an interesting and somewhat puzzling result given the methods used in previous studies, which have used random directions of motion for both adaptation and test events.</p><p>The obvious interpretation would be that past studies have simply adapted to launching in every direction, but that in itself says something about the nature of this direction-specificity: it is not working through opposed detectors. For example, in something like the waterfall illusion adaptation effect, where extended exposure to downward motion leads to illusory upward motion on neutral-motion stimuli, the effect simply doesn't work if motion in two opposed directions are shown (i.e., you don't see illusory motion in both directions, you just see nothing). The fact that adaptation to launching in multiple directions doesn't seem to cancel out the adaptation effect in past work raises interesting questions about how directionality is being coded in the underlying process. In addition, one limitation of the current method is that it's not clear whether the motion-direction-specificity is also itself retinotopically-specific, that is, if one retinotopic location were adapted to launching in one direction and a different retinotopic location adapted to launching in the opposite direction, would each test location show the adaptation effect only for events in the direction presented at that location?</p><p>The second experiment tests whether the adaptation effect is similarly sensitive to differences in speed. The short answer is no; adaptation events at one speed affect test events at another. Furthermore, this is not surprising given that Kominsky &amp; Scholl (2020) showed adaptation transfer between events with differences in speeds of the individual objects in the event (whereas all events in this experiment used symmetrical speeds). This experiment is still novel and it establishes that the speed-insensitivity of these adaptation effects is fairly general, but I would certainly have been surprised if it had turned out any other way.</p><p>The third experiment tests color (as a marker of object identity), and pits it against motion direction. The results demonstrate that adaptation to red-launching-green generates an adaptation effect for green-launching-red, provided they are moving in roughly the same direction, which provides a nice internal replication of Experiment 1 in addition to showing that the adaptation effect is not sensitive to object identity. This result forms an interesting contrast with the infant causal perception literature. Multiple papers (starting with Leslie &amp; Keeble, 1987) have found that 6-8-month-old infants are sensitive to reversals in causal roles exactly like the ones used in this experiment. The success of adaptation transfer suggests, very clearly, that this sensitivity is not based only on perceptual processing, or at least not on the same processing that we access with this adaptation procedure. It implies that infants may be going beyond the underlying perceptual processes and inferring genuine causal content. This is also not the first time the adaptation paradigm has diverged from infant findings: Kominsky &amp; Scholl (2020) found a divergence with the object speed differences as well, as infants categorize these events based on whether the speed ratio (agent:patient) is physically plausible (Kominsky et al., 2017), while the adaptation effect transfers from physically implausible events to physically plausible ones. This only goes to show that these adaptation effects don't exhaustively capture the mechanisms of early-emerging causal event representation.</p><p>One overarching point about the analyses to take into consideration: The authors use a Bayesian psychometric curve-fitting approach to estimate a point of subjective equality (PSE) in different blocks for each individual participant based on a model with strong priors about the shape of the function and its asymptotic endpoints, and this PSE is the primary DV across all of the studies. As discussed in Kominsky &amp; Scholl (2020), this approach has certain limitations, notably that it can generate nonsensical PSEs when confronted with relatively extreme response patterns. The authors mentioned that this happened once in Experiment 3, and that participant had to be replaced. An alternate approach is simply to measure the proportion of 'pass' reports overall to determine if there is an adaptation effect. The results here do not change based on which analytical strategy is used, which ultimately just goes to show that the effects are very robust.</p><p>In general, this paper adds further evidence for something like a 'launching' detector in the visual system, but beyond that it specifies some interesting questions for future work about how exactly such a detector might function.</p><p>Kominsky, J. F., &amp; Scholl, B. J. (2020). Retinotopic adaptation reveals distinct categories of causal perception. Cognition, 203, 104339. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2020.104339">https://doi.org/10.1016/j.cognition.2020.104339</ext-link></p><p>Kominsky, J. F., Strickland, B., Wertz, A. E., Elsner, C., Wynn, K., &amp; Keil, F. C. (2017). Categories and Constraints in Causal Perception. Psychological Science, 28(11), 1649-1662. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797617719930">https://doi.org/10.1177/0956797617719930</ext-link></p><p>Leslie, A. M., &amp; Keeble, S. (1987). Do six-month-old infants perceive causality? Cognition, 25(3), 265-288. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0010-0277(87)80006-9">https://doi.org/10.1016/S0010-0277(87)80006-9</ext-link></p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93454.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper presents evidence from three behavioral experiments that causal impressions of &quot;launching events&quot;, in which one object is perceived to cause another object to move, depend on motion direction-selective processing. Specifically, the work uses an adaptation paradigm (Rolfs et al., 2013), presenting repetitive patterns of events matching certain features to a single retinal location, then measuring subsequent perceptual reports of a test display in which the degree of overlap between two discs was varied, and participants could respond &quot;launch&quot; or &quot;pass&quot;. The three experiments report results of adapting to motion direction, motion speed and &quot;object identity&quot;, and examine how the psychometric curves for causal reports shift in these conditions depending on the similarity of adapter and test. While causality reports in the test display were selective for motion direction (Experiment 1), they were not selective for adapter-test speed differences (Experiment 2) nor for changes in object identity induced via color swap (Experiment 3). These results support the notion of a biological implementation of causality perception in the visual system, possibly even independently of computations of object identity.</p><p>Strengths:</p><p>The setup of the research question and hypotheses are exceptional. The authors thoroughly discuss relevant literature to clearly link their launch/pass paradigm to impressions of causality, strengthening their hypothesis and conclusions. The experiments are carefully performed (appropriate equipment, careful control of eye movements). The slip adaptor is a really nice control condition and effectively mitigates the need to control for motion direction with a drifting grating or similar. Participants were measured with sufficient precision, and a power curve analysis was conducted to determine the sample size. Data analysis and statistical quantification is appropriate. Data and analysis code will be shared on publication, in keeping with open science principles. The paper is concise and well written.</p><p>Weaknesses:</p><p>I would like to emphasise that in the employed paradigm and previously conducted similar study, the only report options are &quot;launch&quot; or &quot;pass&quot;. As pointed out by the authors' reply, the adaptation to launches seems to be a highly specific process and likely is a consequence of the causal interaction between the objects. I would nonetheless be interested to see which of the stimulus features driving the adaptation effect observed here are relevant/irrelevant to subjective causal impressions in an experiment.</p><p>References:</p><p>Rolfs, M., Dambacher, M., &amp; Cavanagh, P. (2013). Visual Adaptation of the Perception of Causality. Current Biology, 23(3), 250-254. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2012.12.017">https://doi.org/10.1016/j.cub.2012.12.017</ext-link></p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93454.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ohl</surname><given-names>Sven</given-names></name><role specific-use="author">Author</role><aff><institution>Humboldt-Universität zu Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Rolfs</surname><given-names>Martin</given-names></name><role specific-use="author">Author</role><aff><institution>Humboldt-Universität zu Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review</bold>):</p><p>Summary:</p><p>The authors investigated causal inference in the visual domain through a set of carefully designed experiments, and sound statistical analysis. They suggest the early visual system has a crucial contribution to computations supporting causal inference.</p><p>Strengths:</p><p>I believe the authors target an important problem (causal inference) with carefully chosen tools and methods. Their analysis rightly implies the specialization of visual routines for causal inference and the crucial contribution of early visual systems to perform this computation. I believe this is a novel contribution and their data and analysis are in the right direction.</p><p>Weaknesses:</p><p>In my humble opinion, a few aspects deserve more attention:</p><p>(1) Causal inference (or causal detection) in the brain should be quite fundamental and quite important for human cognition/perception. Thus, the underlying computation and neural substrate might not be limited to the visual system (I don't mean the authors did claim that). In fact, to the best of my knowledge, multisensory integration is one of the best-studied perceptual phenomena that has been conceptualized as a causal inference problem.</p><p>Assuming the causal inference in those studies (Shams 2012; Shams and Beierholm 2022; Kording et al. 2007; Aller and Noppeney 2018; Cao et al. 2019) (and many more e.g., by Shams and colleagues), and the current study might share some attributes, one expects some findings in those domains are transferable (at least to some degree) here as well. Most importantly, underlying neural correlates that have been suggested based on animal studies and invasive recording that has been already studied, might be relevant here as well.</p><p>Perhaps the most relevant one is the recent work from the Harris group on mice (Coen et al. 2021). I should emphasize, that I don't claim they are necessarily relevant, but they can be relevant given their common roots in the problem of causal inference in the brain. This is a critical topic that the authors may want to discuss in their manuscript.</p></disp-quote><p>We thank the reviewer. We addressed this point of the public review in our reply to the reviewer’s suggestions (and add it here again for convenience). The literature on the role of occipital, parietal and frontal brain areas in causal inference is also addressed in the response to point 3 of the public review.</p><p>“We used visual adaptation to carve out a bottom-up visual routine for detecting causal interactions in form of launching events. However, we know that more complex behaviors of perceiving causal relations can result from integrating information across space (e.g., in causal capture; Scholl &amp; Nakayama, 2002), across time (postdictive influence; Choi &amp; Scholl, 2006), and across sensory modalities (Sekuler, Sekuler, &amp; Lau, 1997). Bayesian causal inference has been particularly successful as a normative framework to account for multisensory integration (Körding et al., 2007; Shams &amp; Beierholm, 2022). In that framework, the evidence for a common-cause hypothesis is competing with the evidence for an independent-causes hypothesis (Shams &amp; Beierholm, 2022). The task in our experiments could be similarly formulated as two competing hypotheses for the second disc’s movement (i.e., the movement was caused by the first disc vs. the movement occurred autonomously). This framework also emphasizes the distributed nature of the neural implementation for solving such inferences, showing the contributions of parietal and frontal areas in addition to sensory processing (for review see Shams &amp; Beierholm, 2022). Moreover, even visual adaptation to contrast in mouse primary visual cortex is influenced by top-down factors such as behavioral relevance— suggesting a complex implementation of the observed adaptation results (Keller et al. 2017). The present experiments, however, presented purely visual events that do not require an integration across processing domains. Thus, the outcome of our suggested visual routine can provide initial evidence from within the visual system for a causal relation in the environment that may then be integrated with signals from other domains (e.g., auditory signals). Determining exactly how the perception of causality relates to mechanisms of causal inference and the neural implementation thereof is an exciting avenue for future research. Note, however, that perceived causality can be distinguished from judged causality: Even when participants are aware that a third variable (e.g., a color change) is the best predictor of the movement of the second disc in launching events, they still perceive the first disc as causing the movement of the second disc (Schlottmann &amp; Shanks, 1992).”</p><disp-quote content-type="editor-comment"><p>(2) If I understood correctly, the authors are arguing pro a mere bottom-up contribution of early sensory areas for causal inference (for instance, when they wrote &quot;the specialization of visual routines for the perception of causality at the level of individual motion directions raises the possibility that this function is located surprisingly early in the visual system *as opposed to a higher-level visual computation*.&quot;). Certainly, as the authors suggested, early sensory areas have a crucial contribution, however, it may not be limited to that. Recent studies progressively suggest perception as an active process that also weighs in strongly, the topdown cognitive contributions. For instance, the most simple cases of perception have been conceptualized along this line (Martin, Solms, and Sterzer 2021) and even some visual illusion (Safavi and Dayan 2022), and other extensions (Kay et al. 2023). Thus, I believe it would be helpful to extend the discussion on the top-down and cognitive contributions of causal inference (of course that can also be hinted at, based on recent developments). Even adaptation, which is central in this study can be influenced by top-down factors (Keller et al. 2017). I believe, based on other work of Rolfs and colleagues, this is also aligned with their overall perspective on vision.</p></disp-quote><p>Indeed, we assessed bottom-up contributions to the perception of a causal relation. We agree with the reviewer that in more complex situations, for instance, in the presence of contextual influences or additional auditory signals, the perception of a causal relation may not be limited to bottom-up vision. While we had acknowledged this in the original manuscript (see excerpts below), we now make it even more explicit:</p><p>“[…] we know that more complex behaviors of perceiving causal relations can result from integrating information across space (e.g., in causal capture; Scholl &amp; Nakayama, 2002), across time (postdictive influence; Choi &amp; Scholl, 2006), and across sensory modalities (Sekuler, Sekuler, &amp; Lau, 1997).”</p><p>“[…] Neurophysiological studies support the view of distributed neural processing underlying sensory causal interactions with the visual system playing a major role.”</p><p>“[…] Interestingly, single cell recordings in area F5 of the primate brain revealed that motor areas are contributing to the perception of causality (Caggiano et al., 2016; Rolfs, 2016), emphasizing the distributed nature of the computations underlying causal interactions. This finding also stresses that the detection, and the prediction, of causality is essential for processes outside sensory systems (e.g., for understanding other’s actions, for navigating, and for avoiding collisions). The neurophysiology subserving causal inference further extend the candidate cortical areas that might contibute to the detection of causal relations, emphasizing the role of the frontal cortex for the flexible integration of multisensory representations (Cao et al., 2019; Coen et al., 2023).”</p><p>However, there is also ample evidence that the perception of a simple causal relation—as we studied it in our experiments—escapes top-down cognitive influences. The perception of causality in launching events is described as automatic and irresistible, meaning that participants have the spontaneous impression of a causal relation, and participants typically do not voluntarily switch between a causal and a noncausal percept. This irresistibility has led several authors to discuss a modular organization underlying the detection of such events (Michotte, 1963; Scholl &amp; Tremoulet, 2000). This view is further supported by a study that experimentally manipulated the contingencies between the movement of the two discs (Schlottmann &amp; Shanks, 1992). In one condition the authors created a launching event where the second disc’s movement was perfectly correlated with a color change, but only sometimes coincided with the first disc’s movement offset. Nevertheless, participants reported seeing that the first disc caused the movement of second disc (regardless of the stronger statistical relationship with the color change). However, when asked to make conscious causal judgments, participants were aware of the color change as the true cause of the second disc’s motion—therefore recognizing its more reliable correlation. This study strongly suggests that perceived and judged causality (i.e., cognitive causal inference) can be dissociated (Schlottmann &amp; Shanks, 1992). We have added this reference in the revised manuscript. Overall, we argue that our study focused on a visual routine that could be implemented in a simple bottom-up fashion, but we acknowledge throughout the manuscript, that in a more complex situation (e.g., integrating information from other sensory domains) the implementation could be realized in a more distributed fashion including top-down influences as in multisensory integration. However, it is important to stress that these potential top-down influences would be automatic and should not be confused with voluntary cognitive influences.</p><p>“Note, however, that perceived causality can be distinguished from judged causality (Schlottmann &amp; Shanks, 1992). Even when participants are aware that a third variable (e.g., a color change) is the best predictor of the movement of the second disc in launching events, they still perceive the first disc as causing the movement of the second disc (Schlottmann &amp; Shanks, 1992).”</p><disp-quote content-type="editor-comment"><p>(3) The authors rightly implicate the neural substrate of causal inference in the early sensory system. Given their study is pure psychophysics, a more elaborate discussion based on other studies that used brain measurements is needed (in my opinion) to put into perspective this conclusion. In particular, as I mentioned in the first point, the authors mainly discuss the potential neural substrate of early vision, however much has been done about the role of higher-tier cortical areas in causal inference e.g., see (Cao et al. 2019; Coen et al. 2021).</p></disp-quote><p>In the revised manuscript, we addressed the limitations of a purely psychophysical approach and acknowledged alternative implementations in the Discussion section.</p><p>“Note that, while the present findings demonstrate direction-selectivity, it remains unclear where exactly that visual routine is located. As pointed out, it is also possible that the visual routine is located higher up in the visual system (or distributed across multiple levels) and is only using a directional-selective population response as input.”</p><p>Moreover, we cite also the two suggested papers when referring to the role of cortical areas in causal inference (Cao et al, 2019; Coen et al., 2023):</p><p>“Neurophysiological studies support the view of distributed neural processing underlying sensory causal interactions with the visual system playing a major role. Imaging studies in particular revealed a network for the perception of causality that is also involved in action observation (Blakemore et al., 2003; Fonlupt, 2003; Fugelsang et al., 2005; Roser et al., 2005). The fact that visual adaptation of causality occurs in a retinotopic reference frame emphazises the role of retinotopically organized areas within that network (e.g., V5 and the superior temporal sulcus). Interestingly, single cell recordings in area F5 of the primate brain revealed that motor areas are contributing to the perception of causality (Caggiano et al., 2016; Rolfs, 2016), emphasizing the distributed nature of the computations underlying causal interactions, and also stressing that the detection, and the prediction, of causality is essential for processes outside purely sensory systems (e.g., for understanding other’s actions, for navigating, and for avoiding collisions). The neurophysiological underpinnings in causal inference further extend the candidate cortical areas that might contibute to the detection of causal relations, emphasizing the role of the frontal cortex for the flexible integration of multisensory representations (Cao et al., 2019; Coen et al., 2023).”</p><disp-quote content-type="editor-comment"><p>There were many areas in this manuscript that I liked: clever questions, experimental design, and statistical analysis.</p></disp-quote><p>Thank you so much.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>I congratulate the authors again on their manuscript and hope they will find my review helpful. Most of my notes are suggestions to the authors, and I hope will help them to improve the manuscript. None are intended to devalue their (interesting) work.</p></disp-quote><p>We would like to thank the reviewer for their thoughtful and encouraging comments.</p><disp-quote content-type="editor-comment"><p>In the following, I use pX-lY template to refer to a particular page number, say page number X (pX), and line number, say line number Y (lY).</p><p>Major concerns and suggestions</p><p>- I would suggest simplifying the abstract and significance statement or putting more background in it. It's hard (at least for me) to understand if one is not familiar with the task used in this study.</p></disp-quote><p>We followed the reviewer’s suggestion and added more background in the beginning of the abstract.</p><p>We made the following changes:</p><p>“Detecting causal relations structures our perception of events in the world. Here, we determined for visual interactions whether generalized (i.e., feature-invariant) or specialized (i.e., feature-selective) visual routines underlie the perception of causality. To this end, we applied a visual adaptation protocol to assess the adaptability of specific features in classical launching events of simple geometric shapes. We asked observers to report whether they observed a launch or a pass in ambiguous test events (i.e., the overlap between two discs varied from trial to trial). After prolonged exposure to causal launch events (the adaptor) defined by a particular set of features (i.e., a particular motion direction, motion speed, or feature conjunction), observers were less likely to see causal launches in subsequent ambiguous test events than before adaptation. Crucially, adaptation was contingent on the causal impression in launches as demonstrated by a lack of adaptation in non-causal control events. We assessed whether this negative aftereffect transfers to test events with a new set of feature values that were not presented during adaptation. Processing in specialized (as opposed to generalized) visual routines predicts that the transfer of visual adaptation depends on the feature-similarity of the adaptor and the test event. We show that negative aftereffects do not transfer to unadapted launch directions but do transfer to launch events of different speed. Finally, we used colored discs to assign distinct feature-based identities to the launching and the launched stimulus. We found that the adaptation transferred across colors if the test event had the same motion direction as the adaptor. In summary, visual adaptation allowed us to carve out a visual feature space underlying the perception of causality and revealed specialized visual routines that are tuned to a launch’s motion direction.”</p><disp-quote content-type="editor-comment"><p>- The authors highlight the importance of studying causal inference and understanding the underlying mechanisms by probing adaptation, however, their introduction justifying that is, in my humble opinion, quite short. Perhaps in the cited paper, this is discussed extensively, but I'd suggest providing some elaboration in the manuscript. Otherwise, the study would be very specific to certain visual phenomena, rather than general mechanisms.</p></disp-quote><p>We have carefully considered the reviewer’s set of comments and concerns (e.g., the role of top-down influences, the contributions of the frontal cortex, and illustration of the computational level). They all appear to share the theme that the reviewer looks at our study from the perspective of Bayesian inference. We conducted the current study in the tradition of classical phenomena in the field of the perception of causality (in the tradition of Michotte, 1963 and as reviewed in Scholl &amp; Tremoulet, 2000) which aims to uncover the relevant visual parameters and rules for detecting causal relations in the visual domain. Indeed, we think that a causal inference perspective promises a lot of new insights into the mechanisms underlying the classical phenomena described for the perception of causality. In the revised manuscript, we discuss therefore causal inference and how it relates to the current study. We now emphasize that in our study, (a) we used visual adaptation to reveal the bottom-up processes that allow for the detection of a causal interaction in the visual domain, (b) that the perception of causality also integrates signals from other domains (which we do not study here), and (c) that the neural substrates underlying the perception of causality might be best described by a distributed network. By discussing Bayesian causal inference, we point out promising avenues for future research that may bridge the fields of the perception of causality and Bayesian causal inference. However, we also emphasize that perceived causality and judged causality can be dissociated (Schlottmann &amp; Shanks, 1992).</p><p>We added the following discussion:</p><p>“We used visual adaptation to carve out a bottom-up visual routine for detecting causal interactions in form of launching events. However, we know that more complex behaviors of perceiving causal relations can result from integrating information across space (e.g., in causal capture; Scholl &amp; Nakayama, 2002), across time (postdictive influence; Choi &amp; Scholl, 2006), and across sensory modalities (Sekuler, Sekuler, &amp; Lau, 1997). Bayesian causal inference has been particularly successful as a normative framework to account for multisensory integration (Körding et al., 2007; Shams &amp; Beierholm, 2022). In that framework, the evidence for a common-cause hypothesis is competing with the evidence for an independent-causes hypothesis (Shams &amp; Beierholm, 2022). The task in our experiments could be similarly formulated as two competing hypotheses for the second disc’s movement (i.e., the movement was caused by the first disc vs. the second disc did not move). This framework also emphasizes the distributed nature of the neural implementation for solving such inferences, showing the contributions of parietal and frontal areas in addition to sensory processing (for review see Shams &amp; Beierholm, 2022). Moreover, even visual adaptation to contrast in mouse primary visual cortex is influenced by top-down factors such as behavioral relevance— suggesting a complex implementation of the observed adaptation results (Keller et al. 2017). The present experiments, however, presented purely visual events that do not require an integration across processing domains. Thus, the outcome of our suggested visual routine can provide initial evidence from within the visual system for a causal relation in the environment that may then be integrated with signals from other domains (e.g., auditory signals). Determining exactly how the perception of causality relates to mechanisms of causal inference and the neural implementation thereof is an exciting avenue for future research. Note, however, that perceived causality can be distinguished from judged causality: Even when participants are aware that a third variable (e.g., a color change) is the best predictor of the movement of the second disc in launching events, they still perceive the first disc as causing the movement of the second disc (Schlottmann &amp; Shanks, 1992).”</p><disp-quote content-type="editor-comment"><p>- I'd suggest, at the outset, already set the context, that your study of causal inference in the brain is specifically targeting the visual domain, if you like, in the discussion connect it better to general ideas about causal inference in the brain (like the works by Ladan Shams and colleagues).</p></disp-quote><p>We would like to thank the reviewer for this comment. We followed the reviewer’s suggestion and made clear from the beginning that this paper is about the detection of causal relations in the visual domain. In the revised manuscript we write:</p><p>“Here, we will study the mechanisms underlying the computations of causal interactions in the visual domain by capitalizing on visual adaptation of causality (Kominsky &amp; Scholl, 2020; Rolfs et al., 2013). Adaptation is a powerful behavioral tool for discovering and dissecting a visual mechanism (Kohn, 2007; Webster, 2015) that provides an intriguing testing ground for the perceptual roots of causality.”</p><p>As described in our reply to the previous comment, we now also discussed the ideas about causal inference.</p><disp-quote content-type="editor-comment"><p>- To better illustrate the implication of your study on the computational level, I'd suggest putting it in the context of recent approaches to perception (point 2 of my public review). I think this is also aligned with the comment of Reviewer#3 on your line 32 (recommendation for authors).</p></disp-quote><p>In the revised manuscript, we now discuss the role of top-down influences in causal inference when addressing point 2 of the reviewer’s public review.</p><disp-quote content-type="editor-comment"><p>Minor concerns and suggestions</p><p>- On p2-l3, I'd suggest providing a few examples for generalized and or specialized visual routines (given the importance of the abstract). I only got it halfway through the introduction.</p></disp-quote><p>We thank the reviewer for highlighting the need to better introduce the concept of a visual routine. We have chosen the term visual routine to emphasize that we locate the part of the mechanism that is affected by the adaptation in our experiments in the visual system. At the same time, the concept leaves space with respect to the extent to which the mechanism further involves mid- and higher-level processes. In the revised manuscript, we now refer to Ullman (1987) who introduced the concept of a visual routine—the idea of a modular operation that sequentially processes spatial and feature information. Moreover, we refer to the concept of attentional sprites (Cavanagh, Labianca, &amp; Thornton, 2001)—attention-based visual routines that allow the visual system to semi-independently handle complex visual tasks (e.g., identifying biological motion).</p><p>We add the following footnote to the introduction:</p><p>“We use the term visual routine here to highlight that our adaptation experiments can reveal a causality detection mechanism that resides in the visual system. At the same time, calling it a routine emphasizes similarities with a local, semi-independent operation (e.g., the recognition of familiar motion patterns; see also Ullman, 1987; Cavanagh, Labianca, &amp; Thornton, 2001) that can engage mid- and higher-level processes (e.g., during causal capture, Scholl &amp; Nakayama, 2002; or multisensory integration, Körding et al., 2007).”</p><p>In the abstract we now write:</p><p>“Here, we determined for visual interactions whether generalized (i.e., feature-invariant) or specialized (i.e., feature-selective) visual routines underlie the perception of causality.”</p><disp-quote content-type="editor-comment"><p>- On p4-l31, I'd suggest mentioning the Matlab version. I have experienced differences across different versions of Matlab (minor but still ...).</p></disp-quote><p>We added the Matlab Version.</p><disp-quote content-type="editor-comment"><p>- On p6-l46 OSF-link is missing (that contains data and code).</p></disp-quote><p>Thank you. We made the OSF repository public and added the link to the revised manuscript.</p><p>We added the following information to the revised manuscript.</p><p>“The data analysis code has been deposited at the Open Science Framework and is publicly available <ext-link ext-link-type="uri" xlink:href="https://osf.io/x947m/">https://osf.io/x947m/</ext-link>.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>This paper seeks to determine whether the human visual system's sensitivity to causal interactions is tuned to specific parameters of a causal launching event, using visual adaptation methods. The three parameters the authors investigate in this paper are the direction of motion in the event, the speed of the objects in the event, and the surface features or identity of the objects in the event (in particular, having two objects of different colors). The key method, visual adaptation to causal launching, has now been demonstrated by at least three separate groups and seems to be a robust phenomenon. Adaptation is a strong indicator of a visual process that is tuned to a specific feature of the environment, in this case launching interactions. Whereas other studies have focused on retinotopically specific adaptation (i.e., whether the adaptation effect is restricted to the same test location on the retina as the adaptation stream was presented to), this one focuses on feature specificity.</p><p>The first experiment replicates the adaptation effect for launching events as well as the lack of adaptation event for a minimally different non-causal 'slip' event. However, it also finds that the adaptation effect does not work for launching events that do not have a direction of motion more than 30 degrees from the direction of the test event. The interpretation is that the system that is being adapted is sensitive to the direction of this event, which is an interesting and somewhat puzzling result given the methods used in previous studies, which have used random directions of motion for both adaptation and test events.</p><p>The obvious interpretation would be that past studies have simply adapted to launching in every direction, but that in itself says something about the nature of this direction-specificity: it is not working through opposed detectors. For example, in something like the waterfall illusion adaptation effect, where extended exposure to downward motion leads to illusory upward motion on neutral-motion stimuli, the effect simply doesn't work if motion in two opposed directions is shown (i.e., you don't see illusory motion in both directions, you just see nothing). The fact that adaptation to launching in multiple directions doesn't seem to cancel out the adaptation effect in past work raises interesting questions about how directionality is being coded in the underlying process.</p></disp-quote><p>We would like to thank the reviewer for that thoughtful comment. We added the described implication to the manuscript:</p><p>“While the present study demonstrates direction-selectivity for the detection of launches, previous adaptation protocols demonstrated successful adaptation using adaptors with random motion direction (Rolfs et al., 2013; Kominsky &amp; Scholl, 2020). These results therefore suggest independent direction-specific routines, in which adaptation to launches in one direction does not counteract an adaptation to launches in the opposite direction (as for example in opponent color coding).”</p><disp-quote content-type="editor-comment"><p>In addition, one limitation of the current method is that it's not clear whether the motion direction-specificity is also itself retinotopically-specific, that is, if one retinotopic location were adapted to launching in one direction and a different retinotopic location adapted to launching in the opposite direction, would each test location show the adaptation effect only for events in the direction presented at that location?</p></disp-quote><p>This is an interesting idea! Because previous adaptation studies consistently showed retinotopic adaptation of causality, we would not expect to find transfer of directional tuning for launches to other locations. We agree that the suggested experiment on testing the reference frame of directional specificity constitutes an interesting future test of our findings.</p><disp-quote content-type="editor-comment"><p>The second experiment tests whether the adaptation effect is similarly sensitive to differences in speed. The short answer is no; adaptation events at one speed affect test events at another. Furthermore, this is not surprising given that Kominsky &amp; Scholl (2020) showed adaptation transfer between events with differences in speeds of the individual objects in the event (whereas all events in this experiment used symmetrical speeds). This experiment is still novel and it establishes that the speed-insensitivity of these adaptation effects is fairly general, but I would certainly have been surprised if it had turned out any other way.</p></disp-quote><p>We thank the reviewer for highlighting the link to an experiment reported in Kominsky &amp; Scholl (2020). We report the finding of that experiment now in the revised manuscript.</p><p>We added the following paragraph in the discussion:</p><p>“For instance, we demonstrated a transfer of adaptation across speed for symmetrical speed ratios. This result complements a previous finding that reported that the adaptation to triggering events (with an asymmetric speed ratio of 1:3) resulted in significant retinotopic adaptation of ambiguous (launching) test events of different speed ratios (i.e., test events with a speed ratio of 1:1 and of 1:3; Kominsky &amp; Scholl, 2020).”</p><disp-quote content-type="editor-comment"><p>The third experiment tests color (as a marker of object identity), and pits it against motion direction. The results demonstrate that adaptation to red-launching-green generates an adaptation effect for green-launching-red, provided they are moving in roughly the same direction, which provides a nice internal replication of Experiment 1 in addition to showing that the adaptation effect is not sensitive to object identity. This result forms an interesting contrast with the infant causal perception literature. Multiple papers (starting with Leslie &amp; Keeble, 1987) have found that 6-8-month-old infants are sensitive to reversals in causal roles exactly like the ones used in this experiment. The success of adaptation transfer suggests, very clearly, that this sensitivity is not based only on perceptual processing, or at least not on the same processing that we access with this adaptation procedure. It implies that infants may be going beyond the underlying perceptual processes and inferring genuine causal content. This is also not the first time the adaptation paradigm has diverged from infant findings: Kominsky &amp; Scholl (2020) found a divergence with the object speed differences as well, as infants categorize these events based on whether the speed ratio (agent:patient) is physically plausible (Kominsky et al., 2017), while the adaptation effect transfers from physically implausible events to physically plausible ones. This only goes to show that these adaptation effects don't exhaustively capture the mechanisms of early-emerging causal event representation.</p></disp-quote><p>We would like to thank the reviewer for highlighting the similarities (and differences) to the seminal study by Leslie and Keeble (1987). We included a discussion with respect to that paper in the revised manuscript. Indeed, that study showed a recovery from habituation to launches after reversal of the launching events. In their study, the reversal condition resulted in a change of two aspects, (1) motion direction and (2) a change of what color is linked to either cause (i.e., agent) or effect (i.e, patient). Our study, based on visual adaptation in adults, suggests that switching the two colors is not necessary for a recovery from the habituation, provided the motion direction is reversed. Importantly, the reversal of the motion direction only affected the perception of causality after adapting to launches (but not to slip events), which is consistent with Leslie and Keeble’s (1987) finding that the effect of a reversal is contingent on habituation/adaptation to a causal relationship (and is not observed for non-causal delayed launches). Based on our findings, we predict that switching colors without changing the event’s motion direction would not result in a recovery from habituation. Obviously, for infants, color may play a more important role for establishing an object identity than it does for adults, which could explain potential differences. We also agree with the reviewer’s point that the adaptation protocol might tap into different mechanisms than revealed by habituation studies in infants (e.g, Kominsky et al., 2017 vs. Kominsky &amp; Scholl, 2020).</p><p>We revised the manuscript accordingly when discussing the role of direction selectivity in our study:</p><p>“Habituation studies in six-months-old infants also demonstrated that the reversal of a launch resulted in a recovery from habituation to launches (while a non-causal control condition of delayed-launches did not; Leslie &amp; Keeble, 1987). In their study, the reversal of motion direction was accompanied by a reversal of the color assignment to the cause-effectrelationship. In contrast, our findings suggest, that in adults color does not play a major role in the detection of a launch. Future studies should further delineate similarities and differences obtained from adaptation studies in adults and habituation studies in children (e.g., Kominsky et al., 2017; Kominsky &amp; Scholl, 2020).”</p><disp-quote content-type="editor-comment"><p>One overarching point about the analyses to take into consideration: The authors use a Bayesian psychometric curve-fitting approach to estimate a point of subjective equality (PSE) in different blocks for each individual participant based on a model with strong priors about the shape of the function and its asymptotic endpoints, and this PSE is the primary DV across all of the studies. As discussed in Kominsky &amp; Scholl (2020), this approach has certain limitations, notably that it can generate nonsensical PSEs when confronted with relatively extreme response patterns. The authors mentioned that this happened once in Experiment 3 and that a participant had to be replaced. An alternate approach is simply to measure the proportion of 'pass' reports overall to determine if there is an adaptation effect. I don't think this alternate analysis strategy would greatly change the results of this particular experiment, but it is robust against this kind of self-selection for effects that fit in the bounds specified by the model, and may therefore be worth including in a supplemental section or as part of the repository to better capture the individual variability in this effect.</p></disp-quote><p>We largely agree with these points. Indeed, we adopted the non-parametric analysis for a recent series of experiments in which the psychometric curves were more variable (Ohl &amp; Rolfs, Vision Sciences Society Meeting 2024). In the present study, however, the model fits were very convincing. In Figures S1, S2 and S3 we show the model fits for each individual observer and condition on top of the mean proportion of launch reports. The inferential statistics based on the points of subjective equality, therefore, allowed us to report our findings very concisely.</p><disp-quote content-type="editor-comment"><p>In general, this paper adds further evidence for something like a 'launching' detector in the visual system, but beyond that, it specifies some interesting questions for future work about how exactly such a detector might function.</p></disp-quote><p>We thank the reviewer for this positive overall assessment.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>Generally, the paper is great. The questions I raised in the public review don't need to be answered at this time, but they're exciting directions for future work.</p></disp-quote><p>We would like to thank the reviewer for the encouraging comments and thoughtful ideas on how to improve the manuscript.</p><disp-quote content-type="editor-comment"><p>I would have liked to see a little more description of the model parameters in the text of the paper itself just so readers know what assumptions are going into the PSE estimation.</p></disp-quote><p>We followed the reviewer’s suggestion and added more information regarding the parameter space (i.e., ranges of possible parameters of the logistic model) that we used for obtaining the model fits.</p><p>Specifically, we added the following information in the manuscript:</p><p>“For model fitting, we constrained the range of possible estimates for each parameter of the logistic model. The lower asymptote for the proportion of reported launches was constrained to be in the range 0–0.75, and the upper asymptote in the range 0.25–1. The intercept of the logistic model was constrained to be in the range 1–15, and the slope was constrained to be in the range –20 to –1.”</p><p>The models provided very good fits as can be appreciated by the fits per individual and experimental condition which we provide in response to the public comments. Please note, that all data and analysis scripts are available at the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/x947m/">https://osf.io/x947m/</ext-link>).</p><disp-quote content-type="editor-comment"><p>I also have a recommendation about Figure 1b: Color-code &quot;Feature A&quot;, &quot;Feature B&quot;, and &quot;Feature C&quot; and match those colors with the object identity/speed/direction text. I get what the figure is trying to convey but to a naive reader there's a lot going on and it's hard to interpret.</p></disp-quote><p>We followed the reviewer’s suggestion and revised the visualization accordingly.</p><disp-quote content-type="editor-comment"><p>If you have space, figures showing the adaptation and corresponding test events for each experimental manipulation would also be great, particularly since the naming scheme of the conditions is (necessarily) not entirely consistent across experiments. It would be a lot of little figures, I know, but to people who haven't spent as long staring at these displays as we have, they're hard to envision based on description alone.</p></disp-quote><p>We followed the reviewer’s recommendation and added a visualization of the adaptor and the test events for the different experiments in Figure 2.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p></disp-quote><p>We thank the reviewer for their thoughtful comments, which we carefully addressed to improve the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Summary:</p><p>This paper presents evidence from three behavioral experiments that causal impressions of &quot;launching events&quot;, in which one object is perceived to cause another object to move, depending on motion direction-selective processing. Specifically, the work uses an adaptation paradigm (Rolfs et al., 2013), presenting repetitive patterns of events matching certain features to a single retinal location, then measuring subsequent perceptual reports of a test display in which the degree of overlap between two discs was varied, and participants could respond &quot;launch&quot; or &quot;pass&quot;. The three experiments report results of adapting to motion direction, motion speed, and &quot;object identity&quot;, and examine how the psychometric curves for causal reports shift in these conditions depending on the similarity of the adapter and test. While causality reports in the test display were selective for motion direction (Experiment 1), they were not selective for adapter-test speed differences (Experiment 2) nor for changes in object identity induced via color swap (Experiment 3). These results support the notion that causal perception is computed (in part) at relatively early stages of sensory processing, possibly even independently of or prior to computations of object identity.</p><p>Strengths:</p><p>The setup of the research question and hypotheses is exceptional. The experiments are carefully performed (appropriate equipment, and careful control of eye movements). The slip adaptor is a really nice control condition and effectively mitigates the need to control motion direction with a drifting grating or similar. Participants were measured with sufficient precision, and a power curve analysis was conducted to determine the sample size. Data analysis and statistical quantification are appropriate. Data and analysis code are shared on publication, in keeping with open science principles. The paper is concise and well-written.</p><p>Weaknesses:</p><p>The biggest uncertainty I have in interpreting the results is the relationship between the task and the assumption that the results tell us about causality impressions. The experimental logic assumes that &quot;pass&quot; reports are always non-causal impressions and &quot;launch&quot; reports are always causal impressions. This logic is inherited from Rolfs et al (2013) and Kominsky &amp; Scholl (2020), who assert rather than measure this. However, other evidence suggests that this assumption might not be solid (Bechlivanidis et al., 2019). Specifically, &quot;[our experiments] reveal strong causal impressions upon first encounter with collision-like sequences that the literature typically labels &quot;non-causal&quot;&quot; (Bechlivanidis et al., 2019) -- including a condition that is similar to the current &quot;pass&quot;. It is therefore possible that participants' &quot;pass&quot; reports could also involve causal experiences.</p></disp-quote><p>We agree with the reviewer that our study assumes that the launch-pass dichotomy can be mapped onto a dimension of causal to non-causal impressions. Please note that the choice for this launch-pass task format was intentional. We consider it an advantage that subjects do not have to report causal vs non-causal impressions directly, as it allows us to avoid the oftencriticized decision biases that come with asking participants about their causal impression (Joynson, 1971; for a discussion see Choi &amp; Scholl, 2006). This comes obviously at the cost that participants did not directly report their causal impression in our experiments. There is however evidence that increasing overlap between the discs monotonically decreases the causal impression when directly asking participants to report their causal impression (Scholl &amp; Nakayama, 2004). We believe, therefore, that the assumption of mapping between launchesto-passes and causal-to-noncausal is well-justified. At the same time, the expressed concern emphasizes the need to develop further, possibly implicit measure for causal impressions (see Völter &amp; Huber, 2021).</p><p>However, as pointed out by the reviewer, a recent paper demonstrated that on first encounter participants can have impressions in response to a pass event that are different from clearly non-causal impressions (Bechlivanidis et al., 2019). As demonstrated in the same paper, displaying a canonical launch decreased the impression of causality when seeing pass events in subsequent trials. In our study, participants completed an entire training session before running the main experiments. It is therefore reasonable to expect that participants observed passes as non-causal events given the presence of clear causal references. Nevertheless, we now acknowledge this concern directly in the revised manuscript.</p><p>We added the following paragraph to the discussion:</p><p>“In our study, we assessed causal perception by asking observers to report whether they observed a launch or a pass in events of varying ambiguity. This method assumes that launches and passes can be mapped onto a dimension that ranges from causal to non-causal impressions. It has been questioned whether pass events are a natural representative of noncausal events: Observers often report high impressions of causality upon first exposure to pass events, which then decreased after seeing a canonical launch (Bechlivanidis, Schlottmann, &amp; Lagnado, 2019). In our study, therefore, participants completed a separate session that included canonical launches before starting the main experiment.”</p><disp-quote content-type="editor-comment"><p>Furthermore, since the only report options are &quot;launch&quot; or &quot;pass&quot;, it is also possible that &quot;launch&quot; reports are not indications of &quot;I experienced a causal event&quot; but rather &quot;I did not experience a pass event&quot;. It seems possible to me that different adaptation transfer effects (e.g. selectivity to motion direction, speed, or color-swapping) change the way that participants interpret the task, or the uncertainty of their impression. For example, it could be that adaptation increases the likelihood of experiencing a &quot;pass&quot; event in a direction-selective manner, without changing causal impressions. Increases of &quot;pass&quot; impressions (or at least, uncertainty around what was experienced) would produce a leftward shift in the PSE as reported in Experiment 1, but this does not necessarily mean that experiences of causal events changed. Thus, changes in the PSEs between the conditions in the different experiments may not directly reflect changes in causal impressions. I would like the authors to clarify the extent to which these concerns call their conclusions into question.</p></disp-quote><p>Indeed, PSE shifts are subject to cognitive influences and can even be voluntarily shifted (Morgan et al., 2012). We believe that decision biases (e.g., reporting the presence of launch before adaptation vs. reporting the absence of a pass after the adaptation) are unlikely to explain the high specificity of aftereffects observed in the current study. While such aftereffects are very typical of visual processing (Webster, 2015), it is unclear how a mechanism that increase the likelihood of perceiving a pass could account for the retinotopy of adaptation to launches (Rolfs et al., 2013) or the recently reported selective transfer of adaptation for only some causal categories (Kominsky et al., 2020). The latter authors revealed a transfer of adaptation from triggering to launching, but not from entraining events to launching. Based on these arguments, we decided to not include this point in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Leaving these concerns aside, I am also left wondering about the functional significance of these specialised mechanisms. Why would direction matter but speed and object identity not? Surely object identity, in particular, should be relevant to real-world interpretations and inputs of these visual routines? Is color simply too weak an identity?</p></disp-quote><p>We agree that it would be beneficial to have mechanisms in place that are specific for certain object identities. Overall, our results fit very well to established claims that only spatiotemporal parameters mediate the perception of causality (Michotte, 1963; Leslie, 1984; Scholl &amp; Tremoulet, 2000). We have now explicitly listed these references again in the revised manuscript. It is important to note, that an understanding of a causal relation could suffice to track identity information based purely on spatiotemporal contingencies, neglecting distinguishing surface features.</p><p>We revised the manuscript and state:</p><p>“Our findings therefore provide additional support for the claim that an event’s spatiotemporal parameters mediate the perception of causality (Michotte, 1963; Leslie, 1984; Scholl &amp; Tremoulet, 2000).”</p><p>Moreover, we think our findings of directional selectivity have functional relevance. First, direction-selective detection of collisions allows for an adaptation that occurs separately for each direction. That means that the visual system can calibrate these visual routines for detecting causal interactions in response to real-world statistics that reflect differences in directions. For instance, due to gravity, objects will simply fall to the ground. Causal relation such as launches are likely to be more frequent in horizontal directions, along a stable ground. Second, we think that causal visual events are action-relevant, that is, acting on (potentially) causal events promises an advantage (e.g., avoiding a collision, or quickly catching an object that has been pushed away). The faster we can detect such causal interactions, the faster we can react to them. Direction-selective motion signals are available in the first stages of visual processing. Visual routines that are based on these direction-selective motion signals promise to enable such fast computations. Please note, however, that while our present findings demonstrate direction-selectivity, they do not pinpoint where exactly that visual routine is located. It is quite possible that the visual routine is located higher up in the visual system, relying on a direction-selective population response as input.</p><p>We added these points to the discussion of the functional relevance:</p><p>“We suggest that at least two functional benefits result from a specialized visual routine for detecting causality. First, a direction-selective detection of launches allows adaptation to occur separately for each direction. That means that the visual system can automatically calibrate the sensitivity of these visual routines in response to real-world statistics. For instance, while falling objects drop vertically towards the ground, causal relations such as launches are common in horizontal directions moving along a stable ground. Second, we think that causal visual events are action-relevant, and the faster we can detect such causal interactions, the faster we can react to them. Direction-selective motion signals are available very early on in the visual system. Visual routines that are based on these direction-selective motion signals may enable faster detection. While our present findings demonstrate direction-selectivity, they do not pinpoint where exactly that visual routine is located. It is possible that the visual routine is located higher up in the visual system (or distributed across multiple levels), relying on a direction-selective population response as input.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>- The concept of &quot;visual routines&quot; is used without introduction; for a general-interest audience it might be good to include a definition and reference(s) (e.g. Ullman.).</p></disp-quote><p>Thank you very much for highlighting that point. We have chosen the term visual routine to emphasize that we locate the part of the mechanism that is affected by the adaptation in our experiments in the visual system, but at the same time it leaves space regarding the extent to which the mechanism further involves mid- and higher-level processes. The term thus has a clear reference to a visual routine by Ullman (1987). We have now addressed what we mean by visual routine, and we also included the reference in the revised manuscript.</p><p>We add the following footnote to the introduction:</p><p>“We use the term visual routine here to highlight that our adaptation experiments can reveal a causality detection mechanism that resides in the visual system. At the same time, calling it a routine emphasizes similarities with a local, semi-independent operation (e.g., the recognition of familiar motion patterns; see also Ullman, 1987; Cavanagh, Labianca, &amp; Thornton, 2001) that can engage mid- and higher-level processes (e.g., during causal capture, Scholl &amp; Nakayama, 2002; or multisensory integration, Körding et al., 2007).”</p><disp-quote content-type="editor-comment"><p>- I would appreciate slightly more description of the phenomenology of the WW adaptors: is this Michotte's &quot;entraining&quot; event? Does it look like one disc shunts the other?</p></disp-quote><p>The stimulus differs from Michotte's entrainment event in both spatiotemporal parameters and phenomenology. We added videos for the launch, pass and slip events as Supplementary Material.</p><p>Moreover, we described the slip event in the methods section:</p><p>“In two additional sessions, we presented slip events as adaptors to control that the adaptation was specific for the impression of causality in the launching events. Slip events are designed to match the launching events in as many physical properties as possible while producing a very different, non-causal phenomenology. In slip events, the first peripheral disc also moves towards a stationary disc. In contrast to launching events, however, the first disc passes the stationary disc and stops only when it is adjacent to the opposite edge of the stationary disc. While slip events do not elicit a causal impression, they have the same number of objects and motion onsets, the same motion direction and speed, as well as the same spatial area of the event as launches.”</p><p>In the revised manuscript, we added also more information on the slip event in the beginning of the results section. Importantly, the stimulus typically produces the impression of two independent movements and thus serves as a non-causal control condition in our study. Only anecdotally, some observers (not involved in this study) who saw the stimulus spontaneously described their phenomenology of seeing a slip event as a double step or a discus throw.</p><p>We added the following description to the results section:</p><p>“Moreover, we compared the visual adaptation to launches to a (non-causal) control condition in which we presented slip events as adaptor. In a slip event, the initially moving disc passes completely over the stationary disc, stops immediately on the other side, and then the initially stationary disc begins to move in the same direction without delay. Thus, the two movements are presented consecutively without a temporal gap. This stimulus typically produces the impression of two independent (non-causal) movements.”</p><disp-quote content-type="editor-comment"><p>- In general more illustrations of the different conditions (similar to Figure 1c but for the different experimental conditions and adaptors) might be helpful for skim readers.</p></disp-quote><p>We followed the reviewer’s recommendation and added a visualization of the adaptor and the test events for the different experiments in Figure 2.</p><disp-quote content-type="editor-comment"><p>- Were the luminances of the red and green balls in experiment 3 matched? Were participants checked for color anomalous vision?</p></disp-quote><p>Yes, we checked for color anomalous vision using the color test Tafeln zur Prüfung des Farbensinnes/Farbensehens (Kuchenbecker &amp; Broschmann, 2016). We added that information to the manuscript. The red and green discs were not matched for luminance. We measured the luminance after the experiment (21 cd/m<sup>2</sup> for the green disc and 6 cd/m<sup>2</sup> for the red disc). Please note, that the differences in luminance should not pose a problem for the interpretation of the results, as we see a transfer of the adaptation across the two different colors.</p><p>We added the following information to the manuscript:</p><p>“The red and green discs were not matched for luminance. Measurements obtained after the experiments yielded a luminance of 21 cd/m<sup>2</sup> for the green disc and 6 cd/m<sup>2</sup> for the red disc.”</p><p>“All observers had normal or corrected-to-normal vision and color vision as assessed using the color test Tafeln zur Prüfung des Farbensinnes/Farbensehens (Kuchenbecker &amp; Broschmann, 2016).”</p><disp-quote content-type="editor-comment"><p>- Relationship of this work to the paper by Arnold et al., (2015). That paper suggested that some effects of adaptation of launching events could be explained by an adaptation of object shape, not by causality per se. It is superficially difficult to see how one could explain the present results from the perspective of object &quot;squishiness&quot; -- why would this be direction selective? In other words, the present results taken at face value call the &quot;squishiness&quot; explanation into question. The authors could consider an explanation to reconcile these findings in their discussion.</p></disp-quote><p>Indeed, the paper by Arnold and colleagues (2014) suggested that a contact-launch adaptor could lead to a squishiness aftereffect—arguing that the object elasticity changed in response to the adaptation. Importantly, the same study found an object-centered adaptation effect rather than a retinotopic adaptation effect. However, the retinotopic nature of the negative aftereffect as used in our study has been repeatedly replicated (for instance Kominsky &amp; Scholl, 2020). Thus, the divergent results of Arnold and colleagues may have resulted from differences in the task (i.e., observers had to judge whether they perceived a soft vs. hard bounce), or the stimuli (i.e., bounces of a disc and a wedge, and the discs moving on a circular trajectory). It would be important to replicate these results first and then determine whether their squishiness effect would be direction-selective as well. We now acknowledge the study by Arnold and colleagues in the discussion:</p><p>“The adaptation of causality is spatially specific to the retinotopic coordinates of the adapting stimulus (Kominsky &amp; Scholl, 2020; Rolfs et al., 2013; for an object-centered elasiticity aftereffect using a related stimulus on a circular motion path, see Arnold et al., 2015), suggesting that the detection of causal interactions is implemented locally in visual space.”</p><disp-quote content-type="editor-comment"><p>- Line 32: &quot;showing that a specialized visual routine for launching events exists even within separate motion direction channels&quot;. This doesn't necessarily mean the routine is within each separate direction channel, only that the output of the mechanism depends on the population response over motion direction. The critical motion computation could be quite high level -- e.g. global pattern motion in MST. Please clarify the claim.</p></disp-quote><p>We agree with the reviewer, that it is also possible that critical parts of the visual routine could simply use the aggregated population response over motion direction at higher-levels of processing. We acknowledge this possibility in the discussion of the functional relevance of the proposed mechanism and when suggesting that a distributed brain network may contribute to the perception of causality.</p><p>We would like to highlight the following two revised paragraphs.</p><p>“[…] Second, we think that causal visual events are action-relevant, and the faster we can detect such causal interactions, the faster we can react to them. Direction-selective motion signals are available very early on in the visual system. Visual routines that are based on these direction-selective motion signals may enable faster detection. While our present findings demonstrate direction-selectivity, they do not pinpoint where exactly that visual routine is located. It is possible that the visual routine is located higher up in the visual system (or distributed across multiple levels), relying on a direction-selective population response as input.”</p><p>Moreover, when discussing the neurophysiological literature we write:</p><p>“Interestingly, single cell recordings in area F5 of the primate brain revealed that motor areas are contributing to the perception of causality (Caggiano et al., 2016; Rolfs, 2016), emphasizing the distributed nature of the computations underlying causal interactions. This finding also stresses that the detection, and the prediction, of causality is essential for processes outside purely sensory systems (e.g., for understanding other’s actions, for navigating, and for avoiding collisions).”</p><disp-quote content-type="editor-comment"><p>- p. 10 line 30: typo &quot;particual&quot;.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>- p. 10 line 37: &quot;This findings rules out (...)&quot; should be singular &quot;This finding rules out (...)&quot;.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>- Spelling error throughout: &quot;underly&quot; should be &quot;underlie&quot;.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>- p.11 line 29: &quot;emerges fast and automatic&quot; should be &quot;automatically&quot;.</p></disp-quote><p>Done.</p></body></sub-article></article>