<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">95160</article-id><article-id pub-id-type="doi">10.7554/eLife.95160</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95160.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Sensory-memory interactions via modular structure explain errors in visual working memory</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Jun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2484-2494</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Hanqi</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Lim</surname><given-names>Sukbin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9936-5293</contrib-id><email>sukbin.lim@nyu.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cve4549</institution-id><institution>Weiyang College, Tsinghua University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution>Neural Science</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vpsdb40</institution-id><institution>NYU-ECNU Institute of Brain and Cognitive Science</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>UT Austin</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Interdisciplinary Graduate Program in Quantitative Biosciences, Georgia Institute of Technology, Atlanta, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>School of Mathematics, Georgia Institute of Technology, Atlanta, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>10</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP95160</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-01-03"><day>03</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-01-03"><day>03</day><month>01</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.09.566396"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-04-04"><day>04</day><month>04</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95160.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-21"><day>21</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95160.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-11"><day>11</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95160.3"/></event></pub-history><permissions><copyright-statement>© 2024, Yang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Yang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-95160-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-95160-figures-v1.pdf"/><abstract><p>Errors in stimulus estimation reveal how stimulus representation changes during cognitive processes. Repulsive bias and minimum variance observed near cardinal axes are well-known error patterns typically associated with visual orientation perception. Recent experiments suggest that these errors continuously evolve during working memory, posing a challenge that neither static sensory models nor traditional memory models can address. Here, we demonstrate that these evolving errors, maintaining characteristic shapes, require network interaction between two distinct modules. Each module fulfills efficient sensory encoding and memory maintenance, which cannot be achieved simultaneously in a single-module network. The sensory module exhibits heterogeneous tuning with strong inhibitory modulation reflecting natural orientation statistics. While the memory module, operating alone, supports homogeneous representation via continuous attractor dynamics, the fully connected network forms discrete attractors with moderate drift speed and nonuniform diffusion processes. Together, our work underscores the significance of sensory-memory interaction in continuously shaping stimulus representation during working memory.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>working memory</kwd><kwd>sensory perception</kwd><kwd>efficient coding</kwd><kwd>attractor dynamics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology of the People's Republic of China</institution></institution-wrap></funding-source><award-id>STI2030-Major Projects No.2021ZD0203700</award-id><principal-award-recipient><name><surname>Lim</surname><given-names>Sukbin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology of the People's Republic of China</institution></institution-wrap></funding-source><award-id>2021ZD0203705</award-id><principal-award-recipient><name><surname>Lim</surname><given-names>Sukbin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008410</institution-id><institution>NYU Shanghai</institution></institution-wrap></funding-source><award-id>Summer Undergraduate Research Program (SURP)</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Jun</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Two-module networks, combining sensory efficient coding and attractor dynamics for memory maintenance, effectively explain working memory error patterns, highlighting the role of sensory-memory interactions in continuously shaping memory representations.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain does not faithfully represent external stimuli. Even for low-level features like orientation, spatial frequency, or color of visual stimuli, their internal representations are thought to be modified by a range of cognitive processes, including perception, memory, and decision (<xref ref-type="bibr" rid="bib18">Geisler, 2008</xref>; <xref ref-type="bibr" rid="bib55">Webster, 2015</xref>; <xref ref-type="bibr" rid="bib4">Bays et al., 2022</xref>). Experimental studies quantified such modification by analyzing behavior data or decoding neural activities. For instance, biases of errors, the systematic deviation from the original stimuli, observed in estimation tasks have been used as indirect evidence to infer changes in the internal representations of stimuli (<xref ref-type="bibr" rid="bib57">Wei and Stocker, 2017</xref>).</p><p>One important source of biases is adaptation to environmental statistics, such as the nonuniform stimulus distribution found in nature or the limited range in specific settings. Cardinal repulsion, which refers to the systematic shift away from the horizontal and vertical orientations observed in many perceptual tasks, is one of the examples (<xref ref-type="bibr" rid="bib12">de Gardelle et al., 2010</xref>). Theoretical works suggest that such a bias pattern reflects the prevalence of the cardinal orientations in natural scenes (<xref ref-type="bibr" rid="bib20">Girshick et al., 2011</xref>). Similarly, the variance of errors for orientation stimuli was found to be inversely proportional to the stimulus statistics, minimum at cardinal and maximum at oblique orientations (<xref ref-type="bibr" rid="bib50">van Bergen et al., 2015</xref>). It was postulated that the dependence of biases and variance of errors on natural statistics results from sensory encoding optimized to enhance precision around the most common stimuli (<xref ref-type="bibr" rid="bib17">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib56">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="bib57">Wei and Stocker, 2017</xref>).</p><p>On the other hand, there is a growing body of evidence indicating that error patterns are not solely influenced by sensory encoding but are also shaped by memory processes. In delayed estimation tasks, where participants are presented with stimuli followed by a delay period during which they rely on their working memory for estimation, it has been observed that representations of orientation or color stimuli undergo gradual and continuous modifications throughout the delay period (<xref ref-type="bibr" rid="bib37">Panichello et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Bae, 2021</xref>; <xref ref-type="bibr" rid="bib22">Gu et al., 2023</xref>). Such dynamic error patterns are inconsistent with sensory encoding models, most of which only establish a static relationship between stimuli and internal representations.</p><p>Traditional working memory models are not suitable either. Most of them are constructed to faithfully maintain information about stimuli during the delay period, and thus, the memory representation has a similar geometry as that of the stimuli (<xref ref-type="bibr" rid="bib54">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib29">Khona and Fiete, 2022</xref>). For continuous stimuli such as orientation, location, direction, or color, all stimuli are equally maintained in ring-like memory activities, predicting no biases (<xref ref-type="bibr" rid="bib63">Zhang, 1996</xref>; <xref ref-type="bibr" rid="bib10">Compte et al., 2000</xref>; <xref ref-type="bibr" rid="bib6">Burak and Fiete, 2009</xref>).</p><p>How can we explain error patterns in working memory tasks that are similar to those observed in perception tasks? Here, we claim that not a single-module but a two-module network with recursive interaction is required. Each module has a distinct role – sensory encoding and memory maintenance. To illustrate this, we use orientation stimuli and examine how their representations change during the delayed estimation tasks. We employ two approaches to find solutions for generating correct error patterns. The first extends previously suggested sensory encoding models, while the second modifies low-dimensional memory models based on attractor dynamics. These approaches are integrated into the network models, which link network connectivity to neuronal tuning properties and behavioral error patterns and reveal the attractor dynamics through low-dimensional projection. Our results show that the sensory-memory interacting networks outperform single-module networks with better control over the shapes and evolution of dynamic error patterns. Furthermore, our network models emphasize the importance of inhibitory tuning in sensory circuits for generating correct error patterns under typical associative learning of natural statistics. Finally, we provide testable predictions regarding the effect of perturbations in sensory-memory interactions on error patterns in delayed estimation tasks.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Low-dimensional attractor models</title><p>In natural images, cardinal orientations are the most prevalent (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Error patterns in estimation tasks show dependence on such natural statistics, such as biases away from cardinal orientations where the variance of errors is nonetheless minimal (<xref ref-type="fig" rid="fig1">Figure 1B and C</xref>). In delayed estimation tasks, such a bias pattern is consolidated in time (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Also, experimental data suggested that estimation errors increase with a longer delay (<xref ref-type="bibr" rid="bib59">Wimmer et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Schneegans and Bays, 2018</xref>), while the precision is still highest at cardinal orientations (<xref ref-type="bibr" rid="bib51">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib3">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib50">van Bergen et al., 2015</xref>). Thus, we assumed that the variance of errors increases as keeping its characteristic shape (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). To explain these errors across orientations and over time, we first explored the underlying working memory mechanism. We considered low-dimensional attractor models with input noise that describe the drift and diffusion of the memory states. Here, we show that two prominent classes of previously suggested models are inconsistent with experimental observations and examine what modification to the models is required.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Error patterns of orientation stimuli in delayed estimation tasks and low-dimensional attractor models.</title><p>(<bold>A–C</bold>) Characteristic patterns of natural statistics of orientation stimuli <inline-formula><mml:math id="inf1"><mml:mi>θ</mml:mi></mml:math></inline-formula> (<bold>A</bold>), bias (<bold>B</bold>), and standard deviation (SD; <bold>C</bold>) during the delay period observed experimentally. Cardinal orientations are predominant in natural images (<bold>A</bold>). Bias and SD increase during the delay period, keeping patterns of repulsive bias (<bold>B</bold>) and minimum variance (<bold>C</bold>) around cardinal orientations. These characteristic patterns are visualized using trigonometric functions, and the range is normalized by their maximum values. Red vertical lines correspond to representative cardinal and oblique orientations, and with a periodicity of the error patterns, we only show the gray-shaded range in the remaining panels. (<bold>D–L</bold>) Comparison of different attractor models. (<bold>D–F</bold>) Continuous attractors with constant noise. Energy potential is flat (<bold>D</bold>), resulting in no bias (<bold>E</bold>) and uniform SD with uniform noise (<bold>F</bold>). (<bold>G–L</bold>) Discrete attractors with constant (<bold>G–I</bold>) and nonuniform noise (<bold>J–L</bold>). The discrete attractor models have potential hills and wells at cardinal and oblique orientations, respectively (<bold>G, J</bold>). While the bias patterns depend only on the energy landscape (<bold>H, K</bold>), SD representing variability also depends on noise (<bold>I, L</bold>). For the correct SD pattern (<bold>L</bold>), uneven noise with its maxima at the obliques (<bold>J</bold>) is required. Bias and SD patterns in the attractor models were obtained by running one-dimensional drift-diffusion models (see Methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig1-v1.tif"/></fig><p>The most widely accepted model for working memory of orientation stimuli has continuous attractor dynamics, which assumes that all orientations are equally encoded and maintained (<xref ref-type="fig" rid="fig1">Figure 1D–F</xref>). Each attractor corresponds to the memory state for different stimuli and forms a continuous ring following the geometry of orientation stimuli. The dynamics along continuous attractors are conceptually represented as movement along a flat energy landscape (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Without external input, there is no systematic shift of mean activity, i.e., no drift during the delay period (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Also, under the assumption of equal influence of noise for all orientations, the variance of errors is spatially flat with constant diffusion along the ring, while the overall magnitude increases over time due to the accumulation of noise (<xref ref-type="fig" rid="fig1">Figure 1F</xref>).</p><p>While such continuous attractor models have been considered suitable for memory storage of continuous stimuli, they cannot capture drift dynamics observed during the delay period. Instead, discrete attractor models with uneven energy landscapes have been suggested with the energy wells corresponding to discrete attractors (<xref ref-type="fig" rid="fig1">Figure 1G–I</xref>). As evolution toward a few discrete attractors creates drift dynamics, the bias increases during the delay (<xref ref-type="fig" rid="fig1">Figure 1H</xref>). Also, discrete attractor models naturally produce nonuniform variance patterns. Even with constant noise along the ring, variance becomes minimum/maximum at the attractors/repellers due to the drift dynamics (<xref ref-type="fig" rid="fig1">Figure 1I</xref>). However, discrete attractor models with constant noise yield inconsistent results when inferring the locus of attractors from the bias and variance patterns observed in the data. Cardinal orientations should be the repeller to account for cardinal repulsion. In contrast, the minimum variance observed at the cardinal orientations suggests they should be the attractors.</p><p>How can such inconsistency be resolved? One possible solution is discrete attractor models with nonuniform noise amplitude (<xref ref-type="fig" rid="fig1">Figure 1J</xref>). Let’s consider that attractors are formed at oblique orientations to generate correct bias patterns (<xref ref-type="fig" rid="fig1">Figure 1K</xref>). Additionally, we assumed that noise has the highest amplitude at the obliques. When the difference in the noise amplitude is large enough to overcome the attraction toward the obliques, the models can produce correct variance patterns, maximum at the obliques and minimum at cardinal orientations (<xref ref-type="fig" rid="fig1">Figure 1L</xref>). In sum, unlike two prominent memory models, continuous attractors or discrete attractors with constant noise, discrete attractors with maximum noise at the obliques could reproduce experimentally observed error patterns of orientation stimuli. Note that these attractor models often simplify the full network dynamics. Namely, the drift and diffusion terms are derived by projecting network dynamics onto low-dimensional memory states (<xref ref-type="bibr" rid="bib7">Burak and Fiete, 2012</xref>; <xref ref-type="bibr" rid="bib11">Darshan and Rivkind, 2022</xref>). Thus, it is still in question whether there exist memory networks that can implement attractor dynamics with correct drift and diffusion terms.</p></sec><sec id="s2-2"><title>Bayesian sensory model and extension</title><p>Before exploring full memory network models, we note that previous theoretical works for sensory processing suggested that Bayesian inference with efficient coding could generate the repulsive bias and the lowest variance at cardinal orientations (<xref ref-type="bibr" rid="bib56">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="bib57">Wei and Stocker, 2017</xref>). Efficient coding theory suggests the sensory system should enhance the sensitivity around more common stimuli. For orientation stimuli, precision should be highest around cardinal directions, which could be achieved by sharpening the likelihood functions. Equipped with Bayesian optimal readout, such a sensory system could reproduce correct error patterns observed in perceptual tasks for various visual stimuli, including orientations (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Extension of Bayesian sensory models.</title><p>(<bold>A</bold>) Schematics of extension to memory processing. We adapted the previous Bayesian models (<xref ref-type="bibr" rid="bib56">Wei and Stocker, 2015</xref>) for sensory encoding where <inline-formula><mml:math id="inf2"><mml:mi>θ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the input and output of sensory modules, respectively. We added a memory module where it maintains <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with the addition of memory noise <inline-formula><mml:math id="inf5"><mml:mi>ξ</mml:mi></mml:math></inline-formula>. The output of the memory module, <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, is fed back to the sensory module as the input for the next iteration. (<bold>B</bold>) Illustration of the first iteration of sensory-memory interaction. Prior distribution follows the natural statistics (top), resulting in a sharper likelihood function near cardinal orientations (middle). Combining prior and likelihood functions leads to the posterior distribution of decoded <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (light colors at the bottom), which is broadened with the addition of memory noise (dark colors at the bottom). Different curves correspond to different initial <inline-formula><mml:math id="inf8"><mml:mi>θ</mml:mi></mml:math></inline-formula>. (<bold>C</bold>) Bias (top) and SD (bottom) patterns obtained from decoded <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the first, second, and third iterations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig2-v1.tif"/></fig><p>However, such models only account for the relationship between external and perceived stimuli during sensory processing, resulting in static error patterns. Here, we extended the framework so that the system can maintain information about the stimulus after its offset while bias and variance of errors grow in time (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We added a memory stage to Bayesian sensory models such that the memory stage receives the output of the sensory stage and returns it as the input after the maintenance. For instance, let’s denote the external orientation stimulus given during the stimulus period as <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. The sensory stage receives <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> as input and generates the perceived orientation, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which varies from trial to trial with sensory noise (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Through the memory stage, <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is returned as the input to the sensory stage for the next iteration with the addition of memory noise <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>Such a recursive process mimics interactions between sensory and memory systems where the sensory system implements efficient coding and Bayesian inference, and the memory system faithfully maintains information. As the recursive process iterates, the distribution of the internal representation of orientation broadens due to the accumulation of noise from the sensory and memory systems. This leads to an increase in bias and variance at each step while keeping their characteristic shapes (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Thus, recurrent interaction between sensory and memory systems during the delay period, each of which meets different demands, successfully reproduces correct error patterns observed in memory tasks.</p></sec><sec id="s2-3"><title>Network models with sensory and memory modules</title><p>Next, we construct network models capturing the sensory-memory interactions formalized under the Bayesian framework. We consider two-module networks where each module corresponds to the sensory and memory systems. To generate orientation selectivity, both modules have a columnar architecture where neurons in each column have a similar preference for orientation (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). However, their connectivity structures are different (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The memory module in isolation resembles the traditional ring attractor network with a strong and homogeneous recurrent connection. This enables the memory module in isolation to maintain information about all orientations equally during the delay period (<xref ref-type="fig" rid="fig3">Figure 3B–F</xref>, right). Conversely, the recurrent connectivity strengths in the sensory module are relatively weak, such that without connection to the memory module, the activities during the delay period decay back to the baseline levels (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left). Furthermore, the connectivity strengths across columns are heterogeneous, particularly stronger at the obliques. As a result, the tuning curves near cardinal orientations can be sharper and denser, consistent with experimental observations showing a larger number of cardinally tuned neurons (<xref ref-type="bibr" rid="bib34">Li et al., 2003</xref>; <xref ref-type="bibr" rid="bib46">Shen et al., 2014</xref>) and their narrower tuning (<xref ref-type="bibr" rid="bib34">Li et al., 2003</xref>; <xref ref-type="bibr" rid="bib30">Kreile et al., 2011</xref>; <xref ref-type="fig" rid="fig3">Figure 3C–F</xref>, left). Different response activities of the two modules in isolation are demonstrated in their response manifolds as more dispersed representations around cardinal orientations in the sensory module, compared to the ring-like geometry of the memory module (<xref ref-type="fig" rid="fig3">Figure 3F</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Network models of sensory and memory circuits in isolation, implementing efficient coding and ring attractor dynamics, respectively.</title><p>(<bold>A</bold>) Schematics of columnar architecture for orientation selectivity. Neurons in the same column have similar preferred orientations, and recurrent connections are a combination of local excitation and global inhibition, represented as triangles and circles, respectively. (<bold>B–F</bold>) Connectivity and tuning properties of the sensory network (left column) and memory network (right column). (<bold>B</bold>) Example connectivity strengths. We indexed neurons by <inline-formula><mml:math id="inf15"><mml:mi>ψ</mml:mi></mml:math></inline-formula> ranging uniformly from 0° to 180°. The connectivity strengths depend only on <inline-formula><mml:math id="inf16"><mml:mi>ψ</mml:mi></mml:math></inline-formula>’s of the presynaptic and postsynaptic neurons. Each curve shows the connectivity strengths from presynaptic neuron <inline-formula><mml:math id="inf17"><mml:mi>ψ</mml:mi></mml:math></inline-formula> to an example postsynaptic neuron. Unlike the homogeneous connectivity in the memory network (right), the sensory connectivity is heterogeneous, and its degree is denoted by <inline-formula><mml:math id="inf18"><mml:mi>α</mml:mi></mml:math></inline-formula>. (<bold>C</bold>) Heterogeneous tuning curves for different stimulus <inline-formula><mml:math id="inf19"><mml:mi>θ</mml:mi></mml:math></inline-formula> in the sensory network in the stimulus period (left) and homogeneous ones in the memory network in the delay period (right). The memory network can sustain persistent activity in isolation, while the sensory network cannot. (<bold>D</bold>) Histograms of the preferred orientations. We measured the maximum of the tuning curve of each neuron, denoted as <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (Methods). The heterogeneous sensory network has more cardinally tuned neurons. (<bold>E</bold>) Widths of tuning curves measured at the half maximum of the tuning curves (Methods). The sensory tuning curves sharpen around cardinal orientations. Each neuron is labeled with its index <inline-formula><mml:math id="inf21"><mml:mi>ψ</mml:mi></mml:math></inline-formula> as in (<bold>B</bold>). (<bold>F</bold>) Neural manifolds projected onto the first two principal components of activities during the stimulus period (left) and during the delay period (right). The neural manifold of the sensory network resembles a curved ellipsoid, while the manifold corresponding to the homogeneous memory network is a perfect ring.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig3-v1.tif"/></fig><p>For sensory-memory interacting networks, we connected the two modules with intermodule connections set to be stronger between neurons with similar orientation selectivity (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Activity profiles in both modules follow that of the sensory module – heterogeneous with narrower and denser tuning curves around cardinal orientations, leading to higher sensitivity (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Such activity pattern is maintained even during the delay period when recurrent connections in the memory module support activities of both sensory and memory modules (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, right). Note that while sensory activities convey stimulus information during the delay period, their overall firing rates are much lower than those during the stimulus period with weak interconnection strengths. Such low firing rates may lead to both positive and negative evidence of sustained activity in early sensory areas (<xref ref-type="bibr" rid="bib33">Leavitt et al., 2017</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Network model with interacting sensory and memory modules generates correct error patterns in delayed estimation tasks.</title><p>(<bold>A</bold>) Schematic of two-module architecture. The sensory and memory modules are connected via feedforward and feedback connectivity to form a closed loop. The sensory module receives external input with orientation <inline-formula><mml:math id="inf22"><mml:mi>θ</mml:mi></mml:math></inline-formula> while internal representation is decoded from the memory module, denoted as <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>B</bold>) Tuning curves of sensory (upper panels) and memory (lower panels) modules at the end of the stimulus epoch (i.e. the beginning of the delay epoch; left panels) and during the delay period (right panels). Note that while both modules can sustain persistent activity in the delay period, the firing rates of the sensory module are significantly lower than those in the stimulus period (upper right). (<bold>C–E</bold>) Bias (<bold>C</bold>), standard deviation (SD; <bold>D</bold>), and Fisher information (FI; <bold>E</bold>) patterns. Error patterns evaluated at 1, 2.5, and 4 s into the delay are consistent with the characteristic patterns observed experimentally in delayed estimation tasks (<xref ref-type="fig" rid="fig1">Figure 1A–C</xref>). However, the low SD right after the stimulus offset in (<bold>D</bold>) deviates from error patterns seen in perception tasks (see Discussion). While FI decays due to noise accumulation, it is largest around cardinal orientations, corresponding to a smaller discrimination threshold (<bold>E</bold>). In (<bold>C</bold>) and (<bold>D</bold>), shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Bias (<bold>A</bold>) and SD (<bold>B</bold>) patterns decoded from activities of sensory module.</title><p>All parameters are the same as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Dynamics of bias and tuning properties of sensory-memory interacting network models.</title><p>(<bold>A</bold>) Maximum firing rate of <inline-formula><mml:math id="inf24"><mml:mi>ψ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>22.2</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for all stimulus orientations. The vertical gray line represents the end of the stimulus presentation. Both sensory and memory modules show lower but sustained activities during the delay period. (<bold>B</bold>) Bias evolution to input orientation <inline-formula><mml:math id="inf25"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>18</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. The bias increases both in the stimulus and delay periods, while its increasing speed is reduced during the delay period. (<bold>C</bold>) Tuning width indices (WI) measuring the asymmetry of tuning widths at cardinal and oblique orientations (Methods). WI also increases in the whole process, indicating the tuning curves of the neural population become more heterogeneous. All parameters are the same as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig4-figsupp2-v1.tif"/></fig></fig-group><p>When the internal representation of the orientation stimulus is read from the memory module using a population vector decoder mimicking Bayesian optimal readout (<xref ref-type="bibr" rid="bib15">Fischer, 2010</xref>), the sensory-memory interacting network exhibits repulsive bias and minimum variance at cardinal orientations, inheriting from efficient sensory coding (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). Similar error patterns were observed when decoded from activities of the sensory module (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Such bias increases during the delay period with increasing asymmetry of tuning widths despite lower firing rates than the stimulus period (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). At the same time, errors gradually increase due to noise accumulation in time, as in typical memory networks (<xref ref-type="bibr" rid="bib10">Compte et al., 2000</xref>; <xref ref-type="bibr" rid="bib7">Burak and Fiete, 2012</xref>). Note that the variance of errors is negligible during stimulus presentation when the external input overwhelms internal noise, which may not fully account for the variability observed during perception tasks (see Discussion). We obtained Fisher information measuring sensitivity at each orientation from the neural responses (see Methods). Opposite to the variance of errors, Fisher information is highest at cardinal orientations, while it decreases during the delay period (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Thus, the sensory-memory interacting network model that mechanistically embodies the extension of the Bayesian sensory model correctly reproduces the error patterns observed in delayed estimation tasks.</p></sec><sec id="s2-4"><title>Analysis of low-dimensional memory states</title><p>To further understand the mechanisms of generating the correct error patterns in sensory-memory interacting networks, we analyzed the network dynamics during the delay period. For this, we identified the low-dimensional manifold that has slow dynamics during the delay period, which corresponds to the memory states (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We projected the dynamics along this manifold to obtain the drift and diffusion terms (<xref ref-type="fig" rid="fig5">Figure 5A–C</xref>; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). The drift term shows similar patterns to cardinal repulsion (<xref ref-type="fig" rid="fig5">Figure 5B and E</xref>). Integrating this drift for orientation yields the energy function, which is minimum at the obliques (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). This suggests that the network implements discrete attractor dynamics with attractors formed at the obliques. The diffusion term is also uneven – the noise amplitude is maximum at the obliques so that despite attraction toward them, the variance of errors can be maximum (<xref ref-type="fig" rid="fig5">Figure 5C and F</xref>). Note that while we use Poisson noise in all units to replicate neuronal spike variability, the pattern of noise coefficients remains unchanged even with constant Gaussian noise (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). This lower variance near cardinal orientations arises from more dispersed representations of stimuli, as the noise coefficient is inversely proportional to the distance between stimulus representations (<xref ref-type="disp-formula" rid="equ21">Equation 21</xref>). Thus, the nonuniform characteristics of both drift and diffusion processes stem from the heterogeneous connections within the sensory module and align with the solution identified in low-dimensional memory models (<xref ref-type="fig" rid="fig1">Figure 1J–L</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Low-dimensional dynamics along memory manifold and their dependence on heterogeneity degrees in the sensory module.</title><p>(<bold>A</bold>) Low-dimensional projection along the memory states. Left panel: The memory manifold projected to the first two principal components (PCs) associated with the vector fields. Right panel: Example drift-diffusion trajectories along the memory manifold starting at <inline-formula><mml:math id="inf26"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>112.5</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. (<bold>B, C</bold>) Velocity (<bold>B</bold>) and noise coefficients (<bold>C</bold>) corresponding to drift and diffusion processes. Different gray scales represent different heterogeneity degrees in the sensory module, <inline-formula><mml:math id="inf27"><mml:mi>α</mml:mi></mml:math></inline-formula>, in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. The velocity with which the remembered orientation drifts to the obliques in a noise-free network (<bold>B</bold>). A larger noise coefficient around the obliques overcomes the underlying drift dynamics and causes the standard deviation pattern to reach its maxima at the obliques (<bold>C</bold>). (<bold>D</bold>) Equivalent one-dimensional energy potential derived from the velocity in (<bold>B</bold>). (<bold>E, F</bold>) Example bias (<bold>E</bold>) and standard deviation (<bold>F</bold>) patterns at 4 s into the delay. The shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Comparison between bias and standard deviation (SD) patterns of the full network model (orangish) and low-dimensional projection (bluish curves).</title><p>From top to bottom, each row corresponds to sensory-memory interacting networks in <xref ref-type="fig" rid="fig5">Figure 5</xref> with <italic>α</italic> = 0.03 (<bold>A, D</bold>), 0.04 (<bold>B, E</bold>), and 0.05 (<bold>C, F</bold>), respectively. We projected the dynamics onto the left (<bold>A–C</bold>) and right (<bold>D–F</bold>) eigenvectors of the Jacobian matrix obtained from local dynamics along the memory states (Methods). The initial orientations of the low-dimensional model were set to be the orientations decoded from the full model at 1 s into the delay. We compared the increase of bias from then on, i.e., at 1.2 s, 1.5 s, and 2 s into the delay for the full model, but 0.2 s, 0.5 s, and 1 s for the low-dimensional model. Low-dimensional projection captures characteristic patterns well despite relatively larger deviation in the SD compared to bias, and we found that projecting to the right eigenvector (<bold>D–F</bold>) generally yields better predictions than projecting to the left eigenvector (<bold>A–C</bold>). All parameters are the same as in <xref ref-type="fig" rid="fig5">Figure 5</xref> except for <italic>α</italic>. Shaded areas (too narrow to be seen) mark the ± s.e.m. of 3000 realizations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Standard deviation (SD) pattern remains consistent under different noise types.</title><p>When Poisson noise is replaced by uniform Gaussian noise with‍ <inline-formula><mml:math id="inf28"><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, the diffusion term and variability of errors are affected (<bold>A–C</bold>), but the drift and bias patterns remain unchanged (not shown). The distance between stimulus representation <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">∥</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">∥</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> determining noise coefficients (<bold>A</bold>), (<xref ref-type="disp-formula" rid="equ21">Equation 21</xref>), noise coefficients (<bold>B</bold>), and SD (<bold>C</bold>) exhibit similar profiles and dependence on <inline-formula><mml:math id="inf30"><mml:mi>α</mml:mi></mml:math></inline-formula> as those under Poisson noise (see <xref ref-type="fig" rid="fig5">Figure 5C and F</xref>). <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 0.2 and all remaining parameters are the same as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig5-figsupp2-v1.tif"/></fig></fig-group><p>Next, we examined how heterogeneity of connectivity in the sensory module affects the dynamics along the memory states. The magnitude of heterogeneity is denoted as <italic>α</italic>, and larger <italic>α</italic> represents a larger asymmetry of connectivity strengths at cardinal and oblique orientations (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left). When <italic>α</italic> increases, the asymmetry of drift and energy levels becomes more prominent, leading to a more rapid increase in bias (<xref ref-type="fig" rid="fig5">Figure 5B, D, and E</xref>). The diffusion term is also more asymmetric, compensating for stronger attraction to the obliques (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Thus, for larger <italic>α</italic>, the variability of errors is still higher at the obliques (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). Another important parameter influencing error patterns is the intermodal connectivity strengths (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Similar to the effect of increasing <italic>α</italic>, increases in feedforward or feedback strengths cause the energy levels to become more asymmetrical (<xref ref-type="fig" rid="fig6">Figure 6A and E</xref>), leading to a larger bias (<xref ref-type="fig" rid="fig6">Figure 6B and F</xref>). Conversely, the noise coefficient is less affected (<xref ref-type="fig" rid="fig6">Figure 6C and G</xref>), and the variance of errors decreases as the drift force becomes stronger (<xref ref-type="fig" rid="fig6">Figure 6D and H</xref>). Note that bias and variance patterns depend on the product of feedforward and feedback connections, denoted as <italic>γ</italic>, such that for a fixed <italic>γ</italic>, the error patterns remain similar (<xref ref-type="fig" rid="fig6">Figure 6I and J</xref>). In sum, the bias and variability of errors are determined by the degree of heterogeneity in the sensory module (<italic>α</italic>) and intermodal connectivity strengths (<italic>γ</italic>) as both <italic>α</italic> and <italic>γ</italic> affect the asymmetry of drift term similarly, while the asymmetry of diffusion term is more strongly influenced by <italic>α</italic> (<xref ref-type="fig" rid="fig6">Figure 6K and L</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Error patterns and low-dimensional dynamics for different intermodal connectivity strengths.</title><p>(<bold>A–J</bold>) Low-dimensional dynamics and error patterns with varying feedforward and feedback connection strengths, denoted by <italic>J</italic><sub>f</sub> and <italic>J</italic><sub>b</sub>. (<bold>K, L</bold>) Potential differences and noise coefficient indices comparing low-dimensional dynamics at cardinal and oblique orientations for changing <italic>J</italic><sub>b</sub> and heterogeneity degree, <inline-formula><mml:math id="inf32"><mml:mi>α</mml:mi></mml:math></inline-formula>. Increasing both feedforward (<bold>A–D</bold>) and feedback (<bold>E–H</bold>) connection strengths deepens the potential difference (<bold>A, E, K</bold>) and increases the bias (<bold>B, F</bold>), similar to the effects of <inline-formula><mml:math id="inf33"><mml:mi>α</mml:mi></mml:math></inline-formula> increases in <xref ref-type="fig" rid="fig5">Figure 5D and E</xref>. In contrast, the profile of noise coefficients is less affected (<bold>C, G, L</bold>) and the SD pattern gets flattened with stronger drift (<bold>D, H</bold>). Bias and SD patterns depend on the product of feedforward and feedback connection strengths (<bold>I, J</bold>). Bias and SD are estimated at 4 s (<bold>B, D, F, H</bold>) or 1 s (<bold>I,J</bold>) into the delay and shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig6-v1.tif"/></fig></sec><sec id="s2-5"><title>Importance of heterogeneously tuned inhibition</title><p>We showed that network models realizing sensory-memory interactions reproduce correct error patterns, where each module has a different connectivity structure. Previous work suggested that such a heterogeneous connection of the sensory system may arise from experience-dependent synaptic modification (<xref ref-type="bibr" rid="bib36">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib64">Zylberberg et al., 2011</xref>). For example, typical Hebbian learning is thought to potentiate connectivity strengths between neurons whose preferred stimuli are more frequently encountered. For orientations, cardinal directions are predominant in natural scenes. Thus, if experience-dependent learning occurs mainly at the excitatory synapses, the excitatory connections near cardinal orientations become stronger in the sensory module. This is opposite to the previously discussed case where the sensory module has the strongest connection at the obliques. With the strongest excitatory connections at cardinal orientations, the error patterns are reversed, resulting in cardinal attraction instead of repulsion, and the lowest variance occurs at the obliques.</p><p>Inhibitory synaptic connections can also be modified through learning (<xref ref-type="bibr" rid="bib53">Vogels et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Khan et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Larisch et al., 2021</xref>). Here, we considered that experience-dependent learning exists in both excitatory and inhibitory pathways and similarly shapes their connectivity (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). We assumed that excitatory and inhibitory connections are segregated and stronger near cardinal orientations (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). We modulated the heterogeneity degree of both excitatory and inhibitory connections, denoted as <italic>α</italic> and <italic>β</italic>, respectively (<xref ref-type="fig" rid="fig7">Figure 7B–D</xref>). The ratio between <italic>α</italic> and <italic>β</italic> determines the direction and magnitude of bias and variance patterns (<xref ref-type="fig" rid="fig7">Figure 7C and D</xref>). For relatively larger <italic>α</italic>, the network shows cardinal attraction and minimum variance of errors at the obliques (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). Reversely, for relatively larger <italic>β</italic> with stronger modulation in inhibitory connections, the network reproduced cardinal repulsion and minimum variance of errors at cardinal orientations, consistent with experiments (<xref ref-type="fig" rid="fig7">Figure 7F</xref>). With a larger difference between <italic>α</italic> and <italic>β</italic>, such patterns of bias and variance are potentiated and minimum Fisher information across orientations decreases, corresponding to memory loss (<xref ref-type="fig" rid="fig7">Figure 7C and D</xref>; <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Thus, this emphasizes the important role of heterogeneously tuned inhibition in shaping the sensory response for higher precision at cardinal orientations and enabling the sensory-memory interacting network to generate correct error patterns.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Stronger inhibitory synaptic modulation is required for correct error patterns.</title><p>(<bold>A</bold>) Segregation of excitatory (blue) and inhibitory (red) synaptic pathways. (<bold>B</bold>) Example excitatory (left) and inhibitory (right) connectivity strengths of the sensory module. The heterogeneity degrees of excitatory and inhibitory connections are denoted by <inline-formula><mml:math id="inf34"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf35"><mml:mi>β</mml:mi></mml:math></inline-formula>, respectively. Unlike combined excitation and inhibition in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, the connectivity strengths are maximal around cardinal orientations. (<bold>C, D</bold>) Bias with stimulus at 22.5° (<bold>C</bold>) and standard deviation (SD) index (<bold>D</bold>) estimated at 1 s into the delay for different values of <inline-formula><mml:math id="inf36"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mi>β</mml:mi></mml:math></inline-formula>. SD index compares the SD at the cardinal and oblique orientations (Methods). (<bold>E, F</bold>) Example bias (left) and SD (right) patterns when excitatory modulation overwhelms inhibitory modulation (<inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.068</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <bold>E</bold>) and when inhibitory modulation is stronger (<inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <bold>F</bold>). In (<bold>C</bold>) and (<bold>D</bold>), green (yellow) pentagrams mark the parameters used in (<bold>E</bold>) and (<bold>F</bold>). Stronger inhibitory modulation is required for correct bias and variance patterns (<bold>F</bold>) and green regions in (<bold>C</bold> and <bold>D</bold>). In (<bold>E</bold>) and (<bold>F</bold>), shaded areas mark the ±s.e.m. of 1000 realizations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Relationship between drift speed and memory loss in two-module (<bold>A–C</bold>) and one-module (<bold>D–F</bold>) networks.</title><p>(<bold>A, D</bold>) Drift speed for different heterogeneity degrees, <inline-formula><mml:math id="inf40"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf41"><mml:mi>β</mml:mi></mml:math></inline-formula>. (<bold>B, E</bold>) Minimum Fisher information (FI). The upper panels were obtained with a coarse parameter grid, and lower panels were obtained with a fine grid but only along parameters for the smallest bias increase (red circle) and the orthogonal direction (red triangle). When bias speed is large with unbalanced excitation and inhibition strengths (triangle), the minimum FI decreases quickly in both two-module and one-module networks, suggesting memory loss. On the other hand, along the direction with the smallest bias increase (circle), the minimum FI is relatively high. The FI was estimated at 4 s into the delay epoch using 1000 realizations. (<bold>C, F</bold>) Negative correlation between minimum FI and drift speed.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Error patterns in sensory networks with long intrinsic time constants.</title><p>In the sensory module alone (<inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>), with an intrinsic time constant <italic>τ</italic>=1 s in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, the bias (<bold>A</bold>) and standard deviation (SD) (<bold>B</bold>) remain nearly unchanged during a 5 s delay period. Preferred orientations are estimated 10 s after stimulus onset, and all other parameters are the same as those in <xref ref-type="fig" rid="fig7">Figure 7F</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig7-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Comparison to alternative circuit structures</title><p>So far, we have shown the sufficiency of sensory-memory interacting networks with different connectivity structures featuring heterogeneous-homogeneous recurrent connections within each module. Here, we explore whether such architecture is necessary by comparing its performance with alternative circuit structures for sensory-memory interactions. One candidate mechanism involves having the heterogeneous sensory network maintain memory with a long intrinsic time constant, similar to having autapses (<xref ref-type="bibr" rid="bib45">Seung et al., 2000</xref>). However, this model fails to replicate the evolution of error patterns during the delay period as a long intrinsic time constant slows down the overall dynamics, thus hindering the evolution of error patterns (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). Alternatively, we focused on a two-module network with variations in connectivity structure. We assumed that sensory and memory modules still serve their distinctive functions, namely, sensory encoding and memory maintenance, with weak/strong recurrent connections in sensory/memory modules. On the other hand, the heterogeneity of connections in other circuits might differ as homogeneous-homogeneous, homogeneous-heterogeneous, and heterogeneous-heterogeneous connections for sensory-memory modules.</p><p>Circuits with homogeneous connections in both sensory and memory modules are similar to previous continuous attractor models for working memory, such that the energy landscape and noise amplitude are uniform for all orientations (<xref ref-type="fig" rid="fig1">Figure 1D–F</xref>). Such architecture is not suitable as it generates no bias in errors and flat variance patterns. This leaves the latter two types of configurations, which require heterogeneous connections within the memory module. With a strong recurrent connection within the memory module, its heterogeneous activity pattern dominates overall activities in sensory-memory interacting networks, which makes it analogous to an isolated memory module. Thus, we examined the property of the memory module alone, which can maintain memory while generating heterogeneous responses without connection to the sensory module (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Network model with memory module only cannot reproduce correct error patterns.</title><p>(<bold>A</bold>) Schematics of one-module network with heterogeneous and strong recurrent connections that enable both efficient coding and memory maintenance. (<bold>B</bold>) Example tuning curves at the end of the stimulus epoch (left) and at 4 s into the delay epoch (right). (<bold>C, D</bold>) Bias with stimulus at 22.5° (<bold>C</bold>) and standard deviation (SD) index (<bold>D</bold>) estimated at 1 s into the delay for different heterogeneity degrees of excitatory and inhibitory connections, denoted by <inline-formula><mml:math id="inf43"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:mi>β</mml:mi></mml:math></inline-formula>. For the parameters that generate reasonable bias patterns, the SD index is always negative, which indicates that the SD pattern is inconsistent with experimental findings. (<bold>E</bold>) Bias (left), and SD (right) patterns in the delay. While the bias pattern is correct, the SD reaches maxima around cardinal orientations, unlike the experiments. In (<bold>C</bold>) and (<bold>D</bold>), the yellow pentagram marks the parameters used in (<bold>E</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig8-v1.tif"/></fig><p>To generate the correct bias pattern, we assumed that excitatory and inhibitory pathways in the memory module are stronger near cardinal orientations, as we previously considered for the sensory module in the sensory-memory interacting network (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>). However, memory circuits with heterogeneous connections have problems in maintaining the information and reproducing correct error patterns (<xref ref-type="fig" rid="fig8">Figure 8C–E</xref>). First, memory circuits alone require fine-tuning of heterogeneity whose range generating a moderate drift speed is at least one order of magnitude smaller than that of the two-module network (<xref ref-type="fig" rid="fig8">Figure 8C and D</xref>). Deviation from this range results in a fast drift toward oblique orientations, leading to rapid loss of information during the delay period (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Second, despite the correct bias direction, the variance pattern is reversed such that the variance of errors is minimal at the oblique orientations (<xref ref-type="fig" rid="fig8">Figure 8E</xref>). Varying the heterogeneity in excitatory and inhibitory connections shows that such rapid drift and reversed error patterns are prevalent across different parameters (<xref ref-type="fig" rid="fig8">Figure 8C and D</xref>).</p><p>To understand why a heterogeneous memory circuit alone fails to reproduce correct error patterns, we compared its low-dimensional dynamics along the memory states to that of the sensory-memory interacting networks. For the network with a similar range of bias and variance on average, we compared their energy landscape and noise amplitude, which vary similarly in both networks with minimum energy level and maximum noise at the oblique orientations (<xref ref-type="fig" rid="fig9">Figure 9A–F</xref>). However, the energy difference between cardinal and oblique orientations in a single memory circuit model is bigger than that in a sensory-memory interacting network (<xref ref-type="fig" rid="fig9">Figure 9C</xref>, left in <xref ref-type="fig" rid="fig9">Figure 9G, H</xref>). In contrast, the difference in noise amplitude is smaller (<xref ref-type="fig" rid="fig9">Figure 9D–F</xref>, right in <xref ref-type="fig" rid="fig9">Figure 9G, H</xref>). The attraction at the obliques is much stronger, leading to the correct bias patterns, but too rapid an increase. Also, smaller differences in noise amplitude cannot overcome strong drift dynamics, leading to the minimum variance of errors at the obliques and reversed variance patterns. Even for different types or levels of noise, such as Gaussian noise with varying amplitude, distinctive error patterns in one-module and two-module networks are maintained (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Comparison of low-dimensional dynamics between two-module and one-module network models.</title><p>(<bold>A, B</bold>) Bias and standard deviation (SD) patterns of two-module (<bold>A</bold>) and one-module (<bold>B</bold>) networks, adapted from <xref ref-type="fig" rid="fig7">Figure 7F</xref> and <xref ref-type="fig" rid="fig8">Figure 8E</xref>, respectively. The averages of bias and SD over different <inline-formula><mml:math id="inf45"><mml:mi>θ</mml:mi></mml:math></inline-formula> at 4 s into the delay are similar in the two networks. (<bold>C–F</bold>) Low-dimensional dynamics of two-module (black) and one-module (red) networks. In both networks, the energy potential (<bold>C</bold>), the distance between stimulus representation, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">∥</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">∥</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and its inverse determining noise coefficients (<bold>D, E</bold>; <xref ref-type="disp-formula" rid="equ21">Equation 21</xref>), and the noise coefficients (<bold>F</bold>) exhibit similar profiles. However, the two-module network has a shallower potential (<bold>C</bold>) but larger heterogeneity in <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">∥</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">∥</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and the noise coefficient profile (<bold>D–F</bold>). These differences make it possible for the SD to become smaller around cardinal orientations in the two-module network (right in <bold>A</bold>), while drift dynamics overwhelm and the SD pattern is opposite to that of the noise coefficient in the one-module network (right in <bold>B</bold>). (<bold>G, H</bold>) Potential difference (left) and index of noise coefficients (right) comparing low-dimensional dynamics at the cardinal and oblique orientations in two-module (<bold>G</bold>) and one-module (<bold>H</bold>) networks. The two-module network shows a smaller potential difference and more heterogeneous noise coefficients over a broad range of heterogeneity (see the color bars in <bold>G</bold> and <bold>H</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Error patterns remain unchanged under different levels of noise.</title><p>Bias (left) and standard deviation (SD) (right) in two-module (<bold>A, B</bold>) and one-module (<bold>C, D</bold>) networks with uniform Gaussian noise‍ <inline-formula><mml:math id="inf48"><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></inline-formula> at varying <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> levels. For each heterogeneity level <italic>α</italic>, the error patterns are unchanged across different <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> values. Bias and SD are estimated 1 s into the delay period from 6000 realizations. All other parameters are the same as in <xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig8">8</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig9-figsupp1-v1.tif"/></fig></fig-group><p>For an intuitive understanding of how connectivity heterogeneity affects the degrees of asymmetry in drift and diffusion differently in one-module and two-module networks, consider a simple case where only the excitatory connection exhibits heterogeneity, the degree of which is denoted by <italic>α</italic>. For memory maintenance, the overall recurrent connections need to be strong enough to overcome intrinsic decay, simplified to <italic>w</italic>=1. In the one-module network, <italic>α</italic> in the memory module causes deviations from perfect tuning, creating potential differences at cardinal and oblique orientations as 1±<italic>α</italic>. In the two-module network, with <italic>w</italic>=1 fulfilled by the memory module, <italic>α</italic> in the sensory module acts as a perturbation. The effect of <italic>α</italic> is modulated by the intermodal connectivity strengths, denoted by <italic>γ</italic>, and potential differences at cardinal and oblique orientations can be represented as 1±<italic>γα</italic>. Thus, while a relatively large <italic>α</italic> leads to too fast drift in the one-module network, the drift speed in the two-module network could remain modest with small <italic>γ</italic>&lt;1. Conversely, even with small <italic>γ</italic>, the asymmetry of noise coefficients can be large enough to produce correct variance patterns because the noise coefficient is more strongly influenced by <italic>α</italic> in the two-module network (<xref ref-type="fig" rid="fig6">Figure 6</xref>). In sum, compared to a heterogeneous memory circuit alone, interactions between heterogeneous sensory and homogeneous memory modules are advantageous due to an additional degree of freedom, intermodal connectivity strengths, which allows better control of energy and noise difference at cardinal and oblique orientations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While higher association areas have long been considered as a locus of working memory (<xref ref-type="bibr" rid="bib42">Roussy et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Mejías and Wang, 2022</xref>), recent human studies found memory signals in early sensory areas, prompting a re-evaluation of their role in working memory (<xref ref-type="bibr" rid="bib60">Xu, 2020</xref>; <xref ref-type="bibr" rid="bib1">Adam et al., 2022</xref>). Our work extends the traditional memory models (<xref ref-type="bibr" rid="bib54">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib29">Khona and Fiete, 2022</xref>) with novel insights into the significance of stimulus-specific sensory areas. We showed how sensory-memory interactions can elucidate changes in the internal representation of orientation stimuli and their behavioral readout during memory tasks. The observed error patterns suggest that the network meets two demands simultaneously: efficient encoding that reflects natural statistics and memory maintenance for successful retrieval of stimuli after a delay. Achieving both demands for orientation stimuli conflicts in a one-module network. Efficient encoding necessitates asymmetrical connections, resulting in inconsistent bias and variance patterns and overly rapid drift in the one-module network unless fine-tuned. In contrast, connecting sensory and memory modules can generate error patterns correctly and with less need for fine-tuning heterogeneity for slow drift. Efficient coding of natural statistics in the sensory module underscores the role of inhibitory plasticity. Low-dimensional projection onto memory states reveals that drift and diffusion processes governing working memory dynamics closely resemble the bias and variance patterns derived under Bayesian sensory models. It also elucidates how the magnitudes of bias and variance change depending on the heterogeneity of sensory connections and intermodal connectivity strengths.</p><p>Our model makes testable predictions to differentiate two-module and one-module networks using perturbation, such as transcranial magnetic stimulation (TMS). Many studies have found that during the delay period, TMS can intervene with the feedforward signal from sensory areas through which working memory is consolidated (<xref ref-type="bibr" rid="bib52">van de Ven et al., 2012</xref>) (but see <xref ref-type="bibr" rid="bib1">Adam et al., 2022</xref>, for mixed effects of TMS and related debate). Under such perturbations, the ability to maintain information in the memory module will not be affected due to strong recurrent connections in both two-module and one-module networks. However, we expect different effects on bias patterns — in the two-module network, the bias will stop systematically drifting toward the obliques, reducing systematic repulsion (<xref ref-type="fig" rid="fig10">Figure 10</xref>). This accompanies the nonincreasing heterogeneity of tuning curves after the disruption, marked by their tuning width indices (see Methods). In contrast, in the one-module network, perturbation does not incur changes in error patterns as memory activities are less dependent on the sensory module during the delay period. Thus, perturbation studies can be used to reveal the role of the sensory module in shaping the error patterns during working memory. Note that our model cannot predict the effects of distractors during working memory, as such effects do not experimentally lead to changes in error patterns (<xref ref-type="bibr" rid="bib40">Rademaker et al., 2019</xref>). The effect of distractors and direct intervention in the intermodule connections may differ due to potential differences in the encoding of distractors compared to task-relevant stimuli. More advanced models are required to comprehensively understand the influence of distractors and the processing of ongoing visual stimuli or the storage of multiple stimuli.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Effect of perturbations in sensory-memory interaction on error patterns.</title><p>(<bold>A, B</bold>) Example bias (<bold>A</bold>) and standard deviation (<bold>B</bold>) patterns when we assumed that transcranial magnetic stimulation (TMS) is applied to interrupt the feedforward signal from 2.5 s into the delay. Shaded areas mark the ±s.e.m. of 1000 realizations. (<bold>C, D</bold>) Evolution of bias with example cue orientation at <inline-formula><mml:math id="inf51"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>18</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> (<bold>C</bold>) and the tuning width indices in the memory network (WI; <bold>C</bold>) representing the asymmetry of tuning widths at cardinal and oblique orientations (Methods). Two vertical dashed lines mark the end of the stimulus epoch and the beginning of TMS disruption, respectively. Solid and dashed curves correspond to with and without perturbations, respectively. Both bias (<bold>C</bold>) and WI (<bold>D</bold>) stop increasing when TMS is on (<bold>C, D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95160-fig10-v1.tif"/></fig><p>Our work suggests biologically plausible network mechanisms for the previously postulated efficient coding and Bayesian inference principles, relating network connectivity to tuning properties and error patterns. Previous normative explanations for systematic bias observed in perception tasks also suggested possible neural substrates for efficient coding, such as asymmetrical gain, width, or density of tuning curves across stimulus features (<xref ref-type="bibr" rid="bib17">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib56">Wei and Stocker, 2015</xref>). Our work narrowed the mechanism to denser and narrower tuning curves at cardinal orientations, consistent with neurophysiological recordings in the visual cortex (<xref ref-type="bibr" rid="bib34">Li et al., 2003</xref>; <xref ref-type="bibr" rid="bib30">Kreile et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Shen et al., 2014</xref>). We implemented a population vector decoder reflecting neuronal preferred orientations, which approximates Bayesian optimal readout (<xref ref-type="bibr" rid="bib15">Fischer, 2010</xref>). Compared to a previous work adapting efficient coding theories with static tuning curves to account for error patterns in working memory tasks (<xref ref-type="bibr" rid="bib48">Taylor and Bays, 2018</xref>), our extension to memory processes demonstrated how neural activities and behavior readout change dynamically during the delay period. Notably, recent work combined dynamic change of signal amplitude with static tuning curves to capture different time courses of estimation precision during sensory encoding and memory maintenance (<xref ref-type="bibr" rid="bib49">Tomić and Bays, 2023</xref>). Our network models embody such phenomenological models as the networks exhibit changes in overall firing rates after the stimulus offset.</p><p>Like our study, a few recent studies have employed attractor dynamics to explain dynamic error patterns observed for visual color memory (<xref ref-type="bibr" rid="bib37">Panichello et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Pollock and Jazayeri, 2020</xref>; <xref ref-type="bibr" rid="bib14">Eissa and Kilpatrick, 2023</xref>). Behavior studies showed attractive bias and minimum variance around the prevalent colors, which one-module discrete attractor models could reproduce. However, these models cannot be generalized to other visual stimuli, such as orientations, spatial locations, or directions, of which the responses show repulsive bias away from the common stimuli (<xref ref-type="bibr" rid="bib57">Wei and Stocker, 2017</xref>). Also, a one-module network storing color memory requires fine-tuned heterogeneity for moderate drift speed. While the desired low-dimensional manifold and drift dynamics can be engineered in the one-module network (<xref ref-type="bibr" rid="bib38">Pollock and Jazayeri, 2020</xref>), its biological mechanism needs further investigation. The two-module network considered in our study also requires fine-tuning of homogeneity in the memory module and heterogeneity in the sensory module. However, the condition of asymmetrical connections in the sensory module is less stringent as they have a weaker influence on the entire dynamics than those in the memory module. Fine-tuning of homogeneous connections in the memory module can be mediated through activity-dependent plasticity, such as short-term facilitation (<xref ref-type="bibr" rid="bib27">Itskov et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Hansel and Mato, 2013</xref>; <xref ref-type="bibr" rid="bib44">Seeholzer et al., 2019</xref>) or long-term plasticity (<xref ref-type="bibr" rid="bib41">Renart et al., 2003</xref>; <xref ref-type="bibr" rid="bib21">Gu and Lim, 2022</xref>). Also, recent work showed that continuous attractors formed under unstructured, heterogeneous connections are robust against synaptic perturbations (<xref ref-type="bibr" rid="bib11">Darshan and Rivkind, 2022</xref>). Thus, the two-module networks can control the drift speed better with possible additional mechanisms that promote homogeneous memory states. It needs further exploration whether they can be generalized to other stimuli like color, possibly involving additional categorical structures (<xref ref-type="bibr" rid="bib26">Hardman et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Pratte et al., 2017</xref>).</p><p>Our current study is limited to the dynamic evolution of memory representation for a single orientation stimulus and its associated error patterns, which does not capture nuanced error patterns in broader experimental settings (<xref ref-type="bibr" rid="bib23">Hahn and Wei, 2024</xref>). For instance, while shorter stimulus presentations with no explicit delay led to larger biases experimentally, our current model, which starts activities from a flat baseline, shows an increase in bias throughout the stimulus presentation (<xref ref-type="bibr" rid="bib12">de Gardelle et al., 2010</xref>). Additionally, the error variance during stimulus presentation is almost negligible compared to that during the delay period, as the external input overwhelms the internal noise. These mismatches during stimulus presentation have minimal impact on activities during the delay period when the internal dynamics dominate. Nonetheless, the model needs further refinement to accurately reproduce activities during stimulus presentation, possibly by incorporating more biologically plausible baseline activities. Also, a recent Bayesian perception model suggested different types of noise like external noise or variations in loss functions that adjust tolerance to small errors may help explain various error patterns observed across different modalities (<xref ref-type="bibr" rid="bib23">Hahn and Wei, 2024</xref>). Even for memories involving multiple items, noise can be critical in determining error patterns, as encoding more items might cause higher noise for each individual item (<xref ref-type="bibr" rid="bib8">Chunharas et al., 2022</xref>).</p><p>The modularity structure in the brain is thought to be advantageous for fast adaptation to changing environments (<xref ref-type="bibr" rid="bib47">Simon, 1995</xref>; <xref ref-type="bibr" rid="bib9">Cole et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Frankland and Greene, 2020</xref>). Recent works showed that recurrent neural networks trained for multiple cognitive tasks form clustered neural activities and modular dynamic motifs to repurpose shared functions for flexible computation (<xref ref-type="bibr" rid="bib61">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>). Resonant with these computational findings, an fMRI study showed that shared representation across distinct visual stimuli emerges during the delay period (<xref ref-type="bibr" rid="bib31">Kwak and Curtis, 2022</xref>). Although our work focuses on a single task, it highlights the necessity of having dedicated sensory and memory modules, and a memory module with ring geometry can be repurposed for various visual stimuli such as motion, spatial location, and color. It is reminiscent of the flexible working memory model, which proposes connections between multiple sensory modules and a control module (<xref ref-type="bibr" rid="bib5">Bouchacourt and Buschman, 2019</xref>). However, a key distinction lies in the role of the control module. Unlike the flexible working memory model that loses memory without sensory-control interactions, our work suggests that the memory module can independently maintain memory, while interaction with the sensory module continuously shapes the internal representation, potentially consolidating prior beliefs regarding natural statistics. The sensory-memory interaction and network architecture derived from dynamic changes of single stimulus representation can be a cornerstone for future studies in more complex conditions, such as under the stream of visual inputs (<xref ref-type="bibr" rid="bib60">Xu, 2020</xref>; <xref ref-type="bibr" rid="bib1">Adam et al., 2022</xref>) or with high or noisy memory loads (<xref ref-type="bibr" rid="bib4">Bays et al., 2022</xref>).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Low-dimensional attractor models</title><p>To illustrate error patterns in different low-dimensional attractor models shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, we considered a one-dimensional stochastic differential equation given as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are orientation and standard Brownian motion at time <italic>t,</italic> respectively. We assumed that the drift and noise coefficients <italic>μ</italic> and <italic>σ</italic> only depend on <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf55"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi mathvariant="script">D</mml:mi></mml:msqrt></mml:math></inline-formula> with diffusion coefficient <inline-formula><mml:math id="inf56"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula>.</p><p>For continuous attractor models in <xref ref-type="fig" rid="fig1">Figure 1D–F</xref>, <italic>μ</italic> and <italic>σ</italic> were set to be constant as <inline-formula><mml:math id="inf57"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>°</mml:mo></mml:math></inline-formula>. For discrete attractor models in <xref ref-type="fig" rid="fig1">Figure 1G–L</xref>, we assumed that the energy function <inline-formula><mml:math id="inf59"><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is proportional to <inline-formula><mml:math id="inf60"><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1G and J</xref>) so that the drift term <inline-formula><mml:math id="inf61"><mml:mi>μ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>4</mml:mn><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf62"><mml:mi>μ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> . In these attractor models, the constant noise in <xref ref-type="fig" rid="fig1">Figure 1G–I</xref> is <inline-formula><mml:math id="inf63"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>°</mml:mo></mml:math></inline-formula> and the nonuniform noise in <xref ref-type="fig" rid="fig1">Figure 1J–L</xref> is <inline-formula><mml:math id="inf64"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>°</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>. The biases and standard deviation (SD) of errors were plotted at <italic>T</italic>=1, 2, and 3 with 50,000 iterations. For the numerical simulation, d<italic>t</italic> =0.01.</p></sec><sec id="s4-2"><title>Bayesian sensory models and extension</title><p>In <xref ref-type="fig" rid="fig2">Figure 2</xref>, we first constructed the sensory inference process, which receives orientation input <inline-formula><mml:math id="inf65"><mml:mi>θ</mml:mi></mml:math></inline-formula>, forms a corresponding noisy sensory representation <inline-formula><mml:math id="inf66"><mml:mi>m</mml:mi></mml:math></inline-formula> given <inline-formula><mml:math id="inf67"><mml:mi>θ</mml:mi></mml:math></inline-formula>, and then infers <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as an estimate of the input orientation from the encoded representation <inline-formula><mml:math id="inf69"><mml:mi>m</mml:mi></mml:math></inline-formula>. This inference is made in a Bayesian manner based on likelihood function <inline-formula><mml:math id="inf70"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and orientation prior <inline-formula><mml:math id="inf71"><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>To construct <inline-formula><mml:math id="inf72"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></inline-formula> we followed the procedure given in <xref ref-type="bibr" rid="bib56">Wei and Stocker, 2015</xref>, and the summary is as follows. We started from the sensory space of <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where both discriminability and Fisher information <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are uniform, and all likelihood functions <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are homogeneous von Mises functions. And since <inline-formula><mml:math id="inf76"><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>∝</mml:mo><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> under the efficient coding condition, the sensory space of <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the stimulus space of <inline-formula><mml:math id="inf78"><mml:mi>θ</mml:mi></mml:math></inline-formula> can be mapped by forward and backward mappings <inline-formula><mml:math id="inf79"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf81"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the cumulative distribution function of prior <inline-formula><mml:math id="inf82"><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Thus, likelihood functions <inline-formula><mml:math id="inf83"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> can be obtained by taking homogeneous von Mises likelihoods in the sensory space and transforming them back to the stimulus space using <inline-formula><mml:math id="inf84"><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. To sum up the upper half of the procedural diagram in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, the sensory module receives <inline-formula><mml:math id="inf85"><mml:mi>θ</mml:mi></mml:math></inline-formula>, encodes it in <inline-formula><mml:math id="inf86"><mml:mi>m</mml:mi></mml:math></inline-formula> following <inline-formula><mml:math id="inf87"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, and decodes <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> using likelihood functions and prior <inline-formula><mml:math id="inf89"><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>As an extension to include a memory process, the decoded <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is passed on to the memory module, where <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is maintained with the addition of memory noise <inline-formula><mml:math id="inf92"><mml:mi>ξ</mml:mi></mml:math></inline-formula>. The output of the memory module, <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, is fed back to the sensory module as the new input. This completes one iteration of sensory-memory interaction. The whole process is then repeated recursively, resulting in increased biases and standard deviations in the <inline-formula><mml:math id="inf94"><mml:mi>θ</mml:mi></mml:math></inline-formula> statistics at subsequent iterations (call them <inline-formula><mml:math id="inf95"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the input of iteration <inline-formula><mml:math id="inf96"><mml:mi>i</mml:mi></mml:math></inline-formula>).</p><p>For <xref ref-type="fig" rid="fig2">Figure 2B and C</xref>, we set the von Mises sensory-space likelihoods to be <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:math></inline-formula>. These likelihood functions are transformed by <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>∫</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf100"><mml:mi>q</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Each internal representation <inline-formula><mml:math id="inf101"><mml:mi>m</mml:mi></mml:math></inline-formula> is sampled from <inline-formula><mml:math id="inf102"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, after which <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is estimated as the mean of the posterior <inline-formula><mml:math id="inf104"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mi>q</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. With the parameters chosen above, the inferred samples of <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> after the first sensory iteration have a circular standard deviation of <inline-formula><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>1.3</mml:mn><mml:mo>°</mml:mo></mml:math></inline-formula> at cardinal orientations. To have comparable memory and sensory noise levels, we set the memory noise as <inline-formula><mml:math id="inf107"><mml:mi>ξ</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1.3</mml:mn><mml:mo>°</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></inline-formula> which is added on top of the sensory outputs. Thus, the memory outputs of the first iteration <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> have a standard deviation of 1.84<sup>o</sup> at the cardinals. The first three iterations’ memory output statistics are plotted in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, i.e., bias(<inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), bias(<inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), bias(<inline-formula><mml:math id="inf111"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), and SD(<inline-formula><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), SD(<inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), SD(<inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). The statistics were computed from 10,000 iterations of the simulation. The magnitude of biases and standard deviations vary for different sensory or memory noise levels, while the overall patterns and the increasing temporal trend are unchanged (not shown).</p></sec><sec id="s4-3"><title>Firing rate models</title><p>For network models, we considered sensory circuits with heterogeneous connections (<xref ref-type="fig" rid="fig3">Figure 3</xref>), memory circuits with homogeneous connections (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and heterogeneous connections (<xref ref-type="fig" rid="fig8">Figures 8</xref> and <xref ref-type="fig" rid="fig9">9</xref>), and sensory-memory interacting circuits (<xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig7">7</xref>, <xref ref-type="fig" rid="fig9">9</xref>, and <xref ref-type="fig" rid="fig10">10</xref>). In all cases, the activities of neurons are described by their firing rates and synaptic states, denoted by <inline-formula><mml:math id="inf115"><mml:mi>r</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf116"><mml:mi>s</mml:mi></mml:math></inline-formula>. For columnar structure encoding orientation stimuli, we indexed the neurons by uniformly assigning them indices <inline-formula><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>180</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for <inline-formula><mml:math id="inf118"><mml:mi>i</mml:mi></mml:math></inline-formula> from 1 to <inline-formula><mml:math id="inf119"><mml:mi>N</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf120"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of neurons in each population. For sensory or memory networks alone, the dynamics of neuron <inline-formula><mml:math id="inf121"><mml:mi>i</mml:mi></mml:math></inline-formula> are described by the following equations:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>ext</mml:mtext><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>τ</mml:mi><mml:msubsup><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the superscripts <inline-formula><mml:math id="inf122"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:mi>j</mml:mi></mml:math></inline-formula> are the neuronal indices, and the subscript <inline-formula><mml:math id="inf124"><mml:mi>k</mml:mi></mml:math></inline-formula> is either <inline-formula><mml:math id="inf125"><mml:mi mathvariant="normal">s</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf126"><mml:mi mathvariant="normal">m</mml:mi></mml:math></inline-formula>, representing sensory or memory circuits. For the sensory-memory interacting network, the dynamics are given as<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>b</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mtext>ext,s</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>f</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mtext>ext,m</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>for </mml:mtext><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mtext>s or f</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>for </mml:mtext><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mtext>m or b</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where activities and synaptic inputs are represented in the vector and matrix multiplication form, shown in bold cases. The additional subscripts <inline-formula><mml:math id="inf127"><mml:mi mathvariant="normal">f</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf128"><mml:mi mathvariant="normal">b</mml:mi></mml:math></inline-formula> represent feedforward and backward connections between sensory and memory modules.</p><p>In both <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref>, <inline-formula><mml:math id="inf129"><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the low pass filtered <inline-formula><mml:math id="inf130"><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> with synaptic time constant <inline-formula><mml:math id="inf131"><mml:mi>τ</mml:mi></mml:math></inline-formula> and with the addition of <inline-formula><mml:math id="inf132"><mml:mi>ξ</mml:mi></mml:math></inline-formula> approximating Poisson noise. We modeled <inline-formula><mml:math id="inf133"><mml:mi>ξ</mml:mi></mml:math></inline-formula> as the Gaussian process with covariance <inline-formula><mml:math id="inf134"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mi mathvariant="normal">`</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi mathvariant="normal">`</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, following <xref ref-type="bibr" rid="bib7">Burak and Fiete, 2012</xref>. We assumed that the rate dynamics are relatively fast such that <inline-formula><mml:math id="inf135"><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> equals the input current-output rate transfer function <italic>f</italic>. The input current is the sum of external input <inline-formula><mml:math id="inf136"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the synaptic currents from other neurons in the network, which are the postsynaptic states <inline-formula><mml:math id="inf137"><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> weighted by synaptic strengths <inline-formula><mml:math id="inf138"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. The transfer function <italic>f</italic> has the Naka-Rushton form (<xref ref-type="bibr" rid="bib58">Wilson, 1999</xref>) given as<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> denotes the linear rectification function. The transfer functions differ in the sensory and memory modules, denoted as <inline-formula><mml:math id="inf140"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, respectively.</p></sec><sec id="s4-4"><title>Synaptic inputs in network models</title><p>Note that for all network models, we only considered excitatory neurons under the assumption that the inhibitory synaptic pathways have relatively fast dynamics. Thus, recurrent connectivity strengths, <inline-formula><mml:math id="inf142"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, within sensory and memory modules, reflect summed excitation and inhibition, and thus, can have either positive or negative signs. On the other hand, we assumed that intermodal interactions, <inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, are dominantly excitatory and, thus, can be only positive.</p><p>All <inline-formula><mml:math id="inf146"><mml:mi>W</mml:mi></mml:math></inline-formula>’s can be defined using neuronal indices of post- and presynaptic neurons as<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> without segregating excitation and inhibition in <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig6">6</xref>, <inline-formula><mml:math id="inf148"><mml:mi>N</mml:mi></mml:math></inline-formula> is the population size of sensory module, <inline-formula><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is the sum of a constant global inhibition and a short-range excitatory connection as<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>I,</mml:mtext><mml:mtext>s</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>E,s</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>E,s</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the heterogeneity degree of excitatory connectivity, and <inline-formula><mml:math id="inf152"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is the width of local excitatory connections.</p><p>When we segregated excitation and inhibition and considered the heterogeneity of inhibitory connection in <xref ref-type="fig" rid="fig7">Figures 7</xref>—<xref ref-type="fig" rid="fig10">10</xref>, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> is replaced with<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>I,s</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>E,</mml:mtext><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>E,s</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the degree of heterogeneity of inhibitory connections. Note the signs of modulation change in <xref ref-type="disp-formula" rid="equ6 equ7">Equations 6 and 7</xref> such that when only excitation is modulated in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, the connectivity strengths near the obliques are strong. In contrast, when excitation and inhibition are both modulated in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, the connectivity strengths near cardinal orientations are strong.</p><p>For the memory module, <inline-formula><mml:math id="inf154"><mml:mi>N</mml:mi></mml:math></inline-formula> is the population size of the memory module, <inline-formula><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>. Without heterogeneity in <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig10">10</xref>, <inline-formula><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is defined as<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>I,m</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>E,</mml:mtext><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>E,m</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In contrast, for the one-module network model in <xref ref-type="fig" rid="fig8">Figure 8</xref>, the connectivity of the memory module is heterogeneous, as in the sensory module in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, and is defined as<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>I,m</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>E,</mml:mtext><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>E,m</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The feedforward and feedback connectivity are similarly defined as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>f</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>f</mml:mtext></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mtext>m</mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mtext>s</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>b</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>b</mml:mtext></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mtext>s</mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mtext>m</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Note the connectivity strength is normalized by the size of the presynaptic population so that the total synaptic current remains the same for different population sizes.</p><p>For the external inputs with orientation <inline-formula><mml:math id="inf157"><mml:mi>θ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf158"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext,s</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> in the sensory module is modeled as<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>ext,s</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>ε</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>ext,s</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf159"><mml:mi>ε</mml:mi><mml:mo>∈</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula> determines the stimulus tuning of the input, <inline-formula><mml:math id="inf160"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext,s</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> determines the width, and <inline-formula><mml:math id="inf161"><mml:mi>C</mml:mi></mml:math></inline-formula> describes the contrast (<xref ref-type="bibr" rid="bib24">Hansel and Sompolinsky, 1998</xref>).</p><p>For the memory network not connected to the sensory module in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig8">8</xref>, we assumed stimulus-specific input as<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>ext, m</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf162"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a constant background input. When the memory module receives the inputs from the sensory population in <xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig10">10</xref>, we assumed <inline-formula><mml:math id="inf163"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext, m</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is constant as <inline-formula><mml:math id="inf164"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-5"><title>Analysis of network activities</title><p>We used population vector decoding to extract the internal representation of orientation and quantified how such representation deviated from the original stimulus. We also examined how tuning properties and Fisher information change during the delay period.</p><p>Note that while we indexed neurons uniformly with <inline-formula><mml:math id="inf165"><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between 0° and 180°, the maximum of the tuning curve of neuron <inline-formula><mml:math id="inf166"><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can change dynamically and differ from <inline-formula><mml:math id="inf167"><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We defined the preferred feature (PF) of neuron <inline-formula><mml:math id="inf168"><mml:mi>i</mml:mi></mml:math></inline-formula> as the maximum of its tuning curve when the tuning curve reaches a steady state in the presence of external input. For numerical estimation, we set the stimulus-present encoding epoch to 5 s to obtain the steady states of tuning curves. The tuning width is given as the full width at half maximum (FWHM) of the tuning curve. To estimate PF and FWHM, we did a cubic spline interpolation to increase the number of sample orientations to 1000. The tuning width index (WI) is given as<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>W</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To estimate the internal representation of orientation in the network models, denoted as <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we utilized the population vector decoder (<xref ref-type="bibr" rid="bib19">Georgopoulos et al., 1986</xref>)<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mover><mml:mi>θ</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf170"><mml:mi>N</mml:mi></mml:math></inline-formula> denotes the number of neurons and <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the PF of neuron <inline-formula><mml:math id="inf172"><mml:mi>j</mml:mi></mml:math></inline-formula>. The orientation is always decoded from the memory network tuning curves <inline-formula><mml:math id="inf173"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> except for <xref ref-type="fig" rid="fig10">Figure 10A</xref>. The estimation bias <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since the bias is typically small enough, we computed the estimation standard deviation (SD) as the SD of bias using linear statistics. The SD index is defined as<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The Fisher information (FI) is estimated by assuming that the probability density function <inline-formula><mml:math id="inf175"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is Gaussian as<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf176"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula> denotes the variance of the firing rate of memory neuron <inline-formula><mml:math id="inf177"><mml:mi>i</mml:mi><mml:mo>.</mml:mo></mml:math></inline-formula> Thus, we can estimate the FI of memory neuron <inline-formula><mml:math id="inf178"><mml:mi>i</mml:mi></mml:math></inline-formula> based on the empirical mean and variance of the firing rate at time <inline-formula><mml:math id="inf179"><mml:mi>t</mml:mi></mml:math></inline-formula> as<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and the total FI is the summation of the FI of all memory neurons, given as <inline-formula><mml:math id="inf180"><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p></sec><sec id="s4-6"><title>Drift and diffusivity in network models</title><p>Although the modulation breaks the continuity of the ring attractor and forms two discrete attractors at the obliques, there is still a one-dimensional trajectory <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to which the noise-free dynamics quickly converge. We can linearize the system in the vicinity of this trajectory if the noise is small (<xref ref-type="bibr" rid="bib7">Burak and Fiete, 2012</xref>). Note that the dynamics of the synaptic variables in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> can be put into the following form:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mi>τ</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and by linearizing around the stable trajectory <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we get<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have ignored the zeroth- and higher-order terms. The drift velocity <inline-formula><mml:math id="inf183"><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is estimated by projecting the noise-free dynamics along the normalized right eigenvector <italic>u</italic> of <italic>K</italic> with the largest real part of the eigenvalue<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">∥</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The coefficient of diffusion can be obtained in the same way<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="fraktur">D</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∥</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">∥</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The noise coefficient is given as <inline-formula><mml:math id="inf184"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi mathvariant="script">D</mml:mi></mml:msqrt></mml:math></inline-formula>. Hence, we have reduced the high-dimensional dynamics to a simple one-dimensional stochastic differential equation as in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> as<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and the potential <inline-formula><mml:math id="inf185"><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is obtained by the relation <inline-formula><mml:math id="inf186"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. To quantitatively measure the heterogeneity of noise coefficient across different orientations, we define the noise coefficient index as follows:<disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>45</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-7"><title>Network parameters and simulations</title><p>Unless otherwise specified, <inline-formula><mml:math id="inf187"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo> </mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The connectivity parameters are <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>E,</mml:mtext><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>I,s</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.35</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>E,m</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>I,m</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>f</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mtext>b</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>E,s</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.36</mml:mn><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>I,s</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.1</mml:mn><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mi>π</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the external input, we set <inline-formula><mml:math id="inf191"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>ext,s</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the modulation of the sensory network, unless otherwise specified, we set <inline-formula><mml:math id="inf193"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:math></inline-formula> when only the excitatory plasticity is considered, and <inline-formula><mml:math id="inf194"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.08</mml:mn></mml:math></inline-formula> when the inhibitory plasticity is added. As for the modulation of the single-layer memory network, we set <inline-formula><mml:math id="inf195"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>2.4</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. For the transfer function, <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for sensory <inline-formula><mml:math id="inf197"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mtext>s</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>6.6</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for memory <inline-formula><mml:math id="inf199"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>We uniformly sampled 50 cue orientations in <inline-formula><mml:math id="inf200"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>180</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>. The visual cue lasts for 0.5 s except for the estimation of the PFs. In the grid parameter search figures, the delay epochs last for 1 s. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, we set <inline-formula><mml:math id="inf201"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.07</mml:mn></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig5">Figure 5A</xref>, the manifold corresponds to the synaptic variables at 4 s into the delay with <inline-formula><mml:math id="inf202"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>. We uniformly sampled 100 cue orientations for the manifold.</p><p>To compute the drift velocity and noise coefficient in <xref ref-type="fig" rid="fig5">Figures 5</xref>, <xref ref-type="fig" rid="fig6">6</xref>, and <xref ref-type="fig" rid="fig9">9</xref>, we use the stable trajectory <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at 1 s into the delay to ensure the fast transient dynamics induced by stimulus offset fully decays. The stable trajectory is parameterized by the 50 cue orientations to numerically compute <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>All simulations of ordinary or stochastic differential equations of the network models were done using the Euler method with <inline-formula><mml:math id="inf205"><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:math></inline-formula>. We checked that similar results hold for smaller <inline-formula><mml:math id="inf206"><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:math></inline-formula>. Example bias and standard deviation patterns were estimated from 1000 independent realizations. The Fisher information patterns were estimated from 3000 independent realizations. The grid search of maximum bias at <inline-formula><mml:math id="inf207"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>22.5</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and standard deviation index were computed from 3000 realizations.</p><p>All simulations were run in MATLAB. The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/KYang-N/Cardinal-Repulsion">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib62">Yang, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Visualization, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-95160-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/KYang-N/Cardinal-Repulsion">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib62">Yang, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We appreciate X Wei for sharing the code for Bayesian inference models. JY was supported by the NYU Shanghai Summer Undergraduate Research Program (SURP). SL received STI2030-Major Projects, No. 2021ZD0203700/2021ZD0203705. HZ and SL also acknowledge the support of the Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning and the NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Adam</surname><given-names>KCS</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>Evidence for, and challenges to, sensory recruitment models of visual working memory</chapter-title><person-group person-group-type="editor"><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Bainbridge</surname><given-names>WA</given-names></name></person-group><source>Visual Memory</source><publisher-name>Routledge</publisher-name><fpage>5</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.4324/9781003158134-2</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>GY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural evidence for categorical biases in location and orientation representations in a working memory task</article-title><source>NeuroImage</source><volume>240</volume><elocation-id>118366</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118366</pub-id><pub-id pub-id-type="pmid">34242785</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noise in neural populations accounts for errors in working memory</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>3632</fpage><lpage>3645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3204-13.2014</pub-id><pub-id pub-id-type="pmid">24599462</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>P</given-names></name><name><surname>Schneegans</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Brady</surname><given-names>TF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Representation and Computation in Working Memory</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/kubr9</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchacourt</surname><given-names>F</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A flexible model of working memory</article-title><source>Neuron</source><volume>103</volume><fpage>147</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.020</pub-id><pub-id pub-id-type="pmid">31103359</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id><pub-id pub-id-type="pmid">19229307</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fundamental limits on persistent activity in networks of noisy neurons</article-title><source>PNAS</source><volume>109</volume><fpage>17645</fpage><lpage>17650</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117386109</pub-id><pub-id pub-id-type="pmid">23047704</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>An adaptive perspective on visual working memory distortions</article-title><source>Journal of Experimental Psychology. General</source><volume>151</volume><fpage>2300</fpage><lpage>2323</lpage><pub-id pub-id-type="doi">10.1037/xge0001191</pub-id><pub-id pub-id-type="pmid">35191726</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Reynolds</surname><given-names>JR</given-names></name><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Repovs</surname><given-names>G</given-names></name><name><surname>Anticevic</surname><given-names>A</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1348</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1038/nn.3470</pub-id><pub-id pub-id-type="pmid">23892552</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname><given-names>A</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>910</fpage><lpage>923</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.9.910</pub-id><pub-id pub-id-type="pmid">10982751</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darshan</surname><given-names>R</given-names></name><name><surname>Rivkind</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learning to represent continuous variables in heterogeneous neural networks</article-title><source>Cell Reports</source><volume>39</volume><elocation-id>110612</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.110612</pub-id><pub-id pub-id-type="pmid">35385721</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Kouider</surname><given-names>S</given-names></name><name><surname>Sackur</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An oblique illusion modulated by visibility: non-monotonic sensory integration in orientation processing</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1167/10.10.6</pub-id><pub-id pub-id-type="pmid">20884471</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title><source>Neuroscience</source><volume>01</volume><elocation-id>e3870</elocation-id><pub-id pub-id-type="doi">10.1101/2022.08.15.503870</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eissa</surname><given-names>TL</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning efficient representations of environmental priors in working memory</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011622</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011622</pub-id><pub-id pub-id-type="pmid">37943956</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>International Joint Conference on Neural Networks (IJCNN)</article-title><conf-name>The 2010 International 900 Joint Conference on Neural Networks (IJCNN</conf-name><conf-loc>Barcelona, Spain</conf-loc></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>SM</given-names></name><name><surname>Greene</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Concepts and compositionality: in search of the brain’s language of thought</article-title><source>Annual Review of Psychology</source><volume>71</volume><fpage>273</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122216-011829</pub-id><pub-id pub-id-type="pmid">31550985</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>D</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title><source>Neural Computation</source><volume>26</volume><fpage>2103</fpage><lpage>2134</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id><pub-id pub-id-type="pmid">25058702</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual perception and the statistical properties of natural scenes</article-title><source>Annual Review of Psychology</source><volume>59</volume><fpage>167</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.58.110405.085632</pub-id><pub-id pub-id-type="pmid">17705683</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Kettner</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Neuronal population coding of movement direction</article-title><source>Science</source><volume>233</volume><fpage>1416</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1126/science.3749885</pub-id><pub-id pub-id-type="pmid">3749885</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>AR</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>J</given-names></name><name><surname>Lim</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unsupervised learning for robust working memory</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009083</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009083</pub-id><pub-id pub-id-type="pmid">35500033</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Lim</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>HJ</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Choe</surname><given-names>M</given-names></name><name><surname>Yoo</surname><given-names>DG</given-names></name><name><surname>Ryu</surname><given-names>J</given-names></name><name><surname>Lim</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Decision-consistent bias mediated by drift dynamics of human visual working memory</article-title><source>Neuroscience</source><volume>01</volume><elocation-id>e6818</elocation-id><pub-id pub-id-type="doi">10.1101/2023.06.28.546818</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahn</surname><given-names>M</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A unifying theory explains seemingly contradictory biases in perceptual estimation</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>793</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1038/s41593-024-01574-x</pub-id><pub-id pub-id-type="pmid">38360947</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>Modeling feature selectivity in local cortical circuits</chapter-title><person-group person-group-type="editor"><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name></person-group><source>Methods in Neuronal Modeling: From Ions to Networks</source><publisher-name>MIT Press</publisher-name><fpage>499</fpage><lpage>567</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>Mato</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Short-term plasticity explains irregular persistent activity in working memory tasks</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>133</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3455-12.2013</pub-id><pub-id pub-id-type="pmid">23283328</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardman</surname><given-names>KO</given-names></name><name><surname>Vergauwe</surname><given-names>E</given-names></name><name><surname>Ricker</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Categorical working memory representations are used in delayed estimation of continuous colors</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>43</volume><fpage>30</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1037/xhp0000290</pub-id><pub-id pub-id-type="pmid">27797548</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itskov</surname><given-names>V</given-names></name><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Short-term facilitation may stabilize parametric working memory trace</article-title><source>Frontiers in Computational Neuroscience</source><volume>5</volume><elocation-id>40</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2011.00040</pub-id><pub-id pub-id-type="pmid">22028690</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>AG</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Chadwick</surname><given-names>A</given-names></name><name><surname>Blot</surname><given-names>A</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct learning-induced changes in stimulus selectivity and interactions of GABAergic interneuron classes in visual cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>851</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0143-z</pub-id><pub-id pub-id-type="pmid">29786081</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khona</surname><given-names>M</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Attractor and integrator networks in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>23</volume><fpage>744</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1038/s41583-022-00642-0</pub-id><pub-id pub-id-type="pmid">36329249</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreile</surname><given-names>AK</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Altered visual experience induces instructive changes of orientation preference in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>13911</fpage><lpage>13920</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2143-11.2011</pub-id><pub-id pub-id-type="pmid">21957253</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwak</surname><given-names>Y</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unveiling the abstract format of mnemonic representations</article-title><source>Neuron</source><volume>110</volume><fpage>1822</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.016</pub-id><pub-id pub-id-type="pmid">35395195</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larisch</surname><given-names>R</given-names></name><name><surname>Gönner</surname><given-names>L</given-names></name><name><surname>Teichmann</surname><given-names>M</given-names></name><name><surname>Hamker</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensory coding and contrast invariance emerge from the control of plastic inhibition over emergent selectivity</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009566</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009566</pub-id><pub-id pub-id-type="pmid">34843455</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leavitt</surname><given-names>ML</given-names></name><name><surname>Mendoza-Halliday</surname><given-names>D</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sustained activity encoding working memories: not fully distributed</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>328</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.04.004</pub-id><pub-id pub-id-type="pmid">28515011</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Peterson</surname><given-names>MR</given-names></name><name><surname>Freeman</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Oblique effect: a neural basis in the visual cortex</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>204</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1152/jn.00954.2002</pub-id><pub-id pub-id-type="pmid">12611956</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mejías</surname><given-names>JF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mechanisms of distributed working memory in a large-scale network of macaque neocortex</article-title><source>eLife</source><volume>11</volume><elocation-id>e72136</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.72136</pub-id><pub-id pub-id-type="pmid">35200137</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Error-correcting dynamics in visual working memory</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3366</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11298-3</pub-id><pub-id pub-id-type="pmid">31358740</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollock</surname><given-names>E</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Engineering recurrent neural networks from task-relevant manifolds and dynamics</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008128</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008128</pub-id><pub-id pub-id-type="pmid">32785228</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pratte</surname><given-names>MS</given-names></name><name><surname>Park</surname><given-names>YE</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accounting for stimulus-specific variation in precision reveals a discrete capacity limit in visual working memory</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>43</volume><fpage>6</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1037/xhp0000302</pub-id><pub-id pub-id-type="pmid">28004957</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id><pub-id pub-id-type="pmid">31263205</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Song</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Robust spatial working memory through homeostatic synaptic scaling in heterogeneous cortical networks</article-title><source>Neuron</source><volume>38</volume><fpage>473</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00255-1</pub-id><pub-id pub-id-type="pmid">12741993</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roussy</surname><given-names>M</given-names></name><name><surname>Mendoza-Halliday</surname><given-names>D</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural substrates of visual perception and working memory: two sides of the same coin or two different coins?</article-title><source>Frontiers in Neural Circuits</source><volume>15</volume><elocation-id>764177</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2021.764177</pub-id><pub-id pub-id-type="pmid">34899197</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneegans</surname><given-names>S</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Drift in neural population activity causes working memory to deteriorate over time</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>4859</fpage><lpage>4869</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3440-17.2018</pub-id><pub-id pub-id-type="pmid">29703786</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeholzer</surname><given-names>A</given-names></name><name><surname>Deger</surname><given-names>M</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stability of working memory in continuous attractor networks under the control of short-term plasticity</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006928</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006928</pub-id><pub-id pub-id-type="pmid">31002672</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Reis</surname><given-names>BY</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The autapse: a simple illustration of short-term analog memory storage by tuned synaptic feedback</article-title><source>Journal of Computational Neuroscience</source><volume>9</volume><fpage>171</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1023/a:1008971908649</pub-id><pub-id pub-id-type="pmid">11030520</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>G</given-names></name><name><surname>Tao</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>EL</given-names></name><name><surname>Chino</surname><given-names>YM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Oblique effect in visual area 2 of macaque monkeys</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/14.2.3</pub-id><pub-id pub-id-type="pmid">24511142</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Near-decomposability and complexity: how a mind resides in a brain</chapter-title><person-group person-group-type="editor"><name><surname>Morowitz</surname><given-names>H</given-names></name><name><surname>Singer</surname><given-names>J</given-names></name></person-group><source>The Mind, the Brain, and Complex Adaptive Systems</source><publisher-name>Addison-Wesley</publisher-name><fpage>25</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.4324/9780429492761-3</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>R</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient coding in visual working memory accounts for stimulus-specific variations in recall</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7132</fpage><lpage>7142</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1018-18.2018</pub-id><pub-id pub-id-type="pmid">30006363</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tomić</surname><given-names>I</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A Dynamic Neural Resource Model Bridges Sensory and Working Memory</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.27.534406</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Bergen</surname><given-names>RS</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Pratte</surname><given-names>MS</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory uncertainty decoded from visual cortex predicts behavior</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1728</fpage><lpage>1730</lpage><pub-id pub-id-type="doi">10.1038/nn.4150</pub-id><pub-id pub-id-type="pmid">26502262</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname><given-names>R</given-names></name><name><surname>Shin</surname><given-names>H</given-names></name><name><surname>Chou</surname><given-names>WC</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Variability in encoding precision accounts for visual short-term memory limitations</article-title><source>PNAS</source><volume>109</volume><fpage>8780</fpage><lpage>8785</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117465109</pub-id><pub-id pub-id-type="pmid">22582168</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>V</given-names></name><name><surname>Jacobs</surname><given-names>C</given-names></name><name><surname>Sack</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Topographic contribution of early visual cortex to short-term memory consolidation: a transcranial magnetic stimulation study</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>4</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3261-11.2012</pub-id><pub-id pub-id-type="pmid">22219265</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Doyon</surname><given-names>N</given-names></name><name><surname>Gilson</surname><given-names>M</given-names></name><name><surname>Haas</surname><given-names>JS</given-names></name><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Maffei</surname><given-names>A</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name><name><surname>Wierenga</surname><given-names>CJ</given-names></name><name><surname>Woodin</surname><given-names>MA</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Inhibitory synaptic plasticity: spike timing-dependence and putative network function</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>119</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00119</pub-id><pub-id pub-id-type="pmid">23882186</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual adaptation</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>547</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035509</pub-id><pub-id pub-id-type="pmid">26858985</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>XX</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Bayesian observer model constrained by efficient coding can explain “anti-Bayesian” percepts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1038/nn.4105</pub-id><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>XX</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Lawful relation between perceptual bias and discriminability</article-title><source>PNAS</source><volume>114</volume><fpage>10244</fpage><lpage>10249</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619153114</pub-id><pub-id pub-id-type="pmid">28874578</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Spikes, Decisions, and Actions: The Dynamical Foundations of Neuroscience</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>K</given-names></name><name><surname>Nykamp</surname><given-names>DQ</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1038/nn.3645</pub-id><pub-id pub-id-type="pmid">24487232</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revisit once more the sensory storage account of visual working memory</article-title><source>Visual Cognition</source><volume>28</volume><fpage>433</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1080/13506285.2020.1818659</pub-id><pub-id pub-id-type="pmid">33841024</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0310-2</pub-id><pub-id pub-id-type="pmid">30643294</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Sensory-Memory Interactions in Visual Working Memory</data-title><version designator="swh:1:rev:50907de7466e34572636c4338daeca9c66f5f370">swh:1:rev:50907de7466e34572636c4338daeca9c66f5f370</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:899a1b8690f2e97e1d9bd31824a1d16931b9c886;origin=https://github.com/KYang-N/Cardinal-Repulsion;visit=swh:1:snp:b3f449de787a8897d95a27eebf613850e826b86f;anchor=swh:1:rev:50907de7466e34572636c4338daeca9c66f5f370">https://archive.softwareheritage.org/swh:1:dir:899a1b8690f2e97e1d9bd31824a1d16931b9c886;origin=https://github.com/KYang-N/Cardinal-Repulsion;visit=swh:1:snp:b3f449de787a8897d95a27eebf613850e826b86f;anchor=swh:1:rev:50907de7466e34572636c4338daeca9c66f5f370</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-06-02112.1996</pub-id><pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Murphy</surname><given-names>JT</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell receptive fields</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002250</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002250</pub-id><pub-id pub-id-type="pmid">22046123</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95160.4.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>UT Austin</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> computational study provides new insights into how neural dynamics may lead to time-evolving behavioral errors as observed in certain working-memory tasks. By combining ideas from efficient coding and attractor neural networks, the authors construct a two-module network model to capture the sensory-memory interactions and the distributed nature of working memory representations. They provide <bold>convincing</bold> evidence supporting that their two-module network, although none of the alternative circuit structures they considered can account for error patterns reported in orientation-estimation tasks with delays.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95160.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Working memory is imperfect - memories accrue error over time and are biased towards certain identities. For example, previous work has shown memory for orientation is more accurate near the cardinal directions (i.e., variance in responses is smaller for horizontal and vertical stimuli) while being biased towards diagonal orientations (i.e., there is a repulsive bias away from horizontal and vertical stimuli). The magnitude of errors and biases increase the longer an item is held in working memory and when more items are held in working memory (i.e., working memory load is higher). Previous work has argued that biases and errors could be explained by increased perceptual acuity at cardinal directions. However, these models are constrained to sensory perception and do not explain how biases and errors increase over time in memory. The current manuscript builds on this work to show how a two-layer neural network could integrate errors and biases over a memory delay. In brief, the model includes a 'sensory' layer with heterogenous connections that lead to the repulsive bias and decreased error at the cardinal directions. This layer is then reciprocally connected with a classic ring attractor layer. Through their reciprocal interactions, the biases in the sensory layer are constantly integrated into the representation in memory. In this way, the model captures the distribution of biases and errors for different orientations that has been seen in behavior and their increasing magnitude with time. The authors compare the two-layer network to a simpler one-network model, showing that the one model network is harder to tune and shows an attractive bias for memories that have lower error (which is incompatible with empirical results).</p><p>Strengths:</p><p>The manuscript provides a nice review of the dynamics of items in working memory, showing how errors and biases differ across stimulus space. The two-layer neural network model is able to capture the behavioral effects as well as relate to neurophysiological observations that memory representations are distributed across sensory cortex and prefrontal cortex.</p><p>The authors use multiple approaches to understand how the network produces the observed results. For example, analyzing the dynamics of memories in the low-dimensional representational space of the networks provides the reader with an intuition for the observed effects.</p><p>As a point of comparison with the two-layer network, the authors construct a heterogenous one-layer network (analogous to a single memory network with embedded biases). They argue that such a network is incapable of capturing the observed behavioral effects but could potentially explain biases and noise levels in other sensory domains where attractive biases have lower errors (e.g., color).</p><p>The authors show how changes in the strength of Hebbian learning of excitatory and inhibitory synapses can change network behavior. This argues for relatively stronger learning in inhibitory synapses, an interesting prediction.</p><p>The manuscript is well-written. In particular, the figures are well done and nicely schematize the model and the results.</p><p>Weaknesses:</p><p>Despite its strengths, the manuscript does have some weaknesses. These weaknesses are adequately discussed in the manuscript and motivate future research.</p><p>One weakness is that the model is not directly fit to behavioral data, but rather compared to a schematic of behavioral data. As noted above, the model provides insight into the general phenomenon of biases in working memory. However, because the models are not fit directly to data, they may miss some aspects of the data.</p><p>In addition, directly fitting the models to behavioral data could allow for a broader exploration of parameter space for both the one-layer and two-layer models (and their alternatives). Such an approach would provide stronger support for the papers claims (such as &quot;....these evolving errors...require network interaction between two distinct modules.&quot;). That being said, the manuscript does explore several alternative models and also acknowledges the limitation of not directly fitting behavior, due to difficulties in fitting complex neural network models to data.</p><p>One important behavioral observation is that both diffusive noise and biases increase with the number of items in working memory. The current model does not capture these effects and it isn't clear how the model architecture could be extended to capture these effects. That being said, the authors note this limitation in the Discussion and present it as a future direction.</p><p>Overall:</p><p>Overall, the manuscript was successful in building a model that captured the biases and noise observed in working memory. This work complements previous studies that have viewed these effects through the lens of optimal coding, extending these models to explain the effects of time in memory. In addition, the two-layer network architecture extends previous work with similar architectures, adding further support to the distributed nature of working memory representations.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95160.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this manuscript, Yang et al. present a modeling framework to understand the pattern of response biases and variance observed in delayed-response orientation estimation tasks. They combine a series of modeling approaches to show that coupled sensory-memory networks are in a better position than single-area models to support experimentally observed delay-dependent response bias and variance in cardinal compared to oblique orientations. These errors can emerge from a population-code approach that implements efficient coding and Bayesian inference principles and is coupled to a memory module that introduces random maintenance errors. A biological implementation of such operation is found when coupling two neural network modules, a sensory module with connectivity inhomogeneities that reflect environment priors, and a memory module with strong homogeneous connectivity that sustains continuous ring attractor function. Comparison with single-network solutions that combine both connectivity inhomogeneities and memory attractors shows that two-area models can more easily reproduce the patterns of errors observed experimentally.</p><p>Strengths:</p><p>The model provides an integration of two modeling approaches to the computational bases of behavioral biases: one based on Bayesian and efficient coding principles, and one based on attractor dynamics. These two perspectives are not usually integrated consistently in existing studies, which this manuscript beautifully achieves. This is a conceptual advancement, especially because it brings together the perceptual and memory components of common laboratory tasks.</p><p>The proposed two-area model provides a biologically plausible implementation of efficient coding and Bayesian inference principles, which interact seamlessly with a memory buffer to produce a complex pattern of delay-dependent response errors. No previous model had achieved this.</p><p>Weaknesses:</p><p>The correspondence between the various computational models is not clearly shown. It is not easy to see clearly this correspondence because network function is illustrated with different representations for different models. In particular, the Bayesian model of Figure 2 is illustrated with population responses for different stimuli and delays, while the attractor models of Figure 3 and 4 are illustrated with neuronal tuning curves but not population activity.</p><p>The proposed model has stronger feedback than feedforward connections between the sensory and memory modules (J_f = 0.1 and J_b = 0.25). This is not the common assumption when thinking about hierarchical processing in the brain. The manuscript argues that error patterns remain similar as long as the product of J_f and J_b is constant, so it is unclear why the authors preferred this network example as opposed to one with J_b = 0.1 and J_f = 0.25.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95160.4.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The present study proposes a neural circuit model consisting of coupled sensory and memory networks to explain the circuit mechanism of the cardinal effect in orientation perception which is characterized by the bias towards the oblique orientation and the largest variance at the oblique orientation.</p><p>Strengths:</p><p>The authors have done numerical simulations and preliminary analysis of the neural circuit model to show the model successfully reproduces the cardinal effect. And the paper is well-written overall. As far as I know, most of the studies on the cardinal effect are at the level of statistical models, and the current study provides one possibility of how neural circuit models reproduce such an effect.</p><p>Weaknesses:</p><p>There are no major weaknesses and flaws in the present study, although I suggest the author conduct further analysis to deepen our understanding of the circuit mechanism of the cardinal effects.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95160.4.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Jun</given-names></name><role specific-use="author">Author</role><aff><institution>Tsinghua University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Hanqi</given-names></name><role specific-use="author">Author</role><aff><institution>New York University Shanghai</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Lim</surname><given-names>Sukbin</given-names></name><role specific-use="author">Author</role><aff><institution>New York University Shanghai</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>I appreciate the revisions made by the author which address all of my concerns.</p><p>Nevertheless, I have some new questions when I read the paper again. These questions are not necessarily criticisms of the paper, which may reflect the gap in my understanding. Meanwhile, it also reflects the writing might be improved further.</p><p>- Fig. 1:</p><p>I understand that a critical assumption for generating the required result is that the oblique orientation has lower &quot;energy&quot; than the cardinal orientation (Fig. 1G). Meanwhile, I always have a concept that typically the energy is defined as the negative of log probability. If we take the log probability plotted in Fig. 1A, that will generate an energy landscape that is upside down compared with current Fig. 1G. How should I understand this discrepancy?</p></disp-quote><p>As the reviewer pointed out, a higher prior distribution near cardinal orientations causes cardinal attraction in typical Bayesian models, which can correspond to lower energy around these orientations. Additionally, in the context of learning natural statistics, Hebbian plasticity in excitatory connections strengthens recurrent connections and drives attraction toward more prevalent stimuli within neural circuits.</p><p>However, as demonstrated by Wei and Stocker (2015), Bayesian inference model can also produce cardinal repulsion when optimizing encoding efficiency. In our network, this efficient encoding is achieved through heterogeneous lateral connections and inhibitory Hebbian plasticity in the sensory module, resulting in lower energy near oblique orientations. Thus, the shape of prior distribution does not have a direct one-to-one correspondence with the bias pattern or the dynamic energy landscape.</p><disp-quote content-type="editor-comment"><p>- Fig. 3 and its corresponding text.</p><p>I understand and agree the Fig. 3B&amp;C that neurons near cardinal orientations are shaper and denser. But why the stimulus representation around cardinal orientations are sparser compared with the oblique orientation? Isn't more neurons around cardinal orientation implying a less sparser representation?</p></disp-quote><p>Indeed, with sharper tuning curves, having more neurons can result in a sparser representation. Consider an extreme case where each orientation, discretized by 1°, is represented by only one active neuron with a tuning width of 1°. While this would require more neurons to represent overall stimuli compared to cases with wider tuning curves, each stimulus would be represented by fewer neurons, aligning with the traditional concept of sparse coding.</p><p>However, in Fig. 3 and corresponding text, we did not measure the sparseness of active neurons for each orientation. Instead, we used the term ‘sparser representation’ to describe the increased distance between representations of different stimuli near the cardinal orientations. Although this increased distance can be consistent with the traditional concept of sparse coding, to avoid any confusion, we have revised the term ‘sparser representation’ to ‘more dispersed representation’ in the 3rd paragraph in pg. 5 and the 3rd paragraph in pg. 6.</p></body></sub-article></article>