<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82914</article-id><article-id pub-id-type="doi">10.7554/eLife.82914</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Task-dependent optimal representations for cerebellar learning</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-290943"><name><surname>Xie</surname><given-names>Marjorie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1456-4811</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290944"><name><surname>Muscinelli</surname><given-names>Samuel P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5256-2289</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273236"><name><surname>Decker Harris</surname><given-names>Kameron</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3716-6173</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-268861"><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2422-6576</contrib-id><email>a.litwin-kumar@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05wn7r715</institution-id><institution>Department of Computer Science, Western Washington University</institution></institution-wrap><addr-line><named-content content-type="city">Bellingham</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Western University</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>09</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82914</elocation-id><history><date date-type="received" iso-8601-date="2022-08-22"><day>22</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-09-05"><day>05</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-08-15"><day>15</day><month>08</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.15.504040"/></event></pub-history><permissions><copyright-statement>© 2023, Xie et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Xie et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82914-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82914-figures-v2.pdf"/><abstract><p>The cerebellar granule cell layer has inspired numerous theoretical models of neural representations that support learned behaviors, beginning with the work of Marr and Albus. In these models, granule cells form a sparse, combinatorial encoding of diverse sensorimotor inputs. Such sparse representations are optimal for learning to discriminate random stimuli. However, recent observations of dense, low-dimensional activity across granule cells have called into question the role of sparse coding in these neurons. Here, we generalize theories of cerebellar learning to determine the optimal granule cell representation for tasks beyond random stimulus discrimination, including continuous input-output transformations as required for smooth motor control. We show that for such tasks, the optimal granule cell representation is substantially denser than predicted by classical theories. Our results provide a general theory of learning in cerebellum-like systems and suggest that optimal cerebellar representations are task-dependent.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>representation</kwd><kwd>learning</kwd><kwd>cerebellum</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32-NS06492</award-id><principal-award-recipient><name><surname>Xie</surname><given-names>Marjorie</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Muscinelli</surname><given-names>Samuel P</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Swartz Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Muscinelli</surname><given-names>Samuel P</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001906</institution-id><institution>Washington Research Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Decker Harris</surname><given-names>Kameron</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000861</institution-id><institution>Burroughs Wellcome Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><award-id>GAT3708</award-id><principal-award-recipient><name><surname>Xie</surname><given-names>Marjorie</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI-1707398</award-id><principal-award-recipient><name><surname>Xie</surname><given-names>Marjorie</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Classical theoretical approaches to studying the cerebellar cortex are generalized to a broader set of learning tasks, revealing that the optimal value for the sparsity of granule cell activity is task-dependent.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A striking property of cerebellar anatomy is the vast expansion in number of granule cells compared to the mossy fibers that innervate them (<xref ref-type="bibr" rid="bib30">Eccles et al., 1967</xref>). This anatomical feature has led to the proposal that the function of the granule cell layer is to produce a high-dimensional representation of lower-dimensional mossy fiber activity (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib22">Cayco-Gajic and Silver, 2019</xref>). In such theories, granule cells integrate information from multiple mossy fibers and respond in a nonlinear manner to different input combinations. Detailed theoretical analysis has argued that anatomical parameters such as the ratio of granule cells to mossy fibers (<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>), the number of inputs received by individual granule cells (<xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>), and the distribution of granule-cell-to-Purkinje-cell synaptic weights <xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref> have quantitative values that maximize the dimension of the granule cell representation and learning capacity. Sparse activity, which increases dimension, is also cited as a key property of this representation (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; but see <xref ref-type="bibr" rid="bib79">Spanne and Jörntell, 2015</xref>). Sparsity affects both learning speed (<xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>) and generalization, the ability to predict correct labels for previously unseen inputs (<xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>).</p><p>Theories that study the effects of dimension on learning typically focus on the ability of a system to perform categorization tasks with random, high-dimensional inputs (<xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>). In this case, increasing the dimension of the granule cell representation increases the number of inputs that can be discriminated. However, cerebellar circuits participate in diverse behaviors, including dexterous movements, inter-limb coordination, the formation of internal models, and cognitive behaviors (<xref ref-type="bibr" rid="bib43">Ito and Itō, 1984</xref>; <xref ref-type="bibr" rid="bib86">Wolpert et al., 1998</xref>; <xref ref-type="bibr" rid="bib80">Strick et al., 2009</xref>). Cerebellum-like circuits, such as the insect mushroom body and the electrosensory system of electric fish, support other functions such as associative learning (<xref ref-type="bibr" rid="bib56">Modi et al., 2020</xref>) and the cancellation of self-generated sensory signals (<xref ref-type="bibr" rid="bib47">Kennedy et al., 2014</xref>), respectively. This diversity raises the question of whether learning high-dimensional categorization tasks is a sufficient framework for probing the function of granule cells and their analogs.</p><p>Several recent studies have reported dense activity in cerebellar granule cells in response to sensory stimulation or during motor control tasks (<xref ref-type="bibr" rid="bib46">Jörntell and Ekerot, 2006</xref>; <xref ref-type="bibr" rid="bib48">Knogler et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Wagner et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Giovannucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Badura and De Zeeuw, 2017</xref>; <xref ref-type="bibr" rid="bib84">Wagner et al., 2019</xref>), at odds with classical theories (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>). Moreover, there is evidence that granule cell firing rates differ across cerebellar regions (<xref ref-type="bibr" rid="bib39">Heath et al., 2014</xref>; <xref ref-type="bibr" rid="bib85">Witter and De Zeeuw, 2015</xref>). In contrast to this reported dense activity in cerebellar granule cells, odor responses in Kenyon cells, the analogs of granule cells in the <italic>Drosophila</italic> mushroom body, are sparse, with 5–10% of neurons responding to odor stimulation (<xref ref-type="bibr" rid="bib81">Turner et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Honegger et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Lin et al., 2014</xref>).</p><p>We propose that these differences can be explained by the capacity of representations with different levels of sparsity to support learning of different tasks. We show that the optimal level of sparsity depends on the structure of the input-output relationship of a task. When learning input-output mappings for motor control tasks, the optimal granule cell representation is much denser than predicted by previous analyses. To explain this result, we develop an analytic theory that predicts the performance of cerebellum-like circuits for arbitrary learning tasks. The theory describes how properties of cerebellar architecture and activity control these networks’ inductive bias: the tendency of a network toward learning particular types of input-output mappings (<xref ref-type="bibr" rid="bib77">Sollich, 1998</xref>; <xref ref-type="bibr" rid="bib44">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021b</xref>; <xref ref-type="bibr" rid="bib76">Simon et al., 2021</xref>). The theory shows that inductive bias, rather than the dimension of the representation alone, is necessary to explain learning performance across tasks. It also suggests that cerebellar regions specialized for different functions may adjust the sparsity of their granule cell representations depending on the task.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In our model, a granule cell layer of <inline-formula><mml:math id="inf1"><mml:mi>M</mml:mi></mml:math></inline-formula> neurons receives connections from a random subset of <inline-formula><mml:math id="inf2"><mml:mi>N</mml:mi></mml:math></inline-formula> mossy fiber inputs. Because <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≫</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> in the cerebellar cortex and cerebellum-like structures (approximately <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for the neurons presynaptic to a single Purkinje cell in the cat brain; <xref ref-type="bibr" rid="bib30">Eccles et al., 1967</xref>), we refer to the granule cell layer as the expansion layer and the mossy fiber layer as the input layer (<xref ref-type="fig" rid="fig1">Figure 1A and B</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic of cerebellar cortex model.</title><p>(<bold>A</bold>) Mossy fiber inputs (blue) project to granule cells (green), which send parallel fibers that contact a Purkinje cell (black). (<bold>B</bold>) Diagram of neural network model. <inline-formula><mml:math id="inf6"><mml:mi>D</mml:mi></mml:math></inline-formula> task variables are embedded, via a linear transformation <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, in the activity of <inline-formula><mml:math id="inf8"><mml:mi>N</mml:mi></mml:math></inline-formula> input layer neurons. Connections from the input layer to the expansion layer are described by a synaptic weight matrix <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>C</bold>) Illustration of task subspace. Points <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in a <inline-formula><mml:math id="inf11"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional space of task variables are embedded in a <inline-formula><mml:math id="inf12"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional subspace of the <inline-formula><mml:math id="inf13"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional input layer activity <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf15"><mml:mi>D</mml:mi></mml:math></inline-formula>=2, <inline-formula><mml:math id="inf16"><mml:mi>N</mml:mi></mml:math></inline-formula>=3 illustrated).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig1-v2.tif"/></fig><p>A typical assumption in computational theories of the cerebellar cortex is that inputs are randomly distributed in a high-dimensional space (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib14">Billings et al., 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>). While this may be a reasonable simplification in some cases, many tasks, including cerebellum-dependent tasks, are likely best-described as being encoded by a low-dimensional set of variables. For example, the cerebellum is often hypothesized to learn a forward model for motor control (<xref ref-type="bibr" rid="bib86">Wolpert et al., 1998</xref>), which uses sensory input and motor efference to predict an effector’s future state. Mossy fiber activity recorded in monkeys correlates with position and velocity during natural movement (<xref ref-type="bibr" rid="bib82">van Kan et al., 1993</xref>). Sources of motor efference copies include motor cortex, whose population activity lies on a low-dimensional manifold (<xref ref-type="bibr" rid="bib84">Wagner et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Huang et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Churchland et al., 2010</xref>; <xref ref-type="bibr" rid="bib88">Yu et al., 2009</xref>). We begin by modeling the low dimensionality of inputs and later consider more specific tasks.</p><p>We therefore assume that the inputs to our model lie on a <inline-formula><mml:math id="inf17"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional subspace embedded in the <inline-formula><mml:math id="inf18"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional input space, where <inline-formula><mml:math id="inf19"><mml:mi>D</mml:mi></mml:math></inline-formula> is typically much smaller than <inline-formula><mml:math id="inf20"><mml:mi>N</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We refer to this subspace as the ‘task subspace’ (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). A location in this subspace is described by a <inline-formula><mml:math id="inf21"><mml:mi>D</mml:mi></mml:math></inline-formula> dimensional vector <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, while the corresponding input layer activity is given by <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> an <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> matrix describing the embedding of the task variables in the input layer. An <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> effective weight matrix <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">A</mml:mi><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which describes the selectivity of expansion layer neurons to task variables, is determined by <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> input-to-expansion-layer synaptic weight matrix <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The activity of neurons in the expansion layer is given by:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf31"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is a rectified linear activation function <inline-formula><mml:math id="inf32"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> applied element-wise. Our results also hold for other threshold-polynomial activation functions. The scalar threshold <inline-formula><mml:math id="inf33"><mml:mi>θ</mml:mi></mml:math></inline-formula> is shared across neurons and controls the coding level, which we denote by <inline-formula><mml:math id="inf34"><mml:mi>f</mml:mi></mml:math></inline-formula>, defined as the average fraction of neurons in the expansion layer that are active. We show results for <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, since extremely dense codes are rarely observed in experiments (<xref ref-type="bibr" rid="bib63">Olshausen and Field, 2004</xref>; see Discussion). For analytical tractability, we begin with the case where the entries of <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent Gaussian random variables, as in previous theories (<xref ref-type="bibr" rid="bib69">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>). This holds when the columns of <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are orthonormal (ensuring that the embedding of the task variables in the input layer preserves their geometry) and the entries of <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent and Gaussian. Later, we will show that networks with more realistic connectivity behave similarly to this case.</p><sec id="s2-1"><title>Optimal coding level is task-dependent</title><p>In our model, a learning task is defined by a mapping from task variables <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to an output <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, representing a target change in activity of a readout neuron, for example a Purkinje cell. The limited scope of this definition implies our results should not strongly depend on the influence of the readout neuron on downstream circuits. The readout adjusts its incoming synaptic weights from the expansion layer to better approximate this target output. For example, for an associative learning task in which sensory stimuli are classified into categories such as appetitive or aversive, the task may be represented as a mapping from inputs to two discrete firing rates corresponding to the readout’s response to stimuli of each category (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). In contrast, for a forward model, in which the consequences of motor commands are computed using a model of movement dynamics, an input encoding the current sensorimotor state is mapped to a continuous output representing the readout neuron’s tuning to a predicted sensory variable (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Optimal coding level depends on task.</title><p>(<bold>A</bold>) A random categorization task in which inputs are mapped to one of two categories (+1 or –1). Gray plane denotes the decision boundary of a linear classifier separating the two categories. (<bold>B</bold>) A motor control task in which inputs are the sensorimotor states <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of an effector which change continuously along a trajectory (gray) and outputs are components of predicted future states <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>C</bold>) Schematic of random categorization tasks with <inline-formula><mml:math id="inf43"><mml:mi>P</mml:mi></mml:math></inline-formula> input-category associations. The value of the target function <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (color) is a function of two task variables <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub>. (<bold>D</bold>) Schematic of tasks involving learning a continuously varying Gaussian process target parameterized by a length scale <inline-formula><mml:math id="inf45"><mml:mi>γ</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) Error rate as a function of coding level for networks trained to perform random categorization tasks similar to (<bold>C</bold>). Arrows mark estimated locations of minima. (<bold>F</bold>) Error as a function of coding level for networks trained to fit target functions sampled from Gaussian processes. Curves represent different values of the length scale parameter <inline-formula><mml:math id="inf46"><mml:mi>γ</mml:mi></mml:math></inline-formula>. Standard error of the mean is computed across 20 realizations of network weights and sampled target functions in (<bold>E</bold>) and 200 in (<bold>F</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Sparse coding levels are sufficient for random categorization tasks irrespective of number of samples, noise level, and dimension.</title><p>(<bold>A</bold>) Error as a function of coding level for networks trained to perform random categorization tasks (as in <xref ref-type="fig" rid="fig2">Figure 2E</xref> but with a wider range of associations <inline-formula><mml:math id="inf47"><mml:mi>P</mml:mi></mml:math></inline-formula>). Performance is measured for noisy instances of previously seen inputs. <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines indicate the performance of a readout of the input layer. Standard error of the mean was computed across 20 realizations of network weights and tasks. (<bold>B</bold>) Same as in (<bold>A</bold>) but fixing the number of associations and varying the noise <inline-formula><mml:math id="inf49"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> which controls the deviation of test patterns from training patterns. <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Same as in (<bold>A</bold>) but varying the input dimension <inline-formula><mml:math id="inf51"><mml:mi>D</mml:mi></mml:math></inline-formula>. To improve performance for small <inline-formula><mml:math id="inf52"><mml:mi>D</mml:mi></mml:math></inline-formula>, we fixed the coding level for each pattern. <inline-formula><mml:math id="inf53"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> For small <inline-formula><mml:math id="inf54"><mml:mi>D</mml:mi></mml:math></inline-formula>, the curve of error rate against coding level is more flat, but low coding levels are still sufficient to saturate performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Task-dependence of optimal coding level is consistent across activation functions.</title><p>Error as a function of coding level for networks with (<bold>A</bold>) Heaviside and (<bold>B</bold>) rectified power-law (with power 2, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) nonlinearity in the expansion layer. Networks learned Gaussian process targets. Dashed lines indicate the performance of a readout of the input layer. Standard error of the mean was computed across 10 realizations of network weights and tasks in (<bold>A</bold>) and 50 in (<bold>B</bold>). Parameters: <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Task-dependence of optimal coding level is consistent across input dimensions.</title><p>Error as a function of coding level for networks learning Gaussian process targets with input dimension <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> (<bold>A</bold>) and <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> (<bold>B</bold>). Dashed lines indicate the performance of a readout of the input layer. Standard error of the mean was computed across 10 realizations of network weights and tasks. Parameters: <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Error as a function of coding level across different values of <inline-formula><mml:math id="inf63"><mml:mi>P</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mi>γ</mml:mi></mml:math></inline-formula>.</title><p>Dots denote performance of a readout of the expansion layer in simulations. Thin lines denote performance of a readout of the input layer in simulations. Thick lines denote theory for expansion layer readout performance. Standard error of the mean was computed across 10 realizations of network weights and tasks. Parameters: <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig2-figsupp4-v2.tif"/></fig></fig-group><p>To examine how properties of the expansion layer representation influence learning performance across tasks, we designed two families of tasks: one modeling categorization of random stimuli, which is often used to study the performance of expanded neural representations (<xref ref-type="bibr" rid="bib69">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>), and the other modeling learning of a continuously varying output. The former we refer to as a ‘random categorization task’ and is parameterized by the number of input pattern-to-category associations <inline-formula><mml:math id="inf67"><mml:mi>P</mml:mi></mml:math></inline-formula> learned during training (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). During the training phase, the network learns to associate random input patterns <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with random binary categories <inline-formula><mml:math id="inf70"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The elements of <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are drawn i.i.d. from a normal distribution with mean 0 and variance <inline-formula><mml:math id="inf72"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>. We refer to <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as ‘training patterns’. To assess the network’s generalization performance, it is presented with ‘test patterns’ generated by adding noise (parameterized by a noise magnitude <inline-formula><mml:math id="inf74"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>; see Methods) to the training patterns. Tasks with continuous outputs (<xref ref-type="fig" rid="fig2">Figure 2D</xref>) are parameterized by a length scale that determines how quickly the output changes as a function of the input (specifically, input-output functions are drawn from a Gaussian process with length scale <inline-formula><mml:math id="inf75"><mml:mi>γ</mml:mi></mml:math></inline-formula> for variations in <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>; see Methods). In this case, both training and test patterns are drawn uniformly on the unit sphere. Later, we will also consider tasks implemented by specific cerebellum-like systems. See <xref ref-type="table" rid="table1">Table 1</xref> for a summary of parameters throughout this study.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of simulation parameters.</title><p><inline-formula><mml:math id="inf78"><mml:mi>M</mml:mi></mml:math></inline-formula>: number of expansion layer neurons. <inline-formula><mml:math id="inf79"><mml:mi>N</mml:mi></mml:math></inline-formula>: number of input layer neurons. <inline-formula><mml:math id="inf80"><mml:mi>K</mml:mi></mml:math></inline-formula>: number of connections from input layer to a single expansion layer neuron. <inline-formula><mml:math id="inf81"><mml:mi>S</mml:mi></mml:math></inline-formula>: total number of connections from input to expansion layer. <inline-formula><mml:math id="inf82"><mml:mi>f</mml:mi></mml:math></inline-formula>: expansion layer coding level. <inline-formula><mml:math id="inf83"><mml:mi>D</mml:mi></mml:math></inline-formula>: number of task variables. <inline-formula><mml:math id="inf84"><mml:mi>P</mml:mi></mml:math></inline-formula>: number of training patterns. <inline-formula><mml:math id="inf85"><mml:mi>γ</mml:mi></mml:math></inline-formula>: Gaussian process length scale. <inline-formula><mml:math id="inf86"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>: magnitude of noise for random categorization tasks. We do not report <inline-formula><mml:math id="inf87"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf88"><mml:mi>K</mml:mi></mml:math></inline-formula> for simulations in which <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> contains Gaussian i.i.d. elements as results do not depend on these parameters in this case.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Figure panel</th><th align="left" valign="bottom">Network parameters</th><th align="left" valign="bottom">Task parameters</th></tr></thead><tbody><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figure 2E</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figures 2F</xref>, <xref ref-type="fig" rid="fig4">4G</xref> and <xref ref-type="fig" rid="fig5">5B</xref> (full)</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf93"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5B and E</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf94"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf95"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6A</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf96"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf97"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6B</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf98"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>700</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf99"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6C</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf101"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig6">Figure 6D</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf103"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7A</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; see Methods</td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7B</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf107"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7C</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf108"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>206</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>K</mml:mi><mml:mo>≤</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">see Methods</td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7">Figure 7D</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf109"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf110"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; see Methods</td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">See Figure</td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf113"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf115"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf119"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf120"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf121"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>We trained the readout to approximate the target output for training patterns and generalize to unseen test patterns. The network’s prediction is <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for tasks with continuous outputs, or <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for categorization tasks, where <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are the synaptic weights of the readout from the expansion layer. These weights were set using least squares regression. Performance was measured as the fraction of incorrect predictions for categorization tasks, or relative mean squared error for tasks with continuous targets: <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the expectation is across test patterns.</p><p>We began by examining the dependence of learning performance on the coding level of the expansion layer. For random categorization tasks, performance is maximized at low coding levels (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), consistent with previous results (<xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>). The optimal coding level remains below 0.1 in the model, regardless of the number of associations <inline-formula><mml:math id="inf126"><mml:mi>P</mml:mi></mml:math></inline-formula>, the level of input noise, and the dimension <inline-formula><mml:math id="inf127"><mml:mi>D</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For continuously varying outputs, the dependence is qualitatively different (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). The optimal coding level depends strongly on the length scale, with learning performance for slowly varying functions optimized at much higher coding levels than quickly varying functions. This dependence holds for different choices of threshold-nonlinear functions (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) or input dimension (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) and is most pronounced when the number of training patterns is limited (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). Our observations are at odds with previous theories of the role of sparse granule cell representations (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib14">Billings et al., 2014</xref>) and show that sparse activity does not always optimize performance for this broader set of tasks.</p></sec><sec id="s2-2"><title>Geometry of the expansion layer representation</title><p>To determine how the optimal coding level depends on the task, we begin by quantifying how the expansion layer transforms the geometry of the task subspace. Later we will address how this transformation affects the ability of the network to learn a target. For ease of analysis, we will assume for now that inputs are normalized, <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so that they lie on the surface of a sphere in <inline-formula><mml:math id="inf129"><mml:mi>D</mml:mi></mml:math></inline-formula> dimensions. The set of neurons in the expansion layer activated by an input <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are those neurons <inline-formula><mml:math id="inf131"><mml:mi>i</mml:mi></mml:math></inline-formula> for which the alignment of their effective weights with the input, <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, exceeds the activation threshold <inline-formula><mml:math id="inf133"><mml:mi>θ</mml:mi></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Increasing <inline-formula><mml:math id="inf134"><mml:mi>θ</mml:mi></mml:math></inline-formula> reduces the size of this set of neurons and hence reduces the coding level.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Effect of coding level on the expansion layer representation.</title><p>(<bold>A</bold>) Effect of activation threshold on coding level. A point on the surface of the sphere represents a neuron with effective weights <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Blue region represents the set of neurons activated by <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e., neurons whose input exceeds the activation threshold <inline-formula><mml:math id="inf137"><mml:mi>θ</mml:mi></mml:math></inline-formula> (inset). Darker regions denote higher activation. (<bold>B</bold>) Effect of coding level on the overlap between population responses to different inputs. Blue and red regions represent the neurons activated by <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. Overlap (purple) represents the set of neurons activated by both stimuli. High coding level leads to more active neurons and greater overlap. (<bold>C</bold>) Kernel <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for networks with rectified linear activation functions (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), normalized so that fully overlapping representations have an overlap of 1, plotted as a function of overlap in the space of task variables. The vertical axis corresponds to the ratio of the area of the purple region to the area of the red or blue regions in (<bold>B</bold>). Each curve corresponds to the kernel of an infinite-width network with a different coding level <inline-formula><mml:math id="inf141"><mml:mi>f</mml:mi></mml:math></inline-formula>. (<bold>D</bold>) Dimension of the expansion layer representation as a function of coding level for a network with <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig3-v2.tif"/></fig><p>Different inputs activate different sets of neurons, and more similar inputs activate sets with greater overlap. As the coding level is reduced, this overlap is also reduced (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In fact, this reduction in overlap is greater than the reduction in number of neurons that respond to either of the individual inputs, reflecting the fact that representations with low coding levels perform ‘pattern separation’ (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, compare purple and red or blue regions).</p><p>This effect is summarized by the ‘kernel’ of the network (<xref ref-type="bibr" rid="bib72">Schölkopf and Smola, 2002</xref>; <xref ref-type="bibr" rid="bib67">Rahimi and Recht, 2007</xref>), which measures overlap of representations in the expansion layer as a function of the task variables:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> show that the threshold <inline-formula><mml:math id="inf144"><mml:mi>θ</mml:mi></mml:math></inline-formula>, which determines the coding level, influences the kernel through its effect on the expansion layer activity <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. When inputs are normalized and the effective weights are Gaussian, we compute a semi-analytic expression for the kernel of the expansion layer in the limit of a large expansion (<inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>; see Appendix). In this case, the kernel depends only on the overlap of the task variables, <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Plotting the kernel for different choices of coding level demonstrates that representations with lower coding levels exhibit greater pattern separation (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>). This is consistent with the observation that decreasing the coding level increases the dimension of the representation (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p></sec><sec id="s2-3"><title>Frequency decomposition of kernel and task explains optimal coding level</title><p>We now relate the geometry of the expansion layer representation to performance across the tasks we have considered. Previous studies focused on high-dimensional, random categorization tasks in which inputs belong to a small number of well-separated clusters whose centers are random uncorrelated patterns. Generalization is assessed by adding noise to previously observed training patterns (<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). In this case, performance depends only on overlaps at two spatial scales: the overlap between training patterns belonging to different clusters, which is small, and the overlap between training and test patterns belonging to the same cluster, which is large (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). For such tasks, the kernel evaluated near these two values—specifically, the behavior of <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> near <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a measure of within-cluster noise—fully determines generalization performance (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; see Appendix). Sparse expansion layer representations reduce the overlap of patterns belonging to different clusters, increasing dimension and generalization performance (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Frequency decomposition of network and target function.</title><p>(<bold>A</bold>) Geometry of high-dimensional categorization tasks where input patterns are drawn from random, noisy clusters (light regions). Performance depends on overlaps between training patterns from different clusters (green) and on overlaps between training and test patterns from the same cluster (orange). (<bold>B</bold>) Distribution of overlaps of training and test patterns in the space of task variables for a high-dimensional task (<inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>) with random, clustered inputs as in (<bold>A</bold>) and a low-dimensional task (<inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) with inputs drawn uniformly on a sphere. (<bold>C</bold>) Overlaps in (<bold>A</bold>) mapped onto the kernel function. Overlaps between training patterns from different clusters are small (green). Overlaps between training and test patterns from the same cluster are large (orange). (<bold>D</bold>) Schematic illustration of basis function decomposition, for eigenfunctions on a square domain. (<bold>E</bold>) Kernel eigenvalues (normalized by the sum of eigenvalues across modes) as a function of frequency for networks with different coding levels. (<bold>F</bold>) Power <inline-formula><mml:math id="inf154"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> as a function of frequency for Gaussian process target functions. Curves represent different values of <inline-formula><mml:math id="inf155"><mml:mi>γ</mml:mi></mml:math></inline-formula>, the length scale of the Gaussian process. Power is averaged over 20 realizations of target functions. (<bold>G</bold>) Generalization error predicted using kernel eigenvalues (<bold>E</bold>) and target function decomposition (<bold>F</bold>) for the three target function classes shown in (<bold>F</bold>). Standard error of the mean is computed across 100 realizations of network weights and target functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Error as a function of coding level for learning pure-frequency spherical harmonic functions.</title><p>Frequency is indexed by <inline-formula><mml:math id="inf156"><mml:mi>k</mml:mi></mml:math></inline-formula>. Errors are calculated using analytically using <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> and represent the predictions of the theory for an infinitely large expansion. Curves are symmetric around <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> except for <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Results are shown for <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Frequency content of categorization tasks.</title><p>Power as a function of frequency for random categorization tasks (colors) and for Gaussian process task (black). Power is averaged over realizations of target functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig4-figsupp2-v2.tif"/></fig></fig-group><p>We study tasks where training patterns used for learning and test patterns used to assess generalization are both drawn according to a distribution over a low-dimensional space of task variables. While the mean overlap between pairs of random patterns remains zero regardless of dimension, fluctuations around the mean increase when the space is low dimensional, leading to a broader distribution of overlaps (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In this case, generalization performance depends on values of the kernel function evaluated across this entire range of overlaps. Methods from the theory of kernel regression (<xref ref-type="bibr" rid="bib77">Sollich, 1998</xref>; <xref ref-type="bibr" rid="bib44">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021b</xref>; <xref ref-type="bibr" rid="bib76">Simon et al., 2021</xref>) capture these effects by quantifying a network’s performance on a learning task through a decomposition of the target function into a set of basis functions (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). Performance is assessed by summing the contribution of each mode in this decomposition to generalization error.</p><p>The decomposition expresses the kernel as a sum of eigenfunctions weighted by eigenvalues, <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The eigenfunctions are determined by the network architecture and the distribution of inputs. As we show below, the eigenvalues <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> determine the ease with which each corresponding eigenfunction <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>—one element of the basis function decomposition—is learned by the network. Under our present assumptions of Gaussian effective weights and uniformly distributed, normalized input patterns, the eigenfunctions are the spherical harmonic functions. These functions are ordered by increasing frequency, with higher frequencies corresponding to functions that vary more quickly as a function of the task variables. Spherical harmonics are defined for any input dimension; for example, in two dimensions they are the Fourier modes. We find that coding level substantially changes the frequency dependence of the eigenvalues associated with these eigenfunctions (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Higher coding levels increase the relative magnitude of the low frequency eigenvalues compared to high-frequency eigenvalues. As we will show, this results in a different inductive bias for networks with different coding levels.</p><p>To calculate learning performance for an arbitrary task, we decompose the target function in the same basis as that of the kernel:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The coefficient <inline-formula><mml:math id="inf164"><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> quantifies the weight of mode <inline-formula><mml:math id="inf165"><mml:mi>α</mml:mi></mml:math></inline-formula> in the decomposition. For the Gaussian process targets, we have considered, increasing length scale corresponds to a greater relative contribution of low versus high frequency modes (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Using these coefficients and the eigenvalues (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), we obtain an analytical prediction of the mean-squared generalization error (‘Error’) for learning any given task (<xref ref-type="fig" rid="fig4">Figure 4G</xref>; see Methods):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub> do not depend on <inline-formula><mml:math id="inf166"><mml:mi>α</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib20">Canatar et al., 2021b</xref>; <xref ref-type="bibr" rid="bib76">Simon et al., 2021</xref>; see Methods). <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> illustrates that for equal values of <inline-formula><mml:math id="inf167"><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>, modes with greater <inline-formula><mml:math id="inf168"><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> contribute less to the generalization error.</p><p>Our theory reveals that the optima observed in <xref ref-type="fig" rid="fig2">Figure 2F</xref> are a consequence of the difference in eigenvalues of networks with different coding levels. This reflects an inductive bias (<xref ref-type="bibr" rid="bib77">Sollich, 1998</xref>; <xref ref-type="bibr" rid="bib44">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021b</xref>; <xref ref-type="bibr" rid="bib76">Simon et al., 2021</xref>) of networks with low and high coding levels toward the learning of high and low frequency functions, respectively (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, the coding level’s effect on a network’s inductive bias, rather than dimension alone, determines learning performance. Previous studies that focused only on random categorization tasks did not observe this dependence, since errors in such tasks are dominated by the learning of high frequency components, for which sparse activity is optimal (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p></sec><sec id="s2-4"><title>Performance of sparsely connected expansions</title><p>To simplify our analysis, we have so far assumed full connectivity between input and expansion layers without a constraint on excitatory or inhibitory synaptic weights. In particular, we have assumed that the effective weight matrix <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> contains independent Gaussian entries (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, top). However, synaptic connections between mossy fibers and granule cells are sparse and excitatory (<xref ref-type="bibr" rid="bib71">Sargent et al., 2005</xref>), with a typical in-degree of <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> mossy fibers per granule cell (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, bottom). We therefore analyzed the performance of model networks with more realistic connectivity. Surprisingly, when <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is sparse and nonnegative, both overall generalization performance and the task-dependence of optimal coding level remain unchanged (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Performance of networks with sparse connectivity.</title><p>(<bold>A</bold>) Top: Fully connected network. Bottom: Sparsely connected network with in-degree <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and excitatory weights with global inhibition onto expansion layer neurons. (<bold>B</bold>) Error as a function of coding level for fully connected Gaussian weights (gray curves) and sparse excitatory weights (blue curves). Target functions are drawn from Gaussian processes with different values of length scale <inline-formula><mml:math id="inf173"><mml:mi>γ</mml:mi></mml:math></inline-formula> as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>C</bold>) Distributions of synaptic weight correlations <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <italic>i</italic>th row of <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, for pairs of expansion layer neurons in networks with different numbers of input layer neurons <inline-formula><mml:math id="inf177"><mml:mi>N</mml:mi></mml:math></inline-formula> (colors) when <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>. Black distribution corresponds to fully connected networks with Gaussian weights. We note that when <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, the distribution of correlations for random Gaussian weight vectors is uniform on <inline-formula><mml:math id="inf181"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> as shown (for higher dimensions the distribution has a peak at 0). (<bold>D</bold>) Schematic of the selectivity of input layer neurons to task variables in distributed and clustered representations. (<bold>E</bold>) Error as a function of coding level for networks with distributed (black, same as in <bold>B</bold>) and clustered (orange) representations. (<bold>F</bold>) Distributions of <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for pairs of expansion layer neurons in networks with distributed and clustered input representations when <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Standard error of the mean was computed across 200 realizations in (<bold>B</bold>) and 100 in (<bold>E</bold>), orange curve.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig5-v2.tif"/></fig><p>To understand this result, we examined how <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> shape the statistics of the effective weights onto the expansion layer neurons <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. A desirable property of the expansion layer representation is that these effective weights sample the space of task variables uniformly (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), increasing the heterogeneity of tuning of expansion layer neurons (<xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>). This occurs when <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix of independent random Gaussian entries. If the columns of <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are orthornormal and <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is fully-connected with independent Gaussian entries, <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> has this uniform sampling property.</p><p>However, when <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is sparse and nonnegative, expansion layer neurons that share connections from the same input layer neurons receive correlated input currents. When <inline-formula><mml:math id="inf194"><mml:mi>N</mml:mi></mml:math></inline-formula> is small and <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is random, fluctuations in <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> lead to biases in the input layer’s sampling of task variables which are inherited by the expansion layer. We quantify this by computing the distribution of correlations between the effective weights for pairs of expansion layer neurons, <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This distribution indeed deviates from uniform sampling when <inline-formula><mml:math id="inf198"><mml:mi>N</mml:mi></mml:math></inline-formula> is small (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). However, even when <inline-formula><mml:math id="inf199"><mml:mi>N</mml:mi></mml:math></inline-formula> is moderately large (but much less than <inline-formula><mml:math id="inf200"><mml:mi>M</mml:mi></mml:math></inline-formula>), only small deviations from uniform sampling of task variables occur for low dimensional tasks as long as <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>K</mml:mi><mml:mo>≪</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Appendix). In contrast, for high-dimensional tasks (<inline-formula><mml:math id="inf202"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>≪</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is sufficient, in agreement with previous findings (<xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>). For realistic cerebellar parameters (<inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>), the distribution is almost indistinguishable from that corresponding to uniform sampling (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), consistent with the similar learning performance of these two cases (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>In the above analysis, an important assumption is that <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is dense and random, so that the input layer forms a distributed representation in which each input layer neuron responds to a random combination of task variables (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, top). If, on the other hand, the input layer forms a clustered representation containing groups of neurons that each encode a single task variable (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, bottom), we may expect different results. Indeed, with a clustered representation, sparse connectivity dramatically reduces performance (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). This is because the distribution of <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> deviates substantially from that corresponding to uniform sampling (<xref ref-type="fig" rid="fig5">Figure 5F</xref>), even as <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> (see Appendix). Specifically, increasing <inline-formula><mml:math id="inf209"><mml:mi>N</mml:mi></mml:math></inline-formula> does not reduce the probability of two expansion layer neurons being connected to input layer neurons that encode the same task variables and therefore receiving highly correlated currents. As a result, expansion layer neurons do not sample task variables uniformly and performance is dramatically reduced.</p><p>Our results show that networks with small <inline-formula><mml:math id="inf210"><mml:mi>K</mml:mi></mml:math></inline-formula>, moderately large <inline-formula><mml:math id="inf211"><mml:mi>N</mml:mi></mml:math></inline-formula>, and a distributed input layer representation approach the performance of networks that sample task variables uniformly. This equivalence validates the applicability of our theory to these more realistic networks. It also argues for the importance of distributed sensorimotor representations in the cortico-cerebellar pathway, consistent with the distributed nature of representations in motor cortex (<xref ref-type="bibr" rid="bib75">Shenoy et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Muscinelli et al., 2023</xref>).</p></sec><sec id="s2-5"><title>Optimal cerebellar architecture is consistent across tasks</title><p>A history of theoretical modeling has shown a remarkable correspondence between anatomical properties of the cerebellar cortex and model parameters optimal for learning. These include the in-degree <inline-formula><mml:math id="inf212"><mml:mi>K</mml:mi></mml:math></inline-formula> of granule cells (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>), the expansion ratio of the granule cells to the mossy fibers <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi>M</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>), and the distribution of synaptic weights from granule cells to Purkinje cells (<xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref>; <xref ref-type="bibr" rid="bib27">Clopath et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Clopath and Brunel, 2013</xref>). In these studies, model performance was assessed using random categorization tasks. We have shown that optimal coding level is dependent on the task being learned, raising the question of whether optimal values of these architectural parameters are also task-dependent.</p><p>Sparse connectivity (<inline-formula><mml:math id="inf214"><mml:mi>K</mml:mi></mml:math></inline-formula>=4, consistent with the typical in-degree of cerebellar granule cells) has been shown to optimize learning performance in cerebellar cortex models (<xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>). We examined the performance of networks with different granule cell in-degrees learning Gaussian process targets. The optimal in-degree is small for all the tasks we consider, suggesting that sparse connectivity is sufficient for high performance across a range of tasks (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). This is consistent with the previous observation that the performance of a sparsely connected network approaches that of a fully connected network (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Task-independence of optimal anatomical parameters.</title><p>(<bold>A</bold>) Error as a function of in-degree <inline-formula><mml:math id="inf215"><mml:mi>K</mml:mi></mml:math></inline-formula> for networks learning Gaussian process targets. Curves represent different values of <inline-formula><mml:math id="inf216"><mml:mi>γ</mml:mi></mml:math></inline-formula>, the length scale of the Gaussian process. The total number of synaptic connections <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is held constant. This constraint introduces a trade-off between having many neurons with small synaptic degree and having fewer neurons with large synaptic degree (<xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>). <inline-formula><mml:math id="inf218"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Error as a function of expansion ratio <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi>M</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> for networks learning Gaussian process targets. <inline-formula><mml:math id="inf222"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>700</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) Distribution of granule-cell-to-Purkinje cell weights <inline-formula><mml:math id="inf225"><mml:mi>w</mml:mi></mml:math></inline-formula> for a network trained on nonnegative Gaussian process targets with <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Granule-cell-to-Purkinje cell weights are constrained to be nonnegative (<xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref>). (<bold>D</bold>) Fraction of granule-cell-to-Purkinje cell weights that are silent in networks learning nonnegative Gaussian process targets (blue) and random categorization tasks (gray).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig6-v2.tif"/></fig><p>Previous studies also showed that the expansion ratio from mossy fibers to granule cells <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>M</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> controls the dimension of the granule cell representation (<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>). The dimension increases with expansion ratio but saturates as expansion ratio approaches the anatomical value (<inline-formula><mml:math id="inf230"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for the inputs presynaptic to an individual Purkinje cell). These studies assumed that mossy fiber activity is uncorrelated (<inline-formula><mml:math id="inf232"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>) rather than low-dimensional (<inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). This raises the question of whether a large expansion is beneficial when <inline-formula><mml:math id="inf234"><mml:mi>D</mml:mi></mml:math></inline-formula> is small. We find that when the number of training patterns <inline-formula><mml:math id="inf235"><mml:mi>P</mml:mi></mml:math></inline-formula> is sufficiently large, performance still improves as <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi>M</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> approaches its anatomical value, showing that Purkinje cells can exploit their large number of presynaptic inputs even in the case of low-dimensional activity (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p><xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref> showed that the distribution of granule-cell-to-Purkinje cell synaptic weights is consistent with the distribution that maximizes the number of random binary input-output mappings stored. This distribution exhibits a substantial fraction of silent synapses, consistent with experiments. These results also hold for analog inputs and outputs (<xref ref-type="bibr" rid="bib28">Clopath and Brunel, 2013</xref>) and for certain forms of correlations among binary inputs and outputs (<xref ref-type="bibr" rid="bib27">Clopath et al., 2012</xref>). However, the case we consider, where targets are a smoothly varying function of task variables, has not been explored. We observe a similar weight distribution for these tasks (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), with the fraction of silent synapses remaining high across coding levels (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). The fraction of silent synapses is lower for networks learning Gaussian process targets than those learning random categorization tasks, consistent with the capacity of a given network for learning such targets being larger (<xref ref-type="bibr" rid="bib27">Clopath et al., 2012</xref>).</p><p>Although optimal coding level is task-dependent, these analyses suggest that optimal architectural parameters are largely task-independent. Whereas coding level tunes the inductive bias of the network to favor the learning of specific tasks, these architectural parameters control properties of the representation that improve performance across tasks. In particular, sparse connectivity and a large expansion support uniform sampling of low-dimensional task variables (consistent with <xref ref-type="fig" rid="fig5">Figure 5C</xref>), while a large fraction of silent synapses is a consequence of a readout that maximizes learning performance (<xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref>).</p></sec><sec id="s2-6"><title>Modeling specific behaviors dependent on cerebellum-like structures</title><p>So far, we have considered analytically tractable families of tasks with parameterized input-output functions. Next, we extend our results to more realistic tasks constrained by the functions of specific cerebellum-like systems, which include both highly structured, continuous input-output mappings and random categorization tasks.</p><p>To model the cerebellum’s role in predicting the consequences of motor commands (<xref ref-type="bibr" rid="bib86">Wolpert et al., 1998</xref>), we examined the optimal coding level for learning the dynamics of a two-joint arm (<xref ref-type="bibr" rid="bib31">Fagg et al., 1997</xref>). Given an initial state, the network predicts the change in the future position of the arm (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Performance is optimized at substantially higher coding levels than for random categorization tasks, consistent with our previous results for continuous input-output mappings (<xref ref-type="fig" rid="fig2">Figure 2E and F</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Optimal coding level across tasks and neural systems.</title><p>(<bold>A</bold>) Left: Schematic of two-joint arm. Center: Cerebellar cortex model in which sensorimotor task variables at time <inline-formula><mml:math id="inf237"><mml:mi>t</mml:mi></mml:math></inline-formula> are used to predict hand position at time <inline-formula><mml:math id="inf238"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:math></inline-formula>. Right: Error as a function of coding level. Black arrow indicates location of optimum. Dashed line indicates performance of a readout of the input layer. (<bold>B</bold>) Left: Odor categorization task. Center: <italic>Drosophila</italic> mushroom body model in which odors activate olfactory projection neurons and are associated with a binary category (appetitive or aversive). Right: Error rate, similar to (<bold>A</bold>), right. (<bold>C</bold>) Left: Schematic of electrosensory system of the mormyrid electric fish, which learns a negative image to cancel the self-generated feedback from electric organ discharges sensed by electroreceptors. Center: Electrosensory lateral line lobe (ELL) model in which MG cells learn a negative image. Right: Error as a function of coding level. Gray arrow indicates location of coding level estimated from biophysical parameters (<xref ref-type="bibr" rid="bib47">Kennedy et al., 2014</xref>). (<bold>D</bold>) Left: Schematic of the vestibulo-cular reflex (VOR). Head rotations with velocity <inline-formula><mml:math id="inf239"><mml:mi>H</mml:mi></mml:math></inline-formula> trigger eye motion in the opposite direction with velocity <inline-formula><mml:math id="inf240"><mml:mi>E</mml:mi></mml:math></inline-formula>. During VOR adaptation, organisms adapt to different gains (<inline-formula><mml:math id="inf241"><mml:mrow><mml:mi>E</mml:mi><mml:mo>/</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>). Center: Cerebellar cortex model in which the target function is the Purkinje cell’s firing rate as a function of head velocity. Right: Error, similar to (<bold>A</bold>), right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Optimal coding levels in the presence of spiking noise.</title><p>(<bold>A</bold>) Error as a function of coding level in a spiking model. The firing rate of neuron <italic>i</italic> (in Hz) is given by <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf243"><mml:mi>g</mml:mi></mml:math></inline-formula> is a gain term that adjusts the amplitude of the activity and <inline-formula><mml:math id="inf244"><mml:mi>θ</mml:mi></mml:math></inline-formula> is the activation threshold. The spike count <italic>s</italic><sub><italic>i</italic></sub> for a neuron <inline-formula><mml:math id="inf245"><mml:mi>i</mml:mi></mml:math></inline-formula> in response to pattern µ is sampled from a Poisson distribution: <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the time window in which a Purkinje cell integrates spikes, and is set to 0.1 s. Coding level is measured as the fraction of cells with a nonzero spike count. Coding level is adjusted by tuning either the activation threshold <inline-formula><mml:math id="inf247"><mml:mi>θ</mml:mi></mml:math></inline-formula> (top) or the gain <inline-formula><mml:math id="inf248"><mml:mi>g</mml:mi></mml:math></inline-formula> (bottom). Black curve shows the performance of a rate model as in the main text. Standard error of the mean was computed across 10 realizations of network weights. (<bold>B</bold>) Mean spike count of active expansion layer neurons during the time window <inline-formula><mml:math id="inf249"><mml:mi>τ</mml:mi></mml:math></inline-formula> as a function of coding level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Task-dependence of optimal coding level remains consistent under an online climbing fiber-based plasticity rule.</title><p>During each epoch of training, the network is presented with all patterns in a randomized order, and the learned weights are updated with each pattern (see Methods). Networks were presented with 30 patterns and trained for 20,000 epochs, with a learning rate of <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.7</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Other parameters: <inline-formula><mml:math id="inf251"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>A</bold>) Performance of an example network during online learning, measured as relative mean squared error across training epochs. Parameters: <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf253"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Generalization error as a function of coding level for networks trained with online learning (solid lines) or unregularized least squares (dashed lines) for Gaussian process tasks with different length scales (colors). Standard error of the mean was computed across 20 realizations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82914-fig7-figsupp2-v2.tif"/></fig></fig-group><p>The mushroom body, a cerebellum-like structure in insects, is required for learning of associations between odors and appetitive or aversive valence (<xref ref-type="bibr" rid="bib56">Modi et al., 2020</xref>). This behavior can be represented as a mapping from random representations of odors in the input layer to binary category labels (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The optimal coding level in a model with parameters consistent with the <italic>Drosophila</italic> mushroom body is less than 0.1, consistent with our previous results for random categorization tasks (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) and the sparse odor-evoked responses in <italic>Drosophila</italic> Kenyon cells (<xref ref-type="bibr" rid="bib81">Turner et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Honegger et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Lin et al., 2014</xref>).</p><p>The prediction and cancellation of self-generated sensory feedback has been studied extensively in mormyrid weakly electric fish and depends on the electrosensory lateral line lobe (ELL), a cerebellum-like structure (<xref ref-type="bibr" rid="bib12">Bell et al., 2008</xref>). Granule cells in the ELL provide a temporal basis for generating negative images that are used to cancel self-generated feedback (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). We extended a detailed model of granule cells and their inputs (<xref ref-type="bibr" rid="bib47">Kennedy et al., 2014</xref>) to study the influence of coding level on the effectiveness of this basis. The performance of this model saturated at relatively high coding levels, and notably the coding level corresponding to biophysical parameters estimated from data coincided with the value at which further increases in performance were modest. This observation suggests that coding level is also optimized for task performance in this system.</p><p>A canonical function of the mammalian cerebellum is the adjustment of the vestibulo-ocular reflex (VOR), in which motion of the head is detected and triggers compensatory ocular motion in the opposite direction. During VOR learning, Purkinje cells are tuned to head velocity, and their tuning curves are described as piecewise linear functions (<xref ref-type="bibr" rid="bib53">Lisberger et al., 1994</xref>; <xref ref-type="fig" rid="fig7">Figure 7D</xref>). Although <italic>in vivo</italic> population recordings of granule cells during VOR adaptation are not, to our knowledge, available for comparison, our model predicts that performance for learning such tuning curves is high across a range of coding levels and shows that sparse codes are sufficient (although not necessary) for such tasks (<xref ref-type="fig" rid="fig7">Figure 7D</xref>).</p><p>These results predict diverse coding levels across different behaviors dependent on cerebellum-like structures. The odor categorization and VOR tasks both have input-output mappings that exhibit sharp nonlinearities and can be efficiently learned using sparse representations. In contrast, the forward modeling and feedback cancellation tasks have smooth input-output mappings and exhibit denser optima. These observations are consistent with our previous finding that more structured tasks favor denser coding levels than do random categorization tasks (<xref ref-type="fig" rid="fig2">Figure 2E and F</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have shown that the optimal granule cell coding level depends on the task being learned. While sparse representations are suitable for learning to categorize inputs into random categories, as predicted by classic theories, tasks involving structured input-output mappings benefit from denser representations (<xref ref-type="fig" rid="fig2">Figure 2</xref>). This reconciles such theories with the observation of dense granule cell activation during movement (<xref ref-type="bibr" rid="bib48">Knogler et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Wagner et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Giovannucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Badura and De Zeeuw, 2017</xref>; <xref ref-type="bibr" rid="bib84">Wagner et al., 2019</xref>). We also show that, in contrast to the task-dependence of optimal coding level, optimal anatomical values of granule cell and Purkinje cell connectivity are largely task-independent (<xref ref-type="fig" rid="fig6">Figure 6</xref>). This distinction suggests that a stereotyped cerebellar architecture may support diverse representations optimized for a variety of learning tasks.</p><sec id="s3-1"><title>Relationship to previous theories</title><p>Previous studies assessed the learning performance of cerebellum-like systems with a model Purkinje cell that associates random patterns of mossy fiber activity with one of two randomly assigned categories (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>), a common benchmark for artificial learning systems (<xref ref-type="bibr" rid="bib35">Gerace et al., 2022</xref>). In this case, a low coding level increases the dimension of the granule cell representation, permitting more associations to be stored and improving generalization to previously unseen inputs. The optimal coding level is low but not arbitrarily low, as extremely sparse representations introduce noise that hinders generalization (<xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>).</p><p>To examine a broader family of tasks, our learning problems extend previous studies in several ways. First, we consider inputs that may be constrained to a low-dimensional task subspace. Second, we consider input-output mappings beyond random categorization tasks. Finally, we assess generalization error for arbitrary locations on the task subspace, rather than only for noisy instances of previously presented inputs. As we have shown, these considerations require a complete analysis of the inductive bias of cerebellum-like networks (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Our analysis generalizes previous approaches (<xref ref-type="bibr" rid="bib10">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>) that focused on dimension and noise alone. In particular, both dimension and noise for random patterns can be directly calculated from the kernel function (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; see Appendix).</p><p>Our theory builds upon techniques that been developed for understanding properties of kernel regression (<xref ref-type="bibr" rid="bib77">Sollich, 1998</xref>; <xref ref-type="bibr" rid="bib44">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Bordelon et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Canatar et al., 2021b</xref>; <xref ref-type="bibr" rid="bib76">Simon et al., 2021</xref>). Kernel approximations of wide neural networks are a major area of current research providing analytically tractable theories (<xref ref-type="bibr" rid="bib67">Rahimi and Recht, 2007</xref>; <xref ref-type="bibr" rid="bib44">Jacot et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Chizat et al., 2018</xref>). Prior studies have analyzed kernels corresponding to networks with zero (<xref ref-type="bibr" rid="bib25">Cho and Saul, 2010</xref>) or mean-zero Gaussian thresholds (<xref ref-type="bibr" rid="bib11">Basri et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Jacot et al., 2018</xref>), which in both cases produce networks with a coding level of 0.5. Ours is the first kernel study of the effects of nonzero average thresholds. Our full characterization of the eigenvalue spectra and their decay rates as a function of the threshold extends previous work (<xref ref-type="bibr" rid="bib8">Bach, 2017</xref>; <xref ref-type="bibr" rid="bib13">Bietti and Bach, 2021</xref>). Furthermore, artificial neural network studies typically assume either fully-connected or convolutional layers, yet pruning connections after training barely degrades performance (<xref ref-type="bibr" rid="bib38">Han et al., 2015</xref>; <xref ref-type="bibr" rid="bib89">Zhang et al., 2018</xref>). Our results support the idea that sparsely connected networks may behave like dense ones if the representation is distributed (<xref ref-type="fig" rid="fig5">Figure 5</xref>), providing insight into the regimes in which pruning preserves performance.</p><p>Other studies have considered tasks with smooth input-output mappings and low-dimensional inputs, finding that heterogeneous Golgi cell inhibition can improve performance by diversifying individual granule cell thresholds (<xref ref-type="bibr" rid="bib78">Spanne and Jörntell, 2013</xref>). Extending our model to include heterogeneous thresholds is an interesting direction for future work. Another proposal states that dense coding may improve generalization (<xref ref-type="bibr" rid="bib79">Spanne and Jörntell, 2015</xref>). Our theory reveals that whether or not dense coding is beneficial depends on the task.</p></sec><sec id="s3-2"><title>Assumptions and extensions</title><p>We have made several assumptions in our model for the sake of analytical tractability. When comparing the inductive biases of networks with different coding levels, our theory assumes that inputs are normalized and distributed uniformly in a linear subspace of the input layer activity. This allows us to decompose the target function into a basis in which we can directly compare eigenvalues, and hence learning performance, for different coding levels (<xref ref-type="fig" rid="fig4">Figure 4E–G</xref>). A similar analysis can be performed when inputs are not uniformly distributed, but in this case the basis is determined by an interplay between this distribution and the nonlinearity of expansion layer neurons, making the analysis more complex (see Appendix). We have also assumed that generalization is assessed for inputs drawn from the same distribution as used for learning. Recent and ongoing work on out-of-distribution generalization may permit relaxations of this assumption (<xref ref-type="bibr" rid="bib74">Shen et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Canatar et al., 2021a</xref>).</p><p>When analyzing properties of the granule cell layer, our theory also assumes an infinitely wide expansion. When <inline-formula><mml:math id="inf254"><mml:mi>P</mml:mi></mml:math></inline-formula> is small enough that performance is limited by number of samples, this assumption is appropriate, but finite-size corrections to our theory are an interesting direction for future work. We also have not explicitly modeled inhibitory input provided by Golgi cells, instead assuming such input can be modeled as a change in effective threshold, as in previous studies (<xref ref-type="bibr" rid="bib14">Billings et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>). This is appropriate when considering the dimension of the granule cell representation (<xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>), but more work is needed to extend our model to the case of heterogeneous inhibition.</p><p>Another key assumption concerning the granule cells is that they sample mossy fiber inputs randomly, as is typically assumed in Marr-Albus models (<xref ref-type="bibr" rid="bib55">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib2">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Cayco-Gajic et al., 2017</xref>). Other studies instead argue that granule cells sample from mossy fibers with highly similar receptive fields (<xref ref-type="bibr" rid="bib33">Garwicz et al., 1998</xref>; <xref ref-type="bibr" rid="bib16">Brown and Bower, 2001</xref>; <xref ref-type="bibr" rid="bib46">Jörntell and Ekerot, 2006</xref>) defined by the tuning of mossy fiber and climbing fiber inputs to cerebellar microzones (<xref ref-type="bibr" rid="bib4">Apps et al., 2018</xref>). This has led to an alternative hypothesis that granule cells serve to relay similarly tuned mossy fiber inputs and enhance their signal-to-noise ratio (<xref ref-type="bibr" rid="bib46">Jörntell and Ekerot, 2006</xref>; <xref ref-type="bibr" rid="bib36">Gilbert and Chris Miall, 2022</xref>) rather than to re-encode inputs. Another hypothesis is that granule cells enable Purkinje cells to learn piece-wise linear approximations of nonlinear functions (<xref ref-type="bibr" rid="bib78">Spanne and Jörntell, 2013</xref>). However, several recent studies support the existence of heterogeneous connectivity and selectivity of granule cells to multiple distinct inputs at the local scale (<xref ref-type="bibr" rid="bib41">Huang et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Ishikawa et al., 2015</xref>). Furthermore, the deviation of the predicted dimension in models constrained by electron-microscopy data as compared to randomly wired models is modest (<xref ref-type="bibr" rid="bib61">Nguyen et al., 2023</xref>). Thus, topographically organized connectivity at the macroscopic scale may coexist with disordered connectivity at the local scale, allowing granule cells presynaptic to an individual Purkinje cell to sample heterogeneous combinations of the subset of sensorimotor signals relevant to the tasks that Purkinje cell participates in. Finally, we note that the optimality of dense codes for learning slowly varying tasks in our theory suggests that observations of a lack of mixing (<xref ref-type="bibr" rid="bib45">Jörntell and Ekerot, 2002</xref>) for such tasks are compatible with Marr-Albus models, as in this case nonlinear mixing is not required.</p><p>We have quantified coding level by the fraction of neurons that are above firing threshold. We focused on coding levels <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, as extremely dense codes are rarely found in experiments (<xref ref-type="bibr" rid="bib63">Olshausen and Field, 2004</xref>), but our theory applies for <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> as well. In general, representations with coding levels of <inline-formula><mml:math id="inf257"><mml:mi>f</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf258"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> perform similarly in our model due to the symmetry of most of their associated eigenvalues (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> and Appendix). Under the assumption that the energetic costs associated with neural activity are minimized, the <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> region is likely the biologically plausible one. We also note that coding level is most easily defined when neurons are modeled as rate, rather than spiking units. To investigate the consistency of our results under a spiking code, we implemented a model in which granule cell spiking exhibits Poisson variability and quantify coding level as the fraction of neurons that have nonzero spike counts (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>; <xref ref-type="fig" rid="fig7">Figure 7C</xref>). In general, increased spike count leads to improved performance as noise associated with spiking variability is reduced. Granule cells have been shown to exhibit reliable burst responses to mossy fiber stimulation (<xref ref-type="bibr" rid="bib23">Chadderton et al., 2004</xref>), motivating models using deterministic responses or sub-Poisson spiking variability. However, further work is needed to quantitatively compare variability in model and experiment and to account for more complex biophysical properties of granule cells (<xref ref-type="bibr" rid="bib70">Saarinen et al., 2008</xref>).</p><p>For the Purkinje cells, our model assumes that their responses to granule cell input can be modeled as an optimal linear readout. Our model therefore provides an upper bound to linear readout performance, a standard benchmark for the quality of a neural representation that does not require assumptions on the nature of climbing fiber-mediated plasticity, which is still debated. Electrophysiological studies have argued in favor of a linear approximation (<xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref>). To improve the biological applicability of our model, we implemented an online climbing fiber-mediated learning rule and found that optimal coding levels are still task-dependent (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). We also note that although we model several timing-dependent tasks (<xref ref-type="fig" rid="fig7">Figure 7</xref>), our learning rule does not exploit temporal information, and we assume that temporal dynamics of granule cell responses are largely inherited from mossy fibers. Integrating temporal information into our model is an interesting direction for future investigation.</p></sec><sec id="s3-3"><title>Implications for cerebellar representations</title><p>Our results predict that qualitative differences in the coding levels of cerebellum-like systems, across brain regions or across species, reflect an optimization to distinct tasks (<xref ref-type="fig" rid="fig7">Figure 7</xref>). However, it is also possible that differences in coding level arise from other physiological differences between systems. In the <italic>Drosophila</italic> mushroom body, which is required for associative learning of odor categories, random and sparse subsets of Kenyon cells are activated in response to odor stimulation, consistent with our model (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; <xref ref-type="bibr" rid="bib81">Turner et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Honegger et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Lin et al., 2014</xref>). In a model of the electrosensory system of the electric fish, the inferred coding level of a model constrained by the properties of granule cells is similar to that which optimizes task performance (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Within the cerebellar cortex, heterogeneity in granule cell firing has been observed across cerebellar lobules, associated with both differences in intrinsic properties (<xref ref-type="bibr" rid="bib39">Heath et al., 2014</xref>) and mossy fiber input (<xref ref-type="bibr" rid="bib85">Witter and De Zeeuw, 2015</xref>). It would be interesting to correlate such physiological heterogeneity with heterogeneity in function across the cerebellum. Our model predicts that regions involved in behaviors with substantial low-dimensional structure, for example smooth motor control tasks, may exhibit higher coding levels than regions involved in categorization or discrimination of high-dimensional stimuli.</p><p>Our model also raises the possibility that individual brain regions may exhibit different coding levels at different moments in time, depending on immediate behavioral or task demands. Multiple mechanisms could support the dynamic adjustment of coding level, including changes in mossy fiber input (<xref ref-type="bibr" rid="bib64">Ozden et al., 2012</xref>), Golgi cell inhibition (<xref ref-type="bibr" rid="bib29">Eccles et al., 1966</xref>; <xref ref-type="bibr" rid="bib65">Palay and Chan-Palay, 1974</xref>), retrograde signaling from Purkinje cells (<xref ref-type="bibr" rid="bib51">Kreitzer and Regehr, 2001</xref>), or unsupervised plasticity of mossy fiber-to-granule cell synapses (<xref ref-type="bibr" rid="bib73">Schweighofer et al., 2001</xref>). The predictions of our model are not dependent on which of these mechanisms are active. A recent study demonstrated that local synaptic inhibition by Golgi cells controls the spiking threshold and hence the population coding level of cerebellar granule cells in mice (<xref ref-type="bibr" rid="bib32">Fleming et al., 2022</xref>). Further, the authors observed that granule cell responses to sensory stimuli are sparse when movement-related selectivity is controlled for. This suggests that dense movement-related activity and sparse sensory-evoked activity are not incompatible.</p><p>While our analysis makes clear qualitative predictions concerning comparisons between the optimal coding levels for different tasks, in some cases it is also possible to make quantitative predictions about the location of the optimum for a single task. Doing so requires determining the appropriate time interval over which to measure coding level, which depends on the integration time constant of the readout neuron. It also requires estimates of the firing rates and biophysical properties of the expansion layer neurons. In the electrosensory system, for which a well-calibrated model exists and the learning objective is well-characterized (<xref ref-type="bibr" rid="bib47">Kennedy et al., 2014</xref>), we found that the coding level estimated based on the data is similar to that which optimizes performance (<xref ref-type="fig" rid="fig7">Figure 7C</xref>).</p><p>If coding level is task-optimized, our model predicts that manipulating coding level artificially will diminish performance. In the <italic>Drosophila</italic> mushroom body, disrupting feedback inhibition from the GABAergic anterior paired lateral neuron onto Kenyon cells increases coding level and impairs odor discrimination (<xref ref-type="bibr" rid="bib52">Lin et al., 2014</xref>). A recent study demonstrated that blocking inhibition from Golgi cells onto granule cells results in denser granule cell population activity and impairs performance on an eye-blink conditioning task (<xref ref-type="bibr" rid="bib32">Fleming et al., 2022</xref>). These examples demonstrate that increasing coding level during sensory discrimination tasks, for which sparse activity is optimal, impairs performance. Our theory predicts that decreasing coding level during a task for which dense activity is optimal, such as smooth motor control, would also impair performance.</p><p>While dense activity has been taken as evidence against theories of combinatorial coding in cerebellar granule cells (<xref ref-type="bibr" rid="bib48">Knogler et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">Wagner et al., 2019</xref>), our theory suggests that the two are not incompatible. Instead, the coding level of cerebellum-like regions may be determined by behavioral demands and the nature of the input to granule-like layers (<xref ref-type="bibr" rid="bib59">Muscinelli et al., 2023</xref>). Sparse coding has also been cited as a key property of sensory representations in the cerebral cortex (<xref ref-type="bibr" rid="bib62">Olshausen and Field, 1996</xref>). However, recent population recordings show that such regions exhibit dense movement-related activity (<xref ref-type="bibr" rid="bib58">Musall et al., 2019</xref>), much like in cerebellum. While the theory presented in this study does not account for the highly structured recurrent interactions that characterize cerebrocortical regions, it is possible that these areas also operate using inductive biases that are shaped by coding level in a similar manner to our model.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Network model</title><p>The expansion layer activity is given by <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> describes the selectivity of expansion layer neurons to task variables. For most simulations, <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> matrix sampled with random, orthonormal columns and <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <inline-formula><mml:math id="inf265"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix with i.i.d. unit Gaussian entries. The nonlinearity <inline-formula><mml:math id="inf266"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is a rectified linear activation function <inline-formula><mml:math id="inf267"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> applied element-wise. The input layer activity <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p><sec id="s4-1-1"><title>Sparsely connected networks</title><p>To model sparse excitatory connectivity, we generated a sparse matrix <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where each row contains precisely <inline-formula><mml:math id="inf271"><mml:mi>K</mml:mi></mml:math></inline-formula> nonzero elements at random locations. The nonzero elements are either identical and equal to 1 (homogeneous excitatory weights) or sampled from a unit truncated normal distribution (heterogeneous excitatory weights). To model global feedforward inhibition that balances excitation, <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a dense matrix with every element equal to <inline-formula><mml:math id="inf274"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>E</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For <xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="fig" rid="fig6">Figure 6A and B</xref>, <xref ref-type="fig" rid="fig7">Figure 7B</xref>, sparsely connected networks were generated with homogeneous excitatory weights and global inhibition. For <xref ref-type="fig" rid="fig5">Figure 5E</xref>, the network with clustered representations was generated with homogeneous excitatory weights without global inhibition. For <xref ref-type="fig" rid="fig5">Figure 5C and F</xref>, networks were generated with heterogeneous excitatory weights and global inhibition.</p></sec><sec id="s4-1-2"><title>Clustered representations</title><p>For clustered input-layer representations, each input layer neuron encodes one task variable (that is, <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a block matrix, with nonoverlapping blocks of <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> elements equal to 1 for each task variable). In this case, in order to obtain good performance, we found it necessary to fix the coding level for each input pattern, corresponding to winner-take-all inhibition across the expansion layer.</p></sec><sec id="s4-1-3"><title>Dimension</title><p>The dimension of the expansion layer representation (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) is given by <xref ref-type="bibr" rid="bib1">Abbott et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow class="MJX-TeXAtom-ORD"><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the eigenvalues of the covariance matrix <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of expansion layer responses (not to be confused with <inline-formula><mml:math id="inf279"><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>, the eigenvalues of the kernel operator). The covariance is computed by averaging over inputs <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec id="s4-2"><title>Learning tasks</title><sec id="s4-2-1"><title>Random categorization task</title><p>In a random categorization task (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, <xref ref-type="fig" rid="fig7">Figure 7B</xref>), the network learns to associate a random input pattern <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with a random binary category <inline-formula><mml:math id="inf283"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The elements of <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are drawn i.i.d. from a normal distribution with mean 0 and variance <inline-formula><mml:math id="inf285"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>. Test patterns <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are generated by adding noise to the training patterns:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For <xref ref-type="fig" rid="fig2">Figure 2E</xref>, <xref ref-type="fig" rid="fig7">Figure 7B</xref>, and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, we set <inline-formula><mml:math id="inf288"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2-2"><title>Gaussian process tasks</title><p>To generate a family of tasks with continuously varying outputs (<xref ref-type="fig" rid="fig2">Figure 2D and F</xref>, <xref ref-type="fig" rid="fig4">Figure 4F and G</xref>, <xref ref-type="fig" rid="fig5">Figure 5B</xref>, and <xref ref-type="fig" rid="fig6">Figure 6</xref>), we sampled target functions from a Gaussian process (<xref ref-type="bibr" rid="bib68">Rasmussen and Williams, 2006</xref>), <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="script">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with covariance<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf290"><mml:mi>γ</mml:mi></mml:math></inline-formula> determines the spatial scale of variations in <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Training and test patterns are drawn uniformly on the unit sphere.</p></sec><sec id="s4-2-3"><title>Learning of readout weights</title><p>With the exception of the ELL task and <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>, we performed unregularized least squares regression to determine the readout weights <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the ELL sensory cancellation task (<xref ref-type="fig" rid="fig7">Figure 7C</xref>), we used <inline-formula><mml:math id="inf293"><mml:msup><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> regularization, a.k.a. ridge regression:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">g</mml:mi><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="bold">g</mml:mi><mml:mi mathvariant="bold">e</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf294"><mml:msub><mml:mi>α</mml:mi><mml:mi>ridge</mml:mi></mml:msub></mml:math></inline-formula> is the regularization parameter. Solutions were found using Python’s scikit-learn package (<xref ref-type="bibr" rid="bib66">Pedregosa, 2011</xref>).</p><p>In <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>, we implement a model of an online climbing fiber-mediated plasticity rule. The climbing fiber activity <inline-formula><mml:math id="inf295"><mml:mi>c</mml:mi></mml:math></inline-formula> is assumed to encode the error between the target and the network prediction <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. During each of <inline-formula><mml:math id="inf297"><mml:msub><mml:mi>N</mml:mi><mml:mi>epochs</mml:mi></mml:msub></mml:math></inline-formula> training epochs, the <inline-formula><mml:math id="inf298"><mml:mi>P</mml:mi></mml:math></inline-formula> training patterns are shuffled randomly and each pattern is presented one at a time. For each pattern µ, the weights are updated according to <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>⋅</mml:mo><mml:mi>c</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Parameter values were <inline-formula><mml:math id="inf300"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.7</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>epochs</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-2-4"><title>Performance metrics</title><p>For tasks with continuous targets, the prediction of the network is given by <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are the synaptic weights of the readout from the expansion layer. Error is measured as relative mean squared error (an expectation across patterns <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the test set): <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. In practice we use a large test set to estimate this error over <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> drawn from the distribution of test patterns. For categorization tasks, the network’s prediction is given by <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Performance is measured as the fraction of incorrect predictions. Error bars represent standard error of the mean across realizations of network weights and tasks.</p></sec><sec id="s4-2-5"><title>Optimal granule–Purkinje cell weight distribution</title><p>We adapted our model to allow for comparisons with <xref ref-type="bibr" rid="bib17">Brunel et al., 2004</xref> by constraining readout weights <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be nonnegative and adding a bias, <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To guarantee that the target function is nonnegative, we set <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for the random categorization task and <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the Gaussian process tasks. The weights and bias were determined with the Python convex optimization package cvxopt (<xref ref-type="bibr" rid="bib3">Andersen et al., 2011</xref>).</p></sec></sec><sec id="s4-3"><title>Model of two-joint arm</title><p>We implemented a biophysical model of a planar two-joint arm (<xref ref-type="bibr" rid="bib31">Fagg et al., 1997</xref>). The state of the arm is specified by six variables: joint angles <inline-formula><mml:math id="inf311"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf312"><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, angular velocities <inline-formula><mml:math id="inf313"><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf314"><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and torques <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub>. The upper and lower segments of the arm have lengths <italic>l</italic><sub>1</sub> and <italic>l</italic><sub>2</sub> and masses <italic>m</italic><sub>1</sub> and <italic>m</italic><sub>2</sub>, respectively. The arm has the following dynamics:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the inertia matrix and <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the matrix of centrifugal, Coriolis, and friction forces:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>l</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>l</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>l</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf317"><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is the center of mass of the lower arm, <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub> are moments of inertia and <italic>D</italic><sub>1</sub> and <italic>D</italic><sub>2</sub> are friction terms of the upper and lower arm respectively. These parameters were <inline-formula><mml:math id="inf318"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> kg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf319"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> kg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf320"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.3</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> m</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf321"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.35</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> m</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf322"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.21</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> m</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf323"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext> kg m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf324"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.12</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext> kg m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf325"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext> kg m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf326"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext> kg m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The task is to predict the position of the hand based on the forward dynamics of the two-joint arm system, given the arm initial condition and the applied torques. More precisely, the <inline-formula><mml:math id="inf327"><mml:mi>P</mml:mi></mml:math></inline-formula> network inputs <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> were generated by sampling 6-dimensional Gaussian vectors with covariance matrix <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, to account for the fact that angles, angular velocities and torques might vary on different scales across simulations. For our results, we used <inline-formula><mml:math id="inf330"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf331"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Each sample <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> was then normalized and used to generate initial conditions of the arm, by setting <inline-formula><mml:math id="inf333"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>1</mml:mn><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf334"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>π</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf335"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>3</mml:mn><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf336"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>4</mml:mn><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Torques were generated by setting <inline-formula><mml:math id="inf337"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mn>1</mml:mn><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>5</mml:mn><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf338"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>6</mml:mn><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The target was constructed by running the dynamics of the arm forward in time for a time <inline-formula><mml:math id="inf339"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.2</mml:mn><mml:mo>⁢</mml:mo><mml:mtext> s</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, and by computing the difference in position of the “hand” (i.e. the end of the lower segment) in Cartesian coordinates. As a result, the target in this task is two-dimensional, with each target dimension corresponding the one of the two Cartesian coordinates of the hand. The overall performance is assessed by computing the error on each task separately and then averaging the errors.</p></sec><sec id="s4-4"><title>Model of electrosensory lateral line lobe (ELL)</title><p>We simulated 20,000 granule cells using the biophysical model of <xref ref-type="bibr" rid="bib47">Kennedy et al., 2014</xref>. We varied the granule cell layer coding level by adjusting the spiking threshold parameter in the model. For each choice of threshold, we generated 30 different trials of spike rasters. Each trial is 160ms long with a 1ms time bin and consists of a time-locked response to an electric organ discharge command. Trial-to-trial variability in the model granule cell responses arises from noise in the mossy fiber responses. To generate training and testing data, we sampled 4 trials (<inline-formula><mml:math id="inf340"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:math></inline-formula> patterns) from the 30 total trials for training and 10 trials for testing (1600 patterns). Coding level is measured as the fraction of granule cells that spike at least once in the training data. We repeated this sampling process 30 times.</p><p>The targets were smoothed broad-spike responses of 15 MG cells time-locked to an electric organ discharge command measured during experiments (<xref ref-type="bibr" rid="bib57">Muller et al., 2019</xref>). The original data set consisted of 55 MG cells, each with a 300ms long spike raster with a 1ms time bin. The spike rasters were trial-averaged and then smoothed with a Gaussian-weighted moving average with a 10ms time window. Only MG cells whose maximum spiking probability across all time bins exceeded 0.01 after smoothing were included in the task. The same MG cell responses were used for both training and testing. To match the length of the granule cell data, we discarded MG cell data beyond 160ms and then concatenated 4 copies of the 160ms long responses for training and 10 copies for testing. We measured the ability of the model to construct MG cell targets out of granule cell activity, generalizing across noise in granule cell responses. Errors for each MG cell target were averaged across the 30 repetitions of sampling of training and testing data, and then averaged across targets. Standard error of the mean was computed across the 30 repetitions.</p></sec><sec id="s4-5"><title>Model of vestibulo-ocular reflex (VOR)</title><p>Recordings of Purkinje cell activity in monkeys suggest that these neurons exhibit piecewise-linear tuning to head velocity (<xref ref-type="bibr" rid="bib53">Lisberger et al., 1994</xref>). Thus, we designed piecewise-linear target functions representing Purkinje cell firing rate as a function of head velocity <inline-formula><mml:math id="inf341"><mml:mi>v</mml:mi></mml:math></inline-formula>, a one-dimensional input:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mi>c</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Inputs <inline-formula><mml:math id="inf342"><mml:mi>v</mml:mi></mml:math></inline-formula> were sampled uniformly from <inline-formula><mml:math id="inf343"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> 100 times. We generated 25 total target functions using all combinations of slopes <italic>m</italic><sub>1</sub> and <italic>m</italic><sub>2</sub> sampled from 5 equally spaced points on the interval <inline-formula><mml:math id="inf344"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. We set <inline-formula><mml:math id="inf345"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf346"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Mossy fiber responses to head velocity input were modeled as exponential tuning curves:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <italic>g</italic><sub><italic>j</italic></sub> is a gain term, <inline-formula><mml:math id="inf347"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> determines a mossy fiber preference for positive or negative velocities, and <italic>b</italic><sub><italic>j</italic></sub> is the baseline firing rate. We generated 24 different tuning curves from all combinations of the following parameter values: The gain <italic>g</italic><sub><italic>j</italic></sub> was sampled from 6 equally spaced points on the interval <inline-formula><mml:math id="inf348"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, <italic>r</italic><sub><italic>j</italic></sub> was set to either –1 or 1, and <italic>b</italic><sub><italic>j</italic></sub> was set to either 0 or 1. Qualitative results did not depend strongly on this parameterization. Mossy fiber to granule cell weights were random zero-mean Gaussians. Errors were averaged across targets.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Investigation, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Investigation, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82914-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Code implementing the model is available on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/marjoriexie/cerebellar-task-dependent">https://github.com/marjoriexie/cerebellar-task-dependent</ext-link> (copy archived at <xref ref-type="bibr" rid="bib87">Xie, 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank L F Abbott, N A Cayco-Gajic, N Sawtell, and S Fusi for helpful discussions and comments on the manuscript. The authors also thank S Muller for contributing data, code, and helpful discussions for analysis of ELL data. M X was supported by NIH grant T32-NS064929. S M was supported by the Simons and Swartz Foundations. K D H was supported by a grant from the Washington Research Foundation. A L-K was supported by the Simons and Burroughs Wellcome Foundations. M X, S M, and A L-K were also supported by the Gatsby Charitable Foundation and NSF NeuroNex award DBI-1707398.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Interactions between intrinsic and stimulus-evoked activity in recurrent neural networks</chapter-title><person-group person-group-type="editor"><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><source>The Dynamic Brain: An Exploration of Neuronal Variability and Its Functional Significance</source><publisher-name>Oxford University Press</publisher-name><fpage>65</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195393798.001.0001</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albus</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>A theory of cerebellar function</article-title><source>Mathematical Biosciences</source><volume>10</volume><fpage>25</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/0025-5564(71)90051-4</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>M</given-names></name><name><surname>Dahl</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Vanderberghe</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Interior-point methods for large-scale cone programming</chapter-title><person-group person-group-type="editor"><name><surname>Andersen</surname><given-names>M</given-names></name></person-group><source>Optimization for Machine Learning</source><publisher-name>MIT Press</publisher-name><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.7551/mitpress/8996.001.0001</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Apps</surname><given-names>R</given-names></name><name><surname>Hawkes</surname><given-names>R</given-names></name><name><surname>Aoki</surname><given-names>S</given-names></name><name><surname>Bengtsson</surname><given-names>F</given-names></name><name><surname>Brown</surname><given-names>AM</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Ebner</surname><given-names>TJ</given-names></name><name><surname>Isope</surname><given-names>P</given-names></name><name><surname>Jörntell</surname><given-names>H</given-names></name><name><surname>Lackey</surname><given-names>EP</given-names></name><name><surname>Lawrenson</surname><given-names>C</given-names></name><name><surname>Lumb</surname><given-names>B</given-names></name><name><surname>Schonewille</surname><given-names>M</given-names></name><name><surname>Sillitoe</surname><given-names>RV</given-names></name><name><surname>Spaeth</surname><given-names>L</given-names></name><name><surname>Sugihara</surname><given-names>I</given-names></name><name><surname>Valera</surname><given-names>A</given-names></name><name><surname>Voogd</surname><given-names>J</given-names></name><name><surname>Wylie</surname><given-names>DR</given-names></name><name><surname>Ruigrok</surname><given-names>TJH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cerebellar modules and their role as operational cerebellar processing units: a consensus paper</article-title><source>Cerebellum</source><volume>17</volume><fpage>654</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1007/s12311-018-0952-3</pub-id><pub-id pub-id-type="pmid">29876802</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Atkinson</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Spherical Harmonics and Approximations on the Unit Sphere: An Introduction</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-25983-8</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparseness and expansion in sensory representations</article-title><source>Neuron</source><volume>83</volume><fpage>1213</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id><pub-id pub-id-type="pmid">25155954</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sharp Analysis of Low-Rank Kernel Matrix Approximations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1208.2015">https://arxiv.org/abs/1208.2015</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Breaking the curse of dimensionality with convex neural networks</article-title><source>Journal of Machine Learning Research</source><volume>18</volume><fpage>1</fpage><lpage>53</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badura</surname><given-names>A</given-names></name><name><surname>De Zeeuw</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cerebellar granule cells: dense, rich and evolving representations</article-title><source>Current Biology</source><volume>27</volume><fpage>R415</fpage><lpage>R418</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.04.009</pub-id><pub-id pub-id-type="pmid">28586665</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>3844</fpage><lpage>3856</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2753-12.2013</pub-id><pub-id pub-id-type="pmid">23447596</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Basri</surname><given-names>R</given-names></name><name><surname>Jacobs</surname><given-names>D</given-names></name><name><surname>Kasten</surname><given-names>Y</given-names></name><name><surname>Kritchman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The convergence rate of neural networks for learned functions of different frequencies</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>4761</fpage><lpage>4771</lpage></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>CC</given-names></name><name><surname>Han</surname><given-names>V</given-names></name><name><surname>Sawtell</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cerebellum-like structures and their implications for cerebellar function</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094225</pub-id><pub-id pub-id-type="pmid">18275284</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bietti</surname><given-names>A</given-names></name><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep Equals Shallow for ReLU Networks in Kernel Regimes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2009.14397">https://arxiv.org/abs/2009.14397</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billings</surname><given-names>G</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Lőrincz</surname><given-names>A</given-names></name><name><surname>Nusser</surname><given-names>Z</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Network structure within the cerebellar input layer enables lossless sparse encoding</article-title><source>Neuron</source><volume>83</volume><fpage>960</fpage><lpage>974</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.020</pub-id><pub-id pub-id-type="pmid">25123311</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Canatar</surname><given-names>A</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spectrum dependent learning curves in kernel regression and wide neural networks</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>1024</fpage><lpage>1034</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>IE</given-names></name><name><surname>Bower</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Congruence of mossy fiber and climbing fiber tactile projections in the lateral hemispheres of the rat cerebellum</article-title><source>The Journal of Comparative Neurology</source><volume>429</volume><fpage>59</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1002/1096-9861(20000101)429:1&lt;59::aid-cne5&gt;3.0.co;2-3</pub-id><pub-id pub-id-type="pmid">11086289</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Hakim</surname><given-names>V</given-names></name><name><surname>Isope</surname><given-names>P</given-names></name><name><surname>Nadal</surname><given-names>JP</given-names></name><name><surname>Barbour</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Optimal information storage and the distribution of synaptic weights: perceptron versus Purkinje cell</article-title><source>Neuron</source><volume>43</volume><fpage>745</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.08.023</pub-id><pub-id pub-id-type="pmid">15339654</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Saddlepoint Approximations with Applications</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511619083</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canatar</surname><given-names>A</given-names></name><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Out-of-distribution generalization in kernel regression</article-title><source>Advances in Neural Information Processing Systems</source><volume>34</volume><fpage>12600</fpage><lpage>12612</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canatar</surname><given-names>A</given-names></name><name><surname>Bordelon</surname><given-names>B</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2914</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23103-1</pub-id><pub-id pub-id-type="pmid">34006842</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cayco-Gajic</surname><given-names>NA</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1116</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01109-y</pub-id><pub-id pub-id-type="pmid">29061964</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cayco-Gajic</surname><given-names>NA</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Re-evaluating circuit mechanisms underlying pattern separation</article-title><source>Neuron</source><volume>101</volume><fpage>584</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.044</pub-id><pub-id pub-id-type="pmid">30790539</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chadderton</surname><given-names>P</given-names></name><name><surname>Margrie</surname><given-names>TW</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Integration of quanta in cerebellar granule cells during sensory processing</article-title><source>Nature</source><volume>428</volume><fpage>856</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1038/nature02442</pub-id><pub-id pub-id-type="pmid">15103377</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chizat</surname><given-names>L</given-names></name><name><surname>Oyallon</surname><given-names>E</given-names></name><name><surname>Bach</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On Lazy Training in Differentiable Programming</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.07956">https://arxiv.org/abs/1812.07956</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>Y</given-names></name><name><surname>Saul</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Large-margin classification in infinite neural networks</article-title><source>Neural Computation</source><volume>22</volume><fpage>2678</fpage><lpage>2697</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00018</pub-id><pub-id pub-id-type="pmid">20608866</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Sugrue</surname><given-names>LP</given-names></name><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Clark</surname><given-names>AM</given-names></name><name><surname>Hosseini</surname><given-names>P</given-names></name><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Bradley</surname><given-names>DC</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Chang</surname><given-names>SW</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name><name><surname>Lisberger</surname><given-names>SG</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name><name><surname>Finn</surname><given-names>IM</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>369</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1038/nn.2501</pub-id><pub-id pub-id-type="pmid">20173745</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Nadal</surname><given-names>JP</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Storage of correlated patterns in standard and bistable Purkinje cell models</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002448</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002448</pub-id><pub-id pub-id-type="pmid">22570592</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Optimal properties of analog perceptrons with excitatory weights</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002919</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002919</pub-id><pub-id pub-id-type="pmid">23436991</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eccles</surname><given-names>JC</given-names></name><name><surname>Llinás</surname><given-names>R</given-names></name><name><surname>Sasaki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>The mossy fibre-granule cell relay of the cerebellum and its inhibitory control by Golgi cells</article-title><source>Experimental Brain Research</source><volume>1</volume><fpage>82</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1007/BF00235211</pub-id><pub-id pub-id-type="pmid">5910945</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eccles</surname><given-names>JC</given-names></name><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Szentágothai</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>The Cerebellum as a Neuronal Machine</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-662-13147-3</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fagg</surname><given-names>AH</given-names></name><name><surname>Sitkoff</surname><given-names>N</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name><name><surname>Houk</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Cerebellar learning for control of a two-link arm in muscle space</article-title><conf-name>Proceedings of International Conference on Robotics and Automation</conf-name><fpage>2638</fpage><lpage>2644</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>EA</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name><name><surname>Tadross</surname><given-names>MR</given-names></name><name><surname>Hull</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Local Synaptic Inhibition Mediates Cerebellar Granule Cell Pattern Separation Necessary for Learned Sensorimotor Associations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.20.492839</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garwicz</surname><given-names>M</given-names></name><name><surname>Jörntell</surname><given-names>H</given-names></name><name><surname>Ekerot</surname><given-names>C-F</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cutaneous receptive fields and topography of mossy fibres and climbing fibres projecting to cat cerebellar C3 zone</article-title><source>The Journal of Physiology</source><volume>512</volume><fpage>277</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7793.1998.277bf.x</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerace</surname><given-names>F</given-names></name><name><surname>Loureiro</surname><given-names>B</given-names></name><name><surname>Krzakala</surname><given-names>F</given-names></name><name><surname>Mézard</surname><given-names>M</given-names></name><name><surname>Zdeborová</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Generalisation error in learning with random features and the hidden manifold model*</article-title><source>Journal of Statistical Mechanics</source><volume>2021</volume><elocation-id>124013</elocation-id><pub-id pub-id-type="doi">10.1088/1742-5468/ac3ae6</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gerace</surname><given-names>F</given-names></name><name><surname>Krzakala</surname><given-names>F</given-names></name><name><surname>Loureiro</surname><given-names>B</given-names></name><name><surname>Stephan</surname><given-names>L</given-names></name><name><surname>Zdeborová</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Gaussian Universality of Linear Classifiers with Random Labels in High-Dimension</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.13303">https://arxiv.org/abs/2205.13303</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>M</given-names></name><name><surname>Chris Miall</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>How and why the cerebellum recodes input signals: an alternative to machine learning</article-title><source>The Neuroscientist</source><volume>28</volume><fpage>206</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1177/1073858420986795</pub-id><pub-id pub-id-type="pmid">33559532</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Badura</surname><given-names>A</given-names></name><name><surname>Deverett</surname><given-names>B</given-names></name><name><surname>Najafi</surname><given-names>F</given-names></name><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Ozden</surname><given-names>I</given-names></name><name><surname>Kloth</surname><given-names>AD</given-names></name><name><surname>Pnevmatikakis</surname><given-names>E</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>De Zeeuw</surname><given-names>CI</given-names></name><name><surname>Medina</surname><given-names>JF</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cerebellar granule cells acquire a widespread predictive feedback signal during motor learning</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>727</fpage><lpage>734</lpage><pub-id pub-id-type="doi">10.1038/nn.4531</pub-id><pub-id pub-id-type="pmid">28319608</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Mao</surname><given-names>H</given-names></name><name><surname>Dally</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1510.00149">https://arxiv.org/abs/1510.00149</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heath</surname><given-names>NC</given-names></name><name><surname>Rizwan</surname><given-names>AP</given-names></name><name><surname>Engbers</surname><given-names>JDT</given-names></name><name><surname>Anderson</surname><given-names>D</given-names></name><name><surname>Zamponi</surname><given-names>GW</given-names></name><name><surname>Turner</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The expression pattern of a Cav3-Kv4 complex differentially regulates spike output in cerebellar granule cells</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>8800</fpage><lpage>8812</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0981-14.2014</pub-id><pub-id pub-id-type="pmid">24966380</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honegger</surname><given-names>KS</given-names></name><name><surname>Campbell</surname><given-names>RAA</given-names></name><name><surname>Turner</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cellular-resolution population imaging reveals robust sparse coding in the <italic>Drosophila</italic> mushroom body</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>11772</fpage><lpage>11785</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1099-11.2011</pub-id><pub-id pub-id-type="pmid">21849538</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>CC</given-names></name><name><surname>Sugino</surname><given-names>K</given-names></name><name><surname>Shima</surname><given-names>Y</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Bai</surname><given-names>S</given-names></name><name><surname>Mensh</surname><given-names>BD</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Convergence of pontine and proprioceptive streams onto multimodal cerebellar granule cells</article-title><source>eLife</source><volume>2</volume><elocation-id>e00400</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00400</pub-id><pub-id pub-id-type="pmid">23467508</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishikawa</surname><given-names>T</given-names></name><name><surname>Shimuta</surname><given-names>M</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multimodal sensory integration in single cerebellar granule cells in vivo</article-title><source>eLife</source><volume>4</volume><elocation-id>e12916</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12916</pub-id><pub-id pub-id-type="pmid">26714108</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Itō</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1984">1984</year><source>The Cerebellum and Neural Control</source><publisher-name>Raven Press</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jacot</surname><given-names>A</given-names></name><name><surname>Gabriel</surname><given-names>F</given-names></name><name><surname>Hongler</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural tangent kernel: Convergence and generalization in neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jörntell</surname><given-names>H</given-names></name><name><surname>Ekerot</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Reciprocal bidirectional plasticity of parallel fiber receptive fields in cerebellar Purkinje cells and their afferent interneurons</article-title><source>Neuron</source><volume>34</volume><fpage>797</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00713-4</pub-id><pub-id pub-id-type="pmid">12062025</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jörntell</surname><given-names>H</given-names></name><name><surname>Ekerot</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Properties of somatosensory synaptic integration in cerebellar granule cells in vivo</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>11786</fpage><lpage>11797</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2939-06.2006</pub-id><pub-id pub-id-type="pmid">17093099</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Alviña</surname><given-names>K</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sawtell</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A temporal basis for predicting the sensory consequences of motor commands in an electric fish</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>416</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1038/nn.3650</pub-id><pub-id pub-id-type="pmid">24531306</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knogler</surname><given-names>LD</given-names></name><name><surname>Markov</surname><given-names>DA</given-names></name><name><surname>Dragomir</surname><given-names>EI</given-names></name><name><surname>Štih</surname><given-names>V</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sensorimotor representations in cerebellar granule cells in larval zebrafish are dense, spatially organized, and non-temporally patterned</article-title><source>Current Biology</source><volume>27</volume><fpage>1288</fpage><lpage>1302</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.03.029</pub-id><pub-id pub-id-type="pmid">28434864</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koltchinskii</surname><given-names>V</given-names></name><name><surname>Giné</surname><given-names>E</given-names></name><name><surname>Gine</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Random matrix approximation of spectra of integral operators</article-title><source>Bernoulli</source><volume>6</volume><elocation-id>113</elocation-id><pub-id pub-id-type="doi">10.2307/3318636</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosambi</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="1943">1943</year><article-title>Statistics in function space</article-title><source>Journal of the Indian Mathematical Society</source><volume>7</volume><fpage>76</fpage><lpage>88</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreitzer</surname><given-names>AC</given-names></name><name><surname>Regehr</surname><given-names>WG</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Retrograde inhibition of presynaptic calcium influx by endogenous cannabinoids at excitatory synapses onto Purkinje cells</article-title><source>Neuron</source><volume>29</volume><fpage>717</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00246-x</pub-id><pub-id pub-id-type="pmid">11301030</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>AC</given-names></name><name><surname>Bygrave</surname><given-names>AM</given-names></name><name><surname>de Calignon</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>T</given-names></name><name><surname>Miesenböck</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>559</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1038/nn.3660</pub-id><pub-id pub-id-type="pmid">24561998</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisberger</surname><given-names>SG</given-names></name><name><surname>Pavelko</surname><given-names>TA</given-names></name><name><surname>Bronte-Stewart</surname><given-names>HM</given-names></name><name><surname>Stone</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neural basis for motor learning in the vestibuloocular reflex of primates. II. Changes in the responses of horizontal gaze velocity Purkinje cells in the cerebellar flocculus and ventral paraflocculus</article-title><source>Journal of Neurophysiology</source><volume>72</volume><fpage>954</fpage><lpage>973</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.72.2.954</pub-id><pub-id pub-id-type="pmid">7983548</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal degrees of synaptic connectivity</article-title><source>Neuron</source><volume>93</volume><fpage>1153</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id><pub-id pub-id-type="pmid">28215558</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>A theory of cerebellar cortex</article-title><source>The Journal of Physiology</source><volume>202</volume><fpage>437</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1969.sp008820</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Modi</surname><given-names>MN</given-names></name><name><surname>Shuai</surname><given-names>Y</given-names></name><name><surname>Turner</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The <italic>Drosophila</italic> mushroom body: from architecture to algorithm in a learning circuit</article-title><source>Annual Review of Neuroscience</source><volume>43</volume><fpage>465</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-080317-0621333</pub-id><pub-id pub-id-type="pmid">32283995</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>SZ</given-names></name><name><surname>Zadina</surname><given-names>AN</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sawtell</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Continual learning in a multi-layer network of an electric fish</article-title><source>Cell</source><volume>179</volume><fpage>1382</fpage><lpage>1392</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.10.020</pub-id><pub-id pub-id-type="pmid">31735497</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muscinelli</surname><given-names>SP</given-names></name><name><surname>Wagner</surname><given-names>MJ</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Optimal routing to cerebellum-like structures</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>1630</fpage><lpage>1641</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01403-7</pub-id><pub-id pub-id-type="pmid">37604889</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nash</surname><given-names>D</given-names></name><name><surname>Klamkin</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>A spherical characterization of the normal distribution</article-title><source>Journal of Mathematical Analysis and Applications</source><volume>55</volume><fpage>156</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/0022-247X(76)90285-7</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>TM</given-names></name><name><surname>Thomas</surname><given-names>LA</given-names></name><name><surname>Rhoades</surname><given-names>JL</given-names></name><name><surname>Ricchi</surname><given-names>I</given-names></name><name><surname>Yuan</surname><given-names>XC</given-names></name><name><surname>Sheridan</surname><given-names>A</given-names></name><name><surname>Hildebrand</surname><given-names>DGC</given-names></name><name><surname>Funke</surname><given-names>J</given-names></name><name><surname>Regehr</surname><given-names>WG</given-names></name><name><surname>Lee</surname><given-names>W-CA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Structured cerebellar connectivity supports resilient pattern separation</article-title><source>Nature</source><volume>613</volume><fpage>543</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05471-w</pub-id><pub-id pub-id-type="pmid">36418404</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sparse coding of sensory inputs</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>481</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.07.007</pub-id><pub-id pub-id-type="pmid">15321069</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozden</surname><given-names>I</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name><name><surname>Hoogland</surname><given-names>TM</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Wang</surname><given-names>SSH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Widespread state-dependent shifts in cerebellar activity in locomoting mice</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e42650</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0042650</pub-id><pub-id pub-id-type="pmid">22880068</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Palay</surname><given-names>SL</given-names></name><name><surname>Chan-Palay</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1974">1974</year><source>Cerebellar Cortex: Cytology and Organization</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-65581-4</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rahimi</surname><given-names>A</given-names></name><name><surname>Recht</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Random features for large-scale kernel machines</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>CE</given-names></name><name><surname>Williams</surname><given-names>CKI</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Gaussian Processes for Machine Learning</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/3206.001.0001</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saarinen</surname><given-names>A</given-names></name><name><surname>Linne</surname><given-names>ML</given-names></name><name><surname>Yli-Harja</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Stochastic differential equation model for cerebellar granule cell excitability</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000004</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000004</pub-id><pub-id pub-id-type="pmid">18463700</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargent</surname><given-names>PB</given-names></name><name><surname>Saviane</surname><given-names>C</given-names></name><name><surname>Nielsen</surname><given-names>TA</given-names></name><name><surname>DiGregorio</surname><given-names>DA</given-names></name><name><surname>Silver</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Rapid vesicular release, quantal variability, and spillover contribute to the precision and reliability of transmission at a glomerular synapse</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>8173</fpage><lpage>8187</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2051-05.2005</pub-id><pub-id pub-id-type="pmid">16148225</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweighofer</surname><given-names>N</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Lay</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Unsupervised learning of granule cell sparse codes enhances cerebellar adaptive control</article-title><source>Neuroscience</source><volume>103</volume><fpage>35</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/s0306-4522(00)00548-0</pub-id><pub-id pub-id-type="pmid">11311786</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Cui</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Towards Out-of-Distribution Generalization: A Survey</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2108.13624">https://arxiv.org/abs/2108.13624</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical control of arm movements: A dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150509</pub-id><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>JB</given-names></name><name><surname>Dickens</surname><given-names>M</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Theory of the Inductive Bias and Generalization of Kernel Regression and Wide Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.03922">https://arxiv.org/abs/2110.03922</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sollich</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Approximate learning curves for Gaussian processes</article-title><conf-name>9th International Conference on Artificial Neural Networks</conf-name><pub-id pub-id-type="doi">10.1049/cp:19991148</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spanne</surname><given-names>A</given-names></name><name><surname>Jörntell</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Processing of multi-dimensional sensorimotor information in the spinal and cerebellar neuronal circuitry: A new hypothesis</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002979</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002979</pub-id><pub-id pub-id-type="pmid">23516353</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spanne</surname><given-names>A</given-names></name><name><surname>Jörntell</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Questioning the role of sparse coding in the brain</article-title><source>Trends in Neurosciences</source><volume>38</volume><fpage>417</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2015.05.005</pub-id><pub-id pub-id-type="pmid">26093844</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strick</surname><given-names>PL</given-names></name><name><surname>Dum</surname><given-names>RP</given-names></name><name><surname>Fiez</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Cerebellum and nonmotor function</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>413</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125606</pub-id><pub-id pub-id-type="pmid">19555291</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>GC</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Olfactory representations by <italic>Drosophila</italic> mushroom body neurons</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>734</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1152/jn.01283.2007</pub-id><pub-id pub-id-type="pmid">18094099</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kan</surname><given-names>PL</given-names></name><name><surname>Gibson</surname><given-names>AR</given-names></name><name><surname>Houk</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Movement-related inputs to intermediate cerebellum of the monkey</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>74</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.1.74</pub-id><pub-id pub-id-type="pmid">8433135</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>MJ</given-names></name><name><surname>Kim</surname><given-names>TH</given-names></name><name><surname>Savall</surname><given-names>J</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cerebellar granule cells encode the expectation of reward</article-title><source>Nature</source><volume>544</volume><fpage>96</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/nature21726</pub-id><pub-id pub-id-type="pmid">28321129</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>MJ</given-names></name><name><surname>Kim</surname><given-names>TH</given-names></name><name><surname>Kadmon</surname><given-names>J</given-names></name><name><surname>Nguyen</surname><given-names>ND</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared cortex-cerebellum dynamics in the execution and learning of a motor task</article-title><source>Cell</source><volume>177</volume><fpage>669</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.02.019</pub-id><pub-id pub-id-type="pmid">30929904</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witter</surname><given-names>L</given-names></name><name><surname>De Zeeuw</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>In vivo differences in inputs and spiking between neurons in lobules VI/VII of neocerebellum and lobule X of archaeocerebellum</article-title><source>Cerebellum</source><volume>14</volume><fpage>506</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1007/s12311-015-0654-z</pub-id><pub-id pub-id-type="pmid">25735968</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Miall</surname><given-names>RC</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Internal models in the cerebellum</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>338</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(98)01221-2</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>cerebellar-task-dependents</data-title><version designator="swh:1:rev:f56a3522566ac2273a65e0cd1d5f717e034c9312">swh:1:rev:f56a3522566ac2273a65e0cd1d5f717e034c9312</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d284e024f90aff0f391b48b4c5423d01bb57c1cf;origin=https://github.com/marjoriexie/cerebellar-task-dependent;visit=swh:1:snp:1434845b85acbf3f3093e0827e0e98af57e246de;anchor=swh:1:rev:f56a3522566ac2273a65e0cd1d5f717e034c9312">https://archive.softwareheritage.org/swh:1:dir:d284e024f90aff0f391b48b4c5423d01bb57c1cf;origin=https://github.com/marjoriexie/cerebellar-task-dependent;visit=swh:1:snp:1434845b85acbf3f3093e0827e0e98af57e246de;anchor=swh:1:rev:f56a3522566ac2273a65e0cd1d5f717e034c9312</ext-link></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>May</surname><given-names>A</given-names></name><name><surname>Dao</surname><given-names>T</given-names></name><name><surname>Ré</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Low-precision random fourier features for memory-constrained kernel approximation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1811.00155">https://arxiv.org/abs/1811.00155</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>1 Connection between kernel and previous theories</title><p>Previous theories (<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>) studied generalization performance for random clusters of inputs associated with binary targets, where test patterns are formed by adding noise to training patterns (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The readout is trained using a supervised Hebbian rule with mean-subtracted expansion layer responses, <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The net input to a readout in response to a test pattern <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> from cluster µ is <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The statistics of <inline-formula><mml:math id="inf353"><mml:msup><mml:mi>g</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> determine generalization performance. For a Hebbian readout, the error rate is expressed in terms of the signal-to-noise ratio (SNR) (<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>):<disp-formula id="equ14"><label>(A1)</label><mml:math id="m14"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>SNR is given in terms of the mean and variance of <inline-formula><mml:math id="inf354"><mml:msup><mml:mi>g</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>:<disp-formula id="equ15"><label>(A2)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:msup><mml:mi>g</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The numerator of SNR is proportional to the average overlap of the expansion layer representations of training and test patterns belonging to the same cluster, which can be expressed in terms of the kernel function <inline-formula><mml:math id="inf355"><mml:mi>K</mml:mi></mml:math></inline-formula>:<disp-formula id="equ16"><label>(A3)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:msup><mml:mi>g</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>μ</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>μ</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For large networks with Gaussian i.i.d. expansion weights, <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the above equation reduces to <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf359"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>train</mml:mi><mml:mo>/</mml:mo><mml:mi>test</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the typical overlap of training and test patterns belonging to the same cluster. When <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf361"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>train</mml:mi><mml:mo>/</mml:mo><mml:mi>test</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be written as <inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where Δ is a measure of within-cluster noise (<xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>).</p><p><xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref> demonstrated that, for random categorization tasks and when <inline-formula><mml:math id="inf363"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf364"><mml:mi>D</mml:mi></mml:math></inline-formula> are large, <inline-formula><mml:math id="inf365"><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf366"><mml:mi>C</mml:mi></mml:math></inline-formula> is a constant and <inline-formula><mml:math id="inf367"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is given by<disp-formula id="equ17"><label>(A4)</label><mml:math id="m17"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Z</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>≠</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Z</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>≠</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>assuming the entries of <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are zero-mean. <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msup><mml:mi mathvariant="bold">a</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">a</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> normalizes the overlaps to the typical overlap of a pattern with itself. The quantity <inline-formula><mml:math id="inf371"><mml:msup><mml:mi>Q</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> is the ratio of the variance of overlaps between patterns belonging to different clusters in the expansion layer to that of the input layer. This describes the extent to which the geometry of the input layer representation is preserved in the expansion layer. When overlaps in the input layer are small, as they are for random clusters, <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mi>Q</mml:mi><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf373"><mml:mrow><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. This relation illustrates that, for random clusters and <inline-formula><mml:math id="inf374"><mml:mrow><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf375"><mml:mi>Q</mml:mi></mml:math></inline-formula> is equal to the slope of the normalized kernel function <inline-formula><mml:math id="inf376"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> evaluated at <inline-formula><mml:math id="inf377"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref> also showed that the dimension of the expansion layer representation is equal to <inline-formula><mml:math id="inf378"><mml:mfrac><mml:msup><mml:mi>C</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <inline-formula><mml:math id="inf379"><mml:msup><mml:mi>C</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> is a constant.</p><p>Thus, for the random categorization task studied in <xref ref-type="bibr" rid="bib6">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib54">Litwin-Kumar et al., 2017</xref>, dimension and readout SNR can be calculated by evaluating <inline-formula><mml:math id="inf380"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>train</mml:mi><mml:mo>/</mml:mo><mml:mi>test</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the slope of <inline-formula><mml:math id="inf381"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at <inline-formula><mml:math id="inf382"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s9"><title>2 Dot-product kernels with arbitrary threshold</title><p>As <inline-formula><mml:math id="inf383"><mml:mrow><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, the normalized dot product between features (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) converges pointwise to<disp-formula id="equ18"><label>(A5)</label><mml:math id="m18"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a row of the weight matrix <inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (without loss of generality, the first row) with entries drawn i.i.d. from a Gaussian distribution <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Our goal is to compute <xref ref-type="disp-formula" rid="equ18">Equation A5</xref> for a given <inline-formula><mml:math id="inf387"><mml:mi>θ</mml:mi></mml:math></inline-formula> and inputs drawn on the unit sphere <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Because the Gaussian weight distribution is spherically symmetric, <xref ref-type="disp-formula" rid="equ18">Equation A5</xref> restricted to the unit sphere <italic>for any nonlinearity</italic> is only a function of the dot-product <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>:=</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, making the kernel a dot-product kernel <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Denote by <inline-formula><mml:math id="inf391"><mml:msub><mml:mi>J</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the entries of <inline-formula><mml:math id="inf392"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf393"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf394"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> be the pre-activations for each input. Then <inline-formula><mml:math id="inf395"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are jointly Gaussian with mean 0, variance 1, and covariance <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. If <inline-formula><mml:math id="inf397"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can re-parameterize these pre-activations as the sum of an independent and shared component <inline-formula><mml:math id="inf398"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf399"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. In these coordinates, <xref ref-type="disp-formula" rid="equ18">Equation A5</xref> becomes<disp-formula id="equ19"><label>(A6)</label><mml:math id="m19"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>z</mml:mi><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the second line follows from the conditional independence of <inline-formula><mml:math id="inf402"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf403"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> and the third from the fact that they are identically distributed. Similarly, if <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can write <inline-formula><mml:math id="inf405"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf406"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>We will use <xref ref-type="disp-formula" rid="equ19">Equation A6</xref> to solve for the kernel assuming <inline-formula><mml:math id="inf407"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is a ReLU nonlinearity. Let<disp-formula id="equ20"><label>(A7)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Using the fact that <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is nonzero only when <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e. for <inline-formula><mml:math id="inf410"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, we obtain<disp-formula id="equ21"><label>(A8)</label><mml:math id="m21"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>T</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Performing a similar calculation for <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and collecting the results leads to:<disp-formula id="equ22"><label>(A9)</label><mml:math id="m22"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(A10)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ24"><label>(A11)</label><mml:math id="m24"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(A12)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>and</mml:mtext></mml:mstyle><mml:mspace width="1em"/><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt></mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s10"><title>3 Spherical harmonic decompositions</title><p>Our theory of generalization requires us to work in function spaces which are natural to the problem. The spherical harmonics are the natural basis for working with dot-product kernels on the sphere. For a thorough treatment of spherical harmonics, see <xref ref-type="bibr" rid="bib5">Atkinson and Han, 2012</xref>, whose notation we generally follow. Both our kernel and Gaussian process (GP) tasks are defined over the sphere in <inline-formula><mml:math id="inf412"><mml:mi>D</mml:mi></mml:math></inline-formula> dimensions<disp-formula id="equ26"><label>(A13)</label><mml:math id="m26"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mi>D</mml:mi></mml:msup><mml:mo>:</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>A spherical harmonic <inline-formula><mml:math id="inf413"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>—where <inline-formula><mml:math id="inf414"><mml:mi>k</mml:mi></mml:math></inline-formula> indexes frequency and <inline-formula><mml:math id="inf415"><mml:mi>m</mml:mi></mml:math></inline-formula> indexes modes of the same frequency—is a harmonic homogeneous polynomial of degree <inline-formula><mml:math id="inf416"><mml:mi>k</mml:mi></mml:math></inline-formula> restricted to the sphere <inline-formula><mml:math id="inf417"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. For each frequency <inline-formula><mml:math id="inf418"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℤ</mml:mi></mml:mrow></mml:math></inline-formula>, there are <inline-formula><mml:math id="inf419"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> linearly independent polynomials, where<disp-formula id="equ27"><label>(A14)</label><mml:math id="m27"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:mfrac><mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><sec sec-type="appendix" id="s10-1"><title>3.1 Decomposition of the kernel and target function</title><p>We remind the reader here of the setting for our theory:</p><list list-type="order"><list-item><p>Ridge regression using random features with a dot-product limiting kernel.</p></list-item><list-item><p>Data drawn uniformly from the unit sphere.</p></list-item></list><p>Let <inline-formula><mml:math id="inf420"><mml:mi>σ</mml:mi></mml:math></inline-formula> be the Lebesgue measure on <inline-formula><mml:math id="inf421"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We will denote the surface area of the sphere as<disp-formula id="equ28"><label>(A15)</label><mml:math id="m28"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>On the other hand, the uniform probability measure on the sphere, denoted by <inline-formula><mml:math id="inf422"><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>, must integrate to 1, so <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, we define the space of real-valued square integrable functions <inline-formula><mml:math id="inf424"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the Hilbert space with inner product<disp-formula id="equ29"><label>(A16)</label><mml:math id="m29"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf425"><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mi>f</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. The space <inline-formula><mml:math id="inf426"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined analogously.</p><p>Eigendecompositions describe the action of linear operators, not functions, thus we must associate a linear operator with our kernel for its eigenvalues to make sense. The kernel eigenvalues <inline-formula><mml:math id="inf427"><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> that we will use to compute the error are the eigenvalues of the integral operator <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> defined as<disp-formula id="equ30"><label>(A17)</label><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This is because <inline-formula><mml:math id="inf429"><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> is the data distribution, and these eigenvalues are approximated by the eigenvalues of the kernel matrix evaluated on a large but finite dataset (<xref ref-type="bibr" rid="bib49">Koltchinskii et al., 2000</xref>). Similarly, we define the analogous operator <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> under the measure <inline-formula><mml:math id="inf431"><mml:mi>σ</mml:mi></mml:math></inline-formula> with eigenvalues <inline-formula><mml:math id="inf432"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, the eigenvalues are related by<disp-formula id="equ31"><label>(A18)</label><mml:math id="m31"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>ξ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and they share the same eigenfunctions, up to normalization. For the rest of this section we will study eigendecompositions of operator <inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which may be translated into statements about <inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> via (18) (These differences are liable to cause some confusion and pain when reading the literature).</p><p>Under mild technical conditions that our kernels satisfy, Mercer’s theorem states that positive semidefinite kernels can be expanded as a series in the orthonormal basis of eigenfunctions <inline-formula><mml:math id="inf436"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> weighted by nonnegative eigenvalues <inline-formula><mml:math id="inf437"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ32"><label>(A19)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>ξ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Again, <inline-formula><mml:math id="inf438"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are eigenpairs for the operator <inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and form an orthonormal set under the <inline-formula><mml:math id="inf440"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> inner product.</p><p>As stated earlier, the kernel (<xref ref-type="disp-formula" rid="equ18">Equation A5</xref>) is spherically symmetric and thus a dot-product kernel. Because of this, we can take the eigenfunctions <inline-formula><mml:math id="inf441"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> to be the spherical harmonics <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The index <inline-formula><mml:math id="inf443"><mml:mi>α</mml:mi></mml:math></inline-formula> is a multi-index into mode <inline-formula><mml:math id="inf444"><mml:mi>m</mml:mi></mml:math></inline-formula> of frequency <inline-formula><mml:math id="inf445"><mml:mi>k</mml:mi></mml:math></inline-formula>. Writing the Mercer decomposition in the spherical harmonic basis gives:<disp-formula id="equ33"><label>(A20)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Because our kernel is rotation invariant, all <inline-formula><mml:math id="inf446"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> harmonics of frequency <inline-formula><mml:math id="inf447"><mml:mi>k</mml:mi></mml:math></inline-formula> share eigenvalue <inline-formula><mml:math id="inf448"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Any function in <inline-formula><mml:math id="inf449"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be expanded in the spherical harmonic basis as follows:<disp-formula id="equ34"><label>(A21)</label><mml:math id="m34"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> with </mml:mtext></mml:mstyle><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The expansion is analogous to that of the Fourier series. In fact when <inline-formula><mml:math id="inf450"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, the spherical harmonics are sines and cosines on the unit circle.</p></sec><sec sec-type="appendix" id="s10-2"><title>3.2 Ultraspherical polynomials</title><p>Adding together all harmonics of a given frequency relates them to a polynomial in <inline-formula><mml:math id="inf451"><mml:mi>t</mml:mi></mml:math></inline-formula> by the addition formula<disp-formula id="equ35"><label>(A22)</label><mml:math id="m35"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The polynomial <inline-formula><mml:math id="inf452"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf453"><mml:mi>k</mml:mi></mml:math></inline-formula> th ultraspherical polynomial. These are also called Legendre or Gegenbauer polynomials, although these usually have different normalizations and can be defined more generally.</p><p>The ultraspherical polynomials <inline-formula><mml:math id="inf454"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> form an orthogonal basis for<disp-formula id="equ36"><mml:math id="m36"><mml:mrow><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As special cases, <inline-formula><mml:math id="inf455"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf456"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the classical Chebyshev and Legendre polynomials, respectively. For any <inline-formula><mml:math id="inf457"><mml:mi>D</mml:mi></mml:math></inline-formula>, the first two of these polynomials are <inline-formula><mml:math id="inf458"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf459"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. We use the Rodrigues formula (<xref ref-type="bibr" rid="bib5">Atkinson and Han, 2012</xref>), which holds for <inline-formula><mml:math id="inf460"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf461"><mml:mrow><mml:mi>D</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, to generate these polynomials:<disp-formula id="equ37"><label>(A23)</label><mml:math id="m37"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Combining <xref ref-type="disp-formula" rid="equ33">Equation A20</xref> with the addition formula (<xref ref-type="disp-formula" rid="equ35">Equation A22</xref>), we can express the kernel in terms of ultraspherical polynomials evaluated at the dot-product of the inputs:<disp-formula id="equ38"><label>(A24)</label><mml:math id="m38"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s10-3"><title>3.3 Computing kernel eigenvalues</title><p>The Funk-Hecke theorem states that<disp-formula id="equ39"><label>(A25)</label><mml:math id="m39"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ39">Equation A25</xref> implies that the eigenvalues of <inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are given as<disp-formula id="equ40"><label>(A26)</label><mml:math id="m40"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For our kernels, the kernel eigenvalues can be conveniently computed using polar coordinates. When the entries of <inline-formula><mml:math id="inf463"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are i.i.d. unit Gaussian,<disp-formula id="equ41"><mml:math id="m41"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>⋯</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>D</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf464"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf465"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The ReLU nonlinearity is positively homogeneous, so <inline-formula><mml:math id="inf466"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We can write<disp-formula id="equ42"><label>(A27)</label><mml:math id="m42"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>:=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have introduced a new kernel <inline-formula><mml:math id="inf467"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>shell</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which is <inline-formula><mml:math id="inf468"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> times the dot-product kernel that arises when the weights are distributed uniformly on the sphere (<inline-formula><mml:math id="inf469"><mml:mi>σ</mml:mi></mml:math></inline-formula> is not the probability measure). The above equation shows that the network restricted to inputs <inline-formula><mml:math id="inf470"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> has different kernels depending on whether the weights are sampled according to a Gaussian distribution or uniformly on the sphere. Without the threshold, this difference disappears due to the positive homogeneity of the ReLU (<xref ref-type="bibr" rid="bib26">Churchland et al., 2010</xref>).</p><p>Next we expand the nonlinearity in the spherical harmonic basis (following <xref ref-type="bibr" rid="bib13">Bietti and Bach, 2021</xref>; <xref ref-type="bibr" rid="bib8">Bach, 2017</xref>)<disp-formula id="equ43"><label>(A28)</label><mml:math id="m43"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf471"><mml:mi>k</mml:mi></mml:math></inline-formula> th coefficient is given by the Funk-Hecke formula (<xref ref-type="disp-formula" rid="equ39">Equation A25</xref>) as<disp-formula id="equ44"><label>(A29)</label><mml:math id="m44"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and we explicitly note the dependence on <inline-formula><mml:math id="inf472"><mml:mi>θ</mml:mi></mml:math></inline-formula>. Using the representation <xref ref-type="disp-formula" rid="equ43">Equation A28</xref>, we can recover the eigendecomposition:<disp-formula id="equ45"><label>(A30)</label><mml:math id="m45"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold">J</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which follows from orthonormality and the addition formula (<xref ref-type="disp-formula" rid="equ35">Equation A22</xref>). We have that <inline-formula><mml:math id="inf473"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf474"><mml:mi>k</mml:mi></mml:math></inline-formula> th eigenvalue of <inline-formula><mml:math id="inf475"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>shell</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Using <xref ref-type="disp-formula" rid="equ45">Equation A30</xref> in <xref ref-type="disp-formula" rid="equ42">Equation A27</xref> leads to<disp-formula id="equ46"><label>(A31)</label><mml:math id="m46"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>i.e. the eigenvalues satisfy<disp-formula id="equ47"><label>(A32)</label><mml:math id="m47"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><sec sec-type="appendix" id="s10-3-1"><title>3.3.1 Eigenvalues of <inline-formula><mml:math id="inf476"><mml:msub><mml:mi>K</mml:mi><mml:mi>shell</mml:mi></mml:msub></mml:math></inline-formula></title><p>It is possible to compute <inline-formula><mml:math id="inf477"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> analytically (<xref ref-type="bibr" rid="bib13">Bietti and Bach, 2021</xref>; <xref ref-type="bibr" rid="bib8">Bach, 2017</xref>). Letting<disp-formula id="equ48"><label>(A33)</label><mml:math id="m48"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mi>t</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>we have that <xref ref-type="disp-formula" rid="equ44">Equation A29</xref> reduces to <inline-formula><mml:math id="inf478"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ48">Equation A33</xref> requires <inline-formula><mml:math id="inf479"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but <inline-formula><mml:math id="inf480"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ47">Equation A32</xref> as <inline-formula><mml:math id="inf481"><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. So we take <inline-formula><mml:math id="inf482"><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which still assures that <xref ref-type="disp-formula" rid="equ44">Equation A29</xref> is satisfied. For the rest of this section, assume wlog that <inline-formula><mml:math id="inf483"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Using Rodrigues’ formula (<xref ref-type="disp-formula" rid="equ37">Equation A23</xref>) in <xref ref-type="disp-formula" rid="equ48">Equation A33</xref> gives<disp-formula id="equ49"><mml:math id="m49"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>:=</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mi>t</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mi>t</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which may be integrated by parts. We will treat <inline-formula><mml:math id="inf484"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and 1 separately.</p><p>In the case of <inline-formula><mml:math id="inf485"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, since <inline-formula><mml:math id="inf486"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> we have the integral of a derivative, so for <inline-formula><mml:math id="inf487"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula><disp-formula id="equ50"><mml:math id="m50"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>When <inline-formula><mml:math id="inf488"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> we find that<disp-formula id="equ51"><mml:math id="m51"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thickmathspace"/><mml:msub><mml:mrow/><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mspace width="thickmathspace"/><mml:msub><mml:mrow/><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf489"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we integrate by parts once and find that for <inline-formula><mml:math id="inf490"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ52"><mml:math id="m52"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>When <inline-formula><mml:math id="inf491"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we have a straightforward integral<disp-formula id="equ53"><mml:math id="m53"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Finally, for <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we obtain<disp-formula id="equ54"><mml:math id="m54"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mrow/><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mrow/><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s10-3-2"><title>3.3.2 Properties of the eigenvalues of <inline-formula><mml:math id="inf493"><mml:msub><mml:mi>K</mml:mi><mml:mi>shell</mml:mi></mml:msub></mml:math></inline-formula></title><p>The above show that for <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula><disp-formula id="equ55"><label>(A34)</label><mml:math id="m55"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Taking <inline-formula><mml:math id="inf495"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> leads to <inline-formula><mml:math id="inf496"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, since fewer derivatives than <inline-formula><mml:math id="inf497"><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> appear in <xref ref-type="disp-formula" rid="equ55">Equation A34</xref>, which reflects the fact that higher degree ultraspherical polynomials are orthogonal to a linear function. Furthermore, since <inline-formula><mml:math id="inf498"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is an even function, the parity of <italic>a</italic><sub><italic>k</italic></sub> as a function of <inline-formula><mml:math id="inf499"><mml:mi>θ</mml:mi></mml:math></inline-formula> matches the parity of <inline-formula><mml:math id="inf500"><mml:mi>k</mml:mi></mml:math></inline-formula>. However, <italic>a</italic><sub><italic>k</italic></sub> appears squared in <xref ref-type="disp-formula" rid="equ47">Equation A32</xref>, so <inline-formula><mml:math id="inf501"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> will always be an even function of <inline-formula><mml:math id="inf502"><mml:mi>θ</mml:mi></mml:math></inline-formula>. This explains the parity symmetry of the eigenvalues with coding level for <inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. Also, <xref ref-type="disp-formula" rid="equ55">Equation A34</xref> for <inline-formula><mml:math id="inf504"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> gives <inline-formula><mml:math id="inf505"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf506"><mml:mi>k</mml:mi></mml:math></inline-formula> is odd, as was shown by <xref ref-type="bibr" rid="bib8">Bach, 2017</xref>; <xref ref-type="bibr" rid="bib11">Basri et al., 2019</xref>. This is because<disp-formula id="equ56"><mml:math id="m56"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>j</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext> if </mml:mtext><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mtext> is odd</mml:mtext></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>because the <inline-formula><mml:math id="inf507"><mml:mi>j</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf508"><mml:mrow><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> terms have opposite parity and cancel.</p><p>We may also compute the tail asymptotics of these eigenvalues for large <inline-formula><mml:math id="inf509"><mml:mi>k</mml:mi></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf510"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf511"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so we want to evaluate<disp-formula id="equ57"><mml:math id="m57"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mfrac><mml:mo>∮</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mfrac><mml:mo>∮</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>for large <inline-formula><mml:math id="inf512"><mml:mi>p</mml:mi></mml:math></inline-formula> at <inline-formula><mml:math id="inf513"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>θ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The first line follows from Cauchy’s intergral formula for a counterclockwise contour encircling <inline-formula><mml:math id="inf514"><mml:mi>t</mml:mi></mml:math></inline-formula>, and the second comes from defining<disp-formula id="equ58"><mml:math id="m58"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>:=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>when <inline-formula><mml:math id="inf515"><mml:mi>p</mml:mi></mml:math></inline-formula> is large and <inline-formula><mml:math id="inf516"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> is constant. We will use the saddle point method (<xref ref-type="bibr" rid="bib18">Butler, 2007</xref>) to evaluate the contour integral asymptotically, ignoring the <inline-formula><mml:math id="inf517"><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> term in the exponent. Note that the only singularity in the original integrand occurs at <inline-formula><mml:math id="inf518"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>The function <inline-formula><mml:math id="inf519"><mml:mi>F</mml:mi></mml:math></inline-formula> has derivatives<disp-formula id="equ59"><mml:math id="m59"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mi>F</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ60"><mml:math id="m60"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mi>F</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We find the saddle points by setting <inline-formula><mml:math id="inf520"><mml:mrow><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. This leads to a quadratic equation with two roots: <inline-formula><mml:math id="inf521"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>±</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>sgn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Since these are evaluated at <inline-formula><mml:math id="inf522"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf523"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, both roots are complex, <inline-formula><mml:math id="inf524"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf525"><mml:mrow><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>′′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Also, the saddle points avoid the singularity in the original integrand, so we can deform our contour to pass through these points and apply the standard approximation.</p><p>Applying the saddle point approximation, we obtain<disp-formula id="equ61"><mml:math id="m61"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>≃</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mfrac><mml:mo>∮</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≃</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mo>!</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mo>!</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mo>!</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>z</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>z</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo><mml:mn>2</mml:mn><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mo>!</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>for some <inline-formula><mml:math id="inf526"><mml:mi>c</mml:mi></mml:math></inline-formula> which is constant in <inline-formula><mml:math id="inf527"><mml:mi>p</mml:mi></mml:math></inline-formula> and depends on <inline-formula><mml:math id="inf528"><mml:mi>D</mml:mi></mml:math></inline-formula>. In the last step, we use that <inline-formula><mml:math id="inf529"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>≤</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> since <inline-formula><mml:math id="inf530"><mml:msub><mml:mi>z</mml:mi><mml:mo>±</mml:mo></mml:msub></mml:math></inline-formula> are conjugate pairs with magnitude 1.</p><p>Now recall the full equation for the coefficients:<disp-formula id="equ62"><mml:math id="m62"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Plugging in the result from the saddle point approximation, substituting <inline-formula><mml:math id="inf531"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and dropping all terms that are constant in <inline-formula><mml:math id="inf532"><mml:mi>k</mml:mi></mml:math></inline-formula>, we find that<disp-formula id="equ63"><mml:math id="m63"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>≤</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf533"><mml:msup><mml:mi>C</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> is a new constant. The rate of <inline-formula><mml:math id="inf534"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is the same decay rate found by <xref ref-type="bibr" rid="bib8">Bach, 2017</xref>; <xref ref-type="bibr" rid="bib13">Bietti and Bach, 2021</xref> using a different mathematical technique for <inline-formula><mml:math id="inf535"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. These decay rates are important for obtaining general worst-case bounds for kernel learning of general targets; (<xref ref-type="bibr" rid="bib7">Bach, 2012</xref>) is an example.</p></sec></sec><sec sec-type="appendix" id="s10-4"><title>3.4 Gaussian process targets</title><p>Taking our target function to be a GP on the unit sphere <inline-formula><mml:math id="inf536"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with some covariance function <inline-formula><mml:math id="inf537"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can represent our target function by performing an eigendecomposition of the covariance operator <inline-formula><mml:math id="inf538"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. When <inline-formula><mml:math id="inf539"><mml:mi>C</mml:mi></mml:math></inline-formula> itself is spherically symmetric and positive definite, this becomes<disp-formula id="equ64"><label>(A35)</label><mml:math id="m64"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf540"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> are the eigenvalues. Then a sample from the GP with this covariance function is a random series<disp-formula id="equ65"><label>(A36)</label><mml:math id="m65"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munderover><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf541"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by the Kosambi-Karhunen–Loève theorem (<xref ref-type="bibr" rid="bib50">Kosambi, 1943</xref>). In other words, the coefficient of <inline-formula><mml:math id="inf542"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the series expansion of <inline-formula><mml:math id="inf543"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is <inline-formula><mml:math id="inf544"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>We take the squared exponential covariance on the sphere<disp-formula id="equ66"> <label>(A37)</label><mml:math id="m66"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>for <inline-formula><mml:math id="inf545"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and length scale <inline-formula><mml:math id="inf546"><mml:mi>γ</mml:mi></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s10-5"><title>3.5 Numerical details</title><p>All of our spherical harmonic expansions are truncated at frequency <inline-formula><mml:math id="inf547"><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. This is typically <inline-formula><mml:math id="inf548"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> for experiments in <inline-formula><mml:math id="inf549"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> dimensions. In higher dimensions, <inline-formula><mml:math id="inf550"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> grows very quickly in <inline-formula><mml:math id="inf551"><mml:mi>k</mml:mi></mml:math></inline-formula>, requiring truncation at a lower frequency.</p><p>To compute the kernel eigenvalues <inline-formula><mml:math id="inf552"><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, we can either numerically integrate the Funk-Hecke formula (<xref ref-type="disp-formula" rid="equ39">Equation A25</xref>) or compute the coefficients <inline-formula><mml:math id="inf553"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> semi-analytically, following <xref ref-type="disp-formula" rid="equ55">Equation A34</xref>, then integrate <xref ref-type="disp-formula" rid="equ47">Equation A32</xref> with numerical quadrature and rescale by <xref ref-type="disp-formula" rid="equ31">Equation A18</xref>.</p><p>We use the Funk-Hecke formula (<xref ref-type="disp-formula" rid="equ39">Equation A25</xref>) and numerical quadrature to find <inline-formula><mml:math id="inf554"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. To compute the expected error using <xref ref-type="disp-formula" rid="equ67">Equation A38</xref>, we use <inline-formula><mml:math id="inf555"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. After generating a sample from the GP, we normalize the functions by dividing the labels and coefficients by their standard deviation. This ensures that the relative mean squared error is equivalent to the mean squared error computed in the next section.</p></sec></sec><sec sec-type="appendix" id="s11"><title>4 Calculation of generalization error</title><p>The generalization error of kernel ridge regression is derived in <xref ref-type="bibr" rid="bib19">Canatar et al., 2021a</xref>; <xref ref-type="bibr" rid="bib76">Simon et al., 2021</xref>; <xref ref-type="bibr" rid="bib34">Gerace et al., 2021</xref>, which show that the mean squared error, in the absence of noise in the target, can be written as<disp-formula id="equ67"><label>(A38)</label><mml:math id="m67"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf556"><mml:msub><mml:mi>β</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> depend on <inline-formula><mml:math id="inf557"><mml:mi>P</mml:mi></mml:math></inline-formula> and the kernel but not on the target, and <inline-formula><mml:math id="inf558"><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> are the coefficients (<xref ref-type="disp-formula" rid="equ34">Equation A21</xref>) of the target function in the basis <inline-formula><mml:math id="inf559"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The exact form of this expression differs from that given in <xref ref-type="bibr" rid="bib20">Canatar et al., 2021b</xref> due to differences in the conventions we take for our basis expansions. Specifically,<disp-formula id="equ68"><label>(A39)</label><mml:math id="m68"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>κ</mml:mi><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf560"><mml:mi>α</mml:mi></mml:math></inline-formula> indexes the kernel eigenfunctions and<disp-formula id="equ69"><label>(A40)</label><mml:math id="m69"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>χ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ70"><label>(A41)</label><mml:math id="m70"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>κ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf561"><mml:msub><mml:mi>α</mml:mi><mml:mi>ridge</mml:mi></mml:msub></mml:math></inline-formula> the ridge parameter. Note that <xref ref-type="disp-formula" rid="equ70">Equation A41</xref> is an implicit equation for <inline-formula><mml:math id="inf562"><mml:mi>κ</mml:mi></mml:math></inline-formula>, which we solve by numerical root-finding.</p><p>Thus,<disp-formula id="equ71"><label>(A42)</label><mml:math id="m71"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:munder><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf563"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf564"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>κ</mml:mi><mml:mi>P</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12"><title>5 Dense-sparse networks</title><p>To compare with more realistic networks, we break the simplifying assumption that <inline-formula><mml:math id="inf565"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is densely connected and instead consider sparse connections between the input and expansion layer. Consider a random matrix <inline-formula><mml:math id="inf566"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf567"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf568"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf569"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf570"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The entries of <inline-formula><mml:math id="inf571"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are i.i.d. Gaussian, i.e. <inline-formula><mml:math id="inf572"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In contrast, <inline-formula><mml:math id="inf573"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a sparse matrix with <italic>exactly K nonzero entries per row</italic>, and nonzero entries equal to <inline-formula><mml:math id="inf574"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>K</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. With these scaling choices, the elements of <inline-formula><mml:math id="inf575"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are of order <inline-formula><mml:math id="inf576"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, which is appropriate when the input features <italic>x</italic><sub><italic>i</italic></sub> are order 1. This is in contrast to the rest of this paper, where we considered features of order <inline-formula><mml:math id="inf577"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> and therefore assumed order 1 weights. The current scaling allows us to study the properties of <inline-formula><mml:math id="inf578"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for different values of <inline-formula><mml:math id="inf579"><mml:mi>D</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf580"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf581"><mml:mi>K</mml:mi></mml:math></inline-formula>.</p><p>First, we examine properties of <inline-formula><mml:math id="inf582"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> under these assumptions. Recall that the rows of <inline-formula><mml:math id="inf583"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the weights of each hidden layer neuron. Since <inline-formula><mml:math id="inf584"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is Gaussian, any given row <inline-formula><mml:math id="inf585"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is marginally Gaussian and distributed identically to any other row. But the rows are <italic>not</italic> independent, since they are all linear combinations of the rows of <inline-formula><mml:math id="inf586"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the kernel limit of an infinitely large dense-sparse network is equal to that of a fully dense network, but convergence to that kernel behaves differently and requires taking a limit of both <inline-formula><mml:math id="inf587"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. In this section, we study how finite <inline-formula><mml:math id="inf588"><mml:mi>N</mml:mi></mml:math></inline-formula> introduces extra correlations among the rows of <inline-formula><mml:math id="inf589"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> compared to dense networks.</p><p>The distribution of <inline-formula><mml:math id="inf590"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is spherically symmetric in the sense that <inline-formula><mml:math id="inf591"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf592"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> have the same distribution for any rotation matrix <inline-formula><mml:math id="inf593"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. In contrast, a densely connected network with weights <inline-formula><mml:math id="inf594"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> drawn i.i.d. as <inline-formula><mml:math id="inf595"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> will of course have independent rows and also be spherically symmetric. The spherical Gaussian is the only vector random variable which is spherically symmetric with independent entries (<xref ref-type="bibr" rid="bib60">Nash and Klamkin, 1976</xref>). Furthermore, each row of <inline-formula><mml:math id="inf596"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> may be rotated by a <italic>different</italic> orthogonal matrix and the resulting random variable would still have the same distribution.</p><p>With these symmetry considerations in mind, the statistics of the rows of <inline-formula><mml:math id="inf597"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> can be described by their multi-point correlations. The simplest of these is the two-point correlation, which in the case of spherical symmetry is captured by the overlaps:<disp-formula id="equ72"><label>(A43)</label><mml:math id="m72"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The overlap <inline-formula><mml:math id="inf598"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is doubly stochastic: one source of stochasticity are the elements of <inline-formula><mml:math id="inf599"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the second one is the random sampling of nonzero elements of <inline-formula><mml:math id="inf600"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Ideally, we are interested in studying the statistics of <inline-formula><mml:math id="inf601"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> when varying <inline-formula><mml:math id="inf602"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf603"><mml:mi>j</mml:mi></mml:math></inline-formula>, i.e. when <inline-formula><mml:math id="inf604"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> varies (since the rows of <inline-formula><mml:math id="inf605"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are sampled independently from each other). However, this will leave us with the quenched disorder given by the specific realization of <inline-formula><mml:math id="inf606"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To obtain a more general and interpretable result, we want to compute the probability distribution<disp-formula id="equ73"><label>(A44)</label><mml:math id="m73"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Notice that the order in which we perform the averaging is irrelevant.</p><sec sec-type="appendix" id="s12-1"><title>5.1Computation of the moment-generating function</title><p>Instead of computing directly the probability distribution in <xref ref-type="disp-formula" rid="equ73">Equation A44</xref>, we compute the moment-generating function<disp-formula id="equ74"><label>(A45)</label><mml:math id="m74"><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which fully characterizes the probability distribution of <inline-formula><mml:math id="inf607"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We indicate the set of indices in which the <inline-formula><mml:math id="inf608"><mml:mi>i</mml:mi></mml:math></inline-formula>-th row of <inline-formula><mml:math id="inf609"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> takes nonzero values by <inline-formula><mml:math id="inf610"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> such that <inline-formula><mml:math id="inf611"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf612"><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and analogously for the <inline-formula><mml:math id="inf613"><mml:mi>j</mml:mi></mml:math></inline-formula>-th row. We also indicate the intersection <inline-formula><mml:math id="inf614"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∩</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, i.e. the set of indices in which <italic>both</italic> the <inline-formula><mml:math id="inf615"><mml:mi>i</mml:mi></mml:math></inline-formula>-th and the <inline-formula><mml:math id="inf616"><mml:mi>j</mml:mi></mml:math></inline-formula>-th rows are nonzero. <inline-formula><mml:math id="inf617"><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> has size <inline-formula><mml:math id="inf618"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>. Notice that setting <inline-formula><mml:math id="inf619"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> causes <inline-formula><mml:math id="inf620"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> deterministically. With this definitions, the overlap can be written as<disp-formula id="equ75"><label>(A46)</label><mml:math id="m75"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We start by perform swapping the averaging order in <xref ref-type="disp-formula" rid="equ74">Equation A45</xref> and averaging over <inline-formula><mml:math id="inf621"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ76"><mml:math id="m76"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>μ</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>μ</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>μ</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where in the first equality we marginalized over all the elements of <inline-formula><mml:math id="inf622"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> which do not enter the definition of <inline-formula><mml:math id="inf623"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, i.e. we went from having to integrate over <inline-formula><mml:math id="inf624"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> variables to only <inline-formula><mml:math id="inf625"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> variables. In the second equality we factorized the columns of <inline-formula><mml:math id="inf626"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We now explicitly compute integral for a fixed value of <inline-formula><mml:math id="inf627"><mml:mi>k</mml:mi></mml:math></inline-formula>, by reducing it to a Gaussian integral:<disp-formula id="equ77"><mml:math id="m77"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:mo>∫</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>μ</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mi>d</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>D</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>μ</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mi>d</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>D</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf628"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which has a 3-by-3 block structure and can be written as<disp-formula id="equ78"><label>(A47)</label><mml:math id="m78"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mn>1</mml:mn><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf629"><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf630"><mml:mi>n</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf631"><mml:mi>n</mml:mi></mml:math></inline-formula> identity matrix and <inline-formula><mml:math id="inf632"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf633"><mml:mi>n</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf634"><mml:mi>m</mml:mi></mml:math></inline-formula> matrix of all ones (if <inline-formula><mml:math id="inf635"><mml:mi>m</mml:mi></mml:math></inline-formula> is omitted, then it is <inline-formula><mml:math id="inf636"><mml:mi>n</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf637"><mml:mi>n</mml:mi></mml:math></inline-formula>). Due to the block structure, the determinant of the matrix above is identical to the determinant of a 3-by-3 matrix<disp-formula id="equ79"><label>(A48)</label><mml:math id="m79"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>By plugging this result into the expression for the moment-generating function, we have that<disp-formula id="equ80"><label>(A49)</label><mml:math id="m80"><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This expression is our core result, and needs to be averaged over <inline-formula><mml:math id="inf638"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This average can be written explicitly by noticing that, when <inline-formula><mml:math id="inf639"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf640"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> is a random variable that follows a hypergeometric distribution in which the number of draws is equal to number of success state and is equal to <inline-formula><mml:math id="inf641"><mml:mi>K</mml:mi></mml:math></inline-formula>. By using the explicit expression of the probability mass function of a hypergeometric distribution, we have that<disp-formula id="equ81"><label>(A50)</label><mml:math id="m81"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>K</mml:mi><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Notice that the term <inline-formula><mml:math id="inf642"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> yields the same moment-generating function (up to a factor) as for a fully-connected <inline-formula><mml:math id="inf643"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> with Gaussian i.i.d. entries with variance <inline-formula><mml:math id="inf644"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>. In contrast, when <inline-formula><mml:math id="inf645"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> we obtain<disp-formula id="equ82"><label>(A51)</label><mml:math id="m82"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s12-2"><title>5.2 Computation of the moments of <inline-formula><mml:math id="inf646"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></title><p>In this section, we assume that <inline-formula><mml:math id="inf647"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> and use the moment-generating function to compute the moments of <inline-formula><mml:math id="inf648"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The non-central moments of the overlap are easily obtained from the moment-generating function as<disp-formula id="equ83"> <label>(A52)</label><mml:math id="m83"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>μ</mml:mi><mml:mi>q</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which can be computed in a symbolic manipulation tool.</p><p>We now explicitly compute the first two moments of <inline-formula><mml:math id="inf649"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.<disp-formula id="equ84"><label>(A53)</label><mml:math id="m84"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>K</mml:mi><mml:mi>N</mml:mi></mml:mfrac><mml:mspace width="1em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we used the fact that the mean of <inline-formula><mml:math id="inf650"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Hypergeom</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf651"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the second moment, we have<disp-formula id="equ85"><mml:math id="m85"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>while to compute the variance we use the law of total variance<disp-formula id="equ86"><mml:math id="m86"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mi>D</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>As <inline-formula><mml:math id="inf652"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> we have that <inline-formula><mml:math id="inf653"><mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, which is the same variance of the overlap for a fully-connected <inline-formula><mml:math id="inf654"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">J</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> with Gaussian i.i.d. entries. This is expected since when <inline-formula><mml:math id="inf655"><mml:mi>N</mml:mi></mml:math></inline-formula> is large, the probability of <inline-formula><mml:math id="inf656"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf657"><mml:mi>j</mml:mi></mml:math></inline-formula> having common afferents goes to zero.</p></sec><sec sec-type="appendix" id="s12-3"><title>5.3 Comparison to clustered embedding</title><p>Instead of distributed embedding, i.e. <inline-formula><mml:math id="inf658"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> being a Gaussian matrix, here we consider a clustered embedding by setting<disp-formula id="equ87"><label>(A54)</label><mml:math id="m87"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>i.e. the Kronecker product of the <inline-formula><mml:math id="inf659"><mml:mi>D</mml:mi></mml:math></inline-formula>-dimensional identity matrix and a vector of all ones and length <inline-formula><mml:math id="inf660"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>. This means that we can separate the input layer of <inline-formula><mml:math id="inf661"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons in <inline-formula><mml:math id="inf662"><mml:mi>D</mml:mi></mml:math></inline-formula> non overlapping subsets <inline-formula><mml:math id="inf663"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, each of size <inline-formula><mml:math id="inf664"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>, and we can write<disp-formula id="equ88"><label>(A55)</label><mml:math id="m88"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mtext>if </mml:mtext><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="1em"/><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this case the overlap is given by<disp-formula id="equ89"><label>(A56)</label><mml:math id="m89"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mn>1</mml:mn><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf665"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the indicator function, i.e. it is one if the argument is true and zero if the argument is false. We indicate by <inline-formula><mml:math id="inf666"><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> the number of elements of <inline-formula><mml:math id="inf667"><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula> which belongs to group <inline-formula><mml:math id="inf668"><mml:mi>l</mml:mi></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="inf669"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The overlap can then be written as<disp-formula id="equ90"><label>(A57)</label><mml:math id="m90"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The vector <inline-formula><mml:math id="inf670"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> follows a multivariate hypergeometric distribution with <inline-formula><mml:math id="inf671"><mml:mi>D</mml:mi></mml:math></inline-formula> classes, <inline-formula><mml:math id="inf672"><mml:mi>K</mml:mi></mml:math></inline-formula> draws, a population of size <inline-formula><mml:math id="inf673"><mml:mi>N</mml:mi></mml:math></inline-formula>, and number of successes for each class equal to <inline-formula><mml:math id="inf674"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>. Notice that <inline-formula><mml:math id="inf675"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf676"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">K</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent from each other since each neuron samples its pre-synaptic partners independently. We can now compute explicitly the mean of <inline-formula><mml:math id="inf677"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> using the fact that <inline-formula><mml:math id="inf678"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ91"><label>(A58)</label><mml:math id="m91"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:mfrac><mml:mspace width="1em"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Similarly, we can write the second moment of <inline-formula><mml:math id="inf679"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as<disp-formula id="equ92"><label>(A59)</label><mml:math id="m92"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Once again, we can use known result for variance and covariance of multivariate hypergeometric variables to simplify the above expression. Indeed, we can write<disp-formula id="equ93"><label>(A60)</label><mml:math id="m93"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ94"><label>(A61)</label><mml:math id="m94"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>from which we obtain the final expression for the second moment<disp-formula id="equ95"><label>(A62)</label><mml:math id="m95"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82914.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Western University</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.15.504040" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.15.504040"/></front-stub><body><p>Models of cerebellar function and the coding of inputs in the cerebellum often assume that random stimuli are a reasonable stand-in for real stimuli. However, the important contribution of this paper is that conclusions about optimality and sparseness in these models do not generalize to potentially more realistic sets of stimuli, for example, those drawn from a low-dimensional manifold. The mathematical analyses in the paper are convincing and possible limitations, including the abstraction from biological details, are well discussed.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82914.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Western University</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Western University</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Jörntell</surname><given-names>Henrik</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/012a77v79</institution-id><institution>Lund University</institution></institution-wrap><country>Sweden</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.15.504040">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.15.504040v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Task-dependent optimal representations for cerebellar learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Jörn Diedrichsen as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Henrik Jörntell (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission. You will see that the recommended revisions most concern clarity of presentation, as well as tempering some of the claims.</p><p>Essential revisions:</p><p>1) All reviewers had questions about specific details of the study (see below comments). I hope you can use these questions as a guideline to improve the clarity and accessibility of the paper. As suggested by reviewer #2, this may involve moving some details from the supplementary materials to the main manuscript.</p><p>2) The current <italic>eLife</italic> assessment (see below) emphasizes the main limitations of the paper – namely the question of to what degree the conclusions of the paper would change if more specific (and biologically more plausible) details about the cerebellar circuit would be taken into account. I hope you will take the opportunity to respond in detail to these points in your response to the reviewer document, which will be posted alongside the revised version of the manuscript and cover the main issues in the Discussion section of the main document.<italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This paper provides compelling and clear analyses that show that the coding level (sparsity) of the granule-cell layer of a cerebellar-like network does not only change the dimensionality of the representation, but also the inductive bias of the network: Higher sparsity biases the network to learning more high-frequency representations. Depending on the dominant frequencies of the target function, different coding levels are therefore optimal. The results are important/fundamental to theories of cerebellar learning and speak to a relevant ongoing debate in cerebellar learning, but will be of interest to readers outside of cerebellar neurophysiology.</p><p>I had two problems in understanding the paper that the authors hopefully can clarify in the revision:</p><p>Page 8: Third paragraph: At this point in the text it is a bit unclear why the K(x * x') changes shape as shown in Figure 3c. I assume the shape of the kernel K depends on the activation function in the hidden layer. It may be useful for the reader if you could give an example of the Kernel for the specific case you are showing in Figure 3c.</p><p>Page 11, 4th paragraph: Why is the distribution corr(J<sup>eff</sup><sub>i</sub>,J<sup>eff</sup><sub>j</sub>) uniform on -1 to 1? Since both are high-dimensional random vectors, should the correlation not be centered around 0? The target of uniform distribution needs to be better explained to make this analysis accessible.<italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Here, a simple model of cerebellar computation is used to study the dependence of task performance on input type: it is demonstrated that task performance and optimal representations are highly dependent on task and stimulus type. This challenges many standard models which use simple random stimuli and concludes that the granular layer is required to provide a sparse representation. This is a useful contribution to our understanding of cerebellar circuits, though, in common with many models of this type, the neural dynamics and circuit architecture are not very specific to the cerebellum, the model includes the feed-forward structure and the high dimension of the granule layer, but little else. This paper has the virtue of including tasks that are more realistic, but by the paper's own admission, the same model can be applied to the electrosensory lateral line lobe and it could, though it is not mentioned in the paper, be applied to the dentate gyrus and large pyramidal cells of CA3. The discussion does not include specific elements related to, for example, the dynamics of the Purkinje cells or the role of Golgi cells, and, in a way, the demonstration that the model can encompass different tasks and stimuli types is an indication of how abstract the model is. Nonetheless, it is useful and interesting to see a generalization of what has become a standard paradigm for discussing cerebellar function.</p><p>I was impressed by the clarity of this manuscript. My only comment is that I found too much was deferred to the appendix, I thought the bits and pieces in the appendix were very clarifying, and given the appendix contains short pieces of extra information explaining the exact nature of the models and tasks, the manuscript would have been easier to follow and think about if these had just been integrated into the text. Often you read papers and wish some details had been shifted into an appendix since they distract from the flow of the description, but the opposite is true here, integrating the details into the text would've made it more concrete in a useful way and the tables of parameters, as tables, would not have interrupted the flow while making it much easier to see the scale of the models and their architecture.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The paper by Xie et al. is a modelling study of the mossy fiber-to-granule cell-to-Purkinje cell network, reporting that the optimal type of representations in the cerebellar granule cell layer depends on the type task. The paper stresses that the findings indicate a higher overall bias towards dense representations than stated in the literature, but it appears the authors have missed parts of the literature that already reported on this. While the modelling and analysis appear mathematically solid, the model is lacking many known constraints of the cerebellar circuitry, which makes the applicability of the findings to the biological counterpart somewhat limited.</p><p>I have some concerns with the novelty of the main conclusion, here from the abstract:</p><p>'Here, we generalize theories of cerebellar learning to determine the optimal granule cell representation for tasks beyond random stimulus discrimination, including continuous input-output transformations as required for smooth motor control. We show that for such tasks, the optimal granule cell representation is substantially denser than predicted by classic theories.'</p><p>Stated like this, this has in principle already been shown, i.e. for example:</p><p>Spanne and Jorntell (2013) Processing of multi-dimensional sensorimotor information in the spinal and cerebellar neuronal circuitry: a new hypothesis. PLoS Comput Biol. 9(3):e1002979.</p><p>Indeed, even the 2 DoF arm movement control that is used in the present paper as an application, was used in this previous paper, with similar conclusions with respect to the advantage of continuous input-output transformations and dense coding. Thus, already from the beginning of this paper, the novelty aspect of this paper is questionable. Even the conclusion in the last paragraph of the Introduction: 'We show that, when learning input-output mappings for motor control tasks, the optimal granule cell representation is much denser than predicted by previous analyses.' was in principle already shown by this previous paper.</p><p>However, the present paper does add several more specific investigations/characterizations that were not previously explored. Many of the main figures report interesting new model results. However, the model is implemented in a highly generic fashion. Consequently, the model relates better to general neural network theory than to specific interpretations of the function of the cerebellar neuronal circuitry. One good example is the findings reported in Figure 2. These represent an interesting extension to the main conclusion, but they are also partly based on arbitrariness as the type of mossy fiber input described in the random categorization task has not been observed in the mammalian cerebellum under behavior in vivo, whereas in contrast, the type of input for the motor control task does resemble mossy fiber input recorded under behavior (van Kan et al. 1993).</p><p>The overall conclusion states:</p><p>'Our results…suggest that optimal cerebellar representations are task-dependent.'</p><p>This is not a particularly strong or specific conclusion. One could interpret this statement as simply saying: ' if I construct an arbitrary neural network, with arbitrary intrinsic properties in neurons and synapses, I can get outputs that depend on the intensity of the input that I provide to that network.'</p><p>Further, the last sentence of the Introduction states: 'More broadly, we show that the sparsity of a neural code has a task-dependent inﬂuence on learning…' This is very general and unspecific, and would likely not come as a surprise to anyone interested in the analysis of neural networks. It doesn't pinpoint any specific biological problem but just says that if I change the density of the input to a [generic] network, then the learning will be impacted in one way or another.</p><p>The interpretation of the distribution of the mossy fiber inputs to the granule cells, which would have a crucial impact on the results of a study like this, is likely incorrect. First, unlike the papers that the authors cite, there are many studies indicating that there is a topographic organization in the mossy fiber termination, such that mossy fibers from the same inputs, representing similar types of information, are regionally co-localized in the granule cell layer. Hence, there is no support for the model assumption that there is a predominantly random termination of mossy fibers of different origins. This risks invalidating the comparisons that the authors are making, i.e. such as in Figure 3. This is a list of example papers, there are more:</p><p>van Kan, Gibson and Houk (1993) Movement-related inputs to intermediate cerebellum of the monkey. Journal of Neurophysiology.</p><p>Garwicz et al. (1998) Cutaneous receptive fields and topography of mossy fibres and climbing fibres projecting to cat cerebellar C3 zone. The Journal of Physiology.</p><p>Brown and Bower (2001) Congruence of mossy fiber and climbing fiber tactile projections in the lateral hemispheres of the rat cerebellum. The Journal of Comparative Neurology.</p><p>Na, Sugihara, Shinoda (2019) The entire trajectories of single pontocerebellar axons and their lobular and longitudinal terminal distribution patterns in multiple aldolase C-positive compartments of the rat cerebellar cortex. The Journal of Comparative Neurology.</p><p>The nature of the mossy fiber-granule cell recording is also reviewed here:</p><p>Gilbert and Miall (2022) How and Why the Cerebellum Recodes Input Signals: An Alternative to Machine Learning. The Neuroscientist</p><p>Further, considering the recoding idea, the following paper shows that detailed information, as it is provided by mossy fibers, is transmitted through the granule cells without any evidence of recoding: Jorntell and Ekerot (2006) Journal of Neuroscience; and this paper shows that these granule inputs are powerfully transmitted to the molecular layer even in a decerebrated animal (i.e. where only the ascending sensory pathways remains) Jorntell and Ekerot 2002, Neuron.</p><p>I could not find any description of the neuron model used in this paper, so I assume that the neurons are just modelled as linear summators with a threshold (in fact, Figure 5 mentions inhibition, but this appears to be just one big lump inhibition, which basically is an incorrect implementation). In reality, granule cells of course do have specific properties that can impact the input-output transformation, PARTICULARLY with respect to the comparison of sparse versus dense coding, because the low-pass filtering of input that occurs in granule cells (and other neurons) as well as their spike firing stochasticity (Saarinen et al. (2008). Stochastic differential equation model for cerebellar granule cell excitability. PLoS Comput. Biol. 4:e1000004) will profoundly complicate these comparisons and make them less straight forward than what is portrayed in this paper. There are also several other factors that would be present in the biological setting but are lacking here, which makes it doubtful how much information in relation to the biological performance that this modelling study provides:</p><p>What are the types of activity patterns of the inputs? What are the learning rules? What is the topography? What is the impact of Purkinje cell outputs downstream, as the Purkinje cell output does not have any direct action, it acts on the deep cerebellar nuclear neurons, which in turn act on a complex sensorimotor circuitry to exert their effect, hence predictive coding could only become interpretable after the PC output has been added to the activity in those circuits. Where is the differentiated Golgi cell inhibition?</p><p>The problem of these, in my impression, generic, arbitrary settings of the neurons and the network in the model becomes obvious here: 'In contrast to the dense activity in cerebellar granule cells, odor responses in Kenyon cells, the analogs of granule cells in the <italic>Drosophila</italic> mushroom body, are sparse…' How can this system be interpreted as an analogy to granule cells in the mammalian cerebellum when the model does not address the specifics lined up above? I.e. the 'inductive bias' that the authors speak of, defined as 'the tendency of a network toward learning particular types of input-output mappings', would be highly dependent on the specifics of the network model.</p><p>More detailed comments:</p><p>Abstract:</p><p>'In these models [Marr-Albus], granule cells form a sparse, combinatorial encoding of diverse sensorimotor inputs. Such sparse representations are optimal for learning to discriminate random stimuli.' Yes, I would agree with the first part, but I contest the second part of this statement. I think what is true for sparse coding is that the learning of random stimuli will be faster, as in a perceptron, but not necessarily better. As the sparsification essentially removes information, it could be argued that the quality of the learning is poorer. So from that perspective, it is not optimal. The authors need to specify from what perspective they consider sparse representations optimal for learning.</p><p>Introduction:</p><p>'Indeed, several recent studies have reported dense activity in cerebellar granule cells in response to sensory stimulation or during motor control tasks (Knogler et al., 2017; Wagner et al., 2017; Giovannucci et al., 2017; Badura and De Zeeuw, 2017; Wagner et al., 2019), at odds with classic theories (Marr, 1969; Albus, 1971).' In fact, this was precisely the issue that was addressed already by Jorntell and Ekerot (2006) Journal of Neuroscience. The conclusion was that these actual recordings of granule cells in vivo provided essentially no support for the assumptions in the Marr-Albus theories.</p><p>Results:</p><p>First para: There is no information about how the granule cells are modelled.</p><p>Second para: 'A typical assumption in computational theories of the cerebellar cortex is that inputs are randomly distributed in a high-dimensional space.' Yes, I agree, and this is in fact in conflict with the known topographical organization in the cerebellar cortex (see broader comment above). Mossy fiber inputs coding for closely related inputs are co-localized in the cerebellar cortex. I think for this model to be of interest from the point of view of the mammalian cerebellar cortex, it would need to pay more attention to this organizational feature.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82914.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All reviewers had questions about specific details of the study (see below comments). I hope you can use these questions as a guideline to improve the clarity and accessibility of the paper. As suggested by reviewer #2, this may involve moving some details from the supplementary materials to the main manuscript.</p></disp-quote><p>As detailed in our response to Reviewer 2, we have moved a number of equations that clarify our model to the main text, as well as including Table 1 which provides details about model parameters. Furthermore, we have substantially extended our Discussion section on Assumptions and Limitations, which addresses comments on biological plausibility.</p><disp-quote content-type="editor-comment"><p>2) The current eLife assessment (see below) emphasizes the main limitations of the paper – namely the question of to what degree the conclusions of the paper would change if more specific (and biologically more plausible) details about the cerebellar circuit would be taken into account. I hope you will take the opportunity to respond in detail to these points in your response to the reviewer document, which will be posted alongside the revised version of the manuscript and cover the main issues in the Discussion section of the main document.</p></disp-quote><p>As mentioned above, the Assumptions and Limitations section section now includes an extended discussion of several issues relating to biological plausibility, including randomness in connectivity onto granule cells, topographic organization of the cerebellar cortex, influence of Golgi cell inhibition, and several other topics. We have additionally added a figure supplement (Figure 7—figure supplement 2) showing that our qualitative results hold when learning is mediated by an online climbing fiber-dependent learning rule, under the assumption that climbing fibers encode an error signal.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>This paper provides compelling and clear analyses that show that the coding level (sparsity) of the granule-cell layer of a cerebellar-like network does not only change the dimensionality of the representation, but also the inductive bias of the network: Higher sparsity biases the network to learning more high-frequency representations. Depending on the dominant frequencies of the target function, different coding levels are therefore optimal. The results are important/fundamental to theories of cerebellar learning and speak to a relevant ongoing debate in cerebellar learning, but will be of interest to readers outside of cerebellar neurophysiology.</p></disp-quote><p>We appreciate the Reviewer’s positive comments.</p><disp-quote content-type="editor-comment"><p>I had two problems in understanding the paper that the authors hopefully can clarify in the revision:</p><p>Page 8: Third paragraph: At this point in the text it is a bit unclear why the K(x * x') changes shape as shown in Figure 3c. I assume the shape of the kernel K depends on the activation function in the hidden layer. It may be useful for the reader if you could give an example of the Kernel for the specific case you are showing in Figure 3c.</p></disp-quote><p>We agree this point needed more clarity. Figure 3C shows the kernel for a rectified linear (ReLU) activation function. The key qualitative effect is the dependence of the kernel on the coding level <italic>f</italic>, and this effect is present across different choices of activation function (see Figure 2—figure supplement 2). We show the influence of coding level on the kernel visually in Figures 3a and 3b and in Equations 1 and 2. Equation 2 shows that the kernel depends on the hidden layer activation h(x), and Equation 1 shows h(x) depends on the threshold <italic>θ</italic>.</p><p>To communicate this idea more clearly, we added the following sentences (see bolded text) to Figure 3C’s caption:</p><p>(“C) Kernel <italic>K</italic>(x<italic>,</italic>x<sup>0</sup>) for networks with rectified linear activation functions (Equation 1), normalized so that fully overlapping representations have an overlap of 1, plotted as a function of overlap in the space of task variables. The vertical axis corresponds to the ratio of the area of the purple region to the area of the red or blue regions in (B). Each curve corresponds to the kernel of an infinite-width network with a different coding level <italic>f</italic>.”</p><p>In the main text, in the last paragraph on page 8, we also added the following text:</p><p>Equations 1 and 2 show that the threshold <italic>θ</italic>, which determines the coding level, influences the kernel through its effect on the expansion layer activity h(x).</p><disp-quote content-type="editor-comment"><p>Page 11, 4th paragraph: Why is the distribution corr(J<sup>eff</sup><sub>i</sub>,J<sup>eff</sup><sub>j</sub>) uniform on -1 to 1? Since both are high-dimensional random vectors, should the correlation not be centered around 0? The target of uniform distribution needs to be better explained to make this analysis accessible.</p></disp-quote><p>We agree that the uniform distribution might be unexpected. This is in fact a consequence of our assumption in this figure that the points are drawn from a task subspace of dimension <italic>D</italic> = 3. For a unit sphere in <italic>D</italic> = 3 dimensions, the dot product between randomly chosen pairs of points is uniformly distributed on [−1<italic>,</italic>1]. For higher dimensions, the distribution, as the Reviewer expects, is centered at zero but decays away from zero (it is a type of β distribution). We have added a sentence to the figure caption stating this:</p><p>(“C) Distributions of synaptic weight correlations Corr <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>j</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where J<sup>eff</sup><italic><sub>i</sub></italic> is the <italic>i</italic>th row of J<sup>eff</sup>, for pairs of expansion layer neurons in networks with different numbers of input layer neurons <italic>N</italic> (colors) when <italic>K</italic> = 4 and <italic>D</italic> = 3. Black distribution corresponds to fully connected networks with Gaussian weights. We note that when <italic>D</italic> = 3, the distribution of correlations for random Gaussian weight vectors is uniform on [−1<italic>,</italic>1] as shown (for higher dimensions the distribution has a peak at 0).”<italic>Reviewer #2 (Recommendations for the authors):</italic></p><disp-quote content-type="editor-comment"><p>Here, a simple model of cerebellar computation is used to study the dependence of task performance on input type: it is demonstrated that task performance and optimal representations are highly dependent on task and stimulus type. This challenges many standard models which use simple random stimuli and concludes that the granular layer is required to provide a sparse representation. This is a useful contribution to our understanding of cerebellar circuits, though, in common with many models of this type, the neural dynamics and circuit architecture are not very specific to the cerebellum, the model includes the feed-forward structure and the high dimension of the granule layer, but little else. This paper has the virtue of including tasks that are more realistic, but by the paper's own admission, the same model can be applied to the electrosensory lateral line lobe and it could, though it is not mentioned in the paper, be applied to the dentate gyrus and large pyramidal cells of CA3. The discussion does not include specific elements related to, for example, the dynamics of the Purkinje cells or the role of Golgi cells, and, in a way, the demonstration that the model can encompass different tasks and stimuli types is an indication of how abstract the model is. Nonetheless, it is useful and interesting to see a generalization of what has become a standard paradigm for discussing cerebellar function.</p></disp-quote><p>We appreciate the Reviewer’s positive comments. Regarding the simplifications of our model, we agree that we have taken a modeling approach that abstracts away certain details to permit comparisons across systems. We now include an in-depth discussion of our simplifying assumptions (Assumptions and Extensions section in the Discussion) and have further noted the possibility that other biophysical mechanisms we have not accounted for may also underlie differences across systems.</p><p>Our results predict that qualitative differences in the coding levels of cerebellum-like systems, across brain regions or across species, reflect an optimization to distinct tasks (Figure 7). However, it is also possible that differences in coding level arise from other physiological differences between systems.</p><disp-quote content-type="editor-comment"><p>I was impressed by the clarity of this manuscript. My only comment is that I found too much was deferred to the appendix, I thought the bits and pieces in the appendix were very clarifying, and given the appendix contains short pieces of extra information explaining the exact nature of the models and tasks, the manuscript would have been easier to follow and think about if these had just been integrated into the text. Often you read papers and wish some details had been shifted into an appendix since they distract from the flow of the description, but the opposite is true here, integrating the details into the text would've made it more concrete in a useful way and the tables of parameters, as tables, would not have interrupted the flow while making it much easier to see the scale of the models and their architecture.</p></disp-quote><p>1) When we introduce the model in the Results section, we clarify that we use ReLU activation functions throughout the study.</p><p>The activity of neurons in the expansion layer is given by:</p><p>h = <italic>φ</italic>(J<sup>eff</sup>x − <italic>θ</italic>)<italic>,</italic></p><p>where <italic>φ</italic> is a rectified linear activation function <italic>φ</italic>(<italic>u</italic>) = max(<italic>u,</italic>0) applied element-wise. Our results also hold for other threshold-polynomial activation functions.</p><p>2) We provide more specifics about the random categorization task for Figure 2:</p><p>The former we refer to as a “random categorization task” and is parameterized by the number of input pattern-to-category associations <italic>P</italic> learned during training (Figure 2C). During the training phase, the network learns to associate random input patterns x<italic><sup>µ</sup></italic> ∈ R<italic><sup>D</sup></italic> for <italic>µ</italic> = 1<italic>,…,P</italic> with random binary categories <italic>y<sup>µ</sup></italic> = ±1. The elements of x<italic><sup>µ</sup></italic> are drawn i.i.d. from a normal distribution with mean 0 and variance 1<italic>/D</italic>. We refer to x<italic><sup>µ</sup></italic> as “training patterns.” To assess the network’s generalization performance, it is presented with “test patterns” generated by adding noise (parameterized by a noise magnitude; see Methods) to the training patterns. Tasks with continuous outputs (Figure 2D) are parameterized by a length scale that determines how quickly the output changes as a function of the input (specifically, input-output functions are drawn from a Gaussian process with length scale <italic>γ</italic> for variations in <italic>f</italic>(x) as a function of x; see Methods). In this case, both training and test patterns are drawn uniformly on the unit sphere. Later, we will also consider tasks implemented by specific cerebellum-like systems. See Table 1 for a summary of parameters throughout this study.</p><p>3) We specify the readouts and the performance metrics we use:</p><p>We trained the readout to approximate the target output for training patterns and generalize to unseen test patterns. The network’s prediction is <italic>f</italic><sup>ˆ</sup>(x) = w·h(x) for tasks with continuous outputs, or <italic>f</italic><sup>ˆ</sup>(x) = sign(w·h(x)) for categorization tasks, where w are the synaptic weights of the readout from the expansion layer. These weights were set using least squares regression. Performance was measured as the fraction of incorrect predictions for categorization tasks, or relative mean squared error for tasks with continuous targets: Error = <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the expectation is across test patterns.</p><p>4) To clarify differences in model and task parameters used across the different figures, we have moved the table of parameters to the main text. The main text references this table after introducing the model and the tasks used in Figure 2.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The paper by Xie et al. is a modelling study of the mossy fiber-to-granule cell-to-Purkinje cell network, reporting that the optimal type of representations in the cerebellar granule cell layer depends on the type task. The paper stresses that the findings indicate a higher overall bias towards dense representations than stated in the literature, but it appears the authors have missed parts of the literature that already reported on this. While the modelling and analysis appear mathematically solid, the model is lacking many known constraints of the cerebellar circuitry, which makes the applicability of the findings to the biological counterpart somewhat limited.</p></disp-quote><p>We thank the Reviewer for suggesting additional references to include in our manuscript, and for encouraging us to extend our model toward greater biological plausibility and more critically discuss simplifying assumptions we have made. We respond to both the comment about previous literature and about applicability to cerebellar circuitry in detail below.</p><disp-quote content-type="editor-comment"><p>I have some concerns with the novelty of the main conclusion, here from the abstract:</p><p>'Here, we generalize theories of cerebellar learning to determine the optimal granule cell representation for tasks beyond random stimulus discrimination, including continuous input-output transformations as required for smooth motor control. We show that for such tasks, the optimal granule cell representation is substantially denser than predicted by classic theories.'</p><p>Stated like this, this has in principle already been shown, i.e. for example:</p><p>Spanne and Jorntell (2013) Processing of multi-dimensional sensorimotor information in the spinal and cerebellar neuronal circuitry: a new hypothesis. PLoS Comput Biol. 9(3):e1002979.</p><p>Indeed, even the 2 DoF arm movement control that is used in the present paper as an application, was used in this previous paper, with similar conclusions with respect to the advantage of continuous input-output transformations and dense coding. Thus, already from the beginning of this paper, the novelty aspect of this paper is questionable. Even the conclusion in the last paragraph of the Introduction: 'We show that, when learning input-output mappings for motor control tasks, the optimal granule cell representation is much denser than predicted by previous analyses.' was in principle already shown by this previous paper.</p></disp-quote><p>We thank the Reviewer for drawing our attention to Spanne and Jo¨rntell (2013). Our study shares certain similarities with this work, including the consideration of tasks with smooth input-output mappings, such as learning the dynamics of a two-joint arm. However, our study differs substantially, most notably the fact that we focus our study on parametrically varying the degree of sparsity in the granule cell layer to determine the circumstances under which dense versus sparse coding is optimal. To the best of our ability, we can find no result in Spanne and J¨orntell (2013) that indicates the performance of a network as a function of average coding level. Instead, Spanne and Jo¨rntell (2013) propose that inhibition from Golgi cells produces heterogeneity in coding level which can improve performance, which is an interesting but complementary finding to ours. We therefore do not believe that the quantitative computations of optimal coding level that we present are redundant with the results of this previous study. We also note that a key contribution of our study is mathemetical analysis of the inductive bias of networks with different coding levels which supports our conclusions.</p><p>We have included a discussion of Spanne and Jo¨rntell (2013) and (2015) in the revised version of our manuscript:</p><p>“Other studies have considered tasks with smooth input-output mappings and low-dimensional inputs, finding that heterogeneous Golgi cell inhibition can improve performance by diversifying individual granule cell thresholds (Spanne and J¨orntell, 2013). Extending our model to include heterogeneous thresholds is an interesting direction for future work. Another proposal states that dense coding may improve generalization (Spanne and Jo¨rntell, 2015). Our theory reveals that whether or not dense coding is beneficial depends on the task.”</p><disp-quote content-type="editor-comment"><p>However, the present paper does add several more specific investigations/characterizations that were not previously explored. Many of the main figures report interesting new model results. However, the model is implemented in a highly generic fashion. Consequently, the model relates better to general neural network theory than to specific interpretations of the function of the cerebellar neuronal circuitry. One good example is the findings reported in Figure 2. These represent an interesting extension to the main conclusion, but they are also partly based on arbitrariness as the type of mossy fiber input described in the random categorization task has not been observed in the mammalian cerebellum under behavior in vivo, whereas in contrast, the type of input for the motor control task does resemble mossy fiber input recorded under behavior (van Kan et al. 1993).</p></disp-quote><p>We agree that the tasks we consider in Figure 2 are simplified compared to those that we consider elsewhere in the paper. The choice of random mossy fiber input was made to provide a comparison to previous modeling studies that also use random input as a benchmark (Marr 1969, Albus 1971, Brunel 2004, Babadi and Sompolinsky 2014, Billings 2014, LitwinKumar et al., 2017). This baseline permits us to specifically evaluate the effects of lowdimensional inputs (Figure 2) and richer input-output mappings (Figure 2, Figure 7). We agree with the Reviewer that the random and uncorrelated mossy fiber activity that has been extensively used in previous studies is almost certainly an unrealistic idealization of in vivo neural activity—this is a motivating factor for our study, which relaxes this assumption and examines the consequences. To provide additional context, we have updated the following paragraph in the main text Results section:</p><p>“A typical assumption in computational theories of the cerebellar cortex is that inputs are randomly distributed in a high-dimensional space (Marr, 1969; Albus, 1971; Brunel et al., 2004; Babadi and Sompolinsky, 2014; Billings et al., 2014; Litwin-Kumar et al., 2017). While this may be a reasonable simplification in some cases, many tasks, including cerebellumdependent tasks, are likely best-described as being encoded by a low-dimensional set of variables. For example, the cerebellum is often hypothesized to learn a forward model for motor control (Wolpert et al., 1998), which uses sensory input and motor efference to predict an effector’s future state. Mossy fiber activity recorded in monkeys correlates with position and velocity during natural movement (van Kan et al., 1993). Sources of motor efference copies include motor cortex, whose population activity lies on a lowdimensional manifold (Wagner et al., 2019; Huang et al., 2013; Churchland et al., 2010; Yu et al., 2009). We begin by modeling the low dimensionality of inputs and later consider more specific tasks.”</p><disp-quote content-type="editor-comment"><p>The overall conclusion states:</p><p>'Our results….suggest that optimal cerebellar representations are task-dependent.'</p><p>This is not a particularly strong or specific conclusion. One could interpret this statement as simply saying: ' if I construct an arbitrary neural network, with arbitrary intrinsic properties in neurons and synapses, I can get outputs that depend on the intensity of the input that I provide to that network.'</p><p>Further, the last sentence of the Introduction states: 'More broadly, we show that the sparsity of a neural code has a task-dependent inﬂuence on learning…' This is very general and unspecific, and would likely not come as a surprise to anyone interested in the analysis of neural networks. It doesn't pinpoint any specific biological problem but just says that if I change the density of the input to a [generic] network, then the learning will be impacted in one way or another.</p></disp-quote><p>We agree with the Reviewer that our conclusions are quite general, and we have removed the final sentence as we agree it was unspecific. However, we disagree with the Reviewer’s paraphrasing of our results.</p><p>First, we do not select arbitrary intrinsic properties of neurons and synapses. Rather, we construct a simplified model with a key quantity, the neuronal threshold, that we vary parametrically in order to assess the effect of the resulting changes in the representation on performance. Second, we do not vary the intensity/density of inputs provided to the network – this is fixed throughout our study for all key comparisons we perform. Instead, we vary the density (coding level) of the expansion layer representation and quantify its effect on inductive bias and generalization. Finally, our study’s key contribution is an explanation of the heterogeneity in average coding level observed across behaviors and cerebellum-like systems. We go beyond the empirical statement that there is a dependence of performance on the parameter that we vary by developing an analytical theory. Our theory describes the performance of the class of networks that we study and the properties of learning tasks that determine the optimal expansion layer representation.</p><p>To clarify our main contributions, we have updated the final paragraph of the Introduction. We have also removed the sentence that the Reviewer objects to, as it was less specific than the other points we make here.</p><p>We propose that these differences can be explained by the capacity of representations with different levels of sparsity to support learning of different tasks. We show that the optimal level of sparsity depends on the structure of the input-output relationship of a task. When learning input-output mappings for motor control tasks, the optimal granule cell representation is much denser than predicted by previous analyses. To explain this result, we develop an analytic theory that predicts the performance of cerebellum-like circuits for arbitrary learning tasks. The theory describes how properties of cerebellar architecture and activity control these networks’ inductive bias: the tendency of a network toward learning particular types of input-output mappings (Sollich, 1998; Jacot et al., 2018; Bordelon et al., 2020; Canatar et al., 2021; Simon et al., 2021). The theory shows that inductive bias, rather than the dimension of the representation alone, is necessary to explain learning performance across tasks. It also suggests that cerebellar regions specialized for different functions may adjust the sparsity of their granule cell representations depending on the task.</p><disp-quote content-type="editor-comment"><p>The interpretation of the distribution of the mossy fiber inputs to the granule cells, which would have a crucial impact on the results of a study like this, is likely incorrect. First, unlike the papers that the authors cite, there are many studies indicating that there is a topographic organization in the mossy fiber termination, such that mossy fibers from the same inputs, representing similar types of information, are regionally co-localized in the granule cell layer. Hence, there is no support for the model assumption that there is a predominantly random termination of mossy fibers of different origins. This risks invalidating the comparisons that the authors are making, i.e. such as in Figure 3. This is a list of example papers, there are more:</p><p>van Kan, Gibson and Houk (1993) Movement-related inputs to intermediate cerebellum of the monkey. Journal of Neurophysiology.</p><p>Garwicz et al. (1998) Cutaneous receptive fields and topography of mossy fibres and climbing fibres projecting to cat cerebellar C3 zone. The Journal of Physiology.</p><p>Brown and Bower (2001) Congruence of mossy fiber and climbing fiber tactile projections in the lateral hemispheres of the rat cerebellum. The Journal of Comparative Neurology.</p><p>Na, Sugihara, Shinoda (2019) The entire trajectories of single pontocerebellar axons and their lobular and longitudinal terminal distribution patterns in multiple aldolase C-positive compartments of the rat cerebellar cortex. The Journal of Comparative Neurology.</p><p>The nature of the mossy fiber-granule cell recording is also reviewed here:</p><p>Gilbert and Miall (2022) How and Why the Cerebellum Recodes Input Signals: An Alternative to Machine Learning. The Neuroscientist</p><p>Further, considering the recoding idea, the following paper shows that detailed information, as it is provided by mossy fibers, is transmitted through the granule cells without any evidence of recoding: Jorntell and Ekerot (2006) Journal of Neuroscience; and this paper shows that these granule inputs are powerfully transmitted to the molecular layer even in a decerebrated animal (i.e. where only the ascending sensory pathways remains) Jorntell and Ekerot 2002, Neuron.</p></disp-quote><p>We agree that there is strong evidence for a topographic organization in mossy fiber to granule cell connectivity at the microzonal level. We thank the Reviewer for pointing us to specific examples. We acknowledge that our simplified model does not capture the structure of connectivity observed in these studies.</p><p>However, the focus of our model is on cerebellar neurons presynaptic to a single Purkinje cell. Random or disordered distribution of inputs at this local scale is compatible with topographic organization at the microzonal scale. Furthermore, while there is evidence of structured connections at the local scale, models with random connectivity are able to reproduce the dimensionality of granule cell activity within a small margin of error (Nguyen et al., 2022). Finally, our finding that dense codes are optimal for learning slowly varying tasks is consistent with evidence for the lack of re-coding – for such tasks, re-coding may absent because it is not required.</p><p>We have dedicated a section on this issue in the Assumptions and Extensions portion of our Discussion:</p><p>“Another key assumption concerning the granule cells is that they sample mossy fiber inputs randomly, as is typically assumed in Marr-Albus models (Marr, 1969; Albus, 1971; LitwinKumar et al., 2017; Cayco-Gajic et al., 2017). Other studies instead argue that granule cells sample from mossy fibers with highly similar receptive fields (Garwicz et al., 1998; Brown and Bower, 2001; J¨orntell and Ekerot, 2006) defined by the tuning of mossy fiber and climbing fiber inputs to cerebellar microzones (Apps et al., 2018). This has led to an alternative hypothesis that granule cells serve to relay similarly tuned mossy fiber inputs and enhance their signal-to-noise ratio (Jo¨rntell and Ekerot, 2006; Gilbert and Chris Miall, 2022) rather than to re-encode inputs. Another hypothesis is that granule cells enable Purkinje cells to learn piece-wise linear approximations of nonlinear functions (Spanne and J¨orntell, 2013). However, several recent studies support the existence of heterogeneous connectivity and selectivity of granule cells to multiple distinct inputs at the local scale (Huang et al., 2013; Ishikawa et al., 2015). Furthermore, the deviation of the predicted dimension in models constrained by electron-microscopy data as compared to randomly wired models is modest (Nguyen et al., 2022). Thus, topographically organized connectivity at the macroscopic scale may coexist with disordered connectivity at the local scale, allowing granule cells presynaptic to an individual Purkinje cell to sample heterogeneous combinations of the subset of sensorimotor signals relevant to the tasks that Purkinje cell participates in. Finally, we note that the optimality of dense codes for learning slowly varying tasks in our theory suggests that observations of a lack of mixing (J¨orntell and Ekerot, 2002) for such tasks are compatible with Marr-Albus models, as in this case nonlinear mixing is not required.”</p><disp-quote content-type="editor-comment"><p>I could not find any description of the neuron model used in this paper, so I assume that the neurons are just modelled as linear summators with a threshold (in fact, Figure 5 mentions inhibition, but this appears to be just one big lump inhibition, which basically is an incorrect implementation). In reality, granule cells of course do have specific properties that can impact the input-output transformation, PARTICULARLY with respect to the comparison of sparse versus dense coding, because the low-pass filtering of input that occurs in granule cells (and other neurons) as well as their spike firing stochasticity (Saarinen et al. (2008). Stochastic differential equation model for cerebellar granule cell excitability. PLoS Comput. Biol. 4:e1000004) will profoundly complicate these comparisons and make them less straight forward than what is portrayed in this paper. There are also several other factors that would be present in the biological setting but are lacking here, which makes it doubtful how much information in relation to the biological performance that this modelling study provides:</p><p>What are the types of activity patterns of the inputs? What are the learning rules? What is the topography? What is the impact of Purkinje cell outputs downstream, as the Purkinje cell output does not have any direct action, it acts on the deep cerebellar nuclear neurons, which in turn act on a complex sensorimotor circuitry to exert their effect, hence predictive coding could only become interpretable after the PC output has been added to the activity in those circuits. Where is the differentiated Golgi cell inhibition?</p></disp-quote><p>Thank you for these critiques. We have made numerous edits to improve the presentation of the details of our model in the main text of the manuscript. Indeed, granule cells in the main text are modeled as linear sums of mossy fiber inputs with a threshold-linear activation function. A more detailed description of the model for granule cells can now be found in Equation 1 in the Results section:</p><p>“The activity of neurons in the expansion layer is given by:</p><p>h = <italic>φ</italic>(J<sup>eff</sup>x − <italic>θ</italic>)<italic>,</italic> (1)</p><p>where <italic>φ</italic> is a rectified linear activation function <italic>φ</italic>(<italic>u</italic>) = max(<italic>u,</italic>0) applied element-wise. Our results also hold for other threshold-polynomial activation functions. The scalar threshold <italic>θ</italic> is shared across neurons and controls the coding level, which we denote by <italic>f</italic>, defined as the average fraction of neurons in the expansion layer that are active.”</p><p>Most of our analyses use the firing rate model we describe above, but several Supplemental Figures show extensions to this model. As we mention in the Discussion, our results do not depend on the specific choice of nonlinearity (Figure 2—figure supplement 2). We have also considered the possibility that the stochastic nature of granule cell spikes could impact our measures of coding level. In Figure 7—figure supplement 1 we test the robustness of our main conclusion using a spiking model where we model granule cell spikes with Poisson statistics. When measuring coding level in a population of spiking neurons, a key question is at what time window the Purkinje cell integrates spikes. For several choices of integration time windows, we show that dense coding remains optimal for learning smooth tasks. However, we agree with the Reviewer that there are other biological details our model does not address. For example, our spiking model does not capture some of the properties the Saarinen et al. (2008) model captures, including random sub-threshold oscillations and clusters of spikes. Modeling biophysical phenomena at this scale is beyond the scope of our study. We have added this reference to the relevant section of the Discussion:</p><p>“We also note that coding level is most easily defined when neurons are modeled as rate, rather than spiking units. To investigate the consistency of our results under a spiking code, we implemented a model in which granule cell spiking exhibits Poisson variability and quantify coding level as the fraction of neurons that have nonzero spike counts (Figure 7—figure supplement 1; Figure 7C). In general, increased spike count leads to improved performance as noise associated with spiking variability is reduced. Granule cells have been shown to exhibit reliable burst responses to mossy fiber stimulation (Chadderton et al., 2004), motivating models using deterministic responses or sub-Poisson spiking variability. However, further work is needed to quantitatively compare variability in model and experiment and to account for more complex biophysical properties of granule cells (Saarinen et al., 2008).”</p><p>A second concern the Reviewer raises is our implementation of Golgi cell inhibition as a homogeneous rather than heterogeneous input onto granule cells. In simplified models, adding heterogeneous inhibition does not dramatically change the qualitative properties of the expansion layer representation, in particular the dimensionality of the representation (Billings et al., 2014, Cayco-Gajic et al., 2017, Litwin-Kumar et al., 2017). We have added a section about inhibition to our Discussion:</p><p>“We also have not explicitly modeled inhibitory input provided by Golgi cells, instead assuming such input can be modeled as a change in effective threshold, as in previous studies (Billings et al., 2014; Cayco-Gajic et al., 2017; Litwin-Kumar et al., 2017). This is appropriate when considering the dimension of the granule cell representation (Litwin-Kumar et al., 2017), but more work is needed to extend our model to the case of heterogeneous inhibition.”</p><p>Regarding the mossy fiber inputs, as we state in response to paragraph 3, we agree with the Reviewer that the random and uncorrelated mossy fiber activity that has been used in previous studies is an unrealistic idealization of in vivo neural activity. One of the motivations for our model was to relax this assumption and examine the consequences: we introduce correlations in the mossy fiber activity by projecting low-dimensional patterns into the mossy fiber layer (Figure 1B):</p><p>“A typical assumption in computational theories of the cerebellar cortex is that inputs are randomly distributed in a high-dimensional space (Marr, 1969; Albus, 1971; Brunel et al., 2004; Babadi and Sompolinsky, 2014; Billings et al., 2014; Litwin-Kumar et al., 2017). While this may be a reasonable simplification in some cases, many tasks, including cerebellumdependent tasks, are likely best-described as being encoded by a low-dimensional set of variables. For example, the cerebellum is often hypothesized to learn a forward model for motor control (Wolpert et al., 1998), which uses sensory input and motor efference to predict an effector’s future state. Mossy fiber activity recorded in monkeys correlates with position and velocity during natural movement (van Kan et al., 1993). Sources of motor efference copies include motor cortex, whose population activity lies on a low-dimensional manifold (Wagner et al., 2019; Huang et al., 2013; Churchland et al., 2010; Yu et al., 2009). We begin by modeling the low dimensionality of inputs and later consider more specific tasks.</p><p>We therefore assume that the inputs to our model lie on a <italic>D</italic>-dimensional subspace embedded in the <italic>N</italic>-dimensional input space, where <italic>D</italic> is typically much smaller than <italic>N</italic> (Figure 1B). We refer to this subspace as the “task subspace” (Figure 1C).”</p><p>The Reviewer also mentions the learning rule at granule cell to Purkinje cell synapses. We agree that considering online, climbing-fiber-dependent learning is an important generalization. We therefore added a new supplemental figure investigating whether we would still see a difference in optimal coding levels across tasks if online learning were used instead of the least squares solution (Figure 7—figure supplement 2). Indeed, we observed a similar task dependence as we saw in Figure 2F. We have added a new paragraph in the Discussion under Assumptions and Extensions describing our rationale and approach in detail:</p><p>“For the Purkinje cells, our model assumes that their responses to granule cell input can be modeled as an optimal linear readout. Our model therefore provides an upper bound to linear readout performance, a standard benchmark for the quality of a neural representation that does not require assumptions on the nature of climbing fiber-mediated plasticity, which is still debated. Electrophysiological studies have argued in favor of a linear approximation (Brunel et al., 2004). To improve the biological applicability of our model, we implemented an online climbing fiber-mediated learning rule and found that optimal coding levels are still task-dependent (Figure 7—figure supplement 2). We also note that although we model several timing-dependent tasks (Figure 7), our learning rule does not exploit temporal information, and we assume that temporal dynamics of granule cell responses are largely inherited from mossy fibers. Integrating temporal information into our model is an interesting direction for future investigation.</p><p>During each epoch of training, the network is presented with all patterns in a randomized order, and the learned weights are updated with each pattern (see Methods). Networks were presented with 30 patterns and trained for 20<italic>,</italic>000 epochs, with a learning rate of <italic>η</italic> = 0<italic>.</italic>7<italic>/M</italic>. Other parameters: <italic>D</italic> = 3<italic>,M</italic> = 10<italic>,</italic>000.</p><p>A) Performance of an example network during online learning, measured as relative mean squared error across training epochs. Parameters: <italic>f</italic> = 0<italic>.</italic>3, <italic>γ</italic> = 1.</p><p>B) Generalization error as a function of coding level for networks trained with online learning (solid lines) or unregularized least squares (dashed lines) for Gaussian process tasks with different length scales (colors). Standard error of the mean was computed across 20 realizations.”</p><p>Finally, regarding the function of the Purkinje cell, our model defines a learning task as a mapping from inputs to target activity in the Purkinje cell and is thus agnostic to the cell’s downstream effects. We clarify this point when introducing the definition of a learning task:</p><p>“In our model, a learning task is defined by a mapping from task variables <bold>x</bold> to an output <italic>f</italic>(<bold>x</bold>), representing a target change in activity of a readout neuron, for example a Purkinje cell. The limited scope of this definition implies our results should not strongly depend on the influence of the readout neuron on downstream circuits.”</p><disp-quote content-type="editor-comment"><p>The problem of these, in my impression, generic, arbitrary settings of the neurons and the network in the model becomes obvious here: 'In contrast to the dense activity in cerebellar granule cells, odor responses in Kenyon cells, the analogs of granule cells in the <italic>Drosophila</italic> mushroom body, are sparse…' How can this system be interpreted as an analogy to granule cells in the mammalian cerebellum when the model does not address the specifics lined up above? I.e. the 'inductive bias' that the authors speak of, defined as 'the tendency of a network toward learning particular types of input-output mappings', would be highly dependent on the specifics of the network model.</p></disp-quote><p>We agree with the Reviewer that our model makes several simplifying assumptions for mathematical tractability. However, we note that our study is not the first to draw analogies between cerebellum-like systems, including the mushroom body (Bell et al., 2008; Farris, 2011). All the systems we study feature a sparsely connected, expanded granule-like layer that sends parallel fiber axons onto densely connected downstream neurons known to exhibit powerful synaptic plasticity, thus motivating the key architectural assumptions of our model. We have constrained anatomical parameters of the model using data as available (Table 1). However, we agree with the Reviewer that when making comparisons across species there is always a possibility that differences are due to physiological mechanisms we have not fully understood or captured with a model. As such, we can only present a hypothesis for these differences. We have modified our Discussion section on this topic to clearly state this.</p><p>“Our results predict that qualitative differences in the coding levels of cerebellum-like systems, across brain regions or across species, reflect an optimization to distinct tasks (Figure 7). However, it is also possible that differences in coding level arise from other physiological differences between systems.”</p><disp-quote content-type="editor-comment"><p>More detailed comments:</p><p>Abstract:</p><p>'In these models [Marr-Albus], granule cells form a sparse, combinatorial encoding of diverse sensorimotor inputs. Such sparse representations are optimal for learning to discriminate random stimuli.' Yes, I would agree with the first part, but I contest the second part of this statement. I think what is true for sparse coding is that the learning of random stimuli will be faster, as in a perceptron, but not necessarily better. As the sparsification essentially removes information, it could be argued that the quality of the learning is poorer. So from that perspective, it is not optimal. The authors need to specify from what perspective they consider sparse representations optimal for learning.</p></disp-quote><p>This is an important point that we would like to clarify. It is not the case that sparse coding simply speeds up learning. In our study and many related works (Barak et al. 2013; Babadi and Sompolinsky 2014; Litwin-Kumar et al. 2017), learning performance is measured based on the generalization ability of the network – the ability to predict correct labels for previously unseen inputs. As our study and previous studies show, sparse codes are optimal in the sense that they minimize generalization error, independent of any effect on learning speed. To communicate this more effectively, we have added the following sentence to the first paragraph of the Introduction:</p><p>“Sparsity affects both learning speed (Cayco-Gajic et al., 2017), and generalization, the ability to predict correct labels for previously unseen inputs (Barak et al., 2013; Babadi and Sompolinsky, 2014; Litwin-Kumar et al., 2017).”</p><disp-quote content-type="editor-comment"><p>Introduction:</p><p>'Indeed, several recent studies have reported dense activity in cerebellar granule cells in response to sensory stimulation or during motor control tasks (Knogler et al., 2017; Wagner et al., 2017; Giovannucci et al., 2017; Badura and De Zeeuw, 2017; Wagner et al., 2019), at odds with classic theories (Marr, 1969; Albus, 1971).' In fact, this was precisely the issue that was addressed already by Jorntell and Ekerot (2006) Journal of Neuroscience. The conclusion was that these actual recordings of granule cells in vivo provided essentially no support for the assumptions in the Marr-Albus theories.</p></disp-quote><p>In our reading, the main finding of J¨orntell and Ekerot (2006) is that individual granule cells are activated by mossy fibers with overlapping receptive fields driven by a single type of somatosensory input. However, there is also evidence of nonlinear mixed selectivity in granule cells in support of the re-coding hypothesis (Huang et al., 2013; Ishikawa et al., 2015). Jo¨rntell and Ekerot (2006) also suggest that the granule cell layer shares similar topographic organization as mossy fibers, organized into microzones. The existence of topographic organization does not invalidate Marr-Albus theories. As we have suggested earlier, a local combinatorial expansion can coexist with a global topographic organization.</p><p>We have described these considerations in the Assumptions and Extensions portion of the Discussion:</p><p>“Another key assumption concerning the granule cells is that they sample mossy fiber inputs randomly, as is typically assumed in Marr-Albus models (Marr, 1969; Albus, 1971; LitwinKumar et al., 2017; Cayco-Gajic et al., 2017). Other studies instead argue that granule cells sample from mossy fibers with highly similar receptive fields (Garwicz et al., 1998; Brown and Bower, 2001; J¨orntell and Ekerot, 2006) defined by the tuning of mossy fiber and climbing fiber inputs to cerebellar microzones (Apps et al., 2018). This has led to an alternative hypothesis that granule cells serve to relay similarly tuned mossy fiber inputs and enhance their signal-to-noise ratio (Jo¨rntell and Ekerot, 2006; Gilbert and Chris Miall, 2022) rather than to re-encode inputs. Another hypothesis is that granule cells enable Purkinje cells to learn piece-wise linear approximations of nonlinear functions (Spanne and J¨orntell, 2013). However, several recent studies support the existence of heterogeneous connectivity and selectivity of granule cells to multiple distinct inputs at the local scale (Huang et al., 2013; Ishikawa et al., 2015). Furthermore, the deviation of the predicted dimension in models constrained by electron-microscopy data as compared to randomly wired models is modest (Nguyen et al., 2022). Thus, topographically organized connectivity at the macroscopic scale may coexist with disordered connectivity at the local scale, allowing granule cells presynaptic to an individual Purkinje cell to sample heterogeneous combinations of the subset of sensorimotor signals relevant to the tasks that Purkinje cell participates in. Finally, we note that the optimality of dense codes for learning slowly varying tasks in our theory suggests that observations of a lack of mixing (J¨orntell and Ekerot, 2002) for such tasks are compatible with Marr-Albus models, as in this case nonlinear mixing is not required.”</p><p>We have also included the Jo¨rntell and Ekerot (2006) study as a citation in the Introduction:</p><p>“Indeed, several recent studies have reported dense activity in cerebellar granule cells in response to sensory stimulation or during motor control tasks (Jo¨rntell and Ekerot, 2006; Knogler et al., 2017; Wagner et al., 2017; Giovannucci et al., 2017; Badura and De Zeeuw, 2017; Wagner et al., 2019), at odds with classic theories (Marr, 1969; Albus, 1971).”</p><disp-quote content-type="editor-comment"><p>Results:</p><p>First para: There is no information about how the granule cells are modelled.</p></disp-quote><p>We agree that this should information should have been more readily available. We now more completely describe the model in the main text. Our model for granule cells can be found in Equation 1 in the Results section and also the Methods (Network Model):</p><p>“The activity of neurons in the expansion layer is given by:</p><p>h = <italic>φ</italic>(J<sup>eff</sup>x − <italic>θ</italic>)<italic>,</italic> (2)</p><p>where <italic>φ</italic> is a rectified linear activation function <italic>φ</italic>(<italic>u</italic>) = max(<italic>u,</italic>0) applied element-wise. Our results also hold for other threshold-polynomial activation functions. The scalar threshold <italic>θ</italic> is shared across neurons and controls the coding level, which we denote by <italic>f</italic>, defined as the average fraction of neurons in the expansion layer that are active.”</p><disp-quote content-type="editor-comment"><p>Second para: 'A typical assumption in computational theories of the cerebellar cortex is that inputs are randomly distributed in a high-dimensional space.' Yes, I agree, and this is in fact in conflict with the known topographical organization in the cerebellar cortex (see broader comment above). Mossy fiber inputs coding for closely related inputs are co-localized in the cerebellar cortex. I think for this model to be of interest from the point of view of the mammalian cerebellar cortex, it would need to pay more attention to this organizational feature.</p></disp-quote><p>As we discuss in our response to paragraphs 5 and 6, we see the random distribution assumption at the local scale (inputs presynaptic to a single Purkinje cell) as being compatible with topographic organization occurring at the microzone scale. Furthermore, as discussed earlier, we specifically model low-dimensional input as opposed to the random and high-dimensional inputs typically studied in prior models.</p><p>“A typical assumption in computational theories of the cerebellar cortex is that inputs are randomly distributed in a high-dimensional space (Marr, 1969; Albus, 1971; Brunel et al., 2004; Babadi and Sompolinsky, 2014; Billings et al., 2014; Litwin-Kumar et al., 2017). While this may be a reasonable simplification in some cases, many tasks, including cerebellum dependent tasks, are likely best-described as being encoded by a low-dimensional set of variables. For example, the cerebellum is often hypothesized to learn a forward model for motor control (Wolpert et al., 1998), which uses sensory input and motor efference to predict an effector’s future state. Mossy fiber activity recorded in monkeys correlates with position and velocity during natural movement (van Kan et al., 1993). Sources of motor efference copies include motor cortex, whose population activity lies on a low-dimensional manifold (Wagner et al., 2019; Huang et al., 2013; Churchland et al., 2010; Yu et al., 2009). We begin by modeling the low dimensionality of inputs and later consider more specific tasks.</p><p>We therefore assume that the inputs to our model lie on a <italic>D</italic>-dimensional subspace embedded in the <italic>N</italic>-dimensional input space, where <italic>D</italic> is typically much smaller than <italic>N</italic> (Figure 1B).</p><p>We refer to this subspace as the “task subspace” (Figure 1C).”</p><p>References</p><p>Albus, J.S. (1971). A theory of cerebellar function. Mathematical Biosciences <italic>10</italic>, 25–61.</p><p>Apps, R., et al. (2018). Cerebellar Modules and Their Role as Operational Cerebellar Processing Units. Cerebellum <italic>17</italic>, 654–682.</p><p>Babadi, B. and Sompolinsky, H. (2014). Sparseness and expansion in sensory representations. Neuron <italic>83</italic>, 1213–1226.</p><p>Badura, A. and De Zeeuw, C.I. (2017). Cerebellar granule cells: dense, rich and evolving representations. Current Biology <italic>27</italic>, R415–R418.</p><p>Barak, O., Rigotti, M., and Fusi, S. (2013). The sparseness of mixed selectivity neurons controls the generalization–discrimination trade-off. Journal of Neuroscience <italic>33</italic>, 3844– 3856.</p><p>Bell, C.C., Han, V., and Sawtell, N.B. (2008). Cerebellum-like structures and their implications for cerebellar function. Annual Review of Neuroscience <italic>31</italic>, 1–24.</p><p>Billings, G., Piasini, E., Lo˝rincz, A., Nusser, Z., and Silver, R.A. (2014). Network structure within the cerebellar input layer enables lossless sparse encoding. Neuron <italic>83</italic>, 960–974.</p><p>Bordelon, B., Canatar, A., and Pehlevan, C. (2020). Spectrum dependent learning curves in kernel regression and wide neural networks. International Conference on Machine Learning 1024–1034.</p><p>Brown, I.E. and Bower, J.M. (2001). Congruence of mossy fiber and climbing fiber tactile projections in the lateral hemispheres of the rat cerebellum. Journal of Comparative Neurology <italic>429</italic>, 59–70.</p><p>Brunel, N., Hakim, V., Isope, P., Nadal, J.P., and Barbour, B. (2004). Optimal information storage and the distribution of synaptic weights: perceptron versus Purkinje cell. Neuron <italic>43</italic>, 745–757.</p><p>Canatar, A., Bordelon, B., and Pehlevan, C. (2021). Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature Communications <italic>12</italic>, 1–12.</p><p>Cayco-Gajic, N.A., Clopath, C., and Silver, R.A. (2017). Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks. Nature Communications <italic>8</italic>, 1–11.</p><p>Chadderton, P., Margrie, T.W., and Ha¨usser, M. (2004). Integration of quanta in cerebellar granule cells during sensory processing. Nature <italic>428</italic>, 856–860.</p><p>Churchland, M.M., et al. (2010). Stimulus onset quenches neural variability: a widespread cortical phenomenon. Nature Neuroscience <italic>13</italic>, 369–378.</p><p>Farris, S.M. (2011). Are mushroom bodies cerebellum-like structures? Arthropod structure and development <italic>40</italic>, 368–379.</p><p>Garwicz, M., Jorntell, H., and Ekerot, C.F. (1998). Cutaneous receptive fields and topography of mossy fibres and climbing fibres projecting to cat cerebellar C3 zone. The Journal of Physiology <italic>512 ( Pt 1)</italic>, 277–293.</p><p>Gilbert, M. and Chris Miall, R. (2022). How and Why the Cerebellum Recodes Input Signals: An Alternative to Machine Learning. The Neuroscientist <italic>28</italic>, 206–221.</p><p>Giovannucci, A., et al. (2017). Cerebellar granule cells acquire a widespread predictive feedback signal during motor learning. Nature Neuroscience <italic>20</italic>, 727–734.</p><p>Huang, C.C., et al. (2013). Convergence of pontine and proprioceptive streams onto multimodal cerebellar granule cells. <italic>eLife 2</italic>, e00400.</p><p>Ishikawa, T., Shimuta, M., and Ha¨usser, M. (2015). Multimodal sensory integration in single cerebellar granule cells in vivo. <italic>eLife 4</italic>, e12916.</p><p>Jacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems <italic>31</italic>.</p><p>Jo¨rntell, H. and Ekerot, C.F. (2002). Reciprocal Bidirectional Plasticity of Parallel Fiber Receptive Fields in Cerebellar Purkinje Cells and Their Afferent Interneurons. Neuron <italic>34</italic>, 797–806.</p><p>Jo¨rntell, H. and Ekerot, C.F. (2006). Properties of Somatosensory Synaptic Integration in Cerebellar Granule Cells in vivo. Journal of Neuroscience <italic>26</italic>, 11786–11797.</p><p>Knogler, L.D., Markov, D.A., Dragomir, E.I., Stih, V., and Portugues, R. (2017). Senso-ˇ rimotor representations in cerebellar granule cells in larval zebrafish are dense, spatially organized, and non-temporally patterned. Current Biology <italic>27</italic>, 1288–1302.</p><p>Litwin-Kumar, A., Harris, K.D., Axel, R., Sompolinsky, H., and Abbott, L.F. (2017). Optimal degrees of synaptic connectivity. Neuron <italic>93</italic>, 1153–1164.</p><p>Marr, D. (1969). A theory of cerebellar cortex. Journal of Physiology <italic>202</italic>, 437–470.</p><p>Nguyen, T.M., et al. (2022). Structured cerebellar connectivity supports resilient pattern separation. Nature 1–7.</p><p>Saarinen, A., Linne, M.L., and Yli-Harja, O. (2008). Stochastic Differential Equation Model for Cerebellar Granule Cell Excitability. PLOS Computational Biology <italic>4</italic>, e1000004.</p><p>Simon, J.B., Dickens, M., and DeWeese, M.R. (2021). A theory of the inductive bias and generalization of kernel regression and wide neural networks. arXiv: 2110.03922.</p><p>Sollich, P. (1998). Learning curves for Gaussian processes. Advances in Neural Information Processing Systems <italic>11</italic>.</p><p>Spanne, A. and Jo¨rntell, H. (2013). Processing of Multi-dimensional Sensorimotor Information in the Spinal and Cerebellar Neuronal Circuitry: A New Hypothesis. PLOS Computational Biology <italic>9</italic>, e1002979.</p><p>Spanne, A. and Jo¨rntell, H. (2015). Questioning the role of sparse coding in the brain. Trends in Neurosciences <italic>38</italic>, 417–427.</p><p>van Kan, P.L., Gibson, A.R., and Houk, J.C. (1993). Movement-related inputs to intermediate cerebellum of the monkey. Journal of Neurophysiology <italic>69</italic>, 74–94.</p><p>Wagner, M.J., Kim, T.H., Savall, J., Schnitzer, M.J., and Luo, L. (2017). Cerebellar granule cells encode the expectation of reward. Nature <italic>544</italic>, 96–100.</p><p>Wagner, M.J., et al. (2019). Shared cortex-cerebellum dynamics in the execution and learning of a motor task. Cell <italic>177</italic>, 669–682.e24.</p><p>Wolpert, D.M., Miall, R.C., and Kawato, M. (1998). Internal models in the cerebellum. Trends in Cognitive Sciences <italic>2</italic>, 338–347.</p><p>Yu, B.M., et al. (2009). Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. Journal of Neurophysiology <italic>102</italic>, 614–635.</p></body></sub-article></article>