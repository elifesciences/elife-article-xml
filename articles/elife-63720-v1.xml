<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63720</article-id><article-id pub-id-type="doi">10.7554/eLife.63720</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-210506"><name><surname>Segalin</surname><given-names>Cristina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7219-7074</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-210507"><name><surname>Williams</surname><given-names>Jalani</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-133176"><name><surname>Karigo</surname><given-names>Tomomi</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-210508"><name><surname>Hui</surname><given-names>May</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6231-7383</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-181292"><name><surname>Zelikowsky</surname><given-names>Moriel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0465-9027</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-210509"><name><surname>Sun</surname><given-names>Jennifer J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0906-6589</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4235"><name><surname>Perona</surname><given-names>Pietro</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-18547"><name><surname>Anderson</surname><given-names>David J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6175-3872</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-209883"><name><surname>Kennedy</surname><given-names>Ann</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3782-0518</contrib-id><email>ann.kennedy@northwestern.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Department of Computing &amp; Mathematical Sciences, California Institute of Technology</institution><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Division of Biology and Biological Engineering 156-29, TianQiao and Chrissy Chen Institute for Neuroscience, California Institute of Technology</institution><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Howard Hughes Medical Institute, California Institute of Technology</institution><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Department of Neurobiology and Anatomy, University of Utah, Salt Lake City, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Department of Neuroscience, Northwestern University Feinberg School of Medicine, Chicago, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>30</day><month>11</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e63720</elocation-id><history><date date-type="received" iso-8601-date="2020-10-04"><day>04</day><month>10</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-10-14"><day>14</day><month>10</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-07-27"><day>27</day><month>07</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.07.26.222299"/></event></pub-history><permissions><copyright-statement>© 2021, Segalin et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Segalin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63720-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-63720-figures-v1.pdf"/><abstract><p>The study of naturalistic social behavior requires quantification of animals’ interactions. This is generally done through manual annotation—a highly time-consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS’s annotations to human annotations and find that MARS’s pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis in a package that is user-friendly and easily modifiable.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>social behavior</kwd><kwd>pose estimation</kwd><kwd>machine learning</kwd><kwd>computer vision</kwd><kwd>microendoscopic imaging</kwd><kwd>software</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>K99MH117264</award-id><principal-award-recipient><name><surname>Kennedy</surname><given-names>Ann</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>K99MH108734</award-id><principal-award-recipient><name><surname>Zelikowsky</surname><given-names>Moriel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014370</institution-id><institution>Simons Foundation Autism Research Initiative</institution></institution-wrap></funding-source><award-id>401141</award-id><principal-award-recipient><name><surname>Anderson</surname><given-names>David J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005237</institution-id><institution>Helen Hay Whitney Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Kennedy</surname><given-names>Ann</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>Long-term Fellowship</award-id><principal-award-recipient><name><surname>Karigo</surname><given-names>Tomomi</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH123612</award-id><principal-award-recipient><name><surname>Anderson</surname><given-names>David J</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000936</institution-id><institution>Gordon and Betty Moore Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Perona</surname><given-names>Pietro</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>Liying Huang and Charles Trimble</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Perona</surname><given-names>Pietro</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>Simons Collaboration on the Global Brain Foundation 542947</award-id><principal-award-recipient><name><surname>Anderson</surname><given-names>David J</given-names></name><name><surname>Perona</surname><given-names>Pietro</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The Mouse Action Recognition System is a computational pipeline for automated classification of social behaviors in freely interacting mice, accompanied by a graphical interface for analysis of multimodal neuroscience datasets.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain evolved to guide survival-related behaviors, which frequently involve interaction with other animals. Gaining insight into brain systems that control these behaviors requires recording and manipulating neural activity while measuring behavior in freely moving animals. Recent technological advances, such as miniaturized imaging and electrophysiological devices, have enabled the recording of neural activity in freely behaving mice (<xref ref-type="bibr" rid="bib49">Remedios et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Falkner et al., 2020</xref>)—however, to make sense of the recorded neural activity, it is also necessary to obtain a detailed characterization of the animals’ actions during recording. This is usually accomplished via manual scoring of the animals’ actions (<xref ref-type="bibr" rid="bib72">Yang et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Silverman et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Winslow, 2003</xref>). A typical study of freely behaving animals can produce tens to hundreds of hours of video that require manual behavioral annotation (<xref ref-type="bibr" rid="bib74">Zelikowsky et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Shemesh et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Branson et al., 2009</xref>). Scoring for social behaviors often takes human annotators 3–4× the video’s duration to annotate; for long recordings, there is also risk of drops in annotation quality due to drifting annotator attention. It is unclear to what extent individual human annotators within and between different labs agree on the definitions of behaviors, especially the precise timing of behavior onset/offset. When behavior is being analyzed alongside neural recording data, it is also often unclear whether the set of social behaviors that were chosen to annotate are a good fit for explaining the activity of a neural population or whether other, unannotated behaviors with clearer neural correlates may have been missed.</p><p>An accurate, sharable, automated approach to scoring social behavior is thus needed. Use of such a pipeline would enable social behavior measurements in large-scale experiments (e.g., genetic or drug screens), and comparison of datasets generated across the neuroscience community by using a common set of definitions and classification methods for behaviors of interest. Automation of behavior classification using machine learning methods poses a potential solution to both the time demand of annotation and to the risk of inter-individual and inter-lab differences in annotation style.</p><p>We present the Mouse Action Recognition System (MARS), a quartet of software tools for automated behavior analysis, training and evaluation of novel pose estimator and behavior classification models, and joint visualization of neural and behavioral data (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This software is accompanied by three datasets aimed at characterizing inter-annotator variability for both pose and behavior annotation. Together, the software and datasets introduced in this paper provide a robust computational pipeline for the analysis of social behavior in pairs of interacting mice and establish essential measures of reliability and sources of variability in human annotations of animal pose and behavior.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The Mouse Action Recognition System (MARS) data pipeline.</title><p>(<bold>A</bold>) Sample use strategies of MARS, including either out-of-the-box application or fine-tuning to custom arenas or behaviors of interest. (<bold>B</bold>) Overview of data extraction and analysis steps in a typical neuroscience experiment, indicating contributions to this process by MARS and Behavior Ensemble and Neural Trajectory Observatory (BENTO). (<bold>C</bold>) Illustration of the four stages of data processing included in MARS.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Mouse Action Recognition System (MARS) camera positioning and sample frames.</title><p>(<bold>A</bold>) Contents of the home cage and positioning of cameras for data collection. (<bold>B</bold>) Sample top- and front-view frames from mice with and without head-attached cables, including representative examples of occlusion and motion blur in the dataset (bottom row of images).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>The Mouse Action Recognition System (MARS) annotation dataset.</title><p>Number of hours scored for each behavior in the 14.2 hr MARS dataset, broken down by training, validation, and test sets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Mouse Action Recognition System (MARS) graphical user interface.</title><p>(1) File navigator, supporting queueing of multiple jobs while tracking is running. (2) User options: specify video source (top-/front-view camera), type of features to extract, and analyses to perform (pose estimation, feature extraction, behavior classification, video output). (3) Display of status updates during analysis. (4) Progress bars for current video and for all jobs in the queue.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig1-figsupp3-v1.tif"/></fig></fig-group><sec id="s1-1"><title>Contributions</title><p>The contributions of this paper are as follows:</p><sec id="s1-1-1"><title>Data</title><p>MARS pose estimators are trained on a novel corpus of manual pose annotations in top- and front-view video (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) of pairs of mice engaged in a standard resident-intruder assay (<xref ref-type="bibr" rid="bib60">Thurmond, 1975</xref>). These data include a variety of experimental manipulations of the resident animal, including mice that are unoperated, cannulated, or implanted with fiberoptic cables, fiber photometry cables, or a head-mounted microendoscope, with one or more cables leading from the animal’s head to a commutator feeding out the top of the cage. All MARS training datasets can be found at <ext-link ext-link-type="uri" xlink:href="https://neuroethology.github.io/MARS/">https://neuroethology.github.io/MARS/</ext-link> under ‘datasets.’</p><sec id="s1-1-1-1"><title>Multi-annotator pose dataset</title><p>Anatomical landmarks (‘keypoints’ in the following) in this training set are manually annotated by five human annotators, whose labels are combined to create a ‘consensus’ keypoint location for each image. 9 anatomical keypoints are annotated on each mouse in the top view, and 13 in the front view (two keypoints, corresponding to the midpoint and end of the tail, are included in this dataset but were omitted in training MARS due to high annotator noise).</p></sec><sec id="s1-1-1-2"><title>Behavior classifier training/testing dataset</title><p>MARS includes three supervised classifiers trained to detect attack, mounting, and close investigation behaviors in tracked animals. These classifiers were trained on 6.95 hr of behavior video, 4 hr of which were obtained from animals with a cable-attached device such as a microendoscope. Separate evaluation (3.85 hr) and test (3.37 hr) sets of videos were used to constrain training and evaluate MARS performance, giving a total of over 14 hr of video (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). All videos were manually annotated on a frame-by-frame basis by a single trained human annotator. Most videos in this dataset are a subset of the recent CalMS mouse social behavior dataset (<xref ref-type="bibr" rid="bib55">Sun et al., 2021a</xref>) (specifically, from Task 1).</p></sec><sec id="s1-1-1-3"><title>Multi-annotator behavior dataset</title><p>To evaluate inter-annotator variability in behavior classification, we also collected frame-by-frame manual labels of animal actions by eight trained human annotators on a dataset of ten 10-min videos. Two of these videos were annotated by all eight annotators a second time a minimum of 10 months later for evaluation of annotator self-consistency.</p></sec></sec></sec><sec id="s1-2"><title>Software</title><p>This paper is accompanied by four software tools, all of which can be found on the MARS project website at: <ext-link ext-link-type="uri" xlink:href="https://neuroethology.github.io/MARS/">https://neuroethology.github.io/MARS/</ext-link>.</p><sec id="s1-2-1"><title>MARS</title><p>An open-source, Python-based tool for running trained detection, pose estimation, and behavior classification models on video data. MARS can be run on a desktop computer equipped with TensorFlow and a graphical processing unit (GPU), and supports both Python command-line and graphical user interface (GUI)-based usage (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). The MARS GUI allows users to select a directory containing videos and will produce as output a folder containing bounding boxes, pose estimates, features, and predicted behaviors for each video in the directory.</p></sec><sec id="s1-2-2"><title>MARS_Developer</title><p>Python suite for training MARS on new datasets and behaviors. It includes the following components: (1) a module for collecting crowdsourced pose annotation datasets, (2) a module for training a MultiBox detector, (3) a module for training a stacked hourglass network for pose estimation, and (4) a module for training new behavior classifiers. It is accompanied by a Jupyter notebook guiding users through the training process.</p></sec><sec id="s1-2-3"><title>MARS_pycocotools</title><p>A fork of the popular COCO API for evaluation of object detection and pose estimation models (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>), used within MARS_Developer. In addition to the original COCO API, it includes added scripts for quantifying performance of keypoint-based pose estimates, as well as added support for computing object keypoint similarity (OKS) scores (see Materials and methods) in laboratory mice.</p></sec><sec id="s1-2-4"><title>The Behavior Ensemble and Neural Trajectory Observatory (BENTO)</title><p>A MATLAB-based GUI for synchronous display of neural recording data, multiple videos, human/automated behavior annotations, spectrograms of recorded audio, pose estimates, and 270 ‘features’ extracted from MARS pose data—such as animals’ velocities, joint angles, and relative positions. It features an interface for fast frame-by-frame manual annotation of animal behavior, as well as a tool to create annotations programmatically by applying thresholds to combinations of the MARS pose features. BENTO also provides tools for exploratory neural data analysis, such as PCA and event-triggered averaging. While BENTO can be linked to MARS to annotate and train classifiers for behaviors of interest, BENTO may also be used independently, and with plug-in support can be used to display pose estimates from other systems such as DeepLabCut (<xref ref-type="bibr" rid="bib34">Mathis et al., 2018</xref>).</p></sec></sec><sec id="s1-3"><title>Related work</title><p>Automated tracking and behavior classification can be broken into a series of computational steps, which may be implemented separately, as we do, or combined into a single module. First, animals are detected, producing a 2D/3D centroid, blob, or bounding box that captures the animal’s location, and possibly its orientation. When animals are filmed in an empty arena, a common approach is to use background subtraction to segment animals from their environments (<xref ref-type="bibr" rid="bib4">Branson et al., 2009</xref>). Deep networks for object detection (such as Inception Resnet [<xref ref-type="bibr" rid="bib58">Szegedy et al., 2017</xref>], Yolo [<xref ref-type="bibr" rid="bib48">Redmon et al., 2016</xref>], or Mask R-CNN [<xref ref-type="bibr" rid="bib22">He et al., 2017</xref>]) may also be used. Some behavior systems, such as Ethovision (<xref ref-type="bibr" rid="bib40">Noldus et al., 2001</xref>), MoTr (<xref ref-type="bibr" rid="bib41">Ohayon et al., 2013</xref>), idTracker (<xref ref-type="bibr" rid="bib46">Pérez-Escudero et al., 2014</xref>), and previous work from our group (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>), classify behavior from this location and movement information alone. MARS uses the MSC-MultiBox approach to detect each mouse prior to pose estimation; this architecture was chosen for its combined speed and accuracy.</p><p>The tracking of multiple animals raises problems not encountered in single-animal tracking systems. First, each animal must be detected, located, and identified consistently over the duration of the video. Altering the appearance of individuals using paint or dye, or selecting animals with differing coat colors, facilitates this task (<xref ref-type="bibr" rid="bib41">Ohayon et al., 2013</xref>, <xref ref-type="bibr" rid="bib17">Gal et al., 2020</xref>). In cases where these manipulations are not possible, animal identity can in some cases be tracked by identity-matching algorithms (<xref ref-type="bibr" rid="bib4">Branson et al., 2009</xref>). The pretrained version of MARS requires using animals of differing coat colors (black and white).</p><p>Second, the posture (‘pose’) of the animal, including its orientation and body part configuration, is computed for each frame and tracked across frames. A pose estimate comprises the position and identity of multiple tracked body parts, either in terms of a set of anatomical ‘keypoints’ (<xref ref-type="bibr" rid="bib61">Toshev and Szegedy, 2014</xref>), shapes (<xref ref-type="bibr" rid="bib8">Dankert et al., 2009</xref>, <xref ref-type="bibr" rid="bib12">Dollár et al., 2010</xref>), or a dense 2D or 3D mesh (<xref ref-type="bibr" rid="bib21">Güler et al., 2018</xref>). Keypoints are typically defined based on anatomical landmarks (nose, ears, paws, digits), and their selection is determined by the experimenter depending on the recording setup and type of motion being tracked.</p><p>Animal tracking and pose estimation systems have evolved in step with the field of computer vision. Early computer vision systems relied on specialized data acquisition setups using multiple cameras and/or depth sensors (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>), and were sensitive to minor changes in experimental conditions. More recently, systems for pose estimation based on machine learning and deep neural networks, including DeepLabCut (<xref ref-type="bibr" rid="bib34">Mathis et al., 2018</xref>), LEAP (<xref ref-type="bibr" rid="bib43">Pereira et al., 2019</xref>), and DeepPoseKit (<xref ref-type="bibr" rid="bib19">Graving et al., 2019</xref>), have emerged as a flexible and accurate tool in behavioral and systems neuroscience. These networks, like MARS’s pose estimator, are more accurate and more adaptable to recording changes than their predecessors (<xref ref-type="bibr" rid="bib54">Sturman et al., 2020</xref>), although they require an initial investment in creating labeled training data before they can be used.</p><p>Third, once raw animal pose data are acquired, a classification or identification of behavior is required. Several methods have been introduced for analyzing the actions of animals in an unsupervised or semi-supervised manner, in which behaviors are identified by extracting features from the animal’s pose and performing clustering or temporal segmentation based on those features, including Moseq (<xref ref-type="bibr" rid="bib68">Wiltschko et al., 2015</xref>), MotionMapper (<xref ref-type="bibr" rid="bib2">Berman et al., 2014</xref>), and multiscale unsupervised structure learning (<xref ref-type="bibr" rid="bib64">Vogelstein et al., 2014</xref>). Unsupervised techniques are said to identify behaviors in a ‘user-unbiased’ manner (although the behaviors identified do depend on how pose is preprocessed prior to clustering). Thus far, they are most successful when studying individual animals in isolation.</p><p>Our goal is to detect complex and temporally structured social behaviors that were previously determined to be of interest to experimenters; therefore, MARS takes a supervised learning approach to behavior detection. Recent examples of supervised approaches to detection of social behavior include <xref ref-type="bibr" rid="bib18">Giancardo et al., 2013</xref>, MiceProfiler (<xref ref-type="bibr" rid="bib10">de Chaumont et al., 2012</xref>), SimBA (<xref ref-type="bibr" rid="bib39">Nilsson, 2020</xref>), and <xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>. Like MARS, SimBA uses a keypoint-based representation of animal pose, obtained via separate software (supported pose representations include DeepLabCut [<xref ref-type="bibr" rid="bib34">Mathis et al., 2018</xref>], DeepPoseKit [<xref ref-type="bibr" rid="bib19">Graving et al., 2019</xref>], SLEAP [<xref ref-type="bibr" rid="bib45">Pereira et al., 2020b</xref>], and MARS itself). In contrast, Giancardo et al., Hong et al., and MiceProfiler are pre-deep-learning methods that characterize animal pose in terms of geometrical primitives (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>, <xref ref-type="bibr" rid="bib18">Giancardo et al., 2013</xref>) or contours extracted using background subtraction (<xref ref-type="bibr" rid="bib10">de Chaumont et al., 2012</xref>). Following pose estimation, all five systems extract a set of handcrafted spatiotemporal features from animal pose: features common to all systems include relative position, animal shape (typically body area), animal movement, and inter-animal orientation. MARS and Hong et al. use additional handcrafted features capturing the orientation and minimum distances between interacting animals. Both MARS and SimBA adopt the rolling feature-windowing method introduced by JAABA (<xref ref-type="bibr" rid="bib26">Kabra et al., 2013</xref>), although choice of windowing differs modestly: SimBA computes raw and normalized feature median, mean, and sum within five rolling time windows, whereas MARS computes feature mean, standard deviation, minimum, and maximum values, and uses three windows. Finally, most methods use these handcrafted features as inputs to trained ensemble-based classifiers: Adaptive Boosting in Hong et al., Random Forests in SimBA, Temporal Random Forests in Giancardo et al., and Gradient Boosting in MARS; MiceProfiler instead identifies behaviors using handcrafted functions. While there are many similarities between the approaches of these tools, direct comparison of performance is challenging due to lack of standardized evaluation metrics. We have attempted to address this issue in a separate paper (<xref ref-type="bibr" rid="bib55">Sun et al., 2021a</xref>).</p><p>A last difference between these five supervised approaches is their user interface and flexibility. Three are designed for out-of-the-box use in single, fixed settings: Giancardo et al. and Hong et al. in the resident-intruder assay, and MiceProfiler in a large open-field arena. SimBA is fully user-defined, functioning in diverse experimental arenas but requiring users to train their own pose estimation and behavior models; a GUI is provided for this purpose. MARS takes a hybrid approach: whereas the core ‘end-user’ version of MARS provides pretrained pose and behavior models that function in a standard resident-intruder assay, MARS_Developer allows users to train MARS pose and behavior models for their own applications. Unique to MARS_Developer is a novel library for collecting crowdsourced pose annotation datasets, including tools for quantifying inter-human variability in pose labels and using this variability to evaluate trained pose models. The BENTO GUI accompanying MARS is also unique: while BENTO does support behavior annotation and (like SimBA) behavior classifier training, it is aimed primarily at exploratory analysis of multimodal datasets. In addition to pose and annotation data, BENTO can display neural recordings and audio spectrograms, and supports basic neural data analyses such as event-triggered averaging, k-means clustering, and dimensionality reduction.</p><p>Lastly, supervised behavior classification can also be performed directly from video frames, forgoing the animal detection and pose estimation steps (<xref ref-type="bibr" rid="bib36">Monfort et al., 2020</xref>, <xref ref-type="bibr" rid="bib62">Tran et al., 2015</xref>). This is usually done by adopting variations of convolutional neural networks (CNNs) to classify frame-by-frame actions or combining CNN and recurrent neural network (RNN) architectures that classify the full video as an action or behavior, and typically requires many more labeled examples than pose-based behavior classification. We chose a pose-based approach for MARS both because it requires fewer training examples and because we find that the intermediate step of pose estimation is useful in its own right for analyzing finer features of animal behavior and is more interpretable than features extracted by CNNs directly from video frames.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Characterizing variability of human pose annotations</title><p>We investigated the degree of variability in human annotations of animal pose for two camera placements—filming animal interactions from above and from the front—using our previously published recording chamber (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). We collected 93 pairs of top- and front-view behavior videos (over 1.5 million frames per view) under a variety of lighting/camera settings, bedding conditions, and experimental manipulations of the recorded animals (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). A subset of 15,000 frames were uniformly sampled from each of the top- and front-view datasets, and manually labeled by trained human annotators for a set of anatomically defined keypoints on the bodies of each mouse (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>, see Materials and methods for description of annotator workforce). 5000 frames in our labeled dataset are from experiments in which the black mouse was implanted with either a microendoscopic, fiber photometry, or optogenetic system attached to a cable of varying color and thickness. This focus on manipulated mice allowed us to train pose estimators to be robust to the presence of devices or cables.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Quantifying human annotation variability in top- and front-view pose estimates.</title><p>(<bold>A, B</bold>) Anatomical keypoints labeled by human annotators in (<bold>A</bold>) top-view and (<bold>B</bold>) front-view movie frames. (<bold>C, D</bold>) Comparison of annotator labels in (<bold>C</bold>) top-view and (<bold>D</bold>) front-view frames. Top row: left, crop of original image shown to annotators (annotators were always provided with the full video frame), right, approximate figure of the mouse (traced for clarity). Middle-bottom rows: keypoint locations provided by three example annotators, and the extracted ‘ground truth’ from the median of all annotations. (<bold>E, F</bold>) Ellipses showing variability of human annotations of each keypoint in one example frame from (<bold>E</bold>) top view and (<bold>F</bold>) front view (N = 5 annotators, 1 standard deviation ellipse radius). (<bold>G, H</bold>) Variability in human annotations of mouse pose for the top-view video, plotted as the percentage of human annotations falling within radius X of ground truth for (<bold>G</bold>) top-view and (<bold>H</bold>) front-view frames.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig2-v1.tif"/></fig><p>To assess annotator reliability, each keypoint in each frame was annotated by five individuals; the median across annotations was taken to be the ground-truth location of that keypoint as we found this approach to be robust to outliers (<xref ref-type="fig" rid="fig2">Figure 2C–F</xref>, see Materials and methods). To quantify annotator variability, we adapted the widely used percent correct Keypoints (PCK) metric used for pose estimate evaluation (<xref ref-type="bibr" rid="bib73">Yang and Ramanan, 2013</xref>). First, for each frame, we computed the distance of each keypoint by each annotator to the median keypoint location across the remaining four annotators. Next, we computed the percentage of frames for which the annotator closest to ground truth on a given frame was within a radius X, over a range of values of X (<xref ref-type="fig" rid="fig2">Figure 2G and H</xref>, blue lines). Finally, we repeated this calculation using the annotator furthest from ground truth on each frame (green lines) and the average annotator distance to ground truth on each frame (orange lines), thus giving a sense of the range of human performance in pose annotation. We observed much higher inter-annotator variability for front-view videos compared to top-view videos: 86.2% of human-annotated keypoints fell within a 5 mm radius of ground truth in top-view frames, while only 52.3% fell within a 5 mm radius of ground truth in front-view frames (scale bar in <xref ref-type="fig" rid="fig2">Figure 2E and F</xref>). Higher inter-annotator variability in the front view likely arises from the much higher incidence of occlusion in this view, as can be seen in the sample frames in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>.</p></sec><sec id="s2-2"><title>Pose estimation of unoperated and device-implanted mice in the resident-intruder assay</title><p>We used our human-labeled pose dataset to train a machine learning system for pose estimation in interacting mice. While multiple pose estimation systems exist for laboratory mice (<xref ref-type="bibr" rid="bib34">Mathis et al., 2018</xref>, <xref ref-type="bibr" rid="bib43">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Graving et al., 2019</xref>), we chose to include a novel pose estimation system within MARS for three reasons: (1) to produce an adequately detailed representation of the animal’s posture, (2) to allow integration of MARS with existing tools that detect mice (in the form of bounding boxes) but do not produce detailed pose estimates, and (3) to ensure high-quality pose estimation in cases of occlusion and motion blur during social interactions. Pose estimation in MARS is carried out in two stages: MARS first detects the body of each mouse (<xref ref-type="fig" rid="fig3">Figure 3</xref>), then crops the video frame to the detected bounding box and estimates the animal’s pose within the cropped image (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Performance of the mouse detection network.</title><p>(<bold>A</bold>) Processing stages of mouse detection pipeline. (<bold>B</bold>) Illustration of intersection over union (IoU) metric for the top-view video. (<bold>C</bold>) Precision-recall (PR) curves for multiple IoU thresholds for detection of the two mice in the top-view video. (<bold>D</bold>) Illustration of IoU for the front-view video. (<bold>E</bold>) PR curves for multiple IoU thresholds in the front-view video.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig3-v1.tif"/></fig><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Performance of the stacked hourglass network for pose estimation.</title><p>(<bold>A</bold>) Processing stages of pose estimation pipeline. (<bold>B</bold>) Mouse Action Recognition System (MARS) accuracy for individual body parts, showing performance for videos with vs. without a head-mounted microendoscope or fiber photometry cable on the black mouse. Gray envelop shows the accuracy of the best vs. worst human annotations; dashed black line is median human accuracy. (<bold>C</bold>) Histogram of object keypoint similarity (OKS) scores across frames in the test set. Blue bars: normalized by human annotation variability; orange bars, normalized using a fixed variability of 0.025 (see Materials and methods). (<bold>D</bold>) MARS accuracy for individual body parts in front-view videos with vs. without microendoscope or fiber photometry cables. (<bold>E</bold>) Histogram of OKS scores for the front-view camera. (<bold>F</bold>) Sample video frames (above) and MARS pose estimates (below) in cases of occlusion and motion blur.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Breakdown of Mouse Action Recognition System (MARS) keypoint errors for top- and front-view pose models.</title><p>Left: precision/recall curves as a function of object keypoint similarity (OKS) cutoff; area under the curve indicated in legend. Right: breakdown of error sources and their effect on precision/recall curve at an OKS cutoff of 0.85. Error types are as defined in <xref ref-type="bibr" rid="bib51">Ruggero Ronchi and Perona, 2017</xref>. Classes of keypoint position errors: <italic>Miss</italic>: large localization error; <italic>Swap</italic>: confusion between similar parts of different instances (animals); <italic>Inversion</italic>: confusion between semantically similar parts of the same instance (e.g., left/right ears); <italic>Jitter</italic>: small localization errors; <italic>Opt Score</italic>: mis-ranking of predictions by confidence (not relevant); <italic>Bkg FP</italic>: performance after removing background false positives; <italic>b</italic>: performance after removing false negatives.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig4-figsupp1-v1.tif"/></fig></fig-group><p>MARS’s detector performs MSC-MultiBox detection (<xref ref-type="bibr" rid="bib57">Szegedy et al., 2014</xref>) using the Inception ResNet v2 (<xref ref-type="bibr" rid="bib58">Szegedy et al., 2017</xref>) network architecture (see Materials and methods for details; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). We evaluated detection performance using the intersection over union (IoU) metric (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>) for both top- and front-view datasets (<xref ref-type="fig" rid="fig3">Figure 3B and D</xref>). Plotting precision-recall (PR) curves for various IoU cutoffs revealed a single optimal performance point for both the black and white mice, in both the top and front views (seen as an ‘elbow’ in plotted PR curves, <xref ref-type="fig" rid="fig3">Figure 3C and E</xref>).</p><p>Following detection, MARS estimates the pose of each mouse using a stacked hourglass network architecture with eight hourglass subunits (<xref ref-type="bibr" rid="bib38">Newell et al., 2016</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Stacked hourglass networks achieve high performance in human pose estimation, and similar two-hourglass architectures produce accurate pose estimates in single-animal settings (<xref ref-type="bibr" rid="bib43">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Graving et al., 2019</xref>). The stacked hourglass network architecture pools information across multiple spatial scales of the image to infer the location of keypoints, producing high-quality pose estimates even in cases of occlusion and motion blur (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>To contrast MARS performance to human annotator variability, we first evaluated MARS in terms of the PCK metric introduced in <xref ref-type="fig" rid="fig2">Figure 2</xref>. MARS pose estimates reach the upper limit of human accuracy for both top- and front-view frames, suggesting that the quality of human pose annotation is a limiting factor of the model’s performance (<xref ref-type="fig" rid="fig4">Figure 4B and C</xref>). In the top view, 92% of estimated keypoints fell within 5 mm of ground truth, while in the front view 67% of estimates fell within a 5 mm radius of ground truth (scale bar in <xref ref-type="fig" rid="fig2">Figure 2E</xref>). Because of the poor performance of front-view pose estimation, we opted to use only the top-view video and pose in our supervised behavior classifiers.</p><p>To summarize the performance of MARS’s keypoint estimation model, we also used the OKS metric (<xref ref-type="bibr" rid="bib51">Ruggero Ronchi and Perona, 2017</xref>) as this is widely used in the human pose estimation literature (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>; <xref ref-type="bibr" rid="bib71">Xiao et al., 2018</xref>) and has been adopted by other multi-animal pose estimation tools (<xref ref-type="bibr" rid="bib43">Pereira et al., 2019</xref>). For each body part, OKS computes the distance between ground-truth and predicted keypoints, normalized by the variance <italic>sigma</italic> of human annotators labeling that part. We computed the human variance term sigma from our 15,000-frame manual annotation dataset and observed values ranging from 0.039 to 0.084 in top-view images and 0.087–0.125 in front-view images (see Materials and methods). We also computed OKS using a fixed sigma of 0.025 for all body parts for direct comparison with OKS scores reported by SLEAP (<xref ref-type="bibr" rid="bib45">Pereira et al., 2020b</xref>). Following the approach established by COCO (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>), we report OKS in terms of the mean average precision (mAP) and mean average recall (mAR), as well as by the average precision and average recall at two specific IoU cutoffs (see Materials and methods for details). Results are shown in <xref ref-type="table" rid="table1">Table 1</xref>. Finally, we use the approach of Ronchi and Perona to break down keypoint location errors by class (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>); we find that keypoint error is best accounted for by noise in point placement and by left/right inversion in the front-view pose estimates.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Performance of MARS top-view pose estimation model.</title><p>‘Sigma from data’ column normalizes pose model performance by observed inter-human variability of each keypoint estimate.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Sigma from data</th><th align="left" valign="bottom">Sigma = 0.025</th></tr></thead><tbody><tr><td align="left" valign="bottom">mAP</td><td align="char" char="." valign="bottom">0.902</td><td align="char" char="." valign="bottom">0.628</td></tr><tr><td align="left" valign="bottom">AP@IoU = 50</td><td align="char" char="." valign="bottom">0.99</td><td align="char" char="." valign="bottom">0.967</td></tr><tr><td align="left" valign="bottom">AP@IoU = 75</td><td align="char" char="." valign="bottom">0.957</td><td align="char" char="." valign="bottom">0.732</td></tr><tr><td align="left" valign="bottom">mAR</td><td align="char" char="." valign="bottom">0.924</td><td align="char" char="." valign="bottom">0.681</td></tr><tr><td align="left" valign="bottom">AR@IoU = 50</td><td align="char" char="." valign="bottom">0.991</td><td align="char" char="." valign="bottom">0.97</td></tr><tr><td align="left" valign="bottom">AR@IoU = 75</td><td align="char" char="." valign="bottom">0.97</td><td align="char" char="." valign="bottom">0.79</td></tr></tbody></table><table-wrap-foot><fn><p>mAP: mean average precision; AP: average precision; mAR: mean average recall; IoU: intersection over union; MARS: Mouse Action Recognition System.</p></fn></table-wrap-foot></table-wrap><p>On a desktop computer with 8-core Intel Xeon CPU, 24 Gb RAM, and a 12 GB Titan XP GPU, MARS performs two-animal detection and pose estimation (a total of four operations) at approximately 11 Hz.</p></sec><sec id="s2-3"><title>Quantifying inter-annotator variability in the scoring of social behaviors</title><p>As in pose estimation, different human annotators can show substantial variability in their annotation of animals’ social behaviors, even when those individuals are trained in the same lab. To better understand the variability of human behavioral annotations, we collected annotation data from eight experienced annotators on a common set of 10 behavior videos. Human annotators included three senior laboratory technicians, two postdocs with experience studying mouse social behavior, two graduate students with experience studying mouse social behavior, and one graduate student with previous experience studying fly social behavior. All annotators were instructed to score the same three social behaviors: close investigation, mounting, and attack, and given written descriptions of each behavior (see Materials and methods). Two of the eight annotators showed a markedly different annotation ‘style’ with far longer bouts of some behaviors and were omitted from further analysis (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>We noted several forms of inter-annotator disagreement, consisting of (1) the precise timing of initiation of behavior (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), (2) at what point investigation behavior transitioned to attack, and (3) the extent to which annotators merged together multiple consecutive bouts of the same behavior (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Other inter-annotator differences that we could not characterize could be ascribed to random variation. Inter-annotator differences in behavior quantification were more pronounced when behavior was reported in terms of total bouts rather than cumulative behavior time, particularly for the two omitted annotators (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Quantifying inter-annotator variability in behavior annotations.</title><p>(<bold>A</bold>) Example annotation for attack, mounting, and close investigation behaviors by six trained annotators on segments of male-female (top) and male-male (bottom) interactions. (<bold>B</bold>) Inter-annotator variability in the total reported time mice spent engaging in each behavior. (<bold>C</bold>) Inter-annotator variability in the number of reported bouts (contiguous sequences of frames) scored for each behavior. (<bold>D</bold>) Precision and recall of annotators (humans) 2–6 with respect to annotations by human 1.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Expanded set of human annotations.</title><p>All panels as in <xref ref-type="fig" rid="fig5">Figure 5</xref>, but with the two omitted annotators (humans 7 and 8) included. (<bold>A</bold>) Example annotation for attack, mounting, and close investigation behaviors by eight trained annotators on segments of male-female (top) and male-male (bottom) interactions. (<bold>B</bold>) Inter-annotator variability in the total reported time mice spent engaging in each behavior. (<bold>C</bold>) Inter-annotator variability in the number of reported bouts (contiguous sequences of frames) scored for each behavior. (<bold>D</bold>) Precision and recall of annotators (humans) 2–8 with respect to annotations by human 1 (source of Mouse Action Recognition System [MARS] behavior classifier training annotations).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Within-annotator bias and variance in annotation of attack start time.</title><p>Annotations of all attack bouts in the 10-video dataset by six human annotators. All attack bouts are aligned to the first frame on which at least three human annotators scored attack as occurring. Colored dots then reflect the time when each annotator scored each bout as starting, relative to this aligned time (the group median). Each annotator shows a characteristic bias (a shift in their mean annotation start time before or after the group median) and variance (the spread of annotation start times around this mean) in their annotation style. Some annotators did not score any attack initiated within a ±1 s window of the group median for a given bout: these points are plotted at time –1. Note that the average attack bout in the dataset is 1.65 s long (using annotations from human 1).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Inter-annotator accuracy on individual videos.</title><p>(<bold>A</bold>) Mean precision and recall of annotators 1–6, computed relative to the median of the other five annotators (mean ± SEM). Each plotted point is one video. (<bold>B</bold>) Mean annotator F1 score (harmonic mean of precision and recall) plotted against the mean bout duration for each behavior in each video. Plot suggests a close positive correlation between the average duration of behavior bouts in a video (or dataset) and the accuracy of annotators as computed by precision and recall. (<bold>C</bold>) Mean annotator F1 score plotted against the total number of frames annotated for a given behavior in each video. Correlation is weaker than in (B).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig5-figsupp3-v1.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Inter- and intra-annotator variability.</title><p>We asked eight individuals to all annotate a pair of 10 min videos twice, with at least 10 months between annotation sessions. Box plots in (<bold>B</bold>) and (<bold>D</bold>) show median (line), 25th to 75th percentiles (box), and minimum/maximum values (whiskers). *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001; effect sizes computed as U/(n1 * n2), where n1 and n2 are category sample sizes. (<bold>A</bold>) F1 score within and between annotators: we treated a given annotator (X axis) as ground truth and computed F1 score of each annotator with respect to these labels (for self-comparison, we used the first annotation session as ground truth and the second as ‘prediction’). (<bold>B</bold>) Summary of F1 score values in (<bold>A</bold>), showing mean F1 score vs. self and vs. other across annotators (attack self vs. other: p=0.00623, effect size = 0.00623, Wilcox rank sum test, N = 6 self vs. 15 other; close investigation self vs. other: p=0.0292, effect size = 0.811, Wilcox rank sum test, N = 6 self vs. 15 other). (<bold>C, D</bold>) Same as in (<bold>A</bold>), but including two additional annotators who were more variable (attack self vs. other: p=0.000498, effect size = 0.911, Wilcox rank sum test, N = 8 self vs. 28 other; close investigation self vs. other: p=0.00219, effect size = 0.862, Wilcox rank sum test, N = 8 self vs. 28 other). (<bold>E</bold>) Same data as in (<bold>C</bold>) displayed as a matrix to capture annotator identity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig5-figsupp4-v1.tif"/></fig></fig-group><p>Importantly, the precision and recall of annotators was highly dependent on the dataset used for evaluation; we found that the mean annotator F1 score was well predicted by the mean bout duration of annotations in a video, with shorter bout durations leading to lower annotator F1 scores (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). This suggests that annotator disagreement over the start and stop times of behavior bouts may be a primary form of inter-annotator variability. Furthermore, this finding shows the importance of standardized datasets for evaluating the performance of different automated annotation approaches.</p><p>Finally, to evaluate the stability of human annotations, all eight annotators re-scored 2 of the 10 behavior videos a minimum of 10 months later. We then computed with- vs. between-annotator agreement in terms of F1 score of annotations on these two videos. For both attack and close investigations, within-annotator F1 score was significantly higher than between-annotator F1 score (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>, full stats in <xref ref-type="table" rid="table2">Table 2</xref>). We hypothesize that this effect was not observed for mounting due to the higher within-annotator agreement for that behavior. These findings support our conclusion that inter-annotator variability reflects a genuine difference in annotation style between individuals, rather than inter-annotator variability being due to noise alone.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Statistical significance testing.</title><p>All t-tests are two-sided unless otherwise stated. All tests from distinct samples unless otherwise stated. Effect size for two-sample t-test is Cohen’s d. Effect size for rank sum test is U/(n1 * n2), where n1 and n2 are sample sizes of the two categories.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Figure</th><th align="left" valign="bottom">Panel</th><th align="left" valign="bottom">Identifier</th><th align="left" valign="bottom">Sample size</th><th align="left" valign="bottom">Statistical test</th><th align="left" valign="bottom">Test stat.</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">Effect size</th><th align="left" valign="bottom">DF</th><th align="left" valign="bottom">p-Value</th></tr></thead><tbody><tr><td align="char" char="." rowspan="6" valign="bottom">8</td><td align="left" rowspan="2" valign="bottom">b</td><td align="left" valign="bottom"> Chd8 GH mutant vs. GH control</td><td align="char" char="." valign="bottom">8 het8 wt</td><td align="left" valign="bottom">Two-sample t-test</td><td align="char" char="." valign="bottom">2.31</td><td align="char" char="ndash" valign="bottom">0.216–5.85</td><td align="char" char="." valign="bottom">1.155</td><td align="char" char="." valign="bottom">14</td><td align="char" char="." valign="bottom">0.0367</td></tr><tr><td align="left" valign="bottom"> Nlgn3 GH mutant vs. SH mutant</td><td align="char" char="." valign="bottom">10 GH8 SH</td><td align="left" valign="bottom">Two-sample t-test</td><td align="char" char="." valign="bottom">4.40</td><td align="char" char="ndash" valign="bottom">2.79–7.99</td><td align="char" char="." valign="bottom">1.958</td><td align="char" char="." valign="bottom">16</td><td align="char" char="." valign="bottom">0.000449</td></tr><tr><td align="left" valign="bottom">c</td><td align="left" valign="bottom"> BTBR SH mutant vs. SH control</td><td align="char" char="." valign="bottom">10 het 10 wt</td><td align="left" valign="bottom">Two-sample t-test</td><td align="char" char="." valign="bottom">2.59</td><td align="char" char="ndash" valign="bottom">0.923–8.91</td><td align="char" char="." valign="bottom">1.157</td><td align="char" char="." valign="bottom">18</td><td align="char" char="." valign="bottom">0.0186</td></tr><tr><td align="left" rowspan="3" valign="bottom">e</td><td align="left" valign="bottom"> Close investigate</td><td align="char" char="." valign="bottom">10 het 10 wt</td><td align="left" valign="bottom">Two-sample t-test</td><td align="char" char="." valign="bottom">4.58</td><td align="char" char="ndash" valign="bottom">0.276–0.743</td><td align="char" char="." valign="bottom">2.05</td><td align="char" char="." valign="bottom">18</td><td align="char" char="." valign="bottom">0.000230</td></tr><tr><td align="left" valign="bottom"> Face-directed</td><td align="char" char="." valign="bottom">10 het 10 wt</td><td align="left" valign="bottom">Two-sample t-test</td><td align="char" char="." valign="bottom">3.84</td><td align="char" char="ndash" valign="bottom">0.171–0.582</td><td align="char" char="." valign="bottom">1.72</td><td align="char" char="." valign="bottom">18</td><td align="char" char="." valign="bottom">0.00120</td></tr><tr><td align="left" valign="bottom"> Genital-directed</td><td align="char" char="." valign="bottom">10 het 10 wt</td><td align="left" valign="bottom">Two-sample t-test</td><td align="char" char="." valign="bottom">5.01</td><td align="char" char="ndash" valign="bottom">0.233–0.568</td><td align="char" char="." valign="bottom">2.24</td><td align="char" char="." valign="bottom">18</td><td align="char" char="." valign="bottom">0.0000903</td></tr><tr><td align="left" rowspan="4" valign="bottom">ED 8</td><td align="left" rowspan="2" valign="bottom">b</td><td align="left" valign="bottom"> Attack</td><td align="left" valign="bottom">6 vs. self 15 vs. other</td><td align="left" valign="bottom">Wilcoxon rank sum</td><td align="left" valign="bottom">U = 79</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.878</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.00623</td></tr><tr><td align="left" valign="bottom"> Close investigation</td><td align="left" valign="bottom">6 vs. self 15 vs. other</td><td align="left" valign="bottom">Wilcoxon rank sum</td><td align="left" valign="bottom">U = 73</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.811</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.0292</td></tr><tr><td align="left" rowspan="2" valign="bottom">d</td><td align="left" valign="bottom"> Attack</td><td align="left" valign="bottom">8 vs. self 28 vs. other</td><td align="left" valign="bottom">Wilcoxon rank sum</td><td align="left" valign="bottom">U = 204</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.911</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.000498</td></tr><tr><td align="left" valign="bottom"> Close investigation</td><td align="left" valign="bottom">8 vs. self 28 vs. other</td><td align="left" valign="bottom">Wilcoxon rank sum</td><td align="left" valign="bottom">U = 193</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.862</td><td align="left" valign="bottom">x</td><td align="char" char="." valign="bottom">0.00219</td></tr></tbody></table><table-wrap-foot><fn><p>GH: group-housed; SH: singly housed.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-4"><title>MARS achieves high accuracy in the automated classification of three social behaviors</title><p>To create a training set for automatic detection of social behaviors in the resident-intruder assay, we collected manually annotated videos of social interactions between a male resident (black mouse) and a male or female intruder (white mouse). We found that classifiers trained with multiple annotators' labels of the same actions were less accurate than classifiers trained on a smaller pool of annotations from a single individual. Therefore, we trained classifiers on 6.95 hr of video annotated by a single individual (human #1 in <xref ref-type="fig" rid="fig5">Figure 5</xref>) for attack, mounting, and close investigation behavior. To avoid overfitting, we implemented early stopping of training based on performance on a separate validation set of videos, 3.85 hr in duration. Distributions of annotated behaviors in the training, evaluation, and test sets are reported in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>.</p><p>As input to behavior classifiers, we designed a set of 270 custom spatiotemporal features from the tracked poses of the two mice in the top-view video (full list of features in <xref ref-type="table" rid="table3">Table 3</xref>). For each feature, we then computed the feature mean, standard deviation, minimum, and maximum over windows of 0.1, 0.37, and 0.7 s to capture how features evolved in time. We trained a set of binary supervised classifiers to detect each behavior of interest using the XGBoost algorithm (<xref ref-type="bibr" rid="bib7">Chen and Guestrin, 2016</xref>), then smoothed classifier output and enforced one-hot labeling (i.e., one behavior/frame only) of behaviors with a Hidden Markov Model (HMM) (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>MARS feature definitions.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Name</th><th align="left" valign="bottom">Units</th><th align="left" valign="bottom">Definition</th><th align="left" valign="bottom">Res.</th><th align="left" valign="bottom">Intr.</th></tr></thead><tbody><tr><td align="left" colspan="5" valign="bottom">Position Features</td></tr><tr><td align="left" valign="bottom">(p)_x, (p)_y</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">x,y coordinates of each body part, for p in (nose, left ear, right ear, neck, left hip, right hip, tail)</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">centroid_x, centroid_y</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">x,y coordinates of the centroid of an ellipse fit to the seven keypoints representing the mouse's pose.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">centroid_head_x, centroid_head_y</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">x,y coordinates of the centroid of an ellipse fit to the nose, left and right ear, and neck keypoints.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">centroid_hips_x, centroid_hips_y</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">x,y coordinates of the centroid of an ellipse fit to the left and right hip and tail base keypoints.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">centroid_body_x, centroid_body_y</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">x,y coordinates of the centroid of an ellipse fit to the neck, left and right hip, and tail base keypoints.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">dist_edge_x, dist_edge_y</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance from the centroid of the mouse to the closest vertical (dist_edge_x) or horizontal (dist_edge_y) wall of the home cage.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">dist_edge</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance from the centroid of the mouse to the closest of the four walls of the home cage.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" colspan="5" valign="bottom">Appearance Features</td></tr><tr><td align="left" valign="bottom">phi</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">absolute orientation of the mouse, measured by the orientation of a vector from the centroid of the head to the centroid of the hips.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">ori_head</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">absolute orientation of a vector from the neck to the tip of the nose.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">ori_body</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">absolute orientation of a vector from the tail to the tip of the neck.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">angle_head_body_l, angle_head_body_r</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">angle formed by the left(right) ear, neck and left(right) hip keypoints.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">major_axis_len, minor_axis_len</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">major and minor axis of an ellipse fit to the seven keypoints representing the mouse’s pose.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">axis_ratio</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">major_axis_len/minor_axis_len (as defined above).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">area_ellipse</td><td align="left" valign="bottom">cm^2</td><td align="left" valign="bottom">area of the ellipse fit to the mouse’s pose.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">dist_(p1)(p2)</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between all pairs of keypoints (p1, p2) of the mouse’s pose.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" colspan="5" valign="bottom">Locomotion Features</td></tr><tr><td align="left" valign="bottom">speed</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">mean change in position of centroids of the head and hips (see Position Features), computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">speed_centroid</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">change in position of the mouse’s centroid (see Position Features), computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">acceleration</td><td align="left" valign="bottom">cm/s^2</td><td align="left" valign="bottom">mean change in speed of centroids of the head and hips, computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">acceleration_centroid</td><td align="left" valign="bottom">cm/s^2</td><td align="left" valign="bottom">change in speed of the mouse’s centroid, computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">speed_fwd</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">speed of the mouse in the direction of ori_body (see Appearance Features).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">radial_vel</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">component of the mouse’s centroid velocity along the vector between the centroids of the two mice, computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">tangential_vel</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">component of the mouse’s centroid velocity tangential to the vector between the centroids of the two mice, computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">speed_centroid_w(s)</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">speed of the mouse’s centroid, computed as the change in position between timepoints (s) frames apart (at 30 Hz).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">speed_(p)_w(s)</td><td align="left" valign="bottom">cm/s</td><td align="left" valign="bottom">speed of each keypoint (p) of the mouse’s pose, computed as the change in position between timepoints (s) frames apart (at 30 Hz).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" colspan="5" valign="bottom">Image-based features</td></tr><tr><td align="left" valign="bottom">pixel_change</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">mean squared value of (pixel intensity on current frame minus mean pixel intensity on previous frame) over all pixels, divided by mean pixel intensity on current frame (as defined in [Hong et al].)</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">pixel_change_ubbox_mice</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">pixel change (as above) computed only on pixels within the union of the bounding boxes of the detected mice (when bounding box overlap is greater than 0; 0 otherwise).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">(p)_pc</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">pixel change (as above) within a 20 pixel-diameter square around the keypoint for each body part (p).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">Social features</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">resh_twd_itrhb (resident head toward intruder head/body)</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">binary variable that is one if the centroid of the other mouse is within a –45° to 45° cone in front of the animal.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">rel_angle_social</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">relative angle between the body of the mouse (ori_body) and the line connecting the centroids of both mice.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">rel_dist_centroid</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between the centroids of the two mice.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">rel_dist_centroid_change</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">change in distance between the centroids of the two mice, computed across two consecutive frames.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">rel_dist_gap</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between ellipses fit two the two mice along the vector between the two ellipse centers, equivalent to Feature 13 of [Hong et al].</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">rel_dist_scaled</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between the two animals along the line connecting the two centroids, divided by length of the major axis of one mouse, equivalent to Feature 14 of [Hong et al].</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">rel_dist_head</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between centroids of ellipses fit to the heads of the two mice.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">rel_dist_body</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between centroids of ellipses fit to the bodies of the two mice.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">rel_dist_head_body</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance from the centroid of an ellipse fit to the head of mouse A to the centroid of an ellipse fit to the body of mouse B.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">overlap_bboxes</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">intersection over union of the bounding boxes of the two mice.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">area_ellipse_ratio</td><td align="left" valign="bottom">none</td><td align="left" valign="bottom">ratio of the areas of ellipses fit to the poses of the two mice.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">angle_between</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">angle between mice defined as the angle between the projection of the centroids.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">facing_angle</td><td align="left" valign="bottom">radians</td><td align="left" valign="bottom">angle between head orientation of one mouse and the line connecting the centroids of both animals.</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom">x</td></tr><tr><td align="left" valign="bottom">dist_m1(p1)_m2(p2)</td><td align="left" valign="bottom">cm</td><td align="left" valign="bottom">distance between keypoints of one mouse w.r.t to the other, for all pairs of keypoints (p1, p2).</td><td align="left" valign="bottom">x</td><td align="left" valign="bottom"/></tr></tbody></table><table-wrap-foot><fn><p>MARS: Mouse Action Recognition System.</p></fn></table-wrap-foot></table-wrap><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Performance of behavior classifiers.</title><p>(<bold>A</bold>) Processing stages of estimating behavior from pose of both mice. (<bold>B</bold>) Example output of the Mouse Action Recognition System (MARS) behavior classifiers on segments of male-female and male-male interactions compared to annotations by human 1 (source of classifier training data) and to the median of the six human annotators analyzed in <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>C</bold>) Precision, recall, and precision-recall (PR) curves of MARS with respect to human 1 for each of the three behaviors. (<bold>D</bold>) Precision, recall, and PR curves of MARS with respect to the median of the six human annotators (precision/recall for each human annotator was computed with respect to the median of the other five). (<bold>E</bold>) Mean precision and recall of human annotators vs. MARS, relative to human 1 and relative to the group median (mean ± SEM).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Mouse Action Recognition System (MARS) precision and recall is closely correlated with that of annotators on individual videos.</title><p>(<bold>A</bold>) Mean precision and recall of annotators 1–6 for each behavior in each of the 10 tested videos (plotted points; as in <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>), and MARS precision-recall (PR) curves for those videos. PR curves and points that are the same color correspond to the same video. (<bold>B</bold>) Mean annotator F1 score plotted against MARS’s F1 score for each behavior in each video. Performance of MARS is well predicted by the inter-human F1 score, which is in turn correlated with mean behavior bout duration (see <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Evaluation of Mouse Action Recognition System (MARS) on a larger test set.</title><p>(<bold>A</bold>) Precision-recall (PR) curves of MARS classifiers for test set 1 (‘no cable’), test set 2 (‘with cable’), and for the two sets combined. (<bold>B</bold>) F1 score of MARS classifiers for each behavior in each video, plotted against mean behavior bout duration in that video. Plots show no strong difference in performance between videos in which mice are unoperated (‘no cable’) and videos in which mice are implanted with a head-attached device (‘cable’).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig6-figsupp2-v1.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Training Mouse Action Recognition System (MARS) on new datasets.</title><p>(<bold>A</bold>) Sample frame from CRIM13 dataset. (<bold>B</bold>) Performance of MARS pose estimator fine-tuned to CRIM13 data as a function of fine-tuning training set size. (<bold>C</bold>) 90% percent correct keypoints (PCK) radius on CRIM13 data as a function of training set size. (<bold>D</bold>) Performance of MARS classifiers for three additional social behaviors as a function of training set size (number of frames annotated for the behavior of interest). (<bold>E</bold>) Same classifiers as in (<bold>D</bold>), now showing performance as a function of the number of bouts annotated for the behavior of interest.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig6-figsupp3-v1.tif"/></fig></fig-group><p>When tested on the 10 videos previously scored by multiple human annotators (‘test set 1,’ 1.7 hr of video, behavior breakdown in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), precision and recall of MARS classifiers was comparable to that of human annotators for both attack and close investigation, and slightly below human performance for mounting (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>, humans and MARS both evaluated with respect to human #1; <xref ref-type="video" rid="video1">Video 1</xref>). Varying the threshold of a given binary classifier in MARS produces a precision-recall curve (PR curve) showing the tradeoff between the classifier’s true-positive rate and its false-positive rate (<xref ref-type="fig" rid="fig6">Figure 6B–D</xref>, black lines). Interestingly, the precision and recall scores of different human annotators often fell quite close to this PR curve.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-63720-video1.mp4"><label>Video 1.</label><caption><title>Sample MARS output.</title></caption></media><p>False positive/negatives in MARS output could be due to mistakes by MARS; however, they may also reflect noise or errors in the human annotations serving as our ‘ground truth.’ We therefore also computed the precision and recall of MARS output relative to the pooled (median) labels of all six annotators. To pool annotators, we scored a given frame as positive for a behavior if at least three out of six annotators labeled it as such. Precision and recall of MARS relative to this ‘denoised ground truth’ was further improved, particularly for the attack classifier (<xref ref-type="fig" rid="fig6">Figure 6D and E</xref>).</p><p>The precision and recall of MARS on individual videos was highly correlated with the average precision and recall of individual annotators with respect to the annotator median (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Hence, as for human annotators, precision and recall of MARS are correlated with the average duration of behavior bouts in a video (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>), with longer behavior bouts leading to higher precision and recall values.</p><p>We next tested MARS classifiers on a second set of videos of mice with head-mounted microendoscopes or other cable-attached devices (‘test set 2,’ 1.66 hr of video, behavior breakdown in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). While precision and recall curves differ on this test set (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2A</xref>), we do not observe a difference on individual videos with vs. without cable when controlling for mean bout length in the video (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2B</xref>). We therefore conclude that MARS classifier performance is robust to occlusions and motion artifacts produced by head-mounted recording devices and cables.</p></sec><sec id="s2-5"><title>Training MARS on new data</title><p>While MARS can be used out of the box with no training data, it is often useful to study additional social behaviors or track animals in different environments. The MARS_Developer library allows users to re-train MARS detection and pose estimation models in new settings, and to train their own behavior classifiers from manually annotated videos. The training code in this library is the same code that was used to produce the end-user version of MARS presented in this paper.</p><p>To demonstrate the functionality of MARS_Developer, we trained detection and pose estimation networks on the CRIM13 dataset (<xref ref-type="bibr" rid="bib5">Burgos-Artizzu et al., 2012</xref>). We used the pose_annotation_tools library to crowdsource manual annotation of animal pose on 5,577 video frames, with five workers per frame (cost: $0.38/image). We then trained detection and pose models using the existing MARS detection and pose models as starting points for training. We found that the performance of trained models improved as a function of training set size, plateauing at around 1500 frames (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3A–C</xref>). We also trained behavior classifiers for three additional social behaviors of interest: face-directed sniffing, anogenital-directed sniffing, and intromission, using a subset of the MARS training set annotated for these behaviors. Trained classifiers achieved F1 scores of at least 0.7 for all three behaviors; by training on subsets of the full MARS dataset, we found that classifier performance improves logarithmically with training set size (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3D and E</xref>). Importantly, the number of annotated bouts of a behavior is a better predictor of classifier performance than the number of annotated frames.</p></sec><sec id="s2-6"><title>Integration of video, annotation, and neural recording data in a user interface</title><p>Because one objective of MARS is to accelerate the analysis of behavioral and neural recording data, we developed an open-source interface to allow users to more easily navigate neural recording, behavior video, and tracking data (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). This tool, called the Behavior Ensemble and Neural Trajectory Observatory (BENTO), allows users to synchronously display, navigate, analyze, and save movies from multiple behavior videos, behavior annotations, MARS pose estimates and features, audio recordings, and recorded neural activity (<xref ref-type="video" rid="video2">Video 2</xref>). BENTO is currently MATLAB-based, although a Python version is in development.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Screenshot of the Behavior Ensemble and Neural Trajectory Observatory (BENTO) user interface.</title><p>(<bold>A</bold>, left) The main user interface showing synchronous display of video, pose estimation, neural activity, and pose feature data. (Right) List of data types that can be loaded and synchronously displayed within BENTO. (<bold>B</bold>) BENTO interface for creating annotations based on thresholded combinations of Mouse Action Recognition System (MARS) pose features.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig7-v1.tif"/></fig><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-63720-video2.mp4"><label>Video 2.</label><caption><title>Joint display of video, pose estimates, neural activity, and behavior within BENTO.</title></caption></media><p>BENTO includes an interface for manual annotation of behavior, which can be combined with MARS to train, test, and apply classifiers for novel behaviors. A button in the BENTO interface allows users to launch training of new MARS behavior classifiers directly from their annotations. BENTO also allows users to access MARS pose features directly to create handcrafted filters on behavioral data (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). For example, users may create and apply a filter on inter-animal distance or resident velocity to automatically identify all frames in which feature values fall within a specified range.</p><p>BENTO also provides interfaces for several common analyses of neural activity, including event-triggered averaging, 2D linear projection of neural activity, and clustering of cells by their activity. Advanced users have the option to create additional custom analyses as plug-ins in the interface. BENTO is freely available on GitHub and is supported by documentation and a user wiki.</p></sec><sec id="s2-7"><title>Use case 1: high-throughput social behavioral profiling of multiple genetic mouse model lines</title><p>Advances in human genetics, such as genome-wide association studies (GWAS), have led to the identification of multiple gene loci that may increase susceptibility to autism (<xref ref-type="bibr" rid="bib20">Grove et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">O’Roak et al., 2012</xref>). The laboratory mouse has been used as a system for studying ‘genocopies’ of allelic variants found in humans, and dozens of mouse lines containing engineered autism-associated mutations have been produced in an effort to understand the effect of these mutations on neural circuit development and function (<xref ref-type="bibr" rid="bib53">Silverman et al., 2010</xref>, <xref ref-type="bibr" rid="bib37">Moy and Nadler, 2008</xref>). While several lines show atypical social behaviors, it is unclear whether all lines share a similar behavioral profile or whether different behavioral phenotypes are associated with different genetic mutations.</p><p>We collected and analyzed a 45 hr dataset of male-male social interactions using mice from five different lines: three lines that genocopy autism-associated mutations (Chd8 [<xref ref-type="bibr" rid="bib29">Katayama et al., 2016</xref>], Cul3 [<xref ref-type="bibr" rid="bib14">Dong et al., 2020</xref>], and Nlgn3 [<xref ref-type="bibr" rid="bib59">Tabuchi et al., 2007</xref>]), one inbred line that has previously been shown to exhibit atypical social behavior and is used as an autism ‘model’ (BTBR [<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>]), and a C57Bl/6J control line. For each line, we collected behavior videos during 10 min social interactions with a male intruder and quantified expression of attack and close investigation behaviors using MARS. In autism lines, we tested heterozygotes vs. age-matched wild-type (WT) littermates; BTBR mice were tested alongside age-matched C57Bl/6J mice. Due to the need for contrasting coat colors to distinguish interacting mouse pairs, all mice were tested with BalbC intruder males. Each mouse was tested using a repeated measures design (<xref ref-type="fig" rid="fig8">Figure 8A</xref>): first in their home cage after group-housing with heterozygous and WT littermates, and again after 2 weeks of single-housing.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Application of Mouse Action Recognition System (MARS) in a large-scale behavioral assay.</title><p>All plots: mean ± SEM, N = 8–10 mice per genotype per line (83 mice total); *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001. (<bold>A</bold>) Assay design. (<bold>B</bold>) Time spent attacking by group-housed (GH) and single-housed (SH) mice from each line compared to controls (<italic>Chd8</italic> GH het vs. ctrl: p=0.0367, Cohen’s d = 1.155, two-sample t-test, N = 8 het vs. 8 ctrl; <italic>Nlgn3</italic> het GH vs. SH: p=0.000449, Cohen’s d = 1.958, two-sample t-test, N = 10 GH vs. 8 SH). (<bold>C</bold>) Time spent engaged in close investigation by each condition/line (BTBR SH BTBR vs. ctrl: p=0.0186, Cohen’s d = 1.157, two-sample t-test, N = 10 BTBR vs. 10 ctrl). (<bold>D</bold>) Cartoon showing segmentation of close investigation bouts into face-, body-, and genital-directed investigation. Frames are classified based on the position of the resident’s nose relative to a boundary midway between the intruder mouse’s nose and neck, and a boundary midway between the intruder mouse’s hips and tail base. (<bold>E</bold>) Average duration of close investigation bouts in BTBR mice for investigation as a whole and broken down by the body part investigated (close investigation, p=0.00023, Cohen’s d = 2.05; face-directed p=0.00120, Cohen’s d = 1.72; genital-directed p=0.0000903, Cohen’s d = 2.24; two-sample t-test, N = 10 het vs. 10 ctrl for all).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig8-v1.tif"/></fig><p>Consistent with previous studies, MARS annotations showed increased aggression in group-housed <italic>Chd8<sup>+/-</sup></italic> relative to WT littermate controls (<xref ref-type="fig" rid="fig8">Figure 8B</xref>; full statistical reporting in Table S2). <italic>Nlgn3<sup>+/-</sup></italic> were more aggressive than C57Bl/6J animals, consistent with previous work (<xref ref-type="bibr" rid="bib6">Burrows et al., 2015</xref>), and showed a significant increase in aggression following single-housing. But, interestingly, there was not a statistically significant difference in aggression between single-housed <italic>Nlgn3<sup>+/-</sup></italic> and their WT littermates, which were also aggressive. The increased aggression of WT littermates of <italic>Nlgn3<sup>+/-</sup></italic> may be due to their genetic background (C57Bl6-SV129 hybrid rather than pure C57Bl/6) or could arise from the environmental influence of these mice being co-housed with aggressive heterozygote littermates (<xref ref-type="bibr" rid="bib27">Kalbassi et al., 2017</xref>).</p><p>We also confirmed previous findings (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>) that BTBR mice spend less time investigating intruder mice than C57Bl/6J control animals (<xref ref-type="fig" rid="fig8">Figure 8C</xref>), and that the average duration of close investigation bouts was reduced (<xref ref-type="fig" rid="fig8">Figure 8E</xref>, left). Using MARS’s estimate of the intruder mouse’s pose, we defined two anatomical boundaries on the intruder mouse’s body: a ‘face-body boundary’ midway between the nose and neck keypoints, and a ‘body-genital boundary’ midway between the tail and the center of the hips. We used these boundaries to automatically label frames of close investigation as either face-, body-, or genital-directed investigation (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). This relabeling revealed that in addition to showing shorter investigation bouts in general, the BTBR mice showed shorter bouts of face- and genital-directed investigation compared to C57Bl/6J controls, while the duration of body-directed investigation bouts was not significantly different from controls (<xref ref-type="fig" rid="fig8">Figure 8E</xref>). This finding may suggest a loss of preference for investigation of, or sensitivity to, socially relevant pheromonal cues in the BTBR inbred line.</p><p>Without automation of behavior annotation by MARS, analysis of body part-specific investigation would have required complete manual reannotation of the dataset, a prohibitively slow process. Our findings in BTBR mice therefore demonstrate the power of MARS behavior labels and pose features as a resource for exploratory analysis of behavioral data.</p></sec><sec id="s2-8"><title>Use case 2: finding neural correlates of mounting behavior</title><p>The sensitivity of electrophysiological and two-photon neural imaging methods to motion artifacts has historically required the recording of neural activity to be performed in animals that have been head-fixed or otherwise restrained. However, head-fixed animals cannot perform many naturalistic behaviors, including social behaviors. The emergence of novel technologies such as microendoscopic imaging and silicon probe recording has enabled the recording of neural activity in freely moving animals <xref ref-type="bibr" rid="bib50">Resendez et al., 2016</xref>; however, these techniques still require animals to be fitted with a head-mounted recording device, typically tethered to an attached cable (<xref ref-type="fig" rid="fig9">Figure 9A and B</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Analysis of a microendoscopic imaging dataset using Mouse Action Recognition System (MARS) and Behavior Ensemble and Neural Trajectory Observatory (BENTO).</title><p>(<bold>A</bold>) Schematic of the imaging setup, showing head-mounted microendoscope. (<bold>B</bold>) Sample video frame with MARS pose estimate, showing appearance of the microendoscope and cable during recording. (<bold>C</bold>) Sample behavior-triggered average figure produced by BENTO. (Top) Mount-triggered average response of one example neuron within a 30 s window (mean ± SEM). (Bottom) Individual trials contributing to mount-triggered average, showing animal behavior (colored patches) and neuron response (black lines) on each trial. The behavior-triggered average interface allows the user to specify the window considered during averaging (here 10 s before to 20 s after mount initiation), whether to merge behavior bouts occurring less than X seconds apart, whether to trigger on behavior start or end, and whether to normalize individual trials before averaging; results can be saved as a pdf or exported to the MATLAB workspace. (<bold>D</bold>) Normalized mount-triggered average responses of 28 example neurons in the medial preoptic area (MPOA), identified using BENTO. Grouping of neurons reveals diverse subpopulations of cells responding at different times relative to the onset of mounting (pink dot = neuron shown in panel <bold>C</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63720-fig9-v1.tif"/></fig><p>To demonstrate the utility of MARS and BENTO for these data, we analyzed data from a recent study in the Anderson lab (<xref ref-type="bibr" rid="bib28">Karigo et al., 2021</xref>), in which a male Esr1+ Cre mouse was implanted with a microendoscopic imaging device targeting the medial preoptic area (MPOA), a hypothalamic nucleus implicated in social and reproductive behaviors (<xref ref-type="bibr" rid="bib28">Karigo et al., 2021</xref>; <xref ref-type="bibr" rid="bib67">Wei et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Wu et al., 2014</xref>). We first used MARS to automatically detect bouts of close investigation and mounting while this mouse freely interacted with a female conspecific. Next, video, pose, and annotation data were loaded into BENTO, where additional social behaviors of interest were manually annotated. Finally, we re-loaded video, pose, and annotation data in BENTO alongside traces of neural activity extracted from the MPOA imaging data.</p><p>Using BENTO’s behavior-triggered average plug-in, we visualized the activity of individual MPOA neurons when the animal initiated mounting behavior (<xref ref-type="fig" rid="fig9">Figure 9C</xref>) and identified a subset of 28 imaged neurons whose activity was modulated by mounting. Finally, using a subset of these identified cells, we exported mount-averaged activity from the behavior-triggered average plug-in and visualized their activity as a heatmap (<xref ref-type="fig" rid="fig9">Figure 9D</xref>). This analysis allowed us to quickly browse this imaging dataset and determine that multiple subtypes of mount-modulated neurons exist within the imaged MPOA population, with all analysis except for the final plotting in <xref ref-type="fig" rid="fig9">Figure 9D</xref> performed from within the BENTO user interface.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Automated systems for accurate pose estimation are increasingly available to neuroscientists and have proven to be useful for measuring animal pose and motion in a number of studies (<xref ref-type="bibr" rid="bib9">Datta et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Pereira et al., 2020a</xref>; <xref ref-type="bibr" rid="bib35">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="bib11">Dell et al., 2014</xref>). However, pose alone does not provide insight into an animal’s behavior. Together, MARS and BENTO provide an end-to-end tool for automated pose estimation and supervised social behavior classification in the widely used resident-intruder assay and links these analyses with a GUI for the quick exploration and analysis of joint neural and behavioral datasets. MARS allows users to perform high-throughput screening of social behavior expression and is robust to occlusion and motion from head-mounted recording devices and cables. The pretrained version of MARS does not require users to collect and annotate their own keypoint annotations as training data, and runs out-of-the-box on established hardware (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>). For behavioral data collected in other settings, the MARS_Developer repository allows users to fine-tune MARS’s pose estimator and behavior classifiers with their own data. MARS_Developer also allows users to train their own new behavior classifiers from annotations created within BENTO. Future versions of MARS_Developer will incorporate our fast-learning semi-supervised behavior analysis tool Trajectory Embedding for Behavior Analysis (TREBA <xref ref-type="bibr" rid="bib56">Sun et al., 2021b</xref>) to train behavior classifiers in arbitrary settings.</p><p>There is to date no field-wide consensus definition of attack, mounting, and investigation behaviors: the classifiers distributed with MARS reflect the careful annotations of one individual in the Anderson lab. MARS’s support for classifier re-training allows labs to train MARS on their own annotation data to contrast their ‘in-house’ definitions of social behaviors of interest to those used in the MARS classifiers. Comparison of trained classifiers may help to identify differences in annotation style between individuals to establish a clearer consensus definition of behaviors (<xref ref-type="bibr" rid="bib55">Sun et al., 2021a</xref>).</p><p>MARS operates without requiring multiple cameras or specialized equipment such as a depth camera, unlike our previously published system (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>). MARS is also computationally distinct from our previous work: while Hong et al. used a MATLAB implementation of cascaded pose regression (<xref ref-type="bibr" rid="bib12">Dollár et al., 2010</xref>) to fit an ellipse to the body of each mouse (a form of blob-based tracking), MARS is built using the deep learning package TensorFlow and performs true pose estimation, in that it predicts the location of individual anatomical landmarks on the bodies of the tracked mice. In terms of performance, MARS is also much more accurate and invariant to changes in lighting and to the presence of head-mounted cables compared to our earlier effort. Eliminating the IR-based depth sensor simplifies data acquisition and speeds up processing, and also allows MARS to be used without creating IR artifacts during microendoscopic imaging.</p><p>Comparing pose and behavior annotations from multiple human annotators, we were able to quantify the degree of inter-human variability in both tasks and found that in both cases MARS performs comparably to the best-performing human. This suggests that improving the quality of training data, for example, by providing better visualizations and clearer instructions to human annotators, could help to further improve the accuracy of pose estimation and behavior classification tools such as MARS. Conversely, the inter-human variability in behavior annotation may reflect the fact that animal behavior is too complex and heterogeneous for behavior labels of ‘attack’ and ‘close investigation’ to be applied consistently by multiple annotators. It is unclear whether future behavior classification efforts with more granular action categories could reduce inter-human variability and lead to higher performance by automated classifiers, or whether more granular categories would cause only greater inter-annotator disagreement, while also requiring more annotation time to collect sufficient labeled training examples for each action.</p><p>Unsupervised approaches are a promising alternative to behavior quantification and may eventually bypass the need for human input during behavior discovery (<xref ref-type="bibr" rid="bib68">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Berman et al., 2014</xref>; <xref ref-type="bibr" rid="bib64">Vogelstein et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Luxem et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Hsu and Yttri, 2021</xref>). While current efforts in unsupervised behavior discovery have largely been limited to single animals, the pose estimates and features produced by MARS could potentially prove useful for future investigations that identify behaviors of interest in a user-unbiased manner. Alternatively, unsupervised analysis of MARS pose features may help to reduce redundancy among features, potentially leading to a reduction in the amount of sample data required to train classifiers to detect new behaviors of interest. We have recently developed one self-supervised tool, called Trajectory Embedding for Behavior Analysis (TREBA), that uses learned embeddings of animal movements to learn more effective features for behavior classification (<xref ref-type="bibr" rid="bib56">Sun et al., 2021b</xref>); support for TREBA feature learning will be incorporated into a future release of MARS_Developer.</p><p>Neuroscience has seen an explosive proliferation of tools for automated pose estimation and behavior classification, distributed in accessible open-source packages that have fostered widespread adoption (<xref ref-type="bibr" rid="bib34">Mathis et al., 2018</xref>, <xref ref-type="bibr" rid="bib19">Graving et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Nilsson, 2020</xref>; <xref ref-type="bibr" rid="bib45">Pereira et al., 2020b</xref>; <xref ref-type="bibr" rid="bib66">Walter and Couzin, 2021</xref>). However, these tools still require users to provide their own labeled training data, and their performance still depends on training set size and quality. And while the annotation process post-training is made faster and more painless, each lab is still left to create its own definition for each behavior of interest, with no clear mechanisms in place in the community to standardize or compare classifiers. To address these issues, MARS provides a pretrained pose estimation and behavior classification system, with publicly available, well-characterized training data. By this approach, MARS is intended to facilitate comparison of behavioral results between labs, fostering large-scale screening of social behaviors within a common analysis platform.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data collection</title><sec id="s4-1-1"><title>Animals</title><p><italic>Chd8<sup>+/-</sup></italic> and <italic>Cul3<sup>+/-</sup></italic> mice were obtained from Dr. Mark Zylka, BTBR and <italic>Nlgn3<sup>+/- </sup></italic>mice were obtained from Jackson Labs (BTBR Stock No. 2282, Nlgn3 Stock No. 8475), and wild-type C57Bl/6J and BALB/c mice were obtained from Charles River. All mice were received at 6–10 weeks of age and were maintained in the Caltech animal facility, where they were housed with three same-sex littermates (unless otherwise noted) on a reverse 11 hr dark 13 hr light cycle with food and water ad libitum. Behavior was tested during the dark cycle. All experimental procedures involving the use of live animals or their tissues were performed in accordance with the NIH guidelines and approved by the Institutional Animal Care and Use Committee (IACUC) and the Institutional Biosafety Committee at the California Institute of Technology (Caltech).</p></sec><sec id="s4-1-2"><title>The resident-intruder assay</title><p>Testing for social behaviors using the resident-intruder assay (<xref ref-type="bibr" rid="bib3">Blanchard et al., 2003</xref>) was performed as in <xref ref-type="bibr" rid="bib74">Zelikowsky et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Lee et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Hong et al., 2014</xref>; and <xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>. Experimental mice (‘residents’) were transported in their homecage (with cagemates removed) to a behavioral testing room and acclimatized for 5–15 min. Homecages were then inserted into a custom-built hardware setup (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>) with infrared video captured at 30 fps from top- and front-view cameras (Point Grey Grasshopper3) recorded at 1024 × 570 (top) and 1280 × 500 (front) pixel resolution using StreamPix video software (NorPix). Following two further minutes of acclimatization, an unfamiliar group-housed male or female BALB/c mouse (‘intruder’) was introduced to the cage, and animals were allowed to freely interact for a period of approximately 10 min. BALB/c mice are used as intruders for their white coat color (simplifying identity tracking), as well as their relatively submissive behavior, which reduces the likelihood of intruder-initiated aggression. Social behaviors were manually scored on a frame-by-frame basis, as described in the ‘Mouse behavior annotation’ section below.</p><p>Videos for training of MARS detector, pose estimator, and behavior classifier were selected from previously performed experiments in the Anderson lab. In approximately half of the videos in the training data, mice were implanted with a cranial cannula or with a head-mounted miniaturized microscope (nVista, Inscopix) or optical fiber for optogenetics or fiber photometry, attached to a cable of varying color and thickness. Surgical procedures for these implantations can be found in <xref ref-type="bibr" rid="bib28">Karigo et al., 2021</xref> and <xref ref-type="bibr" rid="bib49">Remedios et al., 2017</xref>.</p></sec><sec id="s4-1-3"><title>Screening of autism-associated mutation lines</title><p>Group-housed heterozygous male mice from mouse lines with autism-associated mutations (<italic>Chd8<sup>+/-</sup></italic>, <italic>Cul3<sup>+/-</sup></italic>, BTBR, and <italic>Nlgn3<sup>+/-</sup></italic>, plus C57Bl/6J control mice) were first tested in a standard resident-intruder assay as outlined above. To control for differences in rearing between lines, an equal number of wild-type littermate male mice from each line were tested on the same day; for the inbred BTBR strain, C57Bl/6J mice were used as controls. Between 8 and 10 animals were tested from each condition, alongside an equal number of controls (<italic>Chd8<sup>+/-</sup></italic>: 10 het + 10 control; <italic>Cul3<sup>+/-</sup></italic>: 8 het + 7 control; BTBR: 10 mice + 10 C57Bl/6J control; <italic>Nlgn3<sup>+/-</sup></italic>: 10 het + 9 control; C57Bl/6J 12 mice). This sample size range was chosen to be in line with similar resident-intruder experiments in the Anderson lab.</p><p><italic>Nlgn3<sup>+/-</sup></italic> (plus wild-type littermate controls), BTBR, and C57Bl/6J were tested at 11–13 weeks of age; <italic>Cul3<sup>+/-</sup></italic> and <italic>Chd8<sup>+/-</sup></italic> mice (plus wild-type littermate controls) were tested at 35–50 weeks of age as previous studies had noted no clear aggression phenotype in younger animals (<xref ref-type="bibr" rid="bib29">Katayama et al., 2016</xref>). Following the resident-intruder assay, mice were housed in social isolation (one mouse per cage, all cage conditions otherwise identical to those of group-housed animals) for at least 2 weeks, and then tested again in the resident-intruder assay with an unfamiliar male intruder. Two Nlgn3 single-housed het animals were excluded from analysis due to corruption of behavior videos by acquisition software. One Chd8 GH wt video was excluded due to uncharacteristic aggression by the BalbC intruder mouse.</p><p>Videos of social interactions were scored on a frame-by-frame basis for mounting, attack, and close investigation behavior using MARS; select videos were also manually annotated for these three behaviors to confirm the accuracy of MARS classifier output. Manual annotation of this dataset was performed blinded to animal genotype.</p></sec><sec id="s4-1-4"><title>Statistical analysis of autism-associated mutation lines</title><p>Testing for effect of mouse cohort (heterozygous mutant or strain vs. wild-type controls) on social behavior expression was performed using a two-tailed t-test. Because the same animals were tested in both group-housed (GH) and singly housed (SH) conditions, testing for effect of housing on social behavior expression was performed using a paired two-tailed t-test. The values tested (total time spent engaging in behavior and average duration of behavior bouts) are approximately Gaussian, justifying the use of the t-test in this analysis (<xref ref-type="table" rid="table2">Table 2</xref>).</p></sec><sec id="s4-1-5"><title>Mouse pose annotation</title><p>Part keypoint annotations are common in computer vision and are included in datasets such as Microsoft COCO (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>), MPII human pose (<xref ref-type="bibr" rid="bib1">Andriluka et al., 2014</xref>), and CUB-200-2011 (<xref ref-type="bibr" rid="bib65">Wah et al., 2011</xref>); they also form the basis of markerless pose estimation systems such as DeepLabCut (<xref ref-type="bibr" rid="bib34">Mathis et al., 2018</xref>), LEAP (<xref ref-type="bibr" rid="bib43">Pereira et al., 2019</xref>), and DeepPoseKit (<xref ref-type="bibr" rid="bib19">Graving et al., 2019</xref>). For MARS, we defined 9 anatomical keypoints in the top-view video (the nose, ears, base of neck, hips, and tail base, midpoint, and endpoint) and 13 keypoints in the front-view video (top-view keypoints plus the four paws). The tail mid- and endpoint annotations were subsequently discarded for training of MARS, leaving 7 keypoints in the top view and 11 in the front view (as in <xref ref-type="fig" rid="fig2">Figure 2A and B</xref>).</p><p>To create a dataset of video frames for labeling, we sampled 64 videos from several years of experimental projects in the Anderson lab, collected by multiple lab members (these videos were distinct from the videos used for behavior classification). While all videos were acquired in our standardized hardware setup, we observed some variability in lighting and camera contrast across the dataset; examples are shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. We extracted a set of 15,000 individual frames each from the top- and front-view cameras, giving a total of 2,700,000 individual keypoint annotations (15,000 frames × (7 top-view + 11 front-view keypoints per mouse) × 2 mice × 5 annotators). 5000 of the extracted frames included resident mice with a fiberoptic cable, cannula, or head-mounted microendoscope with cable.</p><p>We used the crowdsourcing platform Amazon Mechanical Turk (AMT) to obtain manual annotations of pose keypoints. AMT workers were provided with written instructions and illustrated examples of each keypoint, and instructed to infer the location of occluded keypoints. Frames were assigned randomly to AMT workers and were provided as images (i.e., with no temporal information). Each worker was allowed to annotate as many frames as desired until the labeling job was completed. Python scripts for creation of AMT labeling jobs and postprocessing of labeled data are included in the MARS_Developer repository.</p><p>To compensate for annotation noise, each keypoint was annotated by five AMT workers and a ‘ground truth’ location for that keypoint was defined as the median across annotators (see next section) (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>). The median was computed separately in the x and y dimensions. Annotations of individual workers were also postprocessed to correct for common mistakes, such as confusing the left and right sides of the animals. Another common worker error was to mistake the top of the head-mounted microendoscope for the resident animal’s nose; we visually screened for these errors and corrected them manually.</p></sec><sec id="s4-1-6"><title>Consolidating data across annotators</title><p>We evaluated four approaches for consolidating annotator estimates of keypoint locations: the mean, median, geometric median (<xref ref-type="bibr" rid="bib63">Vardi and Zhang, 2000</xref>), and medoid, using simulated data. We simulated a 10,000-frame dataset with n = 5 simulated clicks per frame, in which clicks were scattered around a ground-truth location with either normal or standard Cauchy-distributed noise; the latter was selected for its heavy tail compared to the normal distribution. Across 100 instantiations of this simulation, the mean error between estimated keypoint location and ground truth was as follows (mean ± STD across 100 simulations):</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Noise type</th><th align="left" valign="top">Mean</th><th align="left" valign="top">Median</th><th align="left" valign="top">Geometric median</th><th align="left" valign="top">Medoid</th></tr></thead><tbody><tr><td align="left" valign="top">Normal</td><td align="char" char="plusmn" valign="top">0.561 ± 0.00313</td><td align="char" char="plusmn" valign="top">0.671 ± 0.00376</td><td align="char" char="plusmn" valign="top">0.646 ± 0.00367</td><td align="char" char="plusmn" valign="top">0.773 ± 0.00444</td></tr><tr><td align="left" valign="top">Cauchy</td><td align="char" char="plusmn" valign="top">20.6 ± 42.4</td><td align="char" char="plusmn" valign="top">1.12 ± 0.011</td><td align="char" char="plusmn" valign="top">1.13 ± 0.0121</td><td align="char" char="plusmn" valign="top">1.43 ± 0.0125</td></tr></tbody></table></table-wrap><p>While averaging annotator clicks works well for normally distributed data, it fails when click locations have a heavy tail, which can occur when there is variation in annotator skill or level of focus. We selected the median for use in this study as it performs comparably to the geometric median while being simpler to implement and faster to compute.</p></sec><sec id="s4-1-7"><title>Bounding box annotation</title><p>For both top- and front-view video, we estimated a bounding box by finding the minimal rectangle that contained all 7 (top) or 11 (front) pose keypoints. (For better accuracy in the detection and pose estimation, we discarded the middle and end keypoints of the tail.) We then padded this minimal rectangle by a constant factor to prevent cutoff of body parts at the rectangle border.</p></sec><sec id="s4-1-8"><title>Mouse behavior annotation</title><p>We created an approximately 14 hr dataset of behavior videos, compiled across recent experiments performed by a member of the Anderson lab; this same lab member (‘human 1‘ from the multi-annotator dataset) annotated all videos on a frame-by-frame basis. The videos in this dataset were largely distinct from the videos sampled to create the 15,000-frame pose dataset. Annotators were provided with simultaneous top- and front-view video of interacting mice, and scored every video frame for close investigation, attack, and mounting, using the criteria described below. In some videos, additional behaviors were also annotated; when this occurred, these behaviors were assigned to one of close investigation, attack, mounting, or ‘other’ for the purpose of training classifiers. Definitions of these additional behaviors are listed underneath the behavior to which they were assigned. All behavior definitions reflect an Anderson lab consensus, although as evidenced by our multi-annotator comparison, even such a consensus does not prevent variability in annotation style across individuals. Annotation was performed either in BENTO or using a previously developed custom MATLAB interface (<xref ref-type="bibr" rid="bib13">Dollar, 2016</xref>).</p><p>Close investigation: Resident (black) mouse is in close contact with the intruder (white) and is actively sniffing the intruder anywhere on its body or tail. Active sniffing can usually be distinguished from passive orienting behavior by head bobbing/movements of the resident’s nose.</p><p>Other behaviors converted to the ‘close investigation’ label:</p><list list-type="bullet"><list-item><p>Sniff face: Resident investigation of the intruder’s face (typically eyes and snout).</p></list-item><list-item><p>Sniff genitals: Resident investigation of the intruder’s anogenital region, often occurs by shoving of the resident’s snout underneath the intruder’s tail.</p></list-item><list-item><p>Sniff body: Resident investigation of the intruder’s body, anywhere away from the face or genital regions.</p></list-item><list-item><p>Attempted attack: This behavior was only annotated in a small subset of videos and was grouped with investigation upon visual investigation of annotated bouts and comparison to annotations in other videos. Intruder is in a defensive posture (standing on hind legs, often facing the resident) to protect itself from attack, and resident is close to the intruder, either circling or rearing with front paws out towards/on the intruder, accompanied by investigation. Typically follows or is interspersed with bouts of actual attack; however, the behavior itself more closely resembles investigation.</p></list-item><list-item><p>Attempted mount (or attempted dom mount): This behavior was only annotated in a small subset of videos and was grouped with investigation upon visual investigation of annotated bouts and comparison to annotations in other videos. Resident attempts to climb onto or mount another animal, often accompanied by investigation. Palpitations with forepaws and pelvic thrusts may be present, but the resident is not aligned with the body of the intruder mouse or the intruder mouse may be unreceptive and still moving.</p></list-item></list><p>Attack: High-intensity behavior in which the resident is biting or tussling with the intruder, including periods between bouts of biting/tussling during which the intruder is jumping or running away and the resident is in close pursuit. Pauses during which resident/intruder are facing each other (typically while rearing) but not actively interacting should not be included.</p><p>Mount: Copulatory behavior in which the resident is hunched over the intruder, typically from the rear, and grasping the sides of the intruder using forelimbs (easier to see on the front camera). Early-stage copulation is accompanied by rapid pelvic thrusting, while later-stage copulation (sometimes annotated separately as intromission) has a slower rate of pelvic thrusting with some pausing: for the purpose of this analysis, both behaviors should be counted as mounting; however, periods where the resident is climbing on the intruder but not attempting to grasp the intruder or initiate thrusting should not.</p><p>Other behaviors converted to the ‘mount’ label:</p><list list-type="bullet"><list-item><p>Intromission: Late-stage copulatory behavior that occurs after mounting, with a slower rate of pelvic thrusting. Occasional pausing between bouts of thrusting are still counted as intromission.</p></list-item><list-item><p>Dom mount (or male-directed mounting): This behavior was only annotated in a small subset of videos. Visually similar behavior to mounting; however, typically directed towards a male intruder. The primary feature that distinguishes this behavior from mounting is an absence of ultrasonic vocalizations; bouts are also typically much shorter in duration and terminated by the intruder attempting to evade the resident.</p></list-item></list><p>Other: Behaviors that were annotated in some videos but not included in any of the above categories.</p><list list-type="bullet"><list-item><p>Approach: Resident orients and walks toward a typically stationary intruder, typically followed by periods of close investigation. Approach does not include more high-intensity behavior during which the intruder is attempting to evade the resident, which is instead classified as chase.</p></list-item><list-item><p>Chase: Resident is closely following the intruder around the home cage, while the intruder attempts to evade the resident. Typically interspersed with attempted mount or close investigation. In aggressive encounters, short periods of high-intensity chasing between periods of attack are still labeled as attack (not chase), while longer periods of chasing that do not include further attempts to attack are labeled as chasing.</p></list-item><list-item><p>Grooming: Usually in a sitting position, the mouse will lick its fur, groom with the forepaws, or scratch with any limb.</p></list-item></list></sec><sec id="s4-1-9"><title>Design of behavior training, validation, and test sets</title><p>Videos were randomly assigned to train/validation/test sets by resident mouse identity, that is, all videos of a given resident mouse were assigned to the same dataset. This practice is preferable to random assignment by video frame because the latter can lead to temporally adjacent (and hence highly correlated) frames being distributed into training and test sets, causing severe overestimation of classifier accuracy. (For example, in fivefold cross-validation with train/test set assignments randomized by frame, 96% of test-set frames will have an immediately neighboring frame in the training set.) In contrast, randomization by animal identity ensures that we do not overestimate the accuracy of MARS classifiers and best reflects the expected accuracy end-users can expect when recording under comparable conditions, as the videos in the test set are from mice that MARS has never encountered before.</p><p>Note that because data were randomized by animal identity, relative frequencies of behaviors show some variation between training, validation, and test sets. Furthermore, because some videos (most often miniscope experiments) were annotated in a more granular manner than others, some sets (e.g., test set 1) are dominated by attack/mount/sniff annotations, while other sets include more annotations from other behaviors.</p></sec><sec id="s4-1-10"><title>Behavior annotation by multiple individuals</title><p>For our analysis of inter-annotator variability in behavior scoring, we provided a group of graduate students, postdocs, and technicians in the Anderson lab with the descriptions of close investigation, mounting, and attack given above, and instructed them to score a set of 10 resident-intruder videos, all taken from unoperated mice. Annotators were given front- and top-view video of social interactions, and scored behavior using either BENTO or the Caltech Behavior Annotator (<xref ref-type="bibr" rid="bib13">Dollar, 2016</xref>), both of which support simultaneous display of front- and top-view video and frame-by-frame browsing and scoring. All but one annotator (human 4) had previous experience scoring mouse behavior videos; human 4 had previous experience scoring similar social behaviors in flies.</p><p>When comparing human annotations to ‘annotator median,’ each annotator was compared to the median of the remaining annotators. When taking the median of six annotators, a frame was considered positive for a given behavior if at least three out of six annotators labeled it as positive.</p></sec></sec><sec id="s4-2"><title>The MARS pipeline</title><sec id="s4-2-1"><title>Overview</title><p>MARS processes videos in three steps. First, videos are fed frame-by-frame into detection and pose estimation neural networks (details in following sections). Frames are loaded into a queue and passed through a set of six functions: detection preprocessing, detection, detection postprocessing, pose preprocessing, pose estimation, and pose postprocessing; output of each function is passed into an input queue for the next. MARS uses multithreading to allow each stage in the detection and pose estimation pipeline to run independently on its queued frames, reducing processing time. Pose estimates and bounding boxes are saved every 2000 frames into a json file.</p><p>Second, following pose estimation, MARS extracts a set of features from estimated poses and the original tracked video (for pixel-change features; details in following sections). The MARS interface allows users to extract several versions of features; however, this paper focuses only on the ‘top’ version of features as it requires only top-view video input. Other tested feature sets, which combined the 270 MARS features with additional features extracted from front-view video, showed little improvement in classifier performance; these feature sets are still provided as options in the MARS user interface for potential future applications. The 270 MARS features are extracted and saved as .npz and .mat files (for use with MARS and BENTO, respectively). MARS next applies temporal windowing to these features (see following sections) and saves them as separate .npz and .mat files with a ‘_wnd’ suffix.</p><p>Third, following feature extraction, MARS loads the windowed version of features and uses these as input to a set of behavior classifiers (details in following sections). The output of behavior classifiers is saved as a .txt file using the Caltech Behavior Annotator format. In addition, MARS generates an Excel file that can be used to load video, annotations, and pose estimates into BENTO.</p></sec><sec id="s4-2-2"><title>Mouse detection using the multi-scale convolutional MultiBox detector</title><p>We used the multi-scale convolutional MultiBox (MSC-MultiBox) (<xref ref-type="bibr" rid="bib15">Erhan et al., 2014</xref>; <xref ref-type="bibr" rid="bib57">Szegedy et al., 2014</xref>) approach to train a pair of deep neural networks to detect the black and white mice using our 15,000 frame bounding box annotation dataset. Specifically, we used the Inception-Resnet-v2 architecture (<xref ref-type="bibr" rid="bib58">Szegedy et al., 2017</xref>) with ImageNet pretrained weights, trained using a previously published implementation of MSC-MultiBox for TensorFlow (<ext-link ext-link-type="uri" xlink:href="https://github.com/gvanhorn38/multibox">https://github.com/gvanhorn38/multibox</ext-link>).</p><p>Briefly, MSC-MultiBox computes a short list of up to K possible object detections proposal (bounding boxes) and associated confidence scores denoting the likelihood of that box containing a target object, in this case the black or white mouse. During training, MSC-MultiBox seeks to optimize location and maximize confidence scores of predicted bounding boxes that best match the ground truth, while minimizing confidence scores of predicted bounding boxes that do not match the ground truth. Bounding box location is encoded as the coordinates of the box’s upper-left and lower-right corners, normalized with respect to the image dimensions; confidence scores are scaled between 0 (lowest) and 1 (highest). Once we have the predicted bounding box proposals and confidence score, we used non-maximum suppression (NMS) to select the bounding box proposal that best matches with the ground truth.</p><p>During training, we augmented data with random color variation and left/right flips. We used a learning rate of 0.01, decayed exponentially by 0.94 every four epochs, with an RMSProp optimizer with momentum and decay both set to 0.99 and batch size of 4. Video frames are resized to 299 × 299 during both training and inference. The model was trained on an 8-core Intel i7-6700K CPU with 32 GB RAM and an 8 GB GTX 1080 GPU. All parameters used during training are published online in the detection model config_train.yaml file published in the MARS_Developer repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/neuroecology/MARS_Developer">https://github.com/neuroecology/MARS_Developer</ext-link>.</p></sec><sec id="s4-2-3"><title>Detector evaluation</title><p>Detectors were trained on 12,750 frames from our pose annotation dataset, validated using 750 frames, and evaluated on the remaining 1500 held-out frames, which were randomly sampled from the dataset.</p><p>Model evaluation was performed using the COCO API (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>). We evaluated performance of the MSC-MultiBox detector by computing the IoU between estimated and human-annotated bounding boxes, defined as the area of the intersection of the human-annotated and estimated bounding boxes, divided by the area of their union. PR curves were plotted based on the fraction of MSC-MultiBox detected bounding boxes with an IoU &gt; X, for X in (0.5, 0.75, 0.85, 0.9, 0.95).</p></sec><sec id="s4-2-4"><title>Pose estimation</title><p>Following the detection step (above), we use the stacked hourglass network architecture (<xref ref-type="bibr" rid="bib38">Newell et al., 2016</xref>) to estimate the pose of each mouse in terms of a set of anatomical keypoints (7 keypoints in top-view videos and 11 keypoints in front-view videos) on our 15,000-frame keypoint annotation dataset. We selected the stacked hourglass architecture for its high performance on human pose estimation tasks. The network’s repeated ‘hourglass’ modules shrink an input image to a low resolution, then up-samples it while combining it with features passed via skip connections; representations from multiple scaled versions of the image are thus combined to infer keypoint location. We find that the stacked hourglass network is robust to partial occlusion of the animals using the visible portion of a mouse’s body to infer the location of parts that are hidden.</p><p>To construct the input to the stacked hourglass network, MARS crops each video frame to the bounding box of a given mouse plus an extra 65% width and height, pad the resulting image with zero-value pixels to make it square, and resize to 256 × 256 pixels. Because the stacked hourglass network converts an input image to a heatmap predicting the probability of a target keypoint being present at each pixel, during network training we constructed training heatmaps as 2D Gaussians with standard deviation of 1 px centered on each annotator-provided keypoint. During inference on user data, MARS takes the maximum value of the generated heatmap to be the keypoint’s location. We trained a separate model for pose estimation in front- vs. top-view videos, but for each view the same model was used for both the black and the white mice.</p><p>To improve generalizability of MARS pose estimation, we used several data augmentation manipulations to expand the effective size of our training set, including random blurring (p=0.15, Gaussian blur with standard deviation of 1 or 2 pixels), additive Gaussian noise (pixelwise across image with p=0.15), brightness/contrast/gamma distortion (p=0.5), and jpeg artifacts (p=0.15), random rotation (p=1, angle uniform between 0 and 180), and random padding of the bounding box and random horizontal and vertical flipping (p=0.5 each).</p><p>During training, we used an initial learning rate of 0.00025, fixed learning rate decay by a factor of 0.2 every 33 epochs to a minimum learning rate of 0.0001, and batch size of 8 (all parameters were based on the original stacked hourglass paper; <xref ref-type="bibr" rid="bib38">Newell et al., 2016</xref>). For optimization, we use the RMSProp optimizer with momentum of 0, decay of 0.9. The network was trained using TensorFlow on an 8-core Intel Xeon CPU, with 24 Gb RAM and a 12 GB Titan XP GPU. All parameters used during training are published online in pose and detection model training conFigure files, filename config_train.yaml located in the MARS_Developer repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/neuroecology/MARS_Developer">https://github.com/neuroecology/MARS_Developer</ext-link>.</p></sec><sec id="s4-2-5"><title>Pose evaluation</title><p>Each pose network was trained on 13,500 video frames from our pose annotation dataset and evaluated on the remaining 1500 held-out frames, which were randomly sampled from the full dataset. The same held-out frames were used for both detection and pose estimation steps.</p><p>We evaluated the accuracy of the MARS pose estimator by computing the ‘percent correct keypoints’ metric from COCO (<xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>), defined as the fraction of predicted keypoints on test frames that fell within a radius X of ‘ground truth.’ Ground truth for this purpose was defined as the median of human annotations of keypoint location, computed along x and y axes separately.</p><p>To summarize these curves, we used the OKS metric introduced by <xref ref-type="bibr" rid="bib51">Ruggero Ronchi and Perona, 2017</xref>. Briefly, OKS is a measure of pose accuracy that normalizes errors by the estimated variance of human annotators. Specifically, given a keypoint with ground-truth location X and estimated location X̂, the OKS for a given body part is defined as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>O</mml:mi><mml:mi>K</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Here, k<sup>2</sup> is the size of the instance (the animal) in pixels, and σ<sup>2</sup> is the variance of human annotators for that body part. Thus, an error of Z pixels is penalized more heavily for body parts where human variance is low (such as the nose), and more leniently for body parts where the ground truth itself is more unclear and human variance is higher (such as the sides of the body). We computed σ for each body part from our 15,000-frame dataset, in which each frame was annotated by five individuals. This yielded the following values of σ: for the top-view camera, ‘nose tip’: 0.039; ‘right ear’: 0.045; ‘left ear’: 0.045; ‘neck’: 0.042; ‘right-side body’: 0.067; ‘left-side body’: 0.067; ‘tail base’: 0.044; ‘middle tail’: 0.067; ‘end tail’: 0.084. For the front-view camera, ‘nose tip’: 0.087; ‘right ear’: 0.087; ‘left ear’: 0.087; ‘neck’: 0.093; ‘right-side body’: 0.125; ‘left-side body’: 0.125; ‘tail base’: 0.086; ‘middle tail’: 0.108; ‘end tail’: 0.145; ‘right front paw’: 0.125; ‘left front paw’: 0.125‘ ‘right rear paw‘: 0.125; ‘left rear paw’: 0.125. We also computed OKS values assuming a fixed σ = 0.025 for all body parts, as reported in SLEAP (<xref ref-type="bibr" rid="bib45">Pereira et al., 2020b</xref>).</p><p>OKS values are typically summarized in terms of the mAP and mAR (<xref ref-type="bibr" rid="bib47">Pishchulin et al., 2016</xref>), where precision is true positives/(true positives + false positives), and recall is true positives/(true positives + false negatives). For pose estimation, a true positive occurs when a keypoint is detected falls within some ‘permissible radius’ R of the ground truth.</p><p>To distinguish False positives from false negatives, we take advantage of the fact that MARS returns a confidence S for each keypoint, reflecting the model’s certainty that a keypoint was indeed at the provided location. (MARS’s pose model will return keypoint locations regardless of confidence; however, low confidence is often a good indicator that those locations will be less accurate.) We will therefore call a keypoint a false positive if confidence is above some threshold C but location is far from ground truth, and a false negative otherwise. Because there is always a ground-truth keypoint location (even when occluded), there is no true negative category.</p><p>Given some permissible radius R, we can thus plot precision-recall curves as one would for a classifier, by plotting precision vs. recall as we vary the confidence threshold C from 0 to 1. We summarize this plot by taking the area under the P-R curve, a value called the average precision (AP). We also report the fraction of true positive detections if any confidence score is accepted—called the average recall (AR).</p><p>The last value to set is our permissible radius R: how close does a predicted keypoint have to be to ground truth to be considered correct, and with what units? For units, we will use our previously defined OKS, which ranges from 0 (poor) to 1 (perfect). For choice of R, the accepted approach in machine learning (<xref ref-type="bibr" rid="bib47">Pishchulin et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Lin et al., 2014</xref>) is to compute the AP and AR for each of R = [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95], and then to take the mean value of AP and AR across these 10 values, thus giving the mAP and mAR.</p></sec><sec id="s4-2-6"><title>Pose features</title><p>Building on our previous work (<xref ref-type="bibr" rid="bib24">Hong et al., 2015</xref>), we designed a set of 270 spatiotemporal features extracted from the poses of interacting mice, to serve as input to supervised behavior classifiers. MARS’s features can be broadly grouped into locomotion, position, appearance, social, and image-based categories. <italic>Position features</italic> describe the position of an animal in relation to landmarks in the environment, such as the distance to the wall of the arena. <italic>Appearance-based features</italic> describe the pose of the animal in a single frame, such as the orientation of the head and body or the area of an ellipse fit to the animal’s pose. <italic>Locomotion features</italic> describe the movement of a single animal, such as speed or change of orientation of the animal. <italic>Image-based</italic> features describe the change of pixel intensity between movie frames. Finally, <italic>social features</italic> describe the position or motion of one animal relative to the other, such as inter-animal distance or difference of orientation between the animals. A full list of extracted features and their definitions can be found in <xref ref-type="table" rid="table3">Table 3</xref>. Most features are computed for both the resident and intruder mouse; however, a subset of features are identical for the two animals and are computed only for the resident, as indicated in the table.</p><p>Features are extracted for each frame of a movie, then each feature is smoothed by taking a moving average over three frames. Next, for each feature we compute the mean, standard deviation, minimum, and maximum value of that feature in windows of ±33, ±167, and ±333 ms relative to the current frame, as in <xref ref-type="bibr" rid="bib26">Kabra et al., 2013</xref>; this addition allows MARS to capture how each feature is evolving over time. We thus obtain 12 additional ‘windowed’ features for each original feature; we use 11 of these (omitting the mean of the given feature over ±1 frame) plus the original feature as input to our behavior classifiers, giving a total of 3144 features. For classification of videos with different framerates from the training set, pose trajectories are resampled to match the framerate of the classifier training data.</p><p>In addition to their use for behavior classification, the pose features extracted by MARS can be loaded and visualized within BENTO, allowing users to create custom annotations by applying thresholds to any combination of features. MARS features include many measurements that are commonly used in behavioral studies, such as animal velocity and distance to arena walls (<xref ref-type="table" rid="table3">Table 3</xref>).</p></sec><sec id="s4-2-7"><title>Behavior classifiers</title><p>From our full 14 hr video dataset, we randomly selected a training set of 6.95 hr of video annotated on a frame-by-frame basis by a single individual (human #1 in <xref ref-type="fig" rid="fig5">Figure 5</xref>) for close investigation, mounting, and attack behavior. From these annotated videos, for each behavior we constructed a training set (<bold>X, y</bold>) where <italic>X<sub>i</sub></italic> corresponds to the 3144 windowed MARS pose features on frame <italic>i</italic>, and <italic>y<sub>i</sub></italic> is a binary label indicating the presence or absence of the behavior of interest on frame <italic>i</italic>. We evaluated performance of and performed parameter exploration using a held-out validation set of videos. A common form of error in many of our tested classifiers was to have sequences (1–3 frames) of false negative or false positives that were shorter than the typical behavior annotation bout. To correct these short error bouts, we introduced a postprocessing stage following frame-wise classification, in which the classifier prediction is smoothed using an HMM followed by a three-frame moving average.</p><p>In preliminary exploration, we found that high precision and recall values for individual binary behavior classifiers were achieved by gradient boosting using the XGBoost algorithm (<xref ref-type="bibr" rid="bib7">Chen and Guestrin, 2016</xref>); we therefore used this algorithm for the three classifiers presented in this paper. Custom Python code to train novel behavior classifiers is included with the MARS_Developer software. Classifier hyperparameters may be set by the user, otherwise MARS will provide default values.</p><p>Each trained classifier produces a predicted probability that the behavior occurred, as well as a binarized output created by thresholding that probability value. Following predictions by individual classifiers, MARS combines all classifier outputs to produce a single, multi-class label for each frame of a behavior video. To do so, we select on each frame the behavior label that has the highest predicted probability of occurring; if no behavior has a predicted probability of &gt;0.5, then the frame is labeled as ‘other’ (no behavior occurring). The advantage of this approach over training multi-class XGBoost is that it allows our ensemble of classifiers to be more easily expanded in the future to include additional behaviors of interest because it does not require the original training set to be fully re-annotated for the new behavior.</p></sec><sec id="s4-2-8"><title>Classifier evaluation</title><p>Accuracy of MARS behavior classifiers was estimated in terms of classifier precision and recall, where precision = (number of true positive frames)/(number of true positive and false positive frames), and recall = (number of true positive frames)/(number of true positive and false negative frames). Precision and recall scores were estimated for the set of trained binary classifiers on a held-out test set of videos not seen during classifier training. PR curves were created for each behavior classifier by calculating classifier precision and recall values as the decision threshold (the threshold for classifying a frame as positive for a behavior) is varied from 0 to 1.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Investigation, Methodology, Software, Supervision, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Investigation, Software, Visualization</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>Data curation, Supervision</p></fn><fn fn-type="con" id="con6"><p>Investigation</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Writing – original draft</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Funding acquisition, Project administration, Writing – original draft</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Data curation, Investigation, Project administration, Software, Visualization, Writing – original draft</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures involving the use of live animals or their tissues were performed in accordance with the recommendations in the Guide for the Care and use of Laboratory animals of the National Institutes of Health. All of the animals were handled according to the Institutional Animal Care and Use Committee (IACUC) protocols (IA18-1552); the protocol was approved by the Institutional Biosafety Committee at the California Institute of Technology (Caltech).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-63720-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data used to train and test MARS is hosted by the Caltech library at <ext-link ext-link-type="uri" xlink:href="https://data.caltech.edu">data.caltech.edu</ext-link>. MARS code is publicly available on Github: - Core end-user version of MARS: <ext-link ext-link-type="uri" xlink:href="http://github.com/neuroethology/MARS">http://github.com/neuroethology/MARS</ext-link> - Code for training new MARS models: <ext-link ext-link-type="uri" xlink:href="http://github.com/neuroethology/MARS_Developer">http://github.com/neuroethology/MARS_Developer</ext-link> - Bento interface for browsing data: <ext-link ext-link-type="uri" xlink:href="http://github.com/neuroethology/bentoMAT">http://github.com/neuroethology/bentoMAT</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Zelikowski</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>The Mouse Action Recognition System (MARS): pose annotation data (Version 1.0) [Data set]</data-title><source>CaltechDATA</source><pub-id pub-id-type="doi">10.22002/D1.2011</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Zelikowski</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>The Mouse Action Recognition System (MARS): behavior annotation data (Version 1.0) [Data set]</data-title><source>CaltechDATA</source><pub-id pub-id-type="doi">10.22002/D1.2012</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Segalin</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Zelikowski</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>The Mouse Action Recognition System (MARS): multi-worker behavior annotation data (Version 1.0) [Data set]</data-title><source>CaltechDATA</source><pub-id pub-id-type="doi">10.22002/D1.2121</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Grant Van Horn for providing the original TensorFlow implementation of the MSC-MultiBox detection library, Matteo Ronchi for his pose error diagnosis code, and Mark Zylka for providing the Cul3 and Chd8 mouse lines. Research reported in this publication was supported by the National Institute of Mental Health of the National Institutes of Health under Award Number R01MH123612 and 5R01MH070053 (DJA), K99MH108734 (MZ) and K99MH117264 (AK), and by the Human Frontier Science Program (TK), the Helen Hay Whitney Foundation (AK), the Simons Foundation Autism Research Initiative (DJA), the Gordon and Betty Moore Foundation (PP), and a gift from Liying Huang and Charles Trimble (to PP). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Andriluka</surname><given-names>M</given-names></name><name><surname>Pishchulin</surname><given-names>L</given-names></name><name><surname>Gehler</surname><given-names>P</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>2D Human Pose Estimation: New Benchmark and State of the Art Analysis</article-title><conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>3686</fpage><lpage>3693</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.471</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Choi</surname><given-names>DM</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id><pub-id pub-id-type="pmid">25142523</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>DC</given-names></name><name><surname>Griebel</surname><given-names>G</given-names></name><name><surname>Blanchard</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The Mouse Defense Test Battery: pharmacological and behavioral assays for anxiety and panic</article-title><source>European Journal of Pharmacology</source><volume>463</volume><fpage>97</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/s0014-2999(03)01276-7</pub-id><pub-id pub-id-type="pmid">12600704</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Bender</surname><given-names>J</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>High-throughput ethomics in large groups of <italic>Drosophila</italic></article-title><source>Nature Methods</source><volume>6</volume><fpage>451</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id><pub-id pub-id-type="pmid">19412169</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Burgos-Artizzu</surname><given-names>XP</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Social behavior recognition in continuous video</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1322</fpage><lpage>1329</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2012.6247817</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burrows</surname><given-names>EL</given-names></name><name><surname>Laskaris</surname><given-names>L</given-names></name><name><surname>Koyama</surname><given-names>L</given-names></name><name><surname>Churilov</surname><given-names>L</given-names></name><name><surname>Bornstein</surname><given-names>JC</given-names></name><name><surname>Hill-Yardin</surname><given-names>EL</given-names></name><name><surname>Hannan</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neuroligin-3 mutation implicated in autism causes abnormal aggression and increases repetitive behavior in mice</article-title><source>Molecular Autism</source><volume>6</volume><elocation-id>62</elocation-id><pub-id pub-id-type="doi">10.1186/s13229-015-0055-7</pub-id><pub-id pub-id-type="pmid">26583067</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>XGBoost: a scalable tree boosting system</article-title><conf-name>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</conf-name><fpage>785</fpage><lpage>794</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dankert</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Hoopfer</surname><given-names>ED</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated monitoring and analysis of social behavior in <italic>Drosophila</italic></article-title><source>Nature Methods</source><volume>6</volume><fpage>297</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1310</pub-id><pub-id pub-id-type="pmid">19270697</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Leifer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational neuroethology: a call to action</article-title><source>Neuron</source><volume>104</volume><fpage>11</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.038</pub-id><pub-id pub-id-type="pmid">31600508</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Chaumont</surname><given-names>F</given-names></name><name><surname>Coura</surname><given-names>RD-S</given-names></name><name><surname>Serreau</surname><given-names>P</given-names></name><name><surname>Cressant</surname><given-names>A</given-names></name><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Granon</surname><given-names>S</given-names></name><name><surname>Olivo-Marin</surname><given-names>J-C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Computerized video analysis of social interactions in mice</article-title><source>Nature Methods</source><volume>9</volume><fpage>410</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1924</pub-id><pub-id pub-id-type="pmid">22388289</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dell</surname><given-names>AI</given-names></name><name><surname>Bender</surname><given-names>JA</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name><name><surname>Noldus</surname><given-names>LPJJ</given-names></name><name><surname>Pérez-Escudero</surname><given-names>A</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name><name><surname>Wikelski</surname><given-names>M</given-names></name><name><surname>Brose</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Automated image-based tracking and its application in ecology</article-title><source>Trends in Ecology &amp; Evolution</source><volume>29</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2014.05.004</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Welinder</surname><given-names>P</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cascaded pose regression</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1078</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2010.5540094</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dollar</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Piotr’s Computer Vision Matlab Toolbox</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://pdollar.github.io/toolbox/">https://pdollar.github.io/toolbox/</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Cui</surname><given-names>W</given-names></name><name><surname>Tan</surname><given-names>Z</given-names></name><name><surname>Robinson</surname><given-names>H</given-names></name><name><surname>Gao</surname><given-names>N</given-names></name><name><surname>Luo</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>K</given-names></name><name><surname>Xiong</surname><given-names>W-C</given-names></name><name><surname>Mei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>CUL3 deficiency causes social deficits and anxiety-like behaviors by impairing excitation-inhibition balance through the promotion of cap-dependent translation</article-title><source>Neuron</source><volume>105</volume><fpage>475</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.035</pub-id><pub-id pub-id-type="pmid">31780330</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Toshev</surname><given-names>A</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Scalable Object Detection using Deep Neural Networks</article-title><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><fpage>2147</fpage><lpage>2154</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.276</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Wei</surname><given-names>D</given-names></name><name><surname>Song</surname><given-names>A</given-names></name><name><surname>Watsek</surname><given-names>LW</given-names></name><name><surname>Chen</surname><given-names>I</given-names></name><name><surname>Chen</surname><given-names>P</given-names></name><name><surname>Feng</surname><given-names>JE</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hierarchical representations of aggression in a hypothalamic-midbrain circuit</article-title><source>Neuron</source><volume>106</volume><fpage>637</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.02.014</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gal</surname><given-names>A</given-names></name><name><surname>Saragosti</surname><given-names>J</given-names></name><name><surname>Kronauer</surname><given-names>DJC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>AnTraX: High Throughput Video Tracking of Color-Tagged Insects</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.29.068478</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giancardo</surname><given-names>L</given-names></name><name><surname>Sona</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Sannino</surname><given-names>S</given-names></name><name><surname>Managò</surname><given-names>F</given-names></name><name><surname>Scheggia</surname><given-names>D</given-names></name><name><surname>Papaleo</surname><given-names>F</given-names></name><name><surname>Murino</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Automatic visual tracking and social behaviour analysis with multiple mice</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e74557</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0074557</pub-id><pub-id pub-id-type="pmid">24066146</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grove</surname><given-names>J</given-names></name><name><surname>Ripke</surname><given-names>S</given-names></name><name><surname>Als</surname><given-names>TD</given-names></name><name><surname>Mattheisen</surname><given-names>M</given-names></name><name><surname>Walters</surname><given-names>RK</given-names></name><name><surname>Won</surname><given-names>H</given-names></name><name><surname>Pallesen</surname><given-names>J</given-names></name><name><surname>Agerbo</surname><given-names>E</given-names></name><name><surname>Andreassen</surname><given-names>OA</given-names></name><name><surname>Anney</surname><given-names>R</given-names></name><name><surname>Awashti</surname><given-names>S</given-names></name><name><surname>Belliveau</surname><given-names>R</given-names></name><name><surname>Bettella</surname><given-names>F</given-names></name><name><surname>Buxbaum</surname><given-names>JD</given-names></name><name><surname>Bybjerg-Grauholm</surname><given-names>J</given-names></name><name><surname>Bækvad-Hansen</surname><given-names>M</given-names></name><name><surname>Cerrato</surname><given-names>F</given-names></name><name><surname>Chambert</surname><given-names>K</given-names></name><name><surname>Christensen</surname><given-names>JH</given-names></name><name><surname>Churchhouse</surname><given-names>C</given-names></name><name><surname>Dellenvall</surname><given-names>K</given-names></name><name><surname>Demontis</surname><given-names>D</given-names></name><name><surname>De Rubeis</surname><given-names>S</given-names></name><name><surname>Devlin</surname><given-names>B</given-names></name><name><surname>Djurovic</surname><given-names>S</given-names></name><name><surname>Dumont</surname><given-names>AL</given-names></name><name><surname>Goldstein</surname><given-names>JI</given-names></name><name><surname>Hansen</surname><given-names>CS</given-names></name><name><surname>Hauberg</surname><given-names>ME</given-names></name><name><surname>Hollegaard</surname><given-names>MV</given-names></name><name><surname>Hope</surname><given-names>S</given-names></name><name><surname>Howrigan</surname><given-names>DP</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Hultman</surname><given-names>CM</given-names></name><name><surname>Klei</surname><given-names>L</given-names></name><name><surname>Maller</surname><given-names>J</given-names></name><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Martin</surname><given-names>AR</given-names></name><name><surname>Moran</surname><given-names>JL</given-names></name><name><surname>Nyegaard</surname><given-names>M</given-names></name><name><surname>Nærland</surname><given-names>T</given-names></name><name><surname>Palmer</surname><given-names>DS</given-names></name><name><surname>Palotie</surname><given-names>A</given-names></name><name><surname>Pedersen</surname><given-names>CB</given-names></name><name><surname>Pedersen</surname><given-names>MG</given-names></name><name><surname>dPoterba</surname><given-names>T</given-names></name><name><surname>Poulsen</surname><given-names>JB</given-names></name><name><surname>Pourcain</surname><given-names>BS</given-names></name><name><surname>Qvist</surname><given-names>P</given-names></name><name><surname>Rehnström</surname><given-names>K</given-names></name><name><surname>Reichenberg</surname><given-names>A</given-names></name><name><surname>Reichert</surname><given-names>J</given-names></name><name><surname>Robinson</surname><given-names>EB</given-names></name><name><surname>Roeder</surname><given-names>K</given-names></name><name><surname>Roussos</surname><given-names>P</given-names></name><name><surname>Saemundsen</surname><given-names>E</given-names></name><name><surname>Sandin</surname><given-names>S</given-names></name><name><surname>Satterstrom</surname><given-names>FK</given-names></name><name><surname>Davey Smith</surname><given-names>G</given-names></name><name><surname>Stefansson</surname><given-names>H</given-names></name><name><surname>Steinberg</surname><given-names>S</given-names></name><name><surname>Stevens</surname><given-names>CR</given-names></name><name><surname>Sullivan</surname><given-names>PF</given-names></name><name><surname>Turley</surname><given-names>P</given-names></name><name><surname>Walters</surname><given-names>GB</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Stefansson</surname><given-names>K</given-names></name><name><surname>Geschwind</surname><given-names>DH</given-names></name><name><surname>Nordentoft</surname><given-names>M</given-names></name><name><surname>Hougaard</surname><given-names>DM</given-names></name><name><surname>Werge</surname><given-names>T</given-names></name><name><surname>Mors</surname><given-names>O</given-names></name><name><surname>Mortensen</surname><given-names>PB</given-names></name><name><surname>Neale</surname><given-names>BM</given-names></name><name><surname>Daly</surname><given-names>MJ</given-names></name><name><surname>Børglum</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Identification of common genetic risk variants for autism spectrum disorder</article-title><source>Nature Genetics</source><volume>51</volume><fpage>431</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/s41588-019-0344-8</pub-id><pub-id pub-id-type="pmid">30804558</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Güler</surname><given-names>RA</given-names></name><name><surname>Neverova</surname><given-names>N</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DensePose: Dense Human Pose Estimation in the Wild</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>7297</fpage><lpage>7306</lpage><pub-id pub-id-type="doi">10.1109/CVPR.1996.517044</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Gkioxari</surname><given-names>G</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mask R-CNN</article-title><conf-name>2017 IEEE International Conference on Computer Vision</conf-name><fpage>2980</fpage><lpage>2988</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.322</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>W</given-names></name><name><surname>Kim</surname><given-names>DW</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Antagonistic control of social versus repetitive self-grooming behaviors by separable amygdala neuronal subsets</article-title><source>Cell</source><volume>158</volume><fpage>1348</fpage><lpage>1361</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.07.049</pub-id><pub-id pub-id-type="pmid">25215491</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>W</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Burgos-Artizzu</surname><given-names>XP</given-names></name><name><surname>Zelikowsky</surname><given-names>M</given-names></name><name><surname>Navonne</surname><given-names>SG</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning</article-title><source>PNAS</source><volume>112</volume><fpage>E5351</fpage><lpage>E5360</lpage><pub-id pub-id-type="doi">10.1073/pnas.1515982112</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title><source>Nature Communications</source><volume>12</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname><given-names>M</given-names></name><name><surname>Robie</surname><given-names>AA</given-names></name><name><surname>Rivera-Alba</surname><given-names>M</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalbassi</surname><given-names>S</given-names></name><name><surname>Bachmann</surname><given-names>SO</given-names></name><name><surname>Cross</surname><given-names>E</given-names></name><name><surname>Roberton</surname><given-names>VH</given-names></name><name><surname>Baudouin</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Male and Female Mice Lacking Neuroligin-3 Modify the Behavior of Their Wild-Type Littermates</article-title><source>ENeuro</source><volume>4</volume><elocation-id>ENEURO.0145-17.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0145-17.2017</pub-id><pub-id pub-id-type="pmid">28795135</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Tai</surname><given-names>D</given-names></name><name><surname>Wahle</surname><given-names>IA</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Distinct hypothalamic control of same- and opposite-sex mounting behaviour in mice</article-title><source>Nature</source><volume>589</volume><fpage>258</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2995-0</pub-id><pub-id pub-id-type="pmid">33268894</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katayama</surname><given-names>Y</given-names></name><name><surname>Nishiyama</surname><given-names>M</given-names></name><name><surname>Shoji</surname><given-names>H</given-names></name><name><surname>Ohkawa</surname><given-names>Y</given-names></name><name><surname>Kawamura</surname><given-names>A</given-names></name><name><surname>Sato</surname><given-names>T</given-names></name><name><surname>Suyama</surname><given-names>M</given-names></name><name><surname>Takumi</surname><given-names>T</given-names></name><name><surname>Miyakawa</surname><given-names>T</given-names></name><name><surname>Nakayama</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CHD8 haploinsufficiency results in autistic-like phenotypes in mice</article-title><source>Nature</source><volume>537</volume><fpage>675</fpage><lpage>679</lpage><pub-id pub-id-type="doi">10.1038/nature19357</pub-id><pub-id pub-id-type="pmid">27602517</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Kim</surname><given-names>D-W</given-names></name><name><surname>Remedios</surname><given-names>R</given-names></name><name><surname>Anthony</surname><given-names>TE</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Madisen</surname><given-names>L</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Scalable control of mounting and attack by Esr1+ neurons in the ventromedial hypothalamus</article-title><source>Nature</source><volume>509</volume><fpage>627</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1038/nature13169</pub-id><pub-id pub-id-type="pmid">24739975</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Grewe</surname><given-names>BF</given-names></name><name><surname>Osterhout</surname><given-names>JA</given-names></name><name><surname>Ahanonu</surname><given-names>B</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Dulac</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuronal Representation of Social Information in the Medial Amygdala of Awake Behaving Mice</article-title><source>Cell</source><volume>171</volume><fpage>1176</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.10.015</pub-id><pub-id pub-id-type="pmid">29107332</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Maire</surname><given-names>M</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Zitnick</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Microsoft COCO: Common objects in context</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>740</fpage><lpage>755</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Luxem</surname><given-names>K</given-names></name><name><surname>Fuhrmann</surname><given-names>F</given-names></name><name><surname>Kürsch</surname><given-names>J</given-names></name><name><surname>Remy</surname><given-names>S</given-names></name><name><surname>Bauer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Identifying Behavioral Structure from Deep Variational Embeddings of Animal Motion</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.14.095430</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>8</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.10.008</pub-id><pub-id pub-id-type="pmid">31791006</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monfort</surname><given-names>M</given-names></name><name><surname>Andonian</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Ramakrishnan</surname><given-names>K</given-names></name><name><surname>Bargal</surname><given-names>SA</given-names></name><name><surname>Yan</surname><given-names>T</given-names></name><name><surname>Brown</surname><given-names>L</given-names></name><name><surname>Fan</surname><given-names>Q</given-names></name><name><surname>Gutfreund</surname><given-names>D</given-names></name><name><surname>Vondrick</surname><given-names>C</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Moments in time dataset: one million videos for event understanding</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>42</volume><fpage>502</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2901464</pub-id><pub-id pub-id-type="pmid">30802849</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moy</surname><given-names>SS</given-names></name><name><surname>Nadler</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Advances in behavioral genetics: mouse models of autism</article-title><source>Molecular Psychiatry</source><volume>13</volume><fpage>4</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1038/sj.mp.4002082</pub-id><pub-id pub-id-type="pmid">17848915</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stacked hourglass networks for human pose estimation</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>483</fpage><lpage>499</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simple Behavioral Analysis (SimBA): An Open Source Toolkit for Computer Classification of Complex Social Behaviors in Experimental Animals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.19.049452</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noldus</surname><given-names>LPJJ</given-names></name><name><surname>Spink</surname><given-names>AJ</given-names></name><name><surname>Tegelenbosch</surname><given-names>RAJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>EthoVision: a versatile video tracking system for automation of behavioral experiments</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>33</volume><fpage>398</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.3758/BF03195394</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Avni</surname><given-names>O</given-names></name><name><surname>Taylor</surname><given-names>AL</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Roian Egnor</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Automated multi-day tracking of marked mice for the analysis of social behaviour</article-title><source>Journal of Neuroscience Methods</source><volume>219</volume><fpage>10</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.05.013</pub-id><pub-id pub-id-type="pmid">23810825</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Roak</surname><given-names>BJ</given-names></name><name><surname>Vives</surname><given-names>L</given-names></name><name><surname>Girirajan</surname><given-names>S</given-names></name><name><surname>Karakoc</surname><given-names>E</given-names></name><name><surname>Krumm</surname><given-names>N</given-names></name><name><surname>Coe</surname><given-names>BP</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Ko</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>JD</given-names></name><name><surname>Turner</surname><given-names>EH</given-names></name><name><surname>Stanaway</surname><given-names>IB</given-names></name><name><surname>Vernot</surname><given-names>B</given-names></name><name><surname>Malig</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>C</given-names></name><name><surname>Reilly</surname><given-names>B</given-names></name><name><surname>Akey</surname><given-names>JM</given-names></name><name><surname>Borenstein</surname><given-names>E</given-names></name><name><surname>Rieder</surname><given-names>MJ</given-names></name><name><surname>Nickerson</surname><given-names>DA</given-names></name><name><surname>Bernier</surname><given-names>R</given-names></name><name><surname>Shendure</surname><given-names>J</given-names></name><name><surname>Eichler</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sporadic autism exomes reveal a highly interconnected protein network of de novo mutations</article-title><source>Nature</source><volume>485</volume><fpage>246</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/nature10989</pub-id><pub-id pub-id-type="pmid">22495309</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Willmore</surname><given-names>L</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>SSH</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast Animal Pose Estimation Using Deep Neural Networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>T</given-names></name><name><surname>Tabris</surname><given-names>N</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Ravindranath</surname><given-names>S</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Wang</surname><given-names>ZY</given-names></name><name><surname>Turner</surname><given-names>DM</given-names></name><name><surname>McKenzieSmith</surname><given-names>G</given-names></name><name><surname>Kocher</surname><given-names>SD</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Mala</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>SLEAP: Multi-Animal Pose Tracking</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.31.276246</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-Escudero</surname><given-names>A</given-names></name><name><surname>Vicente-Page</surname><given-names>J</given-names></name><name><surname>Hinz</surname><given-names>RC</given-names></name><name><surname>Arganda</surname><given-names>S</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title><source>Nature Methods</source><volume>11</volume><fpage>743</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2994</pub-id><pub-id pub-id-type="pmid">24880877</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pishchulin</surname><given-names>L</given-names></name><name><surname>Insafutdinov</surname><given-names>E</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Andres</surname><given-names>B</given-names></name><name><surname>Andriluka</surname><given-names>M</given-names></name><name><surname>Gehler</surname><given-names>PV</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deepcut: Joint subset partition and labeling for multi person pose estimation</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>4929</fpage><lpage>4937</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.533</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J</given-names></name><name><surname>Divvala</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Farhadi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>You only look once: unified, real-time object detection</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>779</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remedios</surname><given-names>R</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Zelikowsky</surname><given-names>M</given-names></name><name><surname>Grewe</surname><given-names>BF</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Social behaviour shapes hypothalamic neural ensemble representations of conspecific sex</article-title><source>Nature</source><volume>550</volume><fpage>388</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1038/nature23885</pub-id><pub-id pub-id-type="pmid">29052632</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resendez</surname><given-names>SL</given-names></name><name><surname>Jennings</surname><given-names>JH</given-names></name><name><surname>Ung</surname><given-names>RL</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name><name><surname>Zhou</surname><given-names>ZC</given-names></name><name><surname>Otis</surname><given-names>JM</given-names></name><name><surname>Nomura</surname><given-names>H</given-names></name><name><surname>McHenry</surname><given-names>JA</given-names></name><name><surname>Kosyk</surname><given-names>O</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visualization of cortical, subcortical and deep brain neural circuit dynamics during naturalistic mammalian behavior with head-mounted microscopes and chronically implanted lenses</article-title><source>Nature Protocols</source><volume>11</volume><fpage>566</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1038/nprot.2016.021</pub-id><pub-id pub-id-type="pmid">26914316</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ruggero Ronchi</surname><given-names>M</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation</article-title><conf-name>2017 IEEE International Conference on Computer Vision</conf-name><fpage>369</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.48</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shemesh</surname><given-names>Y</given-names></name><name><surname>Sztainberg</surname><given-names>Y</given-names></name><name><surname>Forkosh</surname><given-names>O</given-names></name><name><surname>Shlapobersky</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-order social interactions in groups of mice</article-title><source>eLife</source><volume>2</volume><elocation-id>e00759</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00759</pub-id><pub-id pub-id-type="pmid">24015357</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silverman</surname><given-names>JL</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Lord</surname><given-names>C</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Behavioural phenotyping assays for mouse models of autism</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>490</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1038/nrn2851</pub-id><pub-id pub-id-type="pmid">20559336</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sturman</surname><given-names>O</given-names></name><name><surname>von Ziegler</surname><given-names>L</given-names></name><name><surname>Schläppi</surname><given-names>C</given-names></name><name><surname>Akyol</surname><given-names>F</given-names></name><name><surname>Grewe</surname><given-names>B</given-names></name><name><surname>Bohacek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep Learning Based Behavioral Analysis Enables High Precision Rodent Tracking and Is Capable of Outperforming Commercial Solutions</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.21.913624</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Chakraborty</surname><given-names>D</given-names></name><name><surname>Mohanty</surname><given-names>SP</given-names></name><name><surname>Wild</surname><given-names>B</given-names></name><name><surname>Sun</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions</article-title><source>AArXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2104.02710">https://arxiv.org/abs/2104.02710</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>JJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name><name><surname>Zhan</surname><given-names>E</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Yue</surname><given-names>Y</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Task Programming: Learning Data Efficient Behavior Representations</article-title><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><fpage>2876</fpage><lpage>2885</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Scalable, High-Quality Object Detection</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.1441">https://arxiv.org/abs/1412.1441</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</article-title><conf-name>Thirty-First AAAI Conference on Artificial Intelligence</conf-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabuchi</surname><given-names>K</given-names></name><name><surname>Blundell</surname><given-names>J</given-names></name><name><surname>Etherton</surname><given-names>MR</given-names></name><name><surname>Hammer</surname><given-names>RE</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Powell</surname><given-names>CM</given-names></name><name><surname>Südhof</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A neuroligin-3 mutation implicated in autism increases inhibitory synaptic transmission in mice</article-title><source>Science</source><volume>318</volume><fpage>71</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1126/science.1146221</pub-id><pub-id pub-id-type="pmid">17823315</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thurmond</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Technique for producing and measuring territorial aggression using laboratory mice</article-title><source>Physiology &amp; Behavior</source><volume>14</volume><fpage>879</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(75)90086-4</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Toshev</surname><given-names>A</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>DeepPose: Human Pose Estimation via Deep Neural Networks</article-title><conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1653</fpage><lpage>1660</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.214</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>D</given-names></name><name><surname>Bourdev</surname><given-names>L</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Torresani</surname><given-names>L</given-names></name><name><surname>Paluri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning Spatiotemporal Features With 3D Convolutional Networks</article-title><conf-name>2015 IEEE International Conference on Computer Vision</conf-name><fpage>4489</fpage><lpage>4497</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.510</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vardi</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The multivariate L1-median and associated data depth</article-title><source>PNAS</source><volume>97</volume><fpage>1423</fpage><lpage>1426</lpage><pub-id pub-id-type="doi">10.1073/pnas.97.4.1423</pub-id><pub-id pub-id-type="pmid">10677477</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogelstein</surname><given-names>JT</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Ohyama</surname><given-names>T</given-names></name><name><surname>Kerr</surname><given-names>RA</given-names></name><name><surname>Truman</surname><given-names>JW</given-names></name><name><surname>Priebe</surname><given-names>CE</given-names></name><name><surname>Zlatic</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Discovery of brainwide neural-behavioral maps via multiscale unsupervised structure learning</article-title><source>Science</source><volume>344</volume><fpage>386</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1126/science.1250298</pub-id><pub-id pub-id-type="pmid">24674869</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Wah</surname><given-names>C</given-names></name><name><surname>Branson</surname><given-names>S</given-names></name><name><surname>Welinder</surname><given-names>P</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><data-title>The Caltech-UCSD Birds-200-2011 Dataset</data-title><source>California Institute of Technology</source><ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">http://www.vision.caltech.edu/visipedia/CUB-200-2011.html</ext-link></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walter</surname><given-names>T</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</article-title><source>eLife</source><volume>10</volume><elocation-id>e64000</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id><pub-id pub-id-type="pmid">33634789</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Y-C</given-names></name><name><surname>Wang</surname><given-names>S-R</given-names></name><name><surname>Jiao</surname><given-names>Z-L</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>J-K</given-names></name><name><surname>Li</surname><given-names>X-Y</given-names></name><name><surname>Li</surname><given-names>S-S</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>X-H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Medial preoptic area in mice is capable of mediating sexually dimorphic behaviors regardless of gender</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>279</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02648-0</pub-id><pub-id pub-id-type="pmid">29348568</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>JM</given-names></name><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Abraira</surname><given-names>VE</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping sub-second structure in mouse behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winslow</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mouse social recognition and preference</article-title><source>Current Protocols in Neuroscience</source><volume>22</volume><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1002/0471142301.ns0816s22</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Autry</surname><given-names>AE</given-names></name><name><surname>Bergan</surname><given-names>JF</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name><name><surname>Dulac</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Galanin neurons in the medial preoptic area govern parental behaviour</article-title><source>Nature</source><volume>509</volume><fpage>325</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1038/nature13307</pub-id><pub-id pub-id-type="pmid">24828191</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Wei</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Simple Baselines for Human Pose Estimation and Tracking</article-title><conf-name>Proceedings of the European Conference on Computer Vision</conf-name><fpage>466</fpage><lpage>481</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Silverman</surname><given-names>JL</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Automated three‐chambered social approach task for mice</article-title><source>Current Protocols in Neuroscience</source><volume>56</volume><fpage>21</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1002/0471142301.ns0826s56</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Articulated human detection with flexible mixtures of parts</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>35</volume><fpage>2878</fpage><lpage>2890</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.261</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zelikowsky</surname><given-names>M</given-names></name><name><surname>Hui</surname><given-names>M</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Choe</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Blanco</surname><given-names>MR</given-names></name><name><surname>Beadle</surname><given-names>K</given-names></name><name><surname>Gradinaru</surname><given-names>V</given-names></name><name><surname>Deverman</surname><given-names>BE</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Neuropeptide Tac2 Controls a Distributed Brain State Induced by Chronic Social Isolation Stress</article-title><source>Cell</source><volume>173</volume><fpage>1265</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.03.037</pub-id><pub-id pub-id-type="pmid">29775595</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63720.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Gal</surname><given-names>Asaf</given-names></name><role>Reviewer</role><aff><institution>The Rockefeller University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Segalin and colleagues present a pair of open-source software tools – MARS and BENTO – for automatic pose detection, social behavior detection, and interactive neural/behavior visualization in mice. MARS builds on previous tools for social behavior annotation, but now eliminating the need for two cameras and for a depth signal, incorporating deep learning, and building in robustness to neural implants. BENTO further extends this work by adding a suite of tools for annotating video frames, visualizing neural activity, and performing simple operations on the neural activity data such as event-triggered averaging. Importantly, Segalin and colleagues also share a large-scale dataset to train this system. Together, these tools will be useful for researchers studying the neural underpinnings of rodent social behavior, in particular with the resident intruder assay.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;The Mouse Action Recognition System (MARS): a software pipeline for automated analysis of social behaviors in mice&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Kate Wassum as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Asaf Gal (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>Segalin and colleagues present a pair of open-source software tools – MARS and BENTO – for automatic pose detection, social behavior detection, and interactive neural/behavior visualization in mice. MARS builds on previous tools for social behavior annotation, but now eliminating the need for two cameras and for a depth signal, incorporating deep learning, and building in robustness to neural implants. BENTO further extends this work by adding a suite of tools for annotating video frames, visualizing neural activity, and performing simple operations on the neural activity data such as event-triggered averaging. Importantly, Segalin and colleagues also share a large-scale dataset to train this system. Together, these tools will be useful for researchers studying the neural underpinnings of rodent social behavior, in particular with the resident intruder assay.</p><p>The reviewers were generally enthusiastic about this submission, but would like to see several revisions before recommending for acceptance in <italic>eLife</italic>. An additional point, however, was the reviewers thought that this article was more suitable as a &quot;Tools and Resources&quot; article (see guidelines here: https://reviewer.elifesciences.org/author-guide/types ), so we ask the authors to make their revisions with this classification in mind.</p><p>Essential revisions:</p><p>1) The authors promise at several points that features will become available in the future, including:</p><p>(1) A Python implementation of BENTO (MARS is already implemented in Python, whereas BENTO is currently implemented in Matlab);</p><p>(2) The ability the ability to detect behaviors in pairs of mice with the same coat color;</p><p>(3) The ability to train MARS on one's own behavior annotations.</p><p>While features 1 and 2 can, of course, wait – software takes time to develop – the absence of feature 3 is a little more confusing. BENTO appears to include an interface for annotating frames. Is it not possible for MARS to read these annotations and to include a framework in which a new classifier is trained? This is important because the repertoire of behaviors captured by the MARS classifier is limited. The previous rodent social behavior detection papers cited by the authors (e.g. refs 29,30 and 33) include a much richer menu of behavior labels (for example SimBA [ref 33] includes &quot;attack, pursuit, lateral threat, anogenital sniffing, allogrooming normal, allogrooming vigorous, mounting, scramble, flee, and upright submissive&quot;). Moreover, the authors have richer annotations available within their own training data. In the Methods, the authors note that many of the original video annotations used for this study actually did include a much higher resolution of labels, but that these labels were collapsed for training MARS. For example, the frames labeled &quot;close investigation&quot; are actually the union of five different labeled categories. &quot;Sniff face, Sniff genitals, Sniff body, Attempted attack, Attempted mount. Why were these combined, given that I would expect them to have very different neurobiological correlates. For example &quot;attempted attack&quot; would appear to have more in common with &quot;Attack&quot; than with &quot;Sniff body&quot;.</p><p>2) The authors argue at several points that supervised classification can benefit the neuroscience community by creating a common definition of social behaviors would be interoperable between labs, that could, for example &quot;facilitate comparison of behavioral results between labs, fostering large‐scale screening of social behaviors within a common analysis platform.&quot; This paper would be stronger if the authors could spell out a formula for finding consensus between annotators; given these principles, perhaps MARS could be trained to reflect this consensus. Perhaps trained MARS models could contain one or more tuning parameters, so that every annotator could be captured by some value(s) of the parameters, thus providing a unified framework while accommodating individual variation in annotation habits?</p><p>3) Multiple datasets are provided, one particularly novel one is the 10 x 10min videos of resident intruder assays, annotated by 8 individuals. It would be spectacular if those videos could be annotated a second time by the same individuals. Mainly, to see if individuals are consistent with themselves (stable style). This would nicely complement the dataset. In the description of the datasets, the relationships are sometimes not clear, e.g. is the person that annotated the large-scale dataset of ~14h also one of the 8 individuals and if yes, then which one?</p><p>4) The reviewers wanted to make sure that the 14 hour dataset will be shared (this was not clear from the manuscript). Moreover, when the reviewers attempted to download the dataset it was corrupted (this was replicated by two reviewers and the reviewing editor). In addition, the reviewers wanted to make sure that the data contains all the appropriate meta-data (e.g., annotation images/videos, which genotype the mice have, when the data was recorded, camera type, etc. )</p><p>5) The analyzed datasets and training data were all collected in the same lab, using the same standard setup, under very similar conditions that do not capture the variability expected across the many labs that use this assay. In order to be useful to other users, the conditions at which the method will work must be explicitly discussed. This is especially concerning, as the version of MARS presented in the paper does not allow users to define their own pipelines or to fine-tune the supplied one with new data. Furthermore, the feature list of the behavioral classifiers contains features that are very setup specific (e.g., distance to arena wall, absolute coordinates etc.), or organism-line specific (e.g. mouse size/area). If the authors indeed aim at creating a standardized behavior classification pipeline, which can be compared across labs, it is essential to show/discuss how the method gives consistent results across at least some of the different experimental variants of this assay.</p><p>6) Although the paper describes the algorithm well, if was hard for the reviewers to judge the actual usability and quality of the tool without access to it. The software should be made available to the reviewers (with documentation).</p><p>7) While a direct comparison to other social tracking methods (e.g., maDLC, SLEAP, others) is not necessary here, it is important to have a more comprehensive error analysis of the tracking (while the animals are in the bounding box, minimally).</p><p>8) Please ensure full statistical reporting in the main manuscript (e.g., t, F values, degrees of freedom, p value, etc.).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63720.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors promise at several points that features will become available in the future, including:</p><p>(1) A Python implementation of BENTO (MARS is already implemented in Python, whereas BENTO is currently implemented in Matlab);</p><p>(2) The ability the ability to detect behaviors in pairs of mice with the same coat color;</p><p>(3) The ability to train MARS on one's own behavior annotations.</p><p>While features 1 and 2 can, of course, wait – software takes time to develop – the absence of feature 3 is a little more confusing. BENTO appears to include an interface for annotating frames. Is it not possible for MARS to read these annotations and to include a framework in which a new classifier is trained? This is important because the repertoire of behaviors captured by the MARS classifier is limited. The previous rodent social behavior detection papers cited by the authors (e.g. refs 29,30 and 33) include a much richer menu of behavior labels (for example SimBA [ref 33] includes &quot;attack, pursuit, lateral threat, anogenital sniffing, allogrooming normal, allogrooming vigorous, mounting, scramble, flee, and upright submissive&quot;). Moreover, the authors have richer annotations available within their own training data. In the Methods, the authors note that many of the original video annotations used for this study actually did include a much higher resolution of labels, but that these labels were collapsed for training MARS. For example, the frames labeled &quot;close investigation&quot; are actually the union of five different labeled categories. &quot;Sniff face, Sniff genitals, Sniff body, Attempted attack, Attempted mount. Why were these combined, given that I would expect them to have very different neurobiological correlates. For example &quot;attempted attack&quot; would appear to have more in common with &quot;Attack&quot; than with &quot;Sniff body&quot;.</p></disp-quote><p>We thank the reviewers for this important feedback, and have attempted to address it point by point below.</p><p>First, regarding the repertoire of behaviors captured by MARS: we chose to focus on attack, mounting, and investigation in this manuscript because (a) these were the three behaviors most consistently annotated in our training and test sets, and (b) these were the three behaviors for which we could directly compare MARS performance to inter-human annotation variability. The other behaviors indicated in the Methods were present only in a subset of videos- typically in the minority of videos with accompanying microendoscopic imaging.</p><p>Nonetheless, to show that MARS can detect other behaviors, we have added text (lines 369-375) and a figure (Figure 6- figure supplement 3) to our resubmission showing performance of classifiers trained for intromission, sniff face, and sniff genital, the three behaviors with cleanest annotations in our dataset.</p><p>Furthermore, we agree with the reviewers that support for retraining MARS would increase the impact of this paper. We have therefore introduced a new Github repository, MARS_Developer, that allows users to train detection models, pose models, and behavior classifiers. The repository includes a detailed MARS_tutorial jupyter notebook that walks users through the steps in training and explains the metrics used for model evaluation in depth. This addition allows MARS to be adapted to function in diverse settings, and lets users train their own behavior classifiers using annotations produced in BENTO. We have also included a button in the BENTO interface to send annotations directly to MARS (lines 385-386 of text).</p><p>MARS_Developer is still in active development for example, we plan to integrate support for our self-supervised behavior learning tool, TREBA (Sun et al. CVPR 2021), however is currently capable of reproducing the original version of MARS from end-to-end, and supports the training of new classifiers for social behavior data.</p><p>Finally, regarding “attempted attack” and “attempted mount”: these were very loosely defined annotations made during exploratory analysis of imaging data; because they are rare, we could not train supervised classifiers to detect them with any reasonable accuracy (nor do we have a great sense of how reliable human annotations for these behaviors would be.) We assigned these behaviors to “investigation” based on close visual inspection, in which we judged how these behaviors would have been otherwise annotated- frames annotated “attempted” mount/attack can best be described as investigation that is somewhat more vigorous, with an apparent “intent” to attack or mount the other mouse.</p><disp-quote content-type="editor-comment"><p>2) The authors argue at several points that supervised classification can benefit the neuroscience community by creating a common definition of social behaviors would be interoperable between labs, that could, for example &quot;facilitate comparison of behavioral results between labs, fostering large‐scale screening of social behaviors within a common analysis platform.&quot; This paper would be stronger if the authors could spell out a formula for finding consensus between annotators; given these principles, perhaps MARS could be trained to reflect this consensus. Perhaps trained MARS models could contain one or more tuning parameters, so that every annotator could be captured by some value(s) of the parameters, thus providing a unified framework while accommodating individual variation in annotation habits?</p></disp-quote><p>We agree with the reviewers that finding consensus between annotators or labs is an important goal. We have pursued multiple projects over the past year investigating methods for capturing and explaining inter-annotator variability, the most successful of which was presented at the CV4Animals Workshop at CVPR this year (1). Briefly, we approached this problem as one of “interpretable machine learning”, which is an active area of research (2). Methods in interpretable machine learning fall into two categories: post-hoc interpretation of black-box models, and methods to train inherently interpretable models. Our workshop paper falls into the latter category of creating interpretable models that allow users to visualize differences in classifiers trained to match different individuals’ annotation styles. (We have also worked on the approach reviewers suggest of creating a low-dimensional representation of “annotation style” space, but thus far this has not yielded strong results.) We have included a copy of this manuscript in our resubmission.</p><p>But while we are working on the problems the reviewers raise, this is an active area of research and we have not yet arrived at a fully satisfying solution to the problem of reconciling annotator differences. What we have done is (1) provided an easy way to compare pose features and multiple annotations of the same video side-by-side within BENTO (now mentioned in the manuscript, line 386-387), (2) created a BENTO module to visualize models created with our “interpretable” behavior classifiers (this is a part of the workshop paper), and (3) released our 10video x 10min x 8annotator dataset to fuel further research into the area of characterizing inter-annotator differences.</p><p>(1) Tjandrasuwita, M., Sun, J. J., Kennedy, A., Chaudhuri, S., and Yue, Y. (2021). Interpreting Expert Annotation Differences in Animal Behavior. arXiv preprint arXiv:2106.06114.</p><p>(2) See, eg: Zachary C Lipton (2018) The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery; Cynthia Rudin (2019) Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead; Yin Lou et al (2012) Intelligible models for classification and regression.</p><disp-quote content-type="editor-comment"><p>3) Multiple datasets are provided, one particularly novel one is the 10 x 10min videos of resident intruder assays, annotated by 8 individuals. It would be spectacular if those videos could be annotated a second time by the same individuals. Mainly, to see if individuals are consistent with themselves (stable style). This would nicely complement the dataset. In the description of the datasets, the relationships are sometimes not clear, e.g. is the person that annotated the large-scale dataset of ~14h also one of the 8 individuals and if yes, then which one?</p></disp-quote><p>We agree with the reviewers that this is an interesting question! Persuading our 8 annotators to manually re-annotate all 10 videos was a tough sell, as this is a huge amount of work. As a compromise, we selected two videos of the original 10 (one male/male and one male/female), and asked annotators to re-annotate those two; we then evaluated within-annotator vs between-annotator agreement on these videos. Despite the long interval between first and second rounds of annotation (approximately 10 months), we found that annotators typically showed high self consistency: self-self agreement was significantly higher than self-other for both attack and investigation annotations. We have included new text (lines 307-314) and a new ED figure (ED Figure 6) summarizing our analysis of this reannotation dataset, and will include the second round of annotations in our dataset release.</p><disp-quote content-type="editor-comment"><p>4) The reviewers wanted to make sure that the 14 hour dataset will be shared (this was not clear from the manuscript). Moreover, when the reviewers attempted to download the dataset it was corrupted (this was replicated by two reviewers and the reviewing editor). In addition, the reviewers wanted to make sure that the data contains all the appropriate meta-data (e.g., annotation images/videos, which genotype the mice have, when the data was recorded, camera type, etc. )</p></disp-quote><p>We apologize for this issue, we fully intend to make the 14 hour dataset (as well as the 30,000 frame pose annotation dataset, and the 10 videos with annotations by 8 lab members) publicly available. We have relocated all our datasets to data.caltech.edu, and updated the download links in the manuscript. We have also now added mouse genotype, recording dates, and camera information for all videos in this dataset; this information is included in an excel spreadsheet accompanying the videos.</p><disp-quote content-type="editor-comment"><p>5) The analyzed datasets and training data were all collected in the same lab, using the same standard setup, under very similar conditions that do not capture the variability expected across the many labs that use this assay. In order to be useful to other users, the conditions at which the method will work must be explicitly discussed. This is especially concerning, as the version of MARS presented in the paper does not allow users to define their own pipelines or to fine-tune the supplied one with new data. Furthermore, the feature list of the behavioral classifiers contains features that are very setup specific (e.g., distance to arena wall, absolute coordinates etc.), or organism-line specific (e.g. mouse size/area). If the authors indeed aim at creating a standardized behavior classification pipeline, which can be compared across labs, it is essential to show/discuss how the method gives consistent results across at least some of the different experimental variants of this assay.</p></disp-quote><p>We agree with the reviewers that this is an important point, and we must stress that no model, including MARS, can be expected to work out of the box in all cases. We have found MARS to be stable across videos taken in different Caltech animal facilities, in different copies of our standardized behavior recording box, across an approximately five-year span of experiments; we expect comparable performance for any lab that records video in a setup akin to that of our previously published hardware.</p><p>When labs do not use this setup, we find that MARS sometimes does not produce satisfying results out of the box. We have therefore expanded our paper to include MARS_Developer, a separate python library for collecting of new pose annotation training data, and for fine-tuning the MARS detection and pose models to novel settings. We have tested this code on videos from several collaborators, and informally have found that MARS’s pose models can be finetuned to work in other top-view videos with varying arena sizes, backgrounds, and resolutions. For the purpose of the paper, we have included a demonstration of fine-tuning MARS to the previously published CRIM13 dataset (lines 364-369).</p><disp-quote content-type="editor-comment"><p>6) Although the paper describes the algorithm well, if was hard for the reviewers to judge the actual usability and quality of the tool without access to it. The software should be made available to the reviewers (with documentation)</p></disp-quote><p>We apologize for this omission. MARS is now publicly available at github.com/neuroethology/MARS<ext-link ext-link-type="uri" xlink:href="http://github.com/neuroethology/MARS">,</ext-link> accompanied by installation instructions, download links for the trained MARS models, sample videos to test performance, and demo scripts. Several research groups have independently confirmed their ability to install MARS and run it on their data- if the reviewers encounter any difficulties, we are happy to assist in troubleshooting via the reviewing editor to ensure reviewer anonymity.</p><p>Similarly, documented code for training new behavior classifiers and fine-tuning the MARS detection and pose models is publicly available at github.com/neuroethology/MARS_Developer<ext-link ext-link-type="uri" xlink:href="http://github.com/neuroethology/MARS_Developer">.</ext-link></p><disp-quote content-type="editor-comment"><p>7) While a direct comparison to other social tracking methods (e.g., maDLC, SLEAP, others) is not necessary here, it is important to have a more comprehensive error analysis of the tracking (while the animals are in the bounding box, minimally).</p></disp-quote><p>We thank the reviewers for prompting this more thorough examination of the MARS pose estimation performance. We used the approach proposed by Ronchi and Perona (2017) to perform a detailed error analysis of MARS pose estimates; we have added mean Average Precision (mAP) and mean Average Recall (mAR) metrics to the text (lines 274-279, Table 1, Methods lines 286-326), as well as a new ED figure (ED Figure 4) in which we investigate pose errors in greater depth.</p><p>Given the large size of the MARS pose annotation dataset (15,000 frames, 5 annotators/frame), we were able to extract reasonable estimates of human annotation accuracy from the data, which we used to estimate the value of σ for the Object Keypoint Similarity metric (see text). We also computed mAP and mAR using a σ of 0.025, allowing direct comparison with the values reported in the SLEAP preprint. Lastly, we have created a Github repository MARS_pycocotools (https://github.com/neuroethology/MARS_pycocotools<ext-link ext-link-type="uri" xlink:href="https://github.com/neuroethology/MARS_pycocotools">)</ext-link>, which is a fork of the widely used COCO API for evaluation of detection and keypoint methods. Our fork allows users to evaluate their own pose models using the mouse pose sigmas estimated from MARS. We hope that this repository will encourage standardization of performance reporting in the animal pose estimation community, just as the COCO API has become a standard for the computer vision community.</p><disp-quote content-type="editor-comment"><p>8) Please ensure full statistical reporting in the main manuscript (e.g., t, F values, degrees of freedom, p value, etc.).</p></disp-quote><p>We thank the reviewers for catching this omission; we have included full statistical reporting in Table 2, and referenced this appropriately from the text.</p></body></sub-article></article>