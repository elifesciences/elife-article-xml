<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">55502</article-id><article-id pub-id-type="doi">10.7554/eLife.55502</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Revealing architectural order with quantitative label-free imaging and deep learning</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-174669"><name><surname>Guo</surname><given-names>Syuan-Ming</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-174670"><name><surname>Yeh</surname><given-names>Li-Hao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2803-5996</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-174671"><name><surname>Folkesson</surname><given-names>Jenny</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4673-0522</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-174672"><name><surname>Ivanov</surname><given-names>Ivan E</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-174673"><name><surname>Krishnan</surname><given-names>Anitha P</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">§</xref></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-174674"><name><surname>Keefe</surname><given-names>Matthew G</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-195126"><name><surname>Hashemi</surname><given-names>Ezzat</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-174675"><name><surname>Shin</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-174676"><name><surname>Chhun</surname><given-names>Bryant B</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-174677"><name><surname>Cho</surname><given-names>Nathan H</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa2">#</xref></contrib><contrib contrib-type="author" id="author-195125"><name><surname>Leonetti</surname><given-names>Manuel D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-195127"><name><surname>Han</surname><given-names>May H</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-90080"><name><surname>Nowakowski</surname><given-names>Tomasz J</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-99116"><name><surname>Mehta</surname><given-names>Shalin B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2542-3582</contrib-id><email>shalin.mehta@czbiohub.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Chan Zuckerberg Biohub</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Anatomy, University of California, San Francisco</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Neurology, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Forstmann</surname><given-names>Birte</given-names></name><role>Reviewing Editor</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Malhotra</surname><given-names>Vivek</given-names></name><role>Senior Editor</role><aff><institution>The Barcelona Institute of Science and Technology</institution><country>Spain</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>§</label><p>Genentech, San Francisco, United States</p></fn><fn fn-type="present-address" id="pa2"><label>#</label><p>University of California, San Francisco, San Francisco, United States</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>27</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e55502</elocation-id><history><date date-type="received" iso-8601-date="2020-01-27"><day>27</day><month>01</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-07-24"><day>24</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Guo et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Guo et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-55502-v2.pdf"/><abstract><p>We report quantitative label-free imaging with phase and polarization (QLIPP) for simultaneous measurement of density, anisotropy, and orientation of structures in unlabeled live cells and tissue slices. We combine QLIPP with deep neural networks to predict fluorescence images of diverse cell and tissue structures. QLIPP images reveal anatomical regions and axon tract orientation in prenatal human brain tissue sections that are not visible using brightfield imaging. We report a variant of U-Net architecture, multi-channel 2.5D U-Net, for computationally efficient prediction of fluorescence images in three dimensions and over large fields of view. Further, we develop data normalization methods for accurate prediction of myelin distribution over large brain regions. We show that experimental defects in labeling the human tissue can be rescued with quantitative label-free imaging and neural network model. We anticipate that the proposed method will enable new studies of architectural order at spatial scales ranging from organelles to tissue.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Microscopy is central to biological research and has enabled scientist to study the structure and dynamics of cells and their components within. Often, fluorescent dyes or trackers are used that can be detected under the microscope. However, this procedure can sometimes interfere with the biological processes being studied.</p><p>Now, Guo, Yeh, Folkesson et al. have developed a new approach to examine structures within tissues and cells without the need for a fluorescent label. The technique, called QLIPP, uses the phase and polarization of the light passing through the sample to get information about its makeup.</p><p>A computational model was used to decode the characteristics of the light and to provide information about the density and orientation of molecules in live cells and brain tissue samples of mice and human. This way, Guo et al. were able to reveal details that conventional microscopy would have missed. Then, a type of machine learning, known as ‘deep learning’, was used to translate the density and orientation images into fluorescence images, which enabled the researchers to predict specific structures in human brain tissue sections.</p><p>QLIPP can be added as a module to a microscope and its software is available open source. Guo et al. hope that this approach can be used across many fields of biology, for example, to map the connectivity of nerve cells in the human brain or to identify how cells respond to infection. However, further work in automating other aspects, such as sample preparation and analysis, will be needed to realize the full benefits.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>label-free imaging</kwd><kwd>inverse algorithms</kwd><kwd>deep learning</kwd><kwd>human tissue</kwd><kwd>polarization</kwd><kwd>phase</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Chan Zuckerberg Biohub</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Guo</surname><given-names>Syuan-Ming</given-names></name><name><surname>Yeh</surname><given-names>Li-Hao</given-names></name><name><surname>Folkesson</surname><given-names>Jenny</given-names></name><name><surname>Ivanov</surname><given-names>Ivan E</given-names></name><name><surname>Keefe</surname><given-names>Matthew G</given-names></name><name><surname>Shin</surname><given-names>David</given-names></name><name><surname>Chhun</surname><given-names>Bryant B</given-names></name><name><surname>Cho</surname><given-names>Nathan H</given-names></name><name><surname>Nowakowski</surname><given-names>Tomasz J</given-names></name><name><surname>Mehta</surname><given-names>Shalin B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Advances in quantitative phase and polarized light microscopy, combined with deep learning, reveal the architecture of human brain tissue.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The function of living systems emerges from the interaction of its components over spatial and temporal scales that range many orders of magnitude. Light microscopy is uniquely useful to record dynamic arrangement of molecules within the context of organelles, of organelles within the context of cells, and of cells within the context of tissues. Combination of fluorescence imaging and automated analysis of image content with deep learning (<xref ref-type="bibr" rid="bib42">Moen et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Belthangady and Royer, 2019</xref>; <xref ref-type="bibr" rid="bib74">Van Valen et al., 2016</xref>) has opened new avenues for understanding complex biological processes. However, characterizing the architecture and dynamics with fluorescence remains challenging in many important biological systems. The choice of label can introduce observation bias in the experiment and may perturb the biological process being studied. For example, labeling cytoskeletal polymers often perturbs their native assembly kinetics (<xref ref-type="bibr" rid="bib9">Belin et al., 2014</xref>). Genetic labeling of human tissue and non-model organisms is not straightforward and the labeling efficiency is often low. Labeling with antibodies or dyes can lead to artifacts and requires careful optimization of the labeling protocols. The difficulty of labeling impedes biological discoveries using these systems. By contrast, label-free imaging requires minimal sample preparation as it measures the sample’s intrinsic properties. Lable-free imaging is capable of visualizing many biological structures simultaneously with minimal photo-toxicity and no photo-bleaching, making it particularly suitable for live-cell imaging. Measurements made without label are often more robust since experimental errors associated with the labeling are avoided. Multiplexed imaging with fluorescence and label-free contrasts enables characterization of the dynamics of labeled molecules in the context of organelles or cells. Thus, label-free imaging provides measurements complementary to fluorescence imaging for a broad range of biological studies, from analyzing architecture of archival human tissue to characterizing organelle dynamics in live cells.</p><p>Classical label-free microscopy techniques such as phase contrast (<xref ref-type="bibr" rid="bib83">Zernike, 1955</xref>), differential interference contrast (DIC) (<xref ref-type="bibr" rid="bib46">Nomarski, 1955</xref>), and polarized light microscopy (<xref ref-type="bibr" rid="bib62">Schmidt, 1926</xref>; <xref ref-type="bibr" rid="bib24">Inoue, 1953</xref>) are qualitative. They turn specimen-induced changes in phase (shape of the wavefront) and polarization (the plane of oscillation of the electric field) of light into intensity modulations that are detectable by a camera. These intensity modulations are related to specimens’ properties via complex non-linear transformation, which makes it difficult to interpret. Computational imaging turns the qualitative intensity modulations into quantitative measurements of specimens’ properties with inverse algorithms based on models of image formation. Quantitative phase imaging (<xref ref-type="bibr" rid="bib55">Popescu et al., 2006</xref>; <xref ref-type="bibr" rid="bib75">Waller et al., 2010</xref>; <xref ref-type="bibr" rid="bib72">Tian and Waller, 2015</xref>) measures optical path length, that is, <italic>specimen phase</italic>, which reports density of the dry mass (<xref ref-type="bibr" rid="bib5">Barer, 1952</xref>). Quantitative polarization microscopy in transmission mode reports angular anisotropy of the optical path length, that is, <italic>retardance</italic>, (<xref ref-type="bibr" rid="bib24">Inoue, 1953</xref>; <xref ref-type="bibr" rid="bib51">Oldenbourg and Mei, 1995</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2013</xref>) and axis of anisotropy, that is, <italic>orientation</italic>, without label.</p><p>Quantitative label-free imaging measures intrinsic properties of the specimen and provides insights into biological processes that may not be obtained with fluorescence imaging. For example, Quantitative phase microscopy (<xref ref-type="bibr" rid="bib53">Park et al., 2018</xref>) has been used to analyze membrane mechanics, density of organelles (<xref ref-type="bibr" rid="bib23">Imai et al., 2017</xref>), cell migration, and recently fast propagation of action potential (<xref ref-type="bibr" rid="bib35">Ling et al., 2019</xref>). Similarly, quantitative polarization microscopy has enabled discovery of the dynamic microtubule spindle (<xref ref-type="bibr" rid="bib24">Inoue, 1953</xref>; <xref ref-type="bibr" rid="bib28">Keefe et al., 2003</xref>), analysis of retrograde flow of F-actin network (<xref ref-type="bibr" rid="bib49">Oldenbourg et al., 2000</xref>), imaging of white matter in adult human brain tissue slices (<xref ref-type="bibr" rid="bib1">Axer et al., 2011a</xref>; <xref ref-type="bibr" rid="bib2">Axer et al., 2011b</xref>; <xref ref-type="bibr" rid="bib39">Menzel et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Mollink et al., 2017</xref>; <xref ref-type="bibr" rid="bib81">Zeineh et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Henssen et al., 2019</xref>), and imaging of activity-dependent structural changes in brain tissue (<xref ref-type="bibr" rid="bib32">Koike-Tani et al., 2019</xref>). Given the complementary information provided by specimen density and anisotropy, a joint imaging of phase and retardance has also been attempted (<xref ref-type="bibr" rid="bib65">Shribak et al., 2008</xref>; <xref ref-type="bibr" rid="bib19">Ferrand et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Baroni et al., 2020</xref>). However, current methods for joint imaging of density and anisotropy are limited in throughput due to complexity of acquisition or can only be used for 2D imaging due to the lack of accurate 3D image formation models. We sought to develop a computational imaging method for joint measurements of phase and retardance of live 3D specimens with simpler light path and higher throughput.</p><p>In comparison to fluorescence measurements that provide molecular specificity, label-free measurements provide physical specificity. Obtaining biological insights from label-free images often requires identifying specific molecular structures. Recently, deep learning has enabled translation of qualitative and quantitative phase images into fluorescence images (<xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Christiansen et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Rivenson et al., 2018a</xref>; <xref ref-type="bibr" rid="bib59">Rivenson et al., 2019</xref>; <xref ref-type="bibr" rid="bib33">Lee et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Petersen et al., 2017</xref>). Among different neural network architecture, U-Net has been widely applied to image segmentation and translation tasks (<xref ref-type="bibr" rid="bib60">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Milletari et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Lee et al., 2019</xref>). U-Net’s success arises primarily from its ability to exploit image features at multiple spatial scales, and its use of skip connections between the encoding and decoding blocks. The skip connections give decoding blocks access to low-complexity, high-resolution features in the encoding blocks. In image translation, images from different modalities (label-free vs. fluorescence in our case) of the same specimen are presented to the neural network model. The neural network model learns the complex transformation from label-free to fluorescence images through the training process. The trained neural network model can predict fluorescence images from label-free images to enable analysis of distribution of a specific molecule. The accuracy with which the molecular structure can be predicted depends not just on the model, but also on the dynamic range and the consistency of the contrast with which the structure is seen in the label-free data. Some of the anisotropic structures are not visible in phase imaging data and therefore cannot be learned from phase imaging data. Reported methods of image translation have not utilized optical anisotropy, which reports important structures such as cell membrane and axon bundles. Furthermore, previous work has mostly demonstrated prediction of single 2D fields of view. Volumetric prediction using 3D U-Net has been reported, but it is computationally expensive, such that downsampling the data at the expense of spatial resolution is required (<xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>). We sought to improve the accuracy of prediction of fluorescence images by using information contained in complementary measurements of density and anisotropy.</p><p>In this work, we report a combination of quantitative label-free imaging and deep learning models to identify biological structures from their density and anisotropy. First, we introduce quantitative label-free imaging with phase and polarization (QLIPP) that visualizes diverse structures by their phase, retardance, and orientation. QLIPP combines quantitative polarization microscopy (<xref ref-type="bibr" rid="bib51">Oldenbourg and Mei, 1995</xref>; <xref ref-type="bibr" rid="bib66">Shribak and Oldenbourg, 2003</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2013</xref>) with the concept of phase from defocus (<xref ref-type="bibr" rid="bib70">Streibl, 1984</xref>; <xref ref-type="bibr" rid="bib75">Waller et al., 2010</xref>; <xref ref-type="bibr" rid="bib71">Streibl, 1985</xref>; <xref ref-type="bibr" rid="bib45">Noda et al., 1990</xref>; <xref ref-type="bibr" rid="bib15">Claus et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Jenkins and Gaylord, 2015a</xref>; <xref ref-type="bibr" rid="bib27">Jenkins and Gaylord, 2015b</xref>; <xref ref-type="bibr" rid="bib68">Soto et al., 2017</xref>), to establish a novel method for volumetric measurement of phase, retardance, and orientation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Data generated with QLIPP can distinguish biological structures at multiple spatial and temporal scales, making it valuable for revealing the architecture of the postmortem archival tissue and organelle dynamics in live cells. QLIPP’s optical path is simpler relative to earlier methods (<xref ref-type="bibr" rid="bib65">Shribak et al., 2008</xref>), reconstruction algorithms are more accurate, and reconstruction software is open-source. QLIPP can be implemented on existing microscopes as a module and can be easily multiplexed with fluorescence. To translate 3D distribution of phase, retardance, and orientation to fluorescence intensities, we implement a computationally efficient multi-channel 2.5D U-Net architecture (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) based on a previously reported single-channel 2.5D U-Net (<xref ref-type="bibr" rid="bib20">Han, 2017</xref>). We use QLIPP for imaging axon tracts and myelination in archival brain tissue sections at two developmental stages. Label-free measurement of anisotropy allowed us to visualize axon orientations across whole sections. We demonstrate that QLIPP data increases accuracy of prediction of myelination in developing human brain as compared to brighfield data. Finally, we demonstrate robustness of the label-free measurements to experimental variations in labeling, which leads to more consistent prediction of myelination than possible with the experimental staining. Collectively, we propose a novel approach for imaging architectural order across multiple biological systems and analyzing it with a judicious combination of physics-driven and data-driven modeling approaches.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Measurements with QLIPP and analysis of structures with 2.5D U-Net.</title><p>(<bold>A</bold>) Light path of the microscope. Volumes of polarization-resolved images are acquired by illuminating the specimen with light of diverse polarization states. Polarization states are controlled using a liquid-crystal universal polarizer. Isotropic material’s optical path length variations cause changes in the wavefront (i.e., phase) of light that is measurable through defocused intensity stack. Anisotropic material not only changes the wavefront, but also changes the polarization of light depending on the degree of optical anisotropy (retardance) and orientation of anisotropy. Intensity Z-stacks of an example specimen, mouse kidney tissue, under five illumination polarization states (<inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>RCP</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>45</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>90</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>135</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are shown. The intensity variations that encode the reconstructed physical properties of isotropic and anisotropic material are illustrated in the stack <italic>I</italic><sub>135</sub>. These polarization-resolved stacks are used to reconstruct (Materials and methods) the specimen’s retardance, slow-axis orientation, and phase. Slow-axis orientation at given voxel reports the axis in the focal plane along which the material is the densest and is represent by a color according to the half-wheel shown in inset. (<bold>B</bold>) Multi-channel, 2.5D U-Net model is trained to predict fluorescent structures from label-free measurements. In this example 3D distribution of F-actin and nuclei are predicted. During training, pairs of label-free images and fluorescence images are supplied as inputs and targets, respectively, to the U-Net model. The model is optimized by minimizing the difference between the model prediction and the target. During inference, only label-free images are used as input to the trained model to predict fluorescence images.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig1-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>QLIPP provides joint measurement of specimen density and anisotropy</title><p>The light path of QLIPP is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. It is a transmission polarization microscope based on computer controlled liquid crystal universal polarizer (<xref ref-type="bibr" rid="bib51">Oldenbourg and Mei, 1995</xref>; <xref ref-type="bibr" rid="bib66">Shribak and Oldenbourg, 2003</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2013</xref>). QLIPP provides an accurate image formation model and corresponding inverse algorithm for simultaneous reconstruction of specimen phase, retardance, and slow axis orientation.</p><p>In QLIPP, specimens are illuminated with five elliptical polarization states for sensitive detection of specimens’ retardance (<xref ref-type="bibr" rid="bib66">Shribak and Oldenbourg, 2003</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2013</xref>). For each illumination, we collect a Z-stack of intensity to capture specimens’ phase information. Variations in the density of the specimen, for example lower density of nuclei relative to the cytoplasm, cause changes in refractive index and distort the wavefront of the incident light. The wavefront distortions lead to detectable intensity modulations through interference in 3D space as the light propagates along the optical axis. Intensity modulations caused by isopropic density variations (specimen phase) can be captured by acquiring a stack of intensities along the optical (Z) axis (<xref ref-type="bibr" rid="bib70">Streibl, 1984</xref>; <xref ref-type="bibr" rid="bib75">Waller et al., 2010</xref>). Anisotropic variations in the specimens’ density result from alignment of molecules along a preferential axis, for example lipid membrane has higher anisotropy relative to the cytoplasm due to the alignment of lipid molecules. This anisotropic density variation (specimen retardance) induces polarization-dependent phase difference. Specimen retardance is often characterized by the axis along which anisotropic material is the densest (slow-axis) or by the axis perpendicular to it (fast-axis) (<xref ref-type="bibr" rid="bib16">de Campos Vidal et al., 1980</xref>; <xref ref-type="bibr" rid="bib61">Salamon and Tollin, 2001</xref>), and the difference in specimen phase between these two axes. In addition, multiple scattering by the specimen can reduce degree of polarization of light. The specimen retardance, slow-axis orientation, and degree of polarization can be measured by probing the specimen with light in different polarization states. We develop a forward model of transformation using the formalism of partial polarization and phase transfer function to describe the relation between specimen physical properties and detected intensities. We then leverage above forward model to design an inverse algorithm that reconstructs quantitative specimen physical properties in 3D from the detected intensity modulations as illustrated in <xref ref-type="fig" rid="fig1">Figure 1A</xref>.</p><p>First, we utilize Stokes vector representation of partially polarized light (<xref ref-type="bibr" rid="bib11">Born and Wolf, 2013</xref>; <xref ref-type="bibr" rid="bib7">Bass et al., 2009</xref>; <xref ref-type="bibr" rid="bib4">Azzam, 2016</xref>) to model the transformation from specimens’ optical properties to acquired intensities (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>). By inverting this transformation, we reconstruct 3D volumes of retardance, slow-axis orientation, brightfield, and degree of polarization. Proper background correction is crucial for detection of low retardance of the biological structures in the presence of high, non-uniform background resulting from the optics or imaging chamber. We use a two-step background correction method (Materials and methods) to correct the non-uniform background polarization (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). In addition to retardance and slow-axis orientation, our use of Stokes formalism enables reconstruction of brightfield and degree of polarization, in contrast to previous work that reconstructs just retardance and slow-axis orientation (<xref ref-type="bibr" rid="bib66">Shribak and Oldenbourg, 2003</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2013</xref>). The degree of polarization measures the fitness of our model with the experiment as explained later and the brightfield images enables reconstruction of specimen phase.</p><p>Second, we utilize phase transfer function formalism (<xref ref-type="bibr" rid="bib71">Streibl, 1985</xref>; <xref ref-type="bibr" rid="bib45">Noda et al., 1990</xref>; <xref ref-type="bibr" rid="bib15">Claus et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Jenkins and Gaylord, 2015a</xref>; <xref ref-type="bibr" rid="bib27">Jenkins and Gaylord, 2015b</xref>; <xref ref-type="bibr" rid="bib68">Soto et al., 2017</xref>) to model how 3D phase information is transformed into brightfield contrast (<xref ref-type="disp-formula" rid="equ12">Equation 17</xref>). Specimen phase information is encoded in the brightfield images but in a complex fashion. In brightfield images, optically dense structures appear in brighter contrast than the background on one side of the focus, almost no contrast at the focus, and darker contrast than the background on the other side of the focus. This is illustrated by 3D brightfield images of nucleoli, the dense sub-nuclear domains inside nuclei (<xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>). We invert our forward model to estimate specimen phase from 3D brightfield stack (<xref ref-type="disp-formula" rid="equ14">Equation 19</xref>). Phase reconstruction from the brightfield volume shows nucleoli in positive contrast relative to background consistently as the nucleoli move through the focus (<xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>). We note that the two-step background correction is essential for background-free retardance and orientation images, but not for phase image (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><p>We illustrate wide applicability of QLIPP with images of human bone osteosarcoma epithelial (U2OS) cells, tissue section from adult mouse braintissue section from In the dividing U2OS cell (<xref ref-type="video" rid="fig2video2">Figure 2—video 2</xref>, <xref ref-type="video" rid="fig2video3">Figure 2—video 3</xref>), the phase image shows three-dimensional dynamics of dense cellular organelles, such as lipid vesicles, nucleoli, and chromosomes.The retardance and slow-axis orientation in U2OS cells (<xref ref-type="video" rid="fig2video2">Figure 2—video 2</xref>, <xref ref-type="video" rid="fig2video3">Figure 2—video 3</xref>) show dynamics of membrane boundaries, spindle, and lipid droplets. We note that the two-step background correction is essential to remove biases in the retardance and orientation images, but not for phase image (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). <xref ref-type="video" rid="fig2video3">Figure 2—video 3</xref> shows that specific organelles can be discerned simply by color-coding the measured phase and retardance, illustrating that quantitative label-free imaging provides specificity to physical properties.</p><p>At larger spatial scale, the phase image identifies cell bodies and axon tracts in mouse and developing human brain tissue sections because of variations in their density. These density variations are more visible and interpretable in phase image as compared to the brightfield image (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Axon tracts appear with noticeably high contrast in retardance and orientation images of mouse and human brain slices (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The high retardance of the axons arises primarily from myelin sheath that has higher density perpendicular to the axon axis (<xref ref-type="bibr" rid="bib16">de Campos Vidal et al., 1980</xref>; <xref ref-type="bibr" rid="bib38">Menzel et al., 2015</xref>). Therefore, the slow axis of the axon tracts is perpendicular to the orientation of the tracts. . <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> and Figure 5 show stitched retardance and orientation images of a whole mouse brain slice, in which not only the white matter tracts, but also orientation of axons in cortical regions is visible. Note that the fine wavy structure in the right hemisphere of the slice is caused by sample preparation artifacts (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Complementary measurements of phase, retardance, and slow-axis orientation distinguish biological structures.</title><p>Brightfield (<italic>BF</italic>), phase (Φ), retardance (ρ), and slow-axis orientation (ω) images of U2OS cells, human brain tissue, and adult mouse brain tissue are shown. In orientation images, slow axis and retardance of the specimen are represented by color (hue) and brightness, respectively. In U2OS cells, chromatin, lipid droplets, membranous organelles, and cell boundaries are visible in phase image due to variations in density, while microtubule spindle, lipid droplets, and cell boundaries are visible due to their anisotropy. In the adult mouse brain slices, axon tracts are more visible in phase, retardance, and orientation images compared to brightfield images, with slow axis perpendicular to the direction of the bundles (cc: corpus callosum, CP: caudoputamen, CTX: cortex). Similar label-free contrast variations are observed in developing human brain tissue slice, but with less ordered tracts compared to the adult mouse brain due to the early age of the brain. The 3D stack of live U2OS cell was acquired with 63 × 1.47 NA oil objective and 0.9 NA illumination, whereas images of mouse and human brain tissue were acquired with 10 × 0.3 NA air objective and 0.2 NA of illumination.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Degree of polarization (<italic>DOP</italic>) images for three specimens shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>When the specimen does not exhibit multiple scattering or diattenuation, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>DOP</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The <italic>DOP</italic> decreases when the specimen exhibits multiple scattering and increases when the specimen exhibits diattenutation, that is, polarization-dependent absorption. The anatomical labels for the mouse brain slice are, cc: corpus callosum, CP: caudoputamen, CTX: cortex.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Effect of background correction methods on reconstructed retardance and phase of the U2OS cell.</title><p>When the specimen has intrinsically low retardance, background correction methods have a large impact on the reconstructed retardance and slow axis orientation. However, the background correction has no significant impact on phase reconstruction. (Left column) Reconstructions without background correction. (Middle column) Background-corrected reconstruction using an experimental images of empty region next to the cells (Right column) Background-corrected reconstruction using images estimated by fitting a very smooth surface to specimen image.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Comparison of brightfield image (top) with quantitative phase image (bottom) of a mouse brain slice.</title><p>The phase image reports density variations at higher contrast. These images are stitches of 48 fields of view and are substantially downsampled to reduce the size. This mouse brain section is a coronal section at around bregma 0.945 mm and is labeled according to Allen brain reference atlas (level 45) (<xref ref-type="bibr" rid="bib34">Lein et al., 2007</xref>). ACB: nucleus accumbens, aco: anterior commissure, olfactory limb, cc: corpus callosum, cing: cingulum bundle, CTX: cortex, CP: caudoputamen, LSr: lateral septal nucleus, rostral part, MOp: primary motor cortex, MOs: secondary motor cortex, MS: medial septal nucleus, SSp: primary somatosensory area, SSs: supplemental somatosensory area, VL: lateral ventricle.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Retardance (top) and orientation (bottom) measurements of a mouse brain slice, which report structural anisotropy and axon orientation (in physical line orientation), respectively.</title><p>The color of the orientation line reports the slow axis orientation of the pixel. We needed to compress the measured dynamic range of retardance by using gamma correction (0.5) to visualize less anisotropic gray matter in the presence of highly anisotropic white matter. These images are stitches of 48 fields of view and are substantially downsampled to reduce size. The peak retardance is 50 nm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig2-figsupp4-v2.tif"/></fig><media id="fig2video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-55502-fig2-video1.mp4"><label>Figure 2—video 1.</label><caption><title>Z-stacks of brightfield and phase images of U2OS cells.</title><p>The same field of view is shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption></media><media id="fig2video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-55502-fig2-video2.mp4"><label>Figure 2—video 2.</label><caption><title>Time-lapse of phase, retardance, and slow axis orientation in a dividing U2OS cell shows differences in density and anisotropy of organelles.</title><p>The same field of view is shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption></media><media id="fig2video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-55502-fig2-video3.mp4"><label>Figure 2—video 3.</label><caption><title>3D rendering of the time-lapse showing diverse structures color coded by their retardance and phase in U2OS cells shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>Phase values from low to high are color-coded with magenta, cyan, and green, respectively. Retardance values are color-coded with yellow. With this pseudo-color scheme, cytoplasm appears in magenta, chromatin appears in magenta and changes to blue as it condenses to chromosomes, nucleoli appear in blue, lipid vesicles appear in green surrounded by yellow rings, and spindles appear in yellow .</p></caption></media></fig-group><p>We show degree of polarization measurements in (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The difference between retardance and degree of polarization is that retardance measures single scattering events within the specimen that alter the polarization of the light, but do not reduce the degree of polarization. On the other hand, low degree of polarization indicates multiple scattering events that reduce the polarization of light and thus mismatch of the specimen optical properties from the model assumptions. In the future, we plan to pursue models that account for diffraction and scattering effects in polarized light microscopy that would enable more precise retrieval of specimen properties.</p><p>Data reported above illustrate simultaneous and quantitative measurements of density, structural anisotropy, and orientation in 3D biological specimens, for the first time to our knowledge. The Python software for QLIPP reconstruction is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mehta-lab/reconstruct-order">https://github.com/mehta-lab/reconstruct-order</ext-link>. In the next sections, we discuss how these complementary label-free measurements enable prediction of fluorescence images and analysis of architecture.</p></sec><sec id="s2-2"><title>2.5D U-Net allows efficient prediction of fluorescent structures from multi-channel label-free images</title><p>In contrast to fluorescence imaging, label-free measurement of density and anisotropy visualize several structures simultaneously but individual structures can be difficult to identify. Label-free measurements are affected by the expression of specific molecules, but do not report the expression directly. To obtain images of specific molecular structures from QLIPP data, we optimized convolutional neural network models to translate 3D label-free stacks into 3D fluorescence stacks.</p><p>Proper prediction of of fluorescent structures with deep learning requires joint optimization of image content, architecture of the neural network, and the training process. The optimization led us to a residual 2.5D U-Net that translates a small stack (5–7 slices) of label-free channels to the central slice of fluorescent channel throughout 3D volume. We use images of the mouse kidney tissue section as a test dataset for optimizing the model architecture and training strategies. We chose the mouse kidney tissue section because it has both anisotropic and isotropic structures (F-actin and nuclei). Additionally, both structures are robustly labeled with no noticeable artifacts. Later we demonstrate predicting the fluorescent labels in specimen where labeling is not robust (Figure 6).</p></sec><sec id="s2-3"><title>Optimization of 2.5D model for prediction of fluorescence images</title><p>Our work builds upon earlier work (<xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>) on predicting fluorescence stacks from brightfield stacks using 3D U-Net. <xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref> showed fluorescence predicted by 3D U-Net is superior than 2D U-Net. However, applying 3D U-Net to microscopy images poses a few limitations. Typical microscopy stacks are bigger in their extent in the focal plane (∼2000 × 2000 pixels) and smaller in extent along the optical axis (usually &lt;40 Z slices). Since the input is isotropically downsampled in the encoding path of the 3D U-Net, it requires sufficiently large number of Z slices to propagate the data through encoding and decoding blocks. As an example, for a minimum of 3 layers in U-Net and 16 pixels at the end of the encoder path, one will need at least 64 Z slices (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Therefore, the use of 3D translation models often requires upsampling of the data in Z, which increases data size and makes training 3D translation model computationally expensive.</p><p>To reduce the computational cost without losing accuracy of prediction, we evaluated the prediction accuracy as a function of model dimensions for a highly ordered, anisotropic structure (F-actin) and for less ordered, isotropic structure (nuclei) in mouse kidney tissue. In mouse kidney tissue, the retardance image highlights capillaries within glomeruli, and brush borders in convoluted tubules, among other components of the tissue. The nuclei appear in darker contrast in the retardance image, because of the isotropic architecture of chromatin. We evaluated three model architectures to predict fluorescence volumes: slice→slice (2D in short) models that predict 2D fluorescence slices from corresponding 2D label-free slices, stack→slice (2.5D in short) models that predict the central 2D fluorescence slice from a stack of adjacent label-free slices, and stack→stack (3D in short) models that predict 3D fluorescent stacks from label-free stacks. For 2.5D models, 3D translation is achieved by predicting one 2D fluorescence plane per stack (z = 3, 5, 7) of label-free inputs. We added a residual connection between the input and output of each block to speed up model training (<xref ref-type="bibr" rid="bib41">Milletari et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Drozdzal et al., 2016</xref>).</p><p>In order to fit 3D models on the GPU, we needed to predict overlapping sub-stacks, which were stitched together to get the whole 3D stack ( see Materials and methods and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the description of the network architecture and training process). We used Pearson correlation coefficient and structural similarity index (SSIM) (<xref ref-type="bibr" rid="bib78">Wang and Bovik, 2009</xref>) between predicted fluorescent stacks and target fluorescent stacks to evaluate the performance of the models (Materials and methods). We report these metrics on the test set (<xref ref-type="table" rid="table1">Table 1</xref>, <xref ref-type="table" rid="table2">Table 2</xref>, <xref ref-type="table" rid="table3">Table 3</xref>), which was not used during the training.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Accuracy of 3D prediction of F-actin from retardance stack using different neural networks.</title><p>Above table lists median values of the Pearson correlation (<italic>r</italic>) and structural similarity index (SSIM) between prediction and target F-actin volumes. We report accuracy metrics for Slice→Slice (2D) ,Stack→Slice (2.5D), and Stack→Stack (3D) models trained to predict F-actin from retardance using Mean Absolute Error (MAE or L1) loss. We segmented target images with a Rosin threshold to discard tiles that mostly contained background pixels. To dissect the differences in prediction accuracy along and perpendicular to the focal plane, we computed (Materials and methods) test metrics separately over XY slices (<italic>r</italic><sub><italic>xy</italic></sub>, SSIM<sub><italic>xy</italic></sub>) and XZ slices (<italic>r</italic><sub><italic>xy</italic></sub>, SSIM<sub><italic>xz</italic></sub>) of the test volumes, as well as over entire test volumes (r<sub><italic>xyz</italic></sub>, SSIM<sub><italic>xyz</italic></sub>). Best performing model according to each metric is displayed in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Translation model</th><th>Input(s)</th><th>r<sub><italic>xy</italic></sub></th><th>r<sub><italic>xz</italic></sub></th><th>r<sub><italic>xyz</italic></sub></th><th>SSIM<sub><italic>xy</italic></sub></th><th>SSIM<sub><italic>xz</italic></sub></th><th>SSIM<sub><italic>xyz</italic></sub></th></tr></thead><tbody><tr><td>Slice→Slice (2D)</td><td>ρ</td><td>0.82</td><td>0.79</td><td>0.83</td><td>0.78</td><td>0.71</td><td>0.78</td></tr><tr><td>Stack→Slice (2.5D, <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>ρ</td><td>0.85</td><td>0.83</td><td>0.86</td><td>0.80</td><td>0.75</td><td>0.81</td></tr><tr><td>Stack→Slice (2.5D, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>ρ</td><td>0.86</td><td>0.84</td><td><bold>0.87</bold></td><td>0.81</td><td>0.76</td><td>0.82</td></tr><tr><td>Stack→Slice (2.5D, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>ρ</td><td><bold>0.87</bold></td><td><bold>0.85</bold></td><td><bold>0.87</bold></td><td><bold>0.82</bold></td><td><bold>0.77</bold></td><td>0.83</td></tr><tr><td>Stack→Stack (3D, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>ρ</td><td>0.86</td><td>0.84</td><td>0.86</td><td><bold>0.82</bold></td><td>0.76</td><td><bold>0.85</bold></td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Accuracy of prediction of F-actin in mouse kidney tissue as a function of input channels.</title><p>Median values of the Pearson correlation (<italic>r</italic>) and structural similarity index (SSIM) between predicted and target volumes of F-actin. We evaluated combinations of brightfield (BF), phase (Φ), retardance (ρ), orientation x (ω<sub><italic>x</italic></sub>), and orientation y (ω<sub><italic>y</italic></sub>), as input. Model training conditions and computation of test metrics is described in <xref ref-type="table" rid="table1">Table 1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Translation model</th><th>Input(s)</th><th>r<sub><italic>xy</italic></sub></th><th>r<sub><italic>xz</italic></sub></th><th>r<sub><italic>xyz</italic></sub></th><th>SSIM<sub><italic>xy</italic></sub></th><th>SSIM<sub><italic>xz</italic></sub></th><th>SSIM<sub><italic>xyz</italic></sub></th></tr></thead><tbody><tr><td>Stack→Slice (2.5D, <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>ρ</td><td>0.86</td><td>0.84</td><td>0.87</td><td>0.81</td><td>0.76</td><td>0.82</td></tr><tr><td/><td>BF</td><td>0.86</td><td>0.84</td><td>0.86</td><td>0.82</td><td>0.77</td><td>0.83</td></tr><tr><td/><td>Φ</td><td>0.87</td><td>0.85</td><td>0.88</td><td><bold>0.83</bold></td><td>0.78</td><td>0.84</td></tr><tr><td/><td>Φ, ρ, ω<sub><italic>x</italic></sub>, ω<sub><italic>y</italic></sub></td><td><bold>0.88</bold></td><td><bold>0.87</bold></td><td><bold>0.89</bold></td><td><bold>0.83</bold></td><td><bold>0.80</bold></td><td><bold>0.85</bold></td></tr><tr><td/><td>BF, ρ, ω<sub><italic>x</italic></sub>, ω<sub><italic>y</italic></sub></td><td><bold>0.88</bold></td><td><bold>0.87</bold></td><td><bold>0.89</bold></td><td><bold>0.83</bold></td><td>0.79</td><td><bold>0.85</bold></td></tr></tbody></table></table-wrap><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Accuracy of prediction of nuclei in mouse kidney tissue.</title><p>Median values of the Pearson correlation (<italic>r</italic>) and structural similarity index (SSIM) between predicted and target volumes of nuclei. See <xref ref-type="table" rid="table2">Table 2</xref> for description.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Translation model</th><th>Input(s)</th><th>r<sub><italic>xy</italic></sub></th><th>r<sub><italic>xz</italic></sub></th><th>r<sub><italic>xyz</italic></sub></th><th>SSIM<sub><italic>xy</italic></sub></th><th>SSIM<sub><italic>xz</italic></sub></th><th>SSIM<sub><italic>xyz</italic></sub></th></tr></thead><tbody><tr><td>Stack→Slice (2.5D, <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>ρ</td><td>0.84</td><td>0.85</td><td>0.85</td><td>0.81</td><td>0.76</td><td>0.82</td></tr><tr><td/><td>BF</td><td>0.87</td><td>0.88</td><td>0.87</td><td>0.82</td><td>0.77</td><td>0.84</td></tr><tr><td/><td>Φ</td><td>0.88</td><td>0.88</td><td>0.88</td><td>0.83</td><td>0.78</td><td>0.85</td></tr><tr><td/><td>Φ, ρ, ω<sub><italic>x</italic></sub>, ω<sub><italic>y</italic></sub></td><td><bold>0.89</bold></td><td>0.89</td><td><bold>0.89</bold></td><td><bold>0.84</bold></td><td><bold>0.80</bold></td><td><bold>0.86</bold></td></tr><tr><td/><td>BF, ρ, ω<sub><italic>x</italic></sub>, ω<sub><italic>y</italic></sub></td><td><bold>0.89</bold></td><td><bold>0.90</bold></td><td><bold>0.89</bold></td><td><bold>0.84</bold></td><td><bold>0.80</bold></td><td><bold>0.86</bold></td></tr></tbody></table></table-wrap><p>The predictions with 2D models show discontinuity artifacts along the depth (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="video" rid="fig3video2">Figure 3—video 2</xref>), as also observed in prior work (<xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>). The 3D model predicts smoother structures along the Z dimension with improved prediction in the XY plane. 2.5D model shows prediction accuracy comparable to 3D model, with higher prediction accuracy as the number of z-slices in the 2.5D model input increases. (<xref ref-type="fig" rid="fig3">Figure 3C and D</xref>; <xref ref-type="table" rid="table1">Table 1</xref>; <xref ref-type="video" rid="fig3video2">Figure 3—video 2</xref>). While 2.5D model shows similar performance to 3D model, we note that we could train the 2.5D model with ∼3× more parameters than 3D model (Materials and methods) in shorter time. In our experiments, training a 3D model with 1.5<italic>M</italic> parameters required 3.2 days, training a 2D model with 2<italic>M</italic> parameters required 6 hr, and training a 2.5D model with 4.8<italic>M</italic> parameters and five input z-slices required 2 days, using ∼100 training volumes. This is because the large memory usage of 3D model significantly limits its training batch size and thus the training speed.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Accuracy of 3D prediction with 2D, 2.5D, and 3D U-Nets.</title><p>Orthogonal sections (XY - top, XZ - bottom, YZ - right) of a glomerulus and its surrounding tissue from the test set are shown depicting (<bold>A</bold>) retardance (input image), (<bold>B</bold>) experimental fluorescence of F-actin stain (target image), and (<bold>C</bold>) Predictions of F-actin (output images) using the retardance image as input with different U-Net architectures. (<bold>D</bold>) Violin plots of structral-similarty metric (SSIM) between images of predicted and target stain in XY and XZ planes. The horizontal dashed lines in the violin plots indicate 25th quartile, median, and 75th quartile of SSIM. The yellow triangle in C highlights a tubule structure, whose prediction can be seen to improve as the model has access to more information along Z. The same field of view is shown in <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>, <xref ref-type="video" rid="fig3video2">Figure 3—video 2</xref>, and <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Schematic illustrating U-Net architectures.</title><p>Schematic of 2D U-Net model used for translating slice→slice and 2.5D U-Net model used for translating stack→slice. The 3D U-Net model used for translating stack→stack is similar to the 2D U-Net, but uses 3D convolutions instead of 2D and is four layers deep instead of 5 layers deep.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig3-figsupp1-v2.tif"/></fig><media id="fig3video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-55502-fig3-video1.mp4"><label>Figure 3—video 1.</label><caption><title>Z-stacks of brightfield, phase, retardance, and orientation images of mouse kidney tissue.</title><p>The same field of view is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, and <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption></media><media id="fig3video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-55502-fig3-video2.mp4"><label>Figure 3—video 2.</label><caption><title>Through focus series showing 3D F-actin distribution in the test field of view shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>We show F-actin distribution (labeled with phalloidin-AF568) acquired on a confocal microscope (target), as well as predicted from 2D, 2.5D, and 3D models trained to translate retardance distribution into fluorescence distributions.</p></caption></media></fig-group><p>The Python code for training our variants of image translation models is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/czbiohub/microDL">https://github.com/czbiohub/microDL</ext-link>.</p></sec><sec id="s2-4"><title>Predicting structures from multiple label-free contrasts improves accuracy</title><p>Considering the trade-off between computation speed and model performance, we adopt 2.5D models with five input Z-slices to explore how combinations of label-free inputs affect the accuracy of prediction of fluorescent structures.</p><p>We found that when multiple label-free measurements are jointly used as inputs, both F-actin and nuclei are predicted with higher fidelity compared to when only a single label-free measurement is used as the input (<xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="table" rid="table3">Table 3</xref>). <xref ref-type="fig" rid="fig4">Figure 4C–D</xref> shows representative structural differences in the predictions of the same glomerulus as <xref ref-type="fig" rid="fig3">Figure 3</xref>. The continuity of prediction along Z-axis improves as more label-free contrasts are used for prediction (<xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>). These results indicate that our model leverages information in complementary physical properties to predict target structures. We note that using complementary label-free contrasts boosts the performance of 2.5D models to exceed the performance of 3D single-channel models without significantly increasing the computation cost (compare <xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="table" rid="table2">Table 2</xref>). Noticeably, fine F-actin bundles have been shown challenging to predict from single label-free input. We found fine F-actin bundles can be predicted from multiple label-free inputs when the model is trained to minimize the difference between the fluorescence target and prediction over only the foreground pixels in the image (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Prediction accuracy improves with multiple label-free contrasts as inputs.</title><p>3D predictions of ordered F-actin and nuclei from different combinations of label-free contrasts using the 2.5D U-Net model. (<bold>A</bold>) Label-free measurements used as inputs for model training: retardance (ρ), phase (Φ), and slow axis orientation (ω). (<bold>B</bold>) The corresponding 3D volume showing the target fluorescent stains. Phalloidin-labeled F-actin in shown green and DAPI labeled nuclei is shown in magenta. (<bold>C</bold>) F-actin and nuclei predicted with single channel models trained on retardance (ρ) and phase (Φ) alone are shown. (<bold>D</bold>) F-actin and nuclei predicted with multi-channel models trained with the combined input of retardance, orientation, and phase. The yellow triangle and white triangle point out structures missing in predicted F-actin and nuclei distributions when only one channel is used as an input, but predicted when all channels are used. (<bold>E</bold>) Violin plots of structral-similarty metric (SSIM) between images of predicted and experimental stain in XY and XZ planes. The horizontal dashed lines indicate 25th quartile, median, and 75th quartile of SSIM. The 3D label-free inputs used for prediction are shown in <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Pearson correlation and SSIM are insensitive to small structural differences in the images.</title><p>(<bold>A</bold>) DAPI labeled nuclei predicted by the model trained with combined input of retardance (ρ), orientation (ω), and brightfield and with brihgtfield alone (<bold>B</bold>) F-actin predicted by the model trained with combined input of retardance (ρ), orientation (ω), and brightfield and with brihgtfield alone. The differences in prediction SSIM and Pearson correlation between the multi-contrast and single contrast models are relatively small (∼0.01) despite the perceivable structural differences shown in the images (indicated by the yellow triangle and white triangle). These structural differences such as missing/spurious nuclei and separate/connected F-actin structures may affect the interpretation of the experimental results depending on the context.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Fine structural features are better predicted with foreground loss.</title><p>F-actin in the mouse kidney slice predicted by the model trained with regular L1 loss (middle) and foreground only L1 loss (right). F-actin was predicted from combined input of retardance (ρ), orientation (ω), and brightfield. Fine F-actin bundles are not visible in prediction with regular L1 loss but visible in prediction with foreground L1 loss.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig4-figsupp2-v2.tif"/></fig><media id="fig4video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-55502-fig4-video1.mp4"><label>Figure 4—video 1.</label><caption><title>Through focus series showing 3D F-actin and nuclei distribution in the test field of view shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>We show overlays of F-actin (phalloidin-AF568) distribution in green and nuclei (DAPI) distribution in magenta DAPI as acquired on confocal (target) and as predicted from models. Predictions were obtained from 2.5D models trained on retardance <inline-formula><mml:math id="inf9"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> alone, phase <inline-formula><mml:math id="inf10"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> alone, and on combination of retardance, orientation, phase <inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p></caption></media></fig-group><p>Interestingly, when only a single contrast is provided as the input, a model trained on phase images has higher prediction accuracy than the model trained on brightfield images. This is possibly because the phase image has consistent, quantitative contrast along z-axis, while the depth-dependent contrast in brightfield images makes the translation task more challenging. This improvement of using phase over brightfield images, however, is not observed when the retardance and orientation are also included as inputs. This is possibly because quantitative retardance and orientation complement the qualitative brightfield input and simplify the translation task.</p><p>In conclusion, above results show that 2.5D multi-contrast models predict 3D structures with superior accuracy than single channel 3D U-Net models, but have multiple practical advantages that facilitate scaling of the approach. In addition, the results show that structures of varying density and order can be learned with higher accuracy when complementary physical properties are combined as inputs.</p></sec><sec id="s2-5"><title>Imaging architecture of mouse and human brain tissue with QLIPP</title><p>Among electron microscopy, light microscopy, and magnetic resonance based imaging of brain architecture, the resolution and throughput of light-microscopy provides the ability to image whole brain slices at single axon resolution in a reasonable time (<xref ref-type="bibr" rid="bib31">Kleinfeld et al., 2011</xref>; <xref ref-type="bibr" rid="bib1">Axer et al., 2011a</xref>; <xref ref-type="bibr" rid="bib2">Axer et al., 2011b</xref>; <xref ref-type="bibr" rid="bib39">Menzel et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Mollink et al., 2017</xref>; <xref ref-type="bibr" rid="bib81">Zeineh et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Henssen et al., 2019</xref>). Light-microscopy is also suitable for imaging biological processes while brain tissue is kept alive (<xref ref-type="bibr" rid="bib48">Ohki et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Koike-Tani et al., 2019</xref>). With quantitative imaging of brain architecture and activity at light resolution, one can envision the possibility of building probabilistic models that relate connectivity and function. QLIPP’s high-resolution, quantitative nature, sensitivity to low anisotropy of gray matter (<xref ref-type="fig" rid="fig2">Figure 2</xref>), and throughput make it attractive for imaging the architecture and activity in brain slices. Here, we explore how QLIPP can be used to visualize the architecture of the sections of adult mouse brain and archival sections of prenatal human brain.</p></sec><sec id="s2-6"><title>Adult mouse brain tissue</title><p>We first imaged an adult mouse brain tissue section located at bregma −1.355 mm (level 68 in Allen brain reference atlas [<xref ref-type="bibr" rid="bib34">Lein et al., 2007</xref>]) with QLIPP and rendered retardance and slow-axis orientation in two ways as shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. The left panel renders the measured retardance in brightness and slow-axis orientation in color, highlighting anatomical features of all sizes. The right panel renders the fast-axis orientation of the mouse brain section (orthogonal to the slow-axis orientation) as colored lines. It has been shown (<xref ref-type="bibr" rid="bib16">de Campos Vidal et al., 1980</xref>; <xref ref-type="bibr" rid="bib38">Menzel et al., 2015</xref>) that when axons are myelinated, the slow axis is perpendicular to the axon axis, while the fast axis is parallel to it. The visualization in the right panel highlights meso-scale axon orientation in the mouse brain tissue with spatial resolution of ∼ 100 μm, that is, each line represents net orientation of the tissue over the area of ∼ 100 μm × 100 μm. The full section rendered with both approaches is shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Analysis of anatomy and axon orientation of an adult mouse brain tissue with QLIPP.</title><p>The retardance and orientation measurements are rendered with two approaches in opposing hemispheres of the mouse brain, respectively. In the left panel, the slow-axis orientation is displayed with color (hue) and the retardance is displayed with brightness as shown by the color legend in bottom-left. In the right panel, the colored lines represent fast axis and the direction of the axon bundles in the brain. The color of the line still represents the slow-axis orientation as shown by color legend in bottom-right. Different cortical layers and anatomical structures are visible through this measurements. This mouse brain section is a coronal section at around bregma −1.355 mm and is labeled according to Allen brain reference atlas (level 68) (<xref ref-type="bibr" rid="bib34">Lein et al., 2007</xref>). cc: corpus callosum, cing: cingulum bundle, CTX: cortex, CP: caudoputamen, fi: fimbria, HPF: hippocampal formation, HY: hypothalamus, int: internal capsule, MOp: primary motor cortex, MOs: secondary motor cortex, opt: optic tract, SSp: primary somatosensory area, SSs: supplemental somatosensory area, TH: thalamus, VL: lateral ventricle.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>The full-size mouse brain images of two rendering approaches shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title><p>In the top panel, the slow-axis orientation is displayed with color (hue) and the retardance is displayed with brightness as shown by the color legend in bottom-left. In the bottom panel, the colored lines represent fast axis and the direction of the axon bundles in the brain. The color of the line still represents the slow-axis orientation as shown by color legend in bottom-right.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig5-figsupp1-v2.tif"/></fig></fig-group><p>By comparing the size and optical measurements in our label-free images against Allen brain reference atlas, we are able to recognize many anatomical landmarks. For example, the corpus callosum (cc) traversing the left and right hemispheres of the brain is a highly anisotropic bundle of axons. The cortex (CTX) is the outermost region of the brain, with axons projecting down towards the corpus callosum and other sub-cortical structures. Within the inner periphery of the corpus callosum, we can identify several more structures such as hippocampus (HPF), lateral ventricle (VL), and caudoputamen (CP). With these evident anatomical landmarks, we are able to reference to Allen brain reference atlas (<xref ref-type="bibr" rid="bib34">Lein et al., 2007</xref>) and label more anatomical areas of the brain such as the sensory (SSp, SSs) and motor (MOp, MOs) cortical areas.</p><p>We also found that six cortical layers are distinguishable in terms of strength of the retardance signal and the orientational pattern. These data are consistent with reports that layer I contains axon bundles parallel to the cortical layer (<xref ref-type="bibr" rid="bib85">Zilles et al., 2016</xref>). Layer VI contains axon bundles that feed to and from the corpus callosum, so the orientation of the axon is not as orthogonal to the cortical layers as the axons in the other layers. The retardance signal arises from the collective anisotropy of myelin sheath wrapping around axons. Layers IV and V contain higher density of cell bodies and correspondingly lower density of the axons, leading to lower signal in retardance.</p></sec><sec id="s2-7"><title>Tissue from developing human brain</title><p>We next imaged brain sections from developing human samples of two different ages, gestational week 24 (GW 24) (<xref ref-type="fig" rid="fig6">Figure 6A–C</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>) and GW20 (<xref ref-type="fig" rid="fig6">Figure 6D–F</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>) which correspond to the earliest stages of oligodendrocyte maturation and early myelination in the cerebral cortex (<xref ref-type="bibr" rid="bib25">Jakovcevski et al., 2009</xref>; <xref ref-type="bibr" rid="bib40">Miller et al., 2012</xref>; <xref ref-type="bibr" rid="bib67">Snaidero and Simons, 2014</xref>). Similar to the observations in the mouse brain section ( <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), the stitched retardance and orientation images show both morphology and orientation of the axon tracts that are not accessible with brightfield or phase imaging, with fast axis orientation parallel to the axon axis. The retardance in subplate is higher than cortical plates at both time points, which is consistent with the reduced myelin density in the cortical plate relative to the white matter. Importantly, with our calibration and background correction procedures (Materials and methods), our imaging approach has the sensitivity to detect axon orientation in the developing cortical plate, despite the lower retardance in developing brain compared to adult brain due to the low myelination in early brain development (<xref ref-type="bibr" rid="bib40">Miller et al., 2012</xref>; <xref ref-type="bibr" rid="bib67">Snaidero and Simons, 2014</xref>). Different cortical layers are visible in the retardance and orientation images at both time points. With this approach, we could identify different anatomical structures in the developing human brain without additional stains by referencing to the developing human brain atlas (<xref ref-type="bibr" rid="bib8">Bayer and Altman, 2003</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>). The individual axon tracts are also visible in phase image while with lower contrast as the phase image measures the density variation but not the axon orientation.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Label-free mapping of axon tracts in developing human brain tissue section.</title><p>(<bold>A</bold>) (top) Stitched image of retardance and slow axis orientation of a gestational week 24 (GW24) brain section from the test set. The slow axis orientation is encoded by color as shown by the legend. (Bottom) Axon orientation indicated by the lines. (<bold>B</bold>) Zoom-ins of retardace + slow axis, axon orientation, and brightfield at brain regions indicated by the yellow and cyan boxes in (<bold>A</bold>). (<bold>C</bold>) Zoom-ins of label-free images at brain regions indicated by the white box in (<bold>B</bold>) (<bold>D–F</bold>) Same as (<bold>A–C</bold>), but for GW20 sample. MZ: marginal zone; CP: cortical plate; SP: subplate; ESS: external sagittal stratum; ISS: internal sagittal stratum; CC: corpus callosum; SVZ: subventricular zone; PcL: paracentral lobule PL: parietal lobe; OL: occipital lobe; TL: temporal lobe. Anatomical regions in (<bold>B</bold>, <bold>D</bold>, and <bold>E</bold>) are identified by referencing to developing human brain atlas (<xref ref-type="bibr" rid="bib8">Bayer and Altman, 2003</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Brightfield and phase images of human brain sections.</title><p>Images from the same field of view shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> for (<bold>A</bold>) GW24 and (<bold>B</bold>) GW20. Scale bars: (<bold>A–C</bold>) <inline-formula><mml:math id="inf12"><mml:mrow><mml:mn>600</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig6-figsupp1-v2.tif"/></fig></fig-group><p>To analyze the variations in the density of the human brain tissue, we reconstructed 2D phase, unlike 3D phase reconstruction for U2OS cells (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and kidney tissue (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The archival tissue was thinner (12 μm thick) than the depth of field (∼16 μm) of the low magnification objective (10X) we used for imaging large areas. <xref ref-type="fig" rid="fig6">Figure 6B,C,E and F</xref> show the retardance, slow-axis orientation, axon orientation, brightfield, and phase images. Major regions such as the subplate and cortical plate can be identified in both samples. While density information represented by brightfield and phase images can identify some of the anatomical structures, axon-specific structures can be better identified with measurements of anisotropy.</p><p>To our knowledge, the above data are the first report of label-free imaging of architecture and axon tract orientation in prenatal brain tissue. The ability to resolve axon orientation in the cortical plate of the developing brain, which exhibits very low retardance, demonstrates the sensitivity and resolution of our approach.</p></sec><sec id="s2-8"><title>Predicting myelination in sections of developing human brain</title><p>Next, we explore how information in the phase and retardance measurements can be used to predict myelination in prenatal human brain. The human brain undergoes rapid myelination during late development as measured with magnetic resonance imaging (MRI) (<xref ref-type="bibr" rid="bib21">Heath et al., 2018</xref>). Interpretation of the myelination from MRI contrast requires establishing its correlation with histological measurements of myelin levels (<xref ref-type="bibr" rid="bib29">Khodanovich et al., 2019</xref>). Robust measurements of myelination in postmortem human brains can provide new insights in myelination of human brain during development and during degeneration. QLIPP data in <xref ref-type="fig" rid="fig6">Figure 6</xref> indicate that label-free measurements are predictive of the level of myelination but relationship among them is complex (<xref ref-type="fig" rid="fig7">Figure 7C and F</xref>). We employed our multi-channel 2D and 2.5D U-Net models to learn the complex transformation from label-free contrasts to myelination. Importantly, we developed a data normalization and training strategy that enables prediction of myelination across large slices and multiple developmental time points. We also found that a properly trained model can rescue inconsistencies in fluorescent labeling of myelin, which is often used as histological groundtruth.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Prediction of myelination in developing human brain from QLIPP data and rescue of inconsistent labeling.</title><p>(<bold>A</bold>) Stitched image of experimental FluoroMyelin stain of the same (GW24) brain section from the test set (top) and FluoroMyelin stain predicted from retardance, slow axis orientation, brightfield by the 2.5D model (bottom). The cyan arrow head indicates large staining artifacts in the experimental FluoroMyelin stain but rescued in model prediction. (<bold>B</bold>) Zoom-ins of experimental and predicted FluoroMyelin stain using different models at brain regions indicated by the yellow box in (<bold>A</bold>) rotated by 90 degrees. From left to right: experimental FluoroMyelin stain; prediction from brightfield using 2D model; prediction from retardance and phase using 2D model; prediction from retardance, phase, and orientation using 2D model; prediction from retardance, brightfield, and orientation using 2.5D model. (<bold>C</bold>) From region shown in (<bold>B</bold>) we show scatter plot and Pearson correlation of target FluoroMyelin intensity v.s. retardace (left), phase (middle), FluoroMyelin intensity predicted from retardance, brightfield, and orientation using 2.5D model (right). Yellow dashed line indicates the function y = x. (<bold>D–F</bold>) Same as (<bold>A–C</bold>), but for GW20 sample. MZ: marginal zone; CP: cortical plate; SP: subplate; ESS: external sagittal stratum; ISS: internal sagittal stratum; CC: corpus callosum; SVZ: subventricular zone; PcL: paracentral lobule PL: parietal lobe; OL: occipital lobe; TL: temporal lobe.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Normalizing training data per dataset yields prediction with correct dynamic range of intensity.</title><p>Predictions of FluoroMyelin in GW20 human brain tissue slice with training data normalized per field of view (left) and across dataset (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Model predicted FluoroMyelin intensity becomes more accurate as more label-free channels are included as input.</title><p>Scatter plot and Pearson correlation of target FluoroMyelin intensity v.s. model predicted FluoroMyelin intensity for region shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. for (<bold>A</bold>) GW24 and (<bold>B</bold>) GW20. We show prediction from brightfield using 2D model (left); prediction from retardance and phase using 2D model (middle); prediction from retardance, phase, and orientation using 2D model (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55502-fig7-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-9"><title>Data pooling for prediction over large sections of prenatal human brain</title><p>In order to train the model, we measured the level of myelination with FluoroMyelin, a lipophilic dye that can stain myelin without permeabilization (<xref ref-type="bibr" rid="bib44">Monsma and Brown, 2012</xref>). We found the detergents used in most permeabilization protocols remove myelin from the tissue and affect our label-free measurements. We trained multi-contrast 2D and 2.5D models with different combinations of label-free input contrasts and FluoroMyelin as the target to predict. To avoid overfitting and build a model that generalizes to different developmental ages and different types of sections of the brain, we pooled imaging datasets from GW20 and GW24 with two different brain sections for each age. The pooled dataset was then split into training, validation, and test set. Similar to the observations in the mouse kidney tissue, the prediction accuracy improves as more label-free contrasts are included in training but with higher accuracy gain compared to the mouse kidney tissue. This is most likely because the additional information provided by adding more label-free channels is more informative for the model to predict the more complex and variable of human brain structures. On the other hand, 2.5D model with all four input channels shows similar performance as 2D model for this dataset due to the relatively large depth of field (∼16 μm) compared to the sample thickness (12 μm thick), so additional Z-slices only provide phase information but no extra structural information along the z dimension. (<xref ref-type="table" rid="table4">Table 4</xref>).</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Accuracy of prediction of FluoroMyelin in human brain tissue slices across two developmental points (GW20 and GW24).</title><p>Median values of the Pearson correlation (<italic>r</italic>) and structural similarity index (SSIM) between predictions of image translation models and target fluorescence. We evaluated combinations of retardance (ρ), orientation x (ω<sub><italic>x</italic></sub>), orientation y (ω<sub><italic>y</italic></sub>), phase (Φ), and brightfield (<italic>BF</italic>) as inputs. These metrics are computed over 15% of the fields of view from two GW20 datasets and two GW24 test datasets. The 2D models take ∼ 4 hours to converge, whereas 2.5D models take ∼ 64 hours to converge.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Translation model</th><th>Input(s)</th><th>r<sub>xy</sub></th><th>SSIM<sub><sup>xy</sup></sub></th></tr></thead><tbody><tr><td>Slice→Slice (2D)</td><td>BF</td><td>0.72</td><td>0.71</td></tr><tr><td/><td>ρ, Φ</td><td>0.82</td><td>0.82</td></tr><tr><td/><td>ρ, ω<sub><italic>x</italic></sub>, ω<sub><italic>y</italic></sub>, Φ</td><td>0.86</td><td><bold>0.85</bold></td></tr><tr><td>Stack→Slice (2.5D, <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td>BF, ρ, ω<sub><italic>x</italic></sub>, ω<sub><italic>y</italic></sub></td><td><bold>0.87</bold></td><td><bold>0.85</bold></td></tr></tbody></table></table-wrap><p>To test the accuracy of prediction over large human brain slices that span multiple fields of view, we predicted FluoroMyelin using label-free images of whole sections from GW24 and GW20 brains that were not used for model training or validation. We ran model inference on each field of view and then stitched the predicted images together to obtain a stitched prediction with 20,000 × 20,000 pixels (<xref ref-type="fig" rid="fig7">Figure 7A and D</xref>). To the best of our knowledge, these are the largest predicted fluorescence image of tissue sections that have been generated. We were able to predict myelination level in sections from both time points with a single model, with increasing accuracy as we included more label-free channels as the input, similar to our observations from the test dataset of the mouse kidney slice (<xref ref-type="table" rid="table4">Table 4</xref> and <xref ref-type="fig" rid="fig7">Figure 7B and E</xref>). The scatter plots of pixel intensities show that model-predicted FluoroMyelin intensities correlate with the target FluoroMyelin stain significantly better than the label-free contrasts alone (<xref ref-type="fig" rid="fig7">Figure 7C</xref> and <xref ref-type="fig" rid="fig7">Figure 7F</xref>). This illustrates the value of predicting fluorescence from label-free contrasts: while the label-free contrasts are predictive of FluoroMyelin stain, the complex relations between them makes estimation of myelin level from label-free contrasts challenging. The neural network can learn the complex transformation from label-free contrasts to FluoroMyelin stain and enables reliable estimation of myelin levels.</p></sec><sec id="s2-10"><title>Data normalization</title><p>In addition to architecture, it is essential to devise proper image normalization for correctly predicting the intensity across different fields of view in large stitched images. We found that per-image normalization commonly applied to image segmentation tasks did not preserve the intensity variation across images and led to artifacts in prediction. The two main issues that need to be accounted for in image translation tasks are: (1) numbers of background pixels vary across images and can bias the normalization parameters if not excluded from normalization (<xref ref-type="bibr" rid="bib80">Yang et al., 2019</xref>), and (2) there are batch variations in the staining and imaging process when pooling multiple datasets together for training. While batch variation is less pronounced in quantitative label-free imaging, it remains quite significant in fluorescence images of stained samples and therefore needs to be corrected. We found that normalizing per-dataset with the median of inter-quartile range of foreground pixel intensities gives the most accurate intensity prediction (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>Notably, the 2D model with phase, retardance, and orientation as the input has correlation and similarity scores close to the best 2.5D model but the training takes just 3.7 hr to converge, while the best 2.5D model takes 64.7 hr to converge (<xref ref-type="table" rid="table4">Table 4</xref>). This is likely because the 2D phase reconstruction captures the density variation encoded in the brightfield Z-stack that is informative for the model to predict axon tracts accurately.</p></sec><sec id="s2-11"><title>Rescue of inconsistent label</title><p>Robust fluorescent labeling usually requires optimization of labeling protocols and precise control of labeling conditions. Sub-optimal staining protocols often lead to staining artifacts and make the samples unusable. Quantitative label-free imaging, on the other hand, provides more robust measurements as it generates contrast in physical units and does not require labeling. Therefore, fluorescence images predicted from quantitative label-free inputs are more robust to experimental variations. For example, we found FluoroMyelin stain intensity faded unevenly over time and formed dark patches in the images (indicated by cyan arrow heads in <xref ref-type="fig" rid="fig7">Figure 7A and D</xref>), possibly due to quenching of FluoroMyelin by the antifade chemical in the mounting media. However, this quenching of dye does not affect the physical properties measured by the label-free channels. Therefore, the model trained on images without artifacts predicted the expected staining pattern even with the failure of experimental stain. This robustness is particularly valuable for precious tissue specimens such as archival prenatal human brain tissue.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have reported QLIPP, a novel computational imaging method for label-free measurement of density and anisotropy from 3D polarization-resolved acquisition. While quantitative fluorescence imaging provides molecular specificity, quantitative label-free imaging provides physical specificity. We show that several organelles can be identified from their density and anisotropy. We also show that multiple regions of mouse brain tissue and archival human brain tissue can be identified without label. We have also reported multi-channel 2.5D U-Net deep learning architecture and training strategies to translate this physical description of the specimen to the molecular description. Next, we discuss how we elected to balance the trade-offs and the future directions of research enabled by innovations reported here.</p><p>We have designed QLIPP to be easy to adopt and multiplex with fluorescence microscopy. Using QLIPP requires a single liquid-crystal polarization modulator and a motorized Z stage. Our open-source Python software is free to use for non-profit research. Shribak (<xref ref-type="bibr" rid="bib65">Shribak et al., 2008</xref>) has reported joint imaging of 2D phase and retardance with orientation-independent differential interference contrast (OI-DIC) and orientation-independent PolScope (OI-POL), which required six polarization modulators and acquisition protocol more complex than QLIPP. Ptychography-based phase retrieval method has been extended with polarization sensitive components for joint imaging of 2D phase and retardance (<xref ref-type="bibr" rid="bib19">Ferrand et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Baroni et al., 2020</xref>), albeit requiring hundreds of images. Our method uses one polarization modulator, compared to six used by OI-DIC, and fewer images (5 × number of Z slices), compared to hundreds in ptychography-based method, for recovery of 3D phase, retardance and orientation. Our measurements also achieve diffraction-limited resolution and provide adequate time resolution for live-cell imaging, as demonstrated by the 3D movie of U2OS cells (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="video" rid="fig2video2">Figure 2—video 2</xref>, <xref ref-type="video" rid="fig2video3">Figure 2—video 3</xref>). We anticipate that the modularity of the optical path and the availability of reconstruction software will facilitate adoption of QLIPP.</p><p>Phase information is inherently present in polarization-resolved acquisition, but can now be reconstructed using forward models and corresponding inverse algorithms reported here. We note that our approach of recovering phase from propagation of light reports the local phase variation rather than the absolute phase. Local phase variation is less sensitive to low spatial frequency or large-scale variations in density as can be seen from phase images n <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. Recovering density at low spatial frequency requires a more elaborate optical path for creating interference with a reference beam and is more difficult to implement than QLIPP (<xref ref-type="bibr" rid="bib30">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Popescu et al., 2006</xref>). Nonetheless, most biological processes can be visualized with the local density variation. Further, our method uses partially coherent illumination, that is, simultaneous illumination from multiple angles, which improves spatial resolution, depth sectioning, and robustness to imperfections in the light path away from the focal plane.</p><p>QLIPP belongs to the class of polarization-resolved imaging in which the specimen is illuminated in transmission. Two other major classes of polarization-sensitive imaging are polarization sensitive optical coherence tomography (PS-OCT) and fluorescence polarization. PS-OCT is a label-free imaging method in which specimen is illuminated in reflection mode. PS-OCT has been used to measure round-trip retardance and diattenuaton of diverse tissues, for example of brain tissue (<xref ref-type="bibr" rid="bib77">Wang et al., 2018</xref>). But, determination of the slow axis in the reflection mode remains challenging due to the fact that light passes through the specimen in two directions. Fluorescence polarization imaging relies on rotationally constrained fluorescent probes (<xref ref-type="bibr" rid="bib17">DeMay et al., 2011</xref>; <xref ref-type="bibr" rid="bib37">Mehta et al., 2016</xref>). Fluorescence polarization measurements report the rotational diffusion and angular distribution of labeled molecules, which differs from QLIPP we have reported here.</p><p>We also note that, similar to other polarization-resolved imaging systems (<xref ref-type="bibr" rid="bib37">Mehta et al., 2016</xref>), our approach reports projection of the anisotropy onto the focal plane rather than 3D anisotropy. Anisotropic structures such as axon bundles, appear isotropic to the imaging system when they are aligned along the optical axis of the imaging path. Methods for imaging 3D anisotropy with various models and systems (<xref ref-type="bibr" rid="bib50">Oldenbourg, 2008</xref>; <xref ref-type="bibr" rid="bib69">Spiesz et al., 2011</xref>; <xref ref-type="bibr" rid="bib3">Axer et al., 2011c</xref>; <xref ref-type="bibr" rid="bib85">Zilles et al., 2016</xref>; <xref ref-type="bibr" rid="bib63">Schmitz et al., 2018a</xref>; <xref ref-type="bibr" rid="bib64">Schmitz et al., 2018b</xref>; <xref ref-type="bibr" rid="bib79">Yang et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Tran and Oldenbourg, 2018</xref>) are now in active development. Recovering 3D anisotropy along with 3D density using forward models that account for diffraction effects in the propagation of polarized light would be an important area of research for the future.</p><p>We demonstrated the potential of QLIPP for sensitive detection of orientation of axon bundles (<xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig6">Figure 6</xref>). Combining these measurements with tractography algorithms can facilitate analysis of mesoscale connectivity. Tractography algorithms developed for diffusion weighted-MRI measurements (<xref ref-type="bibr" rid="bib84">Zhan et al., 2015</xref>) have been adapted to brain images from a lower-resolution polarization microscope (∼60 μm) (<xref ref-type="bibr" rid="bib3">Axer et al., 2011c</xref>). We envision that combining tractography algorithms with anisotropy measured at optical resolution, which reports the orientation of ensemble of axons, will enable development of probabilistic models of connectivity. Although multiple methods for tracing connectivity in the mouse brain at mesoscale (cellular level) have been developed (<xref ref-type="bibr" rid="bib56">Ragan et al., 2012</xref>; <xref ref-type="bibr" rid="bib47">Oh et al., 2014</xref>; <xref ref-type="bibr" rid="bib82">Zeng, 2018</xref>), they have not yet been extended to human brain. The volume of fetal human brain during third trimester <inline-formula><mml:math id="inf14"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>mm</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>mm</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is 3 orders of magnitude larger than the volume of an adult mouse brain (∼1.5 × 10<sup>2</sup> mm<sup>3</sup>). Our data show that label-free measurement of myelination and axon tract orientation is possible with ∼1.5 μm diffraction-limited resolution over the scale of whole fetal human brain sections. Further work in streamlining sample preparation, imaging, data curating, and model training would be required to apply QLIPP to large scale organs.</p><p>Our multi-channel 2.5D deep learning models are designed for efficient analysis of multi-dimensional 3D data. In contrast to earlier work on image translation that demonstrated 2D prediction (<xref ref-type="bibr" rid="bib14">Christiansen et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Rivenson et al., 2018a</xref>; <xref ref-type="bibr" rid="bib58">Rivenson et al., 2018b</xref>), our 2.5D architecture is inspired by <xref ref-type="bibr" rid="bib20">Han, 2017</xref> and provides comparable prediction accuracy at a lower computational cost than using 3D U-Net. Pearson correlation coefficient in 3D for nuclei prediction from brightfield images is 0.87 vs. ∼0.7 reported in <xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>. In comparison to Christiansen et al.’s 2D translation model (<xref ref-type="bibr" rid="bib14">Christiansen et al., 2018</xref>) where the image translation was formulated as a pixel-wise classification task of 8-bit classes, our 2.5D model formulates the image translation as a regression task that allows prediction of much larger dynamic range of gray levels. While training a single model that predicts multiple structures seems appealing, this more complex task requires increasing the model size with the trade-off of longer model training time. Our modeling strategy to train one model to predict only one target allowed us to use significantly smaller models that can fit into the memory of a single GPU for faster training.</p><p>We systematically evaluated how the dimensions and input channels affect the prediction accuracy. Compared to previous work that predict fluorescence images from single label-free contrast (<xref ref-type="bibr" rid="bib52">Ounkomol et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Christiansen et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Rivenson et al., 2018a</xref>; <xref ref-type="bibr" rid="bib58">Rivenson et al., 2018b</xref>), we show that higher prediction accuracy can be achieved by combining multiple label-free contrasts. Additionally, we report the image normalization strategy required to predict large images stitched from smaller fields of view from multi-channel inputs.</p><p>The image quality metrics we use to evaluate the model performance depends on the accuracy of the prediction but also the noise in the target images. A more direct comparison of model performances on the same dataset would be useful in the future. Further, the more flexible 2.5D network allows for application to image data that has only a few Z-slices without up- or down-sampling the data, making it useful for analysis of microscopic images that often has variable number of Z-slices. Even though we focus on image translation in this work, the same 2.5D network can be used for 3D segmentation. 3D segmentation using the 2.5D network bears additional advantages over 3D network, because sparse annotation can be done on a subset of slices sampled from the 3D volume, while 3D network requires all the slices in the input volume to be annotated. The flexibility of sparse annotation allows for better sampling of structural variation in the data with the same effort on manual annotation.</p><p>A common shortfall of machine learning approaches is that they tend not to generalize well. We have shown that our data normalization and training process leads to models of myelination that generalize to two developmental time points. In contrast to reconstruction using physical models, the errors or artifacts in the prediction by machine learning models are highly dependant on the quality of training data and their similarity to the new input data. Therefore, prediction errors made by the machine learning models are difficult to identify in the absence of ground truth. Extending the image translation models such that they predict not just the value, but also provide estimate of the confidence interval of output values, is an important area of research.</p><sec id="s3-1"><title>Conclusion</title><p>In summary, we report reconstruction of specimen density and anisotropy using quantitative label-free imaging with phase and polarization (QLIPP) and prediction of fluorescence distribution from label-free images using deep convolutional neural networks. Our reconstruction algorithms (<ext-link ext-link-type="uri" xlink:href="https://github.com/mehta-lab/reconstruct-order">https://github.com/mehta-lab/reconstruct-order</ext-link>) and computationally efficient U-Net variants (<ext-link ext-link-type="uri" xlink:href="https://github.com/czbiohub/microDL">https://github.com/czbiohub/microDL</ext-link>) facilitate measurement and interpretation of physical properties of the specimens. We reported joint measurement of phase, retardance, and orientation with diffraction-limited spatial resolution in 3D dividing cells and in 2D brain tissue slices. We demonstrated visualization of diverse biological structures: axon tracts and myelination in mouse and human brain slices, and multiple organelles in cells. We demonstrated accurate prediction of fluorescent images from density and anisotropy with multi-contrast 2.5D U-Net model. We demonstrated strategies for accurate prediction myelination in centimeter-scale prenatal human brain tissue slices. We showed that inconsistent labeling of human tissue can be rescued with qualitative label-free imaging and trained models. We anticipate that our approach will enable quantitative label-free analysis of architectural order at multiple spatial and temporal scales, particularly in live cells and clinically-relevant tissues.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type (species) <break/>or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional information</th></tr></thead><tbody><tr><td>biological sample <break/>(<italic>M. musculus</italic>)</td><td>mouse kidney <break/>tissue section</td><td>Thermo-Fisher Scientific</td><td>Cat. # F24630</td><td/></tr><tr><td>biological sample <break/>(<italic>M. musculus</italic>)</td><td>mouse brain <break/>tissue section</td><td>this paper</td><td/><td>mouse line maintained in <break/>M. Han lab, see <break/><italic>Specimen preparation</italic> in <break/>Materials and methods</td></tr><tr><td>biological sample <break/>(<italic>H. sapiens</italic>)</td><td>developing human brain <break/>tissue section</td><td>this paper</td><td/><td>archival tissue stored in <break/>T. Nowakowski lab, see <break/><italic>Specimen preparation</italic> in <break/>Materials and methods</td></tr><tr><td>chemical compound, <break/>drug</td><td>FluoroMyelin</td><td>Thermo-Fisher Scientific</td><td>Cat. # F34652</td><td/></tr><tr><td>software, algorithm</td><td>reconstruction algorithms</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/mehta-lab/reconstruct-order">https://github.com/mehta-lab/reconstruct-order</ext-link></td><td/><td/></tr><tr><td>software, algorithm</td><td>2.5 U-Net</td><td><ext-link ext-link-type="uri" xlink:href="https://github.com/czbiohub/microDL">https://github.com/czbiohub/microDL</ext-link></td><td/><td/></tr><tr><td>software, algorithm</td><td>Micro-Manager 1.4.22</td><td><ext-link ext-link-type="uri" xlink:href="https://micro-manager.org/">https://micro-manager.org/</ext-link></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_016865">SCR_016865</ext-link></td><td/></tr><tr><td>software, algorithm</td><td>OpenPolScope</td><td><ext-link ext-link-type="uri" xlink:href="https://openpolscope.org/">https://openpolscope.org/</ext-link></td><td/><td/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Model of image formation</title><p>We describe dependence of the polarization resolved images on the specimen properties using Stokes vector representation of partially polarized light (<xref ref-type="bibr" rid="bib7">Bass et al., 2009</xref>, Ch.15). This representation allows us to accurately measure the polarization sensitive contrast in the imaging volume. First, we retrieve the coefficients of the specimen’s Mueller matrix that report linear retardance, slow-axis orientation, transmission (brightfield), and degree of polarization. For brevity, we call them ‘Mueller coefficients’ of the specimen in this paper. Mueller coefficients are recovered from the polarization-resolved intensities using the inverse of an instrument matrix that captures how Mueller coefficients are related to acquired intensities. Assuming that the specimen is mostly transparent, more specifically satisfies the first Born approximation (<xref ref-type="bibr" rid="bib11">Born and Wolf, 2013</xref>), we reconstruct specimen phase, retardance, slow axis, and degree of polarization stacks from stacks of Mueller coefficients. The assumption of transparency is generally valid for the structures we are interested in, but does not necessarily hold when the specimen exhibits significant absorption or diattenuation. To ensure that the inverse computation is robust, we need to make judicious decisions about the light path, calibration procedure, and background estimation. A key advantage of Stokes instrument matrix approach is that it easily generalizes to other polarization diverse imaging methods - A polarized light microscope is represented directly by a calibrated instrument matrix.</p><p>For sensitive detection of retardance, it is advantageous to suppress isotropic background by illuminating the specimen with elliptically polarized light of opposite handedness to the analyzer in the detector side (<xref ref-type="bibr" rid="bib66">Shribak and Oldenbourg, 2003</xref>). For experiments reported in this paper, we acquired data by illuminating the specimen sequentially with right-handed circular and elliptical polarized light and analyzed the transmitted left-handed circular polarized light in detection.</p></sec><sec id="s4-2"><title>Forward model: specimen properties → Mueller coefficients</title><p>We assume a weakly scattering specimen modeled by properties of linear retardance ρ, orientation of the slow axis ω, transmission <italic>t</italic>, and depolarization <italic>p</italic>. The Mueller matrix of the specimen can be expressed as a product of two Mueller matrices, <inline-formula><mml:math id="inf15"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, accounting for the effect of transmission and depolarization from the specimen, and <inline-formula><mml:math id="inf16"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>, accounting for the effect of retardance and orientation of the specimen. The expression of <inline-formula><mml:math id="inf17"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> is a standard Mueller matrix of a linear retarder that can be found in <xref ref-type="bibr" rid="bib7">Bass et al., 2009</xref>, Ch.14, and <inline-formula><mml:math id="inf18"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is expressed as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>t</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>With <inline-formula><mml:math id="inf19"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>, the Mueller matrix of the specimen is then given by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>sm</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mo>*</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>*</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mo>*</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>*</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where * signs denote irrelevant entries that cannot be retrieved under our experiment scheme. The relevant entries that are retrievable can be expressed as a vector of Mueller coefficients, which is<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This vector is coincidentally the Stokes vector when right-handed circularly polarized light passing through the specimen. The aim of the measurement we describe in the following paragraphs is to accurately measure these Mueller coefficients at each point in the image plane of the microscope by illuminating the specimen and detecting the scattered light with mutually independent polarization states. Once a map of these Mueller coefficients has been acquired with high accuracy, the specimen properties can be retrieved from the above set of equations.</p></sec><sec id="s4-3"><title>Forward model: Mueller coefficients → intensities</title><p>To acquire the above Mueller coefficients, we illuminate the specimen with a series of right-handed circularly and elliptically polarized light (<xref ref-type="bibr" rid="bib66">Shribak and Oldenbourg, 2003</xref>). The Stokes vectors of our sequential illumination states are given by,<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>135</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf21"><mml:mi>χ</mml:mi></mml:math></inline-formula> is the compensatory retardance controlled by the LC that determines the ellipticity of the four elliptical polarization states.</p><p>After our controlled polarized illumination has passed through the specimen, we detect the left-handed circular polarized light by having a left-handed circular analyzer in front of our sensor. We express the Stokes vector before the sensor as<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>sensor</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>LCA</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>sm</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>RCP</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>45</mml:mn><mml:mo>,</mml:mo><mml:mn>90</mml:mn><mml:mo>,</mml:mo><mml:mn>135</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depending on the illumination states, and <inline-formula><mml:math id="inf23"><mml:msub><mml:mi mathvariant="bold">𝐌</mml:mi><mml:mi>LCA</mml:mi></mml:msub></mml:math></inline-formula> is the Muller matrix of a left-handed circular analyzer (<xref ref-type="bibr" rid="bib7">Bass et al., 2009</xref>, Ch.14). The detected intensity images are the first component of Stokes vector at the sensor under different illuminations (<inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐒</mml:mi><mml:mrow><mml:mi>sensor</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). Stacking the measured intensity images to form a vector<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>I</mml:mi><mml:mi>RCP</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>I</mml:mi><mml:mn>45</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>I</mml:mi><mml:mn>90</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>I</mml:mi><mml:mn>135</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>we can link the relationship between the measured intensity and the specimen vector through an ‘instrument matrix’ <inline-formula><mml:math id="inf25"><mml:mi mathvariant="bold">𝐀</mml:mi></mml:math></inline-formula> as<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐈</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐀𝐦</mml:mi></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Each row of the instrument matrix is determined by the interaction between various illumination polarization states and the specimen’s properties. Any polarization-resolved measurement scheme can be characterized by an instrument matrix that transforms specimen’s polarization property to the measured intensities. Calibration of the polarization imaging system is then done through calibrating this instrument matrix.</p></sec><sec id="s4-4"><title>Computation of Mueller coefficients at image plane</title><p>Once the instrument matrix has been experimentally calibrated, the vector of Mueller coefficients can be obtained from recorded intensities using its inverse (compare <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>),<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐀</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐈</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5"><title>Computation of background corrected specimen properties</title><p>We retrieved the vector of Mueller coefficients, <inline-formula><mml:math id="inf26"><mml:mi mathvariant="bold">𝐦</mml:mi></mml:math></inline-formula>, by solving <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>. Slight strain or misalignment in the optical components or the specimen chamber can lead to background that masks out contrast from the specimen. The background typically varies slowly across the field of view and can introduce spurious correlations in the measurement. It is crucial to correct the vector of Mueller coefficients for non-uniform background retardance that was not accounted for by the calibration process. To correct the non-uniform background retardance, we acquired background polarization images at the empty region of the specimen. We then transformed specimen (<inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>sm</mml:mi></mml:mrow></mml:math></inline-formula>) and background (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>bg</mml:mi></mml:mrow></mml:math></inline-formula>) vectors of Mueller coefficients as follows,<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>m</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>m</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:msubsup><mml:mi>m</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We then reconstructed the background corrected properties of the specimen: brightfield (BF), retardance (ρ), slow axis (ω), and degree of polarization (DOP) from the transformed specimen and background vectors of Mueller coefficients <inline-formula><mml:math id="inf29"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>sm</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>bg</mml:mi></mml:msup></mml:math></inline-formula> using the following equations:<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(11)</mml:mtext></mml:mtd><mml:mtd><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(12)</mml:mtext></mml:mtd><mml:mtd><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(13)</mml:mtext></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(14)</mml:mtext></mml:mtd><mml:mtd><mml:mi>ρ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>arctan</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(15)</mml:mtext></mml:mtd><mml:mtd><mml:mi>ω</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>arctan</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mover><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(16)</mml:mtext></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>When the background cannot be completely removed using the above background correction strategy with a single background measurement, (i.e. the specimen has spatially varying background retardance), we applied a second round of background correction on the measurements. In this second round, we estimated the residual transformed background Mueller coefficients by fitting a low-order 2D polynomial surface to the transformed specimen Mueller coefficients. Specifically, we downsampled each 2048 × 2048 image to 64 × 64 image with 32 × 32 binning. We took the median of each 32 × 32 bin to be each pixel value in the downsampled image. We then fitted a second-order 2D polynomial surface to the downsampled image of each transformed specimen Mueller coefficient to estimate the residual background. With this newly estimated background, we performed another background correction. The effects of two rounds of the background corrections are shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</p></sec><sec id="s4-6"><title>Phase reconstruction</title><p>As seen from <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, the first component in the vector of Mueller coefficients, <italic>m</italic><sub>0</sub>, is equal to the total transmitted intensity of electric field in the focal plane. Assuming a specimen with weak absorption, the intensity variations in a Z-stack encode the phase information via the transport of intensity (TIE) equation (<xref ref-type="bibr" rid="bib70">Streibl, 1984</xref>). In the following, we leverage weak object transfer function (WOTF) formalism (<xref ref-type="bibr" rid="bib71">Streibl, 1985</xref>; <xref ref-type="bibr" rid="bib45">Noda et al., 1990</xref>; <xref ref-type="bibr" rid="bib15">Claus et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Jenkins and Gaylord, 2015a</xref>; <xref ref-type="bibr" rid="bib27">Jenkins and Gaylord, 2015b</xref>; <xref ref-type="bibr" rid="bib68">Soto et al., 2017</xref>) to retrieve 2D and 3D phase from this TIE phase contrast and describe the corresponding inverse algorithm.</p></sec><sec id="s4-7"><title>Forward model for phase reconstruction</title><p>The linear relationship between the 3D phase and the through focus brightfield intensity was established in <xref ref-type="bibr" rid="bib71">Streibl, 1985</xref> with Born approximation and weak object approximation. In our context, we reformulated as (<xref ref-type="bibr" rid="bib71">Streibl, 1985</xref>; <xref ref-type="bibr" rid="bib45">Noda et al., 1990</xref>; <xref ref-type="bibr" rid="bib68">Soto et al., 2017</xref>)<disp-formula id="equ12"><label>(17)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>dc</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mo>⊗</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mo>⊗</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo>⟂</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the 3D spatial coordinate vector, <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>dc</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the constant background of <italic>m</italic><sub>0</sub> component, <inline-formula><mml:math id="inf33"><mml:msub><mml:mo>⊗</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi></mml:msub></mml:math></inline-formula> denotes convolution operation over <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold">𝐫</mml:mi></mml:math></inline-formula> coordinate, Φ refers to phase, μ refers to absorption, <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the phase point spread function (PSF), and <inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the absorption PSF. Strictly, Φ and μ are the real and imaginary part of the scattering potential scaled by <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> is the axial pixel size of the experiment and <italic>k</italic> is the wavenumber of the incident light. When the refractive index of the specimen and that of the environment are close, the real and imaginary scaled scattering potential reduce to two real quantity, phase and absorption.</p><p>When specimen’s thickness is larger than the depth of field of the microscope (usually in experiments with high NA objective), the brightfield intensity stack contains 3D information of specimen’s phase and absorption. Without making more assumptions or taking more data, solving 3D phase and absorption from 3D brightfield is ill-posed because we are solving two unknowns from one measurement. Assuming the absorption of the specimen is negligible (<xref ref-type="bibr" rid="bib45">Noda et al., 1990</xref>; <xref ref-type="bibr" rid="bib27">Jenkins and Gaylord, 2015b</xref>; <xref ref-type="bibr" rid="bib68">Soto et al., 2017</xref>), which generally applies to transparent biological specimens, we turn this problem into a linear deconvolution problem, where 3D phase is retrieved.</p><p>When specimen’s thickness is smaller than the depth of field of the microscope (usually in experiments with low NA objective), the whole 3D intensity stack is coming from merely one effective 2D absorption and phase layer of specimen. We rewrite <xref ref-type="disp-formula" rid="equ12">Equation 17</xref> as (<xref ref-type="bibr" rid="bib15">Claus et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Jenkins and Gaylord, 2015a</xref>)<disp-formula id="equ13"><label>(18)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>+</mml:mo></mml:mtd><mml:mtd><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo>⊗</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo>⊗</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In this situation, we have multiple 2D defocused measurements to solve for one layer of 2D absorption and phase of the specimen.</p></sec><sec id="s4-8"><title>Inverse problem for phase reconstruction</title><p>With the linear relationship between the first component of the Mueller coefficients vector and the phase, we then formulated the inverse problem to retrieve 2D and 3D phase of the specimen.</p><p>When we recognize the specimen as a 3D specimen, we then use <xref ref-type="disp-formula" rid="equ12">Equation 17</xref> and drop the absorption term to estimate specimen’s 3D phase through the following optimization algorithm:<disp-formula id="equ14"><label>(19)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi></mml:munder></mml:mstyle><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mn>0</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mo>⊗</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>Reg</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf39"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mn>0</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>dc</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>τ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> is the regularization parameter for applying different degree of denoising effect, and the regularization term depending on the choice of either Tikhonov or anisotropic total variation (TV) denoiser is expressed<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>When using Tikhonov regularization, this optimization problem has an analytic solution that has previously described by <xref ref-type="bibr" rid="bib45">Noda et al., 1990</xref>; <xref ref-type="bibr" rid="bib27">Jenkins and Gaylord, 2015b</xref>; <xref ref-type="bibr" rid="bib68">Soto et al., 2017</xref>. As for TV regularization, we adopted alternating minimization algorithm that is proposed and applied to phase imaging in <xref ref-type="bibr" rid="bib76">Wang et al., 2008</xref> and <xref ref-type="bibr" rid="bib13">Chen et al., 2018</xref>, respectively, to solve the problem.</p><p>If we consider the specimen as a 2D specimen, we then turn <xref ref-type="disp-formula" rid="equ13">Equation 18</xref> into the following optimization problem:<disp-formula id="equ16"><label>(20)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo>⊗</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo>⊗</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="2em"/><mml:msup><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⊥</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where we have an extra regularization parameter <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>τ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> here for the absorption. When Tikhonov regularization is selected, the analytic solution similar to the one described in <xref ref-type="bibr" rid="bib12">Chen et al., 2016</xref> is adopted.</p><p>When the signal to noise ratio of the brightfield stack is high, Tikhonov regularization gives satisfactory reconstruction in a single step with computation time proportional to the size of the image stack. However, when the noise is high, Tikhonov regularization can lead to high- to medium-frequency artifacts. Using iterative TV denoising algorithm, we can trade-off reconstruction speed with robustness to noise.</p></sec><sec id="s4-9"><title>Specimen preparation</title><p>Mouse kidney tissue slices were purchased (Thermo-Fisher Scientific). In the mouse kidney tissue slice, F-actin was labeled with Alexa Fluor 568 phalloidin and nuclei was labeled with DAPI. U2OS cells were seeded and cultured in a chamber made of two strain-free coverslips that allowed for gas exchange.</p></sec><sec id="s4-10"><title>Mouse brain section</title><p>The mice were anesthetized by inhalation of isoflurane in a chemical fume hood and then perfused with 25 ml phosphate-buffered saline (PBS) into the left cardiac ventricle and subsequently with 25 ml of 4% paraformaldehyde (PFA) in the PBS solution. Thereafter, the brains were post-fixed with 4% PFA for 12–16 hr and then transferred to 30% sucrose solution at the temperature of 4°C for 2–3 days until the tissue sank to the bottom of the container. Then, the brains were embedded in a tissue freezing medium (Tissue-Tek O.C.T compound 4583, Sakura) and kept at the temperature of −80°C. Cryostat-microtome (Leica CM 1850, Huston TX) was used for preparing the tissue sections (12 and 50 µm) at the temperature of −20°C and the slides were stored at the temperature of −20°C until use. In order to analyze myelination with QLIPP, the OCT on the slides were melted by keeping the slides at 37°C for 15–30 min. Then, the slides were washed in PBST (PBS+Tween-20 [0.1%]) for five minutes and then washed in PBS for five minutes and coversliped by mounting media (F4680, FluromountTM aqueousm sigma).</p></sec><sec id="s4-11"><title>Prenatal human brain section</title><p>De-identified brain tissue samples were received with patient consent in accordance with a protocol number approved by the Human Gamete, Embryo, and Stem Cell Research Committee (institutional review board) at the University of California, San Francisco. Human prenatal brain samples were fixed with 4% paraformaldehye in phosphate-buffered solution (PBS) overnight, then rinsed with PBS, dehydrated in 30% sucrose/OCT compound (Agar Scientific) at 4°C overnight, then frozen in OCT at −80°C. Frozen samples were sectioned at 12 μ<italic>m</italic> and mounted on microscope slides. Sections were stained directly with red FluoroMyelin (Thermo-Fisher Scientific, 1:300 in PBS) for 20 min at room temperature, rinsed three times with PBS for 10 min each, then mounted with ProLong Gold antifade (Invitrogen) with a coverslip.</p></sec><sec id="s4-12"><title>Image acquisition and registration</title><p>We implemented LC-PolScope on a Leica DMi8 inverted microscope with Andor Dragonfly confocal for multiplexed acquisition of polarization-resolved images and fluorescence images. We automated the acquisition using Micro-Manager v1.4.22 and OpenPolScope plugin for Micro-Manager that controls liquid crystal universal polarizer (custom device from Meadowlark Optics, specifications available upon request).</p><p>We multiplexed the acquisition of label-free and fluorescence volumes. The volumes were registered using transformation matrices computed from similarly acquired multiplexed volumes of 3D matrix of rings from the ARGO-SIM test target (Argolight).</p><p>In transmitted light microscope, the resolution increases and image contrast decreases with increased numerical aperture of illumination. We used 63 × 1.47 NA oil immersion objective (Leica) and 0.9 NA condenser to achieve a good balance between image contrast and resolution. The mouse kidney tissue slice was imaged using 100 ms exposure for five polarization channels, 200 ms exposure for 405 nm channel (nuclei) at 1.6 mW in the confocal mode, 100 ms exposure for 561 nm channel (F-actin) at 2.8 mW in the confocal mode. The mouse brain slice were imaged using 30 ms exposure for five polarization channels. U2OS cells were imaged using 50 ms exposure for five polarization channels. For training the neural network, we acquired 160 non-overlapping 2048 × 2048 × 45 z-stacks of the mouse kidney tissue slice with Nyquist sampled voxel size 103 nm × 103 nm ×250 nm. Human brain sections were imaged with a 10 × 0.3 NA objective and 0.2 NA condenser with a 200 ms exposure for polarization channels, 250 ms exposure for 568 channel (FluoroMyelin) in the epifluorescence mode. The full brain sections were imaged, approximately 200 images depending on the size of the section, with 5 Z-slices at each location. The registered images mouse kidney tissue slice are available in the BioImage Archive (<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/biostudies/BioImages/studies/S-BIAD25">https://www.ebi.ac.uk/biostudies/BioImages/studies/S-BIAD25</ext-link>).</p></sec><sec id="s4-13"><title>Data preprocessing for model training</title><p>The images were flat-field corrected. For training 3D models, the image volumes were upsampled along Z to match the pixel size in XY using linear interpolation. The images were tiled into 256 × 256 patches with a 50% overlap between patches for 2D and 2.5D models. The volumes were tiled into 128 × 128 × 96 patches for 3D models with a 25% overlap along XYZ. Tiles that had sufficient fluorescence foreground (2D and 2.5D: 20%, 3D: 50%) were used for training. Foreground masks were computed by summing the binary images of nuclei and F-actin obtained from Otsu thresholding in the case of mouse kidney tissue sections, and binary images of FluoroMyelin for the human brain sections. Images of human brain sections were visually inspected and curated to exclude images containing quenching artifacts as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> before training.</p><p>Proper data normalization is essential for predicting the intensity correctly across different fields-of-views. We found the common normalization scheme where each image is normalized by its mean and standard deviation does not produce correct intensity prediction (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). We normalized the images on a per dataset basis to correct the batch variation in the staining and imaging process across different datasets. To balance contributions from different channels during training of multi-contrast models, each channel needs to be scaled to similar range. Specifically, for each channel, we subtracted its median and divided by its inter-quartile range (range defined by 25% and 75% quantiles) of the foreground pixel intensities. We used inter-quartile range to normalize the channel because standard deviation underestimates the spread of the distribution of highly correlated data such as pixels in images.</p></sec><sec id="s4-14"><title>Neural network architecture</title><p>We experimented with 2D, 2.5D and 3D versions of U-Net models <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Across the three U-Net variants, each convolution block in the encoding path consists of two repeats of three layers: a convolution layer, ReLU non-linearity activation, and a batch normalization layer. We added a residual connection from the input of the block to the output of the block to facilitate faster convergence of the model (<xref ref-type="bibr" rid="bib41">Milletari et al., 2016</xref>; <xref ref-type="bibr" rid="bib18">Drozdzal et al., 2016</xref>). 2 × 2 downsampling is applied with 2 × 2 convolution with stride two at the end the each encoding block. On the decoding path, the feature maps were passed through similar convolution blocks, followed by up-sampling using bilinear interpolation. Feature maps output by every level of encoding path were concatenated to feature maps in the decoding path at corresponding levels. The final output block had a convolution layer only.</p><p>The encoding path of our 2D and 2.5D U-Net consists of five layers with 16, 32, 64, 128 and 256 filters respectively, while the 3D U-Net consists of four layers with 16, 32, 64 and 128 filters each due to its higher memory requirement. The 2D and 3D versions use convolution filters of size of 3 × 3 and 3 × 3 × 3 with a stride of 1 for feature extraction and with a stride of 2 for downsampling between convolution blocks.</p><p>The 2.5D U-Net has the similar architecture as the 2D U-Net with following differences:</p><list list-type="order"><list-item><p>The 3D features maps are converted into 2D using skip connections that consist of a N × 1 × 1 valid convolution, where N = 3, 5, 7 is the number of slices in the input.</p></list-item> <list-item><p>Convolution filters in the encoding path are N × 3 × 3.</p></list-item> <list-item><p>In the encoding path, the feature maps are downsampled across blocks using N × 2 × 2 average pooling.</p></list-item> <list-item><p>In the decoding path, the feature maps were upsampled using bilinear interpolation by a factor of 1 × 2 × 2 and the convolution filters in the decoding path are of shape 1 × 3 × 3.</p></list-item></list><p>The 2D , 2.5D, 3D network with single channel input consisted of 2.0 M, 4.8M, 1.5M learnable parameters, respectively.</p></sec><sec id="s4-15"><title>Model training and inference</title><p>We randomly split the images in groups of 70%, 15%, and 15% for training, validation and test. The split are kept consistent across all model training to make the results comparable. All models are trained with Adam optimizer, L1 loss function, and a cyclic learning rate scheduler with a min and max learning rate of 5 × 10<sup>−5</sup> and 6 × 10<sup>−3</sup> respectively. The 2D, 2.5D, 3D network were trained on mini-batches of size 64, 16, and four to accommodate the memory requirements of each model. Models were trained until there was no decrease in validation loss for 20 epochs. The model with minimal validation loss was saved. Single channel 2D models converged in 6 hr, 2.5D model converged in 47 hr and the 3D model converged in 76 hr on NVIDIA Tesla V100 GPU with 32 GB RAM.</p><p>As the models are fully convolutional, model predictions were obtained using full XY images as input for the 2D and 2.5D versions. Due to memory requirements of the 3D model, the test volumes were tiled along x and y while retaining the entire z extent (patch size: 512 × 512 × 96) with an overlap of 32 pixels along X and Y. The predictions were stitched together by linear blending of the model predictions across the overlapping regions. Inference time for a single channel U-Net model was 105, 3 and 18 seconds/frame for 2D, 2.5D, and 3D models respectively, with 2048 × 2048 pixels to a frame.</p></sec><sec id="s4-16"><title>Model evaluation</title><p>Pearson correlation and structural similarity index (SSIM) along the XY, XZ and XYZ dimensions of the test volumes were used for evaluating model performance.</p><p>The Pearson correlation coefficient between a target image <italic>T</italic> and a prediction image <italic>P</italic> is defined as<disp-formula id="equ17"><label>(21)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the covariance of <italic>T</italic> and <italic>P</italic>, and <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>σ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>σ</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> are the standard deviations of <italic>T</italic> and <italic>P</italic> respectively.</p><p>SSIM compares two images using a sliding window approach, with window size <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> for XYZ). Assuming a target window <italic>t</italic> and a prediction window <italic>p</italic>,<disp-formula id="equ18"><label>(22)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mtext>SSIM</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0.03</mml:mn><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, and <italic>L</italic> is the dynamic range of pixel values. Mean and variance are represented by μ and <inline-formula><mml:math id="inf49"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> respectively, and the covariance between <italic>t</italic> and <italic>p</italic> is denoted <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We use <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>. The total SSIM score is the mean score calculated across all windows, SSIM <inline-formula><mml:math id="inf52"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mtext>SSIM</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for a total of <italic>M</italic> windows. For XY and XZ dimensions, we compute one test metric per plane and for XYZ dimension, we compute one test metric per volume.</p><p>Importantly, it is essential to scale the the model prediction back to the original range before normalization for correct calculation of target-prediction SSIM. This is because unlike Pearson correlation coefficient, SSIM is not a scale-independent metrics.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Spyros Dermanis (CZ Biohub) and Bing Wu (CZ Biohub) for providing the mouse brain slice used for acquiring data shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. We thank Greg Huber, Loic Royer, Joshua Batson, Jim Karkanias, Joe DeRisi, and Steve Quake from the Chan Zuckerberg Biohub for numerous discussions. We also thank Eva Dyer from Georgia Tech for discussions about applications of the 2.5D models. This research was supported by the Chan Zuckerberg Biohub.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Data curation, Software, Formal analysis, Validation, Visualization</p></fn><fn fn-type="con" id="con6"><p>Resources, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Resources, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con8"><p>Resources, Methodology</p></fn><fn fn-type="con" id="con9"><p>Resources, Data curation, Software, Validation</p></fn><fn fn-type="con" id="con10"><p>Resources, Investigation</p></fn><fn fn-type="con" id="con11"><p>Resources, Supervision</p></fn><fn fn-type="con" id="con12"><p>Resources, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con13"><p>Conceptualization, Resources, Supervision, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con14"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: De-identified brain tissue samples were received with patient consent in accordance with a protocol approved by the Human Gamete, Embryo, and Stem Cell Research Committee (institutional review board) at the University of California, San Francisco.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-55502-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Our experiments generated imaging data from mouse kidney tissue and human brain tissue slices that are useful for machine learning and other analyses. The data are available in the BioImage Archive (<ext-link ext-link-type="uri" xlink:href="http://www.ebi.ac.uk/bioimage-archive">http://www.ebi.ac.uk/bioimage-archive</ext-link>) under accession number S-BIAD25.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>SM</given-names></name><name><surname>Yeh</surname><given-names>LH</given-names></name><name><surname>Folkesson</surname><given-names>J</given-names></name><name><surname>Ivanov</surname><given-names>IE</given-names></name><name><surname>Krishnan</surname><given-names>AP</given-names></name><name><surname>Keefe</surname><given-names>MG</given-names></name><name><surname>Hashemi</surname><given-names>E</given-names></name><name><surname>Shin</surname><given-names>D</given-names></name><name><surname>Chhun</surname><given-names>B</given-names></name><name><surname>Cho</surname><given-names>N</given-names></name><name><surname>Leonetti</surname><given-names>M</given-names></name><name><surname>Han</surname><given-names>MH</given-names></name><name><surname>Nowakowski</surname><given-names>TJ</given-names></name><name><surname>Mehta</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Revealing architectural order with quantitative label-free imaging and deep learning</data-title><source>BioImage Archive</source><pub-id assigning-authority="EBI" pub-id-type="accession" xlink:href="https://www.ebi.ac.uk/biostudies/BioImages/studies/S-BIAD25?query=S-BIAD25">S-BIAD25</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axer</surname> <given-names>M</given-names></name><name><surname>Grässel</surname> <given-names>D</given-names></name><name><surname>Kleiner</surname> <given-names>M</given-names></name><name><surname>Dammers</surname> <given-names>J</given-names></name><name><surname>Dickscheid</surname> <given-names>T</given-names></name><name><surname>Reckfort</surname> <given-names>J</given-names></name><name><surname>Hütz</surname> <given-names>T</given-names></name><name><surname>Eiben</surname> <given-names>B</given-names></name><name><surname>Pietrzyk</surname> <given-names>U</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>High-resolution fiber tract reconstruction in the human brain by means of three-dimensional polarized light imaging</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00034</pub-id><pub-id pub-id-type="pmid">22232597</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axer</surname> <given-names>H</given-names></name><name><surname>Beck</surname> <given-names>S</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name><name><surname>Schuchardt</surname> <given-names>F</given-names></name><name><surname>Heepe</surname> <given-names>J</given-names></name><name><surname>Flücken</surname> <given-names>A</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name><name><surname>Prescher</surname> <given-names>A</given-names></name><name><surname>Witte</surname> <given-names>OW</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Microstructural analysis of human white matter architecture using polarized light imaging: views from neuroanatomy</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00028</pub-id><pub-id pub-id-type="pmid">22110430</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axer</surname> <given-names>M</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Grässel</surname> <given-names>D</given-names></name><name><surname>Palm</surname> <given-names>C</given-names></name><name><surname>Dammers</surname> <given-names>J</given-names></name><name><surname>Axer</surname> <given-names>H</given-names></name><name><surname>Pietrzyk</surname> <given-names>U</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011c</year><article-title>A novel approach to the human connectome: ultra-high resolution mapping of fiber tracts in the brain</article-title><source>NeuroImage</source><volume>54</volume><fpage>1091</fpage><lpage>1101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.075</pub-id><pub-id pub-id-type="pmid">20832489</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azzam</surname> <given-names>RMA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stokes-vector and Mueller-matrix polarimetry [Invited]</article-title><source>Journal of the Optical Society of America A</source><volume>33</volume><fpage>1396</fpage><lpage>1408</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.33.001396</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barer</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>Interference microscopy and mass determination</article-title><source>Nature</source><volume>169</volume><fpage>366</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1038/169366b0</pub-id><pub-id pub-id-type="pmid">14919571</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baroni</surname> <given-names>A</given-names></name><name><surname>Chamard</surname> <given-names>V</given-names></name><name><surname>Ferrand</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Extending quantitative phase imaging to Polarization-Sensitive materials</article-title><source>Physical Review Applied</source><volume>13</volume><elocation-id>054028</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevApplied.13.054028</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bass</surname> <given-names>M</given-names></name><name><surname>DeCusatis</surname> <given-names>C</given-names></name><name><surname>Enoch</surname> <given-names>JM</given-names></name><name><surname>Lakshminarayanan</surname> <given-names>V</given-names></name><name><surname>Li</surname> <given-names>G</given-names></name><name><surname>MacDonald</surname> <given-names>C</given-names></name><name><surname>Mahajan</surname> <given-names>VN</given-names></name><name><surname>Stryland</surname> <given-names>EV</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Handbook of Optics, Volume I: Geometrical and Physical Optics, Polarized Light, Components and Instruments(Set)</source><publisher-name>McGraw Hill Professional</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bayer</surname> <given-names>SA</given-names></name><name><surname>Altman</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>The Human Brain During the Third Trimester</source><publisher-name>Taylor &amp; Francis</publisher-name></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>BJ</given-names></name><name><surname>Goins</surname> <given-names>LM</given-names></name><name><surname>Mullins</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparative analysis of tools for live cell imaging of actin network architecture</article-title><source>BioArchitecture</source><volume>4</volume><fpage>189</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1080/19490992.2014.1047714</pub-id><pub-id pub-id-type="pmid">26317264</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belthangady</surname> <given-names>C</given-names></name><name><surname>Royer</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Applications, promises, and pitfalls of deep learning for fluorescence image reconstruction</article-title><source>Nature Methods</source><volume>16</volume><fpage>1215</fpage><lpage>1225</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0458-z</pub-id><pub-id pub-id-type="pmid">31285623</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Born</surname> <given-names>M</given-names></name><name><surname>Wolf</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Principles of Optics: Electromagnetic Theory of Propagation, Interference and Diffraction of Light</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Waller</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>3D differential phase contrast microscopy</article-title><source>Biomedical Optics Express</source><volume>7</volume><elocation-id>3940</elocation-id><pub-id pub-id-type="doi">10.1364/BOE.7.003940</pub-id><pub-id pub-id-type="pmid">27867705</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Phillips</surname> <given-names>ZF</given-names></name><name><surname>Waller</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Quantitative differential phase contrast (DPC) microscopy with computational aberration correction</article-title><source>Optics Express</source><volume>26</volume><elocation-id>32888</elocation-id><pub-id pub-id-type="doi">10.1364/OE.26.032888</pub-id><pub-id pub-id-type="pmid">30645449</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christiansen</surname> <given-names>EM</given-names></name><name><surname>Yang</surname> <given-names>SJ</given-names></name><name><surname>Ando</surname> <given-names>DM</given-names></name><name><surname>Javaherian</surname> <given-names>A</given-names></name><name><surname>Skibinski</surname> <given-names>G</given-names></name><name><surname>Lipnick</surname> <given-names>S</given-names></name><name><surname>Mount</surname> <given-names>E</given-names></name><name><surname>O'Neil</surname> <given-names>A</given-names></name><name><surname>Shah</surname> <given-names>K</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Goyal</surname> <given-names>P</given-names></name><name><surname>Fedus</surname> <given-names>W</given-names></name><name><surname>Poplin</surname> <given-names>R</given-names></name><name><surname>Esteva</surname> <given-names>A</given-names></name><name><surname>Berndl</surname> <given-names>M</given-names></name><name><surname>Rubin</surname> <given-names>LL</given-names></name><name><surname>Nelson</surname> <given-names>P</given-names></name><name><surname>Finkbeiner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>In Silico labeling: predicting fluorescent labels in unlabeled images</article-title><source>Cell</source><volume>173</volume><fpage>792</fpage><lpage>803</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.03.040</pub-id><pub-id pub-id-type="pmid">29656897</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Claus</surname> <given-names>RA</given-names></name><name><surname>Naulleau</surname> <given-names>PP</given-names></name><name><surname>Neureuther</surname> <given-names>AR</given-names></name><name><surname>Waller</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Quantitative phase retrieval with arbitrary pupil and illumination</article-title><source>Optics Express</source><volume>23</volume><elocation-id>26672</elocation-id><pub-id pub-id-type="doi">10.1364/OE.23.026672</pub-id><pub-id pub-id-type="pmid">26480180</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Campos Vidal</surname> <given-names>B</given-names></name><name><surname>Mello</surname> <given-names>ML</given-names></name><name><surname>Caseiro-Filho</surname> <given-names>AC</given-names></name><name><surname>Godo</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Anisotropic properties of the myelin sheath</article-title><source>Acta Histochemica</source><volume>66</volume><fpage>32</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/S0065-1281(80)80079-1</pub-id><pub-id pub-id-type="pmid">6776776</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeMay</surname> <given-names>BS</given-names></name><name><surname>Noda</surname> <given-names>N</given-names></name><name><surname>Gladfelter</surname> <given-names>AS</given-names></name><name><surname>Oldenbourg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Rapid and quantitative imaging of excitation polarized fluorescence reveals ordered septin dynamics in live yeast</article-title><source>Biophysical Journal</source><volume>101</volume><fpage>985</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2011.07.008</pub-id><pub-id pub-id-type="pmid">21843491</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Drozdzal</surname> <given-names>M</given-names></name><name><surname>Vorontsov</surname> <given-names>E</given-names></name><name><surname>Chartrand</surname> <given-names>G</given-names></name><name><surname>Kadoury</surname> <given-names>S</given-names></name><name><surname>Pal</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The importance of skip connections in biomedical image segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.04117">https://arxiv.org/abs/1608.04117</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrand</surname> <given-names>P</given-names></name><name><surname>Baroni</surname> <given-names>A</given-names></name><name><surname>Allain</surname> <given-names>M</given-names></name><name><surname>Chamard</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Quantitative imaging of anisotropic material properties with vectorial ptychography</article-title><source>Optics Letters</source><volume>43</volume><elocation-id>763</elocation-id><pub-id pub-id-type="doi">10.1364/OL.43.000763</pub-id><pub-id pub-id-type="pmid">29443988</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Han</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automatic liver lesion segmentation using A deep convolutional neural network method</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.07239">https://arxiv.org/abs/1704.07239</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heath</surname> <given-names>F</given-names></name><name><surname>Hurley</surname> <given-names>SA</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Sampaio-Baptista</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Advances in noninvasive myelin imaging</article-title><source>Developmental Neurobiology</source><volume>78</volume><fpage>136</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1002/dneu.22552</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henssen</surname> <given-names>D</given-names></name><name><surname>Mollink</surname> <given-names>J</given-names></name><name><surname>Kurt</surname> <given-names>E</given-names></name><name><surname>van Dongen</surname> <given-names>R</given-names></name><name><surname>Bartels</surname> <given-names>R</given-names></name><name><surname>Gräβel</surname> <given-names>D</given-names></name><name><surname>Kozicz</surname> <given-names>T</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name><name><surname>Van Cappellen van Walsum</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ex vivo visualization of the trigeminal pathways in the human brainstem using 11.7T diffusion MRI combined with microscopy polarized light imaging</article-title><source>Brain Structure and Function</source><volume>224</volume><fpage>159</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1007/s00429-018-1767-1</pub-id><pub-id pub-id-type="pmid">30293214</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imai</surname> <given-names>R</given-names></name><name><surname>Nozaki</surname> <given-names>T</given-names></name><name><surname>Tani</surname> <given-names>T</given-names></name><name><surname>Kaizu</surname> <given-names>K</given-names></name><name><surname>Hibino</surname> <given-names>K</given-names></name><name><surname>Ide</surname> <given-names>S</given-names></name><name><surname>Tamura</surname> <given-names>S</given-names></name><name><surname>Takahashi</surname> <given-names>K</given-names></name><name><surname>Shribak</surname> <given-names>M</given-names></name><name><surname>Maeshima</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Density imaging of heterochromatin in live cells using orientation-independent-DIC microscopy</article-title><source>Molecular Biology of the Cell</source><volume>28</volume><fpage>3349</fpage><lpage>3359</lpage><pub-id pub-id-type="doi">10.1091/mbc.e17-06-0359</pub-id><pub-id pub-id-type="pmid">28835378</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inoue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>[Polarization optical studies of the mitotic spindle. I. the demonstration of spindle fibers in living cells]</article-title><source>Chromosoma</source><volume>5</volume><fpage>487</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1007/BF01271498</pub-id><pub-id pub-id-type="pmid">13082662</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jakovcevski</surname> <given-names>I</given-names></name><name><surname>Filipovic</surname> <given-names>R</given-names></name><name><surname>Mo</surname> <given-names>Z</given-names></name><name><surname>Rakic</surname> <given-names>S</given-names></name><name><surname>Zecevic</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Oligodendrocyte development and the onset of myelination in the human fetal brain</article-title><source>Frontiers in Neuroanatomy</source><volume>3</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.05.005.2009</pub-id><pub-id pub-id-type="pmid">19521542</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkins</surname> <given-names>MH</given-names></name><name><surname>Gaylord</surname> <given-names>TK</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Quantitative phase microscopy via optimized inversion of the phase optical transfer function</article-title><source>Applied Optics</source><volume>54</volume><elocation-id>8566</elocation-id><pub-id pub-id-type="doi">10.1364/AO.54.008566</pub-id><pub-id pub-id-type="pmid">26479636</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkins</surname> <given-names>MH</given-names></name><name><surname>Gaylord</surname> <given-names>TK</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Three-dimensional quantitative phase imaging via tomographic deconvolution phase microscopy</article-title><source>Applied Optics</source><volume>54</volume><fpage>9213</fpage><lpage>9227</lpage><pub-id pub-id-type="doi">10.1364/AO.54.009213</pub-id><pub-id pub-id-type="pmid">26560576</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keefe</surname> <given-names>D</given-names></name><name><surname>Liu</surname> <given-names>L</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Silva</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Imaging meiotic spindles by polarization light microscopy: principles and applications to IVF</article-title><source>Reproductive BioMedicine Online</source><volume>7</volume><fpage>24</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/S1472-6483(10)61724-5</pub-id><pub-id pub-id-type="pmid">12930570</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khodanovich</surname> <given-names>M</given-names></name><name><surname>Pishchelko</surname> <given-names>A</given-names></name><name><surname>Glazacheva</surname> <given-names>V</given-names></name><name><surname>Pan</surname> <given-names>E</given-names></name><name><surname>Akulov</surname> <given-names>A</given-names></name><name><surname>Svetlik</surname> <given-names>M</given-names></name><name><surname>Tyumentseva</surname> <given-names>Y</given-names></name><name><surname>Anan’ina</surname> <given-names>T</given-names></name><name><surname>Yarnykh</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Quantitative imaging of white and gray matter remyelination in the cuprizone demyelination model using the macromolecular proton fraction</article-title><source>Cells</source><volume>8</volume><elocation-id>1204</elocation-id><pub-id pub-id-type="doi">10.3390/cells8101204</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>D</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>M</given-names></name><name><surname>Oh</surname> <given-names>J</given-names></name><name><surname>Yang</surname> <given-names>S-A</given-names></name><name><surname>Park</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Holotomography: refractive index as an intrinsic imaging contrast for 3-D label-free live cell imaging</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/106328</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinfeld</surname> <given-names>D</given-names></name><name><surname>Bharioke</surname> <given-names>A</given-names></name><name><surname>Blinder</surname> <given-names>P</given-names></name><name><surname>Bock</surname> <given-names>DD</given-names></name><name><surname>Briggman</surname> <given-names>KL</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Helmstaedter</surname> <given-names>M</given-names></name><name><surname>Kaufhold</surname> <given-names>JP</given-names></name><name><surname>Lee</surname> <given-names>W-CA</given-names></name><name><surname>Meyer</surname> <given-names>HS</given-names></name><name><surname>Micheva</surname> <given-names>KD</given-names></name><name><surname>Oberlaender</surname> <given-names>M</given-names></name><name><surname>Prohaska</surname> <given-names>S</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Smith</surname> <given-names>SJ</given-names></name><name><surname>Takemura</surname> <given-names>S</given-names></name><name><surname>Tsai</surname> <given-names>PS</given-names></name><name><surname>Sakmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Large-Scale automated histology in the pursuit of connectomes</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>16125</fpage><lpage>16138</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4077-11.2011</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Koike-Tani</surname> <given-names>M</given-names></name><name><surname>Tominaga</surname> <given-names>T</given-names></name><name><surname>Oldenbourg</surname> <given-names>R</given-names></name><name><surname>Tani</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Instantaneous polarized light imaging reveals activity dependent structural changes of dendrites in mouse hippocampal slices</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/523571</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>Y-H</given-names></name><name><surname>Song</surname> <given-names>J</given-names></name><name><surname>Kim</surname> <given-names>G</given-names></name><name><surname>Jo</surname> <given-names>Y</given-names></name><name><surname>Min</surname> <given-names>H</given-names></name><name><surname>Kim</surname> <given-names>CH</given-names></name><name><surname>Park</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepIS: Deep learning framework for three-dimensional label-free tracking of immunological synapses</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/539858</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lein</surname> <given-names>ES</given-names></name><name><surname>Hawrylycz</surname> <given-names>MJ</given-names></name><name><surname>Ao</surname> <given-names>N</given-names></name><name><surname>Ayres</surname> <given-names>M</given-names></name><name><surname>Bensinger</surname> <given-names>A</given-names></name><name><surname>Bernard</surname> <given-names>A</given-names></name><name><surname>Boe</surname> <given-names>AF</given-names></name><name><surname>Boguski</surname> <given-names>MS</given-names></name><name><surname>Brockway</surname> <given-names>KS</given-names></name><name><surname>Byrnes</surname> <given-names>EJ</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Chen</surname> <given-names>TM</given-names></name><name><surname>Chin</surname> <given-names>MC</given-names></name><name><surname>Chong</surname> <given-names>J</given-names></name><name><surname>Crook</surname> <given-names>BE</given-names></name><name><surname>Czaplinska</surname> <given-names>A</given-names></name><name><surname>Dang</surname> <given-names>CN</given-names></name><name><surname>Datta</surname> <given-names>S</given-names></name><name><surname>Dee</surname> <given-names>NR</given-names></name><name><surname>Desaki</surname> <given-names>AL</given-names></name><name><surname>Desta</surname> <given-names>T</given-names></name><name><surname>Diep</surname> <given-names>E</given-names></name><name><surname>Dolbeare</surname> <given-names>TA</given-names></name><name><surname>Donelan</surname> <given-names>MJ</given-names></name><name><surname>Dong</surname> <given-names>HW</given-names></name><name><surname>Dougherty</surname> <given-names>JG</given-names></name><name><surname>Duncan</surname> <given-names>BJ</given-names></name><name><surname>Ebbert</surname> <given-names>AJ</given-names></name><name><surname>Eichele</surname> <given-names>G</given-names></name><name><surname>Estin</surname> <given-names>LK</given-names></name><name><surname>Faber</surname> <given-names>C</given-names></name><name><surname>Facer</surname> <given-names>BA</given-names></name><name><surname>Fields</surname> <given-names>R</given-names></name><name><surname>Fischer</surname> <given-names>SR</given-names></name><name><surname>Fliss</surname> <given-names>TP</given-names></name><name><surname>Frensley</surname> <given-names>C</given-names></name><name><surname>Gates</surname> <given-names>SN</given-names></name><name><surname>Glattfelder</surname> <given-names>KJ</given-names></name><name><surname>Halverson</surname> <given-names>KR</given-names></name><name><surname>Hart</surname> <given-names>MR</given-names></name><name><surname>Hohmann</surname> <given-names>JG</given-names></name><name><surname>Howell</surname> <given-names>MP</given-names></name><name><surname>Jeung</surname> <given-names>DP</given-names></name><name><surname>Johnson</surname> <given-names>RA</given-names></name><name><surname>Karr</surname> <given-names>PT</given-names></name><name><surname>Kawal</surname> <given-names>R</given-names></name><name><surname>Kidney</surname> <given-names>JM</given-names></name><name><surname>Knapik</surname> <given-names>RH</given-names></name><name><surname>Kuan</surname> <given-names>CL</given-names></name><name><surname>Lake</surname> <given-names>JH</given-names></name><name><surname>Laramee</surname> <given-names>AR</given-names></name><name><surname>Larsen</surname> <given-names>KD</given-names></name><name><surname>Lau</surname> <given-names>C</given-names></name><name><surname>Lemon</surname> <given-names>TA</given-names></name><name><surname>Liang</surname> <given-names>AJ</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Luong</surname> <given-names>LT</given-names></name><name><surname>Michaels</surname> <given-names>J</given-names></name><name><surname>Morgan</surname> <given-names>JJ</given-names></name><name><surname>Morgan</surname> <given-names>RJ</given-names></name><name><surname>Mortrud</surname> <given-names>MT</given-names></name><name><surname>Mosqueda</surname> <given-names>NF</given-names></name><name><surname>Ng</surname> <given-names>LL</given-names></name><name><surname>Ng</surname> <given-names>R</given-names></name><name><surname>Orta</surname> <given-names>GJ</given-names></name><name><surname>Overly</surname> <given-names>CC</given-names></name><name><surname>Pak</surname> <given-names>TH</given-names></name><name><surname>Parry</surname> <given-names>SE</given-names></name><name><surname>Pathak</surname> <given-names>SD</given-names></name><name><surname>Pearson</surname> <given-names>OC</given-names></name><name><surname>Puchalski</surname> <given-names>RB</given-names></name><name><surname>Riley</surname> <given-names>ZL</given-names></name><name><surname>Rockett</surname> <given-names>HR</given-names></name><name><surname>Rowland</surname> <given-names>SA</given-names></name><name><surname>Royall</surname> <given-names>JJ</given-names></name><name><surname>Ruiz</surname> <given-names>MJ</given-names></name><name><surname>Sarno</surname> <given-names>NR</given-names></name><name><surname>Schaffnit</surname> <given-names>K</given-names></name><name><surname>Shapovalova</surname> <given-names>NV</given-names></name><name><surname>Sivisay</surname> <given-names>T</given-names></name><name><surname>Slaughterbeck</surname> <given-names>CR</given-names></name><name><surname>Smith</surname> <given-names>SC</given-names></name><name><surname>Smith</surname> <given-names>KA</given-names></name><name><surname>Smith</surname> <given-names>BI</given-names></name><name><surname>Sodt</surname> <given-names>AJ</given-names></name><name><surname>Stewart</surname> <given-names>NN</given-names></name><name><surname>Stumpf</surname> <given-names>KR</given-names></name><name><surname>Sunkin</surname> <given-names>SM</given-names></name><name><surname>Sutram</surname> <given-names>M</given-names></name><name><surname>Tam</surname> <given-names>A</given-names></name><name><surname>Teemer</surname> <given-names>CD</given-names></name><name><surname>Thaller</surname> <given-names>C</given-names></name><name><surname>Thompson</surname> <given-names>CL</given-names></name><name><surname>Varnam</surname> <given-names>LR</given-names></name><name><surname>Visel</surname> <given-names>A</given-names></name><name><surname>Whitlock</surname> <given-names>RM</given-names></name><name><surname>Wohnoutka</surname> <given-names>PE</given-names></name><name><surname>Wolkey</surname> <given-names>CK</given-names></name><name><surname>Wong</surname> <given-names>VY</given-names></name><name><surname>Wood</surname> <given-names>M</given-names></name><name><surname>Yaylaoglu</surname> <given-names>MB</given-names></name><name><surname>Young</surname> <given-names>RC</given-names></name><name><surname>Youngstrom</surname> <given-names>BL</given-names></name><name><surname>Yuan</surname> <given-names>XF</given-names></name><name><surname>Zhang</surname> <given-names>B</given-names></name><name><surname>Zwingman</surname> <given-names>TA</given-names></name><name><surname>Jones</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Genome-wide atlas of gene expression in the adult mouse brain</article-title><source>Nature</source><volume>445</volume><fpage>168</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1038/nature05453</pub-id><pub-id pub-id-type="pmid">17151600</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ling</surname> <given-names>T</given-names></name><name><surname>Boyle</surname> <given-names>KC</given-names></name><name><surname>Zuckerman</surname> <given-names>V</given-names></name><name><surname>Flores</surname> <given-names>T</given-names></name><name><surname>Ramakrishnan</surname> <given-names>C</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name><name><surname>Palanker</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How neurons move during action potentials</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/765768</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>SB</given-names></name><name><surname>Shribak</surname> <given-names>M</given-names></name><name><surname>Oldenbourg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Polarized light imaging of birefringence and diattenuation at high resolution and high sensitivity</article-title><source>Journal of Optics</source><volume>15</volume><elocation-id>094007</elocation-id><pub-id pub-id-type="doi">10.1088/2040-8978/15/9/094007</pub-id><pub-id pub-id-type="pmid">24273640</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>SB</given-names></name><name><surname>McQuilken</surname> <given-names>M</given-names></name><name><surname>La Riviere</surname> <given-names>PJ</given-names></name><name><surname>Occhipinti</surname> <given-names>P</given-names></name><name><surname>Verma</surname> <given-names>A</given-names></name><name><surname>Oldenbourg</surname> <given-names>R</given-names></name><name><surname>Gladfelter</surname> <given-names>AS</given-names></name><name><surname>Tani</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissection of molecular assembly dynamics by tracking orientation and position of single molecules in live cells</article-title><source>PNAS</source><volume>113</volume><fpage>E6352</fpage><lpage>E6361</lpage><pub-id pub-id-type="doi">10.1073/pnas.1607674113</pub-id><pub-id pub-id-type="pmid">27679846</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menzel</surname> <given-names>M</given-names></name><name><surname>Michielsen</surname> <given-names>K</given-names></name><name><surname>De Raedt</surname> <given-names>H</given-names></name><name><surname>Reckfort</surname> <given-names>J</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A jones matrix formalism for simulating three-dimensional polarized light imaging of brain tissue</article-title><source>Journal of the Royal Society Interface</source><volume>12</volume><elocation-id>20150734</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2015.0734</pub-id><pub-id pub-id-type="pmid">26446561</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Menzel</surname> <given-names>M</given-names></name><name><surname>Reckfort</surname> <given-names>J</given-names></name><name><surname>Weigand</surname> <given-names>D</given-names></name><name><surname>Köse</surname> <given-names>H</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Diattenuation of brain tissue and its impact on 3D polarized light imaging</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.04343">https://arxiv.org/abs/1703.04343</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>DJ</given-names></name><name><surname>Duka</surname> <given-names>T</given-names></name><name><surname>Stimpson</surname> <given-names>CD</given-names></name><name><surname>Schapiro</surname> <given-names>SJ</given-names></name><name><surname>Baze</surname> <given-names>WB</given-names></name><name><surname>McArthur</surname> <given-names>MJ</given-names></name><name><surname>Fobbs</surname> <given-names>AJ</given-names></name><name><surname>Sousa</surname> <given-names>AMM</given-names></name><name><surname>Sestan</surname> <given-names>N</given-names></name><name><surname>Wildman</surname> <given-names>DE</given-names></name><name><surname>Lipovich</surname> <given-names>L</given-names></name><name><surname>Kuzawa</surname> <given-names>CW</given-names></name><name><surname>Hof</surname> <given-names>PR</given-names></name><name><surname>Sherwood</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prolonged myelination in human neocortical evolution</article-title><source>PNAS</source><volume>109</volume><fpage>16480</fpage><lpage>16485</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117943109</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Milletari</surname> <given-names>F</given-names></name><name><surname>Navab</surname> <given-names>N</given-names></name><name><surname>Ahmadi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>V-Net: Fully convolutional neural networks for volumetric medical image segmentation</article-title><conf-name>2016 Fourth International Conference on 3D Vision</conf-name><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1109/3DV.2016.79</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moen</surname> <given-names>E</given-names></name><name><surname>Bannon</surname> <given-names>D</given-names></name><name><surname>Kudo</surname> <given-names>T</given-names></name><name><surname>Graf</surname> <given-names>W</given-names></name><name><surname>Covert</surname> <given-names>M</given-names></name><name><surname>Van Valen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning for cellular image analysis</article-title><source>Nature Methods</source><volume>16</volume><fpage>1233</fpage><lpage>1246</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id><pub-id pub-id-type="pmid">31133758</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mollink</surname> <given-names>J</given-names></name><name><surname>Kleinnijenhuis</surname> <given-names>M</given-names></name><name><surname>Cappellen van Walsum</surname> <given-names>AV</given-names></name><name><surname>Sotiropoulos</surname> <given-names>SN</given-names></name><name><surname>Cottaar</surname> <given-names>M</given-names></name><name><surname>Mirfin</surname> <given-names>C</given-names></name><name><surname>Heinrich</surname> <given-names>MP</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Pallebage-Gamarallage</surname> <given-names>M</given-names></name><name><surname>Ansorge</surname> <given-names>O</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Miller</surname> <given-names>KL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evaluating fibre orientation dispersion in white matter: comparison of diffusion MRI, histology and polarized light imaging</article-title><source>NeuroImage</source><volume>157</volume><fpage>561</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.001</pub-id><pub-id pub-id-type="pmid">28602815</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monsma</surname> <given-names>PC</given-names></name><name><surname>Brown</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FluoroMyelin red is a bright, photostable and non-toxic fluorescent stain for live imaging of myelin</article-title><source>Journal of Neuroscience Methods</source><volume>209</volume><fpage>344</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2012.06.015</pub-id><pub-id pub-id-type="pmid">22743799</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noda</surname> <given-names>T</given-names></name><name><surname>Kawata</surname> <given-names>S</given-names></name><name><surname>Minami</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Three-dimensional phase contrast imaging by an annular illumination microscope</article-title><source>Applied Optics</source><volume>29</volume><fpage>3810</fpage><lpage>3815</lpage><pub-id pub-id-type="doi">10.1364/AO.29.003810</pub-id><pub-id pub-id-type="pmid">20567488</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nomarski</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>Differential microinterferometer with polarized waves</article-title><source>Journal de Physique et Le Radium</source><volume>16</volume><elocation-id>9S</elocation-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname> <given-names>SW</given-names></name><name><surname>Harris</surname> <given-names>JA</given-names></name><name><surname>Ng</surname> <given-names>L</given-names></name><name><surname>Winslow</surname> <given-names>B</given-names></name><name><surname>Cain</surname> <given-names>N</given-names></name><name><surname>Mihalas</surname> <given-names>S</given-names></name><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>Lau</surname> <given-names>C</given-names></name><name><surname>Kuan</surname> <given-names>L</given-names></name><name><surname>Henry</surname> <given-names>AM</given-names></name><name><surname>Mortrud</surname> <given-names>MT</given-names></name><name><surname>Ouellette</surname> <given-names>B</given-names></name><name><surname>Nguyen</surname> <given-names>TN</given-names></name><name><surname>Sorensen</surname> <given-names>SA</given-names></name><name><surname>Slaughterbeck</surname> <given-names>CR</given-names></name><name><surname>Wakeman</surname> <given-names>W</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Feng</surname> <given-names>D</given-names></name><name><surname>Ho</surname> <given-names>A</given-names></name><name><surname>Nicholas</surname> <given-names>E</given-names></name><name><surname>Hirokawa</surname> <given-names>KE</given-names></name><name><surname>Bohn</surname> <given-names>P</given-names></name><name><surname>Joines</surname> <given-names>KM</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Hawrylycz</surname> <given-names>MJ</given-names></name><name><surname>Phillips</surname> <given-names>JW</given-names></name><name><surname>Hohmann</surname> <given-names>JG</given-names></name><name><surname>Wohnoutka</surname> <given-names>P</given-names></name><name><surname>Gerfen</surname> <given-names>CR</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Bernard</surname> <given-names>A</given-names></name><name><surname>Dang</surname> <given-names>C</given-names></name><name><surname>Jones</surname> <given-names>AR</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A mesoscale connectome of the mouse brain</article-title><source>Nature</source><volume>508</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1038/nature13186</pub-id><pub-id pub-id-type="pmid">24695228</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohki</surname> <given-names>K</given-names></name><name><surname>Chung</surname> <given-names>S</given-names></name><name><surname>Ch'ng</surname> <given-names>YH</given-names></name><name><surname>Kara</surname> <given-names>P</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional imaging with cellular resolution reveals precise micro-architecture in visual cortex</article-title><source>Nature</source><volume>433</volume><fpage>597</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1038/nature03274</pub-id><pub-id pub-id-type="pmid">15660108</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldenbourg</surname> <given-names>R</given-names></name><name><surname>Katoh</surname> <given-names>K</given-names></name><name><surname>Danuser</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mechanism of lateral movement of filopodia and radial actin bundles across neuronal growth cones</article-title><source>Biophysical Journal</source><volume>78</volume><fpage>1176</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(00)76675-6</pub-id><pub-id pub-id-type="pmid">10692307</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldenbourg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Polarized light field microscopy: an analytical method using a microlens array to simultaneously capture both conoscopic and orthoscopic views of birefringent objects</article-title><source>Journal of Microscopy</source><volume>231</volume><fpage>419</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2818.2008.02053.x</pub-id><pub-id pub-id-type="pmid">18754996</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldenbourg</surname> <given-names>R</given-names></name><name><surname>Mei</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>New polarized light microscope with precision universal compensator</article-title><source>Journal of Microscopy</source><volume>180</volume><fpage>140</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2818.1995.tb03669.x</pub-id><pub-id pub-id-type="pmid">8537959</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ounkomol</surname> <given-names>C</given-names></name><name><surname>Seshamani</surname> <given-names>S</given-names></name><name><surname>Maleckar</surname> <given-names>MM</given-names></name><name><surname>Collman</surname> <given-names>F</given-names></name><name><surname>Johnson</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy</article-title><source>Nature Methods</source><volume>15</volume><fpage>917</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0111-2</pub-id><pub-id pub-id-type="pmid">30224672</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>Y</given-names></name><name><surname>Depeursinge</surname> <given-names>C</given-names></name><name><surname>Popescu</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Quantitative phase imaging in biomedicine</article-title><source>Nature Photonics</source><volume>12</volume><fpage>578</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41566-018-0253-x</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname> <given-names>D</given-names></name><name><surname>Mavarani</surname> <given-names>L</given-names></name><name><surname>Niedieker</surname> <given-names>D</given-names></name><name><surname>Freier</surname> <given-names>E</given-names></name><name><surname>Tannapfel</surname> <given-names>A</given-names></name><name><surname>Kötting</surname> <given-names>C</given-names></name><name><surname>Gerwert</surname> <given-names>K</given-names></name><name><surname>El-Mashtoly</surname> <given-names>SF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual staining of Colon cancer tissue by label-free raman micro-spectroscopy</article-title><source>The Analyst</source><volume>142</volume><fpage>1207</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1039/C6AN02072K</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popescu</surname> <given-names>G</given-names></name><name><surname>Ikeda</surname> <given-names>T</given-names></name><name><surname>Dasari</surname> <given-names>RR</given-names></name><name><surname>Feld</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Diffraction phase microscopy for quantifying cell structure and dynamics</article-title><source>Optics Letters</source><volume>31</volume><fpage>775</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1364/OL.31.000775</pub-id><pub-id pub-id-type="pmid">16544620</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ragan</surname> <given-names>T</given-names></name><name><surname>Kadiri</surname> <given-names>LR</given-names></name><name><surname>Venkataraju</surname> <given-names>KU</given-names></name><name><surname>Bahlmann</surname> <given-names>K</given-names></name><name><surname>Sutin</surname> <given-names>J</given-names></name><name><surname>Taranda</surname> <given-names>J</given-names></name><name><surname>Arganda-Carreras</surname> <given-names>I</given-names></name><name><surname>Kim</surname> <given-names>Y</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Osten</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Serial two-photon tomography for automated ex vivo mouse brain imaging</article-title><source>Nature Methods</source><volume>9</volume><fpage>255</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1854</pub-id><pub-id pub-id-type="pmid">22245809</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rivenson</surname> <given-names>Y</given-names></name><name><surname>Liu</surname> <given-names>T</given-names></name><name><surname>Wei</surname> <given-names>Z</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Ozcan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>PhaseStain: Digital staining of label-free quantitative phase microscopy images using deep learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.07701">https://arxiv.org/abs/1807.07701</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivenson</surname> <given-names>Y</given-names></name><name><surname>Ceylan Koydemir</surname> <given-names>H</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Wei</surname> <given-names>Z</given-names></name><name><surname>Ren</surname> <given-names>Z</given-names></name><name><surname>Günaydın</surname> <given-names>H</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Göröcs</surname> <given-names>Z</given-names></name><name><surname>Liang</surname> <given-names>K</given-names></name><name><surname>Tseng</surname> <given-names>D</given-names></name><name><surname>Ozcan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Deep learning enhanced Mobile-Phone microscopy</article-title><source>ACS Photonics</source><volume>5</volume><fpage>2354</fpage><lpage>2364</lpage><pub-id pub-id-type="doi">10.1021/acsphotonics.8b00146</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivenson</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Wei</surname> <given-names>Z</given-names></name><name><surname>de Haan</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Wu</surname> <given-names>Y</given-names></name><name><surname>Günaydın</surname> <given-names>H</given-names></name><name><surname>Zuckerman</surname> <given-names>JE</given-names></name><name><surname>Chong</surname> <given-names>T</given-names></name><name><surname>Sisk</surname> <given-names>AE</given-names></name><name><surname>Westbrook</surname> <given-names>LM</given-names></name><name><surname>Wallace</surname> <given-names>WD</given-names></name><name><surname>Ozcan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning</article-title><source>Nature Biomedical Engineering</source><volume>3</volume><fpage>466</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1038/s41551-019-0362-y</pub-id><pub-id pub-id-type="pmid">31142829</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-Net: Convolutional networks for biomedical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Navab</surname> <given-names>N</given-names></name><name><surname>Hornegger</surname> <given-names>J</given-names></name><name><surname>Wells</surname> <given-names>W</given-names></name><name><surname>Frangi</surname> <given-names>A</given-names></name></person-group><source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salamon</surname> <given-names>Z</given-names></name><name><surname>Tollin</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Optical anisotropy in lipid bilayer membranes: coupled plasmon-waveguide resonance measurements of molecular orientation, Polarizability, and shape</article-title><source>Biophysical Journal</source><volume>80</volume><fpage>1557</fpage><lpage>1567</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(01)76128-0</pub-id><pub-id pub-id-type="pmid">11222316</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="1926">1926</year><article-title>Die Bausteine des Tierkörpers in polarisiertem Lichte</article-title><source>Protoplasma</source><volume>1</volume><fpage>618</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1007/BF01603040</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitz</surname> <given-names>D</given-names></name><name><surname>Muenzing</surname> <given-names>SEA</given-names></name><name><surname>Schober</surname> <given-names>M</given-names></name><name><surname>Schubert</surname> <given-names>N</given-names></name><name><surname>Minnerop</surname> <given-names>M</given-names></name><name><surname>Lippert</surname> <given-names>T</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Derivation of fiber orientations from oblique views through human brain sections in 3D-Polarized light imaging</article-title><source>Frontiers in Neuroanatomy</source><volume>12</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2018.00075</pub-id><pub-id pub-id-type="pmid">30323745</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schmitz</surname> <given-names>D</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Lippert</surname> <given-names>T</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>A least squares approach for the reconstruction of nerve fiber orientations from tiltable specimen experiments in 3D-PLI</article-title><conf-name>2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI)</conf-name><fpage>132</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1109/ISBI.2018.8363539</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shribak</surname> <given-names>M</given-names></name><name><surname>LaFountain</surname> <given-names>J</given-names></name><name><surname>Biggs</surname> <given-names>D</given-names></name><name><surname>Inouè</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Orientation-independent differential interference contrast microscopy and its combination with an orientation-independent polarization system</article-title><source>Journal of Biomedical Optics</source><volume>13</volume><elocation-id>014011</elocation-id><pub-id pub-id-type="doi">10.1117/1.2837406</pub-id><pub-id pub-id-type="pmid">18315369</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shribak</surname> <given-names>M</given-names></name><name><surname>Oldenbourg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Techniques for fast and sensitive measurements of two-dimensional birefringence distributions</article-title><source>Applied Optics</source><volume>42</volume><fpage>3009</fpage><lpage>3017</lpage><pub-id pub-id-type="doi">10.1364/AO.42.003009</pub-id><pub-id pub-id-type="pmid">12790452</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snaidero</surname> <given-names>N</given-names></name><name><surname>Simons</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Myelination at a glance</article-title><source>Journal of Cell Science</source><volume>127</volume><fpage>2999</fpage><lpage>3004</lpage><pub-id pub-id-type="doi">10.1242/jcs.151043</pub-id><pub-id pub-id-type="pmid">25024457</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soto</surname> <given-names>JM</given-names></name><name><surname>Rodrigo</surname> <given-names>JA</given-names></name><name><surname>Alieva</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Label-free quantitative 3D tomographic imaging for partially coherent light microscopy</article-title><source>Optics Express</source><volume>25</volume><fpage>15699</fpage><lpage>15712</lpage><pub-id pub-id-type="doi">10.1364/OE.25.015699</pub-id><pub-id pub-id-type="pmid">28789083</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiesz</surname> <given-names>EM</given-names></name><name><surname>Kaminsky</surname> <given-names>W</given-names></name><name><surname>Zysset</surname> <given-names>PK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A quantitative collagen fibers orientation assessment using birefringence measurements: calibration and application to human osteons</article-title><source>Journal of Structural Biology</source><volume>176</volume><fpage>302</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2011.09.009</pub-id><pub-id pub-id-type="pmid">21970947</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Streibl</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Phase imaging by the transport equation of intensity</article-title><source>Optics Communications</source><volume>49</volume><fpage>6</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/0030-4018(84)90079-8</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Streibl</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Three-dimensional imaging by a microscope</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><fpage>121</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000121</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Waller</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Quantitative differential phase contrast imaging in an LED array microscope</article-title><source>Optics Express</source><volume>23</volume><fpage>11394</fpage><lpage>11403</lpage><pub-id pub-id-type="doi">10.1364/OE.23.011394</pub-id><pub-id pub-id-type="pmid">25969234</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname> <given-names>MT</given-names></name><name><surname>Oldenbourg</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mapping birefringence in three dimensions using polarized light field microscopy: the case of the juvenile clamshell</article-title><source>Journal of Microscopy</source><volume>271</volume><fpage>315</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1111/jmi.12721</pub-id><pub-id pub-id-type="pmid">29926918</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Valen</surname> <given-names>DA</given-names></name><name><surname>Kudo</surname> <given-names>T</given-names></name><name><surname>Lane</surname> <given-names>KM</given-names></name><name><surname>Macklin</surname> <given-names>DN</given-names></name><name><surname>Quach</surname> <given-names>NT</given-names></name><name><surname>DeFelice</surname> <given-names>MM</given-names></name><name><surname>Maayan</surname> <given-names>I</given-names></name><name><surname>Tanouchi</surname> <given-names>Y</given-names></name><name><surname>Ashley</surname> <given-names>EA</given-names></name><name><surname>Covert</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning automates the quantitative analysis of individual cells in Live-Cell imaging experiments</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005177</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005177</pub-id><pub-id pub-id-type="pmid">27814364</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waller</surname> <given-names>L</given-names></name><name><surname>Tian</surname> <given-names>L</given-names></name><name><surname>Barbastathis</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Transport of intensity phase-amplitude imaging with higher order intensity derivatives</article-title><source>Optics Express</source><volume>18</volume><fpage>12552</fpage><lpage>12561</lpage><pub-id pub-id-type="doi">10.1364/OE.18.012552</pub-id><pub-id pub-id-type="pmid">20588381</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Yang</surname> <given-names>J</given-names></name><name><surname>Yin</surname> <given-names>W</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A new alternating minimization algorithm for total variation image reconstruction</article-title><source>SIAM Journal on Imaging Sciences</source><volume>1</volume><fpage>248</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1137/080724265</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Magnain</surname> <given-names>C</given-names></name><name><surname>Wang</surname> <given-names>R</given-names></name><name><surname>Dubb</surname> <given-names>J</given-names></name><name><surname>Varjabedian</surname> <given-names>A</given-names></name><name><surname>Tirrell</surname> <given-names>LS</given-names></name><name><surname>Stevens</surname> <given-names>A</given-names></name><name><surname>Augustinack</surname> <given-names>JC</given-names></name><name><surname>Konukoglu</surname> <given-names>E</given-names></name><name><surname>Aganj</surname> <given-names>I</given-names></name><name><surname>Frosch</surname> <given-names>MP</given-names></name><name><surname>Schmahmann</surname> <given-names>JD</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Boas</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>as-PSOCT: volumetric microscopic imaging of human brain architecture and connectivity</article-title><source>NeuroImage</source><volume>165</volume><fpage>56</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.012</pub-id><pub-id pub-id-type="pmid">29017866</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Z</given-names></name><name><surname>Bovik</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mean squared error: love it or leave it? A new look at signal fidelity measures</article-title><source>IEEE Signal Processing Magazine</source><volume>26</volume><fpage>98</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1109/MSP.2008.930649</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>B</given-names></name><name><surname>Jan</surname> <given-names>NJ</given-names></name><name><surname>Brazile</surname> <given-names>B</given-names></name><name><surname>Voorhees</surname> <given-names>A</given-names></name><name><surname>Lathrop</surname> <given-names>KL</given-names></name><name><surname>Sigal</surname> <given-names>IA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Polarized light microscopy for 3-dimensional mapping of collagen fiber architecture in ocular tissues</article-title><source>Journal of Biophotonics</source><volume>11</volume><elocation-id>e201700356</elocation-id><pub-id pub-id-type="doi">10.1002/jbio.201700356</pub-id><pub-id pub-id-type="pmid">29633576</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>L</given-names></name><name><surname>Ghosh</surname> <given-names>RP</given-names></name><name><surname>Franklin</surname> <given-names>JM</given-names></name><name><surname>You</surname> <given-names>C</given-names></name><name><surname>Liphardt</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>NuSeT: a deep learning tool for reliably separating and analyzing crowded cells</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/749754</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeineh</surname> <given-names>MM</given-names></name><name><surname>Palomero-Gallagher</surname> <given-names>N</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name><name><surname>Gräßel</surname> <given-names>D</given-names></name><name><surname>Goubran</surname> <given-names>M</given-names></name><name><surname>Wree</surname> <given-names>A</given-names></name><name><surname>Woods</surname> <given-names>R</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Direct visualization and mapping of the spatial course of fiber tracts at microscopic resolution in the human Hippocampus</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>1779</fpage><lpage>1794</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw010</pub-id><pub-id pub-id-type="pmid">26874183</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mesoscale connectomics</article-title><source>Current Opinion in Neurobiology</source><volume>50</volume><fpage>154</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.03.003</pub-id><pub-id pub-id-type="pmid">29579713</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zernike</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>How I discovered phase contrast</article-title><source>Science</source><volume>121</volume><fpage>345</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1126/science.121.3141.345</pub-id><pub-id pub-id-type="pmid">13237991</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname> <given-names>L</given-names></name><name><surname>Zhou</surname> <given-names>J</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Jin</surname> <given-names>Y</given-names></name><name><surname>Jahanshad</surname> <given-names>N</given-names></name><name><surname>Prasad</surname> <given-names>G</given-names></name><name><surname>Nir</surname> <given-names>TM</given-names></name><name><surname>Leonardo</surname> <given-names>CD</given-names></name><name><surname>Ye</surname> <given-names>J</given-names></name><name><surname>Thompson</surname> <given-names>PM</given-names></name><collab>For The Alzheimer's Disease Neuroimaging Initiative</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Comparison of nine tractography algorithms for detecting abnormal structural brain networks in Alzheimer's disease</article-title><source>Frontiers in Aging Neuroscience</source><volume>7</volume><elocation-id>48</elocation-id><pub-id pub-id-type="doi">10.3389/fnagi.2015.00048</pub-id><pub-id pub-id-type="pmid">25926791</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zilles</surname> <given-names>K</given-names></name><name><surname>Palomero-Gallagher</surname> <given-names>N</given-names></name><name><surname>Gräßel</surname> <given-names>D</given-names></name><name><surname>Schlömer</surname> <given-names>P</given-names></name><name><surname>Cremer</surname> <given-names>M</given-names></name><name><surname>Woods</surname> <given-names>R</given-names></name><name><surname>Amunts</surname> <given-names>K</given-names></name><name><surname>Axer</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Chapter 18 - High-resolution fiber and fiber tract imaging using polarized light microscopy in the human, monkey, rat, and mouse brain</chapter-title><person-group person-group-type="editor"><name><surname>Rockland</surname> <given-names>K. S</given-names></name></person-group><source>Axons and Brain Architecture</source><publisher-loc>San Diego</publisher-loc><publisher-name>Academic Press</publisher-name><fpage>369</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-801393-9.00018-9</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Glossary</title><boxed-text><list list-type="bullet"><list-item><p>QLIPP: Quantitative label-free imaging with phase and polarization.</p></list-item> <list-item><p>specimen phase: optical path length (OPL) of the specimen that is proportional to the product of its thickness and difference in the refractive index relative to the surrounding medium.</p></list-item> <list-item><p>specimen anisotropy: angular anisotropy in OPL, which refers to retardance and slow axis orientation collectively.</p></list-item> <list-item><p>wavefront: a surface in 3D space over which the time of propagation of light from the source is constant.</p></list-item> <list-item><p>phase of a wavefront: time delay of the wavefront, which is affected by the specimen phase.</p></list-item> <list-item><p>retardance: The difference in OPL induced by anisotropy specimen due to polarization-dependent refractive index.</p></list-item> <list-item><p>slow axis orientation: the orientation along which the refractive index of an anisotropic material is the highest. The light polarized along the slow axis experiences the highest phase delay relative to the light polarized along the other axes.</p></list-item> <list-item><p>U-Net: A fully convolution network consisting of a contracting and an expansive path, giving the architecture its U-shape.</p></list-item> <list-item><p>2D (Slice→Slice) U-Net: A U-Net model, using <inline-formula><mml:math id="inf53"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> convolution filters, that predicts a 2D slice from a 2D input slice.</p></list-item> <list-item><p>2.5D (Stack→Slice) U-Net: A U-Net model, using <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> convolution filters in the contracting path and <inline-formula><mml:math id="inf55"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> in the expansive path, that predicts a 2D slice from a small stack of <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> input slices.</p></list-item> <list-item><p>3D (Stack→Stack) U-Net: A U-Net model, using <inline-formula><mml:math id="inf57"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> convolution filters, that predicts a 3D stack from a 3D input stack.</p></list-item> <list-item><p>Normalization per tile: The data used for training neural networks are split into tiles. In this normalization strategy, each time is normalized independently to have zero mean and unit variance.</p></list-item> <list-item><p>Normalization per field of view: In this normalization strategy, each field of view (which consists of 16 tiles) is normalized to have zero mean and unit variance. The variations across tiles capture variations in the input and target data over the field of view.</p></list-item> <list-item><p>Normalization per dataset: In this normalization strategy, whole training set is normalized to have zero mean and unit variance. The variations across tiles capture variations in the input and target data over the entire dataset, for example, a large brain slice.</p></list-item> <list-item><p>SSIM: The Structural SIMilarity (SSIM) index is a method for measuring the similarity between two images.</p></list-item></list></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55502.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Forstmann</surname><given-names>Birte</given-names></name><role>Reviewing Editor</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Van Valen</surname><given-names>David</given-names> </name><role>Reviewer</role><aff><institution>Caltech</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper develops a new computational imaging approach for label-free imaging that uses polarization to acquire joint measurements of density and anisotropy. The authors provide evidence that this label-free approach can be combined with deep learning to effectively characterize the architecture of different samples across multiple spatial scales. To achieve this, they introduce a relatively computationally efficient deep learning architecture based off of the 3D U-Net that can be used to predict structures from multi-channel images as well as rescue inconsistent labelling.</p><p>Overall, the topic is of high interest and the reviewers agree that combining quantitative label-free imaging and deep neural networks of live and postmortem tissue is novel and important. The work is therefore of interest to a broad scientific audience.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Revealing architectural order with quantitative label-free imaging and deep learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Vivek Malhotra as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: David Van Valen (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>This paper develops a new computational imaging approach for label-free imaging that uses polarization to acquire joint measurements of density and anisotropy. The authors provide evidence that this label-free approach can be combined with deep learning to effectively characterize the architecture of different samples across multiple spatial scales. To achieve this, they introduce a relatively computationally efficient deep learning architecture based off of the 3D U-Net that can be used to predict structures from multi-channel images as well as rescue inconsistent labelling.</p><p>Overall, the topic is rated very high and the reviewers agree that combining quantitative label-free imaging and deep neural networks of live and postmortem tissue is valued. However, there are several major concerns the authors need to address before this manuscript can be considered for publication in <italic>eLife</italic>. These are listed below.</p><p>Essential revisions:</p><p>1) The idea of label-free prediction of a specific tissue structure from density and anisotropy measurements is appealing and well described in the manuscript. However, the selected types of tissue and some of the general conclusions drawn (partially from cross-comparisons) are disputable. What makes mouse kidney, mouse brain and prenatal human brain unique in terms of 'revealing architectural order', but still comparable? There seems to be a lack in knowledge of brain anatomy and morphology, which is important to evaluate the results. Although the GW24 and GW20 measurements are exciting, the shown tiny ROIs are not suitable for highlighting the anatomical differences in a convincing way. The deep learning approaches appear to be correctly implemented and applied, but their scalability does not become obvious (although stated in the Abstract). A critical debate about the efforts needed to address entire large-scale organs is missing. The major add-on to state-of-the-art approaches or to previous own publications does not become clear enough.</p><p>2).Ethics: There is no clear statement of the authors concerning ethical approval, origin of samples, etc. The treatment of prenatal human tissue requires other information than the mouse brain and kidney tissue!</p><p>3) The key insight offered by this paper is that because deep learning is data-driven, these methods can be improved by improving data rather than making substantial changes to the algorithms. If there is information missing in the images that is needed to make accurate predictions, why not add it in? To me this is an under-appreciated insight, one that the authors cleverly take advantage of, and one that the life science community as a whole sorely needs to hear. Based on the results presented here, there is a good chance that a number of previously ignored imaging modalities will now have higher value because of what can be done with deep learning. Unfortunately, I don't think the paper as written does a good job of relaying this conceptual shift and this is a substantial issue with the paper. Some of my recommendations to address this would include:</p><p>3a) Restructuring the Introduction. The prior work of Greg Johnson and others should be presented earlier so it is clear that this work builds on theirs. Doing so would make it easier for readers to appreciate that the novelty lies in combining these methods with the author's approach to quantitative label-free images.</p><p>3b) Better describe the novelty and performance gains. On the label-free imaging perspective, it is unclear how much of the work presented here is novel, as opposed to a straightforward application of the author's previous fluorescence based methods. I think this could be better explained. Also, the advantages of their method with respect to archival samples (i.e., obtaining staining information while avoiding potentially damaging stains) should be described earlier. The benefit of these methods for live-cell imaging (obtaining data while avoiding photodamage with respect to fluorescence) should also be mentioned, albeit with the appropriate reference.</p><p>4) In addition to this, the second major issue with this paper is how much of a performance boost does the author's label-free imaging approach provide? While the conceptual shift described above is appealing and should be highlighted, the case the authors make that this transforms one's ability to use image-to-image translation models on biological images is less clear. The authors use both the Pearson correlation and the structural similarity index to quantify their reconstruction of fluorescent actin in U2OS cells and in brain slices. However, the differences between standard label-free imaging (brightfield and phase) and the author's approach (brightfield, phase, retardance, and orientation) appear minor. For instance in Table 2, the difference in Pearson correlation is ~0.01-0.02 (the gap does appear to be bigger for FluoroMyelin, but fewer comparisons are presented). On its surface, this appears to be a minor advance (although one could argue whether it is in the realm of statistical significance) and as an experimentalist, it makes one question whether the &quot;juice&quot; of the author's method is worth the &quot;squeeze&quot;. However, there are certainly cases where minor boosts in accuracy lead to a big difference in one's ability to use a method. While the ability to measure orientation is certainly useful for following neural fibers, it feels like the case that this architectural information is critical to infer fluorescence patterns hasn't been made.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55502.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The idea of label-free prediction of a specific tissue structure from density and anisotropy measurements is appealing and well described in the manuscript. However, the selected types of tissue and some of the general conclusions drawn (partially from cross-comparisons) are disputable. What makes mouse kidney, mouse brain and prenatal human brain unique in terms of 'revealing architectural order', but still comparable? There seems to be a lack in knowledge of brain anatomy and morphology, which is important to evaluate the results. Although the GW24 and GW20 measurements are exciting, the shown tiny ROIs are not suitable for highlighting the anatomical differences in a convincing way. The deep learning approaches appear to be correctly implemented and applied, but their scalability does not become obvious (although stated in the Abstract). A critical debate about the efforts needed to address entire large-scale organs is missing. The major add-on to state-of-the-art approaches or to previous own publications does not become clear enough.</p></disp-quote><p>In this revision, we name our computational imaging method QLIPP (quantitative label-free imaging with phase and polarization) to clarify how it differs from quantitative phase imaging (QPI), quantitative polarization imaging, or fluorescence polarization imaging methods reported before this paper. We have edited the Abstract and Introduction to identify key advances relative to the state-of-the-art:</p><p>a) Joint label-free measurement of density (specimen phase) and anisotropy (retardance and orientation).</p><p>b) High-resolution measurements of density and anisotropy of live 3D cells and prenatal human brain tissue section.</p><p>c) 2.5D multi-channel convolutional neural network (CNN) to predict fluorescence over large fields-of-view.</p><p>Comparisons across data from brain tissue and live cells demonstrate that QLIPP is useful to analyze architecture at multiple spatial scales. The cross-comparisons are meant to, and limited to, illustrate how architecture at multiple scales can be interpreted from the measurements we report. We have re-structured the first result (QLIPP provides joint measurement of specimen density and anisotropy) to emphasize this point.</p><p>Mouse kidney tissue is a test specimen without any staining artifacts that we used to develop the architecture of CNN, training methods, and metrics for evaluation of trained models. The data from mouse kidney tissue are now moved to Figures 3 and 4 that report optimization of the CNN architecture.</p><p>In the Results and discussions, we’ve clarified questions in the analysis of organelle dynamics and architecture of brain tissue that become tractable with the data we report. Specifically, the videos (Video 1 and 2) of a dividing cell illustrate that organelles can be distinguished by pseudo-coloring the density and retardance values. For mouse brain tissue (Figure 5) and human brain tissue (Figure 6), we visualize architecture over centimeter sized slices. For human brain tissue, we predict myelin images at both Gestational Week 24 (GW24) and GW20 (Figure 7). These data illustrate that quantitative imaging of density and anisotropy, combined with deep learning, can reveal architectural order in diverse brain types.</p><p>We agree with the reviewer’s point of view that the scalability of quantitative label-free imaging and deep learning for analysis of the architecture of whole organs needs to be evaluated. The rate limiting factor may not be imaging or analysis, but rather tissue processing. We carefully use the term ‘scale’ only to imply the spatial and temporal scales of measurement and not the ease of using the approach to analyze large-scale organs.</p><disp-quote content-type="editor-comment"><p>2) Ethics: There is no clear statement of the authors concerning ethical approval, origin of samples, etc. The treatment of prenatal human tissue requires other information than the mouse brain and kidney tissue!</p></disp-quote><p>Lack of statement about ethical approval for use of prenatal human tissue was an oversight. We have described the ethical approval in the Materials and methods section.</p><p>“De-identified brain tissue samples were received with patient consent in accordance with a protocol approved by the Human Gamete, Embryo, and Stem Cell Research Committee (institutional review board) at the University of California, San Francisco.”</p><disp-quote content-type="editor-comment"><p>3) The key insight offered by this paper is that because deep learning is data-driven, these methods can be improved by improving data rather than making substantial changes to the algorithms. If there is information missing in the images that is needed to make accurate predictions, why not add it in? To me this is an under-appreciated insight, one that the authors cleverly take advantage of, and one that the life science community as a whole sorely needs to hear. Based on the results presented here, there is a good chance that a number of previously ignored imaging modalities will now have higher value because of what can be done with deep learning. Unfortunately, I don't think the paper as written does a good job of relaying this conceptual shift and this is a substantial issue with the paper. Some of my recommendations to address this would include:</p></disp-quote><p>We do agree with the reviewer that informative data is as important as algorithms for data-driven analysis. We also appreciate the reviewers’ inputs on clarity of contributions, which we’ve used to restructure the Introduction and Results as described in response to comment # 1 above.</p><disp-quote content-type="editor-comment"><p>3a) Restructuring the Introduction. The prior work of Greg Johnson and others should be presented earlier so it is clear that this work builds on theirs. Doing so would make it easier for readers to appreciate that the novelty lies in combining these methods with the author's approach to quantitative label-free images.</p></disp-quote><p>We have re-written the Introduction and Discussion to clarify how our work builds upon Greg Johnson’s work by a) adding new modalities of data and b) by adapting their deep learning architecture for computationally efficient training.</p><disp-quote content-type="editor-comment"><p>3b) Better describe the novelty and performance gains. On the label-free imaging perspective, it is unclear how much of the work presented here is novel, as opposed to a straightforward application of the author's previous fluorescence based methods. I think this could be better explained. Also, the advantages of their method with respect to archival samples (i.e., obtaining staining information while avoiding potentially damaging stains) should be described earlier. The benefit of these methods for live-cell imaging (obtaining data while avoiding photodamage with respect to fluorescence) should also be mentioned, albeit with the appropriate reference.</p></disp-quote><p>In terms of label-free imaging, the novel contribution of QLIPP is a more precise forward model of image formation and corresponding inverse algorithms for joint imaging of density and anisotropy using a simple light path. The light-path is identical to the transmission polarized light microscope (Mehta, Shribak and Oldenbourg, 2013), but different from the fluorescence polarization methods reported earlier by the corresponding author (Mehta et al., 2016). We now clearly distinguish QLIPP from other classes of polarization-resolved methods in the Introduction and the Discussion.</p><p>4) In addition to this, the second major issue with this paper is how much of a performance boost does the author's label-free imaging approach provide? While the conceptual shift described above is appealing and should be highlighted, the case the authors make that this transforms one's ability to use image-to-image translation models on biological images is less clear. The authors use both the Pearson correlation and the structural similarity index to quantify their reconstruction of fluorescent actin in U2OS cells and in brain slices. However, the differences between standard label-free imaging (brightfield and phase) and the author's approach (brightfield, phase, retardance, and orientation) appear minor. For instance in Table 2, the difference in Pearson correlation is ~0.01-0.02 (the gap does appear to be bigger for FluoroMyelin, but fewer comparisons are presented). On its surface, this appears to be a minor advance (although one could argue whether it is in the realm of statistical significance) and as an experimentalist, it makes one question whether the &quot;juice&quot; of the author's method is worth the &quot;squeeze&quot;. However, there are certainly cases where minor boosts in accuracy lead to a big difference in one's ability to use a method. While the ability to measure orientation is certainly useful for following neural fibers, it feels like the case that this architectural information is critical to infer fluorescence patterns hasn't been made.</p><p>To clarify the performance boost achievable using anisotropy data (retardance and orientation), we have added Figure 7 and updated Table 4. We report predicted images of FluoroMyelin over large fields of view and analyze the accuracy of prediction as a function of input data: only bright field; retardance and phase; retardance, phase, and orientation; brightfield, phase, and orientation. As compared to conventionally available data (brightfield), use of QLIPP data provides a significant increase in the accuracy of prediction as can be seen from images in Figure 7B and Figure 7E. We have added plots (Figure 7C, Figure 7F, and Figure 7—figure supplement 1) that show correlation of FluoroMyelin target with input retardance, phase, and FluoroMyelin predicted by our models. As shown in Table 4, using QLIPP data leads to an increase of 0.14 for both Pearson correlation and SSIM between prediction and target, relative to using just brightfield data. The prediction is more accurate with QLIPP data, because the pattern of myelination in the brain is encoded in anisotropy. Although this increase is small on the scale of these metrics (14%), it is biologically significant as can be seen from the images.</p><p>One known issue with image similarity metrics such as Pearson correlation and SSIM is that they can be insensitive to small, but biological relevant features in the image. This is because Pearson correlation and SSIM report the average similarity over the image with every pixel equally weighted.</p><p><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/5705575">https://ieeexplore.ieee.org/document/5705575</ext-link>;</p><p><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.7799">https://arxiv.org/abs/1406.7799</ext-link>;</p><p><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/abstract/document/5596999">https://ieeexplore.ieee.org/abstract/document/5596999</ext-link></p><p>We illustrate above point using Mouse Kidney tissue data (Figure 4—figure supplement 1). The image similarity metrics can change by the same small value when white noise is added or when a nucleus is removed. These sources of distortions cannot be distinguished with image similarity metrics as they do not fully exploit the spatial context. We have also revised the text in the Results (Optimization of 2.5D model with a test dataset) to emphasize this point.</p><p>A more informative metric would weigh pixels with biological information more than other pixels, e.g. a missing nucleus should be weighted more than mismatch in the background fluorescence. We considered image quality metrics that mimic human perception of image quality, e.g. multi-scale SSIM, feature similarity index, and CNN based predictors of quality. However, their applicability to quantitative evaluation of similarity between biological image sets is yet to be established. We chose to report Pearson correlation and SSIM because they are more commonly used in the biology and imaging community. Finding a biologically relevant metric of accuracy of image prediction is an interesting future research topic.</p></body></sub-article></article>