<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="article-commentary" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64384</article-id><article-id pub-id-type="doi">10.7554/eLife.64384</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Insight</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Deep Learning</subject></subj-group></article-categories><title-group><article-title>Tackling the challenges of bioimage analysis</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-127723"><name><surname>Pelt</surname><given-names>Daniël M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8253-0851</contrib-id><email>Daniel.Pelt@cwi.nl</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Daniël M Pelt</bold> is in the Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands</p></bio></contrib><aff id="aff1"><institution>Leiden Institute of Advanced Computer Science, Leiden University</institution><addr-line><named-content content-type="city">Leiden</named-content></addr-line><country>Netherlands</country></aff></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>02</day><month>12</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e64384</elocation-id><history><date date-type="received" iso-8601-date="2020-11-30"><day>30</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-11-30"><day>30</day><month>11</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Pelt</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Pelt</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64384-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary-article" xlink:href="10.7554/eLife.59780"/><abstract><p>Using multiple human annotators and ensembles of trained networks can improve the performance of deep-learning methods in research.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>bioimage informatics</kwd><kwd>deep learning</kwd><kwd>reproducibility</kwd><kwd>objectivity</kwd><kwd>validity</kwd><kwd>fluorescence microscopy</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Zebrafish</kwd></kwd-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Using multiple human annotators and ensembles of trained networks can improve the performance of deep-learning methods in research.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></article-meta></front><body><boxed-text><p><bold>Related research article</bold> Segebarth D, Griebel M, Stein N, R von Collenberg C, Martin C, Fiedler D, Comeras LB, Sah A, Schoeffler V, Lüffe T, Dürr A, Gupta R, Sasi M, Lillesaar C, Lange MD, Tasan RO, Singewald N, Pape HC, Flath CM, Blum R. 2020. On the objectivity, reliability, and validity of deep learning enabled bioimage analyses. <italic>eLife</italic> <bold>9</bold>:e59780. doi: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.7554/eLife.59780">10.7554/eLife.59780</ext-link></p></boxed-text><p>Deep learning has shown promising results in a wide range of imaging problems in recent years (<xref ref-type="bibr" rid="bib2">LeCun et al., 2015</xref>), and has the potential to help researchers by automating the analysis of various kinds of biological images (<xref ref-type="bibr" rid="bib6">Ronneberger et al., 2015</xref>). However, many deep-learning methods require a large amount of 'training data' in order to produce useful results, and this is often not available for bioimage analysis. There is, therefore, a need for deep-learning methods that can make the most from a limited amount of training data. Now, in eLife, Robert Blum (University Hospital Würzburg), Christoph Flath (University of Würzburg) and colleagues – including Dennis Segebarth and Matthias Griebel as joint first authors – provide guidance on how to do this in bioimage analysis (<xref ref-type="bibr" rid="bib7">Segebarth et al., 2020</xref>).</p><p>A common approach to applying deep learning to image analysis involves 'convolutional neural networks': these networks take an input image (such as a microscopy image) and perform many mathematical operations on it to produce an output image (such as a corresponding image with interesting features annotated). A convolutional neural network is characterized by a set of 'learnable parameters', which have to be set to the correct values for the network to perform a given task. The act of finding the correct values for these parameters is called 'training', and several different training techniques are used in practice.</p><p>In supervised learning, training is performed using a set of input images and target output images, and the learnable parameters are iteratively adjusted until the output images produced by the network match the target images. It is important to note that supervised learning involves a large amount of randomness, and that training multiple networks using the same data will result in different networks that produce (slightly) different output images.</p><p>In bioimage analysis, a common task is to annotate certain structures in images produced by techniques such as microscopy, cryo-EM or X-ray tomography (<xref ref-type="bibr" rid="bib3">Meijering et al., 2016</xref>). However, the complicated nature of biological images means that this annotation often has to be done by a human expert, which is time-consuming, labour-intensive and subjective (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Supervised deep learning could provide a way to automate the annotation process, reducing the burden on human experts and enabling analysis of a significantly larger set of images. However, annotating the input images needed to train the network also requires a significant amount of time and effort from a human expert.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Different ways to train a convolutional neural network.</title><p>Segebarth et al. compare three techniques for training convolutional neural networks to analyze bioimages. (<bold>A</bold>) In the standard approach a single human expert annotates images for training a single network. (<bold>B</bold>) In a second approach multiple human experts annotate the same images, and consensus images are used for training: this improves the objectivity of the trained network. (<bold>C</bold>) In a third approach, a technique called model ensembling is added to the second approach, meaning that multiple networks are trained with the same consensus images: this improves the reliability of the results.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64384-fig1-v1.tif"/></fig><p>Several properties of a trained network are important for real-world applications. The first is its objectivity, referring to a lack of influences from the subjective nature of human annotations. The second is its reliability, meaning that the trained network should consistently annotate similar features in the same way. The third is its validity, referring to the truthfulness of the network output (that is, did we annotate what we intended to?). In practice, it is often the case that objectivity, reliability and validity are difficult to achieve when the amount of training data is limited.</p><p>Various approaches to improve the objectivity, reliability and validity of convolutional neural networks have been proposed. Some involve adapting the structure of the network themselves by, for example, reducing the number of learnable parameters (<xref ref-type="bibr" rid="bib5">Pelt and Sethian, 2018</xref>), and some involve adapting the training method by, for example, randomly ignoring parts of the network during training (<xref ref-type="bibr" rid="bib8">Srivastava et al., 2014</xref>). A different approach is to focus on the training data used in supervised learning. Given a network structure and a training method, how can the training data set be optimized to improve objectivity, reliability and validity? In other words, given the time-consuming, labour-intensive and subjective nature of manual annotation, how can a limited period of time from human experts be best utilized to produce a training data set? These questions are currently the subject of active research.</p><p>Segebarth et al. investigate two techniques for improving the objectivity, reliability and validity of trained convolutional neural networks in bioimage analysis. First, they investigate the use of multiple human experts to annotate the same set of training images (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The different annotations of each input image are then combined to create a <italic>consensus</italic> target output image. Since each human expert has their own intended and unintended biases, networks that are trained with data from a single human expert might include the biases of the expert. Using consensus images from multiple experts during training can improve the objectivity of the resulting networks by removing these biases from the training data.</p><p>The second technique is to train multiple convolutional neural networks using the same training data set, and then combine the results when the networks are used to analyse new images (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This technique, called model ensembling, has already proven successful in a wide range of applications (<xref ref-type="bibr" rid="bib1">Krizhevsky et al., 2017</xref>). Model ensembling is based on the randomness involved in training described above: because of this randomness, each trained network will be implicitly biased in their results. By combining the output of multiple networks, these biases are effectively removed, resulting in more reliable results.</p><p>A key contribution of Segebarth et al. was to perform extensive experiments on real images and show that the use of consensus images and model ensembles does indeed improve objectivity, reliability and validity. This provides a recipe for optimizing the generation of training data and for making efficient use of the available data, although this recipe still requires a significant amount of human expert time since each image has to be annotated by multiple experts. The results could also help researchers trying to understand how biases affect trained networks, which could lead to improved network structures and training approaches (<xref ref-type="bibr" rid="bib4">Müller et al., 2019</xref>). And although many questions and challenges remain, the work of Segebarth et al. represents an important step forward in the effort to make the use of deep learning in bioimage analysis feasible.</p></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Communications of the ACM</source><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijering</surname> <given-names>E</given-names></name><name><surname>Carpenter</surname> <given-names>AE</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name><name><surname>Olivo-Marin</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Imagining the future of bioimage analysis</article-title><source>Nature Biotechnology</source><volume>34</volume><fpage>1250</fpage><lpage>1255</lpage><pub-id pub-id-type="doi">10.1038/nbt.3722</pub-id><pub-id pub-id-type="pmid">27926723</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Müller</surname> <given-names>R</given-names></name><name><surname>Kornblith</surname> <given-names>S</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>When does label smoothing help?</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>4694</fpage><lpage>4703</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelt</surname> <given-names>DM</given-names></name><name><surname>Sethian</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A mixed-scale dense convolutional neural network for image analysis</article-title><source>PNAS</source><volume>115</volume><fpage>254</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1073/pnas.1715832114</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-net: Convolutional networks for biomedical image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Navab</surname> <given-names>N</given-names></name><name><surname>Hornegger</surname> <given-names>J</given-names></name><name><surname>wells</surname> <given-names>W</given-names></name><name><surname>Frango</surname> <given-names>A</given-names></name></person-group><source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source><publisher-name>Springer</publisher-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Segebarth</surname> <given-names>D</given-names></name><name><surname>Griebel</surname> <given-names>M</given-names></name><name><surname>Stein</surname> <given-names>N</given-names></name><name><surname>R von Collenberg</surname> <given-names>C</given-names></name><name><surname>Martin</surname> <given-names>C</given-names></name><name><surname>Fiedler</surname> <given-names>D</given-names></name><name><surname>Comeras</surname> <given-names>LB</given-names></name><name><surname>Sah</surname> <given-names>A</given-names></name><name><surname>Schoeffler</surname> <given-names>V</given-names></name><name><surname>Lüffe</surname> <given-names>T</given-names></name><name><surname>Dürr</surname> <given-names>A</given-names></name><name><surname>Gupta</surname> <given-names>R</given-names></name><name><surname>Sasi</surname> <given-names>M</given-names></name><name><surname>Lillesaar</surname> <given-names>C</given-names></name><name><surname>Lange</surname> <given-names>MD</given-names></name><name><surname>Tasan</surname> <given-names>RO</given-names></name><name><surname>Singewald</surname> <given-names>N</given-names></name><name><surname>Pape</surname> <given-names>HC</given-names></name><name><surname>Flath</surname> <given-names>CM</given-names></name><name><surname>Blum</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the objectivity, reliability, and validity of deep learning enabled bioimage analyses</article-title><source>eLife</source><volume>9</volume><elocation-id>e59780</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59780</pub-id><pub-id pub-id-type="pmid">33074102</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname> <given-names>N</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Salakhutdinov</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref></ref-list></back></article>