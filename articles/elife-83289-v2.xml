<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83289</article-id><article-id pub-id-type="doi">10.7554/eLife.83289</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Model discovery to link neural activity to behavioral tasks</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-292178"><name><surname>Costabile</surname><given-names>Jamie D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa1">‡</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-292179"><name><surname>Balakrishnan</surname><given-names>Kaarthik A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-292180"><name><surname>Schwinn</surname><given-names>Sina</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa2">§</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-158431"><name><surname>Haesemeyer</surname><given-names>Martin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2704-3601</contrib-id><email>haesemeyer.1@osu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Department of Neuroscience, The Ohio State University College of Medicine</institution><addr-line><named-content content-type="city">Columbus</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Interdisciplinary Biophysics Graduate Program</institution><addr-line><named-content content-type="city">Columbus</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Clark</surname><given-names>Damon A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>‡</label><p>Hitachi Solutions America, Ltd, Irvine, United States</p></fn><fn fn-type="present-address" id="pa2"><label>§</label><p>Mass Eye and Ear, Massachusetts General Brigham, Boston, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>06</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e83289</elocation-id><history><date date-type="received" iso-8601-date="2022-09-06"><day>06</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-06-05"><day>05</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-09-03"><day>03</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.31.506108"/></event></pub-history><permissions><copyright-statement>© 2023, Costabile, Balakrishnan et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Costabile, Balakrishnan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83289-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83289-figures-v2.pdf"/><abstract><p>Brains are not engineered solutions to a well-defined problem but arose through selective pressure acting on random variation. It is therefore unclear how well a model chosen by an experimenter can relate neural activity to experimental conditions. Here, we developed ‘model identification of neural encoding (MINE).’ MINE is an accessible framework using convolutional neural networks (CNNs) to discover and characterize a model that relates aspects of tasks to neural activity. Although flexible, CNNs are difficult to interpret. We use Taylor decomposition approaches to understand the discovered model and how it maps task features to activity. We apply MINE to a published cortical dataset as well as experiments designed to probe thermoregulatory circuits in zebrafish. Here, MINE allowed us to characterize neurons according to their receptive field and computational complexity, features that anatomically segregate in the brain. We also identified a new class of neurons that integrate thermosensory and behavioral information that eluded us previously when using traditional clustering and regression-based approaches.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>method</kwd><kwd>model-discovery</kwd><kwd>computation</kwd><kwd>thermoregulation</kwd><kwd>calcium imaging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5R01NS123887-02</award-id><principal-award-recipient><name><surname>Costabile</surname><given-names>Jamie D</given-names></name><name><surname>Balakrishnan</surname><given-names>Kaarthik A</given-names></name><name><surname>Haesemeyer</surname><given-names>Martin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006928</institution-id><institution>The Ohio State University Wexner Medical Center</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schwinn</surname><given-names>Sina</given-names></name><name><surname>Haesemeyer</surname><given-names>Martin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Model identification of neural encoding is an accessible system for the analysis of neural data that allows identifying and characterizing arbitrary relationships between neural activity and task-related variables such as behavior, stimuli, or internal states.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Contemporary neuroscience generates large datasets of neural activity in behaving animals (<xref ref-type="bibr" rid="bib16">Engert, 2014</xref>; <xref ref-type="bibr" rid="bib54">Musall et al., 2019b</xref>; <xref ref-type="bibr" rid="bib86">Urai et al., 2022</xref>). To gain insight from these large-scale recordings, it is desirable to identify neurons with activity related to the behavioral task at hand. A common approach to this problem is to intuit a functional form (‘a model’) that relates predictors such as sensory stimuli, motor actions, and internal states to neural activity. Neurons can subsequently be classified into those with activity explained by features of the task and the chosen model and those with activity likely to be unrelated or background noise. A simple yet powerful approach is to use linear regression to explain neural activity as the weighted sum of sensory and motor features recorded during the experiment (<xref ref-type="bibr" rid="bib50">Miri et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib63">Portugues et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>). Since linear regression can accommodate nonlinear transformations of input variables, this technique can encompass diverse relationships between predictors and neural activity. Similarly, more flexible solutions using basis functions (<xref ref-type="bibr" rid="bib62">Poggio, 1990</xref>; <xref ref-type="bibr" rid="bib31">Hastie et al., 2009</xref>) can be used to identify task-related neurons. However, brains are not engineered solutions to a well-defined problem but arose through selective pressure acting on random variation (<xref ref-type="bibr" rid="bib15">Eliasmith and Anderson, 2002</xref>; <xref ref-type="bibr" rid="bib55">Niven and Laughlin, 2008</xref>; <xref ref-type="bibr" rid="bib89">Zabihi et al., 2021</xref>). It is therefore unclear how well neural activity can be captured by a well-defined function chosen by the experimenter as the test model.</p><p>Artificial neural networks (ANNs) can in principle accommodate any mapping of predictors to neural activity (<xref ref-type="bibr" rid="bib33">Hornik et al., 1989</xref>; <xref ref-type="bibr" rid="bib24">Gorban and Wunsch, 1998</xref>). At the same time, they can be designed to generalize well to untrained inputs (<xref ref-type="bibr" rid="bib4">Anders and John, 1991</xref>; <xref ref-type="bibr" rid="bib80">Srivastava et al., 2014</xref>) often overcoming problems related to explosion of variance and overfitting to training data associated with other solutions incorporating large numbers of parameters (<xref ref-type="bibr" rid="bib31">Hastie et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">James et al., 2013</xref>). Due to this flexibility, insights into nonlinear receptive fields of visual and auditory neurons (<xref ref-type="bibr" rid="bib40">Lehky et al., 1992</xref>; <xref ref-type="bibr" rid="bib39">Lau et al., 2002</xref>; <xref ref-type="bibr" rid="bib64">Prenger et al., 2004</xref>; <xref ref-type="bibr" rid="bib85">Ukita et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Keshishian et al., 2020</xref>) and into the encoding of limb motion in somatosensory cortex have been gained using ANNs (<xref ref-type="bibr" rid="bib41">Lucas et al., 2019</xref>). However, an obvious drawback of ANNs is that they are much harder to interpret than models based on intuition and data exploration.</p><p>Here, we introduce ‘model identification of neural encoding’ (MINE). MINE combines convolutional neural networks (CNNs) to learn mappings from predictors (stimuli, behavioral actions, internal states) to neural activity (Figure 1) with a deep characterization of this relationship. This allows discovering a model or functional form from the data that relates predictors to activity and to subsequently describe this model, thereby inverting the usual approach. Using Taylor expansion approaches, MINE reveals the computational complexity such as the nonlinearity of the relationship (Figure 2), characterizes receptive fields as indicators of processing (Figure 3), and reveals on which specific predictors or their interactions the neural activity depends (Figure 4). By incorporating a convolutional layer, temporal or spatial transformations of inputs introduced by the technique (such as calcium indicator effects) or by the brain (such as differentiation of a signal, edge detection) will be learned seamlessly by MINE. These transformations therefore do not have to be captured through a priori transformations of the task variables. While the architecture and hyper-parameters of the CNN used by MINE impose limits on which relationships can be modeled, we consider the convolutional network largely ‘model-free’ because it does not make any explicit assumptions about the underlying probability distributions or functional forms of the data.</p><p>Here, we demonstrate the utility of MINE using a ground-truth dataset (Figures 1–4) and a cortical mouse widefield imaging dataset (Figure 5). We then designed a set of experiments to exhaustively probe thermoregulatory circuits in larval zebrafish (Figures 6 and 7). Specifically, we exploit the flexibility of MINE to provide randomly varied temperature stimuli across zebrafish and imaging planes while maintaining the ability to identify functional groups of neurons based on features of the trained CNNs. Using MINE, we discover a new functional class of neurons integrating thermosensory with behavioral information. Combining MINE with anatomical analysis, we also map functional features derived with MINE to a standard zebrafish brain.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A model discovery approach to identify task-relevant neurons</title><p>MINE uses CNNs to overcome the challenges with predefined models while maintaining interpretability. Feed-forward ANNs are capable of approximating any function (<xref ref-type="bibr" rid="bib12">Cybenko, 1989</xref>; <xref ref-type="bibr" rid="bib33">Hornik et al., 1989</xref>; <xref ref-type="bibr" rid="bib24">Gorban and Wunsch, 1998</xref>) and therefore afford great flexibility in capturing the relationship between ‘predictors’ (sensory stimuli, behavioral actions, internal states, etc.) and the activity of individual neurons. We designed a simple three-layered network architecture (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). It consists of a linear convolutional layer (to accommodate temporal transformations such as calcium indicator effects) and two dense layers that capture nonlinear transformations. The architecture’s simplicity speeds up training and eases interpretation while capturing transformations across time (i.e. the convolutional layers) and nonlinear effects (i.e. dense layers with nonlinear activations functions). We chose a continuously differentiable activation function for our dense network layers. Unlike the popular ReLu nonlinearity, this allows us to calculate higher-order derivatives that capture interaction effects and that allow us to quantify the computational complexity of transformations. Most network hyperparameters including the specific activation function (‘swish,’ <xref ref-type="bibr" rid="bib66">Ramachandran et al., 2017</xref>) were determined by minimizing test error on a small dataset (see ‘Methods’). We chose a fixed length of 10 s for our convolutional filters, which can be adjusted as needed, to capture temporal effects in all subsequent experiments.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Model identification of neural encoding (MINE) accommodates a large set of predictor–activity relationships.</title><p>(<bold>A</bold>) Schematic of the convolutional neural network (CNN) used. (<bold>B</bold>) The predictors that make up the ground-truth dataset. S1 and S2 are continuously variable predictors akin to sensory variables while M1 and M2 are discrete in time, akin to motor or decision variables. Dashed lines indicate a third each of the data with the first two-thirds used for training of models and the last third used for testing. (<bold>C</bold>) Schematic representation of ground-truth response generation. (<bold>D</bold>) Example responses in the ground-truth dataset. Labels on the right refer to the response types shown in (<bold>C</bold>). (<bold>E</bold>) The model predicts activity at time <inline-formula><mml:math id="inf1"><mml:mi>t</mml:mi></mml:math></inline-formula> using predictors across time <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>t</mml:mi><mml:mo mathvariant="normal">-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf3"><mml:mi>t</mml:mi></mml:math></inline-formula> as inputs. The schematic shows how this is related to the generation of training and test data. Top inset shows development of training and test error across training epochs for 20 models trained on the R(S1) × R(S2) response type. Bottom inset shows example prediction (orange) overlaid on response (dark green). (<bold>F</bold>) Squared correlation to test data for a simple linear regression model (blue), a linear regression model including first-order interaction terms and calcium kernel convolution (red), as well as the CNN fit by MINE (orange). Each dot represents the average across 20 models. While the standard deviation is represented by a dash, it is smaller than the dot size in all cases and therefore not visible in the graph.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Model identification of neural encoding (MINE) accommodates a large set of predictor–activity relationships.</title><p>(<bold>A</bold>) Heatmap of all generated responses in the ground-truth dataset 220 total across 11 groups including pure noise responses. (<bold>B</bold>) Schematic of the expanded linear model that includes all first-order interaction terms and convolution with the true ‘calcium kernel’ used in the ground-truth dataset. We note, however, that since multiplication is a nonlinear operation, the generation of the interaction terms after convolution is not exactly the same as the generation of the interaction terms in the ground-truth dataset (where convolution occurs after the multiplication). (<bold>C</bold>) For data not shown in <xref ref-type="fig" rid="fig1">Figure 1F</xref>, squared correlation with test data for a simple linear regression model (blue), a linear regression model including first-order interaction terms and calcium kernel convolution (red), as well as the convolutional neural network (CNN) model fit by MINE (orange). Each dot represents the average across 20 models. While the standard deviation is represented by a dash, it is smaller than the dot size in all cases and therefore not visible in the graph.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig1-figsupp1-v2.tif"/></fig></fig-group><p>To test MINE’s ability to capture neural transformations and encoding of predictors, we generated a ground-truth dataset. This dataset consists of four randomly generated predictors (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Two of these, S1 and S2, vary continuously in time mimicking sensory stimuli. The other two, M1 and M2, are discrete like motor or decision variables. From these predictors, we generated ‘neural responses’ that depend on single predictors or interactions with and without intervening nonlinear transformations (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). We added Gaussian noise to all neural responses after convolution with a calcium kernel to approach conditions that might be observed in functional calcium imaging experiments (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). For each neuron in our ground-truth dataset, we subsequently trained a CNN to predict the activity from the predictors (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). To assess generalization, we split our dataset into a training set (two-thirds of the data) and a validation set (last third) (<xref ref-type="fig" rid="fig1">Figure 1B and D</xref>). Training for 100 epochs led to excellent predictions of ‘neural activity’ while maintaining generalizability as assessed by the squared correlation (<inline-formula><mml:math id="inf4"><mml:msup><mml:mi mathsize="80%">r</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> value) to the test data (<xref ref-type="fig" rid="fig1">Figure 1E and F</xref>).</p><p>We sought to compare our ANN-based approach to another widespread approach to model single-neuron activity, linear regression. Using the four predictors (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) as inputs, a simple linear regression model fails to explain activity in most cases (<xref ref-type="fig" rid="fig1">Figure 1F</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>). This is expected since the chosen type of linear regression model cannot learn the dynamics of the calcium indicator unlike the CNN. We therefore constructed an alternative model in which we convolved the predictors with the known ‘calcium kernel’ (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). In this expanded linear model, we also included all first-order interaction terms by including pairwise products between predictors (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). This type of model, capturing interactions and accounting for an estimated indicator effect, is popular in the analysis of large-scale calcium imaging datasets (<xref ref-type="bibr" rid="bib50">Miri et al., 2011</xref>; <xref ref-type="bibr" rid="bib3">Ahrens et al., 2012</xref>; <xref ref-type="bibr" rid="bib63">Portugues et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Stringer et al., 2019</xref>). As expected, this model matches the performance of the CNN in more response categories including nonlinear interactions (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). However, the function of this model was designed using a posteriori knowledge about the responses. Nonetheless, the expanded linear model is poor in capturing some nonlinear transformations of predictors and fails to capture responses that relate to the time derivative of an input, for example, as expected in adapting neurons (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). While other models could clearly be designed to overcome these challenges, this further illustrates the point that a model-based approach is limited to the constraints of the chosen model.</p><p>As shown, MINE can identify responses that depend on predictors independent of the linearity of these relationships. The underlying CNN is able to learn temporal transformations of inputs such as shifts in time or convolutions. Otherwise these transformations have to be explicitly provided to a regression model or the predictor matrix has to be augmented for the model to implicitly learn them (see the comparison model used in the analysis of zebrafish data below) (<xref ref-type="bibr" rid="bib50">Miri et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>). MINE removes the requirement of estimating calcium response kernels that might differ across different neurons and can also identify responses that depend on derivatives of the input. This means that one predictor such as position can be used to identify neurons that depend on the velocity or acceleration of a stimulus, without the need of augmenting the predictor matrix.</p></sec><sec id="s2-2"><title>MINE characterizes computational complexity</title><p>Linear computations are limited in their expressivity, and it is generally believed that the computational power of the nervous system depends on nonlinear computation (<xref ref-type="bibr" rid="bib34">Hubel and Wiesel, 1968</xref>; <xref ref-type="bibr" rid="bib11">Churchland et al., 1994</xref>; <xref ref-type="bibr" rid="bib9">Carandini et al., 2005</xref>). In spite of this, the responses of many neurons tend to depend almost linearly on sensory or behavioral features (<xref ref-type="bibr" rid="bib50">Miri et al., 2011</xref>; <xref ref-type="bibr" rid="bib84">Thompson et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Pho et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>). The latter idea aligns with the hypothesis that information important to the animal should be linearly decodable by neural circuits (<xref ref-type="bibr" rid="bib43">Marder and Abbott, 1995</xref>; <xref ref-type="bibr" rid="bib15">Eliasmith and Anderson, 2002</xref>; <xref ref-type="bibr" rid="bib78">Shamir and Sompolinsky, 2006</xref>). Disambiguating linear from nonlinear processing therefore provides important insight into circuit structure and function. Once fit to neural data, our CNNs model the transformations from predictors to neural activity (or from neural activity to action). We therefore set out to quantify the complexity of the function these networks implement as a proxy to classifying the actual transformations between stimuli, neural activity, and behavior.</p><p>To arrive at a metric of ‘computational complexity,’ we used Taylor expansion to approximate the function implemented by the CNN. The Taylor expansion approximates a function, such as the CNN, around a specific input by a polynomial of up to infinite order. The weights of individual terms in this polynomial are determined by the values of the derivatives of the CNN output with respect to each input element (in our case predictors and timepoints). These derivatives describe the expected change in the output of the CNN given a small change in the predictor input. Calculating derivatives of increasing order at the point of expansion allows predicting the value of the CNN output at other points (akin to predicting the position of a car in the near future based on its current location, velocity, and acceleration). We specifically compute the first- and second-order partial derivatives of the CNN output with respect to each feature (predictor and timepoint) in the input at the average of the training data. This allows formulating the Taylor expansion of the network around the data mean. It is then possible to compare the quality of Taylor expansions with variable numbers of terms in predicting the true network output. If the network were to implement a linear function, the first-order term (the gradient <inline-formula><mml:math id="inf5"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula> of the function with respect to the input) should suffice to explain a large fraction of the variance in the output (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Nonlinear functions should depend on higher-order terms such as the second-order partial derivatives, <inline-formula><mml:math id="inf6"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula>. We chose to define ‘computational complexity’ based on the requirement of the second- or higher-order terms in the Taylor expansion (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Specifically, we assign complexity 0 if the linear term in the expansion is enough to explain the activity, 1 if the quadratic term is needed, and 2 if higher-order terms are required. Nonlinear neurons are therefore split into two categories, depending on the complexity of the transformation.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Truncations of the Taylor expansion assign computational complexity.</title><p>(<bold>A</bold>) Schematic of the approach. At the data mean, the output of the network is differentiated with respect to the inputs. The first-order derivative (gradient J) and the second-order derivatives (Hessian H) are computed at this point. Comparing the output of truncations of the Taylor expansion can be used to assess the computational complexity of the function implemented by the convolutional neural network (CNN).For example, if the function is linear, it would be expected that a truncation after the linear term explains the vast majority of the variance in the true network output. (<bold>B</bold>) Mixing varying degrees of a nonlinear response function with a linear response (‘Nonlinear contribution’) and its effect on network performance (left, squared correlation to test data), the variance explained by truncation of the Taylor series after the linear term (middle) and the variance explained for a truncation after the second-order term (right). Colored dots relate to the plots of linear correlations in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. (<bold>C</bold>) As in (<bold>B</bold>) but mixing a linear function of a predictor with a linear transformation of the predictor, namely the first-order derivative. (<bold>D</bold>) As in (<bold>B</bold>) but mixing a linear function of a predictor with a rectified (nonlinear) version. (<bold>E</bold>) ROC plot, revealing the performance of a classifier of nonlinearity that is based on the variance explained by the truncation of the Taylor series after the linear term across 500 independent generations of linear/nonlinear mixing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Truncations of the Taylor expansion assign computational complexity.</title><p>(<bold>A–C</bold>) Linear correlations between input predictors and ground-truth data ‘neural activity’ for different mixtures. Colors of correlation plots correspond to colored examples in <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>A</bold>) Mixing of a nonlinear <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>y</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mi mathvariant="normal">tanh</mml:mi><mml:msup><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mi>x</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and linear transformation of the predictor. (<bold>B</bold>) Mixing of a predictor and its derivative. (<bold>C</bold>) Mixing of a predictor and a rectified <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>y</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mi>n</mml:mi><mml:mo mathvariant="italic">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msup><mml:mo mathvariant="normal">+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> version of the predictor. (<bold>D</bold>) Relationship of the linear model score (coefficient of determination of the linear truncation of the Taylor series) and the average curvature of the function implemented by the convolutional neural network (CNN) (see ‘Methods’). Data divided into 50 even-sized quantiles based on the linear model score. Band is bootstrap standard error across all data points within the quantile (N=10 datapoints in each quantile). (<bold>E</bold>) Same as (<bold>D</bold>) but relationship to the nonlinearity coefficient. (<bold>F</bold>) For different thresholds of the linear model score, the fraction of false negative (blue) and false positive (orange) identifications of nonlinearity on the ground-truth data (N=500 independent simulations). Note: outputs are recentered to 0 mean and 1 standard deviation before being passed to the CNN and before being plotted here.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig2-figsupp1-v2.tif"/></fig></fig-group><p>We tested these metrics on ground-truth data in the following manner: we mixed predictors to varying degrees with either linear (as a control) or nonlinear transformations of the same predictors. By increasing the contribution of the transformed predictor, we thereby inject varying amounts of nonlinearity into the response. We then applied MINE, training a CNN to learn the transformation between the predictor and the mixed output and calculating the coefficients of determination (<inline-formula><mml:math id="inf9"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula>) for truncations of the Taylor series after the linear as well as the second-order term. Increasing the contribution of an arbitrary nonlinear function (i.e. <inline-formula><mml:math id="inf10"><mml:mrow><mml:msup><mml:mi mathsize="80%">tanh</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:msup><mml:mi mathsize="80%">x</mml:mi><mml:mn mathsize="80%">3</mml:mn></mml:msup><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) leads to a marked decrease in both of these <inline-formula><mml:math id="inf11"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> values (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Importantly, these metrics do not simply quantify a loss of linear correlation. Calculating the derivative of a function is a linear operation, and increasing the contribution of a derivative of the predictor indeed does not decrease either <inline-formula><mml:math id="inf12"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> value in spite of reducing linear correlation between the predictor and the response (<xref ref-type="fig" rid="fig2">Figure 2C</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A–C</xref>). When increasing the contribution of a rectified linear version of the predictor, the amount of variance explained by the linear truncation drops while this is not the case for the truncation after the second-order term (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). This is in contrast to the contribution of <inline-formula><mml:math id="inf13"><mml:mrow><mml:msup><mml:mi mathsize="80%">tanh</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:msup><mml:mi mathsize="80%">x</mml:mi><mml:mn mathsize="80%">3</mml:mn></mml:msup><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> above and justifies our interpretation of computational complexity as a metric for how much a relationship diverges from linearity.</p><p>To quantify its usefulness to distinguish linear and nonlinear transformations, we systematically evaluated the classification performance of the fraction of variance explained by the linear truncation of the Taylor expansion. To this end, we generated random linear and nonlinear transformations of the predictors (see ‘Methods’), trained the CNN, and calculated the <inline-formula><mml:math id="inf14"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> value of the linear truncation. We compared this <inline-formula><mml:math id="inf15"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> value to two other metrics of nonlinearity: the curvature of the function implemented by the network and the nonlinearity coefficient <xref ref-type="bibr" rid="bib58">Philipp and Carbonell, 2018</xref>; <xref ref-type="bibr" rid="bib59">Philipp, 2021</xref> of the network (see ‘Methods’). Notably, a decrease in the explained variance by the linear truncation led on average to increases in both these metrics of nonlinearity (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D and E</xref>). To quantify classification performance, we used ROC analysis. This analysis revealed that the <inline-formula><mml:math id="inf16"><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> value ranks a nonlinear transformation lower than a linear one in 99% of cases (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Importantly, this metric allows disambiguating linear and nonlinear processing with acceptable false-positive and false-negative rates (<inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) at a cutoff of <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> as a nonlinearity threshold <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1F</xref>; a feature that is highly desirable when applying the classification to real-world data.</p><p>In summary, MINE increases the interpretability of the CNN model and classifies the transformations the model encapsulates according to their computational complexity. Since the CNN encapsulates the transformations that are enacted by neural circuits between stimuli and neural activity or neural activity and behavior, this information provides important insight into the computations that give rise to neural activity.</p></sec><sec id="s2-3"><title>MINE characterizes neural receptive fields</title><p>Receptive fields compactly describe stimulus features that drive a neuron’s response (<xref ref-type="bibr" rid="bib13">Dayan and Abbott, 2001</xref>). They reveal integration times (those times over which coefficients of the receptive field are different from zero) and zero-crossings within the receptive field signal adapting responses that indicate that a neuron encodes the derivative of a stimulus across time (e.g. velocity encoding neurons) or that it detects edges across space. It is therefore desirable to use MINE to extract receptive fields of the neurons fit by the CNN. Because of their descriptive nature, different methods have been developed to extract receptive fields, commonly referred to as ‘system identification’ approaches. The Wiener/Volterra expansion of functions provides a powerful framework for system identification (<xref ref-type="bibr" rid="bib61">Poggio and Reichardt, 1973</xref>; <xref ref-type="bibr" rid="bib2">Aertsen and Johannesma, 1981</xref>; <xref ref-type="bibr" rid="bib21">Friston et al., 1998</xref>; <xref ref-type="bibr" rid="bib42">Mammano, 1990</xref>; <xref ref-type="bibr" rid="bib46">Marmarelis, 2004</xref>; <xref ref-type="bibr" rid="bib51">Mitsis et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Mitsis, 2011</xref>). Since the input to the CNN at the heart of MINE contains information about each predictor across time, the Taylor expansion introduced in the previous section (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) is equivalent to the Volterra expansion of a system processing information across time equal to the history length of the CNN. We were therefore wondering whether we could extract receptive fields from the gradient <inline-formula><mml:math id="inf19"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula> and Hessian <inline-formula><mml:math id="inf20"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula> of the CNN in a manner similar to how they can be derived from the first- and second-order Volterra kernels (<inline-formula><mml:math id="inf21"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula> ; see ‘Methods’) (<xref ref-type="bibr" rid="bib45">Marmarelis, 1997</xref>; <xref ref-type="bibr" rid="bib46">Marmarelis, 2004</xref>; <xref ref-type="bibr" rid="bib51">Mitsis et al., 2007</xref>). To this end, we simulated a system that uses two parallel receptive fields to process an input. The filtered responses are subsequently passed through two differently structured nonlinearities to yield the output (<xref ref-type="fig" rid="fig3">Figure 3A</xref> and ‘Methods’). Notably, due to the structure of the nonlinearities, we expect the first receptive field (here called ‘linear receptive field’) to be equivalent to <inline-formula><mml:math id="inf23"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula>/<inline-formula><mml:math id="inf24"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula> and the second receptive field (here called ‘nonlinear receptive field’) to appear as an eigenvector of <inline-formula><mml:math id="inf25"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model identification of neural encoding (MINE) characterizes linear and nonlinear receptive fields.</title><p>(<bold>A</bold>) Schematic of the test response generation. Inputs S (either white-noise or slow fluctuating) are convolved in parallel with two receptive fields acting as filters. The result of one convolution is transformed by an asymmetric nonlinearity (top), the other through a symmetric one (bottom). The results of these transformations are summed to create the response R that is a stand-in for a neural response that depends on one linear and one nonlinear receptive field. (<bold>B</bold>) When presenting a slowly varying stimulus, the quality of receptive fields extracted by MINE (expressed as the cosine similarity between the true receptive fields and the respective receptive fields obtained by the analysis), as well as direct fitting of first- and second-order Volterra kernels through linear regression (OLS) as well as Ridge regression. Listed <inline-formula><mml:math id="inf27"><mml:mi>α</mml:mi></mml:math></inline-formula> indicates the strength of the Ridge penalty term. Linear receptive field blue, nonlinear orange. Dashed lines indicate median cosine similarity of the receptive fields extracted using MINE. (<bold>C</bold>) Correlation to validation data presented to the fit models as a means to assess generalization. Dashed lines indicate correlation to test data of the MINE model. (<bold>D–F</bold>) Same as (<bold>A–C</bold>) but with smoothly varying receptive fields. All data is across 100 indepedent simulations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Model identification of neural encoding (MINE) characterizes linear and nonlinear receptive fields.</title><p>(<bold>A</bold>) Comparison of example Gaussian white noise (gray) and random wave stimulus (red). (<bold>B</bold>) When presenting a Gaussian white noise stimulus, the quality of receptive fields extracted by MINE (expressed as the cosine similarity between the true receptive fields and the respective receptive fields obtained by the analysis), as well as direct fitting of first- and second-order Volterra kernels through linear regression (OLS). Linear receptive field blue, nonlinear orange. Note that for both MINE and OLS the receptive fields have near perfect similarity and boxes therefore collapse onto the median (black lines). N=10 independent simulations. (<bold>C–H</bold>) For MINE (<bold>C–D</bold>) and two Ridge regression models with indicated penalties (<inline-formula><mml:math id="inf28"><mml:mi>α</mml:mi></mml:math></inline-formula>) visual comparison between the true receptive fields used in the simulation (black) and the extracted linear (blue lines) and nonlinear (orange lines) receptive fields. Thin lines are individual simulations, thick lines represent the average. N=100 independent simulations. (<bold>I</bold>) For MINE and two Ridge regression models with indicated penalties (<inline-formula><mml:math id="inf29"><mml:mi>α</mml:mi></mml:math></inline-formula>), the dependence of the predictive power (correlation of the prediction to true data on a validation set) on the number of training frames included in fitting the models. Error bars are bootstrap standard error across 50 independent simulations. (<bold>J</bold>) Same as (<bold>I</bold>) but instead of the predictive power, the dependence of the overall filter similarity (linear and nonlinear RF) on the number of training frames. Error bars are bootstrap standard error across 50 independent simulations. (<bold>K</bold>) Estimated effective degrees of freedom (see ‘Methods’) for the different models with <inline-formula><mml:math id="inf30"><mml:mi>α</mml:mi></mml:math></inline-formula> again indicating the penalty of the Ridge regression models. (<bold>L</bold>) For MINE, how the effective degrees of freedom are determined by the number of training epochs (blue vs. orange) and the presence/absence of Dropout and the L1 weight constraint.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig3-figsupp1-v2.tif"/></fig></fig-group><p>As a comparison, we used regression to directly fit the Volterra kernels (see ‘Methods’). Notably, just like MINE, apart from the truncation of the series after the second term, the Volterra analysis is highly flexible since most scalar-valued functions can be approximated using an infinite Volterra series (<xref ref-type="bibr" rid="bib87">Volterra, 1959</xref>). System identification often employs specifically designed stimuli such as Gaussian white noise (<xref ref-type="bibr" rid="bib44">Marmarelis and Marmarelis, 1978</xref>; <xref ref-type="bibr" rid="bib38">Korenberg and Hunter, 1990</xref>; <xref ref-type="bibr" rid="bib68">Rieke et al., 1999</xref>; <xref ref-type="bibr" rid="bib76">Schwartz et al., 2006</xref>; <xref ref-type="bibr" rid="bib23">Gollisch and Meister, 2008</xref>), which are difficult to realize in practice and severely restrict experimental conditions. Nonetheless, we first benchmarked both approaches using Gaussian white noise as input (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). As expected, both MINE and directly fitting the Volterra kernels yielded receptive fields that were nearly indistinguishable from the ground truth (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). This demonstrates that our simulation indeed results in receptive fields that can be discovered using MINE or a second-order Volterra model. However, for the analysis to be useful it is critical that receptive fields can be extracted on arbitrary and slowly varying stimuli as expected, for example, during naturalistic behavior. We therefore repeated the analysis using slowly varying stimuli (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). Under these more naturalistic conditions, MINE still yielded well-fitting receptive fields (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C and D</xref>). An ordinary regression fit of the Volterra kernels, on the other hand, failed to recover the receptive fields (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), which is in line with the observation that ANNs form an efficient route to Volterra analysis (<xref ref-type="bibr" rid="bib88">Wray and Green, 1994</xref>). We assumed that this failure was due to a lack of constraints, which meant that the ordinary regression model could not handle the departure from Gaussian white noise stimuli. We therefore refit the Volterra kernels using Ridge regression (<xref ref-type="bibr" rid="bib31">Hastie et al., 2009</xref>) with increasing penalties. Indeed, for high penalties, the direct fit of the Volterra kernels yielded receptive fields of almost comparable quality to MINE (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1E–H</xref>). The CNN model fit by MINE also had slightly higher predictive power compared to the best-fit Volterra model as indicated by higher correlations of predicted activity on a test dataset (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Repeating the same analysis with a second set of filters yielded similar results: MINE proved to be a slightly superior approach to extract the receptive fields (<xref ref-type="fig" rid="fig3">Figure 3D–F</xref>). This is not to say that no other way could be found to extract the receptive fields, but it underlines the fact that MINE is a practical solution to this problem, which yields high-quality receptive fields in addition to other information about the relationship between predictors and neural responses.</p><p>We used the same simulation to assess how the predictive power of the different models (MINE and Ridge regression), as well as the quality of extracted receptive fields, depends on the amount of training data. As expected, predictive power on a validation set increases with the number of training samples for all methods. The Ridge model with the lower penalty outperforms MINE for lower numbers of training samples (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1I</xref>). However, this model does not yield usable receptive fields and in fact both Ridge models show a deterioration of extracted receptive fields for large numbers of training samples (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1J</xref>). This is likely a result of the competition between the regularization penalty and the overall error of the fit. MINE appears more robust to this effect, but for very long sample lengths it may be required to adjust the CNNs regularization as well (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1J</xref>).</p><p>Since Ridge regression constrains linear regression models, we were wondering how the effective degrees of freedom of the CNN and the different models would compare. Interestingly, in spite of having nearly 14,000 parameters, the effective degrees of freedom of the CNN model are &lt;50 and the Ridge regression models that are successful in identifying the receptive fields approach similar effective degrees of freedom (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1K</xref>). This is in line with successful approaches to system identification employing constraints such as Laguerre basis functions (<xref ref-type="bibr" rid="bib21">Friston et al., 1998</xref>; <xref ref-type="bibr" rid="bib46">Marmarelis, 2004</xref>; <xref ref-type="bibr" rid="bib51">Mitsis et al., 2007</xref>). In the case of the CNN used by MINE, the effective degrees of freedom are limited by the L1 penalty on the weights (sparsity constraint), the Dropout, and the limited number of training epochs (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1L</xref>).</p><p>Overall these results suggest that MINE can recover the types of receptive fields of neurons that can be obtained with system identification approaches under a broader set of biologically relevant stimulus conditions.</p></sec><sec id="s2-4"><title>Taylor analysis identifies predictors driving the response</title><p>Compared to regression models, the CNNs seemingly have a drawback: a lack of interpretability. Statistical methods can identify the factors that significantly contribute to a regression model’s output. Similarly, the magnitude of individual weights in models fit to data can give an idea of the importance of specific predictors. Corresponding overt parameters do not exist in the CNN model. In principle, the extracted receptive fields could be used to identify contributing predictors since it would be expected that they are ‘unstructured’ for unimportant inputs. However, what to consider as ‘unstructured’ is not clearly defined. Theoretically, it would be possible to refit successful CNN models in a stepwise manner, leaving out or adding in specific predictors (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2018</xref>). However, since we are interested in uncovering the relationships between predictors, this could only succeed if all individual combinations between inputs are tested. This would be prohibitive for large sets of sensory and behavioral predictors as it would require repeatedly retraining all networks.</p><p>We therefore again utilized Taylor expansion. To account for the relationships that cannot be sufficiently explained by the second-order Taylor expansion shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, we perform local expansions at various points instead of one expansion around the data average (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This allows for variation in the network gradient <inline-formula><mml:math id="inf31"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula> and Hessian <inline-formula><mml:math id="inf32"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula> in cases of high computational complexity. Since the Taylor decomposition is a sum of terms that depend on individual inputs or their combinations, we can use it to determine how important individual predictors, such as sensory stimuli, are in shaping the neural response (<xref ref-type="fig" rid="fig4">Figure 4A</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). Across our ground-truth dataset, we find a high correlation between the change in output of the Taylor expansion and the true change in network output (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, left panel), indicating that the local expansions using first- and second-order derivatives sufficiently capture the relationship between predictors and neural activity. Importantly, decomposition is a way to efficiently test the importance of different predictors in contributing to network output and hence neural activity. We calculate a ‘Taylor metric’ score, which measures the fraction of explained variance of the response that is lost when terms are removed from the full Taylor expansion (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Taylor decomposition reveals contributing single factors and interactions.</title><p>(<bold>A</bold>) The neural model translates predictors into neural activity. By Taylor decomposition, the function implemented by the convolutional neural network (CNN) can be linearized locally. Relating changes in predictors to changes in activity for full and partial linearizations reveals those predictors and interactions that contribute to neural activity. (<bold>B</bold>) Example of Taylor metric computation. Left: relationship between the CNN output and the full Taylor approximation. Middle: after removal of the term that contains the S1 predictor. Right: after removal of the term that describes the interaction between S1 and S2. (<bold>C–E</bold>) Three example responses and associated Taylor metrics. Red bars indicate predictors that are expected to contribute, blue bars those that should not contribute. Error bars are 95% bootstrap confidence intervals across N=20 independent simulations. (<bold>C</bold>) M2 response type. (<bold>D</bold>) R(S1) × R(S2) response type. Arrowheads indicate the metrics that are shown in the example (right and middle) of (<bold>B</bold>). (<bold>E</bold>) Response type encoding the absolute value of S1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Taylor decomposition reveals contributing single factors and interactions.</title><p>(<bold>A</bold>) Schematic highlighting key parts of the decomposition that isolates individual factors. (<bold>B–H</bold>) Remaining responses and associated Taylor metrics. Red bars indicate predictors that are expected to contribute, blue bars those that should not contribute. Error bars are 95% bootstrap confidence intervals across N=20 independent simulations. (<bold>B</bold>) Response type only depending on S1 predictor. (<bold>C</bold>) Response type only depending on S2 predictor. (<bold>D</bold>) Response type only depending on M1 predictor. (<bold>E</bold>) Response to multiplicative interaction of S1 and S2 predictors. (<bold>F</bold>) Response to multiplicative interaction of rectified S1 and original M1 predictor. Note that the S2 × M1 term that should contribute has the lowest metric. (<bold>G</bold>) Response thresholding S2 predictor. (<bold>H</bold>) Response to the derivative of S1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig4-figsupp1-v2.tif"/></fig></fig-group><p>On our ground-truth dataset, the Taylor metric correctly identifies the contributing predictors and their interactions. Sorting individual terms by this metric consistently ranks those that we expect to contribute (<xref ref-type="fig" rid="fig4">Figure 4C–E</xref>, red bars) higher than those that should not (<xref ref-type="fig" rid="fig4">Figure 4C–E</xref>, blue bars). This is true both for individual predictors and interaction terms in the case where the response depends on the product of inputs (<xref ref-type="fig" rid="fig4">Figure 4C–E</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B-H</xref>).</p><p>In summary, MINE was able to correctly identify contributions of predictors, such as sensory stimuli or behavioral actions, to neural responses by local expansions of the trained CNNs. MINE also correctly identifies nonlinear interactions in generating the neural responses on our ground-truth dataset. This indicates that we can further reduce the lack of interpretability of the CNN models and approach the expressivity of linear regression models while maintaining the ability to model nonlinear transformations and interactions of task variables.</p></sec><sec id="s2-5"><title>MINE characterizes cortical sensorimotor processing</title><p>Encouraged by MINE’s ability to identify responses related to behavior and stimuli and its ability to characterize the nature of that relationship, we wanted to test the method on biological data. We applied MINE to a publicly available widefield calcium imaging dataset recorded in the mouse cortex (<xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>). The dataset consists of 22 task-related predictors (stimuli, reward states, instructed, and noninstructed movements) and calcium activity time series across 200 temporal components that were used to compress each session’s widefield imaging data <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref> from 13 mice. It had previously been analyzed using linear regression (<xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>). We analyzed one session from each of the 13 mice present in the dataset with MINE (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). As in the ground-truth data, we split the data into a training set (first two-thirds of the session time) and a test-set (last third). To be able to capture not only the activity caused by past stimuli but also the preparatory activity leading up to movements, we shifted the predictor traces with respect to the activity trace (see ‘Methods’). As expected, correlations between MINE predictions and true activity on the test dataset are overall smaller than on the training set; however, in the majority of cases the CNN generalizes well to the new data (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Given that many individual neurons contribute to each pixel and temporal component within this dataset, and that a lot of brain activity is likely unrelated to the behavioral task, we expect that a large fraction of variance in each component is likely unexplainable by any model that does not take internal brain states into account. We therefore chose a lenient correlation cutoff of <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi mathsize="80%">r</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0.1</mml:mn></mml:mrow></mml:math></inline-formula> for the test data (red line in <xref ref-type="fig" rid="fig5">Figure 5B</xref>) to decide that a component had been successfully fit by MINE (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>). On average, this led to the identification of &gt;91% of all components per session (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) on which Taylor metric and complexity were computed. Notably, MINE assigns low complexity to the majority of components; only 3% of fit components have a linear approximation score &lt;0.8. This means that the relationships between predictors and neural activity are largely linear (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). While this may seem surprising given the high nonlinearity of cortical processing, it is likely caused by the low resolution of the data. Each component blends the activity of hundreds of thousands of neurons. Averaging across many individual neurons that each may have their own nonlinear responses likely obfuscates nonlinear effects.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model identification of neural encoding (MINE) identifies cortical features of sensorimotor processing during a learned task.</title><p>(<bold>A</bold>) Simplified schematic of the widefield imaging experiment conducted in <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>. (<bold>B</bold>) MINE test data vs. training data correlations on 200 temporal components from 13 sessions across 13 mice. Black dashed line is identity, red dashed line is the test correlation threshold to decide that a component had been identified by MINE. (<bold>C</bold>) In each of the 13 sessions, the fraction of identified components (dots) as well as the average (bar). (<bold>D</bold>) Across 200 components each in the 13 sessions, the distribution of the linear score computed by MINE (coefficient of variation for the truncation of the Taylor expansion after the linear term as in <xref ref-type="fig" rid="fig2">Figure 2</xref>). (<bold>E</bold>) Across all components from all 13 sessions that have been identified by MINE, the Taylor metrics that were significantly larger than 0. Components (rows) have been sorted according to the predictor with the maximal Taylor metric. (<bold>F</bold>) Per-pixel Taylor metric scores for the right visual stimulus (‘rVisStim’) subtracted from those of the left visual stimulus (‘lVisStim’). A, anterior; L, left; R, right. (<bold>G</bold>) As in (<bold>F</bold>) but the sum of the visual stimulus Taylor metrics (‘lVisStim+rVisStim’) has been subtracted from the Taylor metric of the whisking predictor (‘Whisk’). (<bold>H</bold>) As in (<bold>F</bold>) but the Taylor metric for the right grab predictor (“rGrab”) has been subtracted from the Taylor metric of the left grab predictor (‘lGrab’). (<bold>I</bold>) Clustering of per-pixel receptive fields, separating excitatory (bold lines) from inhibitory responses (pale lines). Pixels were selected to only include the top 10% Taylor metrics for visual (left plot) and grab (right plot) predictors; left blue, right red. Numbers in parentheses indicate cluster size. The gray dashed line indicates time 0, that is, the time at which calcium activity is measured. Note that the sensory receptive field (visual) is exclusively in the past (future stimuli do not drive the neurons) while the motor receptive fields (grab) slightly overlap with the future, indicating that neurons ramp up activity before the behavioral action.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Model identification of neural encoding (MINE) identifies cortical features of sensorimotor processing during a learned task.</title><p>(<bold>A</bold>) Per-pixel Taylor metric scores for the right visual stimulus (‘rVisStim’) subtracted from those of the left visual stimulus (‘lVisStim’) for the 12 mice not shown in <xref ref-type="fig" rid="fig5">Figure 5F</xref>. (<bold>B</bold>) As (<bold>A</bold>) but the sum of the visual stimulus Taylor metrics subtracted from the Taylor metrics of the whisking predictor for the same 12 mice. (<bold>C</bold>) As (<bold>A</bold>) but the Taylor metric for the right grab predictor has been subtracted from the Taylor metric of the left grab predictor.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig5-figsupp1-v2.tif"/></fig></fig-group><p>Across all sessions, we found a broad representation of predictors (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). As expected from the findings of <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>, uninstructed movements appear overrepresented sporting the largest Taylor metric in more than half of the fit components. We next mapped the results of MINE back into acquisition space and recalculated the Taylor metrics for each imaging pixel for one example session. The obtained results further validate MINE’s utility on biological data. Specifically, visual stimuli are shown to contribute most strongly to responses in visual cortical areas with the expected left–right asymmetry (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). At the same time, comparing visual sensory responses to whisk responses shows that cortical regions enhanced for whisking (<xref ref-type="fig" rid="fig5">Figure 5G</xref>) correspond very well to regions marked strongly with a whisk event kernel in Figure 2 of <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>. Furthermore, instructed left and right grab events largely contribute to motor cortical regions, again with the expected left–right asymmetry (<xref ref-type="fig" rid="fig5">Figure 5H</xref>). Repeating this procedure on all sessions (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) reveals general agreement. But we note that not all sessions seem to have regions that are as clearly related to the chosen inputs according to Taylor analysis, which is especially apparent for left and right instructed grabs (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>).</p><p>We next sought to determine the receptive fields that govern stimulus processing in individual pixels (<xref ref-type="bibr" rid="bib40">Lehky et al., 1992</xref>; <xref ref-type="bibr" rid="bib13">Dayan and Abbott, 2001</xref>). We extracted the receptive fields across pixels that are strongly related to either left/right visual stimuli or left/right instructed grabs. We broadly clustered the receptive fields into two groups (see ‘Methods’) to separate excitatory and inhibitory effects. Receptive fields for left and right visual stimuli in contralateral brain regions are highly similar to each other (<xref ref-type="fig" rid="fig5">Figure 5I</xref>, left). The excitatory effect is stronger than the inhibitory effect (larger coefficients in the positive receptive fields) and both are clearly biphasic. This indicates that the events around the time of the stimulus as well as <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">∼</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathsize="80%">1.5</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%" mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the past strongly influence the activity of the visual neurons. We note that this biphasic structure mimics the linear regression kernel in Figure 3b of <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>. The visual receptive fields have essentially zero weight after the current time, which is expected since future stimuli are unlikely to influence current neural activity. The receptive fields of left and right grab neurons, on the other hand, are much sharper, indicating that these neurons influence movement over short timescales (<xref ref-type="fig" rid="fig5">Figure 5I</xref>, right). Furthermore, the grab-related receptive fields contain coefficients different from baseline for up to 100 ms into the future. This suggests preparatory activity, that is, that future movements are reflected in current neural activity.</p><p>In summary, the results presented above demonstrate the applicability of our method to biological data and the potential for identifying diverse feature sets of sensorimotor processing.</p></sec><sec id="s2-6"><title>Functional characterization of thermoregulatory circuits</title><p>Encouraged by MINE’s performance on ground-truth and mouse cortical data, we sought to use it to gain novel insight into zebrafish thermoregulatory circuits. Temporal transformations of stimuli are a notable feature of how zebrafish process temperature information. In previous research, we identified neurons and centers in the larval zebrafish brain that process temperature: specifically, neurons that compute the rate of change of the temperature stimulus (<xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref>) as well as neurons whose activity is consistent with integration of temperature fluctuations (<xref ref-type="bibr" rid="bib27">Haesemeyer et al., 2019</xref>). Due to the nature of these transformations, which were unknown a priori, simple regression-based approaches failed to identify neurons involved in temperature processing. We therefore previously resorted to clustering to identify these neurons. However, because behavior is stochastic, one particular drawback of clustering was that it precluded identifying neurons that integrate thermosensory stimuli and behavioral actions. Since MINE can learn and capture both temporal transformations and interactions, we set out to gain deeper insight into thermoregulatory processing. To this end, we used MINE to identify and characterize neurons during a thermosensory task, revealing predictors contributing to their activity, extracting their receptive fields and characterizing their computational complexity.</p><p>To classify neurons by clustering either requires presenting the same stimulus to every animal and neuron while imaging or it requires a strategy of clustering event-triggered activity. With MINE we do not need to enforce any stimulus constraint. As a result, we were able to probe a wider variety of stimulus conditions. We imaged a total of 750 planes across 25 larval zebrafish that expressed the nuclear calcium indicator H2B:GCaMP6s <xref ref-type="bibr" rid="bib19">Freeman et al., 2014</xref> in all neurons and the excitatory neuron marker vglut2a-mCherry <xref ref-type="bibr" rid="bib73">Satou et al., 2013</xref> in presumed glutamatergic neurons. This dataset provided slightly more than fourfold coverage of the larval zebrafish brain. On each imaging plane, we presented a heat stimulus generated by randomly composing sine waves with frequencies between 0.005 Hz and 0.075 Hz (<xref ref-type="fig" rid="fig6">Figure 6A</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A</xref>). We concurrently recorded elicited tail motion at 250 Hz. We segmented the imaging data using CaImAn (<xref ref-type="bibr" rid="bib22">Giovannucci et al., 2019</xref>), which identified <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">∼</mml:mo><mml:mrow><mml:mn mathsize="80%">433</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> active neurons across our dataset. From the tail motion data, we extracted (1) swim starts and (2) tail features that correlate with swim displacement (‘vigor’) and turn angle (‘direction’) in free swimming experiments (<xref ref-type="fig" rid="fig6">Figure 6A</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Using model identification of neural encoding (MINE) to probe larval zebrafish thermoregulatory circuits.</title><p>(<bold>A</bold>) Experimental design. Larval zebrafish expressing nuclear GCaMP6s in all neurons and mCherry in glutamatergic neurons are imaged under a two-photon microscope while random heat stimuli are provided with a laser and behavior is inferred through tail motion, middle (inset shows a sum projection through the example trial depipcted on left and right and in (B) - edge-length = 400 microns). Left: example temperature trajectory during one trial. Right: behavioral responses recorded during the same trial. (<bold>B</bold>) Deconvolved calcium traces of all neurons identified in the plane imaged during the same trial as in (<bold>A</bold>) (heatmap), sorted by the test correlation achieved by the convolutional neural network (CNN) (plot on the left). Orange arrowheads mark the same timepoints as in (<bold>A</bold>) and (<bold>B</bold>). Orange dashed line indicates the fit cutoff used for deciding that a neuron was identified by MINE, blue line marks Pearson correlation of 0. (<bold>C</bold>) Illustration comparing MINE to the linear regression model. (<bold>D</bold>) Venn diagram illustrating fractions of CaImAn extracted neurons identified by MINE, the comparison LM model or both. (<bold>E</bold>) Plot of functional classes identified by Taylor analysis across 25 fish. Barplot at the top indicates total number of neurons in each class on a logarithmic scale. Dotplot marks the significant Taylor components identified in each functional class. Classes are sorted by size in descending order. Horizontal barplot on the right indicates the total number of neurons with activity depending on a given predictor. Orange filled bars mark classes only driven by the stimulus, green open bars those only driven by behavioral predictors while blue bars mark classes of mixed sensorimotor selectivity. Gray numbers in the row labeled ‘N-fish’ indicate the number of fish in which a given type was identified. The red arrowhead marks the functional type that is analyzed further in <xref ref-type="fig" rid="fig7">Figure 7D</xref>. (<bold>F</bold>) Classification of neurons labeled by reticulospinal backfills (inset shows example labeling) across six fish. Orange filled bars mark classes only driven by the stimulus, green open bars those only driven by behavioral predictors while blue bars mark classes of mixed sensorimotor selectivity. (<bold>G</bold>) For different functional neuron classes identified by MINE, the fraction also identified by the linear comparison model. Error-bars are bootstrap standard errors across N=25 zebrafish larvae. (<bold>H</bold>) Anatomical clustering of stimulus driven (left), behavior driven (middle), and mixed-selectivity (right) neurons. Neurons were clustered based on spatial proximity, and clusters with fewer than 10 neurons were not plotted (see ‘Methods’). Asymmetric patterns for lower abundance classes likely do not point to asymmetry in brain function but rather reveal noise in the anatomical clustering approach.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Using model identification of neural encoding (MINE) to probe larval zebrafish thermoregulatory circuits.</title><p>(<bold>A</bold>) Across all imaged planes of all fish (orange lines), the autocorrelation of the provided stimulus. Black line is the average across all planes (<inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>N</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">750</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Boxenplot of the inter-trial correlations for the temperature stimulus and the three behavioral metrics. Note that we use the term ‘trial’ here to denote equal thirds of the imaging period in one plane that allows us to use two ‘trials’ for training and one for testing the fit artificial neural networks (ANNs) and linear models. Since there is no repetitive structure, the term trial is used loosely here. (<bold>C</bold>) Scatterplot and correlations on free-swimming data between the vigor (left) and directionality (right) tail metrics and free-swimming actual displacements (left) and actual heading-angle changes (right). (<bold>D</bold>) For different test-correlation thresholds (x-axis), the number of units for which the convolutional neural network (CNN) has above-threshold test correlations (considered identified by the CNN). Red curve, real data; purple curve, rotated data where the calcium activity has been rotated by one trial with respect to the stimulus and behavior predictors. Black dashed line indicates the threshold used in the article and lists the ratio of identified units in the real versus rotated data. (<bold>E</bold>) Same as (<bold>D</bold>) but comparing the CNN (replot from <bold>D</bold>) with above threshold correlations for the linear model. Dashed line again indicates the threshold used in the article. (<bold>F</bold>) Identified temperature encoding units in the olfactory epithelium. Orange dots are units that according to Taylor metric encode the stimulus while black dots represent units within the epithelium that were not identified by the CNN to highlight medial enrichment of temperature encoding neurons. (<bold>G–I</bold>) For each analyzed Z-Brain region, the units encoding the stimulus (<bold>G</bold>), behavior (<bold>H</bold>), or both (<bold>I</bold>) as a fraction of the sum across these three groups. Red bars highlight the regions with significant enrichment. Open bars and gray text highlight those regions that are neither significantly enriched nor significantly depleted of the given type while gray bars highlight the regions with significant depletion. Blue line is the expected (average) fraction of the given type. Error bars mark a boot-strapped 95% confidence interval around the average of possible observed fractions if the region would in fact contain the expected fraction of units. Bars for which the top is above this confidence interval are therefore significantly enriched, and bars for which the top is below are significantly depleted. Note that the regions in which &lt;50 neurons total across the listed categories have been identified have not been analyzed for enrichment. Region abbreviations are listed in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig6-figsupp1-v2.tif"/></fig></fig-group><p>We used the information about stimulus and elicited behaviors across time as inputs to MINE fitting CNNs to each neural calcium response (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>). The initial two-thirds of time served as training data and the last third as a test/validation set. Notably, due to the random nature of both our stimulus and the elicited behavior, test and training data were largely uncorrelated (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>). Predictive power over the test data therefore indicates that the neural network model generalizes and truly captures how stimulus and behavioral data are related to neural activity. We chose 50% of explained variance on the test data, corresponding to a correlation of <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi mathsize="80%">r</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:msqrt><mml:mn mathsize="80%">0.5</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula>, as a stringent cutoff to determine whether a neuron can be modeled by MINE. A considerably higher threshold was used on this dataset compared with the cortical data since we recorded single-cell activity. It is therefore less likely that activity of a true responder is mixed with unrelated background activity. Using cyclic permutations of the data as a control revealed that this cutoff corresponds to a 93-fold enrichment over control data (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1D</xref>).</p><p>As a comparison, we fit a linear regression model to this dataset. The inputs included time-shifted versions of stimulus and behavioral variables to allow the model to learn temporal transformations <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref> and thereby put it on the same footing as the CNN model (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). We used Ridge regression <xref ref-type="bibr" rid="bib31">Hastie et al., 2009</xref> to improve generalization of the linear model. At the designated cutoff, a total of <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">∼</mml:mo><mml:mrow><mml:mn mathsize="80%">42</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> neurons were identified using either MINE or regression. 40% of these neurons, however, were exclusively identified by MINE (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), indicating the superiority of the model-discovery approach. In fact, MINE consistently identifies more neurons regardless of the cutoff imposed on test data (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1E</xref>).</p><p>We used MINE to assign the identified neurons to functional types according to (1) whether their encoding of stimulus or behavioral features is linear or nonlinear (computational complexity) and (2) which stimulus and behavioral features drove their activity (Taylor analysis). As before, we determined nonlinearity when the truncation of the Taylor expansion after the linear term did not explain at least 80% of the variance of the CNN output. For the Taylor analysis, we chose a stringent cutoff, requiring the Taylor metric to be significantly &gt;0.1 with a p-value <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> on the whole dataset (Bonferroni correction; effective p-value <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>1.26</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>×</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), to determine whether a predictor influenced the activity of a neuron. Since interaction terms had very small Taylor metrics, we ignored them for neuron classification purposes. In total, we identified 33 functional neuron classes all of which were identified across multiple fish (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). As a control, we labeled reticulospinal neurons in six fish via spinal backfills and as expected the vast majority of these neurons were classified by MINE to be driven by behavioral features (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). Across the brain, we identified multiple functional classes of neurons with mixed selectivity, that is, neurons with activity jointly driven by the temperature stimulus and behavioral outputs (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, blue filled bars). These mark a previously unidentified class of neurons in the thermoregulatory circuit that might play a key role in behavioral thermoregulation: they could allow the animal to relate behavioral output to temperature changes and thereby characterize the thermal landscape. Analyzing the success of the linear comparison model according to functional class revealed a bias toward the identification of behavior-related activity and as expected low computational complexity. Stimulus or mixed-selectivity neurons, on the other hand, were underrepresented in the pool identified by the linear model (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). This points to potential problems of bias when identifying neurons by means of such linear models.</p><p>We registered all our imaging data to a standard zebrafish reference brain (Z-Brain; <xref ref-type="bibr" rid="bib67">Randlett et al., 2015</xref>). This allowed us to assess the distribution of identified functional neuron types throughout the brain using two complementary methods. We performed anatomical clustering (see ‘Methods’) to visualize regions with high densities of each functional type (<xref ref-type="fig" rid="fig6">Figure 6H</xref>) and determined in which annotated anatomical regions a functional neuron type is enriched (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1G–I</xref>). While Stimulus-, Behavior- and Mixed-selectivity neurons are generally broadly distributed, they are enriched in specific brain regions (<xref ref-type="fig" rid="fig6">Figure 6H</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1G-I</xref>). We found stimulus-driven neurons to be enriched in telencephalic regions, as well as the habenula and its output region the interpeduncular nucleus (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1G</xref>). As expected, behavior-driven neurons are enriched in hindbrain regions and the midbrain nucleus of the medial longitudinal fasciculus (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1H</xref>; <xref ref-type="bibr" rid="bib77">Severi et al., 2014</xref>; <xref ref-type="bibr" rid="bib83">Thiele et al., 2014</xref>). Mixed-selectivity neurons occupy some regions shared with stimulus-driven neurons such as telencephalic area M4 and the interpeduncular nucleus but are also strongly enriched in the raphe superior as well as the caudal hypothalamus and the torus longitudinalis (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1I</xref>). Our current dataset did not cover sensory ganglia well (especially not the trigeminal ganglion) with the exception of the olfactory epithelium where we found temperature-sensitive neurons in a medial zone (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1I</xref>). This is in line with reports in mice and frogs that describe specific thermosensitive neurons in the olfactory epithelium of these species (<xref ref-type="bibr" rid="bib74">Schmid et al., 2010</xref>; <xref ref-type="bibr" rid="bib37">Kludt et al., 2015</xref>; <xref ref-type="bibr" rid="bib18">Fleischer, 2021</xref>). Overall these results demonstrate that functional cell types identified by MINE segregate spatially within the brain, likely forming organizational units within the thermoregulatory circuit.</p></sec><sec id="s2-7"><title>Computational features of thermoregulatory circuits</title><p>We next sought to use MINE to analyze computational features of thermoregulatory circuits. We subdivided neural classes according to the behavioral features they control, the sensory features they extract (their receptive fields), and their computational complexity. In the case of mixed-selectivity neurons, we used the predictive power of MINE to gain insight into how they integrate thermosensory and behavioral information. Mapping computational features to the zebrafish brain subsequently revealed the anatomical organization of computational features. Analyzing brain regions for enrichment of encoding different behavioral features suggests a segregation in the control of swim starts, swim speed (vigor), and turn angle (directionality). Even though behavior-related neurons are enriched in the hindbrain overall (<xref ref-type="fig" rid="fig6">Figure 6H</xref>), subdividing the neurons reveals a broader distribution (<xref ref-type="fig" rid="fig7">Figure 7A</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A–C</xref>). Swim start neurons are enriched in the dorsal thalamus as well as the medial hindbrain, while the ventral thalamus, the forebrain, and the locus coeruleus preferentially encode swim speed (<xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A and B</xref>). Neurons that encode turning are enriched in mid- and hindbrain regions (<xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1C</xref>). Notably, these neurons could be involved in controlling these behavioral features or alternatively maintain efference copies of the motor commands for other purposes (<xref ref-type="bibr" rid="bib56">Odstrcil et al., 2022</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Functional subdivisions of thermoregulatory circuits.</title><p>(<bold>A</bold>) Anatomical clustering of neurons encoding swim starts (left), swim vigor (middle), and swim direction (right). Neurons were clustered based on spatial proximity, and clusters with fewer than 10 neurons were not plotted (see ‘Methods’). Asymmetric patterns for lower abundance classes likely do not point to asymmetry in brain function but rather reveal noise in the anatomical clustering approach. (<bold>B</bold>) Subclustering of stimulus-selective neurons according to their temporal receptive field (left heatmap). Right: heatmap visualizes for each cluster what fraction of that cluster is present in the major four subdivisions of the zebrafish brain (Telen., telencephalon; Dien., diencephalon; Mesen., mesencephalon; Rhomb., rhombencephalon). Arrowheads indicate differentially distributed example clusters highlighted in (<bold>C</bold>). (<bold>C</bold>) Temporal receptive fields of example clusters. Each plot shows the influence of a change in temperature at the indicated timepoint on the activity (as measured by calcium) of a neuron within the cluster. Numbers reflect the number of neurons present in each given cluster. Dashed lines indicate 0 where the number of 0-crossings of each receptive field indicate if the neuron responds to absolute temperature value (no crossings, cluster 12), to the first derivative (velocity of temperature, increasing, cluster 1; decreasing cluster 4) or to the second derivative (acceleration of temperature, cluster 3). (<bold>D</bold>) Exemplars of two clusters (full set in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>) of nonlinear mixed-selectivity neurons that integrate thermosensory information with information about swim start. Heatmaps show predicted neural calcium response for varying levels of swim- and sensory drive (see ‘Methods’). Line plots show predicted calcium responses (Y-axis) to different sensory drives (X-axis) at different levels of swim drive (blue bar -5, black bar 0, orange bar +5). (<bold>E</bold>) Average complexity of each receptive field cluster shown in (<bold>B</bold>) (gray bars). Blue horizontal line reveals the total average complexity, and vertical blue lines indicate bootstrapped 95% confidence intervals around the average complexity based on the number of neurons contained within the cluster. If the gray bar is above or below that interval, the complexity within that cluster deviates significantly from the data average complexity. (<bold>F</bold>) As in (<bold>A</bold>) but clustering of neurons of complexity 0 (left), complexity 1 (middle), and complexity 2 (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Functional subdivisions of thermoregulatory circuits.</title><p>(<bold>A–C</bold>) For each analyzed Z-Brain region, the units encoding swim starts (<bold>A</bold>), vigor (<bold>B</bold>), or directionality (<bold>C</bold>) as a fraction of the sum across these three groups. Red bars highlight the regions with significant enrichment. Open bars and gray text highlight those regions that are neither significantly enriched nor significantly depleted of the given type while gray bars highlight the regions with significant depletion. Blue line is the expected (average) fraction of the given type. Error bars mark a boot-strapped 95% confidence interval around the average of possible observed fractions if the region would in fact contain the expected fraction of units. Bars for which the top is above this confidence interval are therefore significantly enriched, and bars for which the top is below are significantly depleted. Note that the regions in which &lt;50 neurons total across the listed categories have been identified have not been analyzed for enrichment. Region abbreviations are listed in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. (<bold>D</bold>) Temporal receptive fields of all clusters. Each plot shows the influence of a change in temperature on the activity (as measured by calcium) of a neuron within the cluster. Numbers after the colon reflect the number of neurons present in each given cluster. Numbers after LM ID indicate how many neurons within the cluster have also been identified by the linear model. Note, however, that the clusters themselves would not have been identified based on the linear model data. Clusters in bold are also shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. (<bold>E</bold>) Exemplars of clusters not shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> of nonlinear mixed-selectivity neurons that integrate thermosensory information with information about swim start. Heatmaps show predicted neural calcium response for varying levels of swim- and sensory drive (see ‘Methods’). Line plots show predicted calcium responses (Y-axis) to different sensory drives (X-axis) at different levels of swim drive (blue bar -5, black bar 0, orange bar +5).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83289-fig7-figsupp1-v2.tif"/></fig></fig-group><p>To investigate the stimulus features that drive temperature encoding neurons, we performed clustering on their receptive fields that we retrieved using MINE as described above (<xref ref-type="fig" rid="fig7">Figure 7B and C</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1D</xref>). A clustering approach based on the cosine similarity of receptive fields (see ‘Methods’) resulted in 15 clusters. For each of these clusters, neurons came from at least 15 fish across our dataset. Each cluster represents a functional neuron class that extracts specific features of the temperature stimulus (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, left). Localizing different clusters in the brain reveals an anatomical organization of processing. Clusters 1 and 4 are enriched in the forebrain (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, right), and their receptive fields suggest that they are most responsive to heating and cooling stimuli over slow timescales, respectively (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Their response to temperature increases or decreases is evidenced by the zero-crossing of their receptive fields while the slow rise/fall of the coefficients suggests computation over longer timescales. Cluster 3, on the other hand, computes the rate of heating over shorter timescales (<xref ref-type="fig" rid="fig7">Figure 7C</xref>) and is almost exclusively localized to the hindbrain (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, right). Cluster 12 represents neurons that are excited by colder temperatures since the receptive field exclusively has negative coefficients (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). This neuron type is found predominantly in hind- and midbrain regions (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, right). We note that some of the uncovered receptive fields (e.g. clusters 1 and 4) have the largest departures from 0 at the start and end of the receptive fields. This might indicate that the chosen history length (here 10  s) is too short and does not cover the entire timescale of processing, which could be a result of the rather slow nuclear calcium indicator we chose for this study.</p><p>Mixed-selectivity neurons could provide zebrafish with important information about the thermal environment such as the slope of temperature gradients the fish can use to thermoregulate. We therefore wondered how these neurons combine thermosensory and behavioral information by visualizing response landscapes (akin to <xref ref-type="bibr" rid="bib32">Heras et al., 2019</xref>). Here, we specifically focused on neurons that are driven by both swim starts and temperature inputs and that were classified as nonlinear (indicated by a red arrowhead in <xref ref-type="fig" rid="fig6">Figure 6E</xref>). We extracted the neurons’ receptive fields for these quantities and analyzed predicted responses of the CNN fit by MINE to combinations of swim- and sensory drive. We defined ‘drive’ as scaled versions of the receptive field since it represents the ideal stimulus driving a neuron (<xref ref-type="bibr" rid="bib13">Dayan and Abbott, 2001</xref>) (see ‘Methods’). Clustering neurons according to these response landscapes (<xref ref-type="fig" rid="fig7">Figure 7D</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>) revealed diverse modes of integration across 10 clusters. Some neurons appear to add behavioral inputs linearly with thermosensory inputs (<xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>, clusters 8 and 9). Other neurons, however, show gating (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, left, and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>, clusters 0 and 1). Here swim drive effectively scales the sensory responsiveness. Importantly, since these are temporal receptive fields, the swim drive input will scale with relative timing of swims, being the strongest when behavior occurs within the time window of the receptive field. These neurons could therefore act as coincidence detectors that inform larval zebrafish that temperature changes are linked to behavior as expected in a temperature gradient. We also found a cluster of neurons that display more complex integration of sensory and motor information (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, right). At lower-than-average swim drive, these neurons symmetrically respond to both low and high sensory drive. However, at high swim drive, the response to positive sensory drive is enhanced, making the response of the neuron to temperature asymmetric in this regime.</p><p>Lastly, we sought to gain insight into computational features themselves, that is, those that occur between the sensory input and the observed activity of the neurons. Specifically, we used MINE to analyze the complexity of these computations according to truncations of the Taylor expansion after the linear and second-order terms (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We scored linear neurons as having complexity 0, those for which the linear truncation explains &lt;80% of variance (nonlinear) but where the second-order expansion explains at least 50% of variance as having complexity 1 and the remainder requiring higher-order terms of the expansion as complexity 2. We found that complexity is mapped onto both functional (<xref ref-type="fig" rid="fig7">Figure 7E</xref>) and anatomical features (<xref ref-type="fig" rid="fig7">Figure 7F</xref>), indicating that this division is not arbitrary but rather indicates a meaningful difference in neuron function. Specifically, averaging the complexity score for different receptive field clusters reveals that three of these clusters have lower complexity than expected, indicating that most neurons in these clusters have responses that linearly depend on the temperature stimulus (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). One cluster (cluster 6) had an average complexity score &gt;1, indicating that the majority of neurons in this cluster compute highly nonlinear transformations of the stimulus. Anatomical clustering of neurons by computational complexity reveals that different complexity classes are generally intermingled (<xref ref-type="fig" rid="fig7">Figure 7G</xref>). However, there appears to be some regionalization. Within the hindbrain, neurons of complexity class 2 are found medially in rhombomeres 1 and 2. We previously found that neurons in this region carry little information about ongoing behavior, which might indicate that they process thermosensory stimuli for other functions (<xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref>). Prominent clusters of complexity 2 neurons can also be found in the habenula and dorsal telencephalon consistent with the idea that these structures are involved in higher-order computations rather than the short-time control of thermoregulatory behaviors. Linear neurons, on the other hand, are enriched in the posterior and medial hindbrain.</p><p>In summary, these data reveal the power of MINE to identify computational features leading to a detailed classification of neurons into functional types. This automated, unbiased, and highly flexible analysis framework has the potential to greatly aid the analysis of large-scale neural data. To facilitate the adoption of MINE, we expose its functionality through a simple Python interface. This interface allows fitting the corresponding CNN to neural data and extracting all metrics (<xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>The programmatic interface to model identification of neural encoding (MINE).</title><p>Details and example usage of the programmatic interface to MINE.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">class MineData</th><th align="left" valign="bottom">class Mine</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>correlations_trained (n_neurons x 1)</italic></td><td align="left" valign="bottom"><italic>train_fraction (float)</italic></td></tr><tr><td align="left" valign="bottom">Correlation of CNN prediction and activity on training portion of data</td><td align="left" valign="bottom">Which fraction of the data is used for training with remainder used for testing</td></tr><tr><td align="left" valign="bottom"><italic>correlations_test (n_neurons x 1)</italic></td><td align="left" valign="bottom"><italic>model_history (integer)</italic></td></tr><tr><td align="left" valign="bottom">Correlation of CNN prediction and activity on test portion of data</td><td align="left" valign="bottom">Number of timepoints the model receives as input</td></tr><tr><td align="left" valign="bottom"><italic>taylor_scores (n_neurons x n_components x 2)</italic></td><td align="left" valign="bottom"><italic>corr_cut (float)</italic></td></tr><tr><td align="left" valign="bottom">Taylor metric for each component (predictor and first -order interaction terms). The first entry along the last dimension is the mean score, the second entry is the bootstrap standard error.</td><td align="left" valign="bottom">If test correlation is less than this value for a neuron, it is considered ‘not fit.’</td></tr><tr><td align="left" valign="bottom"><italic>model_lin_approx_scores (n_neurons x 1)</italic></td><td align="left" valign="bottom"><italic>compute_taylor (bool)</italic></td></tr><tr><td align="left" valign="bottom">Goodness of fit of linear Taylor model around the data mean to determine nonlinearity.</td><td align="left" valign="bottom">If true, compute Taylor metrics and complexity analysis (linear and second-order approximations).</td></tr><tr><td align="left" valign="bottom"><italic>mean_exp_scores (n_neurons x 1)</italic></td><td align="left" valign="bottom"><italic>return_jacobians (bool)</italic></td></tr><tr><td align="left" valign="bottom">Goodness of fit of second-order Taylor model around the data mean to derive complexity.</td><td align="left" valign="bottom">If true, return linear receptive fields.</td></tr><tr><td align="left" valign="bottom"><italic>jacobians (n_fit_neurons x (n_timepoints x n_predictors))</italic></td><td align="left" valign="bottom"><italic>taylor_look_ahead (integer)</italic></td></tr><tr><td align="left" valign="bottom">For each fit neuron, the receptive field of each predictor across time.</td><td align="left" valign="bottom">The number of timepoints to predict ahead when calculating Taylor metrics.</td></tr><tr><td align="left" valign="bottom"><italic>hessians (n_fit_neurons x (n_timepoints x n_predictors) x (n_timepoints x n_predictors))</italic></td><td align="left" valign="bottom"><italic>taylor_pred_every (integer)</italic></td></tr><tr><td align="left" valign="bottom">For each fit neuron, the matrix of second-order partial derivatives. Useful to extract second-order receptive fields.</td><td align="left" valign="bottom">Every how many frames a Taylor expansion should be performed to calculate Taylor metrics.</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Additional settable properties:</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom" rowspan="3"><italic>return_hessians (bool, default False)</italic><break/>If true, return matrices of second-order derivatives<break/><italic>model_weight_store (hdf5 file or group, default None)</italic><break/>If set, trained model weights for all models will be organized and stored in the file/group<break/><italic>n_epochs (integer, default 100)</italic><break/>The number of training epochs.</td></tr><tr><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><italic>save_to_hdf5(file_object, overwrite = False</italic>)</td><td align="left" valign="bottom"><italic>analyze_data(pred_data: List, response_data: Matrix) -&gt;MineData</italic></td></tr><tr><td align="left" valign="bottom">Saves the result data to an hdf5 file or group</td><td align="left" valign="bottom">Takes a list of n_timepoints long predictors and a matrix of n_neurons x n_timepoints size and applies MINE iteratively to fit and characterize CNN relating all predictors to each individual neuron.</td></tr><tr><td align="left" valign="bottom"><bold>Example usage:</bold></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">predictors = [Stimulus, Behavior, State]</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">responses = ca_data</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"># NOTE: If predictors and response are not z-scored,</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"># (mean = 0; standard deviation = 1) Mine will print</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"># a warning</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">miner = Mine (2/3, 50, 0.71, True, True, 25, 5)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">miner.model_weight_store = h5py.File(“m_weights.h5”, ‘a’)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">result_data = miner.analyze_data (predictors, responses)</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">all_fit = result_data.correlations_test &gt;= 0.71</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">is_nonlinear = result_data.model_lin_approx_scores &lt;0.8</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">is_stim_driven = (result_data.taylor_scores[:, 0, 0] – 3 x result_data.taylor_scores[:, 0, 1]) &gt;0</td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>A common goal in analyzing neural data is to relate the activity of neurons to sensory stimuli, behavioral actions, internal states, or other features observed during ongoing behavior. To explore this connection, we neuroscientists often define a model dictating the structure of this relationship. When well thought-out, these defined models have the great advantage that their parameters can be interpreted in a biologically meaningful manner. However, even flexible models, if defined a priori, run the risk of not being able to match transformations occurring in the less than intuitive biological brain. Model-selection based on probabilistic programs overcomes some of these challenges, allowing for a model-free and flexible approach while maintaining predictive power (<xref ref-type="bibr" rid="bib70">Saad et al., 2019</xref>). One drawback of this approach however is reduced interpretability of the nature of the relationship between inputs (in our case, stimuli, behaviors, etc.) and the output (in our case, neural activity). Here, we have presented MINE to overcome some of these challenges and offer an accessible framework for comprehensive analysis. Despite MINE’s increased flexibility over predefined models, it maintains interpretability: through Taylor decomposition approaches, MINE (1) characterizes the computational complexity of neural processing giving rise to the activity of an individual neuron, (2) extracts linear and nonlinear receptive fields that define the ideal inputs driving a neuron, and (c) yields information about which predictors are most important in driving the neural response.</p><sec id="s3-1"><title>Discovering encoding of information by neurons</title><p>Different paradigms are used to interpret neural activity. MINE specifically supports an encoding view, inspecting which information is encoded by the neurons of interest. ANNs have, however, also been used from a decoding perspective, probing what information can be gleaned about ongoing behavior from neural activity (<xref ref-type="bibr" rid="bib20">Frey et al., 2021</xref>; <xref ref-type="bibr" rid="bib75">Schneider et al., 2022</xref>). Here, we restricted the question of information encoding to individual neurons. Nonetheless, MINE could easily be extended to model joint population encoding. The most straightforward approach to this problem would be to decompose the population activity through methods such as principal or independent component analysis. These components would then be fit by MINE instead of single-neuron activity. Similarly, we only demonstrate the relationship of neural activity to external features (stimuli, behavioral actions) but neural population activity or local field potentials could be used as predictors. We currently do not exploit or characterize joint processing strategies. The efficiency of model-fitting could likely be improved by having the CNN model the relationships of multiple neurons (network outputs) to the same predictor inputs. Given the general applicability of the Taylor decomposition strategies, such networks could be analyzed in the same way as the current simpler CNN underlying MINE.</p><p>ANNs have been used extensively as models of brains to identify the principles underlying neural processing (<xref ref-type="bibr" rid="bib47">McClelland et al., 1987</xref>). In the present work, CNNs exclusively serve as tools to find the relationships between features of interest and the activity of individual neurons. The network is a stand-in for the circuits transforming these features upstream (for sensation) or downstream (for behavior), as well as the computational properties of the analyzed neuron. We are therefore interested in the properties of the whole CNN instead of its individual units. Extracting the CNN properties aims to characterize the neuron of interest in terms of how it encodes the features. Such an approach has been used previously to characterize neural receptive fields, recognizing the power of ANNs in capturing nonlinear transformations. For example, characterizations of nonlinear visual receptive fields (<xref ref-type="bibr" rid="bib40">Lehky et al., 1992</xref>; <xref ref-type="bibr" rid="bib39">Lau et al., 2002</xref>; <xref ref-type="bibr" rid="bib64">Prenger et al., 2004</xref>; <xref ref-type="bibr" rid="bib85">Ukita et al., 2019</xref>) and nonlinear auditory receptive fields (<xref ref-type="bibr" rid="bib36">Keshishian et al., 2020</xref>). In retinal processing, works by Baccus and Ganguli <xref ref-type="bibr" rid="bib48">McIntosh et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Tanaka et al., 2019</xref> have used the increased flexibility of neural network models over classical linear–nonlinear models to characterize how retinal circuits process natural scene inputs. Internal neurons in the network matched retinal interneurons that were not used for data fitting and resulting models generalized better to novel data than linear–nonlinear models in spite of increased flexibility. Our comparison to Volterra analysis suggests that this might be because the effective degrees of freedom of CNN models are much lower than the total number of parameters but that the space in which these limited degrees of freedom act is better matched to neural data. We go beyond previous approaches in this study by combining modalities and providing an easily accessible framework for analysis of arbitrary neural data. MINE handles and disambiguates multiple different inputs originating from different domains (such as sensory stimuli and behavioral actions). MINE, furthermore, performs a more detailed characterization producing information about the linearity of the relationship of inputs and neural activity. These advances are made possible through mathematical analysis of the network.</p></sec><sec id="s3-2"><title>Characterizing the model implemented by the CNN</title><p>Different techniques aim at understanding how ANNs arrive at their outputs given a set of inputs (<xref ref-type="bibr" rid="bib71">Samek et al., 2019</xref>). These techniques are generally referred to as ‘explainable machine learning’ and are particularly important in understanding what features classifiers use to make their decisions. These include visualization techniques as well as layer-wise relevance propagation (<xref ref-type="bibr" rid="bib8">Binder et al., 2016</xref>) that aims to identify important input features driving the output of a classifier. In our case, however, we were interested to know which inputs persistently (across experimental time) are involved in driving the output of the network, that is, the activity of the modeled neuron. Notably, once the network is trained the different inputs are not separable anymore. This means that there is no guarantee that leaving out an unimportant input will not affect the output of the trained network. A workaround would be to iteratively train the network with subsets of inputs (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2018</xref>). However, this becomes infeasible even with a moderate number of inputs since all combinations would need to be tested. We therefore chose a continuously differentiable activation function for our network layers. This enabled Taylor decomposition to transform the network, point-by-point, into a separable sum of terms depending on individual inputs. This approach allowed us to infer which specific inputs drive the network output and hence the activity recorded in the modeled neuron. Notably this technique can also identify multiplicative interactions, which indicate coincidence detection. Yet, while such interactions were evident in our ground-truth dataset, we found little evidence of multiplicative interactions in the biological data. This is likely due to added noise obfuscating the contribution of these terms, which is comparatively small even on ground-truth data (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). In fact, most information about a multiplicative interaction would be carried in the first derivative: namely the derivative with respect to one input would vary according to another input. While we did not directly exploit this insight in the present work, we do find evidence of multiplicative interactions in the mixed-selectivity neurons some of which show gating of sensory responses by behavior (<xref ref-type="fig" rid="fig7">Figure 7D</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>).</p><p>To characterize the complexity of the relationship between features (stimuli, behavior, etc.) and neural activity, we analyzed truncations of the Taylor expansion of the network around the data average. While not guaranteed for all functions, we expect that the Taylor series converges for our network and hence that given enough terms the expansion would fully characterize the computation performed by the CNN. The ANN fit by MINE must encapsulate the transformations between the chosen features and the neural activity in order to be able to predict the activity. The classification of linear versus nonlinear processing by the network should therefore apply to the neural transformations as well. Especially in the absence of connectomics data, the metric of computational complexity can serve to group neurons into functional circuits. One might specifically expect that sensory processing proceeds from representations of lower complexity to those of higher complexity, that is, further departures from a simple encoding of stimuli. This type of extraction of more and more complex features is, for example, observed in cortical processing of visual stimuli (<xref ref-type="bibr" rid="bib17">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib14">D’Souza et al., 2022</xref>). We note, however, that computational complexity is not an absolute metric. Comparing the linearity for neurons responding to the same feature (e.g. a stimulus) is useful, but making the comparison across different features likely is not. In our zebrafish dataset, the majority of temperature-driven neurons were classified as nonlinear while most behavior-driven neurons were classified as linear. This, however, does not mean that the stimulus is processed less linearly than the behavior – it is likely a result of our stimulus feature being the raw temperature while our behavior features are removed from the nonlinear transformation that changes swim commands into appropriate tail motion.</p><p>We currently determine linear receptive fields (<xref ref-type="bibr" rid="bib13">Dayan and Abbott, 2001</xref>; <xref ref-type="bibr" rid="bib76">Schwartz et al., 2006</xref>) of fit neurons using the first-order derivatives of the network output with respect to the input. Similar to the analysis of higher-order Volterra kernels (<xref ref-type="bibr" rid="bib46">Marmarelis, 2004</xref>; <xref ref-type="bibr" rid="bib72">Sandler and Marmarelis, 2015</xref>), one could analyze eigenvectors of the matrix of second- or higher-order derivatives to get a deeper understanding of how this receptive field changes with stimuli in the case of neurons classified as nonlinear. Comparing direct fitting of Volterra kernels and extraction of first- and second-order kernels from MINE using Taylor analysis indeed highlights the feasibility of this approach (<xref ref-type="fig" rid="fig3">Figure 3</xref>). One crucial advantage of MINE is that it is considerably more robust to departures of stimuli from uncorrelated white noise with Gaussian distributed intensities, the ideal stimuli used for Volterra analysis (<xref ref-type="bibr" rid="bib44">Marmarelis and Marmarelis, 1978</xref>; <xref ref-type="bibr" rid="bib57">Paninski, 2002</xref>; <xref ref-type="bibr" rid="bib46">Marmarelis, 2004</xref>; <xref ref-type="bibr" rid="bib76">Schwartz et al., 2006</xref>). Such stimuli are impractical, especially when the goal is to study neural activity in freely behaving animals. However, our comparison to directly fitting the Volterra kernels also revealed that it is in fact possible to obtain the receptive fields this way. Volterra analysis, unlike MINE, could be fully unconstrained and therefore pose an advantage. While MINE is constrained by architecture and hyperparameters, our comparisons of extracting receptive fields with MINE versus an unconstrained fit of Volterra kernels revealed the limitations of not imposing any constraints (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). MINE offers a simple-to-use alternative that maintains flexibility while limiting effective degrees of freedom to allow for meaningful fits to real-world data. Independent of MINE, the structure of receptive fields is influenced by the acquisition modality. For our zebrafish data, we used a nuclear localized GCaMP6s. This variant has a long decay time (<xref ref-type="bibr" rid="bib19">Freeman et al., 2014</xref>), and since MINE is fit on calcium data, effects of this decay time will appear in the extracted receptive fields. This likely explains why the receptive fields shown for the zebrafish data extend over many seconds. This issue could be overcome by better deconvolution of the imaging data which would require a higher acquisition framerate.</p></sec><sec id="s3-3"><title>Novel insight into biological circuits</title><p>Applying MINE to biological data revealed its potential to discover the relationships between stimuli and behaviors and neural activity across species and imaging modalities. On a mouse cortical widefield imaging dataset, MINE recovered previously published structure in the neural data. Task elements driving neural activity could be mapped to expected cortical regions and receptive field analysis revealed expected structure in visual receptive fields while the receptive fields associated with neurons encoding instructed movements showed preparatory activity. Performing two-photon calcium imaging in larval zebrafish while simultaneously presenting random temperature stimuli and recording behavior allowed a deeper dive into thermoregulatory circuits. MINE allowed us for the first time to identify the neurons that integrate temperature stimuli and information about behavior. This was not possible previously since regression-based approaches failed to identify the neurons encoding temperature while clustering to identify these neurons relied on removing stochastic motor-related activity (<xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref>). Neurons integrating stimuli and behavior might serve an important role in thermoregulation. Specifically, they might allow larval zebrafish to estimate thermal gradients by comparing behavioral actions and their consequences in terms of changes in environmental temperature. While we have only begun to analyze the responses of these mixed-selectivity neurons, a simple clustering according to their response to combinations of sensory and motor drive revealed a variety of computed features (<xref ref-type="fig" rid="fig7">Figure 7D</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1E</xref>). Combining receptive field analysis with the idea of computational complexity, on the other hand, provides a way to functionally subdivide temperature responsive neurons in great detail. This will greatly aid future studies trying to understand thermoregulatory circuits. Notably, the functional features we identified map to anatomical features in the brain pointing to structural subdivisions mirroring function. The ability to compare neurons, even though we used randomized stimuli, indicates another advantage of MINE. When recording neural activity during free exploration, across animals different neurons will be recorded under very different stimulus and behavior conditions. This means that their responses can usually be compared only by choosing different trigger events (certain behaviors, specific stimulus conditions) and analyzing neural activity around those. MINE, on the other hand, allows comparing networks fit on the whole activity time series removing the requirement and bias of choosing specific triggering events.</p><p>One surprising finding in comparing the cortical and zebrafish brain responses was the prevalence of nonlinear processing in zebrafish and the apparent lack of it in the cortical dataset. This is likely due to the different spatial resolution in the datasets. While the zebrafish data has single-neuron resolution, this is not the case for the cortical widefield data especially since we apply MINE not to individual pixels but activity components. Each of these components combines the activity of thousands of neurons. It is likely that this averaging obfuscates individual nonlinear effects.</p><p>In summary, MINE allows for flexible analysis of the relationship between measured quantities such as stimuli, behaviors, or internal states and neural activity. The flexibility comes from MINE turning analysis on its head: instead of predefining a model, MINE approximates the functional form that relates predictors to neural activity and subsequently discovers this functional form. This is possible since MINE does not make any assumptions about the probability distribution or functional form underlying the data it models. This allows MINE to identify more neurons than regression approaches on our zebrafish dataset (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) while doing so in a less biased manner. The regression approach especially failed on stimulus-driven units, leading to an overrepresentation of behavior-related activity (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). Testing how many neurons in our receptive field clusters were also identified by the linear model (<xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1D</xref>) revealed that the linear model identified neurons belonging to each cluster. This argues that MINE increases sensitivity for identifying neurons, but at this level of analysis MINE did not necessarily identify functional types that were entirely missed by the linear model. However, in four clusters the linear model identified less than five neurons. This effectively means that the cluster would have been missed entirely when using the linear model data as the base for analysis. Accordingly, applying our clustering on the neurons identified by the linear model only recovers 4 instead of the 15 unique receptive field clusters that are obtained with MINE.</p><p>We propose that the fit CNN models the computation of upstream or downstream circuits of the neuron in question but we do not think or investigate whether it models the circuitry itself. This could be investigated in the future, but given the simplicity of the CNN used in MINE it is unlikely to provide interesting details in this manner. Instead, bringing more advanced network-wide analysis methods to bear on the fit CNN could reveal other interesting features of the computation ‘surrounding’ the neuron in question.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>Animal handling and experimental procedures were approved by the Ohio State University Institutional Animal Care and Use Committee (IACUC protocol # 2019A00000137 and 2019A00000137-R1).</p><sec id="s4-1"><title>Design and training of convolutional neural network</title><p>A network that was simple, and therefore quick to train, but maintained expressivity was sought for this study and a three-layer CNN was chosen. We note that the input to the network is a chunk of time equal to the size of the convolutional layers. A similar architecture could therefore be realized without separate convolutional layers by simply expanding the number of input nodes. The first layer was built to contain 80 convolutional filters. We do not expect that the network will have to learn 80 separate filters; however, the advantage is that these filters contain a large amount of pre-training variation. This eases learning of temporal features important for the generation of the neural response. The convolutional layer was kept linear and followed by two 64-unit-sized dense layers with swish activation function, which subsequently fed into a linear output unit. This in effect means that each unit in the first dense layer performs a convolution on the input data with a filter that is a weighted average of the 80 convolutional filters.</p><p>The ground-truth dataset as well as part of a previous zebrafish dataset (<xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref>) were used to optimize the following hyperparameters: a sparsity constraint to aid generalization, the learning rate, the number of training epochs, and the activation function. The activation function was to be continuously differentiable, a requirement for subsequent analyses. Dropout was added during training to further help with generalization; however, the rate was not optimized and set at 50%. The history length (10  s) of the network input was set somewhat arbitrarily, but initial tests with shorter lengths led to problems of fitting the larval zebrafish dataset likely because of the slow calcium indicator time constant of nuclear GCaMP6s (<xref ref-type="bibr" rid="bib19">Freeman et al., 2014</xref>). We note that we did not optimize the overall architecture of the network. This could certainly be done and all subsequent analysis methods are architecture agnostic as long as the network is a feed-forward architecture. <italic>Related Python file in the repository: model.py</italic>.</p></sec><sec id="s4-2"><title>Characterizing the linearity and complexity of the transformation</title><p>To compute measures of the complexity of the relationship between the predictors and neural activity, a Taylor expansion across the predictor average <inline-formula><mml:math id="inf41"><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> was performed and the predictions of truncations of this Taylor expansion were compared to the true model output, sampling predictors every second. The Jacobian (vector of first-order derivatives of the output with respect to the input) and the Hessian (matrix of second-order derivatives of the output with respect to the input) were computed using Tensorflow. We note that since our network has a single output, the Jacobian is not a matrix but simply the gradient vector of the output with respect to the input.</p><p>The Jacobian and Hessian were used to compute the following first- and second-order approximations of the network output using the Taylor expansion:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>These truncations were used to compute a ‘linear approximation score’ <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi mathsize="80%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and a ‘second-order approximation score’ <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi mathsize="80%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">O</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as the coefficients of determination quantifying the variance explained by the truncation of the true network output.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>L</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi mathsize="80%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">O</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="80%">1</mml:mn><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo largeop="true" mathsize="80%" stretchy="false" symmetric="true">∑</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathsize="80%">f</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mn mathsize="80%">2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">d</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi mathsize="80%">v</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We note that for a purely linear function we expect <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi mathsize="80%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to be 1, and this score is therefore a measure of the linearity of the relationship between predictors and the neural response. <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi mathsize="80%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">O</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, on the other hand, quantifies how good a second-order model is in predicting the neural response, and we therefore use it to further define the complexity of the relationship between the predictors and the neural activity.</p><p>Based on ROC analysis on ground-truth data, <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi mathsize="80%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was thresholded at <inline-formula><mml:math id="inf47"><mml:mrow><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0.8</mml:mn></mml:mrow></mml:math></inline-formula>. Neurons for which the linear expansion explained &lt;80% of variance of the true network output were considered nonlinear (see ‘Results’). To assign complexity classes to zebrafish neurons, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi mathsize="80%">S</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">O</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was thresholded at <inline-formula><mml:math id="inf49"><mml:mrow><mml:msup><mml:mi mathsize="80%">R</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0.5</mml:mn></mml:mrow></mml:math></inline-formula>; in other words, neurons for which the seccond-order expansion did not explain at least 50% of the variance of the true network output were considered complex. Then neurons were assigned to one of three respective complexity classes: 0 for neurons for which the linear expansion <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi mathsize="80%">L</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">A</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> indicated linearity, 1 if <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, or 2 if the neuron was deemed nonlinear and <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>As a comparison, two metrics related to the nonlinearity of the function implemented by the CNN were calculated: a ‘curvature metric’ that quantifies the magnitude of the influence of the second-order derivative, and the ‘nonlinearity coefficient’ that approximates the error made by linearizing the network. It is important to note that given random inputs that are unrelated to the training data, it is very likely that the CNN will produce outputs that nonlinearly depend on these inputs. Both metrics are therefore calculated with data that forms part of the training manifold (<xref ref-type="bibr" rid="bib65">Raghu et al., 2023</xref>). We also note that the ‘curvature metric’ does not in fact compute the curvature of the multidimensional function implemented by the CNN. Instead, the average magnitude of the vector induced by the Hessian on a unit magnitude step in the data space is used as a proxy. With <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf56"><mml:mi mathsize="80%">f</mml:mi></mml:math></inline-formula> defined as in ‘Identifying contributing predictors by Taylor decomposition’, the curvature metric is calculated according to<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The nonlinearity coefficient (NLC) was computed according to <xref ref-type="bibr" rid="bib58">Philipp and Carbonell, 2018</xref> quantifying<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">[</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">[</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf57"><mml:mi mathsize="80%">D</mml:mi></mml:math></inline-formula> is the data distribution, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi mathsize="80%">T</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:math></inline-formula> is the trace operator, and <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">v</mml:mi><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></inline-formula> is the data covariance while <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi mathsize="80%">V</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">r</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> Varf is the output variance (since the output in the present case is a single value). The right form highlights that the NLC compares the variance of the linear approximation of the output (<inline-formula><mml:math id="inf61"><mml:mrow><mml:mi mathsize="80%">J</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mi mathsize="80%">T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:msup><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">′</mml:mo></mml:msup><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) with the true output variance.</p><p><italic>Related Python files in the repository: taylorDecomp.py, perf_nlc_nonlin.py and utilities.py</italic>.</p></sec><sec id="s4-3"><title>Comparison to Volterra analysis/determination of receptive fields</title><p>The discrete-time Volterra expansion with finite memory of a function up to order 2 is defined as<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi mathsize="80%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">t</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the response and <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi mathsize="80%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">t</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the stimulus, and <inline-formula><mml:math id="inf64"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf65"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf66"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula> are the zeroth-, first-, and second-order Volterra kernels, respectively.</p><p>This can be rewritten in matrix form where<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>such that<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We note that since our network takes predictors across time as input, it can be easily seen that <inline-formula><mml:math id="inf67"><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> is equivalent to <inline-formula><mml:math id="inf68"><mml:mover accent="true"><mml:mi mathsize="80%">s</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> and therefore that the Taylor expansion (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) of the CNN is equivalent to the Volterra series with finite memory above (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>). This indicates that there is a direct correspondence between the elements of the Taylor expansion and those of the Volterra expansion, where <inline-formula><mml:math id="inf69"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula> corresponds to <inline-formula><mml:math id="inf70"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula> corresponds to <inline-formula><mml:math id="inf72"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula> and our CNN would be an effective means to fit the Volterra kernels (<xref ref-type="bibr" rid="bib88">Wray and Green, 1994</xref>). This suggests that neural receptive fields can be extracted from <inline-formula><mml:math id="inf73"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula> of the Taylor expansion in the same manner as they can be obtained from <inline-formula><mml:math id="inf75"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi mathsize="80%">k</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathsize="80%">2</mml:mn></mml:mrow></mml:math></inline-formula> of the Volterra expansion.</p><p>To determine whether MINE could be used to extract system-level characteristics (such as receptive fields) akin to Volterra analysis, a model was used to turn a randomly generated stimulus into a response by passing the input through two orthogonal filters followed by two nonlinearities. The outputs of these two parallel stages were subsequently combined using addition to form the response. The stimulus was either generated as a Gaussian white noise stimulus (by drawing values <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) or as a smoothly varying random stimulus as described in ‘Ground-truth datasets.’ The model consisted of two sets of orthogonal filters, <inline-formula><mml:math id="inf78"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of length 50 timepoints, as well as two nonlinearities <inline-formula><mml:math id="inf80"><mml:msub><mml:mi mathsize="80%">g</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf81"><mml:msub><mml:mi mathsize="80%">g</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). The terms <inline-formula><mml:math id="inf82"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the filters were chosen because the former is expected to be contained within <inline-formula><mml:math id="inf84"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula>/<inline-formula><mml:math id="inf85"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula>, that is, the linear term of the Volterra expansion while the latter is expected to be contained within <inline-formula><mml:math id="inf86"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula>/<inline-formula><mml:math id="inf87"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula>, that is, the quadratic term of the Volterra expansion. We note, however, that <inline-formula><mml:math id="inf88"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can in addition be extracted from <inline-formula><mml:math id="inf89"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula>/<inline-formula><mml:math id="inf90"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula> due to the particular function chosen for <inline-formula><mml:math id="inf91"><mml:msub><mml:mi mathsize="80%">g</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Given a stimulus <inline-formula><mml:math id="inf92"><mml:mi mathsize="80%">s</mml:mi></mml:math></inline-formula>, the response <inline-formula><mml:math id="inf93"><mml:mi mathsize="80%">r</mml:mi></mml:math></inline-formula> was constructed according to<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>, where * denotes convolution.</p><p>The two nonlinearities were constructed such that <inline-formula><mml:math id="inf94"><mml:msub><mml:mi mathsize="80%">g</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was asymmetric and therefore led to a shift in the average spike rate, while <inline-formula><mml:math id="inf95"><mml:msub><mml:mi mathsize="80%">g</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was symmetric around 0 so that it predominantly influences the variance of the spike rate:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.25</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></disp-formula></p><p>The constant multiplier in <inline-formula><mml:math id="inf96"><mml:msub><mml:mi mathsize="80%">g</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was chosen such that <inline-formula><mml:math id="inf97"><mml:msub><mml:mi mathsize="80%">r</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is in a similar range as <inline-formula><mml:math id="inf98"><mml:msub><mml:mi mathsize="80%">r</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>The model therefore generates a response from which the two filters should be recoverable when fitting the first- and second-order Volterra kernels either through direct means akin to canonical system identification approaches or via the CNN used in MINE.</p><p>Given the Volterra kernels <inline-formula><mml:math id="inf99"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf100"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf101"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="inf102"><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">N</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">¯</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf103"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf104"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula>), we attempted to recover <inline-formula><mml:math id="inf105"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by extracting the principal dynamic modes PDM from the kernels <xref ref-type="bibr" rid="bib45">Marmarelis, 1997</xref> by performing eigen decomposition on an intermediate matrix <inline-formula><mml:math id="inf107"><mml:mi mathsize="80%">Q</mml:mi></mml:math></inline-formula>, which combines all kernels in the following manner:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mi mathsize="80%">Q</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mn mathsize="80%">2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mn mathsize="80%">2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mn mathsize="80%">2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mn mathsize="80%">2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mn mathsize="80%">2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathsize="80%" mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn mathsize="80%">1</mml:mn><mml:mn mathsize="80%">2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" separator="true" stretchy="false"> </mml:mo><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathsize="80%" mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The PDM were then defined as the eigenvectors with the largest eigenvalues (<xref ref-type="bibr" rid="bib45">Marmarelis, 1997</xref>). The eigenvectors with the three largest (positive) eigenvalues were compared to <inline-formula><mml:math id="inf108"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:msub><mml:mi mathsize="80%">f</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by computing the cosine similarity according to <xref ref-type="disp-formula" rid="equ38">Equation 36</xref>. The best matches were reported in all plots in Figures <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. We note that for MINE, it was sufficient to compare the eigenvectors with the largest two eigenvalues; however, in the case of directly fitting the Volterra kernels (see below), the eigenvectors with the largest two eigenvalues often included an eigenvector related to the constant term <inline-formula><mml:math id="inf110"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>As a comparison to MINE, a system identification approach, directly fitting the Volterra kernels, was followed. We note that by creating an appropriate design matrix <inline-formula><mml:math id="inf111"><mml:mi mathsize="80%">X</mml:mi></mml:math></inline-formula>, the elements of <inline-formula><mml:math id="inf112"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf113"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf114"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:math></inline-formula> can be directly obtained through a linear regression fit:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">⇒</mml:mo><mml:mi mathsize="80%">β</mml:mi><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathsize="80%" mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mn mathsize="80%">0</mml:mn><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mn mathsize="80%">1</mml:mn><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathsize="80%" mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi mathsize="80%">k</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">T</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A linear regression fit was used accordingly, either using ordinary least squares or Ridge regression with varying penalties <inline-formula><mml:math id="inf115"><mml:mi mathsize="80%">α</mml:mi></mml:math></inline-formula> as indicated in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. We note that the direct fits were also performed after pre-whitening the design matrix as this might help with model fitting. However, this did not improve median filter quality but increased the variance across simulations (data not shown).</p><p>Effective degrees of freedom were calculated/estimated according to <xref ref-type="bibr" rid="bib31">Hastie et al., 2009</xref>. Specifically, for Ridge regression models, the effective degrees of freedom were calculated analytically according to<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf116"><mml:mi mathsize="80%">α</mml:mi></mml:math></inline-formula> is the Ridge penalty, <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the identity matrix, and <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi mathsize="80%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi></mml:mrow></mml:math></inline-formula> is the trace operator.</p><p>For the CNN used in MINE, the effective degrees of freedom were estimated using simulations according to<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf119"><mml:mover accent="true"><mml:mi mathsize="80%">y</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is the model prediction, <inline-formula><mml:math id="inf120"><mml:mi mathsize="80%">y</mml:mi></mml:math></inline-formula> is the true output, <inline-formula><mml:math id="inf121"><mml:msup><mml:mi mathsize="80%">σ</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> is the error variance, and <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">v</mml:mi></mml:mrow></mml:math></inline-formula> is the covariance. Intuitively, this definition of degrees of freedom measures how small variations in the output affect variations in the predicted output. In other words, models with high degrees of freedom are expected to match predictions to any change in output while models with low degrees of freedom are limited in this regard.</p><p>All fits were repeated for 100 randomly generated stimuli to obtain different estimates for both MINE and Volterra analysis.</p><p><italic>Related Python file in the repository: cnn_sta_test.py, mine_edf.py</italic>.</p></sec><sec id="s4-4"><title>Identifying contributing predictors by Taylor decomposition</title><p>To assess the contribution of individual predictors on the network output, a metric based on the Taylor decomposition of the network was computed. The Taylor decomposition was truncated after the second derivative. Below, Taylor decompositions that include all terms for first- and second-order derivatives will be designated as ‘full.’ For all analyses presented in the article, Taylor expansions were performed every second (every five timepoints at our data rate of 5 Hz) and predictions were made 5 s into the future (25 timepoints at our data rate of 5 Hz).</p><p>The state of the predictors at timepoint <inline-formula><mml:math id="inf123"><mml:mi mathsize="80%">t</mml:mi></mml:math></inline-formula> and the state of the predictors 5 s (in our case, 25 samples) later was obtained. Subsequently, these quantities, together with the Jacobian at timepoint <inline-formula><mml:math id="inf124"><mml:mi mathsize="80%">t</mml:mi></mml:math></inline-formula> and the Hessian at timepoint <inline-formula><mml:math id="inf125"><mml:mi mathsize="80%">t</mml:mi></mml:math></inline-formula>, were used to compute a prediction of the change in network output over 5 s. At the same time, the true change in output was computed. Comparing the predicted changes and true changes via correlation resulted in an <inline-formula><mml:math id="inf126"><mml:msup><mml:mi mathsize="80%">r</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:math></inline-formula> value. This value measures how well the full Taylor expansion approximates the change in network output across experimental time. We note that predicted changes in the output were compared via correlation rather than predicted outputs since we specifically wanted to understand which predictors are important in driving a change in output rather than identifying those that the network might use to set some baseline value of the output. In the following, <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi mathsize="80%">X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">t</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a vector that contains the values of all predictors across all timepoints fed into the network (e.g. in our case 50 timepoints of history for each predictor) in order to model the activity at time <inline-formula><mml:math id="inf128"><mml:mi mathsize="80%">t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf129"><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">x</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the CNN applied to a specific predictor input, <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi mathsize="80%">J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">x</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Jacobian at that input, and <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi mathsize="80%">H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mi mathsize="80%">x</mml:mi><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the Hessian:<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>25</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The Taylor expansion around <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ25"><label>(23)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi>J</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The predicted change in network output according to the Taylor expansion:<disp-formula id="equ26"><label>(24)</label><mml:math id="m26"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The true change in network output:<disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">f</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:msup><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">′</mml:mo></mml:msup><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">→</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Variance of the output change explained by the Taylor prediction using <xref ref-type="disp-formula" rid="equ26">Equation 24</xref> and <xref ref-type="disp-formula" rid="equ27">Equation 25</xref>:<disp-formula id="equ28"><label>(26)</label><mml:math id="m28"><mml:mrow><mml:msubsup><mml:mi mathsize="80%">r</mml:mi><mml:mrow><mml:mi mathsize="80%">F</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msubsup><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="80%">c</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">o</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">r</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">f</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">f</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After the computation of <inline-formula><mml:math id="inf133"><mml:msubsup><mml:mi mathsize="80%">r</mml:mi><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msubsup></mml:math></inline-formula>, terms were removed from the Taylor series. We note that both individual timepoints fed into the network and predictors are separated at this level. However, only the removal of predictors onto the quality of prediction was tested, not the importance of specific timepoints across predictors (i.e. all timepoints across a given predictor or interaction were removed to test the importance of the predictors, instead of removing all predictors at one timepoint to test the importance of a specific timepoint). For the removal of a given predictor, both its linear (via <inline-formula><mml:math id="inf134"><mml:mi mathsize="80%">J</mml:mi></mml:math></inline-formula>) and its squared contribution (via <inline-formula><mml:math id="inf135"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula>) were subtracted from the full Taylor prediction while for the removal of interaction terms only the corresponding interaction term (via <inline-formula><mml:math id="inf136"><mml:mi mathsize="80%">H</mml:mi></mml:math></inline-formula>) was subtracted. The following quantities were computed to arrive at the Taylor metric of single predictors <inline-formula><mml:math id="inf137"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula> or interactions of two predictors <inline-formula><mml:math id="inf138"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf139"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf140"><mml:msub><mml:mi mathsize="80%">J</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula> identify subvectors that contain all elements belonging to the specific predictor <inline-formula><mml:math id="inf141"><mml:mi mathsize="80%">n</mml:mi></mml:math></inline-formula> while <inline-formula><mml:math id="inf142"><mml:msub><mml:mi mathsize="80%">H</mml:mi><mml:mrow><mml:mi mathsize="80%">n</mml:mi><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> identifies all elements in the Hessian at the intersection of all rows belonging to the second derivatives with respect to predictor <inline-formula><mml:math id="inf143"><mml:mi mathsize="80%">n</mml:mi></mml:math></inline-formula> and columns with respect to the predictor <inline-formula><mml:math id="inf144"><mml:mi mathsize="80%">m</mml:mi></mml:math></inline-formula>: change predicted by only considering <inline-formula><mml:math id="inf145"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ29"><label>(27)</label><mml:math id="m29"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Change predicted when removing <inline-formula><mml:math id="inf146"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula> using <xref ref-type="disp-formula" rid="equ26">Equation 24</xref> and <xref ref-type="disp-formula" rid="equ29">Equation 27</xref>:<disp-formula id="equ30"><label>(28)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">f</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">f</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mrow><mml:mi mathsize="80%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathsize="80%">f</mml:mi><mml:mo mathsize="80%" stretchy="false">^</mml:mo></mml:mover><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Change predicted when only considering interaction between <inline-formula><mml:math id="inf147"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">m</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ31"><label>(29)</label><mml:math id="m31"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Change predicted when removing <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> interaction using <xref ref-type="disp-formula" rid="equ26">Equation 24</xref> and <xref ref-type="disp-formula" rid="equ31">Equation 29</xref>:<disp-formula id="equ32"><label>(30)</label><mml:math id="m32"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Variance explained by Taylor prediction after removing <inline-formula><mml:math id="inf150"><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub></mml:math></inline-formula> using <xref ref-type="disp-formula" rid="equ27">Equation 25</xref> and <xref ref-type="disp-formula" rid="equ30">Equation 28</xref>:<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Variance explained by Taylor prediction after removing <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> interaction using <xref ref-type="disp-formula" rid="equ27">Equation 25</xref> and <xref ref-type="disp-formula" rid="equ32">Equation 30</xref>:<disp-formula id="equ34"><label>(32)</label><mml:math id="m34"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>The Taylor metric was then defined as<disp-formula id="equ35"><label>(33)</label><mml:math id="m35"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ36"><label>(34)</label><mml:math id="m36"><mml:mrow><mml:msub><mml:mi mathsize="80%">T</mml:mi><mml:mrow><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">n</mml:mi></mml:msub><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:msub><mml:mi mathsize="80%">x</mml:mi><mml:mi mathsize="80%">m</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="80%">1</mml:mn><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mrow><mml:msubsup><mml:mi mathsize="80%">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="80%">x</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="80%">x</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">m</mml:mi></mml:mrow></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msubsup><mml:mo mathsize="80%" stretchy="false">/</mml:mo><mml:msubsup><mml:mi mathsize="80%">r</mml:mi><mml:mrow><mml:mi mathsize="80%">F</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>respectively. For the zebrafish data, we additionally calculated an overall ‘Behavior’ Taylor metric in which all terms (regular and interaction) that belonged to any behavioral predictor were removed and the Taylor metric was subsequently calculated as outlined above. <italic>Related Python file in the repository: taylorDecomp.py</italic>.</p></sec><sec id="s4-5"><title>Clustering of neurons according to receptive field</title><p>Jacobians were extracted for each neuron at the data average and used as proxies of the receptive field (see section on ‘Comparison to Volterra analysis/determination of receptive fields’). Accordingly, the receptive field of a network and therefore neuron (<inline-formula><mml:math id="inf152"><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) was defined as<disp-formula id="equ37"><label>(35)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">f</mml:mi></mml:msub></mml:mrow><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="80%">J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="80%" minsize="80%">(</mml:mo><mml:mover accent="true"><mml:mi mathsize="80%">x</mml:mi><mml:mo mathsize="80%" stretchy="false">¯</mml:mo></mml:mover><mml:mo maxsize="80%" minsize="80%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This is of course merely an approximation for nonlinear neurons. The first-order derivative of a nonlinear function is not constant and therefore the receptive field will depend on the inputs. To cluster zebrafish sensory neurons according to the receptive fields, the cosine of the angle between the receptive fields of different neurons (<inline-formula><mml:math id="inf153"><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">≠</mml:mo><mml:mi mathsize="80%">j</mml:mi></mml:mrow></mml:math></inline-formula>) (and during the process, clusters) was computed (cosine similarity, <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">S</mml:mi></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ38"><label>(36)</label><mml:math id="m38"><mml:mrow><mml:mrow><mml:mi mathsize="80%">C</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">S</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">i</mml:mi><mml:mi mathsize="80%">T</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo maxsize="80%" minsize="80%">∥</mml:mo><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">i</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">∥</mml:mo></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo maxsize="80%" minsize="80%">∥</mml:mo><mml:mrow><mml:mi mathsize="80%">R</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="80%">F</mml:mi><mml:mi mathsize="80%">j</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="80%" minsize="80%">∥</mml:mo></mml:mrow><mml:mn mathsize="80%">2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Clustering was subsequently performed by a greedy approach as outlined in <xref ref-type="bibr" rid="bib7">Bianco and Engert, 2015</xref>. Briefly: all pairwise CS values were computed. Then, the pair of receptive fields with the highest CS was aggregated into a cluster. The average RF in that cluster was computed. Subsequently all pairwise CS values, now containing that new cluster average, were computed again and the procedure was iteratively repeated until all receptive fields were clustered or no pairwise similarities <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> were observed. The advantages of this clustering method are that it is (1) deterministic and (2) the number of clusters does not have to be specified beforehand. In the case of the receptive fields on the mouse dataset, the matrix of pairwise cosine similarities was used to perform spectral clustering. Spectral clustering finds groups of strong connection (high CS in this case) within a similarity graph that are separated from other groups by weak connections (low CS). The rationale for changing clustering methods was computational speed, which is much higher for spectral clustering.</p><p><italic>Related Python file in the repository: utilities.py</italic>.</p></sec><sec id="s4-6"><title>Ground-truth datasets</title><p>To test the capability of MINE to predict the relationships between predictors and neural activity, four predictors were computer-generated. Two of these were generated by combining sine, square, and triangle waves of random frequencies and amplitudes – simulating smoothly varying sensory inputs. The third predictor was generated as a Poisson process – simulating stochastic, unitary events such as bouts of movement. The last predictor was generated as a Poisson process as well but events were scaled with a random number drawn from a unit normal distribution with mean zero – simulating directed, stochastic movements.</p><p>Model responses were subsequently created by linearly or nonlinearly transforming individual predictors and combining them multiplicatively. After this step, all responses were convolved with a ‘calcium kernel’ and <inline-formula><mml:math id="inf158"><mml:mrow><mml:mrow><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">.</mml:mo><mml:mi mathsize="80%">i</mml:mi><mml:mo mathsize="80%" stretchy="false">.</mml:mo><mml:mi mathsize="80%">d</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">.</mml:mo></mml:mrow></mml:math></inline-formula> Gaussian noise was added to more closely resemble real-world acquisition. MINE was applied to these data: fitting a CNN on two-thirds of the data and calculating the correlation between the CNN prediction and the last third (test set) of the data. As a comparison, two linear regression models were fit to the data as well, again split into training and test sets. One of these models used raw predictors as inputs to predict the response (equivalent to the inputs given to MINE). The other used transformed predictors, convolved with the same calcium kernel applied to the responses above, along with all first-order interaction terms of the predictors to predict the response. Correlations on the test data were subsequently used for comparisons.</p><p><italic>Related Python file in the repository: cnn_fit_test.py</italic>.</p><p>To test the capability of MINE in predicting the nonlinearity of predictor–response relationships, for each test case a new ‘sensory-like predictor’ (introduced above) was generated. For the effect of varying nonlinearity (<xref ref-type="fig" rid="fig3">Figure 3B–D</xref>), the predictor was transformed by (1) squaring the hyperbolic tangent of the predictor, (2) differencing the predictor, or (3) rectifying the predictor using a ‘softplus’ transformation. These transformed versions were subsequently mixed to varying fractions with the original predictor to generate a response. The coefficients of determination of truncations of the Taylor expansion after the linear and quadratic terms were then computed as indicated above. Linear correlations were also performed for some of the mixtures to illustrate the effect of both linear and nonlinear transformations on this metric. For ROC analysis of the linearity metric, a set of 500 by 5 random stimuli were generated. For each set, four responses were generated depending only on the first of the five random stimuli (the others were used as noisy detractors to challenge the CNN fit). Two of the four responses were linear transformations: one was a differencing of the first predictor and the other was a convolution with a randomly generated double-exponential filter. The other two responses were randomized nonlinear transformations. The first was a variable-strength rectification by shifting and scaling a softplus function. The second was a random integer power between one and five of a hyperbolic tangent transformation of the predictor. The known category (linear vs. nonlinear) was subsequently used to calculate false- and true-positive rates at various thresholds for the linearity metric (see above) to perform ROC analysis.</p><p><italic>Related Python file in the repository: cnn_nonlin_test.py</italic>.</p></sec><sec id="s4-7"><title>Processing of data from <xref ref-type="bibr" rid="bib54">Musall et al., 2019b</xref></title><p>The data for Musall (<xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>) was downloaded from the published dataset. Temporal components <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref> were loaded from ‘interpVc.mat,’ spatial components from ‘Vc.mat,’ and predictors from ‘orgregData.mat’ for each analyzed session. Predictors were shifted by 5 s relative to the activity traces. The 10-s-long convolutional filters allowed for observing the influences of events 5 s into the past as well as preparatory activity for actions occurring in the next 5 s. MINE was subsequently used to determine the relationship between predictors and temporal components. The neural activity and the individual terms of the Taylor decomposition were mapped into pixel space. Then spatial maps were generated by computing Taylor metrics as above for each individual pixel time series. This was necessary since the components in ‘interpVc.mat’ are highly linearly dependent, making it impossible to carry measures of explained variance directly from ‘component space’ into ‘pixel space.’ Receptive fields, on the other hand, were directly mapped into pixel space using the spatial component matrix as these are linear. <italic>Related Python files in the repository: processMusall.py, mine.py, plotMusall.py</italic>.</p></sec><sec id="s4-8"><title>Zebrafish experiments</title><p>All calcium imaging was performed on a custom-built two-photon microscope at 2.5 Hz with a pixel resolution of 0.78 <inline-formula><mml:math id="inf159"><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_unit" mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">m</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">/</mml:mo><mml:mi class="ltx_unit" mathsize="80%">p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">x</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">l</mml:mi></mml:mrow></mml:math></inline-formula> and an average power of 12 mW (at sample) at 950 nm (measured optical resolution of the system is <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> lateral and <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>4</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> axial). The brain was divided into six overlapping regions: two divisions along the dorsoventral axis and three divisions along the anterior–posterior axis. The most anterior point in the most anterior region was set to the olfactory epithelium while the most posterior point in the most posterior region was the start of the spinal cord. Larval zebrafish expressing nuclear localized GCaMP in all neurons and mCherry in all glutamatergic neurons (mitfa -/-; elavl3-H2B:GCaMP6s +/-; vGlut2a-mCherry) between 5 and 7 d post fertilization were used for all experiments. Larval zebrafish were mounted and stimulated as previously described (<xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref>) including scan stabilization as previously described to avoid artifacts associated with heating-induced expansion of the preparation. The tail of the larva was free and tracked at 250 Hz using an online tracking method previously described (<xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref>). Whenever our online tail-tracking missed a point along the tail, an image was saved from which the tail was subsequently retracked offline as in <xref ref-type="bibr" rid="bib49">Mearns et al., 2020</xref>. Since the experiments were performed under a new microscope, we recalculated a model translating stimulus laser strength to fish temperature as described in <xref ref-type="bibr" rid="bib25">Haesemeyer et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Haesemeyer et al., 2018</xref> and subsequently referred to as the ‘temperature model’.</p><p>For each imaging plane, 495 s of a ‘random wave’ stimulus were generated by superimposing sine waves of randomized amplitudes with periods between 13 s and 200 s. The stimuli were set to generate temperatures outside the noxious range between 22°C and 30°C. Imaging planes were spaced 5 <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi class="ltx_unit" mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">m</mml:mi></mml:mrow></mml:math></inline-formula> apart and 30 planes were imaged in each fish.</p><p>For reticulospinal backfills Texas-Red Dextran 10,000 MW (Invitrogen D1828) at 12.5% w/v was pressure injected into the spinal cord of larval zebrafish under Tricaine (Syndel MS222) anesthesia. Injections were performed in fish expressing nuclear localized GCaMP6s in all neurons (mitfa -/-; Elavl3-H2B:GCaMP6s) that were embedded in low-melting point agarose with a glass capillary having a 15 <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi class="ltx_unit" mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">m</mml:mi></mml:mrow></mml:math></inline-formula> tip opening. Fish were unembedded after the procedure, woken from anesthesia, and rested overnight. Fish were screened the next day for labeling, embedded again, and imaged under the two-photon microscope.</p></sec><sec id="s4-9"><title>Processing of zebrafish data</title><p>Raw imaging data was preprocessed using CaImAn for motion correction and to identify units and extract associated calcium traces (<xref ref-type="bibr" rid="bib22">Giovannucci et al., 2019</xref>). Laser stimulus data was recorded at 20 Hz and converted to temperatures using the temperature model (see above). The 250 Hz tail data was used to extract boutstarts based on the absolute angular derivative of the tail crossing an empirically determined threshold. The vigor was calculated as the sum of the absolute angular derivative across the swimbout, which is a metric of the energy of tail movement. The directionality was calculated as the sum of the angles of the tip of the tail across the swimbout, which is a metric of how one-sided the tail movement is. Extracted calcium data, stimulus temperatures, and the three behavior metrics were subsequently interpolated/downsampled to a common 5 Hz timebase. To reduce linear dependence, the behavior metrics were subsequently orthogonalized using a modified Gram–Schmidt process.</p><p>Components outside the brain as well as very dim components inside the brain were identified by CaImAn. To avoid analyzing components that likely do not correspond to neurons labeled by GCaMP, all components for which the average brightness across imaging time was &lt;0.1 photons were excluded. The number of analyzed components was thereby reduced from 706,543 to 432,882.</p><p>The CNN was subsequently fit to the remaining components using two-thirds of imaging time as training data. For each neuron, the stimulus presented during recording and the observed behavioral metrics were used as inputs. Both the inputs and the calcium activity traces were standardized to zero mean and unit standard deviation before fitting. For every neuron in which the correlation <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:msqrt><mml:mn>0.5</mml:mn></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, all discussed MINE metrics were extracted.</p><p>To generate the cyclically permuted controls, all calcium data was rotated forward by a third of the experiment length with wrap around. The rationale for constructing the control data in this manner was that it maintains the overall structure of the data, but since both our stimuli and the elicited behavior are stochastic, it should remove any true relationship between predictors and responses. The CNN was fit to the control data in the same manner as to the real data, but no further metrics were extracted. To identify which predictors significantly contributed to neural activity, <inline-formula><mml:math id="inf165"><mml:msub><mml:mi mathsize="80%">R</mml:mi><mml:mrow><mml:mi mathsize="80%">f</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">u</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf166"><mml:msub><mml:mi mathsize="80%">R</mml:mi><mml:mrow><mml:mi mathsize="80%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf167"><mml:msub><mml:mi mathsize="80%">R</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="80%">P</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="80%">n</mml:mi></mml:mrow><mml:mo mathsize="80%" stretchy="false">,</mml:mo><mml:mi mathsize="80%">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were bootstrapped, computing the Taylor metric for each variate. The standard deviation of the bootstrap variate was subsequently used to estimate confidence intervals according to a normal distribution approximation. While confidence intervals could have been computed directly from the bootstrap variates, this would have meant storing all samples for all neurons in questions to retain flexibility in significance cutoff. The significance level was set to p&lt;0.05 after Bonferroni correction across all fit neurons (effective p-value of <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1.26</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). To be considered a driver of neural activity, the Taylor metric had to be larger than 0.1 at this significance threshold.</p><p>The linear comparison model was constructed akin to <xref ref-type="bibr" rid="bib53">Musall et al., 2019a</xref>. All predictors were timeshifted by between 0 and 50 timesteps for a total of 200 predictor inputs to the regression model. This setup emulates the convolutional filters present in the CNN used by MINE. A modified Gram–Schmidt process was used for orthogonalization to avoid the problems of QR decomposition observed in cases of near singular design matrices (singular up to the used floating-point precision). Ridge regression (<xref ref-type="bibr" rid="bib31">Hastie et al., 2009</xref>) was used to improve generalization. Setting the ridge penalty to <inline-formula><mml:math id="inf169"><mml:msup><mml:mn mathsize="80%">10</mml:mn><mml:mrow><mml:mo mathsize="80%" stretchy="false">-</mml:mo><mml:mn mathsize="80%">4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> led to a 25–50% increase of identified neurons on small test datasets compared to a standard regression model.</p><p>All zebrafish stacks (except those acquired after spinal backfills) were first registered to an internal Elavl3-H2B:GCaMP6s reference using CMTK (<xref ref-type="bibr" rid="bib69">Rohlfing and Maurer, 2003</xref>). A transformation from the internal reference to the Z-Brain (<xref ref-type="bibr" rid="bib67">Randlett et al., 2015</xref>) was subsequently performed using ANTS (<xref ref-type="bibr" rid="bib5">Avants et al., 2009</xref>). Micrometer coordinates transformed into Z-Brain pixel coordinates, together with region masks present in the Z-Brain, were subsequently used to assign all neurons to brain regions.</p><p>To identify Z-Brain regions in which a specific functional type is enriched, the following process was used. The total number of all types under consideration in each region was computed (i.e. if the comparison was between stimulus, behavior, and mixed-selectivity neurons, all these neurons were summed, but if the comparison was between different classes of behavior-related neurons only those were summed). Subsequently, the fraction of the type of interest among the total in each region was computed (plotted as bars in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Similarly, an overall fraction of the type across the whole brain was computed (plotted as vertical blue lines in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) This overall fraction was used to simulate a 95% confidence interval of observed fractions given the total number of neurons considered in each region (horizontal blue lines in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Any true fraction above this interval was considered significant enrichment. We note that exact confidence intervals could have been computed from the binomial distribution; however, the simulation approach allowed us to use the same method when computing confidence intervals around the average complexity of receptive field clusters.</p><p>For reticulospinal backfill experiments, neurons labeled by the backfill were manually annotated. This annotation was used to classify neurons (CaImAn identified components) as reticulospinal.</p><p>To study how mixed-selectivity neurons integrate information about the temperature stimulus and swim starts, the following strategy was used. Receptive fields for the temperature and swim start predictor were extracted for each neuron as described above. Subsequently, the drive for an input was defined as a scaled version of the receptive fields after they had been scaled to unit norm. Synthetic input stimuli at different combinations of stimulus and swim drive were subsequently generated and fed into the CNN of the neuron of interest. The response of the CNN was recorded to construct the response landscapes shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. To cluster similar response landscapes, pairwise 2D cross-correlations were computed between the response landscapes of all neurons. Similarity was defined as the maximal value of the cross-correlation to preferentially cluster on similar shapes of the landscape rather than exact alignment. Spectral clustering was used to form the clusters and for each a randomly selected exemplar is shown in the figures. Due to the lack of alignment, cluster averages are not meaningful.</p><p>To perform anatomical clustering, every identified neuron was clustered based on the spatial proximity of their centroids to the centroids of other neurons. A radius of 5 <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi class="ltx_unit" mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">m</mml:mi></mml:mrow></mml:math></inline-formula> (roughly the radius of a neuron in the zebrafish brain) was chosen empirically as the clustering radius. The coordinates of the centroids were then placed in a 3D volume with a spatial resolution of 1 <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi class="ltx_unit" mathsize="80%">μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_unit" mathsize="80%">m</mml:mi></mml:mrow></mml:math></inline-formula>, with each centroid occupying one position in this space. Subsequently, a kernel of the chosen radius was constructed. The kernel was then convolved with the centroids of the neurons to generate a ball around each of them. The goal was that if the space occupied by the balls around two neurons overlap, or are adjacent, the two neurons would be clustered together. This was accomplished using connected-components-3D (William Silversmith, version 3.12, available on PyPi as connected-components-3d; code: <ext-link ext-link-type="uri" xlink:href="https://github.com/seung-lab/connected-components-3d/">https://github.com/seung-lab/connected-components-3d/</ext-link>, copy archived at <xref ref-type="bibr" rid="bib79">Silversmith, 2023</xref>) which was used to cluster the neurons based on their proximity. The connected-components-3D function takes the 3D volume and assigns each occupied voxel to a cluster. The corresponding centroids were then identified and assigned the same cluster label. For plotting, an alpha value was assigned to each centroid based on the number of neurons present in each cluster. The coordinates of the centroids were then plotted and assigned the computed alpha values.</p><p>Experiments were excluded from further analysis for the following reasons: five fish died during functional imaging; eight fish could not be registered to the reference; tail tracking failed during the acquisition of six fish and during the acquisition of four fish problems in the acquisition hardware led to imaging artifacts. <italic>Related Python files in the repository: rwave_data_fit.py, rwave_decompose.py, utilities.py</italic>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>is employed by Hitachi Solutions America, Ltd., Irvine, CA</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal handling and experimental procedures were approved by the Ohio State University Institutional Animal Care and Use Committee (IACUC Protocol #: 2019A00000137 and 2019A00000137-R1).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Z-Brain region abbreviations.</title></caption><media xlink:href="elife-83289-supp1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83289-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data generated in this study is publicly available. All code used in this study is available in the following repositories: MINE: <ext-link ext-link-type="uri" xlink:href="https://github.com/haesemeyer/mine_pub">https://github.com/haesemeyer/mine_pub</ext-link> (copy archieved at <xref ref-type="bibr" rid="bib29">Haesemeyer, 2023</xref>). CaImAn preprocessing: <ext-link ext-link-type="uri" xlink:href="https://github.com/haesemeyer/imaging_pipeline">https://github.com/haesemeyer/imaging_pipeline</ext-link> (copy archieved at <xref ref-type="bibr" rid="bib28">Haesemeyer, 2021</xref>). Tail behavior processing: <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/jcostabile/2p/">https://bitbucket.org/jcostabile/2p/</ext-link>. Raw experimental data has been deposited to DANDI: <ext-link ext-link-type="uri" xlink:href="https://dandiarchive.org/dandiset/000235/0.230316.1600">https://dandiarchive.org/dandiset/000235/0.230316.1600</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dandiarchive.org/dandiset/000236/0.230316.2031">https://dandiarchive.org/dandiset/000236/0.230316.2031</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dandiarchive.org/dandiset/000237/0.230316.1655">https://dandiarchive.org/dandiset/000237/0.230316.1655</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://dandiarchive.org/dandiset/000238/0.230316.1519">https://dandiarchive.org/dandiset/000238/0.230316.1519</ext-link>. Processed data has been deposited on Zenodo: Zebrafish and mouse final processed data: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7737788">https://doi.org/10.5281/zenodo.7737788</ext-link>. Zebrafish and mouse fit CNN weights part 1/2: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7738603">https://doi.org/10.5281/zenodo.7738603</ext-link>. Zebrafish and mouse fit CNN weights part 2/2: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7741542">https://doi.org/10.5281/zenodo.7741542</ext-link>. All data analysis was performed in Python; Tensorflow (<xref ref-type="bibr" rid="bib1">Abadi, 2016</xref>) was used as the deep learning platform.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Balakrishnan</surname><given-names>KA</given-names></name><name><surname>Schwinn</surname><given-names>S</given-names></name><name><surname>Costabile</surname><given-names>JD</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Processed data for &quot;Model-free identification of neural encoding (MINE)&quot; publication</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7737788</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Balakrishnan</surname><given-names>KA</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Thermoregulatory Responses Forebrain</data-title><source>Dandi Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000235/0.230316.1600</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Balakrishnan</surname><given-names>KA</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Thermoregulatory Responses Midbrain</data-title><source>Dandi Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000236/0.230316.2031</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset4"><person-group person-group-type="author"><name><surname>Balakrishnan</surname><given-names>KA</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Thermoregulatory Responses Hindbrain</data-title><source>Dandi Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000237/0.230316.1655</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset5"><person-group person-group-type="author"><name><surname>Schwinn</surname><given-names>S</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Thermoregulatory Responses Reticulospinal system</data-title><source>Dandi Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000238/0.230316.1519</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset6"><person-group person-group-type="author"><name><surname>Balakrishnan</surname><given-names>KA</given-names></name><name><surname>Schwinn</surname><given-names>S</given-names></name><name><surname>Costabile</surname><given-names>JD</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>CNN weight data for &quot;Model-free identification of neural encoding (MINE)&quot; publication - Set 1</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7738603</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset7"><person-group person-group-type="author"><name><surname>Balakrishnan</surname><given-names>KA</given-names></name><name><surname>Schwinn</surname><given-names>S</given-names></name><name><surname>Costabile</surname><given-names>JD</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>CNN weight data for &quot;Model-free identification of neural encoding (MINE)&quot; publication - Set 2</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7741542</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset8"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Dataset from: Single-trial neural dynamics are dominated by richly varied movements</data-title><source>CSHL</source><pub-id pub-id-type="doi">10.14224/1.38599</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Andrew D Bolton and Bradley Cutler for critical comments on the manuscript and the 'Zebrafish Neural Circuits &amp; Behavior' seminar community for early stage feedback and discussion. We would also like to thank participants of NeuroDataShare 2023 for helpful discussions about this work and suggested improvements to our sharing of data. This work was supported by funding from the Ohio State University and NIH BRAIN Initiative R01-NS123887 through NINDS and NIDA to MH.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: a system for large-scale machine learning</article-title><conf-name>Proceedings of the12th USENIX Symposium on Operating Systems Designand Implementation</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aertsen</surname><given-names>AM</given-names></name><name><surname>Johannesma</surname><given-names>PI</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The Spectro-temporal receptive field: A functional characteristic of auditory neurons</article-title><source>Biological Cybernetics</source><volume>42</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1007/BF00336731</pub-id><pub-id pub-id-type="pmid">7326288</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>B.Et al.brain-wide neuronal Dynamics during motor adaptation in Zebrafish.nature</article-title><source>Nature</source><volume>485</volume><fpage>471</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1038/nature11057</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anders</surname><given-names>K</given-names></name><name><surname>John</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A simple weight decay can improve generalization.Adv</article-title><source>Neural Inf. Process. Syst</source><volume>4</volume><fpage>950</fpage><lpage>957</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>B</given-names></name><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Song</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Advanced normalization tools</article-title><source>The Insight Journal</source><volume>1</volume><elocation-id>uvnhin</elocation-id><pub-id pub-id-type="doi">10.54294/uvnhin</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>AS</given-names></name><name><surname>Fernandes</surname><given-names>HL</given-names></name><name><surname>Tomlinson</surname><given-names>T</given-names></name><name><surname>Ramkumar</surname><given-names>P</given-names></name><name><surname>VerSteeg</surname><given-names>C</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>S.Et al.modern machine learning as a benchmark for fitting neural responses.front</article-title><source>Frontiers in Computational Neuroscience</source><volume>12</volume><elocation-id>56</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2018.00056</pub-id><pub-id pub-id-type="pmid">30072887</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>IH</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visuomotor transformations underlying hunting behavior in Zebrafish.Curr</article-title><source>Current Biology</source><volume>25</volume><fpage>831</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.042</pub-id><pub-id pub-id-type="pmid">25754638</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>A</given-names></name><name><surname>Bach</surname><given-names>S</given-names></name><name><surname>Montavon</surname><given-names>G</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name><name><surname>Samek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Layer-wise relevance propagation for deep neural network architectures</article-title><source>InInformation Science and Applications</source><volume>1</volume><fpage>913</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1007/978-981-10-0557-2</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Demb</surname><given-names>JB</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>M.Et al.do we know what the early visual system Does?J</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>10577</fpage><lpage>10597</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3726-05.2005</pub-id><pub-id pub-id-type="pmid">16291931</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Kuan</surname><given-names>AT</given-names></name><name><surname>Nikitchenko</surname><given-names>M</given-names></name><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Chen</surname><given-names>AB</given-names></name><name><surname>Gavornik</surname><given-names>JP</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>X.Et al.brain-wide organization of neuronal activity and Convergent sensorimotor transformations in larval Zebrafish.neuron</article-title><source>Neuron</source><volume>100</volume><fpage>876</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.042</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>PS</given-names></name><name><surname>Sejnowski</surname><given-names>T</given-names></name><name><surname>Patricia</surname><given-names>S</given-names></name><name><surname>Terrence</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><chapter-title>Computational Neuroscience series</chapter-title><person-group person-group-type="editor"><name><surname>Churchland</surname><given-names>PS</given-names></name></person-group><source>The Computational Brain</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262533393.001.0001</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cybenko</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Approximation by Superpositions of a Sigmoidal function.math</article-title><source>Mathematics of Control, Signals, and Systems</source><volume>2</volume><fpage>303</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1007/BF02551274</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Souza</surname><given-names>RD</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Ji</surname><given-names>W</given-names></name><name><surname>Meier</surname><given-names>AM</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name><name><surname>Knoblauch</surname><given-names>K</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>D.Et al.Hierarchical and Nonhierarchical features of the mouse visual cortical network.NAT</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>503</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-28035-y</pub-id><pub-id pub-id-type="pmid">35082302</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Neural Engineering: Computational, Representation, and Dynamics in Neurobiological Systems</source><publisher-loc>Cambridge, MA, USA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The big data problem: turning maps into knowledge.neuron</article-title><source>Neuron</source><volume>83</volume><fpage>1246</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.008</pub-id><pub-id pub-id-type="pmid">25233305</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed Hierarchical processing in the Primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleischer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Grueneberg ganglion: signal Transduction and coding in an olfactory and Thermosensory organ involved in the detection of alarm Pheromones and predator-secreted Kairomones</article-title><source>Cell and Tissue Research</source><volume>383</volume><fpage>535</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1007/s00441-020-03380-w</pub-id><pub-id pub-id-type="pmid">33404842</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Vladimirov</surname><given-names>N</given-names></name><name><surname>Kawashima</surname><given-names>T</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Bennett</surname><given-names>DV</given-names></name><name><surname>Rosen</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>C-T</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>J.Et al.mapping brain activity at scale with cluster computing.NAT</article-title><source>Nature Methods</source><volume>11</volume><fpage>941</fpage><lpage>950</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3041</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname><given-names>M</given-names></name><name><surname>Tanni</surname><given-names>S</given-names></name><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>O’Leary</surname><given-names>A</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Kelly</surname><given-names>J</given-names></name><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Lefort</surname><given-names>J</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>M.Et al.interpreting wide-band neural activity using Convolutional neural</article-title><source>eLife</source><volume>10</volume><elocation-id>e66551</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66551</pub-id><pub-id pub-id-type="pmid">34338632</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Josephs</surname><given-names>O</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Nonlinear event-related responses in fMRI.Magn</article-title><source>Magnetic Resonance in Medicine</source><volume>39</volume><fpage>41</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1002/mrm.1910390109</pub-id><pub-id pub-id-type="pmid">9438436</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Gunn</surname><given-names>P</given-names></name><name><surname>Kalfon</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>BL</given-names></name><name><surname>Koay</surname><given-names>SA</given-names></name><name><surname>Taxidis</surname><given-names>J</given-names></name><name><surname>Najafi</surname><given-names>F</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Khakh</surname><given-names>BS</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Caiman an open source tool for Scalable calcium imaging data analysis</article-title><source>eLife</source><volume>8</volume><elocation-id>38173</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38173</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gollisch</surname><given-names>T</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Modeling Convergent ON and OFF pathways in the early visual system.Biol</article-title><source>Biological Cybernetics</source><volume>99</volume><fpage>263</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1007/s00422-008-0252-y</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gorban</surname><given-names>AN</given-names></name><name><surname>Wunsch</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The general approximation theorem</article-title><conf-name>ICNN ’98 - International Conference on Neural Networks</conf-name><conf-loc>Anchorage, AK, USA</conf-loc><fpage>1271</fpage><lpage>1274</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.1998.685957</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haesemeyer</surname><given-names>M</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The structure and Timescales of heat perception in larval Zebrafish.cell SYST</article-title><source>Cell Systems</source><volume>1</volume><fpage>338</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2015.10.010</pub-id><pub-id pub-id-type="pmid">26640823</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haesemeyer</surname><given-names>M</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A brain-wide circuit model of heat-evoked swimming behavior in larval Zebrafish.Neuron</article-title><source>Neuron</source><volume>98</volume><fpage>817</fpage><lpage>831</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.013</pub-id><pub-id pub-id-type="pmid">29731253</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haesemeyer</surname><given-names>M</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Convergent temperature representations in artificial and biological neural</article-title><source>Neuron</source><volume>103</volume><fpage>1123</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.07.003</pub-id><pub-id pub-id-type="pmid">31376984</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Imaging_Pipeline</data-title><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:58f98028f96751d85b5594474df48a51e79b33e9;origin=https://github.com/haesemeyer/imaging_pipeline;visit=swh:1:snp:43477f559b48f307960a1666c45ad26f483d1ec2;anchor=swh:1:rev:fb900d7e10a23630ad98d8bf35f0ebac32e6e0b6">https://archive.softwareheritage.org/swh:1:dir:58f98028f96751d85b5594474df48a51e79b33e9;origin=https://github.com/haesemeyer/imaging_pipeline;visit=swh:1:snp:43477f559b48f307960a1666c45ad26f483d1ec2;anchor=swh:1:rev:fb900d7e10a23630ad98d8bf35f0ebac32e6e0b6</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Haesemeyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Mine_Pub</data-title><version designator="swh:1:rev:368d75650a1de92d9dd6c7dd1aadff24be6ba379">swh:1:rev:368d75650a1de92d9dd6c7dd1aadff24be6ba379</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:6056b57ba2bee68e3e59091c430e0266f87f0dfc;origin=https://github.com/haesemeyer/mine_pub;visit=swh:1:snp:7cc2e8506d6da955a9f027a344e6af8749b959b3;anchor=swh:1:rev:368d75650a1de92d9dd6c7dd1aadff24be6ba379">https://archive.softwareheritage.org/swh:1:dir:6056b57ba2bee68e3e59091c430e0266f87f0dfc;origin=https://github.com/haesemeyer/mine_pub;visit=swh:1:snp:7cc2e8506d6da955a9f027a344e6af8749b959b3;anchor=swh:1:rev:368d75650a1de92d9dd6c7dd1aadff24be6ba379</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in Parietal cortex during a virtual-navigation decision task.nature</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The elements of statistical learning</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-84858-7</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heras</surname><given-names>FJH</given-names></name><name><surname>Romero-Ferrero</surname><given-names>F</given-names></name><name><surname>Hinz</surname><given-names>RC</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep attention networks reveal the rules of collective motion in Zebrafish.Plos Comput</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007354</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007354</pub-id><pub-id pub-id-type="pmid">31518357</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hornik</surname><given-names>K</given-names></name><name><surname>Stinchcombe</surname><given-names>M</given-names></name><name><surname>White</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Multilayer feedforward networks are universal approximators</article-title><source>Neural Networks</source><volume>2</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(89)90020-8</pub-id><pub-id pub-id-type="pmid">2758477</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields and functional architecture of monkey Striate cortex.J</article-title><source>The Journal of Physiology</source><volume>195</volume><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>James</surname><given-names>G</given-names></name><name><surname>Witten</surname><given-names>D</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>An Introduction to Statistical Learning</source><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer US</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4614-7138-7</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshishian</surname><given-names>M</given-names></name><name><surname>Akbari</surname><given-names>H</given-names></name><name><surname>Khalighinejad</surname><given-names>B</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Estimating and interpreting Nonlinear receptive field of sensory neural responses with deep neural network models</article-title><source>eLife</source><volume>9</volume><elocation-id>e53445</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53445</pub-id><pub-id pub-id-type="pmid">32589140</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kludt</surname><given-names>E</given-names></name><name><surname>Okom</surname><given-names>C</given-names></name><name><surname>Brinkmann</surname><given-names>A</given-names></name><name><surname>Schild</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Integrating temperature with odor processing in the olfactory bulb.J</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7892</fpage><lpage>7902</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0571-15.2015</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korenberg</surname><given-names>MJ</given-names></name><name><surname>Hunter</surname><given-names>IW</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The identification of Nonlinear biological systems: Wiener kernel approaches.Ann</article-title><source>Annals of Biomedical Engineering</source><volume>18</volume><fpage>629</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1007/BF02368452</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>B</given-names></name><name><surname>Stanley</surname><given-names>GB</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Computational subunits of visual cortical neurons revealed by artificial neural networks.Proc</article-title><source>PNAS</source><volume>99</volume><fpage>8974</fpage><lpage>8979</lpage><pub-id pub-id-type="doi">10.1073/pnas.122173799</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehky</surname><given-names>SR</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Predicting responses of Nonlinear neurons in monkey Striate cortex to complex patterns</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>3568</fpage><lpage>3581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-09-03568.1992</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>A</given-names></name><name><surname>Tomlinson</surname><given-names>T</given-names></name><name><surname>Rohani</surname><given-names>N</given-names></name><name><surname>Chowdhury</surname><given-names>R</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Katsaggelos</surname><given-names>AK</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural networks for modeling neural Spiking in S1 cortex</article-title><source>Frontiers in Systems Neuroscience</source><volume>13</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2019.00013</pub-id><pub-id pub-id-type="pmid">30983978</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mammano</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Modeling auditory system Nonlinearities through Volterra series.Biol</article-title><source>Biological Cybernetics</source><volume>63</volume><fpage>307</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1007/BF00203454</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname><given-names>E</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Theory in motion.Curr</article-title><source>Current Opinion in Neurobiology</source><volume>5</volume><fpage>832</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(95)80113-8</pub-id><pub-id pub-id-type="pmid">8805418</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marmarelis</surname><given-names>PZ</given-names></name><name><surname>Marmarelis</surname><given-names>VZ</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>Analysis of Physiological Systems</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer Verlag</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4613-3970-0</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marmarelis</surname><given-names>VZ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Modeling methodology for Nonlinear physiological systems.Ann</article-title><source>Annals of Biomedical Engineering</source><volume>25</volume><fpage>239</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1007/BF02648038</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marmarelis</surname><given-names>VZ</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Nonlinear Dynamic Modeling of Physiological Systems</source><publisher-name>Institute of Electrical and Electronics Engineers</publisher-name><pub-id pub-id-type="doi">10.1002/9780471679370</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Rumelhart</surname><given-names>DE</given-names></name><collab>Pdp research group</collab></person-group><year iso-8601-date="1987">1987</year><source>Parallel Distributed Processing</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/5237.001.0001</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>LT</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning models of the retinal response to natural scenes.Adv</article-title><source>Neural Inf. Process. Syst</source><volume>29</volume><fpage>1369</fpage><lpage>1377</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mearns</surname><given-names>DS</given-names></name><name><surname>Donovan</surname><given-names>JC</given-names></name><name><surname>Fernandes</surname><given-names>AM</given-names></name><name><surname>Semmelhack</surname><given-names>JL</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deconstructing hunting behavior reveals a tightly coupled stimulus-response loop.Curr</article-title><source>Current Biology</source><volume>30</volume><fpage>54</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.11.022</pub-id><pub-id pub-id-type="pmid">31866365</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miri</surname><given-names>A</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Burdine</surname><given-names>RD</given-names></name><name><surname>Aksay</surname><given-names>E</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Regression-based identification of behavior-Encoding neurons during large-scale optical imaging of neural activity at cellular resolution.J</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>964</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1152/jn.00702.2010</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitsis</surname><given-names>GD</given-names></name><name><surname>French</surname><given-names>AS</given-names></name><name><surname>Höger</surname><given-names>U</given-names></name><name><surname>Courellis</surname><given-names>S</given-names></name><name><surname>Marmarelis</surname><given-names>VZ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Principal dynamic mode analysis of action potential firing in a spider Mechanoreceptor.Biol</article-title><source>Biological Cybernetics</source><volume>96</volume><fpage>113</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1007/s00422-006-0108-2</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitsis</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The Volterra-Wiener approach in neuronal modeling.CONF</article-title><source>Proc. IEEE Eng. Med. Biol. Soc</source><volume>1</volume><fpage>5912</fpage><lpage>5915</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2011.6091462</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Single-trial neural Dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Harnessing behavioral diversity to understand neural computations for cognition</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>229</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.011</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niven</surname><given-names>JE</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Energy limitation as a selective pressure on the evolution of sensory systems.J</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1792</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1242/jeb.017574</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odstrcil</surname><given-names>I</given-names></name><name><surname>Petkova</surname><given-names>MD</given-names></name><name><surname>Haesemeyer</surname><given-names>M</given-names></name><name><surname>Boulanger-Weill</surname><given-names>J</given-names></name><name><surname>Nikitchenko</surname><given-names>M</given-names></name><name><surname>Gagnon</surname><given-names>JA</given-names></name><name><surname>Oteiza</surname><given-names>P</given-names></name><name><surname>Schalek</surname><given-names>R</given-names></name><name><surname>Peleg</surname><given-names>A</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name><name><surname>Lichtman</surname><given-names>JW</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>I.Et al.functional and ultrastructural analysis of Reafferent Mechanosensation in larval Zebrafish.Curr</article-title><source>Current Biology</source><volume>32</volume><fpage>176</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.11.007</pub-id><pub-id pub-id-type="pmid">34822765</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Convergence properties of some spike-triggered analysis techniques</article-title><source>Neural Inf. Process. Syst</source><volume>15</volume><fpage>437</fpage><lpage>464</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Philipp</surname><given-names>G</given-names></name><name><surname>Carbonell</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1806.00179</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Philipp</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Nonlinearity Coefficient - a Practical Guide to Neural Architecture Design</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2105.12210</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pho</surname><given-names>GN</given-names></name><name><surname>Goard</surname><given-names>MJ</given-names></name><name><surname>Woodson</surname><given-names>J</given-names></name><name><surname>Crawford</surname><given-names>B</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task-dependent representations of stimulus and choice in mouse Parietal cortex.NAT</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2596</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05012-y</pub-id><pub-id pub-id-type="pmid">29968709</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Considerations on models of movement detection.Kybernetik</article-title><source>Kybernetik</source><volume>13</volume><fpage>223</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1007/BF00274887</pub-id><pub-id pub-id-type="pmid">4359479</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>A theory of how the brain might Work.Cold spring Harb</article-title><source>Symp. Quant. Biol</source><volume>55</volume><fpage>899</fpage><lpage>910</lpage><pub-id pub-id-type="doi">10.1101/SQB.1990.055.01.084</pub-id><pub-id pub-id-type="pmid">2132866</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portugues</surname><given-names>R</given-names></name><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Whole-brain activity maps reveal stereotyped, distributed networks for Visuomotor behavior.neuron</article-title><source>Neuron</source><volume>81</volume><fpage>1328</fpage><lpage>1343</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.019</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prenger</surname><given-names>R</given-names></name><name><surname>Wu</surname><given-names>MCK</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Nonlinear V1 responses to natural scenes revealed by neural network analysis.neural NETW</article-title><source>Neural Networks</source><volume>17</volume><fpage>663</fpage><lpage>679</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2004.03.008</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Raghu</surname><given-names>M</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Kleinberg</surname><given-names>J</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Proceedings of Machine Learning Research</article-title><conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name><fpage>2847</fpage><lpage>2854</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ramachandran</surname><given-names>P</given-names></name><name><surname>Zoph</surname><given-names>B</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Searching for Activation Functions</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1710.05941</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Randlett</surname><given-names>O</given-names></name><name><surname>Wee</surname><given-names>CL</given-names></name><name><surname>Naumann</surname><given-names>EA</given-names></name><name><surname>Nnaemeka</surname><given-names>O</given-names></name><name><surname>Schoppik</surname><given-names>D</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name><name><surname>Lacoste</surname><given-names>AMB</given-names></name><name><surname>Riegler</surname><given-names>C</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Schier</surname><given-names>AF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>O.Et al.whole-brain activity mapping onto a Zebrafish brain Atlas.NAT</article-title><source>Nature Methods</source><volume>12</volume><fpage>1039</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3581</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Warland</surname><given-names>D</given-names></name><name><surname>Van Steveninck</surname><given-names>RDR</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Spikes: Exploring the Neural Code</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohlfing</surname><given-names>T</given-names></name><name><surname>Maurer</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Nonrigid image registration in shared-memory multiprocessor environments with application to brains, breasts</article-title><source>IEEE Transactions on Information Technology in Biomedicine</source><volume>7</volume><fpage>16</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1109/titb.2003.808506</pub-id><pub-id pub-id-type="pmid">12670015</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saad</surname><given-names>FA</given-names></name><name><surname>Cusumano-Towner</surname><given-names>MF</given-names></name><name><surname>Schaechtle</surname><given-names>U</given-names></name><name><surname>Rinard</surname><given-names>MC</given-names></name><name><surname>Mansinghka</surname><given-names>VK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian synthesis of probabilistic programs for automatic data modeling.Proc</article-title><source>Proceedings of the ACM on Programming Languages</source><volume>3</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1145/3290350</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Samek</surname><given-names>W</given-names></name><name><surname>Montavon</surname><given-names>G</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Hansen</surname><given-names>LK</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-030-28954-6</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sandler</surname><given-names>RA</given-names></name><name><surname>Marmarelis</surname><given-names>VZ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Understanding spike-triggered covariance using Wiener theory for receptive field identification.J</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.1167/15.9.16</pub-id><pub-id pub-id-type="pmid">26230978</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Satou</surname><given-names>C</given-names></name><name><surname>Kimura</surname><given-names>Y</given-names></name><name><surname>Hirata</surname><given-names>H</given-names></name><name><surname>Suster</surname><given-names>ML</given-names></name><name><surname>Kawakami</surname><given-names>K</given-names></name><name><surname>Higashijima</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>C.Et al.transgenic tools to characterize neuronal properties of discrete populations of Zebrafish</article-title><source>Development</source><volume>140</volume><fpage>3927</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1242/dev.099531</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmid</surname><given-names>A</given-names></name><name><surname>Pyrski</surname><given-names>M</given-names></name><name><surname>Biel</surname><given-names>M</given-names></name><name><surname>Leinders-Zufall</surname><given-names>T</given-names></name><name><surname>Zufall</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Grueneberg ganglion neurons are finely tuned cold sensors.J</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>7563</fpage><lpage>7568</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0608-10.2010</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learnable Latent Embeddings for Joint Behavioral and Neural Analysis</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2204.00673</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-triggered neural characterization</article-title><source>Journal of Vision</source><volume>6</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/6.4.13</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Severi</surname><given-names>KE</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name><name><surname>Marques</surname><given-names>JC</given-names></name><name><surname>O’Malley</surname><given-names>DM</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural control and modulation of swimming speed in the larval Zebrafish.Neuron</article-title><source>Neuron</source><volume>83</volume><fpage>692</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.06.032</pub-id><pub-id pub-id-type="pmid">25066084</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamir</surname><given-names>M</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Implications of neuronal diversity on population coding.neural Comput</article-title><source>Neural Computation</source><volume>18</volume><fpage>1951</fpage><lpage>1986</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.8.1951</pub-id><pub-id pub-id-type="pmid">16771659</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Silversmith</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Cc3D: connected components on Multilabel 3d images</data-title><version designator="swh:1:rev:5562f4181fc82466b7a94209e34d14909f9b7f8e">swh:1:rev:5562f4181fc82466b7a94209e34d14909f9b7f8e</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b285e99cd81988e215e41c4ed932c7584beb2a1d;origin=https://github.com/seung-lab/connected-components-3d;visit=swh:1:snp:b95c35e68ff78b6bb6ece8fabf8ce6875fda22d8;anchor=swh:1:rev:5562f4181fc82466b7a94209e34d14909f9b7f8e">https://archive.softwareheritage.org/swh:1:dir:b285e99cd81988e215e41c4ed932c7584beb2a1d;origin=https://github.com/seung-lab/connected-components-3d;visit=swh:1:snp:b95c35e68ff78b6bb6ece8fabf8ce6875fda22d8;anchor=swh:1:rev:5562f4181fc82466b7a94209e34d14909f9b7f8e</ext-link></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dropout: A simple way to prevent neural networks from Overfitting</article-title><source>Mach. Learn. Res</source><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>C.Et al.spontaneous behaviors drive multidimensional, Brainwide</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>H</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Maheshwarnathan</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From deep learning to mechanistic understanding in Neuroscience: the structure of retinal prediction.Adv</article-title><source>Neural Inf. Process. Syst</source><volume>32</volume><fpage>8537</fpage><lpage>8547</lpage><pub-id pub-id-type="doi">10.48550/arXiv.1912.06207</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiele</surname><given-names>TR</given-names></name><name><surname>Donovan</surname><given-names>JC</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Descending control of swim posture by a Midbrain nucleus in Zebrafish.neuron</article-title><source>Neuron</source><volume>83</volume><fpage>679</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.018</pub-id><pub-id pub-id-type="pmid">25066082</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>JA</given-names></name><name><surname>Costabile</surname><given-names>JD</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mesencephalic representations of recent experience influence decision making</article-title><source>eLife</source><volume>5</volume><elocation-id>e16572</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16572</pub-id><pub-id pub-id-type="pmid">27454033</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ukita</surname><given-names>J</given-names></name><name><surname>Yoshida</surname><given-names>T</given-names></name><name><surname>Ohki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Characterisation of Nonlinear receptive fields of visual neurons by Convolutional neural network.SCI</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>3791</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-40535-4</pub-id><pub-id pub-id-type="pmid">30846783</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Leifer</surname><given-names>AM</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Large-scale neural recordings call for new insights to link brain and behavior.NAT</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>11</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00980-9</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Volterra</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1959">1959</year><source>Theory of Functionals and of Integral and Integro-Differential Equations</source><publisher-loc>New York</publisher-loc><publisher-name>Dover Publications</publisher-name></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wray</surname><given-names>J</given-names></name><name><surname>Green</surname><given-names>GGR</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Calculation of the Volterra kernels of non-linear dynamic systems using an artificial neural network.Biol</article-title><source>Biological Cybernetics</source><volume>71</volume><fpage>187</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1007/BF00202758</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zabihi</surname><given-names>M</given-names></name><name><surname>Mostafa Kia</surname><given-names>S</given-names></name><name><surname>Wolfers</surname><given-names>T</given-names></name><name><surname>Dinga</surname><given-names>R</given-names></name><name><surname>Llera</surname><given-names>A</given-names></name><name><surname>Bzdok</surname><given-names>D</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Marquand</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Non-Linearity Matters: A Deep Learning Solution to the Generalization of Hidden Brain Patterns across Population Cohorts</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.10.434856</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83289.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Clark</surname><given-names>Damon A</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.31.506108" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.31.506108"/></front-stub><body><p>This useful article describes a sensitive method for identifying the contributions of different behavioral and stimulus parameters to neural activity. The method has been convincingly validated using simulated data and applied to example state-of-the-art datasets from mouse and zebrafish. The method could be productively applied to a wide range of experiments in behavioral and systems neuroscience.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83289.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Clark</surname><given-names>Damon A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Yale University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.31.506108">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.31.506108v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A model-free approach to link neural activity to behavioral tasks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers were positive about the paper's approach. Below are several points agreed upon by the reviewers that would be important to address before potential publication.</p><p>1) R1 had several critiques of the comparison to STA/STC. All reviewers agree that a more complete exposition of the differences between this method and STA/STC would benefit the paper. This could include synthetic cases where the proposed methods works better than STA/STC (for instance requiring less data) or real examples from the datasets where the proposed method is better at classifying cell response types. This paper should describe or show the advantages of this type of analysis over prior ones, ideally using good apples-to-apples comparisons.</p><p>a. Relatedly, R1 was also skeptical of the utility of the linear methods shown in Figure 1; the other reviewers convinced R1 that the limited linear models used were in keeping with standard methods in the zebrafish field. The authors should better describe why these particular (limited) linear models were used, and perhaps cite examples from the field in support of these sorts of models.</p><p>2) The paper was a bit dense in some sections. The reviewers recommend moving some more mathematical details to the supplement and focusing on the most important results in the main text. (An example might be the curvature metric, which was a little opaque and did not seem central.)</p><p>3) The authors should draw finer distinctions between their method and some other, similar, prior methods, like the Ganguli/Baccus papers that used convolutional networks to fit retinal ganglion cell responses, in an approach that seems similar in some ways to the one used here. This relates also to several reviewer comments about what method exactly is being referred to as 'MINE'.</p><p>4) Figure 7: The authors should explain the significance of the result in Figure 7 with respect to signal processing. Are the authors saying that higher order interactions occur deeper in processing? This complexity analysis may also be an example of an approach that is possible with what is presented here but not with STA/STC.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) In MINE's first mention (line 43) and in showing that it can capture various response features, the authors seem to imply that MINE is simply the idea of using ANNs to model neural activity as function of either independent or dependent variables. But surely this has already been done many times. I'm having some trouble seeing what precisely is claimed as new in MINE. My thought was that it is also the first and second order interpretation, but that does not come through in the early descriptions of the method.</p><p>2) Related to this, in L55, the authors remark &quot;Aside from network architecture and initialization conditions MINE is essentially model-free.&quot; This is a pretty big constraint, by the model architecture, choice of activation function, etc. STA and STC and systems identification methods more generally seem far more truly model free. This suggests that the framing and title here might need significant reconsideration.</p><p>3) The authors make several claims about linear models in Figure 1 that I cannot figure out. In particular, at L137, the authors say that the linear model cannot learn the dynamics of the calcium indicator. But why not? If the stimulus is s_t, the kernel is k_t, and the response is r_t = (s*k)_t + eta_t, then this is exactly what a linear model should be able to learn perfectly, directly from the data through a least squares fit at each time point. I don't understand why an expanded linear model should be required to learn the calcium kernel. In 1F, why is the LM not working as well as the ANN for S1 and M2? Those appear to both be simple linear transformations where r_t = (s*k)_t + eta_t: perfect for a linear model and least squares fitting. (In fact, with Gaussian noise, no model should be able to outperform the least squares fit linear model to estimate k, given sufficient data.) Similarly, dS1/dt is still just a linear transformation of S1 and should be easily learned by a linear model, since it's now just r_t = (s*k')_t + eta_t, where k' is the derivative of the calcium kernel. These results – that the authors' linear model is not capturing linear transformations that exactly match the form of the linear model – are truly puzzling.</p><p>4) One of the big proposed advances of this paper is the interpretability of the fitted ANN model. By doing &quot;Taylor expansion&quot; on the model, the authors pull out linear and quadratic terms that describe the dependency of neurons on different non-neural variables (dependent and independent) in these experiments. I have a few comments on this, some more critical than others. Overall, I was not left with a strong sense of what the benefit is, if any, of this method over standard systems identification approaches.</p><p>a. This sort of expansion of a functional has more typically been called a Volterra expansion, and there exists a giant literature on estimating Volterra kernels for nonlinear systems. It would be appropriate to cite some of that work and adopt the terminology used in prior work in the field.</p><p>b. In comparing their methods to spike-triggered analyses, Figure 4 is not a fair comparison, since it has not decorrelated the stimuli in time, as has been in wide practice in systems identification for quite a while (see Korenberg in the 1990s, Baccus and Meister, 2002). Thus the fair comparison is with S4, and 4 should not be viewed as comparable and quite likely removed.</p><p>c. In the comparison between the Jacobian/Hessian of the ANN and the STA/STC equivalents in Figures4 and S4, I don't understand the authors' choice of nonlinearities, which are described after line 1105. In particular, g_sta(x) has a first derivative at x=0, but it's second derivative evaluated at x=0 is also non-zero. By adding this to the other function and putting them through a ReLU (equation 19), there will be both linear and quadratic components for both RFa and RFv, even if g_stc is symmetric, simply because the ReLU is approximated by a linear + quadratic (+ higher) terms. This means that with these functions, I would expect the STA to be a linear combination of the two linear projections (RFa and RFv) and I would expect the STC matrix to have eigenvectors spanning the 2-dimensional space of RFa and RFv. It's less clear why this isn't happening in the simulations. RFa could dominate the STA. In Figure 4B, it seems clear that the STC is contaminated by the STA, and given the analysis above, that is not surprising. Is that what is lowering the cosine similarity for the STC analysis in S4? Is only the first eigenvector being considered? If so, it's frequently the case that the STC dimensions also include the STA dimension; often, one reports only the STC components after the STA has been subtracted off. (See for instance Aljadeff et al., 2016.) It looks here as though the authors just report the eigenvector with largest eigenvalue. Is the STC really not working well, or is it just including components of the other filter (appropriately)? It's hard to believe that STC applied to a point nonlinearity acting on two projections of the stimulus would not return eigenvectors that span those two projections of the stimulus.</p><p>d. This brings up a conceptual point. The decomposition that the authors perform of the ANN into zeroth, first, second, and higher order terms in Figure 7D could just as easily be written for the raw responses of the neurons. In such a case, with an excellent model, the two expansions should be closely related to one another, and in some cases equivalent. The low-order terms in the Volterra series, when estimated experimentally, depend on higher order terms. (For instance a cubic Volterra term would show up in the linear filter.) Wiener series are typically estimated instead, since these produce an orthogonalized basis, so that this dependence is eliminated (for a given amplitude of inputs X). When computing the STC, one can eliminate the linear component first; that would then be the Wiener estimate of the second order kernel. The second order kernel computed as the Hessian of an ANN is not orthogonalized in the same way, so it might produce different results from STC, but it is not clear that this is what the authors show, or intend to show.</p><p>e. The authors on L346 say that higher order terms are contained in the ANN, so that the Jacobian will be different from spike triggered covariance analysis. I think this is correct. However, I'm not sure that makes the Hessian or Jacobian in the ANN a better analysis than the systems identification approaches. In my mind, this is potentially the only advantage of doing this ANN method over STA and STC; but if that's the advantage, then I'd want to be shown very clearly an example where this occurs and where the insight into the system was improved by using the Jacobian/Hessian rather than STA/STC. I imagine that such benefits could accrue when the fitted ANN is close to the true set of transformations of the system, but that would reduce the advantages of being able to do this with any old ANN. Overall, these comments get to the point of: under what conditions would the Jacobian/Hessian/ANN method be better or somehow more informative than STA/STC? And why fit to the ANN if one is going to interpret it with methods equivalent to STA/STC that could be performed directly on the data?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>While the paper could certainly be published in the current form, there are some points that as a reader I would like to have seen addressed further. I therefore offer some comments and suggestions that the authors might like to consider.</p><p>I believe there is a mistake in the formulae given for the Taylor metric in the Methods. Since the R^2 Full is always greater than the R^2 with fewer parameters, the formulae, as written, appear to give a number between 0 and -infinity, whereas I think the intention is that they should give a number between 0 and 1 (as appears in the paper).</p><p>An important consideration when comparing approaches to modeling neuronal responses is how much data is required to fit the model. In the comparison with regression models, or with the spike-triggered average it would be useful to show how the performance of each approach develops with the length of recording.</p><p>The paper could be improved, and the method made more appealing, by more clearly showing qualitative improvements that their approach offers over variations of linear regression methods. In the zebrafish part, there is a convincing argument made that the method allows more neurons to be fit at some threshold, but direct comparison of the outputs of both approaches is restricted to counting the numbers of ROIs identified in various classes. One possible explanation could be that these differences arise because the model-free method has greater sensitivity for identifying neurons within the same classes that are found with the LM method, but it would be more interesting if the model free method finds particular types of neurons that are clearly missed by the LM method. What could be compelling to see, for example, would be (1) differences in the spatial distributions of neuron classes found with the two methods, e.g. regions where neurons are only found with the model-free method or (2) Evidence that there are specific functional classes that are missed with the LM method – e.g. do the RF clusters vary in how well they are picked up by the LM?</p><p>It would be helpful to have some more information about the choice of network architecture, and how this was optimized. For example, 80 features in the convolutional layer seems like a lot for these relatively simple data sets, and it would be good to understand better how this number was chosen and how this choice can affect the outcome.</p><p>Intuitively, it seems like the temporal effects of the calcium indicator response would ideally be applied downstream of the computation of the neural response, but, in the text, this step is described as happening in the first layer of linear convolutional filters. Is this a constraint of the network design, and are there any practical consequences for the possible computations captured by the model?</p><p>I don't like that the distributions of neuronal types seem to be shown in the form of renderings of anatomical masks highlighting regions of enrichment. This rather obscures the true distributions, and makes them appear more cleanly separated than they are. For me, these visualizations don't offer much information that goes beyond the tabulation of regions. A direct representation of the density of different functional classes, as is often used in the field, would be richer in information, and might be a way to highlight qualitative differences between the populations identified using the LM and model-free approaches. Even if it is the case that there are not strong differences in the distributions of functional types, this by itself would be an interesting observation that merits discussion.</p><p>I would like to see more discussion of how the neural network models could be explored or leverage to give more insight into the responses of individual neurons. For example, having identified the relevant variables, and model complexity, could this information be used to explore simpler models in a more constrained way? Could visualization approaches like those used in Heras et al. 2019 https://doi.org/10.1371/journal.pcbi.1007354, be used to explore how the network is combining task variables?</p><p>Since the function is scalar valued, I believe that what is referred to in the paper as the Jacobian is also simply the gradient of the function. Perhaps this could be mentioned explicitly to aid understanding for a broad audience.</p><p>Overall, I congratulate the authors on developing this useful method, and I look forward to seeing it published.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. Overall, as mentioned in my Public Review section, I believe the paper is very valuable and merits publication in <italic>eLife</italic>. I do think that it would be a pity if the manuscript was not carefully re-read and rewritten to make it more accessible for the average reader who may want to implement this analysis on their data. An option would be to shift more material to the Methods and use the main text to convey the method and compare it with the available ones for the 3 datasets that are treated.</p><p>2. It would help to understand the various thresholds that appear throughout. I am, for example, not sure what to conclude from the sentence that starts on line 224 or how to interpret the red dashed line in Figure 5B (line 421).</p><p>3. Given that the zebrafish experiments were performed by the authors, could they comment on the reliability (across animals) in finding the functional activity described in Figures 6 and 7?</p><p>4. The code and data availability is exemplary.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;A model-free approach to link neural activity to behavioral tasks&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been much improved, but there is one large and a few small remaining issues that need to be addressed, as outlined below:</p><p>There is only one issue that remains, and it is the issue of calling this a &quot;model-free&quot; approach, noted by R1. Since no analysis is truly model free, there is a spectrum of how much constraint a model puts on an analysis. There are other approaches discussed in the paper that are closer to the model free end than this one (like STA, STC, MID), so this term does not seem like the right one to use in this case. The authors should find a more accurate description of what their method brings to field.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I appreciate the care that the authors have taken in addressing the concerns raised in the first round. I believe that the major hurdles were cleared in the revision. However, I do have a few comments that I believe should be addressed before final acceptance/publication.</p><p>1) This method is definitely not &quot;model free&quot;. It is a relaxed model requirement, or potentially a very general model, or a model with few assumptions, but I don't think it's just a matter of opinion as to whether it's &quot;model-free&quot;. Especially in comparison with STA and STC and Maximally informative dimensions, which all have even less of an underlying model framework… At a very basic level, the title and acronym must be accurate. Later in the paper, the authors argue that this method's model is actually providing important regularizing power in computing STC, etc. So it is definitely not model free, and the authors must find a different description for the title (and the acronym). I'm sure they can do this, but some suggestions: &quot;Flexible INE&quot;? CNNINE? &quot;Canine&quot;? &quot;Convolutional neurAl Network Identification of Neural Encoding&quot;? Something else? &quot;Model-agnostic&quot;?</p><p>2) Figure 7BC. The temporal kernels found by this method are a bit weird and seem not very biological – much of the power lies at 0 and 10 seconds, the two ends of the filter… Is this because the filters aren't long enough? These look quite different from true filters I'd expect to find in neurons, both in their duration and shape. I think this deserves some kind of explanation. I apologize that I didn't note this on the first round – I clearly got distracted by some other parts of the analysis.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83289.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers were positive about the paper's approach. Below are several points agreed upon by the reviewers that would be important to address before potential publication.</p></disp-quote><p>We thank the reviewers for carefully engaging with our manuscript and providing constructive criticism. We believe that our revisions address the raised concerns and considerably strengthen the paper. We provide detailed responses to individual criticisms below. We have reorganized the paper and simplified our definition of nonlinearity with the intention to make the paper less dense and easier to follow. We have also rewritten our comparison to STA/STC, explicitly comparing MINE to Volterra analysis to demonstrate that MINE provides a simpler and more robust way to extract receptive fields. We specifically note that it wasn’t our intention to claim that MINE could do something that all well-crafted model-based methods fail at, but rather that MINE provides an accessible framework for performing crucial analyses on the relationship of predictors and neural activity. We specifically highlight these points in different parts of the paper.</p><disp-quote content-type="editor-comment"><p>1) R1 had several critiques of the comparison to STA/STC. All reviewers agree that a more complete exposition of the differences between this method and STA/STC would benefit the paper. This could include synthetic cases where the proposed methods works better than STA/STC (for instance requiring less data) or real examples from the datasets where the proposed method is better at classifying cell response types. This paper should describe or show the advantages of this type of analysis over prior ones, ideally using good apples-to-apples comparisons.</p></disp-quote><p>We have changed our strategy of comparing MINE to system-identification approaches. We specifically point out that because of the structure of the CNN underlying MINE (taking predictors across time as inputs) the Taylor expansion of the network is in fact equivalent to the Volterra expansion of a system with finite memory corresponding to the time-length of the inputs given to MINE (Lines 150-153).</p><p>We use synthetic data simulating the action of two different filters and nonlinearities on the data to compare MINE to directly fitting the Volterra kernels via linear regression, an approach that in theory is truly model-free. Just like MINE, ordinary linear regression recovers the filters without problem when the input is Gaussian white noise. This highlights the fact that the comparison is indeed fair: Given constraints on the stimulus, our simulation allows full recovery of the filters. Approaching more naturalistic inputs however, ordinary linear regression fails to recover Volterra kernels from which the filters can be extracted while the filters can still be derived using MINE. We then show that increasing an L2 penalty using Ridge regression allows us to indeed recover the filters from directly fitting the Volterra kernels. However, depending on the structure of the filter, MINE still recovers more faithful representations and MINE is overall better in generalizing to new data when compared with the best-fit Ridge regression models (Lines 169-183).</p><p>We furthermore show that in the process of increasing the Ridge penalty, the effective degrees of freedom of the Ridge regression model approach the effective degrees of freedom of MINE. In other words, system analysis can be indeed truly model-free, e.g. by imposing no constraints on a regression model that fits the Volterra kernels, however, such an approach does not yield useful results. To obtain useful results, the degrees of freedom of the model have to be reduced, either through penalties on a regression fit or by imposing a structured neural network (Lines 193-200). We would also like to point out that the reduction in degrees of freedom is a general theme when applying system analysis to cases in which inputs are not gaussian white noise, whether this is through the use of basis functions, artificial neural networks or constrained regression.</p><disp-quote content-type="editor-comment"><p>a. Relatedly, R1 was also skeptical of the utility of the linear methods shown in Figure 1; the other reviewers convinced R1 that the limited linear models used were in keeping with standard methods in the zebrafish field. The authors should better describe why these particular (limited) linear models were used, and perhaps cite examples from the field in support of these sorts of models.</p></disp-quote><p>We better clarify our choice of model in the first section of the paper. Convolving inputs with a presumed calcium kernel and subsequently generating a model that contains all possible interaction terms is indeed common and we provide corresponding citations from the zebrafish and mouse literature. We also specifically state in the text now (Lines 80-82), that it is not our goal to show that MINE can do something <bold>no other model can</bold> but rather that MINE has advantages over predefined models by flexibly learning data transformations. We would also like to point out that in the comparison to the zebrafish data we already use a regression model that can learn temporal transformations and in spite of that MINE identified more neurons.</p><disp-quote content-type="editor-comment"><p>2) The paper was a bit dense in some sections. The reviewers recommend moving some more mathematical details to the supplement and focusing on the most important results in the main text. (An example might be the curvature metric, which was a little opaque and did not seem central.)</p></disp-quote><p>We have attempted to make the paper less dense. We hope that the reordering of sections as well as the simplification of measuring nonlinearity help with this goal. After showing the capability of MINE to fit arbitrary data relationships we now introduce the Taylor expansion in a section that introduces the derivation of nonlinearity and complexity. Both of these metrics are now based on truncations of the Taylor series which removes the need to introduce curvature and the NLC in detail and which should also increase interpretability of our definition of nonlinearity. We subsequently use the same Taylor expansion to explain the extraction of receptive fields which we contrast with Volterra analysis. Subsequently we provide details for identifying contributing predictors, again via Taylor expansion but this time in a point-wise manner to account for nonlinear relationships, motivated by the section on nonlinearity and complexity. We hope that this flow will make it easier for readers to follow along.</p><disp-quote content-type="editor-comment"><p>3) The authors should draw finer distinctions between their method and some other, similar, prior methods, like the Ganguli/Baccus papers that used convolutional networks to fit retinal ganglion cell responses, in an approach that seems similar in some ways to the one used here. This relates also to several reviewer comments about what method exactly is being referred to as 'MINE'.</p></disp-quote><p>We have addressed this point. We specifically discuss the Ganguli/Baccus papers in the discussion (Lines 482-488). We believe that this discussion leads to new insight in relation to some of our discoveries in the comparison to Volterra analysis. We also fully agree that we never explicitly clarified what MINE is. We now specifically state in the introduction (Lines 21-26) and at the beginning of the discussion (Lines 446-452) that we understand MINE as the combination for obtaining a predictive model of neural activity giving predictor input, characterizing the nonlinearity/complexity of the relationship between inputs and neural activity, identifying receptive fields and characterizing which inputs drive neural activity. As such MINE provides an accessible and easy-to-use interface for a nearly model-free characterization of how neurons encode task-relevant variables.</p><disp-quote content-type="editor-comment"><p>4) Figure 7: The authors should explain the significance of the result in Figure 7 with respect to signal processing. Are the authors saying that higher order interactions occur deeper in processing? This complexity analysis may also be an example of an approach that is possible with what is presented here but not with STA/STC.</p></disp-quote><p>We now explicitly discuss the significance of mapping computational complexity for understanding functional neural circuits (Lines 525-530). We have also increased our analysis of mixed-selectivity neurons highlighting another feature of MINE: Using the power of the predictive model to better understand how neurons integrate different features in our case thermosensory and motor features (Lines 393-411). We believe this to be unique over system-identification approaches as well. Specifically, our analysis in the comparison with Volterra approaches suggests that it would be very difficult to use system analysis if multiple predictors were involved as this further increases the challenge of obtaining a reasonably fitting model when omitting constraints.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) In MINE's first mention (line 43) and in showing that it can capture various response features, the authors seem to imply that MINE is simply the idea of using ANNs to model neural activity as function of either independent or dependent variables. But surely this has already been done many times. I'm having some trouble seeing what precisely is claimed as new in MINE. My thought was that it is also the first and second order interpretation, but that does not come through in the early descriptions of the method.</p></disp-quote><p>We agree with the reviewer that it was never properly spelled out what MINE is. We now clarify this point in the introduction (Lines 21-26) where we explicitly state: “Here we introduce “Model-free identification of neural encoding” (MINE). MINE combines convolutional neural networks (CNN) to learn mappings from predictors (stimuli, behavioral actions, internal states) to neural activity (Figure 1) with a deep characterization of this relationship. Using Taylor expansion approaches, MINE reveals the computational complexity such as the nonlinearity of the relationship (Figure 2), characterizes receptive fields as indicators of processing (Figure 3) and reveals the dependence of neural activity on specific predictors or their interactions (Figure 4).</p><disp-quote content-type="editor-comment"><p>2) Related to this, in L55, the authors remark &quot;Aside from network architecture and initialization conditions MINE is essentially model-free.&quot; This is a pretty big constraint, by the model architecture, choice of activation function, etc. STA and STC and systems identification methods more generally seem far more truly model free. This suggests that the framing and title here might need significant reconsideration.</p></disp-quote><p>While we agree that the architecture of the model, the type of training performed, etc. are indeed constraints, we stand by our point that MINE is essentially model-free when compared with other approaches in particular in the domain of approximating various functional forms. Specifically, while systems identification approaches can in theory be entirely model-free we demonstrate that in practice (outside a white-noise regime) they really aren’t. We compare effective degrees of freedom of the CNN used by MINE to models that directly fit the Volterra kernels to explicitly address this point (Lines 193-200). We would also like to point out that it is a general theme to reduce the degrees of freedom in systems identification when stimuli are departing from gaussian white noise in order to still be useful. One example is the use of Laguerre basis functions which effectively reduce the degrees of freedom of the fit or constrained regression approaches which we used here in the form of Ridge regression.</p><disp-quote content-type="editor-comment"><p>3) The authors make several claims about linear models in Figure 1 that I cannot figure out. In particular, at L137, the authors say that the linear model cannot learn the dynamics of the calcium indicator. But why not? If the stimulus is s_t, the kernel is k_t, and the response is r_t = (s*k)_t + eta_t, then this is exactly what a linear model should be able to learn perfectly, directly from the data through a least squares fit at each time point. I don't understand why an expanded linear model should be required to learn the calcium kernel. In 1F, why is the LM not working as well as the ANN for S1 and M2? Those appear to both be simple linear transformations where r_t = (s*k)_t + eta_t: perfect for a linear model and least squares fitting. (In fact, with Gaussian noise, no model should be able to outperform the least squares fit linear model to estimate k, given sufficient data.) Similarly, dS1/dt is still just a linear transformation of S1 and should be easily learned by a linear model, since it's now just r_t = (s*k')_t + eta_t, where k' is the derivative of the calcium kernel. These results – that the authors' linear model is not capturing linear transformations that exactly match the form of the linear model – are truly puzzling.</p></disp-quote><p>We are sorry about causing this confusion. Our explicit goal in section one was to compare MINE to a bare-bones linear regression model (no time-shifts, no interactions) which is bound to fail as well as a regression model that is often used in the analysis of calcium imaging data: pre-convolution with a calcium kernel and inclusion of interaction terms but no time-shifted regressors. We now provide citations to works using this type of model and given its prevalence we do believe it to be a useful benchmark. It is however absolutely clear that linear models can learn linear convolutions of the data (such as calcium kernels, computations of the derivative, etc.). We now specifically state that “While other models could clearly be designed to overcome these challenges, this further illustrates the point that a model-based approach is limited to the constraints of the chosen model.” (Line 80), to clarify that we do not claim that MINE does something magic. In the zebrafish section however, we use a linear comparison model that can indeed learn temporal dependencies which is the only reason why it can fit ~40% of the data. As a side note: We initially tried to include a linear model that would include all time-shifts and all interactions in section one, however this model never had any predictive power over validation data after fitting. Again, this could likely have been overcome by carefully fitting a regularization parameter but our main point is that MINE provides a convenient and accessible solution to this problem.</p><disp-quote content-type="editor-comment"><p>4) One of the big proposed advances of this paper is the interpretability of the fitted ANN model. By doing &quot;Taylor expansion&quot; on the model, the authors pull out linear and quadratic terms that describe the dependency of neurons on different non-neural variables (dependent and independent) in these experiments. I have a few comments on this, some more critical than others. Overall, I was not left with a strong sense of what the benefit is, if any, of this method over standard systems identification approaches.</p><p>a. This sort of expansion of a functional has more typically been called a Volterra expansion, and there exists a giant literature on estimating Volterra kernels for nonlinear systems. It would be appropriate to cite some of that work and adopt the terminology used in prior work in the field.</p></disp-quote><p>We agree with the reviewer and now explicitly state “Since the input to the CNN at the heart of MINE contains information about each predictor across time, the Taylor expansion introduced in the previous section (Figure 2A) is equivalent to the Volterra expansion of a system processing information across time with a filter memory equivalent to the history length of the CNN.” In line with this statement we now also reframe our former STA/STC comparison in terms of Volterra analysis, e.g., stating: “As a comparison, we used regression to directly fit the Volterra kernels (see Methods). Notably, just like MINE, apart from the truncation of the series after the 2nd term, the Volterra analysis is essentially model-free since any function can be approximated using an infinite Volterra series (Volterra, 1959).” We also provide a more comprehensive citation of literature.</p><disp-quote content-type="editor-comment"><p>b. In comparing their methods to spike-triggered analyses, Figure 4 is not a fair comparison, since it has not decorrelated the stimuli in time, as has been in wide practice in systems identification for quite a while (see Korenberg in the 1990s, Baccus and Meister, 2002). Thus the fair comparison is with S4, and 4 should not be viewed as comparable and quite likely removed.</p></disp-quote><p>We agree with the reviewer. Notably, our new approach, directly fitting the Volterra kernels using linear regression circumvents this problem. We have also expanded our comparison to ensure equal footing between MINE and the comparison approach by including regularization of the fit using Ridge regression.</p><disp-quote content-type="editor-comment"><p>c. In the comparison between the Jacobian/Hessian of the ANN and the STA/STC equivalents in Figures4 and S4, I don't understand the authors' choice of nonlinearities, which are described after line 1105. In particular, g_sta(x) has a first derivative at x=0, but it's second derivative evaluated at x=0 is also non-zero. By adding this to the other function and putting them through a ReLU (equation 19), there will be both linear and quadratic components for both RFa and RFv, even if g_stc is symmetric, simply because the ReLU is approximated by a linear + quadratic (+ higher) terms. This means that with these functions, I would expect the STA to be a linear combination of the two linear projections (RFa and RFv) and I would expect the STC matrix to have eigenvectors spanning the 2-dimensional space of RFa and RFv. It's less clear why this isn't happening in the simulations. RFa could dominate the STA. In Figure 4B, it seems clear that the STC is contaminated by the STA, and given the analysis above, that is not surprising. Is that what is lowering the cosine similarity for the STC analysis in S4? Is only the first eigenvector being considered? If so, it's frequently the case that the STC dimensions also include the STA dimension; often, one reports only the STC components after the STA has been subtracted off. (See for instance Aljadeff et al., 2016.) It looks here as though the authors just report the eigenvector with largest eigenvalue. Is the STC really not working well, or is it just including components of the other filter (appropriately)? It's hard to believe that STC applied to a point nonlinearity acting on two projections of the stimulus would not return eigenvectors that span those two projections of the stimulus.</p></disp-quote><p>In the previous comparison we allowed the STA/STC method access to the first two eigenvectors with the largest eigenvalues because of this seeming complication. We would like to note that we did not specifically design the simulation to be as compatible as possible with STA/STC. This would just have repeated a vast body of literature without being practically that relevant. We believe that the important question is whether given a certain system, its filters can be recovered by the analysis at hand. That being said, we now changed our approach to further alleviate this comparison problem: For both MINE and the linear regression fits we combine the first-order vector (J in the case of the CNN, k1 in the case of Volterra) and the second-order matrix (H in the case of the CNN, k2 in the case of Volterra) and extract the principal dynamic modes from this matrix according to Marmarelis, 2004. Here we extract the three eigenvectors with the highest eigenvalues and assign the best-fit cases to the “linear” and “nonlinear” filter (Figure 3). Notably, these filters are always contained within the eigenvectors with the largest two eigenvalues for MINE while the regression based fit sometimes produces a third vector with a larger eigenvalue. Therefore, by allowing to choose from the highest three we in fact give an advantage to the regression based fit. The fit on white-noise input data clearly validates this approach as it flawlessly recovers the filters for both methods. As expected, an ordinarily linear regression model fails in the case of highly correlated stimuli, but filters can be successfully recovered under strongly regularizing conditions, albeit with slightly worse quality than MINE. Furthermore, as regularization increases, the predictive power of the linear model decreases as well. MINE again has a slight edge in this regard.</p><disp-quote content-type="editor-comment"><p>d. This brings up a conceptual point. The decomposition that the authors perform of the ANN into zeroth, first, second, and higher order terms in Figure 7D could just as easily be written for the raw responses of the neurons. In such a case, with an excellent model, the two expansions should be closely related to one another, and in some cases equivalent. The low-order terms in the Volterra series, when estimated experimentally, depend on higher order terms. (For instance a cubic Volterra term would show up in the linear filter.) Wiener series are typically estimated instead, since these produce an orthogonalized basis, so that this dependence is eliminated (for a given amplitude of inputs X). When computing the STC, one can eliminate the linear component first; that would then be the Wiener estimate of the second order kernel. The second order kernel computed as the Hessian of an ANN is not orthogonalized in the same way, so it might produce different results from STC, but it is not clear that this is what the authors show, or intend to show.</p></disp-quote><p>We fully agree with the reviewer and have addressed this point as shown above. Again, as we point out in multiple places of the manuscript, we do not believe MINE to be a magical tool that can do something a carefully crafted model cannot. It simply provides a practical solution to a real-world problem. This is also further illustrated in the greater robustness of MINE to changes in the size of training data compared to directly fitting the Volterra kernels via regression (Figure S3J-I).</p><disp-quote content-type="editor-comment"><p>e. The authors on L346 say that higher order terms are contained in the ANN, so that the Jacobian will be different from spike triggered covariance analysis. I think this is correct. However, I'm not sure that makes the Hessian or Jacobian in the ANN a better analysis than the systems identification approaches. In my mind, this is potentially the only advantage of doing this ANN method over STA and STC; but if that's the advantage, then I'd want to be shown very clearly an example where this occurs and where the insight into the system was improved by using the Jacobian/Hessian rather than STA/STC. I imagine that such benefits could accrue when the fitted ANN is close to the true set of transformations of the system, but that would reduce the advantages of being able to do this with any old ANN. Overall, these comments get to the point of: under what conditions would the Jacobian/Hessian/ANN method be better or somehow more informative than STA/STC? And why fit to the ANN if one is going to interpret it with methods equivalent to STA/STC that could be performed directly on the data?</p></disp-quote><p>The message we intended with L346 was that the nonlinearity, which would be estimated separately for STA/STC, will be contained within the ANN itself which likely influences the retrieved filters. We did not mean to imply anything in relation to the quality of extracting filters and have struck this comment. The possibility of modeling higher-order terms of the CNN model however likely underlies the fact that it better generalizes to validation data (Figure 3C and F). This of course partly results because our nonlinearities are not designed to be well approximated by a quadratic function, i.e. the function that the fit of the Volterra kernels could fully approximate. While one might argue that this makes the comparison “unfair” we would argue that in real-life problems nonlinearities will seldom be constrained to a specific functional form (as we highlight in the introduction). We do believe that our new analysis provides a clear idea of the advantages of MINE, especially when taken in context of the entire manuscript: MINE allows extracting high-quality filters under more conditions than a direct fit of the Volterra kernels, the resulting model has more predictive power and MINE can extract more information from the fits than can easily be obtained by system identification approaches.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>While the paper could certainly be published in the current form, there are some points that as a reader I would like to have seen addressed further. I therefore offer some comments and suggestions that the authors might like to consider.</p><p>I believe there is a mistake in the formulae given for the Taylor metric in the Methods. Since the R^2 Full is always greater than the R^2 with fewer parameters, the formulae, as written, appear to give a number between 0 and -infinity, whereas I think the intention is that they should give a number between 0 and 1 (as appears in the paper).</p></disp-quote><p>We fully agree with the reviewer and apologize for this oversight. We computed the correct quantities in our code but miswrote the formulas in the method section. We have corrected this mistake.</p><disp-quote content-type="editor-comment"><p>An important consideration when comparing approaches to modeling neuronal responses is how much data is required to fit the model. In the comparison with regression models, or with the spike-triggered average it would be useful to show how the performance of each approach develops with the length of recording.</p></disp-quote><p>We agree with the reviewer and have included a corresponding analysis in our discussion of receptive field extraction. Figure S3I-J and Lines 184-192 in the text.</p><disp-quote content-type="editor-comment"><p>The paper could be improved, and the method made more appealing, by more clearly showing qualitative improvements that their approach offers over variations of linear regression methods. In the zebrafish part, there is a convincing argument made that the method allows more neurons to be fit at some threshold, but direct comparison of the outputs of both approaches is restricted to counting the numbers of ROIs identified in various classes. One possible explanation could be that these differences arise because the model-free method has greater sensitivity for identifying neurons within the same classes that are found with the LM method, but it would be more interesting if the model free method finds particular types of neurons that are clearly missed by the LM method. What could be compelling to see, for example, would be (1) differences in the spatial distributions of neuron classes found with the two methods, e.g. regions where neurons are only found with the model-free method or (2) Evidence that there are specific functional classes that are missed with the LM method – e.g. do the RF clusters vary in how well they are picked up by the LM?</p></disp-quote><p>We agree with the reviewer that this is an important point to address. However, whether MINE identifies qualitatively new types or not will also depend on the level of analysis detail. We currently only perform some coarse divisions of neural types to demonstrate various types of information that can be obtained with MINE. As the reviewer suggested we did compare the anatomical distribution of identified types. Making cluster maps (see below where we address our change in visualization) it does indeed appear as though there are parts of the brain where the linear model does not identify stimulus related neurons (dorsal cerebellum, anterior hindbrain). However, we were not comfortable adding this data to the paper. The lack of neurons in these regions could be because the linear model cannot identify “these types of neurons” or simply because it is less sensitive and by chance did not identify neurons within sparse regions.</p><p>We did assess how many neurons the linear model identifies in each receptive field cluster and added this information to Figure S7D. While there are no clusters that exclusively contain neurons identified by MINE, there are five clusters where the linear model identifies much fewer neurons than expected based on the overall identified fraction. We discuss this result (Lines 594-601) and point to the fact that this has consequences if the linear model data would be used for clustering in the first place, reducing the number of uniquely identified receptive field clusters from 15 to 4. So while MINE largely increases sensitivity, this has real-world consequences on the analysis which we believe to be important.</p><disp-quote content-type="editor-comment"><p>It would be helpful to have some more information about the choice of network architecture, and how this was optimized. For example, 80 features in the convolutional layer seems like a lot for these relatively simple data sets, and it would be good to understand better how this number was chosen and how this choice can affect the outcome.</p></disp-quote><p>We did not systematically explore network architectures and did not optimize the architecture. In part because we wanted to avoid pre-conceiving an architecture that we thought might be best for the data at hand. We have now clarified this in the methods section “Design and training of convolutional network.” In that section we also provide more detail on our rationale for using 80 convolutional layers. We do not think that 80 features are in fact extracted, but having a large number of layers means a large variation in random features that are extracted pre-training which can aid in network training. Since our convolutional layers are linear, each node in the first dense layer in fact uses one weighted average filter that acts on the input data.</p><disp-quote content-type="editor-comment"><p>Intuitively, it seems like the temporal effects of the calcium indicator response would ideally be applied downstream of the computation of the neural response, but, in the text, this step is described as happening in the first layer of linear convolutional filters. Is this a constraint of the network design, and are there any practical consequences for the possible computations captured by the model?</p></disp-quote><p>We agree with the reviewer that in measurements of brain activity, the convolution by the calcium indicator occurs after any other transformations. We perform all convolutions at the input layer to increase the simplicity of the network. After the initial convolutional layers all timing information is lost in the network which reduces the number of required parameters over a design that allows for convolution at the input and just before the output node. While this does limit expressivity of the network somewhat (since we do not allow for convolutions of the results of the nonlinear operations as would be expected in a calcium imaging experiment) we think that these effects are small and justified by the gain in simplicity in the network. Since we fit separate networks for each neuron, having fewer parameters leads to substantial speed gains. We note however, that all procedures implemented in MINE are agnostic to network architecture and it wouldn’t be a fundamental problem to make this change.</p><disp-quote content-type="editor-comment"><p>I don't like that the distributions of neuronal types seem to be shown in the form of renderings of anatomical masks highlighting regions of enrichment. This rather obscures the true distributions, and makes them appear more cleanly separated than they are. For me, these visualizations don't offer much information that goes beyond the tabulation of regions. A direct representation of the density of different functional classes, as is often used in the field, would be richer in information, and might be a way to highlight qualitative differences between the populations identified using the LM and model-free approaches. Even if it is the case that there are not strong differences in the distributions of functional types, this by itself would be an interesting observation that merits discussion.</p></disp-quote><p>We agree with the reviewer and have adjusted our visualizations. While we keep the bar-plot style tabulations for the comparison of stimulus/behavior/mixed and swim start/vigor/direction we now plot centroid maps in all cases. We created these maps using spatial clustering. In brief, we merge neurons into spatial clusters based on proximity (threshold of 5um) and then display all clusters that have at least 10 neurons. While this does not display the entire population of neurons it does give a better sense of the overall distribution. Structure is also visible when showing all neurons, however in 2D projections differences between areas where neurons actually group together versus areas in which there are multiple single neurons across 3D space are obscured. We therefore opted for this middle-ground between thresholding based on anatomical regions and a plot with disregard for spatial grouping. We hope that this addresses the point raised by the reviewer.</p><disp-quote content-type="editor-comment"><p>I would like to see more discussion of how the neural network models could be explored or leverage to give more insight into the responses of individual neurons. For example, having identified the relevant variables, and model complexity, could this information be used to explore simpler models in a more constrained way? Could visualization approaches like those used in Heras et al. 2019 https://doi.org/10.1371/journal.pcbi.1007354, be used to explore how the network is combining task variables?</p></disp-quote><p>We agree with the reviewer and now exploit a similar visualization for the mixed selectivity neurons (Figure 7D and S7E). Since our inputs are stimuli/behaviors across time we cannot directly visualize the effect of one variable on the output. We therefore exploit the receptive fields to provide varying strengths of inputs along the different axis which we refer to as drive. We think that this is also another good example for the power of MINE, exploiting the ability of extracting receptive fields together with the presence of a predictive model that encapsulates nonlinear relationships.</p><disp-quote content-type="editor-comment"><p>Since the function is scalar valued, I believe that what is referred to in the paper as the Jacobian is also simply the gradient of the function. Perhaps this could be mentioned explicitly to aid understanding for a broad audience.</p></disp-quote><p>We fully agree with the reviewer and have adjusted our writing. We still refer to the first order derivatives as J in the formulas but refer to it as the gradient of the function (e.g., Lines 101-111 in the section about computational complexity).</p><disp-quote content-type="editor-comment"><p>Overall, I congratulate the authors on developing this useful method, and I look forward to seeing it published.</p></disp-quote><p>We thank the reviewer for their assessment and hope that our additions have improved the paper.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. Overall, as mentioned in my Public Review section, I believe the paper is very valuable and merits publication in eLife. I do think that it would be a pity if the manuscript was not carefully re-read and rewritten to make it more accessible for the average reader who may want to implement this analysis on their data. An option would be to shift more material to the Methods and use the main text to convey the method and compare it with the available ones for the 3 datasets that are treated.</p></disp-quote><p>We greatly appreciate the reviewer’s positive assessment of our work. We hope that our rewrite addresses concerns with respect to readability and accessibility. We think that readers will now have an easier time following along, after we rearranged the sections of the paper and simplified our analysis of non-linearity. We also attempted to shift more details to the methods, however this wasn’t entirely possible in all places. We had considered presenting MINE as a black-box-model essentially demonstrating its usefulness on the three types of data we present (ground-truth, cortex and zebrafish). We would then have split details to some form of appendix as the reviewer suggests. However, ultimately we felt that this is also not a satisfying solution. While it certainly increases accessibility we worried that it would make it harder to judge the validity of our approach. We hope that we found a satisfying middle-ground in the revision.</p><disp-quote content-type="editor-comment"><p>2. It would help to understand the various thresholds that appear throughout. I am, for example, not sure what to conclude from the sentence that starts on line 224 or how to interpret the red dashed line in Figure 5B (line 421).</p></disp-quote><p>We agree with the reviewer that it is important to clarify thresholds. We have removed the threshold from the Taylor decomposition section (now around line 226) and corresponding figures as it is indeed not very useful. What we tried to say is that it is possible to identify thresholds which can separate true contributing predictors from noise. However, since we do not use this threshold later it really doesn’t serve a purpose.</p><p>We have clarified our choice of threshold for the cortical dataset (now around lines 248-252) which is much lower than the zebrafish one, since the data does not have cellular resolution. It is therefore very likely that every component we fit contains activity from many neurons that aren’t task related, limiting the amount of variance that could possibly be explained by a model.</p><p>We also added an explanation for our threshold used on the zebrafish data and why it is considerably higher than that used on cortical data (Line 319).</p><disp-quote content-type="editor-comment"><p>3. Given that the zebrafish experiments were performed by the authors, could they comment on the reliability (across animals) in finding the functional activity described in Figures 6 and 7?</p></disp-quote><p>We thank the reviewer for this question and we have added this important information to the paper. Specifically, in Figure 6E we now state for each functional type in how many fish it was identified. All but four types were identified across at least 10 fish, with the lowest number being four fish. Accordingly, we state in the text that each type was identified in multiple fish (Line 337). We also counted how many fish contribute to each receptive field cluster. We report in the text that each type was identified in at least 15 fish (Line 383).</p><disp-quote content-type="editor-comment"><p>4. The code and data availability is exemplary.</p></disp-quote><p>We thank the reviewer for this assessment.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been much improved, but there is one large and a few small remaining issues that need to be addressed, as outlined below:</p><p>There is only one issue that remains, and it is the issue of calling this a &quot;model-free&quot; approach, noted by R1. Since no analysis is truly model free, there is a spectrum of how much constraint a model puts on an analysis. There are other approaches discussed in the paper that are closer to the model free end than this one (like STA, STC, MID), so this term does not seem like the right one to use in this case. The authors should find a more accurate description of what their method brings to field.</p></disp-quote><p>We thank the reviewers for again engaging with our manuscript and providing constructive criticism. We have made textual changes (and one figure clarification suggested by R1) to address the remaining concerns. This specifically includes a change to the title and moniker of the method to refrain calling it “model-free”. We agree with R1 that it is important to be precise about this point, and now focus on the idea that the neural network allows the method to be flexible in terms of the function/model it can implement. This was our reason for calling it “model-free” in the first place – not that it is free from constraints on <italic>parameters</italic> but that it is free(er) from constraints on the <italic>functional form</italic>. We therefore changed the title to “Model discovery to link neural activity to behavioral tasks”. We kept the acronym “MINE” and simply struck the term “free”, referring to it as “Model identification of neural encoding” throughout the text. We only kept one reference to the “model-free” idea, when we talk about the lack of assumptions on the underlying distribution of data but added the caveat “largely model-free” in this statement. All other references to the method being “model-free” have been struck from the text.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I appreciate the care that the authors have taken in addressing the concerns raised in the first round. I believe that the major hurdles were cleared in the revision. However, I do have a few comments that I believe should be addressed before final acceptance/publication.</p></disp-quote><p>We thank the reviewer for their assessment and believe that our new changes have addressed the remaining concerns.</p><disp-quote content-type="editor-comment"><p>1) This method is definitely not &quot;model free&quot;. It is a relaxed model requirement, or potentially a very general model, or a model with few assumptions, but I don't think it's just a matter of opinion as to whether it's &quot;model-free&quot;. Especially in comparison with STA and STC and Maximally informative dimensions, which all have even less of an underlying model framework… At a very basic level, the title and acronym must be accurate. Later in the paper, the authors argue that this method's model is actually providing important regularizing power in computing STC, etc. So it is definitely not model free, and the authors must find a different description for the title (and the acronym). I'm sure they can do this, but some suggestions: &quot;Flexible INE&quot;? CNNINE? &quot;Canine&quot;? &quot;Convolutional neurAl Network Identification of Neural Encoding&quot;? Something else? &quot;Model-agnostic&quot;?</p></disp-quote><p>We have changed the title of the manuscript as well as the text to be precise about this point. We now emphasize the idea that the advantage of MINE is that it is flexible with respect to the functional form it can implement, turning model-based analysis on its head: Instead of defining a model and subsequently fitting parameters, MINE will discover a model which we subsequently characterize. As stated above, we therefore changed the title to “Model discovery to link neural activity to behavioral tasks”. We kept the acronym “MINE” and simply struck the term “free”, referring to it as “Model identification of neural encoding” throughout the text. We only kept one reference to the “model-free” idea, when we talk about the lack of assumptions on the underlying distribution of data but added the caveat “largely model-free” in this statement. All other references to the method being “model-free” have been struck from the text. The changes made to address this point are highlighted in the tracked version of the manuscript at the end of this reviewer response.</p><disp-quote content-type="editor-comment"><p>2) Figure 7BC. The temporal kernels found by this method are a bit weird and seem not very biological – much of the power lies at 0 and 10 seconds, the two ends of the filter… Is this because the filters aren't long enough? These look quite different from true filters I'd expect to find in neurons, both in their duration and shape. I think this deserves some kind of explanation. I apologize that I didn't note this on the first round – I clearly got distracted by some other parts of the analysis.</p></disp-quote><p>We agree with the reviewer that this should be addressed. To this end, we have added a caveat to the result section, where we now explicitly state:</p><p>“We note that some of the uncovered receptive fields (e.g., clusters 1 and 4) have the largest departures from 0 at the starts and ends of the receptive fields. This might indicate that the chosen history length (here 10 s) is too short and does not cover the entire timescale of processing which could be a result of the rather slow nuclear calcium indicator we chose for this study.”</p></body></sub-article></article>