<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">62329</article-id><article-id pub-id-type="doi">10.7554/eLife.62329</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Exposing distinct subcortical components of the auditory brainstem response evoked by continuous naturalistic speech</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-178906"><name><surname>Polonenko</surname><given-names>Melissa J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1914-6117</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-19568"><name><surname>Maddox</surname><given-names>Ross K</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2668-0238</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Neuroscience</institution>, <institution>University of Rochester</institution>, <addr-line><named-content content-type="city">Rochester</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution content-type="dept">Neuroscience, Biomedical Engineering</institution>, <institution>University of Rochester</institution>, <addr-line><named-content content-type="city">Rochester</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-27787"><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name><role>Reviewing editor</role><aff><institution>Imperial College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>ross.maddox@rochester.edu</email> (RM);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>17</day><month>02</month><year>2021</year></pub-date><volume>10</volume><elocation-id>e62329</elocation-id><history><date date-type="received"><day>21</day><month>08</month><year>2020</year></date><date date-type="accepted"><day>16</day><month>02</month><year>2021</year></date></history><permissions><copyright-statement>Â© 2021, Polonenko &amp; Maddox</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Polonenko &amp; Maddox</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-62329-v1.pdf"/><abstract><p>Speech processing is built upon encoding by the auditory nerve and brainstem, yet we know very little about how these processes unfold in specific subcortical structures. These structures are deep and respond quickly, making them difficult to study during ongoing speech. Recent techniques begin to address this problem, but yield temporally broad responses with consequently ambiguous neural origins. Here we describe a method that pairs re-synthesized 'peaky' speech with deconvolution analysis of EEG recordings. We show that in adults with normal hearing, the method quickly yields robust responses whose component waves reflect activity from distinct subcortical structures spanning auditory nerve to rostral brainstem. We further demonstrate the versatility of peaky speech by simultaneously measuring bilateral and ear-specific responses across different frequency bands, and discuss important practical considerations such as talker choice. The peaky speech method holds promise as a tool for investigating speech encoding and processing, and for clinical applications.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R00DC014288</award-id><principal-award-recipient><name><surname>Maddox</surname><given-names>Ross K</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects gave written informed consent before the experiment began. All experimental procedures were approved by the University of Rochester Research Subjects Review Board. (#1227).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Python code is available on the lab GitHub account (https://github.com/maddoxlab/peaky-speech). All EEG recordings are posted in the EEG-BIDS format (Pernet et al., 2019) to Dryad (https://doi.org/10.5061/dryad.12jm63xwd). Stimulus files necessary to derive the peaky speech responses are deposited in the same Dryad repository.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Polonenko MJ</collab><collab>Maddox RK</collab></person-group><year iso-8601-date="2020">2020</year><source>Exposing distinct subcortical components of the auditory brainstem response evoked by continuous naturalistic speech</source><ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.12jm63xwd">http://dx.doi.org/10.5061/dryad.12jm63xwd</ext-link><comment>Dryad Digital Repository, doi:10.5061/dryad.12jm63xwd</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-62329-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>