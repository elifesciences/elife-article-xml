<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80880</article-id><article-id pub-id-type="doi">10.7554/eLife.80880</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Nested mechanosensory feedback actively damps visually guided head movements in <italic>Drosophila</italic></article-title></title-group><contrib-group><contrib contrib-type="author" id="author-218812"><name><surname>Cellini</surname><given-names>Benjamin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0609-7662</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="con1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-217322"><name><surname>Mongeau</surname><given-names>Jean-Michel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3292-6911</contrib-id><email>jmmongeau@psu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04p491231</institution-id><institution>Department of Mechanical Engineering, Pennsylvania State University</institution></institution-wrap><addr-line><named-content content-type="city">University Park</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>19</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e80880</elocation-id><history><date date-type="received" iso-8601-date="2022-06-07"><day>07</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-17"><day>17</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-06-20"><day>20</day><month>06</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.26207/1s7c-z156"/></event></pub-history><permissions><copyright-statement>© 2022, Cellini and Mongeau</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Cellini and Mongeau</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80880-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80880-figures-v2.pdf"/><abstract><p>Executing agile locomotion requires animals to integrate sensory feedback, often from multiple sources. For example, human gaze is mediated by multiple feedback loops that integrate visual and vestibular information. A central challenge in studying biological feedback loops is that they are nested and dynamically coupled. Here, we develop a framework based on control theory for unraveling nested feedback systems and apply it to study gaze stabilization in the fruit fly (<italic>Drosophila</italic>). By combining experimental and mathematical methods to manipulate control topologies, we uncovered the role of body-generated mechanosensory feedback nested within visual feedback in the control of head movements. We discovered that visual feedback changed the tuning of head movements across visual motion frequencies whereas mechanosensory feedback damped head movements. Head saccades had slower dynamics when the body was free to move, further pointing to the role of damping via mechanosensory feedback. By comparing head responses between self-generated and externally generated body motion, we revealed a nonlinear gating of mechanosensory feedback that is motor-context dependent. Altogether, our findings reveal the role of nested feedback loops in flies and uncover mechanisms that reconcile differences in head kinematics between body-free and body-fixed flies. Our framework is generalizable to biological and robotic systems relying on nested feedback control for guiding locomotion.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>sensory fusion</kwd><kwd>insect flight</kwd><kwd>control theory</kwd><kwd>vision</kwd><kwd>proprioception</kwd><kwd><italic>Drosophila</italic></kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000181</institution-id><institution>Air Force Office of Scientific Research</institution></institution-wrap></funding-source><award-id>FA9550-20-1-0084</award-id><principal-award-recipient><name><surname>Mongeau</surname><given-names>Jean-Michel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Alfred P. Sloan Foundation</institution></institution-wrap></funding-source><award-id>FG-2021-16388</award-id><principal-award-recipient><name><surname>Mongeau</surname><given-names>Jean-Michel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Motor context and mechanosensory feedback together influence how flies control head movements during visually guided flight maneuvers.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animal locomotion and the associated neural computations are closed-loop (<xref ref-type="bibr" rid="bib18">Cowan et al., 2014</xref>; <xref ref-type="bibr" rid="bib69">Roth et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">Madhav and Cowan, 2020</xref>). During locomotion, sensory systems measure external and internal states. This sensory information is then processed by the brain to guide motor decisions and the resulting movement shapes sensory inputs, thus closing the loop. Visually active animals often integrate visual and mechanosensory information to guide movement through complex environments (<xref ref-type="bibr" rid="bib54">Mongeau et al., 2021</xref>; <xref ref-type="bibr" rid="bib32">Frye, 2010</xref>). For instance, hawk moths integrate information from visual and mechanosensory pathways when feeding from flowers moving in the wind (<xref ref-type="bibr" rid="bib70">Roth et al., 2016</xref>), glass knifefish rely on a combination of visual and electrosensory feedback to regulate their position within a moving refuge (<xref ref-type="bibr" rid="bib79">Sutton et al., 2016</xref>), and flies stabilize vision via antennal feedback (<xref ref-type="bibr" rid="bib34">Fuller et al., 2014b</xref>). For many of these behaviors, the sensors—e.g. eyes and proboscis in hawk moths—measure the same information (e.g. flower motion) in parallel. This is fundamentally a parallel sensory fusion problem where the animal must weight information from parallel pathways (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The Kalman filter has been applied to biological and robotic systems to solve similar sensory fusion problems and determine the optimal weight for each sensor (<xref ref-type="bibr" rid="bib78">Sun and Deng, 2004</xref>; <xref ref-type="bibr" rid="bib25">Ernst and Banks, 2002</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Parallel and nested sensory fusion in biological systems.</title><p>(<bold>A</bold>) Control model of parallel sensory fusion. Multiple sensory systems, <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub>, measure an external reference state <inline-formula><mml:math id="inf1"><mml:mi>R</mml:mi></mml:math></inline-formula> with respect to the system’s motion <inline-formula><mml:math id="inf2"><mml:mi>Y</mml:mi></mml:math></inline-formula>. The information measured by <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub> is fused together in parallel by a neural controller <inline-formula><mml:math id="inf3"><mml:mi>C</mml:mi></mml:math></inline-formula> to maintain equilibrium. The neural controller drives locomotion through the system’s biomechanics <inline-formula><mml:math id="inf4"><mml:mi>P</mml:mi></mml:math></inline-formula>, which feeds back to shape future sensory inputs, thus closing the loop. (<bold>B</bold>) Control model of nested sensory fusion. Same as (A) but one of the sensory systems (S<sub>2</sub>) does not directly measure the external reference state <inline-formula><mml:math id="inf5"><mml:mi>R</mml:mi></mml:math></inline-formula>. Instead the system state is directly fed to the neural controller <inline-formula><mml:math id="inf6"><mml:mi>C</mml:mi></mml:math></inline-formula> (purple). Thus <italic>S</italic><sub>2</sub> is not involved with measuring external sensory states.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig1-v2.tif"/></fig><p>In contrast to parallel sensory fusion—where sensory information operates at the same level in the control hierarchy—sensory feedback is often nested within higher levels of control (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="bibr" rid="bib37">Hardcastle and Krapp, 2016</xref>; <xref ref-type="bibr" rid="bib54">Mongeau et al., 2021</xref>). Consider the goal-directed task of visually navigating through a complex environment. Vision provides slower and higher level information for guidance whereas mechanosensory inputs due to self-motion—measured by the vestibular and somatosensory systems—influence rapid, lower level postural reflexes (<xref ref-type="bibr" rid="bib7">Bent et al., 2004</xref>; <xref ref-type="bibr" rid="bib35">Goldberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib59">Nakahira et al., 2021</xref>). In this context, mechanosensory feedback is <italic>nested</italic> within visual feedback because it is activated by visually guided locomotion, and thus it does not directly relate to the task goal (<xref ref-type="bibr" rid="bib54">Mongeau et al., 2021</xref>). This sensorimotor organization is analogous to cascade control in engineering controller design, where there are inner feedback loops nested within outer loops (<xref ref-type="bibr" rid="bib46">Krishnaswamy et al., 1990</xref>). While many prior studies have investigated how animals integrate sensory information from multiple pathways, the case where one sensory pathway is nested within another has received significantly less attention. How does the brain integrate nested sensory feedback for effective locomotion?</p><p>One exemplar sensorimotor system that includes nested mechanosensory feedback is the gaze stabilization reflex. Primates move their eyes and head in response to visual motion to stabilize gaze, termed the optokinetic response (OKR) (<xref ref-type="bibr" rid="bib47">Land, 2019</xref>). Eye and head movements feedback to shape visual inputs, and measurements of head motion from the vestibular system feedback to keep the eyes steady with respect to the head—termed the vestibulo-ocular reflex (VOR) (<xref ref-type="bibr" rid="bib35">Goldberg et al., 2012</xref>). Although both the OKR and the VOR are reflexive stabilization feedback loops, the VOR is nested within the OKR. Prior work showed that the OKR and VOR are inversely tuned: the OKR responds strongly to low frequencies and the VOR is tuned to higher frequencies (<xref ref-type="bibr" rid="bib73">Schweigart et al., 1997</xref>; <xref ref-type="bibr" rid="bib2">Barnes, 1993</xref>). However, the specific contributions of visual and nested mechanosensory feedback when the OKR and VOR are active together remains unclear. Intriguingly, the gaze stabilization response of flies shows close parallels to the primate visuomotor response, with similar feedback topology, making it an accessible model system to unravel the mechanisms underlying nested feedback control (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Elzinga et al., 2012</xref>).</p><p>Here, we studied nested sensorimotor feedback loops in fruit flies (<italic>Drosophila</italic>), with a specific focus on teasing apart the contributions of visual and nested mechanosensory feedback during gaze stabilization. The gaze stabilization reflex consists of multiple motor systems—the head and body—that operate in closed-loop with the goal of reducing optic flow across the retina and keeping gaze level (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>). The halteres—gyroscope-like organs that encode body velocity by sensing gyroscopic forces and structure the timing of motor outputs in flies (<xref ref-type="bibr" rid="bib30">Fraenkel, 1939</xref>; <xref ref-type="bibr" rid="bib61">Nalbach and Hengstenberg, 1994</xref>; <xref ref-type="bibr" rid="bib20">Dickerson et al., 2019</xref>)—also influence the control of head and body movements about all three rotational axes (<xref ref-type="bibr" rid="bib37">Hardcastle and Krapp, 2016</xref>; <xref ref-type="bibr" rid="bib57">Mureli and Fox, 2015</xref>; <xref ref-type="bibr" rid="bib58">Mureli et al., 2017</xref>; <xref ref-type="bibr" rid="bib60">Nalbach, 1993</xref>; <xref ref-type="bibr" rid="bib64">Rauscher and Fox, 2021</xref>). When visual inputs activate the gaze stabilization reflex and drive a compensatory response of the head and body, the halteres presumably sense the resulting body velocity and provide mechanosensory information that further influences a fly’s visuomotor behavior. Thus it would follow that mechanosensory feedback is inherently nested within visual feedback. Studying gaze stabilization in flies can therefore provide insights into how nested feedback loops interconnect and shape higher level loops in animal locomotion. Established experimental paradigms for studying fly flight provide a unique opportunity to manipulate control topologies, allowing us to break feedback loops and tease out the role of visual and nested mechanosensory feedback. In contrast to prior work that studied the parallel integration of visual and haltere information in open-loop—where flies had their head and body fixed in place (<xref ref-type="bibr" rid="bib22">Dickinson, 1999</xref>; <xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>; <xref ref-type="bibr" rid="bib75">Sherman and Dickinson, 2004</xref>)—here we employ an experimental paradigm that allowed flies to freely move their head and body in closed-loop (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In conjunction with empirical data, we synthesized a control model for mathematically teasing apart the role of nested sensory feedback. We applied this model to study how body-generated visual and nested mechanosensory feedback are integrated during the control of head movements. Our results provide new insights into how nested sensory feedback may be structured across phyla for gaze stabilization.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Control model of visual and nested mechanosensory feedback during gaze stabilization in fly flight.</title><p>(<bold>A</bold>) The control framework used to model and analyze the gaze stabilization system in body-free flies. Flies respond to an external visual perturbation <inline-formula><mml:math id="inf7"><mml:mi>R</mml:mi></mml:math></inline-formula> by attempting to minimize the sensory visual error <inline-formula><mml:math id="inf8"><mml:mi>E</mml:mi></mml:math></inline-formula> measured by their visual system. Neural control circuits in the brain for the head <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and body <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> process the sensory error and send motor control signals to the corresponding biomechanical systems <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to generate head <inline-formula><mml:math id="inf13"><mml:mi>H</mml:mi></mml:math></inline-formula> and body <inline-formula><mml:math id="inf14"><mml:mi>B</mml:mi></mml:math></inline-formula> movements. The fly’s gaze <inline-formula><mml:math id="inf15"><mml:mi>G</mml:mi></mml:math></inline-formula> is controlled by the sum of head and body movements, which feeds back to shape the sensory error entering the visual system. Flies also measure mechanosensory information associated with body motion via the halteres, which is processed in the brain by analogous controllers for the head <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and body <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and also contributes to shaping head and body responses. In this paradigm, mechanosensory feedback is nested within visual feedback. (<bold>B</bold>) The magnetic tether experimental paradigm for body-free flies corresponding to (<bold>A</bold>). A fly is tethered to a pin which is placed in a low-friction bearing and suspended in a magnetic field, allowing free rotation about the vertical (yaw) axis. (<bold>C</bold>) Same as (<bold>A</bold>) but for a body-fixed fly. Note that contributions of body visual feedback and nested mechanosensory feedback due to body motion are no longer present. The fly’s gaze is now purely determined by head movements. (<bold>D</bold>) The rigid tether experimental paradigm for body-fixed flies corresponding to (<bold>C</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig2-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Control model of visual and nested mechanosensory feedback during gaze stabilization in fly flight</title><p>During flight, flies are often knocked off course by gusts of wind or other external perturbations to their flight paths. Such perturbations (<inline-formula><mml:math id="inf18"><mml:mi>R</mml:mi></mml:math></inline-formula>) generate optic flow relative to a fly’s own motion, or sensory error (<inline-formula><mml:math id="inf19"><mml:mi>E</mml:mi></mml:math></inline-formula>), across the retina that is processed by the brain to generate corrective steering maneuvers of the head (<inline-formula><mml:math id="inf20"><mml:mi>H</mml:mi></mml:math></inline-formula>) and/or body (<inline-formula><mml:math id="inf21"><mml:mi>B</mml:mi></mml:math></inline-formula>)—with the goal to minimize <inline-formula><mml:math id="inf22"><mml:mi>E</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>; <xref ref-type="bibr" rid="bib23">Dickinson and Muijres, 2016</xref>). Mechanosensory information from externally-generated and/or self-generated body motion—and measured by the halteres—also elicits corrective movements of the head and wings/body (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="bibr" rid="bib37">Hardcastle and Krapp, 2016</xref>; <xref ref-type="bibr" rid="bib22">Dickinson, 1999</xref>; <xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>; <xref ref-type="bibr" rid="bib39">Hengstenberg, 1988</xref>; <xref ref-type="bibr" rid="bib72">Sandeman, 1980</xref>; <xref ref-type="bibr" rid="bib4">Beatus et al., 2015</xref>). This suite of multisensory reflexes keeps gaze level and appears essential for flight.</p><p>We developed a control model of gaze stabilization about the vertical (yaw) axis to model the flow of visual and mechanosensory information in driving head and body motor responses in flies (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). We based our framework on prior models in flies that demonstrated that inputs from the visual system and halteres sum in the nervous system (<xref ref-type="bibr" rid="bib75">Sherman and Dickinson, 2004</xref>), but go further by including naturalistic closed-loop feedback and mechanics. For the head and body, we modeled the distinct contributions of sensory feedback by separating the neural control of gaze stabilization into two sub-components, one for visual feedback (<inline-formula><mml:math id="inf23"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and the other for mechanosensory feedback (<inline-formula><mml:math id="inf25"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). Critically, we assumed that visual feedback has a gain of –1—because motion in one direction generates equal and opposite optic flow—and that that the mechanosensory neural controllers (<inline-formula><mml:math id="inf27"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) only receive haltere inputs <italic>due to body motion</italic>. The other functions of the halteres related to structuring the timing of motor output are assumed to be contained within the dynamics of the visual controllers (<xref ref-type="bibr" rid="bib20">Dickerson et al., 2019</xref>). The separate neural controller pairs for the head and body in our model ensured that any differences in tuning between the head and body were considered. We assumed approximately linear time-invariant (LTI) dynamics (<xref ref-type="bibr" rid="bib1">Aström et al., 2010</xref>), which is supported by experimental data from prior work (<xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>; <xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). Finally, our model only considered mechanosensory feedback generated from self-generated body motion, asserting that mechanosensory feedback is nested within visual feedback (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>We first modeled the head response by defining the transforms mapping visual and mechanosensory inputs to head motor responses:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf29"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the visual transform from <inline-formula><mml:math id="inf30"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf31"><mml:mi>H</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the mechanosensory transform from <inline-formula><mml:math id="inf33"><mml:mi>B</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf34"><mml:mi>H</mml:mi></mml:math></inline-formula>. These transforms consist of the multiplication of the corresponding neural circuits associated with the visual (<inline-formula><mml:math id="inf35"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and mechanosensory (<inline-formula><mml:math id="inf36"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) control centers of the fly brain and the passive biomechanics of the head-neck system (<inline-formula><mml:math id="inf37"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). We assume that the dynamics of the sensory systems—visual system and halteres—are contained within the dynamics of <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. All the transforms and signals in our model are designated as non-parametric complex-valued functions (see Materials and methods). Throughout, we omit the complex variable <inline-formula><mml:math id="inf40"><mml:mi>s</mml:mi></mml:math></inline-formula> for brevity.</p><p>Using the transforms <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> and <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, we derived an expression for the closed-loop head motor response <inline-formula><mml:math id="inf41"><mml:mi>H</mml:mi></mml:math></inline-formula> as a function of an external visual perturbation <inline-formula><mml:math id="inf42"><mml:mi>R</mml:mi></mml:math></inline-formula> and body motion <inline-formula><mml:math id="inf43"><mml:mi>B</mml:mi></mml:math></inline-formula> (see Materials and methods for more detailed derivations). We first defined the head response as the sum of visual and mechanosensory inputs due to body motion:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the sensory error <inline-formula><mml:math id="inf44"><mml:mi>E</mml:mi></mml:math></inline-formula> is equivalent to the visual perturbation <inline-formula><mml:math id="inf45"><mml:mi>R</mml:mi></mml:math></inline-formula> subtracted by the fly’s gaze (sum of <inline-formula><mml:math id="inf46"><mml:mi>H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:mi>B</mml:mi></mml:math></inline-formula>):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mi>H</mml:mi><mml:mo>-</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Substituting <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> into <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and solving for <inline-formula><mml:math id="inf48"><mml:mi>H</mml:mi></mml:math></inline-formula> yields the expression for the closed-loop head response:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder><mml:mo>-</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>body visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>B</mml:mi><mml:mo movablelimits="false">.</mml:mo></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>body mechanosensory</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Notably, the closed-loop head response of body-free flies (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) is mediated by three sources of sensory feedback: (1) visual feedback from head movements themselves, (2) visual feedback from body motion and (3) nested mechanosensory feedback from body motion. Conversely, the head response of body-fixed flies can be represented as:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder></mml:mrow></mml:math></disp-formula></p><p>where all terms associated with body motion are set to zero (<inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>), leaving only visual feedback from head movements (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We recognize that haltere neural inputs are always present—even when there is no body motion—and can not be completely abolished without removing the halteres. However these inputs are involved with structuring the timing of motor output, not with the encoding of body velocity via sensing gyroscopic forces (<xref ref-type="bibr" rid="bib20">Dickerson et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Fayyazuddin and Dickinson, 1996</xref>; <xref ref-type="bibr" rid="bib29">Fayyazuddin and Dickinson, 1999</xref>). Thus, we lump this tonic function of the halteres into the visual controllers (<inline-formula><mml:math id="inf50"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), which are present in both body-free and body-fixed flies. Crucially, our control model mathematically predicts that the head motor responses of body-free (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) and body-fixed (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) flies will be distinct due to differences in sensory feedback. Therefore, comparing how the head responses of body-free and body-fixed flies differ provides insights into the distinct sensory modalities that influence head control.</p></sec><sec id="s2-2"><title>Sensory feedback generated from body movements alters the magnitude, timing, and performance of head responses</title><p>Our control model predicted that body-free and body-fixed flies should have distinct head motor responses to the same visual perturbation due to the absence of body-generated visual and mechanosensory feedback. To determine whether empirical data supported this prediction, we employed two experimental paradigms: (1) a magnetic tether where flies were tethered to a pin and suspended between two magnets, allowing free, closed-loop body rotation about the vertical (yaw) axis (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and (2) a rigid tether where flies were fixed in place, thus opening body-generated visual and mechanosensory feedback loops (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>We presented flies in both paradigms with visual perturbations consisting of single sinusoids with frequencies spanning 0.7–10.6 Hz to reveal how body-generated feedback influences head motor responses (see Materials and methods). We began by quantifying the body response of body-free flies to understand what frequency range body-generated feedback should have the most impact on head responses. The body was strongly tuned to low frequencies, similar to a low-pass filter (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <italic>red</italic>, <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>–<xref ref-type="video" rid="fig3video2">Figure 3—video 2</xref>, <xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). Thus, body visual feedback reduced the optic flow—or sensory error (<inline-formula><mml:math id="inf52"><mml:mi>E</mml:mi></mml:math></inline-formula>)—entering the visual system, at low frequencies especially (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>, <italic>yellow</italic>, <xref ref-type="fig" rid="fig4">Figure 4A</xref>, <italic>red</italic>). This result, combined with our control model, predicted that any differences between head responses in body-free and body-fixed flies should be the greatest at low frequencies. Because biological systems often exhibit nonlinear behavior to different types of sensory inputs, we also measured fly responses to sum-of-sines visual perturbations as a check for linearity. Although flies responses varied slightly between single-sine and sum-of-sine perturbations, the overall behavior was similar, indicating that our findings are generalizable (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A-B</xref>, <xref ref-type="video" rid="fig3video4">Figure 3—video 4</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Sensory feedback generated from body movements alters the magnitude, timing, and performance of head responses.</title><p>(<bold>A</bold>) The body response (red) of body-free flies to single-sine visual perturbations (grey) with varying frequency. The x-axis is normalized to show four oscillations at each frequency. Note that the body response is larger relative to the visual perturbation at low frequencies, leading to a smaller sensory error signal (yellow) in the head reference frame. Thick lines: mean. Thin lines: individual fly means. (<bold>B</bold>) The distribution of compensation errors in the head reference frame corresponding to the sensory error in (A) normalized by the perturbation amplitude. Values below one indicate that body movements reduced the sensory error while values greater than one indicate that body movements increased the sensory error. (<bold>C</bold>) The head response of body-free (blue) and body-fixed (violet) flies to the same visual perturbation (grey) shown in (A). At low frequencies, the head would often run into the anatomical limits of the neck joint (dashed pink lines). Thick lines: mean response. Thin lines: individual fly means. (<bold>D</bold>) The total distribution of head angular displacements for body-free and body-fixed flies for each perturbation frequency. For each frequency, the body-free and body-fixed head distributions had a different variance (F-test, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Body-free: <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Sum-of-sines body and head responses.</title><p>(<bold>A</bold>) Top: the body response (red) of body-free flies to a sum-of-sines visual perturbation (black). Bottom: the head response of body-free (blue) and body-fixed (violet) flies to the same visual perturbation. Thick lines: mean response. Thin lines: individual fly means. (<bold>B</bold>) The velocity magnitude of the body, and head for body-free and body-fixed paradigms calculated at each frequency present in the visual perturbation (calculated from Chirp-Z transform, see Materials and method). Shaded regions: ±1 STD. (<bold>C</bold>) The total distribution of head angular displacements from body-free and body-fixed flies. *** F-test <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Body-free: <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Saturation-corrected head response and LSSA sensitivity analysis.</title><p>(<bold>A</bold>) A saturation-correction routine applied to the mean head response of body-fixed flies at the 0.7 Hz perturbation frequency. We removed the saturated data points and fit a sine wave to the remaining data points (see Methods). (<bold>B</bold>) A comparison of three different system identification methods: (1) from a Chirp-Z Transform, (2) from least squares spectral analysis (LSSA), and (3) from LSSA applied to the saturation corrected data. We showed that the Chirp-Z Transform and LSSA methods yielded near-identical results, while saturation-correction only effected the lowest two frequencies of the body-fixed head transforms: <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>R</mml:mi><mml:mo>→</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> (violet) and <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>E</mml:mi><mml:mo>→</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> (violet), leading to a slight increase in gain. The transforms indicated above each plot correspond to the legends in <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>. We used the Chip-Z Transform for our analysis in <xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref>, but applied the LSSA saturation correction to the body-fixed head response at the lowest two frequencies. Shaded regions: ±1 STD. Body-free: <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig3-figsupp2-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig3-video1.mp4" id="fig3video1"><label>Figure 3—video 1.</label><caption><title>Comparison of a body-free fly (magnetic tether, left) and body-fixed fly (rigid tether, right) head response to a 1 Hz sine wave visual perturbation.</title><p>The body-stabilized head view is shown below each raw video. Note that head movements are larger in the body-fixed fly.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig3-video2.mp4" id="fig3video2"><label>Figure 3—video 2.</label><caption><title>Same as <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref> but for a 2.1 Hz visual perturbation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig3-video3.mp4" id="fig3video3"><label>Figure 3—video 3.</label><caption><title>Same as <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref> but for a 5.3 Hz visual perturbation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig3-video4.mp4" id="fig3video4"><label>Figure 3—video 4.</label><caption><title>Same as <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref> but for a sum-of-sines visual perturbation.</title></caption></media></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Visual and mechanosensory feedback mediate head control.</title><p>The legend indicates whether the data corresponds to the head or body, , the sources of sensory feedback, and the relevant experiment (or prediction). (<bold>A</bold>) The closed-loop transform from the visual perturbation <inline-formula><mml:math id="inf63"><mml:mi>R</mml:mi></mml:math></inline-formula> to the body response <inline-formula><mml:math id="inf64"><mml:mi>B</mml:mi></mml:math></inline-formula>. Note that the body is primarily tuned to low frequencies. (<bold>B</bold>) The closed-loop transform from the visual perturbation <inline-formula><mml:math id="inf65"><mml:mi>R</mml:mi></mml:math></inline-formula> to the head response <inline-formula><mml:math id="inf66"><mml:mi>H</mml:mi></mml:math></inline-formula> for different sensory feedback conditions. The head transform measured in body-free flies (blue) contains all three sources of feedback (see <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), while the head transform measured in body-fixed flies (purple) contains only head visual feedback (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). (<bold>C</bold>) The predicted (dashed line) transform for the head response with head and body visual feedback (copper, see <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, corresponding to <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>) and the experimentally measured equivalent from a ’replay’ experiment (grey, corresponding to <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2C-E</xref>). The highest two frequencies were omitted in the replay experiment due to limitations of our flight arena display system (see Materials and methods). (<bold>D</bold>) The predicted (dashed line) transform for the response with head visual feedback and body mechanosensory feedback (cyan, see <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>, corresponding to <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2B</xref>). Body-free: <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed replay: <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> flies. For all panels, shaded regions: ±1 STD. Also see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for all plots overlaid to facilitate comparison across groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Sum-of-sines transforms.</title><p>A Same as <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>, but for sum-of-sines visual perturbations with different normalized velocities. (<bold>B</bold>) Same as <xref ref-type="fig" rid="fig5">Figure 5C</xref> but for the sum-of-sines perturbations. Shaded regions: ±1 STD. Body-free: <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Control diagrams and replay experiment for manipulations of sensory feedback.</title><p>(<bold>A</bold>) Control diagram for the prediction of the head response with head and body visual feedback (corresponding to <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>). (<bold>B</bold>) Control diagram for the prediction of the head response with head visual feedback and body mechanosensory feedback (corresponding to <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). (<bold>C</bold>) The experimental equivalent to (<bold>A</bold>). We reintroduced body visual feedback to body-fixed flies by ‘replaying’ the body response measured in the body-free flies. We subtracted the body response <inline-formula><mml:math id="inf72"><mml:mi>B</mml:mi></mml:math></inline-formula> from the original visual perturbation <inline-formula><mml:math id="inf73"><mml:mi>R</mml:mi></mml:math></inline-formula> to generated the replay perturbation <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>R</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:math></inline-formula>. This allowed us to experimentally quantify the effects of head and body visual feedback without body mechanosensory feedback. (<bold>D</bold>) The head response (blue) to the replay perturbation (grey) across perturbation frequencies. (<bold>E</bold>) The head transform from <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>R</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf76"><mml:mi>H</mml:mi></mml:math></inline-formula> measured in the replay experiment (grey) compared to the head transform from <inline-formula><mml:math id="inf77"><mml:mi>R</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf78"><mml:mi>H</mml:mi></mml:math></inline-formula> measured in the original experiment with body-fixed flies. These transforms should be approximately equal, assuming linear-time-invariant dynamics, which was the case. Shaded regions: ±1 STD. Body-fixed: <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed replay: <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> flies. See <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for experimental data corresponding to (A–C).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Overlaid transforms.</title><p>(<bold>A</bold>) Same as <xref ref-type="fig" rid="fig4">Figure 4</xref>, but with all transforms plotted on the same axes and the coherence of the experimentally measured responses. Shaded regions: ±1 STD. Body-free: <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed replay: <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig4-figsupp3-v2.tif"/></fig></fig-group><p>Next, we quantified the head responses of body-free and body-fixed flies. Consistent with prior work, body-free flies generated head movements that were inversely tuned to the body and resembled a high-pass filter, where the head operated with the largest gains at high frequencies (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <italic>blue</italic>)(<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). Consistent with the prediction of our model (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>), the head response of body-fixed flies was appreciably different from that of body-free flies (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A-B</xref>, <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>–<xref ref-type="video" rid="fig3video2">Figure 3—video 2</xref>, <xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>). Specifically, body-fixed flies moved their head with larger magnitude than body-free flies (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Interestingly, head movements in body-fixed flies were often driven to the anatomical limits of the neck joint (approximately ±15°), which was never the case for body-free flies (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This led to head trajectories that were saturated, meaning that the head could possibly have moved with larger amplitude if it were anatomically possible. The total distributions of head angular displacements were likewise significantly different between body-free and body-fixed flies (F-test, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for every frequency) (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). Body-free flies rarely moved their head more than 5° from the neutral position (0°), whereas body-fixed flies regularly moved their head in excess of 10° (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). This was especially prominent at lower frequencies, while head responses at higher frequencies were closer in magnitude (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A, B</xref>). This is consistent with the low-frequency tuning of body movements, where the smaller sensory error in body-free flies at low frequencies likely led to the smaller head motor responses. Body visual feedback also altered the phase, or the timing, of the sensory error signal entering the visual system, leading to differences in the timing of head motor responses in body-free and body-fixed flies (<xref ref-type="fig" rid="fig3">Figure 3A, C</xref>).</p><p>To quantify the performance of head responses, we measured the closed-loop transforms from <inline-formula><mml:math id="inf85"><mml:mi>R</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf86"><mml:mi>H</mml:mi></mml:math></inline-formula> in body-free and body-fixed flies. These transforms mirror <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> and <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, respectively, but normalize the head response with respect to <inline-formula><mml:math id="inf87"><mml:mi>R</mml:mi></mml:math></inline-formula>. While these transforms are complex valued functions, we represented them graphically via gain, phase, and compensation error (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Gain represents the magnitude of the ratio of the perturbation and head <inline-formula><mml:math id="inf88"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> phase represents the difference in timing <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi mathvariant="normal">∠</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, and compensation error describes the normalized magnitude of the sensory error signal <inline-formula><mml:math id="inf90"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mfrac><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula>. A compensation error value of zero indicates ideal performance, a value between zero and one indicates intermediate performance, a value of one indicates that the head response has no effect on performance, and a value greater than one indicates a deleterious response (see Materials and methods). Body-fixed flies operated with overall higher gain than body-free flies (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>, <italic>purple</italic> vs <italic>blue</italic>). Both body-free and body-fixed flies displayed a phase lead (phase &gt;0) at low-frequencies which decreased with increasing frequency, however this was less pronounced in body-fixed flies (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). The larger gain and smaller phase lead in body-fixed flies led to improved performance at low-frequencies, but worse performance at higher frequencies, illustrating that body-fixation leads to tradeoffs in head stabilization performance (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>, <italic>compensation error</italic>). Altogether, the magnitude, timing, and performance of head motor responses were distinct between body-fixed and body-free flies, demonstrating the critical role of sensory feedback in shaping head movements.</p></sec><sec id="s2-3"><title>Visual feedback changes the tuning of head responses across visual motion frequencies</title><p>Body-free and body-fixed flies clearly exhibit distinct head responses (<xref ref-type="fig" rid="fig3">Figure 3</xref>), but what are the individual contributions of visual and nested mechanosensory feedback underlying these differences? To address this question, we used our mathematical model of gaze stabilization (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) combined with behavioral measurements to predict how visual and mechanosensory feedback individually influence head control.</p><p>First, we measured the visual transform <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which is the transform between the sensory error <inline-formula><mml:math id="inf92"><mml:mi>E</mml:mi></mml:math></inline-formula> and the head motor response <inline-formula><mml:math id="inf93"><mml:mi>H</mml:mi></mml:math></inline-formula> in body-fixed flies. We then substituted <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig5">Figure 5C</xref> for visualization of <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) into (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) while setting <inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (indicating no contributions of mechanosensory feedback due to body motion) to predict the effects of body visual feedback on the head motor response (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder><mml:mo>-</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>body visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Nested mechanosensory feedback damps head movements.</title><p>(<bold>A</bold>) The control diagram of the sensory error <inline-formula><mml:math id="inf97"><mml:mi>E</mml:mi></mml:math></inline-formula> to head <inline-formula><mml:math id="inf98"><mml:mi>H</mml:mi></mml:math></inline-formula> transform <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> in body-free flies. Note that this transform includes nested mechanosensory feedback from body motion. (<bold>B</bold>) The control diagram of the sensory error <inline-formula><mml:math id="inf100"><mml:mi>E</mml:mi></mml:math></inline-formula> to head <inline-formula><mml:math id="inf101"><mml:mi>H</mml:mi></mml:math></inline-formula> transform in body-fixed flies, which is simply the visual transform <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. (<bold>C</bold>) The gain and phase of the <inline-formula><mml:math id="inf103"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf104"><mml:mi>H</mml:mi></mml:math></inline-formula> transform for body-free (blue) and body-fixed (purple) flies. Shaded regions: ±1 STD (<bold>D</bold>) The ratio of the <inline-formula><mml:math id="inf105"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf106"><mml:mi>H</mml:mi></mml:math></inline-formula> transform in body-free and body-fixed flies (<inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). If nested mechanosensory feedback from body motion had no effect, we would expect this ratio to have a gain of one and phase of 0 (dashed blue lines). The empirical data has a gain less than one, indicating the head movements are damped by nested mechanosensory feedback. Shaded regions: ±1 STD. Body-free: <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> flies, Body-fixed: <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig5-v2.tif"/></fig><p>Using <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, we generated a prediction of the closed-loop head transform with body visual feedback. Body visual feedback could partially account for the decrease in magnitude and the increase in phase in body-free flies (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <italic>copper</italic>). Notably, head gain at low frequencies shifted from higher gain in body-fixed flies (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <italic>purple</italic>) to lower gain when body visual feedback was introduced (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <italic>copper</italic>), which more closely matched the body-free head response (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <italic>blue</italic>). These visually mediated changes in the head response closely followed the inverse of the body compensation error (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <italic>red</italic>), meaning that the more the body reduced sensory error, the smaller the head magnitude became in body-free flies. These results demonstrate that body visual feedback changes the tuning of head responses from broadband (high gain at all frequencies) to high-pass (high gain only at higher frequencies) (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <italic>copper</italic> vs. <italic>purple</italic>). Interestingly, body visual feedback could not account for the overall decrease in head gain in body-free flies (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <italic>copper</italic> vs <italic>blue</italic>; see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for both plots overlaid), suggesting that other sensory modalities must influence head control.</p><p>We confirmed our prediction from <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> by performing a ‘replay’ experiment, wherein we designed a new visual perturbation for body-fixed flies that had the mean body response of body-free flies subtracted (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2C-E</xref>). In this way, we experimentally reintroduced body visual feedback in body-fixed flies. Due to limitations in spatial resolution of our visual display, we could not properly replay body motion at the two highest frequencies, thus we exclude them. The match between our model prediction (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <italic>copper</italic>) and experimental data (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <italic>grey</italic>) strongly supports the notion that body visual feedback accounts for the change in tuning across visual motion frequencies––but not the overall decrease in magnitude––of head responses between body-free and body-fixed flies. Furthermore, the close match between model and experiments provides some assurance that the head control system can be modeled with LTI assumptions, thus supporting our LTI-based control theoretic framework.</p></sec><sec id="s2-4"><title>Nested mechanosensory feedback damps head movements</title><p>If body visual feedback alone cannot fully account for the difference in head motor responses, then it would follow that mechanosensory feedback due to body motion plays a role in shaping head movements. Body visual feedback predicts that body-free flies should have larger head responses at the highest frequency (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) because body movements increase the sensory error in this range (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <italic>red</italic>, compensation error &gt;1), but this was not observed in our experiments, suggesting that there are other sensory modalities at play (<xref ref-type="fig" rid="fig3">Figure 3C–D</xref>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Prior work showed that flying flies mounted on a motor and rotated about the vertical axis perform compensatory head movements in the opposite direction of the body, even when visual feedback is removed, pointing to the role of haltere-generated mechanosensory feedback (due to body motion) in head control (<xref ref-type="bibr" rid="bib72">Sandeman, 1980</xref>). However, the individual contributions of visual and mechanosensory feedback remain unclear in the control of head movement, particularly due to their nested architecture.</p><p>To estimate the contributions of mechanosensory feedback on head control, we compared the transform from <inline-formula><mml:math id="inf110"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf111"><mml:mi>H</mml:mi></mml:math></inline-formula> in body-free (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and body-fixed (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) flies. In body-fixed flies this transform is purely mediated by visual inputs and equal to <inline-formula><mml:math id="inf112"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, but in body-free flies there is nested mechanosensory feedback that could shape the head response. We defined the <inline-formula><mml:math id="inf113"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf114"><mml:mi>H</mml:mi></mml:math></inline-formula> transform in body-free flies as <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>. We discovered that the gain of <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> was substantially lower, and the phase subtly larger, than <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, suggesting that nested mechanosensory feedback due to body motion has a transformative influence on head control (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). By computing the ratio <inline-formula><mml:math id="inf118"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf119"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> we discovered that the overall gain from <inline-formula><mml:math id="inf120"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf121"><mml:mi>H</mml:mi></mml:math></inline-formula> decreased by a factor of ~0.4 and phase increased by ~20° (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). This ratio describes the effective weighting of nested mechanosensory feedback with respect to visual feedback. These results strongly suggest that the neural signals generated from mechanosensory pathways serve to actively damp head movements. Although this analysis does not isolate the precise sensory mechanism (halteres, antenna, wing proprioceptors, etc.), our findings strongly suggest that the observed change in head control is driven by a mechanosensory modality that measures body motion, thus strongly implicating halteres (<xref ref-type="bibr" rid="bib22">Dickinson, 1999</xref>; <xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>).</p><p>Similar to how we could predict the contributions of body visual feedback on the closed-loop head response, we used <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> to generate a mathematical prediction of the head response with body mechanosensory feedback (<xref ref-type="fig" rid="fig4">Figure 4B</xref>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>e</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>a</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo movablelimits="false">,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mtext>body mechanosensory</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:munder></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>However, it was not possible to measure the mechanosensory transform <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> directly from our experimental data, because visual feedback was always present. Therefore, we derived an expression equivalent to <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> with <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> in place of <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see Materials and methods):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Our prediction of the head response with body mechanosensory feedback due to body motion also partially accounted for the increase in head movement magnitude and decrease in phase in body-fixed flies, but similarly to body visual feedback, could not fully account for the difference (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>, <italic>cyan</italic> vs <italic>blue</italic>). Notably, mechanosensory feedback due to body motion led to an overall decrease in head gain across visual motion frequencies in body-free flies (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>), whereas visual feedback primarily attenuated low frequency visual motion and changed the tuning of the head response from broadband to high-pass (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). While prior experiments in body-fixed flies mounted on a motor showed that mechanosensory information from the halteres primarily mediates high-frequency steering responses (<xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>), our results strongly suggest that haltere feedback due to body motion has a considerable influence even at lower frequencies. This emergent low-frequency response is likely a property of closed-loop dynamics (due to the body being free to move) that would not be evident in open-loop (body-fixed) conditions. Altogether, out results reveal the precise roles of body-generated visual and mechanosensory feedback in shaping head movement: visual feedback changes the tuning from broad-band to high-pass and mechanosensory feedback reduces the overall magnitude.</p></sec><sec id="s2-5"><title>Head damping is present during self-generated but not externally generated body motion</title><p>Our findings strongly suggest that the change in the transform from <inline-formula><mml:math id="inf125"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf126"><mml:mi>H</mml:mi></mml:math></inline-formula> is primarily brought about by a mechanosensory pathway from <inline-formula><mml:math id="inf127"><mml:mi>B</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf128"><mml:mi>H</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf129"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). In body-free flies, where <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>B</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, this pathway shapes head responses via nested sensory feedback (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Although it was impossible to measure <inline-formula><mml:math id="inf131"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> directly in body-free or body-fixed flies because head visual feedback was always present, our control framework (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) allowed us to estimate <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and make a prediction of how nested mechanosensory feedback influences head control. We solved for <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>:<disp-formula id="equ10"> <label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which can equivalently be represented as:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mi>E</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Our prediction of <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> exhibited low gain at low frequencies, but gain swiftly increased with increasing frequency, consistent with previous work that showed that the halteres are most sensitive to high body frequencies/angular velocities in open-loop (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <italic>pink</italic>) (<xref ref-type="bibr" rid="bib22">Dickinson, 1999</xref>; <xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>). The shape of <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> resembled a high-pass filter, suggesting that the halteres may be primarily tuned to angular acceleration in open-loop (<xref ref-type="bibr" rid="bib72">Sandeman, 1980</xref>). This result likely explains why prior studies in body-fixed (open-loop) flies reported that the halteres encode body velocity like a high-pass filter—they were measuring <inline-formula><mml:math id="inf136"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> without self-generated sensory feedback (<xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>). The high gain may seem counter-intuitive—seeing as we argue that mechanosensory feedback actually <italic>decreases</italic> the magnitude of head movements (<xref ref-type="fig" rid="fig4">Figure 4D</xref>)—however the phase response between –70° and –200° (average –119°) means that the head steering response elicited by mechanosensory feedback destructively interferes (opposite direction) with the visually elicited head steering response, leading to an overall decrease in the magnitude of head movements in body-free flies. An alternate interpretation of these data is that mechanosensory information is subtracted from, rather than added to (as in <xref ref-type="fig" rid="fig3">Figure 3A</xref>), visual information in the nervous system (i.e., negative feedback).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Head damping is present during self-generated but not externally generated body motion.</title><p>(<bold>A</bold>) The predicted transforms from body mechanosensory information to the head response <inline-formula><mml:math id="inf137"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for self-generated body motion (pink) and externally generated body motion (blue). Shaded regions: ±1 STD. (<bold>B</bold>) The control framework outlining how externally generated body motion influences head response via mechanosensory feedback. Note that head visual feedback is still present even if there is no external visual perturbation since the head is free to move. (<bold>C</bold>) Experimental equivalent to (<bold>B</bold>). Flies were mounted to the shaft of a stepper motor and the body motion measured in body-free flies was replayed on the motor. Note that the visual display was also mounted to the motor shaft, effectively removing body visual feedback, while leaving mechanosensory feedback intact. (<bold>D</bold>) The head response (blue) of flies during the experiment where body motion (red) was replayed on the motor. Thin blue lines show the response of individual flies. Thick grey line shows the mean passive head response of an anesthetized fly to the same replayed body motion. Also see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. (<bold>E</bold>) Coherence for the visual transform from <inline-formula><mml:math id="inf138"><mml:mi>R</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf139"><mml:mi>H</mml:mi></mml:math></inline-formula> in body-free (blue) and body-fixed (violet) flies compared to the mechanosensory transform from <inline-formula><mml:math id="inf140"><mml:mi>B</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf141"><mml:mi>H</mml:mi></mml:math></inline-formula> measured from the motor experiment. Note that the mechanosensory transform has much lower coherence, indicative of an uncoordinated response. Shaded regions: ±1 STD. (<bold>F</bold>) The distribution of all active head displacements (blue) compared to the distribution of all passive head displacements from the motor experiment (grey). Motor experiments: <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> flies.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Passive head movements.</title><p>(<bold>A</bold>) Flies were anesthetized with triethylamine (commercially available as FlyNap, Carolina Biological Supply) and rigidly tethered to the shaft of a stepper motor (Nema 17). The body motion of actively flying flies in the magnetic tether was replayed on the motor and the passive (due to body motion alone) head motion was measure. (<bold>B</bold>) The mean body angular position of actively flying flies in the magnetic tether (red, same as <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>) and the replayed body motion on the motor (grey, all trials shown but variance is small so it looks like one line). (<bold>C</bold>) Same as (<bold>B</bold>) but for the frequency domain (Chirp-Z Transform) of the body velocity. Dashed lines indicate the frequencies present in the sum-of-sine perturbation. (<bold>D</bold>) The head of actively flying flies in the magnetic tether (blue, same as <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>) and the passive head response in anesthetized flies (all trials: grey, mean: black). (<bold>E</bold>) Same as (<bold>D</bold>) but for the frequency domain (Chirp-Z Transform) of the head velocity.The right y-axis (teal) indicates the coherence between the head and body of anesthetized flies (shaded region: ±1 STD). Note that very little passive head motion is elicited by body motion alone. <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> flies, <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>97</mml:mn></mml:mrow></mml:math></inline-formula> trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig6-figsupp1-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig6-video1.mp4" id="fig6video1"><label>Figure 6—video 1.</label><caption><title>A fly was mounted on a motor and rotated about the vertical (yaw) axis such that the body angle matched that of a magnetically tethered fly in response to a 1 Hz sine wave visual perturbation (see <xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>).</title><p>Body visual feedback was removed by mounting the visual scene to the motor. The head response (right) of the same fly in the body-stabilized coordinate frame. Note that the head response was generally uncoordinated with body motion.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig6-video2.mp4" id="fig6video2"><label>Figure 6—video 2.</label><caption><title>Same as <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref> but for a 2.1 Hz perturbation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig6-video3.mp4" id="fig6video3"><label>Figure 6—video 3.</label><caption><title>Same as <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref> but for a 5.3 Hz perturbation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig6-video4.mp4" id="fig6video4"><label>Figure 6—video 4.</label><caption><title>Same as <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref> but for an anesthetized fly.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig6-video5.mp4" id="fig6video5"><label>Figure 6—video 5.</label><caption><title>Same as <xref ref-type="video" rid="fig6video2">Figure 6—video 2</xref> but for an anesthetized fly.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig6-video6.mp4" id="fig6video6"><label>Figure 6—video 6.</label><caption><title>Same as <xref ref-type="video" rid="fig6video3">Figure 6—video 3</xref> but for an anesthetized fly.</title></caption></media></fig-group><p>An interesting idea to consider is that the damping due to mechanosensory feedback we uncovered is only present during self-generated (i.e., nested) rather than externally generated body motion (e.g. from a gust of wind). The yaw flight axis is inherently stable—as opposed to pitch—so flies may only require mechanosensory feedback during self-generated yaw turns, where flies need to damp out their own motion (<xref ref-type="bibr" rid="bib80">Taha et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Faruque and Sean Humbert, 2010a</xref>; <xref ref-type="bibr" rid="bib27">Faruque and Sean Humbert, 2010b</xref>). Although this idea mainly applies to the control of body movements, the head may be controlled similarly. To this end, we designed an experiment where we imposed externally generated body motion with no body visual feedback to uncover <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for externally generated, rather than self-generated body motion (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). We mounted rigidly tethered flies to the shaft of a stepper motor and replayed the recorded body motion of body-free flies <inline-formula><mml:math id="inf146"><mml:mi>B</mml:mi></mml:math></inline-formula>, while measuring their head responses <inline-formula><mml:math id="inf147"><mml:mi>H</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Crucially, the visual display was also fixed to the motor shaft, so as to remove any visual feedback generated from body motion.</p><p>Intriguingly, the head response to externally generated body motion was small and generally uncoordinated with body motion (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, <xref ref-type="video" rid="fig3video4">Figure 3—video 4</xref>–<xref ref-type="video" rid="fig6video2">Figure 6—video 2</xref>). We computed the coherence—a measure of linear correlation in frequency domain where values near one indicate high correlation and values near zero indicate low correlation—between the externally generated body motion and the head response and found that the head operated with a coherence of ~0.5. Compared to the head responses driven by visual motion—which operated with coherence near 1—our results demonstrate that flies do not have a robust head response to externally generated body motion about the yaw axis (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), corroborating previous work that measured wing movements (<xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>). We computed <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from these experiments using our control framework:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mi>B</mml:mi></mml:mfrac></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and compared the response to <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> computed for self-generated body motion (<xref ref-type="disp-formula" rid="equ1">Equation 10</xref>–<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>). We discovered that <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for externally generated body motion displayed gains nearly an order of magnitude smaller than <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for self-generated body motion (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The phase estimates were highly variable due to the low-coherence response (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). These findings strongly suggest that mechanosensory information is integrated in a nonlinear fashion, that is dependent on the type of body motion: externally- vs self-generated. The precise mechanism that underlies the gating is unclear, although it is likely that self-generated turns evoke mechanosensory-dependent activity of muscles within the neck motor system (<xref ref-type="bibr" rid="bib40">Huston and Krapp, 2009</xref>).</p></sec><sec id="s2-6"><title>Mechanical properties of the neck joint prevent passively generated head motion</title><p>To ensure that the head responses we measured in flies mounted on the motor and in the magnetic tether (where the body also moves the same way) were elicited by sensory feedback—not generated mechanically from body motion—we repeated the same experiment illustrated in <xref ref-type="fig" rid="fig6">Figure 6C</xref>, but for anesthetized flies. This approach allowed us to isolate any passively generated head movements due to body motion that were not under active neural control. We found that passively generated head movements were much smaller than head movements of actively flying flies (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, <italic>grey</italic> vs <italic>blue</italic>, <xref ref-type="video" rid="fig6video4 fig6video5 fig6video6">Figure 6—videos 4–6</xref>). The head rarely moved more than 0.5° in anesthetized flies, compared to 2° in active flies, demonstrating that sensory feedback is the primary driver of head movements (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). This was consistent for a sum-of-sines replay experiment (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Interestingly, the passive mechanics of the neck joint (stiffness, damping, etc.) effectively decoupled the head from the body, which could simplify the neural control of head movements because flies would not have to account for passive head motion (<xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>). Neck passive mechanics could also help keep the head stable during rapid turns or large external perturbations such as turbulent gusts of wind.</p></sec><sec id="s2-7"><title>Head saccades are actively damped by mechanosensory feedback</title><p>Our results thus far strongly implicate mechanosensory feedback due to body motion in damping smooth head responses during self-generated, but not externally generated turns. However, in addition to the smooth head and body movements during gaze stabilization, flies also perform rapid, saccadic turns of the head and body (<xref ref-type="bibr" rid="bib11">Cellini and Mongeau, 2020b</xref>; <xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Mongeau and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib16">Collett and Land, 1975</xref>; <xref ref-type="bibr" rid="bib6">Bender and Dickinson, 2006b</xref>; <xref ref-type="bibr" rid="bib55">Muijres et al., 2015</xref>). Prior studies suggest that, once executed, body saccades are visually open-loop, as body saccade duration is on the same order as visuomotor delays and altering visual feedback during body saccades does not change their dynamics (<xref ref-type="bibr" rid="bib5">Bender and Dickinson, 2006a</xref>; <xref ref-type="bibr" rid="bib53">Mongeau and Frye, 2017</xref>). However, mechanosensory feedback is thought to play a role in eliciting the wing-braking response to terminate body saccades (<xref ref-type="bibr" rid="bib11">Cellini and Mongeau, 2020b</xref>; <xref ref-type="bibr" rid="bib5">Bender and Dickinson, 2006a</xref>). Head saccades are thought to be similarly visually open-loop (<xref ref-type="bibr" rid="bib45">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>). However, as prior work has shown that visual and mechanosensory inputs converge at the neck motor center (<xref ref-type="bibr" rid="bib51">Milde et al., 1987</xref>; <xref ref-type="bibr" rid="bib77">Strausfeld and Seyan, 1985</xref>), we hypothesized that mechanosensory feedback due to body motion also influences head saccade dynamics. Specifically, due to the damping effects of mechanosensory feedback we uncovered during self-generated body motion, we predicted that head saccades in body-free flies should be of smaller magnitude than in body-fixed flies.</p><p>To test this hypothesis, we culled head saccades from body-free and body-fixed flies presented with a static visual stimulus using a previously described method (<xref ref-type="fig" rid="fig7">Figure 7A–B</xref>, <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>, <xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Mongeau and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib71">Salem et al., 2020</xref>). Consistent with our prediction, head saccades in body-free flies displayed smaller amplitude and peak velocity than head saccades in body-fixed flies, suggesting that mechanosensory feedback damps head saccades (<xref ref-type="fig" rid="fig7">Figure 7C–D</xref>), as it does for whole-body saccades (<xref ref-type="bibr" rid="bib5">Bender and Dickinson, 2006a</xref>). Interestingly, head saccades in body-free flies were also immediately followed by a head movement that returned the head to the neutral position (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). However, this return head movement was absent, or much slower, in body-fixed flies, suggesting that mechanosensory feedback plays an important role in terminating, or braking, head saccades (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). By fitting a decaying exponential (total damping time constant) to the head trajectory immediately after the head saccade responses, we discovered that body-fixed flies took ~8 times longer to return to baseline than body-free flies (Wilcoxon rank sum, p &lt; 0.001) (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). Interestingly, during the return head movement in body-free flies, the body was still in motion (<xref ref-type="fig" rid="fig7">Figure 7C</xref>), suggesting that body-generated feedback, or lack thereof, is the mechanism driving this difference in behavior. Because visual sensory feedback has little effect on saccade dynamics (<xref ref-type="bibr" rid="bib5">Bender and Dickinson, 2006a</xref>), this damping of head saccades is likely driven by nested mechanosensory feedback—although some degree of passive (mechanical) damping is likely present as well (<xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>). We found that head saccades performed in dark visual conditions followed similar trajectories, supporting the notion that mechanosensory, not visual, feedback mediates head saccade damping (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Intriguingly, the decrease in damping in body-fixed flies could also explain why wing saccades last upwards of 500ms in body-fixed flies, while body saccades in free flight typically last only 50–100ms (<xref ref-type="bibr" rid="bib11">Cellini and Mongeau, 2020b</xref>). While prior work has demonstrated that local haltere and wing muscle proprioceptive feedback feedback influence visuomotor gain (<xref ref-type="bibr" rid="bib43">Kathman and Fox, 2019</xref>; <xref ref-type="bibr" rid="bib57">Mureli and Fox, 2015</xref>; <xref ref-type="bibr" rid="bib58">Mureli et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bartussek and Lehmann, 2016</xref>; <xref ref-type="bibr" rid="bib49">Lehmann and Bartussek, 2017</xref>), it is unlikely that this mechanism could explain the attenuated saccade dynamics, due to saccades being visually open-loop (<xref ref-type="bibr" rid="bib5">Bender and Dickinson, 2006a</xref>). Overall, our findings strongly suggest that nested mechanosensory feedback has a significant influence on the control of both smooth head movements and head saccades in flies.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Head saccades are actively damped by mechanosensory feedback.</title><p>(<bold>A</bold>) Example body (red) and head (blue) trajectories for a body-free fly in the magnetic tether presented with a static visual stimulus. Rapid flight turns called saccades are highlighted. Note that head saccades are followed by a head movement that returns the head to the center position. Also see <xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>. (<bold>B</bold>) Same as A) but for a body-fixed fly. Head movements are shown in purple. Note that head saccades are not followed by a return head movement. (<bold>C</bold>) Left y-axis: averaged head saccade displacement (top) and velocity (bottom) for body-free and body-fixed flies. Right y-axis: averaged body saccade displacement (top) and velocity (bottom). Note that saccades typically last less than 200ms (bold portion of head and body trajectories indicate saccades), but an extra second of data is shown to illustrate the difference between the body-free and body-fixed head movements after a saccade. Inset shows the first 200ms of head and body trajectories. Shaded regions: ±1 STD. (<bold>D</bold>) Distributions of head saccade amplitude, peak velocity, duration, and damping time constant. The damping time constant <inline-formula><mml:math id="inf152"><mml:mi>τ</mml:mi></mml:math></inline-formula> was computed by fitting a decaying exponential to the head response directly after a saccade. ***Wilcoxon rank sum and t-test, p &lt; 0.001. Body-free: <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> flies, <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>566</mml:mn></mml:mrow></mml:math></inline-formula> saccades, Body-fixed: <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> flies, <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>346</mml:mn></mml:mrow></mml:math></inline-formula> saccades. (<bold>E</bold>) Proposed neural architecture for haltere-related damping of head movements for self-generated vs. externally-generated body motion. When body motion is self-generated, head and body motor commands are sent in parallel with an efferent signal, effectively closing a gate that allows mechanosensory feedback due to body motion to damp head movements. When body motion is externally generated, this gate is open and body motion has little effect on head movements (<xref ref-type="fig" rid="fig6">Figure 6A and (D</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Head saccades in darkness.</title><p>A Same as <xref ref-type="fig" rid="fig7">Figure 7D</xref>, but for a dark visual environment. Shaded regions: ±1 STD. Body-free: <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> flies, <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:math></inline-formula> saccades. Body-fixed: <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> flies, <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>33</mml:mn></mml:mrow></mml:math></inline-formula> saccades.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-fig7-figsupp1-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-80880-fig7-video1.mp4" id="fig7video1"><label>Figure 7—video 1.</label><caption><title>A body-free fly (magnetic tether) presented with a static visual background performing simultaneous body and head saccades.</title><p>The left video shows the raw images and the right video shows the images in the body-stabilized coordinate frame. See <xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref> for videos of saccades in body-fixed flies (<xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>).</p></caption></media></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We developed a mathematical model of gaze stabilization that accounted for the role of visual feedback and nested mechanosensory feedback in mediating head responses in flies (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Our model predicted differences in head responses between body-free and body-fixed flies based on changes in sensory feedback, which we confirmed with experimental data (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We revealed that visual feedback influenced the frequency tuning of head movements, whereas nested mechanosensory feedback due to body motion reduced the overall magnitude of head responses during smooth movements and saccades via active damping (<xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig7">7</xref>). By comparing head responses during self-generated and externally generated body motion, we uncovered a nonlinear gating of body-generated mechanosensory feedback on head movements influenced by self-motion. Overall, our findings unravel multisensory integration within nested sensory feedback loops in insect flight. We provide a framework amenable to study nested biological feedback loops across phyla.</p><sec id="s3-1"><title>Change in head movements between body-free and body-fixed flies is an emergent property of reflexive feedback</title><p>We discovered that body-fixed flies exhibited exaggerated head movements compared to body-free flies, which mirrors their exaggerated wing movements (<xref ref-type="bibr" rid="bib31">Fry et al., 2005</xref>). At face value, it might appear that body-fixed flies are adapting to the lack of stabilizing body movements by moving their head with larger magnitude. However, such a mechanism implies that flies must learn and change their neural controller (<inline-formula><mml:math id="inf161"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2A–B</xref>), to compensate for body fixation. Instead, our results support the notion that visual feedback underlies these changes and enables flies to partially compensate for body fixation. In other words, the larger head movements observed in body-fixed flies are an emergent property of reflexive feedback. In essence, the increase in sensory error due to body fixation (<xref ref-type="fig" rid="fig3">Figure 3</xref>) elicits a larger head motor response immediately, without requiring flies to learn a new neural controller. In this way, flies have some degree of built-in redundancy in their gaze stabilization system. An emergent property of this type of system is robustness to changes in the dynamics of one of the ‘motors’ (head or body). For example, wing damage is a common injury experienced by insects which could impair their ability to stabilize gaze via body movements during flight (<xref ref-type="bibr" rid="bib63">Rajabi et al., 2020</xref>). The change in visual feedback following wing damage could enable insects to rapidly compensate with their head, rather than learn how to stabilize gaze with a damaged wing (<xref ref-type="bibr" rid="bib56">Muijres et al., 2017</xref>). Indeed, we show that the head’s performance improves at low frequencies when the body is fixed (<xref ref-type="fig" rid="fig5">Figure 5B–C</xref>). This idea is further supported by behavioral evidence in primates, where eye movements in monkeys increase in magnitude to compensate for sudden head fixation with no detectable change in gaze (head +eye) velocity (<xref ref-type="bibr" rid="bib9">Bizzi, 1981</xref>; <xref ref-type="bibr" rid="bib48">Lanman et al., 1978</xref>). Tuned sensory feedback can be preferable to learning because sensory feedback acts on the order of neural latency and does not require animals to learn new strategies. However, sensory feedback and learning are not mutually exclusive and flies likely exhibit both (<xref ref-type="bibr" rid="bib85">Wolf et al., 1992</xref>).</p></sec><sec id="s3-2"><title>Nested mechanosensory feedback actively damps head movements</title><p>Mechanosensory feedback plays an important role in flight stability. Flies with bilateral haltere ablations or immobilized halteres cannot achieve stable flight (even in magnetically tethered assays), directly implicating mechanosensory feedback from the halteres in flight stabilization (<xref ref-type="bibr" rid="bib67">Ristroph et al., 2013</xref>). For the body, it appears that the low (~5ms) sensory delays associated with the halteres act synergistically with the slower (~30ms) visual system (<xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>; <xref ref-type="bibr" rid="bib59">Nakahira et al., 2021</xref>), thereby permitting larger visual gains (<xref ref-type="bibr" rid="bib24">Elzinga et al., 2012</xref>). In hawk moths, mechanosensory feedback from the antennae is nested within visual feedback during flower tracking, which is critical for high-frequency performance (<xref ref-type="bibr" rid="bib19">Dahake et al., 2018</xref>). This suggests that the structure of visuo-mechanosensory integration may be a preserved feature of insect flight. Visuo-mechanosensory integration also likely explains why wing responses are exaggerated in body-fixed flight, because mechanosensory feedback is not present to actively damp out steering responses (<xref ref-type="bibr" rid="bib24">Elzinga et al., 2012</xref>; <xref ref-type="bibr" rid="bib81">Taylor et al., 2008</xref>). However, the role of mechanosensory feedback and stability is less clear when considering the control of head movements. Indeed, the biomechanics of the head-neck system are inherently stable (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>), so what is the role of mechanosensory feedback in the head control system?</p><p>About the roll and pitch axes, head movements serve to maintain level gaze by offsetting variations in body roll and pitch (<xref ref-type="bibr" rid="bib37">Hardcastle and Krapp, 2016</xref>; <xref ref-type="bibr" rid="bib38">Hengstenberg, 1984</xref>). However, we show that this is largely not the case for about the yaw axis during externally generated body movements (<xref ref-type="fig" rid="fig6">Figure 6</xref>), suggesting that there is another mechanism at play. We discovered that mechanosensory feedback actively damped head movements in body-free flies (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>), similar to the proposed active damping of wing movements (<xref ref-type="bibr" rid="bib24">Elzinga et al., 2012</xref>). The active damping of head movements decreased head excursions, and occurrences of head saturation were reduced to near zero (<xref ref-type="fig" rid="fig3">Figure 3C and D</xref>). An interesting possibility is that mechanosensory feedback from body movements may act as a centering reflex to keep the head aligned relative to the thorax and thus prevent the head from reaching large angular excursions. Indeed, gaze stabilization quickly degrades as the head reaches its anatomical limits (<xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>). Another potential explanation is that the effects of mechanosensory feedback on head control are simply a result of coupled neural pathways between body control and head control. The descending sensorimotor pathways associated with the head and body have some overlap, suggesting that similar information—such as damping commands from the halteres—could be shared between the head and body (<xref ref-type="bibr" rid="bib62">Namiki et al., 2018</xref>). Revealing the precise role of mechanosensory feedback will require analyses of the neural pathways associated with head and body control.</p></sec><sec id="s3-3"><title>Nested proprioception across phyla</title><p>Our work in flies shows that sensing body motion via mechanosensory feedback has a transformative influence on a task that is driven by visual inputs (optomotor response). But similar proprioceptive mechanisms exist across phyla, and nested proprioception is likely a prevalent feature of animal locomotion. Many locomotor tasks in which an animal’s whole body moves in response to some external stimuli—such as flower tracking in moths (<xref ref-type="bibr" rid="bib76">Sponberg et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Roth et al., 2016</xref>), refuge tracking in fish (<xref ref-type="bibr" rid="bib68">Roth et al., 2011</xref>; <xref ref-type="bibr" rid="bib82">Uyanik et al., 2020</xref>), and wall following in cockroaches (<xref ref-type="bibr" rid="bib17">Cowan et al., 2006</xref>; <xref ref-type="bibr" rid="bib52">Mongeau et al., 2015</xref>)—likely involve proprioceptive feedback. Our framework could be applied to tease apart the role of proprioceptive mechanisms—such as the role of antennae, the vestibular system, or other mechanosensors—in task-level control. For instance, flies appear to use their antenna to damp out their visually guided groundspeed controller in a nested fashion (<xref ref-type="bibr" rid="bib33">Fuller et al., 2014a</xref>). A comparable experiment in mice or fish, where vestibular feedback from the the inner-ear is abolished (via chemical labyrinthectomy) could provide insights into how proprioception shapes locomotion in vertebrates (<xref ref-type="bibr" rid="bib41">Ito et al., 2019</xref>). Altogether, our framework is generalizable for teasing out the role of nested proprioception in a range of animal behaviors.</p></sec><sec id="s3-4"><title>Distinguishing between self-generated and externally generated body motion</title><p>Mechanosensory feedback due to body motion had little influence on head movements when body motion was externally generated as opposed to self-generated (<xref ref-type="fig" rid="fig6">Figure 6</xref>). For a LTI system, one would expect the same sensory inputs to lead to the same outputs. However, flies did not follow this expectation, suggesting that motor-related signals or visual feedback gate (non-linearly) mechanosensory feedback. We propose a model in which self-generated head/wing steering commands are sent in parallel with a signal that opens a gate to allow mechanosensory information to flow to the neck motor center (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). One possible mechanism at the neural level is that flies actively modulate gyroscopic sensing via haltere steering muscles.</p><p>Recent work confirmed that the haltere muscles are actively modulated by visual inputs during flight (<xref ref-type="bibr" rid="bib21">Dickerson, 2020</xref>). The &quot;control-loop&quot; hypothesis—originally proposed by <xref ref-type="bibr" rid="bib15">Chan et al., 1998</xref>—suggests that visual inputs modulate haltere muscle activity, which then regulate mechanosensory feedback by recruiting haltere campaniform sensilla (<xref ref-type="bibr" rid="bib15">Chan et al., 1998</xref>; <xref ref-type="bibr" rid="bib20">Dickerson et al., 2019</xref>). One possibility is that visual inputs could modulate haltere muscle activity and increase the magnitude of gyroscopic inputs, thus leading to damped head dynamics. Due to the body-fixed assays required for electrophysiology, it has not been impossible to determine whether gyroscopic inputs are modulated by visual inputs, but our results suggest that this could be the case. Altogether, our findings provide an extension of the control-loop hypothesis for the more specific case of gyroscopic sensing, that distinguishes between motor context (self-generated vs. externally generated body motion).</p><p>We cannot discount other mechanisms—such as haltere afferents gating subpopulations of neck motor neurons’ responses to visual stimuli—as the integration of visual and mechanosensory information is often nonlinear in insect flight (<xref ref-type="bibr" rid="bib75">Sherman and Dickinson, 2004</xref>; <xref ref-type="bibr" rid="bib40">Huston and Krapp, 2009</xref>; <xref ref-type="bibr" rid="bib36">Haag et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Kathman and Fox, 2019</xref>). Alternatively, gating may be modulated by an efference copy during self-generated turning maneuvers. These two hypotheses could not be teased apart here because flies mounted to a motor almost immediately stop flight if visual inputs conflict with prescribed motor rotation, that is, it was necessary to mount the visual display to the motor shaft for flies to sustain flight, thereby eliminating retinal slip due to externally generation motion, as in prior work (<xref ref-type="bibr" rid="bib74">Sherman and Dickinson, 2003</xref>).</p></sec><sec id="s3-5"><title>Neurophysiological evidence for gating of visual and mechanosensory information</title><p>The nonlinear gating we described here corroborates neurophysiological data on the influence of haltere <italic>tonic</italic> inputs on neck motor neurons. Recordings from a subpopulation of neck motor neurons demonstrated that information from the eyes and halteres is combined nonlinearly (<xref ref-type="bibr" rid="bib40">Huston and Krapp, 2009</xref>). Specifically, some neck motor neurons do not generate action potentials in response to visual motion alone, but will generate action potentials when the halteres are beating simultaneously and providing tonic inputs. Furthermore, the ventral cervical nerve motoneuron (VCNM) cell—which mediates head control—receives input from visual, haltere, and antennal sensory neurons (<xref ref-type="bibr" rid="bib36">Haag et al., 2010</xref>). Visual motion alone generates subthreshold activity, but when combined with mechanosensory inputs (antennae or halteres), causes the VCNM to spike. Notably, VCNM integrates a central input reflecting the behavioral state of the fly (flight and non-flight). While the influence of haltere tonic activity on downstream circuits has been characterized, at present it is unclear how <italic>gyroscopic</italic> inputs from the halteres influence neck motor neurons, primarily due to technical limitations of using fixed neurophysiological preparations. An interesting possibility is that some neck motor neurons, in the presence of gyroscopic feedback, could actively brake head movements thus providing a mechanism for active damping.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal preparation</title><p>We prepared flies according to a previously described protocol (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>). Briefly, we cold-anesthetized 3- to 5-day-old females flies (wild-type <italic>Drosophila melanogaster</italic>) by cooling them on a Peltier stage maintained at ~4° C.Following cold anesthesia, we fixed stainless steel minutien pins (100 µm diameter, Fine Science Tools, Foster City, CA) to the thorax of each fly using UV-activated glue (XUVG-1, Newall). We fixed the pin at angle of ~30°, consistent with the body’s angle of attack in freely flying flies. We allowed ~1 hr for recovery. For the body-free condition, we suspended each fly between two magnets, allowing free rotation along the yaw (vertical) axis (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The pin was fit into a sapphire bearing which has a coefficient of friction of ~0.1 (Vee jewel bearing, Bird Precision), which flies can readily overcome (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). The inertia of the pin was less than 1% of the fly’s inertia. Further, using an electromagnetic simulation we previously showed that frictional forces due to the pin-bearing interface are about two orders of magnitude smaller than forces generated in flight (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). Thus flies can readily overcome this friction, as previously shown (<xref ref-type="bibr" rid="bib53">Mongeau and Frye, 2017</xref>). For rigidly tethered (body-fixed) flies, all preparations were the same except we fixed flies to tungsten pins (A-M Systems) which were rigidly held in place (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). This is in contrast to previous work that instead rigidly tethered flies with a 90° angle with respect to the pin and angled the pin itself to 30°, although this difference has little effect on the head response (<xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>; <xref ref-type="bibr" rid="bib13">Cellini et al., 2021</xref>).</p></sec><sec id="s4-2"><title>Flight simulator</title><p>The virtual reality flight simulator illustrated in <xref ref-type="fig" rid="fig2">Figure 2C–D</xref> has been described elsewhere (<xref ref-type="bibr" rid="bib53">Mongeau and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib65">Reiser and Dickinson, 2008</xref>). The display consists of an array of <inline-formula><mml:math id="inf163"><mml:mrow><mml:mn>96</mml:mn><mml:mo>×</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula> light emitting diodes (LEDs, each subtending 3.75° on the eye) that wrap around the fly, subtending 360° horizontally and 60° vertically. We recorded the voltage signal output from our flight arena’s visual display with a data acquisition system (Measurement Computing, USB-1208FS-PLUS), which measures the displacement of our prescribed visual perturbation. We used this signal as the input to our model to ensure we accurately quantified what flies were actually seeing during experiments. We placed flies in the center of the arena and provided illumination from below with an array of twelve 940 nm LEDs and two 940 nm LEDs above. Body-free and body-fixed flies were examined using the same flight simulator. We recorded video at 100 <inline-formula><mml:math id="inf164"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with an infrared-sensitive camera placed directly below the fly (Basler acA640–750 µm). We used our custom computer vision software suite, CrazyFly (<ext-link ext-link-type="uri" xlink:href="https://github.com/boc5244/CrazyFly">https://github.com/boc5244/CrazyFly</ext-link>, <xref ref-type="bibr" rid="bib12">Cellini, 2021</xref>) to analyze body and head kinematics. The tracking algorithms have been described elsewhere (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). We measured the body angular position with respect to a global (flight arena) coordinate frame and head angular position relative to the body in each frame in our recorded videos.</p></sec><sec id="s4-3"><title>Visual perturbations</title><p>We primarily employed seven previously constructed single-sines visual perturbations <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at frequencies <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [0.7, 1, 1.5, 2.1, 3.5, 5.3, 10.6] Hz, designed to elicit robust head and body responses across a broad frequency range (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). The amplitude <inline-formula><mml:math id="inf167"><mml:mi>A</mml:mi></mml:math></inline-formula> at each frequency was chosen such that the velocity was normalized to <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 250°<inline-formula><mml:math id="inf169"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (mean speed of 159°<inline-formula><mml:math id="inf170"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). The perturbations can be represented as:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As in prior work, we also employed three previously constructed sum-of-sines visual perturbations (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). Briefly, each visual perturbation consisted of a 20-s sum-of-sines signal with nine logarithmically spaced frequencies components <italic>f</italic><sub><italic>i</italic></sub> in increments of 0.05 Hz, where no frequency was a prime harmonic of another. The amplitude of each frequency component was chosen such that the velocity of each component was normalized to thee values of <inline-formula><mml:math id="inf171"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [42, 70, 95]°<inline-formula><mml:math id="inf172"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and the phase <inline-formula><mml:math id="inf173"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> was randomized. The perturbations can be represented as:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>9</mml:mn></mml:munderover><mml:mrow><mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>These single-sine and sum-of-sines visual perturbations cover the frequency range of natural scene dynamics that a fly would normally experience in free flight (<xref ref-type="bibr" rid="bib44">Kern et al., 2005</xref>). All visual perturbations were displayed on our flight simulator as a grating with 30° spatial wavelength. This ensured that our perturbations had a mean temporal frequency of ~5 Hz, near the optimum of the motion vision pathway in <italic>Drosophila</italic> (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>, <xref ref-type="bibr" rid="bib42">Jung et al., 2011</xref>).</p></sec><sec id="s4-4"><title>Non-parametric system identification</title><p>Using a previously described method (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>), we applied frequency-domain system identification to determine non-parametric frequency-response functions from behavioral data. For a given input (ex: visual perturbation <inline-formula><mml:math id="inf174"><mml:mi>R</mml:mi></mml:math></inline-formula> or sensory error <inline-formula><mml:math id="inf175"><mml:mi>E</mml:mi></mml:math></inline-formula>) and output (ex: head <inline-formula><mml:math id="inf176"><mml:mi>H</mml:mi></mml:math></inline-formula> or body <inline-formula><mml:math id="inf177"><mml:mi>B</mml:mi></mml:math></inline-formula>) signal, we aimed to determine the relative magnitude (gain) and timing (phase) in frequency domain. We first detrended and low-pass filtered (cutoff 40 Hz) each signal in time-domain to remove low-frequency drift and high-frequency noise. We then transformed the input and output signals into frequency domain using a Chirp-Z Transform (<xref ref-type="bibr" rid="bib66">Remple and Tischler, 2006</xref>; <xref ref-type="bibr" rid="bib84">Windsor et al., 2014</xref>) at frequency points between 0–50 Hz in increments of 0.05 Hz. We divided the resulting complex response of the output signal by the complex response of the input signal, resulting in the frequency-response function <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> describing the transformation between input and output. We made no explicit assumption of linearity in <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as flies’ visuomotor responses tend to be marginally nonlinear (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cellini and Mongeau, 2020a</xref>). However, the high coherence or our transforms (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>), the consistent responses between single-sine and sum-of-sines perturbations (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> vs <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), and the close match between our replay experiment and theoretical prediction (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>) suggests that linear techniques can still be used with a high degree of confidence.</p><p>We extracted the gain and phase by taking the magnitude <inline-formula><mml:math id="inf180"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> and angle <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi mathvariant="normal">∠</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the complex response, respectively. We calculated the compensation error for each closed-loop frequency response function by computing the distance between the <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the perfect compensation condition <inline-formula><mml:math id="inf183"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (gain = 1, phase = 0°) on the complex plane (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>; <xref ref-type="bibr" rid="bib76">Sponberg et al., 2015</xref>). Compensation error can be expressed as:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We also calculated the coherence of each closed-loop transform using the MATLAB routine <italic>mscohere</italic> to ensure that head and body movements were sufficiently related to the visual perturbations (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Wherever there was saturation in the head response for body-fixed flies (as in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, top), we applied saturation-corrected least-squares-spectral-analysis (LSSA) (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Briefly, we removed the saturated portion of the data (where velocity was near zero) and fit a sine wave to the remaining un-saturated data (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Then we corrected the gain of any transforms affected by the saturation (<inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>R</mml:mi><mml:mo>→</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>E</mml:mi><mml:mo>→</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> for body-fixed flies) (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). To confirm that LSSA itself was not changing our results, we compared all transforms for the Chirp-Z transform and LSSA (without saturation correction) methods. Both methods yielded virtually identical results (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). Only the lowest two frequencies showed any difference in gain after the saturation correction routine and phase was unaffected across all frequencies and all methods (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>).</p></sec><sec id="s4-5"><title>Uncertainty propagation in frequency response functions</title><p>Experimentally measured frequency-response functions—such as the transforms between <inline-formula><mml:math id="inf186"><mml:mi>R</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf187"><mml:mi>H</mml:mi></mml:math></inline-formula> in body-free and body-fixed flies shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> and the <inline-formula><mml:math id="inf188"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf189"><mml:mi>H</mml:mi></mml:math></inline-formula> transforms in <xref ref-type="fig" rid="fig5">Figure 5</xref> —were measured for each fly and the mean and standard deviation of the gain, phase, and compensation error were calculated across flies (using circular statistics for phase <xref ref-type="bibr" rid="bib8">Berens, 2009</xref>). However, we were not able to apply the same statistical framework to estimate confidence intervals when computing mathematical predictions, such as in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, because our derived equations combined data sets with different groups of flies. For example, <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was measured in body-fixed flies, but <inline-formula><mml:math id="inf191"><mml:mi>B</mml:mi></mml:math></inline-formula> in body-free flies. Therefore, we estimated confidence intervals using a propagation of uncertainly analysis as described in prior work (<xref ref-type="bibr" rid="bib70">Roth et al., 2016</xref>).</p></sec><sec id="s4-6"><title>Stepper motor experiments</title><p>Flies were rigidly tethered to the shaft of a stepper motor (Nema 17) (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). The stepper motor was controlled with a motor driver (TB6600) with a resolution of 0.225° per step, thus providing smooth motion of the body. We controlled the motor by sending step and direction signals to the driver from a DAQ. We printed a black and white grating with 30° spatial wavelength (matching the grating displayed on our flight simulator) on standard A4 paper. We fixed the grating in a circular pattern to the motor shaft using a custom 3D printed part (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This ensured that any rotations of the motor—and thus the fly’s body—did not induce any visual feedback. We replayed the mean body motion measured from actively flying flies in the magnetic tether (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <italic>red</italic>) on the motor and measured the corresponding head response for actively flying flies (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, <italic>blue</italic>) and anesthetized flies (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, <italic>grey</italic>). We used triethylamine (commercially available as FlyNap, Carolina Biological Supply) to anesthetize flies. Also see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for the passive head response to a sum-of-sines perturbation.</p></sec><sec id="s4-7"><title>Control framework and derivation of closed-loop head responses</title><p>We synthesized our control framework based on previous work on the control of head and body movements in flies, where head and body velocity are the state variables (<xref ref-type="bibr" rid="bib14">Cellini et al., 2022</xref>). However, we made all computations based on head and body displacements, as taking the derivative (i.e. computing the velocity) of complex signals does not change the mathematical relationship between such signals (i.e. gain and phase stay the same). Furthermore, numerical differentiation typically amplifies noise (<xref ref-type="bibr" rid="bib83">van Breugel et al., 2020</xref>). When computing complex valued transforms (e.g., <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), we calculated the gain and phase for each frequency of the visual perturbation and constructed a non-parametric curve that consisted of the collection of these gains and phase values. All algebra done with these complex valued transforms was done by converting the gain <inline-formula><mml:math id="inf193"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> and phase <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi mathvariant="normal">∠</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into a single complex number <inline-formula><mml:math id="inf195"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">∠</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and substituting into the expressions we derive below.</p><p>To derive the expressions for the head response under different sensory feedback conditions, we started by considering the body-free case where sources of feedback are present. Head motion <inline-formula><mml:math id="inf196"><mml:mi>H</mml:mi></mml:math></inline-formula> can be written as the sum of sensory error <inline-formula><mml:math id="inf197"><mml:mi>E</mml:mi></mml:math></inline-formula> multiplied by the visual transform <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and body motion <inline-formula><mml:math id="inf199"><mml:mi>B</mml:mi></mml:math></inline-formula> multiplied by the mechanosensory transform <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> based on <xref ref-type="fig" rid="fig2">Figure 2A</xref>:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf201"><mml:mi>E</mml:mi></mml:math></inline-formula> is equivalent to the visual perturbation <inline-formula><mml:math id="inf202"><mml:mi>R</mml:mi></mml:math></inline-formula> subtracted by the fly’s gaze (sum of <inline-formula><mml:math id="inf203"><mml:mi>H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf204"><mml:mi>B</mml:mi></mml:math></inline-formula>):<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mi>H</mml:mi><mml:mo>-</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This framework omits the role of neck proprioceptive feedback, as the neck sensory system is intact in both body-free and body-fixed flies. Substituting <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> into <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> yields:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>H</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we can solve for <inline-formula><mml:math id="inf205"><mml:mi>H</mml:mi></mml:math></inline-formula> to obtain <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>. Normalizing by <inline-formula><mml:math id="inf206"><mml:mi>R</mml:mi></mml:math></inline-formula> yields the closed-loop transform from <inline-formula><mml:math id="inf207"><mml:mi>R</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf208"><mml:mi>H</mml:mi></mml:math></inline-formula> that we show in <xref ref-type="fig" rid="fig4">Figure 4</xref>:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mi>B</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>body visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mi>B</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>body mechanosensory</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Because the body also has its own associated visual and mechanosensory transforms <inline-formula><mml:math id="inf209"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), we could remove the <inline-formula><mml:math id="inf211"><mml:mi>B</mml:mi></mml:math></inline-formula> term from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> and <xref ref-type="disp-formula" rid="equ19">Equation 19</xref> by substituting <inline-formula><mml:math id="inf212"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf213"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. However, the resulting expression is lengthy and does not provide intuitive insights into the different sources of feedback as in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> and <xref ref-type="disp-formula" rid="equ19">Equation 19</xref> , therefore we chose not to include it.</p><p>To obtain the body-fixed closed-loop transform corresponding to <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, we set <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>To obtain the closed-loop transform without body mechanosensory feedback (body and head visual feedback only) we set <inline-formula><mml:math id="inf215"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mi>B</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>body visual</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>To obtain the closed-loop transform without body visual feedback (body mechanosensory feedback and head visual feedback only) we modified <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> such that <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and re-derived <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>, effectively removing the second term:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>head visual</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mi>B</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext>body mechanosensory</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>feedback</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>However, there is an interesting trick where we use the transform from <inline-formula><mml:math id="inf217"><mml:mi>E</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf218"><mml:mi>H</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf219"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig5">Figure 5A</xref>) to simplify this expression and remove the <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> term. <inline-formula><mml:math id="inf221"><mml:mi>H</mml:mi></mml:math></inline-formula> can be written equivalently to <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> as:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>By substituting <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> (with <inline-formula><mml:math id="inf222"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) into <xref ref-type="disp-formula" rid="equ23">Equation 23</xref> and solving for <inline-formula><mml:math id="inf223"><mml:mi>H</mml:mi></mml:math></inline-formula>, we obtain <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> which we can normalize by <inline-formula><mml:math id="inf224"><mml:mi>R</mml:mi></mml:math></inline-formula> to obtain:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>When making predictions using <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, and <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, we used data-driven methods rather than fitting closed-form transfer function models to the data. Thus, our transforms in <xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref> do not explicitly assume a model order.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80880-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code and data is available on Penn State ScholarSphere at this link: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.26207/qpxv-5v60">https://doi.org/10.26207/qpxv-5v60</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Cellini</surname><given-names>M</given-names></name><name><surname>Mongeau</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Unraveling nested feedback loops in insect gaze stabilization (Data for manuscript)</data-title><source>Scholarsphere</source><pub-id pub-id-type="doi">10.26207/qpxv-5v60</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Mark Frye and Martha Rimniceanu for valuable comments. This material is based upon work supported by the Air Force Office of Scientific Research (FA9550-20-1-0084) and an Alfred P Sloan Research Fellowship to JMM.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aström</surname><given-names>KJ</given-names></name><name><surname>Murray</surname><given-names>RM</given-names></name><name><surname>Johan Åström</surname><given-names>K</given-names></name><name><surname>Murray</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Feedback Systems: An Introduction for Scientists and Engineers</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Visual-Vestibular interaction in the control of head and eye movement: the role of visual feedback and predictive mechanisms</article-title><source>Progress in Neurobiology</source><volume>41</volume><fpage>435</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1016/0301-0082(93)90026-o</pub-id><pub-id pub-id-type="pmid">8210413</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartussek</surname><given-names>J</given-names></name><name><surname>Lehmann</surname><given-names>FO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Proprioceptive feedback determines visuomotor gain in <italic>Drosophila</italic></article-title><source>Royal Society Open Science</source><volume>3</volume><elocation-id>150562</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.150562</pub-id><pub-id pub-id-type="pmid">26909184</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beatus</surname><given-names>T</given-names></name><name><surname>Guckenheimer</surname><given-names>JM</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Controlling roll perturbations in fruit flies</article-title><source>Journal of the Royal Society, Interface</source><volume>12</volume><elocation-id>105</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2015.0075</pub-id><pub-id pub-id-type="pmid">25762650</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bender</surname><given-names>JA</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2006">2006a</year><article-title>A comparison of visual and haltere-mediated feedback in the control of body saccades in <italic>Drosophila melanogaster</italic></article-title><source>The Journal of Experimental Biology</source><volume>209</volume><fpage>4597</fpage><lpage>4606</lpage><pub-id pub-id-type="doi">10.1242/jeb.02583</pub-id><pub-id pub-id-type="pmid">17114395</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bender</surname><given-names>JA</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2006">2006b</year><article-title>Visual stimulation of saccades in magnetically tethered <italic>Drosophila</italic></article-title><source>The Journal of Experimental Biology</source><volume>209</volume><fpage>3170</fpage><lpage>3182</lpage><pub-id pub-id-type="doi">10.1242/jeb.02369</pub-id><pub-id pub-id-type="pmid">16888065</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bent</surname><given-names>LR</given-names></name><name><surname>Inglis</surname><given-names>JT</given-names></name><name><surname>McFadyen</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>When is vestibular information important during walking?</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>1269</fpage><lpage>1275</lpage><pub-id pub-id-type="doi">10.1152/jn.01260.2003</pub-id><pub-id pub-id-type="pmid">15102904</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>CircStat: A MATLAB toolbox for circular statistics</article-title><source>Journal of Statistical Software</source><volume>31</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v031.i10</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1981">1981</year><chapter-title>Eye‐Head coordination</chapter-title><person-group person-group-type="editor"><name><surname>Greger</surname><given-names>R</given-names></name></person-group><source>Comprehensive Physiology</source><publisher-name>Wiley</publisher-name><fpage>1321</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1002/cphy</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cellini</surname><given-names>B</given-names></name><name><surname>Mongeau</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Active vision shapes and coordinates flight motor responses in flies</article-title><source>PNAS</source><volume>117</volume><fpage>23085</fpage><lpage>23095</lpage><pub-id pub-id-type="doi">10.1073/pnas.1920846117</pub-id><pub-id pub-id-type="pmid">32873637</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cellini</surname><given-names>B</given-names></name><name><surname>Mongeau</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Hybrid visual control in fly flight: insights into gaze shift via saccades</article-title><source>Current Opinion in Insect Science</source><volume>42</volume><fpage>23</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.cois.2020.08.009</pub-id><pub-id pub-id-type="pmid">32896628</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Cellini</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>CrazyFly</data-title><version designator="705f9f6">705f9f6</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/boc5244/CrazyFly">https://github.com/boc5244/CrazyFly</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cellini</surname><given-names>B</given-names></name><name><surname>Salem</surname><given-names>W</given-names></name><name><surname>Mongeau</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mechanisms of punctuated vision in fly flight</article-title><source>Current Biology</source><volume>31</volume><fpage>4009</fpage><lpage>4024</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.080</pub-id><pub-id pub-id-type="pmid">34329590</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cellini</surname><given-names>B</given-names></name><name><surname>Salem</surname><given-names>W</given-names></name><name><surname>Mongeau</surname><given-names>JMM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Complementary feedback control enables effective gaze stabilization in animals</article-title><source>PNAS</source><volume>119</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2121660119</pub-id><pub-id pub-id-type="pmid">35503912</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>WP</given-names></name><name><surname>Prete</surname><given-names>F</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Visual input to the efferent control system of a fly’s “ gyroscope. ”</article-title><source>Science</source><volume>280</volume><fpage>289</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1126/science.280.5361.289</pub-id><pub-id pub-id-type="pmid">9535659</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>TS</given-names></name><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Visual control of flight behaviour in the hoverflysyritta pipiens L</article-title><source>Journal of Comparative Physiology ? A</source><volume>99</volume><fpage>1</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1007/BF01464710</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>NJ</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Full</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Task-level control of rapid wall following in the american cockroach</article-title><source>The Journal of Experimental Biology</source><volume>209</volume><fpage>1617</fpage><lpage>1629</lpage><pub-id pub-id-type="doi">10.1242/jeb.02166</pub-id><pub-id pub-id-type="pmid">16621943</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>NJ</given-names></name><name><surname>Ankarali</surname><given-names>MM</given-names></name><name><surname>Dyhr</surname><given-names>JP</given-names></name><name><surname>Madhav</surname><given-names>MS</given-names></name><name><surname>Roth</surname><given-names>E</given-names></name><name><surname>Sefati</surname><given-names>S</given-names></name><name><surname>Sponberg</surname><given-names>S</given-names></name><name><surname>Stamper</surname><given-names>SA</given-names></name><name><surname>Fortune</surname><given-names>ES</given-names></name><name><surname>Daniel</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Feedback control as a framework for understanding tradeoffs in biology</article-title><source>Integrative and Comparative Biology</source><volume>54</volume><fpage>223</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1093/icb/icu050</pub-id><pub-id pub-id-type="pmid">24893678</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahake</surname><given-names>A</given-names></name><name><surname>Stöckl</surname><given-names>AL</given-names></name><name><surname>Foster</surname><given-names>JJ</given-names></name><name><surname>Sane</surname><given-names>SP</given-names></name><name><surname>Kelber</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The roles of vision and antennal mechanoreception in hawkmoth flight control</article-title><source>eLife</source><volume>7</volume><elocation-id>e37606</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37606</pub-id><pub-id pub-id-type="pmid">30526849</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickerson</surname><given-names>BH</given-names></name><name><surname>de Souza</surname><given-names>AM</given-names></name><name><surname>Huda</surname><given-names>A</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Flies regulate wing motion via active control of a dual-function gyroscope</article-title><source>Current Biology</source><volume>29</volume><fpage>3517</fpage><lpage>3524</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.08.065</pub-id><pub-id pub-id-type="pmid">31607538</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickerson</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Timing precision in fly flight control: integrating mechanosensory input with muscle physiology</article-title><source>Proceedings. Biological Sciences</source><volume>287</volume><elocation-id>20201774</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2020.1774</pub-id><pub-id pub-id-type="pmid">33323088</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Haltere-mediated equilibrium reflexes of the fruit fly, <italic>Drosophila melanogaster</italic></article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>354</volume><fpage>903</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1098/rstb.1999.0442</pub-id><pub-id pub-id-type="pmid">10382224</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickinson</surname><given-names>MH</given-names></name><name><surname>Muijres</surname><given-names>FT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The aerodynamics and control of free flight manoeuvres in <italic>drosophila</italic></article-title><source>Philosophical Transactions of the Royal Society B</source><volume>371</volume><elocation-id>20150388</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0388</pub-id><pub-id pub-id-type="pmid">27528778</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elzinga</surname><given-names>MJ</given-names></name><name><surname>Dickson</surname><given-names>WB</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The influence of sensory delay on the yaw dynamics of a flapping insect</article-title><source>Journal of the Royal Society, Interface</source><volume>9</volume><fpage>1685</fpage><lpage>1696</lpage><pub-id pub-id-type="doi">10.1098/rsif.2011.0699</pub-id><pub-id pub-id-type="pmid">22188766</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faruque</surname><given-names>I</given-names></name><name><surname>Sean Humbert</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>Dipteran insect flight dynamics. part 1 longitudinal motion about hover</article-title><source>Journal of Theoretical Biology</source><volume>264</volume><fpage>538</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1016/j.jtbi.2010.02.018</pub-id><pub-id pub-id-type="pmid">20170664</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faruque</surname><given-names>I</given-names></name><name><surname>Sean Humbert</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Dipteran insect flight dynamics. part 2: lateral-directional motion about hover</article-title><source>Journal of Theoretical Biology</source><volume>265</volume><fpage>306</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.jtbi.2010.05.003</pub-id><pub-id pub-id-type="pmid">20470783</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fayyazuddin</surname><given-names>A</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Haltere afferents provide direct, electrotonic input to a steering motor neuron in the blowfly, Calliphora</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>5225</fpage><lpage>5232</lpage><pub-id pub-id-type="pmid">8756451</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fayyazuddin</surname><given-names>A</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Convergent mechanosensory input structures the firing phase of a steering motor neuron in the blowfly, Calliphora</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>1916</fpage><lpage>1926</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.4.1916</pub-id><pub-id pub-id-type="pmid">10515981</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraenkel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1939">1939</year><article-title>The function of the halteres of flies (diptera)</article-title><source>Proceedings of the Zoological Society of London</source><volume>A109</volume><fpage>69</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1111/j.1096-3642.1939.tb00049.x</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fry</surname><given-names>SN</given-names></name><name><surname>Sayaman</surname><given-names>R</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The aerodynamics of hovering flight in <italic>Drosophila</italic></article-title><source>The Journal of Experimental Biology</source><volume>208</volume><fpage>2303</fpage><lpage>2318</lpage><pub-id pub-id-type="doi">10.1242/jeb.01612</pub-id><pub-id pub-id-type="pmid">15939772</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multisensory systems integration for high-performance motor control in flies</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>347</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.002</pub-id><pub-id pub-id-type="pmid">20202821</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuller</surname><given-names>SB</given-names></name><name><surname>Karpelson</surname><given-names>M</given-names></name><name><surname>Censi</surname><given-names>A</given-names></name><name><surname>Ma</surname><given-names>KY</given-names></name><name><surname>Wood</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Controlling free flight of a robotic fly using an onboard vision sensor inspired by insect ocelli</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20140281</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0281</pub-id><pub-id pub-id-type="pmid">24942846</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuller</surname><given-names>SB</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Murray</surname><given-names>RM</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Flying <italic>Drosophila</italic> stabilize their vision-based velocity controller by sensing wind with their antennae</article-title><source>PNAS</source><volume>111</volume><fpage>E1182</fpage><lpage>E1191</lpage><pub-id pub-id-type="doi">10.1073/pnas.1323529111</pub-id><pub-id pub-id-type="pmid">24639532</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goldberg</surname><given-names>JM</given-names></name><name><surname>Wilson</surname><given-names>VJ</given-names></name><name><surname>Cullen</surname><given-names>KE</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Broussard</surname><given-names>DM</given-names></name><name><surname>Buttner-Ennever</surname><given-names>J</given-names></name><name><surname>Fukushima</surname><given-names>K</given-names></name><name><surname>Minor</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>The Vestibular System</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195167085.001.0001</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Wertz</surname><given-names>A</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Central gating of fly optomotor response</article-title><source>PNAS</source><volume>107</volume><fpage>20104</fpage><lpage>20109</lpage><pub-id pub-id-type="doi">10.1073/pnas.1009381107</pub-id><pub-id pub-id-type="pmid">21045125</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardcastle</surname><given-names>BJ</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evolution of biological image stabilization</article-title><source>Current Biology</source><volume>26</volume><fpage>R1010</fpage><lpage>R1021</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.059</pub-id><pub-id pub-id-type="pmid">27780044</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hengstenberg</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1984">1984</year><chapter-title>Roll-stabilization during flight of the blowfly’s head and body by mechanical and visual cues</chapter-title><person-group person-group-type="editor"><name><surname>Varjú</surname><given-names>D</given-names></name><name><surname>Schnitzler</surname><given-names>HU</given-names></name></person-group><source>Localization and Orientation in Biology and Engineering</source><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>121</fpage><lpage>134</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hengstenberg</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Mechanosensory control of compensatory head roll during flight in the blowflycalliphora erythrocephala Meig</article-title><source>Journal of Comparative Physiology A</source><volume>163</volume><fpage>151</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1007/BF00612425</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huston</surname><given-names>SJ</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Nonlinear integration of visual and haltere inputs in fly neck motor neurons</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13097</fpage><lpage>13105</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2915-09.2009</pub-id><pub-id pub-id-type="pmid">19846697</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>T</given-names></name><name><surname>Tatsumi</surname><given-names>K</given-names></name><name><surname>Takimoto</surname><given-names>Y</given-names></name><name><surname>Nishimura</surname><given-names>T</given-names></name><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Yamanaka</surname><given-names>T</given-names></name><name><surname>Takeda</surname><given-names>N</given-names></name><name><surname>Wanaka</surname><given-names>A</given-names></name><name><surname>Kitahara</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Vestibular compensation after vestibular dysfunction induced by arsanilic acid in mice</article-title><source>Brain Sciences</source><volume>9</volume><elocation-id>E329</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci9110329</pub-id><pub-id pub-id-type="pmid">31752103</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>SN</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Haag</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Flight activity alters velocity tuning of fly motion-sensitive neurons</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>9231</fpage><lpage>9237</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1138-11.2011</pub-id><pub-id pub-id-type="pmid">21697373</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kathman</surname><given-names>ND</given-names></name><name><surname>Fox</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Representation of haltere oscillations and integration with visual inputs in the fly central complex</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>4100</fpage><lpage>4112</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1707-18.2019</pub-id><pub-id pub-id-type="pmid">30877172</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Lindemann</surname><given-names>JP</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Function of a fly motion-sensitive neuron matches eye movements during free flight</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e171</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030171</pub-id><pub-id pub-id-type="pmid">15884977</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>AJ</given-names></name><name><surname>Fenk</surname><given-names>LM</given-names></name><name><surname>Lyu</surname><given-names>C</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Quantitative predictions orchestrate visual signaling in <italic>Drosophila</italic></article-title><source>Cell</source><volume>168</volume><fpage>280</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.12.005</pub-id><pub-id pub-id-type="pmid">28065412</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnaswamy</surname><given-names>PR</given-names></name><name><surname>Rangaiah</surname><given-names>GP</given-names></name><name><surname>Jha</surname><given-names>RK</given-names></name><name><surname>Deshpande</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>When to use cascade control</article-title><source>Industrial &amp; Engineering Chemistry Research</source><volume>29</volume><fpage>2163</fpage><lpage>2166</lpage><pub-id pub-id-type="doi">10.1021/ie00106a033</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Eye movements in man and other animals</article-title><source>Vision Research</source><volume>162</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2019.06.004</pub-id><pub-id pub-id-type="pmid">31254533</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanman</surname><given-names>J</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name><name><surname>Allum</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The coordination of eye and head movement during smooth pursuit</article-title><source>Brain Research</source><volume>153</volume><fpage>39</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(78)91127-7</pub-id><pub-id pub-id-type="pmid">98220</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehmann</surname><given-names>FO</given-names></name><name><surname>Bartussek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural control and precision of flight muscle activation in <italic>Drosophila</italic></article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>203</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1007/s00359-016-1133-9</pub-id><pub-id pub-id-type="pmid">27942807</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madhav</surname><given-names>MS</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The synergy between neuroscience and control theory: the nervous system as inspiration for hard control challenges</article-title><source>Annual Review of Control, Robotics, and Autonomous Systems</source><volume>3</volume><fpage>243</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1146/annurev-control-060117-104856</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milde</surname><given-names>JJ</given-names></name><name><surname>Seyan</surname><given-names>HS</given-names></name><name><surname>Strausfeld</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The neck motor system of the flycalliphora erythrocephala</article-title><source>Journal of Comparative Physiology A</source><volume>160</volume><fpage>225</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1007/BF00609728</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongeau</surname><given-names>JM</given-names></name><name><surname>Sponberg</surname><given-names>SN</given-names></name><name><surname>Miller</surname><given-names>JP</given-names></name><name><surname>Full</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sensory processing within cockroach antenna enables rapid implementation of feedback control for high-speed running maneuvers</article-title><source>The Journal of Experimental Biology</source><volume>218</volume><fpage>2344</fpage><lpage>2354</lpage><pub-id pub-id-type="doi">10.1242/jeb.118604</pub-id><pub-id pub-id-type="pmid">26026042</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongeau</surname><given-names>JMM</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>Drosophila</italic> spatiotemporally integrates visual signals to control saccades</article-title><source>Current Biology</source><volume>27</volume><fpage>2901</fpage><lpage>2914</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.08.035</pub-id><pub-id pub-id-type="pmid">28943085</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongeau</surname><given-names>JM</given-names></name><name><surname>Schweikert</surname><given-names>LE</given-names></name><name><surname>Davis</surname><given-names>AL</given-names></name><name><surname>Reichert</surname><given-names>MS</given-names></name><name><surname>Kanwal</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multimodal integration across spatiotemporal scales to guide invertebrate locomotion</article-title><source>Integrative and Comparative Biology</source><volume>61</volume><fpage>842</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1093/icb/icab041</pub-id><pub-id pub-id-type="pmid">34009312</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muijres</surname><given-names>FT</given-names></name><name><surname>Elzinga</surname><given-names>MJ</given-names></name><name><surname>Iwasaki</surname><given-names>NA</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Body saccades of <italic>Drosophila</italic> consist of stereotyped banked turns</article-title><source>The Journal of Experimental Biology</source><volume>218</volume><fpage>864</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1242/jeb.114280</pub-id><pub-id pub-id-type="pmid">25657212</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muijres</surname><given-names>FT</given-names></name><name><surname>Iwasaki</surname><given-names>NA</given-names></name><name><surname>Elzinga</surname><given-names>MJ</given-names></name><name><surname>Melis</surname><given-names>JM</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Flies compensate for unilateral wing damage through modular adjustments of wing and body kinematics</article-title><source>Interface Focus</source><volume>7</volume><elocation-id>20160103</elocation-id><pub-id pub-id-type="doi">10.1098/rsfs.2016.0103</pub-id><pub-id pub-id-type="pmid">28163885</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mureli</surname><given-names>S</given-names></name><name><surname>Fox</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Haltere mechanosensory influence on tethered flight behavior in <italic>Drosophila</italic></article-title><source>The Journal of Experimental Biology</source><volume>218</volume><fpage>2528</fpage><lpage>2537</lpage><pub-id pub-id-type="doi">10.1242/jeb.121863</pub-id><pub-id pub-id-type="pmid">26113141</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mureli</surname><given-names>S</given-names></name><name><surname>Thanigaivelan</surname><given-names>I</given-names></name><name><surname>Schaffer</surname><given-names>ML</given-names></name><name><surname>Fox</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cross-Modal influence of mechanosensory input on gaze responses to visual motion in <italic>Drosophila</italic></article-title><source>The Journal of Experimental Biology</source><volume>220</volume><fpage>2218</fpage><lpage>2227</lpage><pub-id pub-id-type="doi">10.1242/jeb.146282</pub-id><pub-id pub-id-type="pmid">28385799</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakahira</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Doyle</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Diversity-enabled sweet spots in layered architectures and speed-accuracy trade-offs in sensorimotor control</article-title><source>PNAS</source><volume>118</volume><elocation-id>e1916367118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.1916367118</pub-id><pub-id pub-id-type="pmid">34050009</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nalbach</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The halteres of the blowfly Calliphora</article-title><source>Journal of Comparative Physiology A</source><volume>173</volume><fpage>293</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1007/BF00212693</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nalbach</surname><given-names>G</given-names></name><name><surname>Hengstenberg</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The halteres of the blowfly calliphora II. three-dimensional organization of compensatory reactions to real and simulated rotations</article-title><source>Journal of Comparative Physiology A</source><volume>175</volume><fpage>695</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1007/BF00191842</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Namiki</surname><given-names>S</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name><name><surname>Wong</surname><given-names>AM</given-names></name><name><surname>Korff</surname><given-names>W</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The functional organization of descending sensory-motor pathways in <italic>Drosophila</italic></article-title><source>eLife</source><volume>7</volume><elocation-id>e34272</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34272</pub-id><pub-id pub-id-type="pmid">29943730</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajabi</surname><given-names>H</given-names></name><name><surname>Dirks</surname><given-names>JH</given-names></name><name><surname>Gorb</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Insect wing damage: causes, consequences and compensatory mechanisms</article-title><source>The Journal of Experimental Biology</source><volume>223</volume><elocation-id>jeb215194</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.215194</pub-id><pub-id pub-id-type="pmid">32366698</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauscher</surname><given-names>MJ</given-names></name><name><surname>Fox</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Haltere and visual inputs sum linearly to predict wing (but not gaze) motor output in tethered flying drosophila</article-title><source>Proceedings. Biological Sciences</source><volume>288</volume><elocation-id>20202374</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2020.2374</pub-id><pub-id pub-id-type="pmid">33499788</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A modular display system for insect behavioral neuroscience</article-title><source>Journal of Neuroscience Methods</source><volume>167</volume><fpage>127</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.07.019</pub-id><pub-id pub-id-type="pmid">17854905</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Remple</surname><given-names>RK</given-names></name><name><surname>Tischler</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Aircraft and Rotorcraft System Identification</source><publisher-loc>Reston ,VA</publisher-loc><publisher-name>American Institute of Aeronautics and Astronautics</publisher-name><pub-id pub-id-type="doi">10.2514/4.861352</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ristroph</surname><given-names>L</given-names></name><name><surname>Ristroph</surname><given-names>G</given-names></name><name><surname>Morozova</surname><given-names>S</given-names></name><name><surname>Bergou</surname><given-names>AJ</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Guckenheimer</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>ZJ</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Active and passive stabilization of body pitch in insect flight</article-title><source>Journal of the Royal Society, Interface</source><volume>10</volume><elocation-id>20130237</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2013.0237</pub-id><pub-id pub-id-type="pmid">23697713</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>E</given-names></name><name><surname>Zhuang</surname><given-names>K</given-names></name><name><surname>Stamper</surname><given-names>SA</given-names></name><name><surname>Fortune</surname><given-names>ES</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Stimulus predictability mediates a switch in locomotor smooth pursuit performance for eigenmannia virescens</article-title><source>The Journal of Experimental Biology</source><volume>214</volume><fpage>1170</fpage><lpage>1180</lpage><pub-id pub-id-type="doi">10.1242/jeb.048124</pub-id><pub-id pub-id-type="pmid">21389203</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>E</given-names></name><name><surname>Sponberg</surname><given-names>S</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A comparative approach to closed-loop computation</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>54</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.11.005</pub-id><pub-id pub-id-type="pmid">24709601</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>E</given-names></name><name><surname>Hall</surname><given-names>RW</given-names></name><name><surname>Daniel</surname><given-names>TL</given-names></name><name><surname>Sponberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Integration of parallel mechanosensory and visual pathways resolved through sensory conflict</article-title><source>PNAS</source><volume>113</volume><fpage>12832</fpage><lpage>12837</lpage><pub-id pub-id-type="doi">10.1073/pnas.1522419113</pub-id><pub-id pub-id-type="pmid">27791056</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salem</surname><given-names>W</given-names></name><name><surname>Cellini</surname><given-names>B</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name><name><surname>Mongeau</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fly eyes are not still: a motion illusion in <italic>Drosophila</italic> flight supports parallel visual processing</article-title><source>The Journal of Experimental Biology</source><volume>223</volume><elocation-id>jeb212316</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.212316</pub-id><pub-id pub-id-type="pmid">32321749</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sandeman</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Angular acceleration, compensatory head movements and the halteres of flies (Lucilia serricata)</article-title><source>Journal of Comparative Physiology ? A</source><volume>136</volume><fpage>361</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1007/BF00657358</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweigart</surname><given-names>G</given-names></name><name><surname>Mergner</surname><given-names>T</given-names></name><name><surname>Evdokimidis</surname><given-names>I</given-names></name><name><surname>Morand</surname><given-names>S</given-names></name><name><surname>Becker</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Gaze stabilization by optokinetic reflex (OKR) and vestibulo-ocular reflex (VOR) during active head rotation in man</article-title><source>Vision Research</source><volume>37</volume><fpage>1643</fpage><lpage>1652</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(96)00315-x</pub-id><pub-id pub-id-type="pmid">9231230</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname><given-names>A.</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A comparison of visual and haltere-mediated equilibrium reflexes in the fruit fly <italic>Drosophila melanogaster</italic></article-title><source>The Journal of Experimental Biology</source><volume>206</volume><fpage>295</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1242/jeb.00075</pub-id><pub-id pub-id-type="pmid">12477899</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname><given-names>A</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Summation of visual and mechanosensory feedback in <italic>Drosophila</italic> flight control</article-title><source>The Journal of Experimental Biology</source><volume>207</volume><fpage>133</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1242/jeb.00731</pub-id><pub-id pub-id-type="pmid">14638840</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sponberg</surname><given-names>S</given-names></name><name><surname>Dyhr</surname><given-names>JP</given-names></name><name><surname>Hall</surname><given-names>RW</given-names></name><name><surname>Daniel</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Insect flight. luminance-dependent visual processing enables moth flight in low light</article-title><source>Science</source><volume>348</volume><fpage>1245</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1126/science.aaa3042</pub-id><pub-id pub-id-type="pmid">26068850</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strausfeld</surname><given-names>NJ</given-names></name><name><surname>Seyan</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Convergence of visual, haltere, and prosternai inputs at neck motor neurons of Calliphora erythrocephala</article-title><source>Cell and Tissue Research</source><volume>240</volume><fpage>601</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1007/BF00216350</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>SL</given-names></name><name><surname>Deng</surname><given-names>ZL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Multi-sensor optimal information fusion Kalman filter</article-title><source>Automatica</source><volume>40</volume><fpage>1017</fpage><lpage>1023</lpage><pub-id pub-id-type="doi">10.1016/j.automatica.2004.01.014</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>EE</given-names></name><name><surname>Demir</surname><given-names>A</given-names></name><name><surname>Stamper</surname><given-names>SA</given-names></name><name><surname>Fortune</surname><given-names>ES</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dynamic modulation of visual and electrosensory gains for locomotor control</article-title><source>Journal of the Royal Society, Interface</source><volume>13</volume><elocation-id>118</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2016.0057</pub-id><pub-id pub-id-type="pmid">27170650</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taha</surname><given-names>HE</given-names></name><name><surname>Kiani</surname><given-names>M</given-names></name><name><surname>Hedrick</surname><given-names>TL</given-names></name><name><surname>Greeter</surname><given-names>JSM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Vibrational control: a hidden stabilization mechanism in insect flight</article-title><source>Science Robotics</source><volume>5</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.1126/scirobotics.abb1502</pub-id><pub-id pub-id-type="pmid">32999048</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>GK</given-names></name><name><surname>Bacic</surname><given-names>M</given-names></name><name><surname>Bomphrey</surname><given-names>RJ</given-names></name><name><surname>Carruthers</surname><given-names>AC</given-names></name><name><surname>Gillies</surname><given-names>J</given-names></name><name><surname>Walker</surname><given-names>SM</given-names></name><name><surname>Thomas</surname><given-names>ALR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>New experimental approaches to the biology of flight control systems</article-title><source>The Journal of Experimental Biology</source><volume>211</volume><fpage>258</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1242/jeb.012625</pub-id><pub-id pub-id-type="pmid">18165253</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uyanik</surname><given-names>I</given-names></name><name><surname>Sefati</surname><given-names>S</given-names></name><name><surname>Stamper</surname><given-names>SA</given-names></name><name><surname>Cho</surname><given-names>KA</given-names></name><name><surname>Ankarali</surname><given-names>MM</given-names></name><name><surname>Fortune</surname><given-names>ES</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Variability in locomotor dynamics reveals the critical role of feedback in task control</article-title><source>eLife</source><volume>9</volume><elocation-id>e51219</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51219</pub-id><pub-id pub-id-type="pmid">31971509</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Breugel</surname><given-names>F</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Numerical differentiation of noisy data: a unifying multi-objective optimization framework</article-title><source>IEEE Access</source><volume>8</volume><fpage>196865</fpage><lpage>196877</lpage><pub-id pub-id-type="doi">10.1109/access.2020.3034077</pub-id><pub-id pub-id-type="pmid">33623728</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Windsor</surname><given-names>SP</given-names></name><name><surname>Bomphrey</surname><given-names>RJ</given-names></name><name><surname>Taylor</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Vision-based flight control in the hawkmoth hyles lineata</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20130921</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2013.0921</pub-id><pub-id pub-id-type="pmid">24335557</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>R</given-names></name><name><surname>Voss</surname><given-names>A</given-names></name><name><surname>Hein</surname><given-names>S</given-names></name><name><surname>Heisenberg</surname><given-names>M</given-names></name><name><surname>Sullivan</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Can a fly ride a bicycle?</article-title><source>Philosophical Transactions of the Royal Society of London. Series B</source><volume>337</volume><fpage>261</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0104</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80880.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The manuscript makes an important contribution to feedback control in neural systems. The analysis and modeling together make a compelling case for a nested system, combining visual with mechanosensory feedback, for head and body control in the fruit fly. The experiments that support these results are compelling and well-executed and the strategies for dissecting and modeling feedback are valuable to the field, and broadly applicable to other neural control systems. This paper will reach a wide audience; researchers investigating biological control systems, visual feedback, and gaze stabilization will all be interested in these results.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80880.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Unraveling nested feedback loops in insect gaze stabilization: Mechanosensory feedback actively damps visually guided head movements in fly flight&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Ronald Calabrese as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>All of the reviewers enjoyed this paper but thought that some revisions to the presentation of results and the discussion would improve the manuscript. New experiments are discussed in the individual reviews but are not required for this revision. However, the reviewers all felt that including alternate paradigms, hypotheses, and the experiments that do or could distinguish them are crucial adds to the text. Those and other essential elements are summarized here, with individual reviews included at the end:</p><p>Essential revisions:</p><p>1) It is important to distinguish this model from prior ones in flies, from ones in vertebrates, and from other potential models that could account for the data. This kind of hypothesis testing of model architectures seems like it would add a lot to the paper, especially if you could rule out classes of models and suggest multiple alternative models consistent with your data (and other data in the field). Please see R3's comments along these lines, especially.</p><p>2) Issues with the presentation of the results:</p><p>(2a) Presentation issues should be addressed to clarify experiments and what each is doing/testing. Reviewers found some of the figures hard to follow, which was surprising given what seemed like relatively straightforward modeling. Please see R2's comments along these lines, in particular.</p><p>(2b) All reviewers found the presentation of the nested versus feedback architecture confusing, on different levels. Definitely clarify if dissecting this is an assertion from the outset (and if so, please modify that claim according to the detailed feedback from R3 and R3), or a hypothesis that is being tested. If the latter, please make it easier to read out the weight of the evidence supporting the nested feedback hypothesis, along the lines of R1's comments.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This paper aims to dissect the structure of feedback control in the stabilization of gaze when both the external world and the body and head are in motion. To achieve this, sensory-motor systems must integrate visual and self-motion cues, but the precise structure of that integration is not generally known in invertebrate systems. The authors focus on the fly as a model system, where previous work establishes a firm grounding for the results but gaps in knowledge of how canonical experimental manipulations, e.g. anchoring the body, affect motor responses still abound. Using an elegant experimental design where the same visual inputs are delivered during body-fixed and body-free tethered flight, the authors are able to quantify how gaze stabilization is impacted by the two forms of feedback. The work reveals that visual feedback shifts the scale of head movements when the external world moves at different frequencies, but that the self-motion cues from body rotations serve to dampen head movements and are nested within the visual control feedback loop. The nonlinearity in this nested control system is quantified convincingly in the paper.</p><p>Main strengths:</p><p>– The experimental design and analyses are well-motivated and executed.</p><p>– There are clear differences between the head movements and frequencies responses to external visual perturbations in the head-fixed and head-free conditions.</p><p>– The proposed model accounts for the empirical data in the two scenarios nicely.</p><p>Main weaknesses:</p><p>– The strength of the evidence for the differentiation between the two feedback schemes was not clear, and Figures 4 and 5 were hard to follow without more information.</p><p>– It was not clear if the model proposed is unique as opposed to simply sufficient for explaining the empirical data.</p><p>The work will be of interest to motor and systems neuroscientists who study feedback control, across a broad range of species. Biomechanics researchers will benefit from the framework laid out here and this will inspire future work to uncover the possible mechanisms of this control. Beyond biology, engineers and robotics researchers will take interest in this kind of nested feedback control, for the design of bio-inspired robotic systems.</p><p>There is a strong assumption about the analytical form of the feedback gain control (G/(1+G)), and this needs a sentence at least of justification and background in the Results.</p><p>Figures 4 and 5 highlight the main results of the work, but it was hard to figure out the strength of the evidence for the nested control topology from the figures. It would greatly enhance the broader impact of the work if these figures were made more intuitive for the reader. Perhaps the figures could start by showing a cartoon of what the results should look like in the extreme case of each feedback scenario and weighting, to set expectations.</p><p>Are there other options for the control system that would produce different results in the body-fixed versus body-free flies? It seems like this isn't the only feedback control scheme possible, so a more careful discussion of why the one proposed might be the <italic>unique</italic> solution to the problem and match the data is crucial.</p><p>Something needs to be said in the Discussion about how this adds to what we already knew from the primate literature about nested VOR feedback within OKR feedback. Does this new work point to new mechanisms? In the OKR, there's been good work showing that similar feedback is achieved in primates and zebrafish, but with very different circuitry. Can similarly crisp claims be highlighted here?</p><p>Are there new experiments suggested by these results in other species that could broaden the impact of work in the future?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In this work, the authors present a model for mechanosensory feedback nested inside a visual feedback loop, both controlling body and head yaw rotations. Using a variety of experiments, they fit this model to behavioral data in the fruit fly, where head and body yaw rotations can be easily measured, and in some cases, feedback can be manipulated. They use this data to fit their model and draw conclusions about how different feedback loops interact to stabilize the gaze in the fly.</p><p>The strength of this paper is in its rigorous approach to modeling the feedback in the fly's interactions with the visual world. It manages to fit its model non-parametrically at several different ethologically relevant frequencies of feedback. The comparisons of behavior with and without mechanosensory feedback are illuminating, as is the comparison of voluntary with involuntary mechanical feedback. One weakness of the paper is in its presentation, which can be a little opaque for non-specialists in control theory.</p><p>This paper provides a methodology for dissecting how different feedback systems interact and combine to jointly control behavior. While the specific manipulations available in the fly are not universally available, the approach seems likely to be useful for investigating many systems.</p><p>Overall, this work looks well done and contributes valuably to understanding how head and body feedback systems work in tandem to stabilize gaze in flies. Most of my major comments relate to the presentation.</p><p>Major comments</p><p>1) In the introduction, it would help if the authors laid out a little more about what's known and not known, and what precisely this paper is adding to the literature. For instance, the authors state that it's already known that mechanosensory feedback represents nested feedback inside the visual feedback loop. So what's left is merely fitting the model to data? Or are there alternative models that could be tested and ruled out with this data? (If there are, I think the framework of testing alternatives could be powerfully convincing about how predictive this particular model is.) At the end of the introduction, I was left puzzled about what the authors were adding.</p><p>2) The stimulus pattern should be defined. Pictures show a square wave grating; is this accurate? Does it matter? What was the wavelength? It looks like a 30 d period or so from the illustrations, which would put maximum temporal frequencies of the moving pattern at ~250 d/s / (30 d) = 8 Hz, which is about right for maximally driving optomotor responses.</p><p>Questions:</p><p>a. The perturbation signal R is a displacement but is measured presumably as a velocity by the eyes, and the direction-selective signal from the eye is a nonlinear function of velocity. If the tuning of the velocity signal is different for guiding body vs. head movements, does that matter or does that fit easily into this theory? In the presented model, there's only one single visual feedback signal to both body and head.</p><p>b. In the fastest oscillating stimuli, the pattern only moves back and forth by 2 pixels or so, and I believe these LEDs have something like 8 brightness levels. Is the intended stimulus really accurately captured by this display?</p><p>3) The model section of the methods should be clearer about what the different signals and coefficients are. As I understand it, everything is complex, so the products represent both gain and phase shifts of sinusoids, represented as complex numbers. It would be helpful to define why R should be thought of as displacement rather than velocity, and whether H, B, and G represent angles or angular velocities. Head angle is relative to the body, so angle seems reasonable, but I'd expect body orientation signals to be angular velocities or even accelerations. This might all not matter since it's all in a linear framework, but I think this could nonetheless be made clearer to non-specialists by defining the variables and terminology more explicitly. In the text, there's a reference to a complex s, which I assume is part of the integrand for a Laplace transform, but this could be spelled out more clearly or not mentioned at all since Laplace transforms are otherwise avoided. Then these gain and phase shifts are computed for each frequency of the stimulus, and non-parametric curves are found for each complex coefficient.</p><p>4) There's at least one alternative way to break the feedback here, and I'm curious about why it wasn't used to test or fit models. Instead of breaking the mechanosensory feedback loop, one could leave it in place, and instead, place flies in a virtual open loop, so that there is no visual feedback from the behaviors. It might be hard to track the head in real time to do this, but I'm interested to know if there are tests of the theory that could result from this sort of perturbation to the system. Along the same lines, gluing the head to the thorax would remove one source of gaze feedback and could be used to test the model for body movements. Are these interesting tests to do? (I'm not necessarily asking for these experiments.)</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The goal of this paper is to use the fruit fly <italic>Drosophila melanogaster</italic> to assess the relative contributions of vision and mechanosensory feedback in controlling head motion about the vertical, or yaw, axis. The authors perform a set of behavioral experiments comparing flies that are free to rotate in the yaw plane with rigidly tethered flies, using a control theoretic framework to make quantitative predictions about the importance of each sensory modality. They propose a model where mechanosensory feedback is nonlinearly integrated with visual feedback to control head steering, but only in the presence of whole-body rotations.</p><p>Overall, I find the paper well-written and the data very nicely presented. I appreciate the authors' formal use of control theory to make algebraic predictions about how the flies should respond to each perturbation and think this work adds a great deal to understanding the differences between free and tethered flight. I also like the conceptual approach of comparing parallel and nested sensory fusion problems in locomotion. That being said, I do have some major concerns about the approach that needs to be seriously addressed.</p><p>Control model and &quot;eliminating&quot; haltere feedback</p><p>This paper compares gaze stabilization in flies that can freely rotate about the yaw axis with those that are rigidly tethered. Crucially, in figure 2A, haltere feedback is presented as being a nested feedback loop that is only the result of the animal's body mechanics. In addition, the legend for 2C states, &quot;Note that contributions of body visual and mechanosensory feedback are no longer present and all nested feedback is gone.&quot; In light of recent work, specifically Dickerson et al. 2019, I do not think the authors' view on either matter is correct. As that paper shows, the haltere is providing constant input to the wing steering system-even in the absence of body rotations (It is also worth noting that Fayazzuddin and Dickinson 1999 proposed a model of wing steering muscle function where the wing and haltere provide constant, rhythmic input). Those experiments relied on imaging from the haltere axon terminals in the brain that likely synapse onto neck motor neurons that help control gaze (Strausfeld and Seyan 1989). Moreover, that feedback is partially under visual control; the haltere steering muscles change the trajectory of the haltere in the presence of visual input alone, modulating the feedback it provides to the wing steering system. I am not sure if that makes the haltere system parallel or nested with the visual system, but it certainly means that haltere feedback is not solely due to body mechanics. More importantly, this knowledge of physiology means that in a rigidly tethered fly, the authors cannot fully eliminate haltere input. This has tremendous implications for their modeling efforts, as they can never fully bring Ghead,M to zero. This may explain why, in Figure 4, body visual feedback alone cannot account for changes in head gain. It also means that a diagram like Figure 5B is essentially not possible in an intact fly, as the haltere signal is ever-present.</p><p>Proposed neural architecture</p><p>The authors propose a model of head stabilization in which the visual system sends motor commands to the neck in parallel with a gating command to the haltere that is only present during body motion. To me, this is essentially the &quot;control-loop&quot; hypothesis, proposed by Chan et al. 1998 and confirmed by Dickerson et al. 2019. In that model, the halteres provide continuous, wingbeat-synchronous feedback during flight. As the fly takes visual input, the haltere steering muscle motor neurons receive commands relayed by the visual system, altering the haltere's motion. This, in turn, recruits more campaniform sensilla for each wing stroke, which fire at different preferred phases from those providing the initial rhythm signal. Then, due to the haltere's direct, excitatory connection with the wing steering muscles, this changes the timing or recruitment of the wing steering system, changing aerodynamic forces and the fly's trajectory. This suggests that the haltere's gyroscopic sensing is an epiphenomenon that coopts its likely ancestral role in regulating the timing of the wing steering system, rather than the other way around. Again, whether this means that the visual → haltere connection is parallel or nested within the visual loop proposed by the authors, I am not certain, though I lean toward the former. Additionally, it is crucial to note that the haltere has collateral projections to the neck motor centers. Thus, as the visual system manipulates haltere kinematics and mechanosensory feedback, the haltere is controlling head motion in a reciprocal fashion, even when there are no imposed body motions. Even the nonlinear gating of neck motor neurons the authors note here is not entirely in keeping with the model proposed by Huston and Krapp 2009. There, the presence of haltere beating or visual stimulus alone was not enough to cause the neck MNs to fire. However, simultaneous haltere beating and visual stimulus did, implying that the fly need only be in flight (or walking, in the case of Calliphora) for the halteres to help control head motion; Coriolis forces due to body rotations imposed or otherwise, need not be present. The only difference I can see between what the authors propose and the control-loop hypothesis is that they focus on the head (which, again, is covered by the revised model of Dickerson et al.) and that the nonlinear damping gate requires body motion (which is inconsistent with the findings of Huston and Krapp).</p><p>I think the most critical change is rethinking the control model of visual and mechanosensory feedback in light of our understanding of the haltere motor system. As noted earlier, the experiments with rigidly tethered flies do not fully eliminate haltere feedback, which greatly impacts the math used to make predictions about how the animals respond to various perturbations. I recognize this requires a severe overhaul of the manuscript, but my concern is that by considering the haltere as merely a passive gyroscopic sensor leaves out a number of potential explanations for the data in Figures 4 and 5. Additionally, the authors need to think hard about whether the haltere is controlled in parallel or nested with the visual system, given that they have a reciprocal relationship even in the case of a rigidly tethered fly.</p><p>I was rather surprised in the section about active damping of head saccades that there was almost no mention of the recent work by Kim et al. 2017 showing that head motion during saccades seems to follow a feedforward motor program (or Strausfeld and Seyan's 1988 (?) work detailing how vision and haltere info combine to help control head motion). Furthermore, the head velocities for body-free and rigidly tethered flies seem similar, which points to it being a feedforward motor program, a la Kim et al. If you subtract body displacement from the free-rotating head motion, do you get a similar result? That would hint that head isn't overcompensating during body-fixed experiments and is driven more reflexively, as proposed in the discussion. I would also recommend looking at Bartussek and Lehmann 2017 for the impact of haltere mechanosensory input on 'visuomotor' gain, or the work from the Fox lab.</p><p>Finally, the authors either need to detail how their model is distinct from the control-loop hypothesis or back off their claim of novelty and show that their work lends further evidence to that model. I would also prefer if the figure panel for the model is either more anatomically accurate or stuck with the block diagram framing of information flow.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80880.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>[…]</p><p>There is a strong assumption about the analytical form of the feedback gain control (G/(1+G)), and this needs a sentence at least of justification and background in the Results.</p></disp-quote><p>Because the system we consider in this manuscript has nested feedback, the <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> form does not fully describe our model structure (as this only applies to the traditional one-sensor control system block diagram). If one were to consider <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as the transform from sensory error <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to head motion <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (which contains the nested feedback loop in our framework), then we can use this expression to describe our model. In this case, the assumption is about the visual feedback, which we assume has a gain of -1. We believe this is a valid assumption because optic flow is inversely proportional to the motion of the eyes relative to the world. We describe in more detail why we believe this model structure is appropriate below. We have also added a line clarifying this idea when we introduce our model.</p><disp-quote content-type="editor-comment"><p>Figures 4 and 5 highlight the main results of the work, but it was hard to figure out the strength of the evidence for the nested control topology from the figures. It would greatly enhance the broader impact of the work if these figures were made more intuitive for the reader. Perhaps the figures could start by showing a cartoon of what the results should look like in the extreme case of each feedback scenario and weighting, to set expectations.</p></disp-quote><p>We agree that Figure 4–5 could benefit from being made more intuitive and have made multiple changes to make the presentation clearer.</p><p>For Figure 4, the baseline feedback case where there is only head visual feedback (purple curve) is what we would expect the data to look like in every experiment/prediction if body visual and mechanosensory feedback had no effect. Thus, we now show all the head data in Figure 4 with respect to this curve, which allows for more explicit comparisons. We have also updated the legend and now use cartoons to illustrate the effect of the different types of feedback.</p><p>For Figure 5D, we have added the baseline prediction for the ratio of the <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> if body mechanosensory feedback had no effect (gain = 1, phase = 0), and explicitly indicated that values &lt;1 mean that head motion is damped by mechanosensory feedback.</p><disp-quote content-type="editor-comment"><p>Are there other options for the control system that would produce different results in the body-fixed versus body-free flies? It seems like this isn't the only feedback control scheme possible, so a more careful discussion of why the one proposed might be the unique solution to the problem and match the data is crucial.</p></disp-quote><p>In short, yes. A number of models with increasingly complex structures could be fit to our empirical data. However, we assert a parsimonious model structure that is well-suited for mathematically teasing apart the roles of the various sensory modalities during fly gaze stabilization. We believe that there is strong evidence supporting of our proposed model structure.</p><p>The match between our prediction of the effects of body visual feedback on the head response and the experimental replay experiment data (see Figure 4C, tan vs gray) is the most compelling evidence that supports our proposed model of visual feedback, as well as implying linearity. In a way, this is expected because the gain of the visual feedback loop (-1) is based on physics, i.e., moving one way elicits optic flow in an equal and opposite direction. The same visual feedback structure has been applied to model visuomotor tasks in other animals such as fish and moths, and has been shown to be similarly linear (Roth et al., 2011; Sponberg et al., 2015). To our knowledge, there has not been other models proposed for visual feedback in flies or other animals that would provide a more parsimonious explanation for the data.</p><p>In comparison to the structure of visual feedback, how flies integrate mechanosensory feedback during self-motion is slightly less clear. However, the <italic>nested</italic> structure of mechanosensory feedback is an inherent property of visually elicited behaviors—meaning that a visual stimulus elicits movement (due to the gaze stabilization reflex), which only then activates mechanosensory feedback, thus mechanosensory feedback is nested in this context. For clarity, the nested feedback architecture is not a hypothesis, but an assertion, when locomotion is reflexively driven by visual inputs. Our model is broadly consistent with previous work which suggested an inner-loop (nested) structure for haltere feedback (Elzinga et al., 2012). The model structure is also consistent with the role of antenna feedback in damping groundspeed control in flies (Fuller et al., 2014).</p><p>How mechanosensory is combined with visual information in the brain is not fully resolved. Seminal work showed that there is feedback from the halteres to the flight control muscles, even in the absence of visual inputs (Dickinson, 1999; Fayyazuddin and Dickinson, 1996). Furthermore, visual and haltere inputs due to body motion sum when presented together in rigidly tethered flies, although wing responses (like head responses we measured) were somewhat uncoordinated about the yaw axis (Sherman and Dickinson, 2004, 2003). A limitation of these prior studies is that they considered haltere inputs that were externally generated and in open-loop (flies were rigidly tethered to a motor) so it is difficult to say whether the same topology applies to gyroscopic haltere inputs due to self-motion, i.e. nested feedback. However, these data from prior studies do support the summing of visual and mechanosensory inputs at the neural level, which we maintain in our model structure (as outlined in Figure 2A). A major contribution of our work is the synthesis of a parsimonious linear model that can capture the empirical data.</p><p>For transparency, the parallel control topology we show in Figure 1A is not a permissible model structure for our study because it explicitly assumes that both sensory systems (i.e., visual and mechanosensory) receive the same external reference input. The general form of this model (based on Figure 1A) is:<inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>PC</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In this case, even if one of the sensory systems is abolished (for example vision in flies):<inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>PC</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, there will be a nonzero response <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to the refence input <inline-formula><mml:math id="sa2m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This parallel model is excellent for examining a fly’s response to coupled visual and haltere inputs (e.g., due to a gust of wind, etc.), but it is not appropriate for our analysis because the fly’s motion is reflexively driven by visual inputs. We present the parallel model to distinguish the nested model structure from prior models, e.g., (Roth et al., 2016).</p><p>In the experimental paradigm we employed, there is no external reference for the halteres, thus any feedback must be nested (meaning that there will not be any haltere feedback due to body motion unless visual motion first elicits body motion). This nested model takes the form (based on Figure 2B):</p><p><inline-formula><mml:math id="sa2m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>PC</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where abolishing <inline-formula><mml:math id="sa2m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> leads to <inline-formula><mml:math id="sa2m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, no matter what the input <inline-formula><mml:math id="sa2m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is. We now explicitly state from the onset that the nested feedback architecture we employ is an assertion, not a hypothesis.</p><disp-quote content-type="editor-comment"><p>Something needs to be said in the Discussion about how this adds to what we already knew from the primate literature about nested VOR feedback within OKR feedback. Does this new work point to new mechanisms? In the OKR, there's been good work showing that similar feedback is achieved in primates and zebrafish, but with very different circuitry. Can similarly crisp claims be highlighted here?</p></disp-quote><p>To our knowledge, by and large the primate literature on the VOR/OKR considers these visual and mechanosensory feedback loops summing in a parallel topology (due to the experimental design)—as opposed to mechanosensory signals being nested with visual feedback (reviewed in (Goldberg et al., 2012)). Further, to our knowledge, only a select few studies in primates have considered nested mechanosensory feedback (Schweigart et al., 1997), but have not attempted to unravel the contributions of the different feedback modalities in shaping the control of the head/eyes in the way that we have here. We now briefly discuss these ideas in the introduction and have added a few sentences in the discussion outlining the new contributions of our work.</p><disp-quote content-type="editor-comment"><p>Are there new experiments suggested by these results in other species that could broaden the impact of work in the future?</p></disp-quote><p>Yes, there are absolutely some interesting experiments focused on uncovering the role of nested mechanosensory/proprioceptive feedback that could be carried out in other species. Indeed, most animals have proprioceptive sensory systems that likely support higher level behaviors. In insects without halteres, such as moths, the antennae are thought to fulfill this role (Sane et al., 2007). We have added a Discussion section titled “Nested proprioception across phyla” considering these ideas.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>[…]</p><p>Overall, this work looks well done and contributes valuably to understanding how head and body feedback systems work in tandem to stabilize gaze in flies. Most of my major comments relate to the presentation.</p><p>Major comments</p><p>1) In the introduction, it would help if the authors laid out a little more about what's known and not known, and what precisely this paper is adding to the literature. For instance, the authors state that it's already known that mechanosensory feedback represents nested feedback inside the visual feedback loop. So what's left is merely fitting the model to data? Or are there alternative models that could be tested and ruled out with this data? (If there are, I think the framework of testing alternatives could be powerfully convincing about how predictive this particular model is.) At the end of the introduction, I was left puzzled about what the authors were adding.</p></disp-quote><p>While it is generally accepted that mechanosensory feedback due to self-motion (i.e., nested feedback) is present during visual driven tasks, the structure of this feedback and how it interacts with visual feedback during gaze stabilization in flight is presently unclear. Our work is the first (to our knowledge) to propose a neuromechanical model for the integration of visual and nested mechanosensory feedback based on empirical data and to quantify the effects of nested feedback on gaze stabilization. We agree that the introduction could benefit from clarification and have added a few sentences in the text discussing the novelty of our work.</p><disp-quote content-type="editor-comment"><p>2) The stimulus pattern should be defined. Pictures show a square wave grating; is this accurate? Does it matter? What was the wavelength? It looks like a 30 d period or so from the illustrations, which would put maximum temporal frequencies of the moving pattern at ~250 d/s / (30 d) = 8 Hz, which is about right for maximally driving optomotor responses.</p></disp-quote><p>Yes, we displayed a square wave pattern with a spatial wavelength of 30°, which is accurately illustrated in Figure 2. This is specified in the methods section titled “Visual perturbations”. We now clarify in this section that this spatial wavelength with our prescribed visual motion yields mean temporal frequencies of ~5 Hz (with a max of ~8 Hz), which is right around the optimum of the motion vision pathway in <italic>Drosophila</italic> (Duistermars et al., 2007a).</p><disp-quote content-type="editor-comment"><p>Questions:</p><p>a. The perturbation signal R is a displacement but is measured presumably as a velocity by the eyes, and the direction-selective signal from the eye is a nonlinear function of velocity. If the tuning of the velocity signal is different for guiding body vs. head movements, does that matter or does that fit easily into this theory? In the presented model, there's only one single visual feedback signal to both body and head.</p></disp-quote><p>The tuning of the velocity signal is in fact different for guiding the head and body, but this fits quite nicely into our theory. The tuning of the head and body with respect to visual motion is accounted for in their corresponding neural controllers (<inline-formula><mml:math id="sa2m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) shown in Figure 2A. Effectively, <inline-formula><mml:math id="sa2m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represent how the brain processes <inline-formula><mml:math id="sa2m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> differently for the head and body. We have shown in our previous work (Cellini et al., 2022) that <inline-formula><mml:math id="sa2m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is tuned closely to velocity while <inline-formula><mml:math id="sa2m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is actually tuned more strongly to acceleration. This difference in tuning is what explains why the body has a low-pass filter response and the head is more like a high-pass filter, as shown in Figure 2A,C and Figure 3A-B, corroborating our recent work (Cellini et al., 2022).</p><p>A nice property of our model and non-parametric approach is that is does not matter what the baseline tuning of the head and body are (velocity sensitive, acceleration sensitive, etc.). We isolate and measure <inline-formula><mml:math id="sa2m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (which contain <inline-formula><mml:math id="sa2m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) by removing mechanosensory feedback in body-fixed flies, and then use the response of body-free flies to work backwards and compute the effects of mechanosensory feedback. This way, our model can account for the differences in tuning between body-free and body-fixed flies, rather than focusing on the tuning itself. We have added a sentence in the text clarifying that <inline-formula><mml:math id="sa2m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> account for differences in tuning between the head and body.</p><disp-quote content-type="editor-comment"><p>b. In the fastest oscillating stimuli, the pattern only moves back and forth by 2 pixels or so, and I believe these LEDs have something like 8 brightness levels. Is the intended stimulus really accurately captured by this display?</p></disp-quote><p>The fastest oscillating stimuli does indeed only move between three pixels (one at the 0 position, and then 3.75°m in each direction). This results in something similar to a square wave being displayed on our flight simulator (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, left). However, when transformed into frequency-domain, the resulting signal is very similar to the idealized one (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, right). We account for any discrepancies in our analysis by using the actual displayed signal as the input to our model. This ensures that we are modeling a fly’s response to the frequency content of what they are actually seeing. We would not expect a fly’s response to be much different between the idealized and actual signals anyway, because a fly’s visual system low-pass filters high-frequency components of visual motion, which would make the square wave appear smoother than it actually is, e.g. see (Duistermars et al., 2007b) for a confirmation experiment. We now clarify in the text that we record the voltage signal from our visual display, which measures the displacement of our stimulus, and use that signal as the input to our model.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title><italic>Left</italic>: the idealized smooth sine wave at the highest frequency designed for our flight simulator (black) vs the actual displayed signal (red).</title><p>Note that our flight simulator display has an angular resolution of 3.75°. <italic>Right</italic>: same as the left, but for the Fast-Fourier Transform (FFT) magnitude of the two signals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80880-sa2-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>3) The model section of the methods should be clearer about what the different signals and coefficients are. As I understand it, everything is complex, so the products represent both gain and phase shifts of sinusoids, represented as complex numbers. It would be helpful to define why R should be thought of as displacement rather than velocity, and whether H, B, and G represent angles or angular velocities. Head angle is relative to the body, so angle seems reasonable, but I'd expect body orientation signals to be angular velocities or even accelerations. This might all not matter since it's all in a linear framework, but I think this could nonetheless be made clearer to non-specialists by defining the variables and terminology more explicitly. In the text, there's a reference to a complex s, which I assume is part of the integrand for a Laplace transform, but this could be spelled out more clearly or not mentioned at all since Laplace transforms are otherwise avoided. Then these gain and phase shifts are computed for each frequency of the stimulus, and non-parametric curves are found for each complex coefficient.</p></disp-quote><p>We fully acknowledge that flies primarily measure the velocity of wide-field visual perturbations with their visual system, as this has been shown extensively in prior work. Therefore, the gain and phase between the body <inline-formula><mml:math id="sa2m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext> </mml:mtext><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and visual perturbation <inline-formula><mml:math id="sa2m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be most precisely thought of as a ratio of velocities in frequency domain. However, a nice mathematical property of linear frequency-domain system identification is that this ratio is equivalent for displacements, velocities, accelerations, etc. For example, consider the case where if <inline-formula><mml:math id="sa2m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are defined as displacements. Multiplying them by the complex variable <inline-formula><mml:math id="sa2m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is equivalent to taking their derivatives in frequency domain (i.e., converting them to velocities) so the ratio is then defined as: <inline-formula><mml:math id="sa2m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mtext>sB</mml:mtext><mml:mtext>sR</mml:mtext></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mi>R</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> The <inline-formula><mml:math id="sa2m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> term cancels out, so our results would not be affected by converted these signals to velocities, or simply keeping them as displacements. However, computing displacement from velocity involves taking a numerical derivative, which amplifies noise (Van Breugel et al., 2020), thus we prefer to make all calculations using displacements. To make these points clearer, we have clarified in the text the definition of the complex variable <inline-formula><mml:math id="sa2m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and explained our reasoning for making our calculations using displacements instead of velocities.</p><disp-quote content-type="editor-comment"><p>4) There's at least one alternative way to break the feedback here, and I'm curious about why it wasn't used to test or fit models. Instead of breaking the mechanosensory feedback loop, one could leave it in place, and instead, place flies in a virtual open loop, so that there is no visual feedback from the behaviors. It might be hard to track the head in real time to do this, but I'm interested to know if there are tests of the theory that could result from this sort of perturbation to the system. Along the same lines, gluing the head to the thorax would remove one source of gaze feedback and could be used to test the model for body movements. Are these interesting tests to do? (I'm not necessarily asking for these experiments.)</p></disp-quote><p>We find this approach and the corresponding experiments tremendously intriguing, as it involves more potential manipulations to the control topology that could provide insight into the inner workings of the feedback system. One could potentially use a virtual/augmented reality system to abolish visual feedback from the head and/or body in real-time, which would correspond to changing the model from Equation 3 (now Equation 5) in the text to any of the three following forms:</p><p>1) Removed body visual feedback: <inline-formula><mml:math id="sa2m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>2) Removed head visual feedback: <inline-formula><mml:math id="sa2m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>3) Removed head and body visual feedback: <inline-formula><mml:math id="sa2m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>These expressions make predictions for the corresponding experiments in the virtual reality system and could provide further validation of our model. Our group has just recently developed a system to accomplish real-time virtual reality for the body in the magnetic tether system—although not yet for the head, as this is substantially more challenging to do in real-time than offline due to the need to find the neck joint in each frame for a rotating body. Thus, only the feedback case described by the equation in (1) is possible for us to achieve experimentally at present. (1) corresponds to Equation 6 (now Equation 8) in the text, and the prediction we made about the head response when body visual feedback is removed but mechanosensory feedback is still present (Figure 4D, cyan). Based on this prediction we would expect the head to operate with lower gain than when the body is fixed, but higher gain than when the body is free with natural body visual feedback. We are currently working on another manuscript that is beyond the scope of this study and now briefly discuss how these feedback manipulations fit into our framework within the discussion.</p><p>Regarding the second point about fixing the head and how this might affect the control of the body, we have previously published data on these experiments (for the sum-of-sines visual inputs). The data are presented in our previous paper (Cellini et al., 2022), but not integrated with the framework we introduce here. Our previous analysis showed that fixing the head has a modest, but significant, effect on the body gain and phase at high frequencies (where the head is typically the most active), which can be predicted by our control model. While these results support our proposed model and are further evidence for linearity, we believe that including these data/modeling is a bit beyond the scope of the current manuscript and does not directly address the role of nested mechanosensory feedback. Therefore, we prefer to leave this out of the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The goal of this paper is to use the fruit fly <italic>Drosophila melanogaster</italic> to assess the relative contributions of vision and mechanosensory feedback in controlling head motion about the vertical, or yaw, axis. The authors perform a set of behavioral experiments comparing flies that are free to rotate in the yaw plane with rigidly tethered flies, using a control theoretic framework to make quantitative predictions about the importance of each sensory modality. They propose a model where mechanosensory feedback is nonlinearly integrated with visual feedback to control head steering, but only in the presence of whole-body rotations.</p><p>Overall, I find the paper well-written and the data very nicely presented. I appreciate the authors' formal use of control theory to make algebraic predictions about how the flies should respond to each perturbation and think this work adds a great deal to understanding the differences between free and tethered flight. I also like the conceptual approach of comparing parallel and nested sensory fusion problems in locomotion. That being said, I do have some major concerns about the approach that needs to be seriously addressed.</p><p>Control model and &quot;eliminating&quot; haltere feedback</p><p>This paper compares gaze stabilization in flies that can freely rotate about the yaw axis with those that are rigidly tethered. Crucially, in figure 2A, haltere feedback is presented as being a nested feedback loop that is only the result of the animal's body mechanics. In addition, the legend for 2C states, &quot;Note that contributions of body visual and mechanosensory feedback are no longer present and all nested feedback is gone.&quot; In light of recent work, specifically Dickerson et al. 2019, I do not think the authors' view on either matter is correct. As that paper shows, the haltere is providing constant input to the wing steering system-even in the absence of body rotations (It is also worth noting that Fayazzuddin and Dickinson 1999 proposed a model of wing steering muscle function where the wing and haltere provide constant, rhythmic input). Those experiments relied on imaging from the haltere axon terminals in the brain that likely synapse onto neck motor neurons that help control gaze (Strausfeld and Seyan 1989). Moreover, that feedback is partially under visual control; the haltere steering muscles change the trajectory of the haltere in the presence of visual input alone, modulating the feedback it provides to the wing steering system. I am not sure if that makes the haltere system parallel or nested with the visual system, but it certainly means that haltere feedback is not solely due to body mechanics. More importantly, this knowledge of physiology means that in a rigidly tethered fly, the authors cannot fully eliminate haltere input. This has tremendous implications for their modeling efforts, as they can never fully bring Ghead,M to zero. This may explain why, in Figure 4, body visual feedback alone cannot account for changes in head gain. It also means that a diagram like Figure 5B is essentially not possible in an intact fly, as the haltere signal is ever-present.</p></disp-quote><p>We thank the reviewer for the detailed insights into the neurobiology of the haltere-wing system. We agree that our phrasing and terminology regarding “eliminating haltere feedback” could be misleading and needs to be revised.</p><p>As the reviewer points out, the halteres have been implicated in two primary functions:</p><p>1) as a gyroscope to sense body motion and</p><p>2) as a ‘metronome’ that regulates and structures the timing of motor outputs via tonic input (wings, and likely the head as well).</p><p>In our work, we have focused exclusively on function (1), as the emphasis of this manuscript is on how body motion influences head movements (from both vision and proprioception). Therefore, when we state that we have eliminated haltere (or mechanosensory) feedback, we are referring to eliminating function (1) of the halteres, i.e. inputs due to body motion.</p><p>As the reviewer accurately states, we have not eliminated all the functions of the halteres by fixing the body. This could only be achieved by ablating the halteres themselves (or potentially with genetic silencing), which would result in both functions (1) and (2) being eliminated. While bilaterally removing the halteres is an option in body-fixed flies and has been shown to have a modest effect on head movements (Mureli et al., 2017), this is unfortunately not possible in body-free flies (free or magnetically tethered) because flies immediately become unstable. This means that, even if we were to ablate the halteres in body-fixed flies, it would be difficult to infer if any differences in head responses we observed between body-free and body-fixed flies were due to functions (1) or (2). Keeping the halteres intact in both cases ensures that function (2) is active in both cases and any differences we observe between body-free and body-fixed flies are due to function (1).</p><p>Fortunately, even though we cannot remove function (2) of the halteres, we believe our model is still appropriate for understanding the role of function (1) without major alterations to the framework. As the reviewer states, visual motion drives steering commands of the head/wings in parallel with commands to the halteres themselves, regardless of whether body motion occurs. These haltere commands structure the timing of motor output in conjunction with visual inputs. Therefore, we think of function (2) more as a sub-component of the visual controller <inline-formula><mml:math id="sa2m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that mediates optomotor steering than as part of the mechanosensory controller <inline-formula><mml:math id="sa2m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that senses body motion (although haltere movements certainly affect how gyroscopic information is measured). Thus, we can lump function (2) in with the visual controller, rather than the mechanosensory controller, in order to investigate the role of function (1) of the halteres—because function (1) is the only thing that changes between conditions. For clarity, when we fix the body to isolate and measure <inline-formula><mml:math id="sa2m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, function (2) is still present, and thus contained within the dynamics of <inline-formula><mml:math id="sa2m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This same <inline-formula><mml:math id="sa2m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is active in body-free flies; thus, we can be sure that any differences we see between body-free and body-fixed flies in not due to function (2). While we acknowledge that this means we have not truly isolated the pure visual controller (because function 2) is always present, this does not affect any of our conclusions about function (1) of the halteres (i.e., gyroscopic inputs due to body motion).</p><p>To reconcile our model with prior work on function (2) of the halteres, we have added a section in the beginning of the results discussing the dual-function role of the halteres and changed our language throughout to be clear that fixing the body only removes function (1). Specifically, we now specify “…eliminated haltere/mechanosneosry feedback due to body motion…”, to distinguish from haltere feedback due to function (2).</p><disp-quote content-type="editor-comment"><p>Proposed neural architecture</p><p>The authors propose a model of head stabilization in which the visual system sends motor commands to the neck in parallel with a gating command to the haltere that is only present during body motion. To me, this is essentially the &quot;control-loop&quot; hypothesis, proposed by Chan et al. 1998 and confirmed by Dickerson et al. 2019. In that model, the halteres provide continuous, wingbeat-synchronous feedback during flight. As the fly takes visual input, the haltere steering muscle motor neurons receive commands relayed by the visual system, altering the haltere's motion. This, in turn, recruits more campaniform sensilla for each wing stroke, which fire at different preferred phases from those providing the initial rhythm signal. Then, due to the haltere's direct, excitatory connection with the wing steering muscles, this changes the timing or recruitment of the wing steering system, changing aerodynamic forces and the fly's trajectory. This suggests that the haltere's gyroscopic sensing is an epiphenomenon that coopts its likely ancestral role in regulating the timing of the wing steering system, rather than the other way around. Again, whether this means that the visual → haltere connection is parallel or nested within the visual loop proposed by the authors, I am not certain, though I lean toward the former. Additionally, it is crucial to note that the haltere has collateral projections to the neck motor centers. Thus, as the visual system manipulates haltere kinematics and mechanosensory feedback, the haltere is controlling head motion in a reciprocal fashion, even when there are no imposed body motions. Even the nonlinear gating of neck motor neurons the authors note here is not entirely in keeping with the model proposed by Huston and Krapp 2009. There, the presence of haltere beating or visual stimulus alone was not enough to cause the neck MNs to fire. However, simultaneous haltere beating and visual stimulus did, implying that the fly need only be in flight (or walking, in the case of Calliphora) for the halteres to help control head motion; Coriolis forces due to body rotations imposed or otherwise, need not be present. The only difference I can see between what the authors propose and the control-loop hypothesis is that they focus on the head (which, again, is covered by the revised model of Dickerson et al.) and that the nonlinear damping gate requires body motion (which is inconsistent with the findings of Huston and Krapp).</p></disp-quote><p>We further thank the reviewer for these insights. It is our understanding that the “control-loop” model proposed by (Chan et al., 1998) primarily refers to the halteres’ role in structuring motor output. This model is well supported by (Dickerson et al., 2019), but does not yet reveal how gyroscopic inputs due to body motion might be modulated by haltere muscles (as all experiments were performed on body-fixed flies and the specific campaniform arrays involved in sensing gyroscopic forces are currently ambiguous). However, the discussion in (Dickerson et al., 2019) sets up some nice hypotheses for how haltere muscles might act to recruit haltere campaniforms of different types—some that are <italic>insensitive</italic> to gyroscopic forces, and some that are <italic>sensitive</italic> to gyroscopic forces. We believe our work builds on these hypotheses and provides preliminary evidence that gyroscopic-sensitive inputs might be actively modulated by visual inputs, as flies’ head responses were strikingly different when they were controlling their own body motion <italic>vs</italic> when body motion was externally imposed. While our data does not provide evidence at the level of neural circuits, we believe our work stands nicely next to (Dickerson et al., 2019) and (Chan et al., 1998) and provides additional hypotheses to be tested. We now include additional discussion in the text (Discussion section: Distinguishing between self-generated and externally generated body motion) on these ideas and reconcile our data with prior models in the literature.</p><disp-quote content-type="editor-comment"><p>I think the most critical change is rethinking the control model of visual and mechanosensory feedback in light of our understanding of the haltere motor system. As noted earlier, the experiments with rigidly tethered flies do not fully eliminate haltere feedback, which greatly impacts the math used to make predictions about how the animals respond to various perturbations. I recognize this requires a severe overhaul of the manuscript, but my concern is that by considering the haltere as merely a passive gyroscopic sensor leaves out a number of potential explanations for the data in Figures 4 and 5. Additionally, the authors need to think hard about whether the haltere is controlled in parallel or nested with the visual system, given that they have a reciprocal relationship even in the case of a rigidly tethered fly.</p></disp-quote><p>See our response above for more detail. From our understanding of the metronome function of the halteres, i.e., providing tonic input, this internal feedback loop should be equally present in both body-free and body-fixed flies, and therefore does not offer a clear justification for why we see damping of head movements in body-free, but not body-fixed, flies. We have clarified in the introduction and the beginning of the results that, while the halteres serve to structure motor output independently of body motion, that any differences we observe in body-free flies strongly suggest that haltere inputs due to body motion (gyroscope forces) are the underlying cause<bold>.</bold></p><disp-quote content-type="editor-comment"><p>I was rather surprised in the section about active damping of head saccades that there was almost no mention of the recent work by Kim et al. 2017 showing that head motion during saccades seems to follow a feedforward motor program (or Strausfeld and Seyan's 1988 (?) work detailing how vision and haltere info combine to help control head motion). Furthermore, the head velocities for body-free and rigidly tethered flies seem similar, which points to it being a feedforward motor program, a la Kim et al. If you subtract body displacement from the free-rotating head motion, do you get a similar result? That would hint that head isn't overcompensating during body-fixed experiments and is driven more reflexively, as proposed in the discussion. I would also recommend looking at Bartussek and Lehmann 2017 for the impact of haltere mechanosensory input on 'visuomotor' gain, or the work from the Fox lab.</p></disp-quote><p>Kim et al., 2017 argues that head roll during a head saccade follows a feedforward motor program based on data showing that the head will roll (in addition to yaw) to offset body roll during a saccade, even though there is no body roll in the magnetic tether they used for experiments. We confirmed this result in a recent paper and provided further evidence for the feedforward hypothesis by studying head saccades in a rigid tether where there is no body motion (Cellini et al., 2021). While we agree that this data is consistent with a <italic>visually</italic> open-loop feedforward motor program, we believe that our data strongly supports the idea that mechanosensory feedback is present during saccades, especially during braking, which is consistent with previous work in a similar paradigm (Bender and Dickinson, 2006). While the peak velocities of head saccades in body-free and body-fixed flies are similar in our data (although statistically different), the most prominent difference is in how long it takes the head to return to baseline after the initial rapid movement (as shown in Figure 7). Furthermore, the amplitude of head saccades in body-free flies is considerably smaller than in body-fixed flies. These stark differences, even in the absence of visual features (see Figure S7), strongly suggest that mechanosensory feedback from body motion underlies this behavior. This is consistent with Kim et al. 2017, as our data still show that head saccades are likely visually open loop (visual features don’t change response). The rapid (5-10ms) response time of mechanosensory feedback is also well within the saccade duration (~50ms), so it is reasonable to assume that mechanosensory information can shape head saccade dynamics, even if they are visually open-loop. All head saccade data are presented in the body reference frame (head relative to the body), so subtracting (or adding) body movement would not change this interpretation. We have clarified in the text that head saccades are visually open-loop, but that mechanosensory feedback likely mediates braking. We now also cite (Kim et al., 2017), (Milde et al., 1987) , and (Strausfeld and Seyan, 1985) in the section discussing saccades. Note that we believe the reviewer was referring to Strausfeld and Seyan's 1985 and/or 1987 work (Milde et al., 1987; Strausfeld and Seyan, 1985), as we could not find a relevant study from 1988.</p><p>While (Bartussek and Lehmann, 2016; Lehmann and Bartussek, 2017) and (Kathman and Fox, 2019; Mureli et al., 2017; Mureli and Fox, 2015) present intriguing results describing how local haltere/wing proprioception shapes motor output (likely by modulating visuomotor gain), we feel that because these studies focus on the tonic function of the haltere (all experiments in body-fixed flies), that they do not address the role of the <italic>gyroscopic</italic> haltere inputs we investigate here. We have added a line in the Results section on saccades clarifying these ideas.</p><disp-quote content-type="editor-comment"><p>Finally, the authors either need to detail how their model is distinct from the control-loop hypothesis or back off their claim of novelty and show that their work lends further evidence to that model. I would also prefer if the figure panel for the model is either more anatomically accurate or stuck with the block diagram framing of information flow.</p></disp-quote><p>See our above comments. We have also modified the panel in Figure 7E to follow our block diagram format.</p><p>References</p><p>Bartussek J, Lehmann F-O. 2016. Proprioceptive feedback determines visuomotor gain in <italic>Drosophila</italic>. <italic>R Soc Open Sci</italic> 3. doi:10.1098/rsos.150562</p><p>Bender JA, Dickinson MH. 2006. A comparison of visual and haltere-mediated feedback in the control of body saccades in <italic>Drosophila melanogaster</italic>. <italic>J Exp Biol</italic> 209:4597–4606. doi:10.1242/jeb.02583</p><p>Cellini B, Mongeau J-M. 2020. Active vision shapes and coordinates flight motor responses in flies. <italic>Proc Natl Acad Sci</italic> 117:23085–23095. doi:10.1073/pnas.1920846117</p><p>Cellini B, Salem W, Mongeau J-M. 2021. Mechanisms of punctuated vision in fly flight. <italic>Curr Biol</italic> 31:4009-4024.e3. doi:10.1016/j.cub.2021.06.080</p><p>Cellini B, Salem W, Mongeau J-MM. 2022. Complementary feedback control enables effective gaze stabilization in animals. <italic>Proc Natl Acad Sci</italic> 119:e2121660119. doi:https://doi.org/10.1073/pnas.2121660119</p><p>Chan WP, Prete F, Dickinson MH. 1998. Visual Input to the Efferent Control System of a Fly’s “Gyroscope.” <italic>Science (80- )</italic> 280:289–292. doi:10.1126/science.280.5361.289</p><p>Dickerson BH, de Souza AM, Huda A, Dickinson MH. 2019. Flies Regulate Wing Motion via Active Control of a Dual-Function Gyroscope. <italic>Curr Biol</italic> 29:3517-3524.e3. doi:10.1016/j.cub.2019.08.065</p><p>Dickinson MH. 1999. Haltere–mediated equilibrium reflexes of the fruit fly, <italic>Drosophila melanogaster</italic>. <italic>Philos Trans R Soc London Ser B Biol Sci</italic> 354:903–916. doi:10.1098/rstb.1999.0442</p><p>Duistermars BJ, Chow DM, Condro M, Frye MA. 2007a. The spatial, temporal and contrast properties of expansion and rotation flight optomotor responses in <italic>Drosophila</italic>. <italic>J Exp Biol</italic> 210:3218–3227. doi:10.1242/jeb.007807</p><p>Duistermars BJ, Reiser MB, Zhu Y, Frye MA. 2007b. Dynamic properties of large-field and small-field optomotor flight responses in <italic>Drosophila</italic>. <italic>J Comp Physiol A Neuroethol Sensory, Neural, Behav Physiol</italic> 193:787–799. doi:10.1007/s00359-007-0233-y</p><p>Elzinga MJ, Dickson WB, Dickinson MH. 2012. The influence of sensory delay on the yaw dynamics of a flapping insect. <italic>J R Soc Interface</italic> 9:1685–1696. doi:10.1098/rsif.2011.0699</p><p>Fayyazuddin A, Dickinson MH. 1996. Haltere Afferents Provide Direct, Electrotonic Input to a Steering Motor Neuron in the Blowfly, Calliphora. <italic>J Neurosci</italic> 16:5225–5232. doi:10.1523/JNEUROSCI.16-16-05225.1996</p><p>Fuller SB, Straw AD, Peek MY, Murray RM, Dickinson MH. 2014. Flying <italic>Drosophila</italic> stabilize their vision-based velocity controller by sensing wind with their antennae. <italic>Proc Natl Acad Sci</italic> 111:E1182–E1191. doi:10.1073/pnas.1323529111</p><p>Goldberg JM, Wilson VJ, Cullen KE, Angelaki DE, Broussard DM, Buttner-Ennever J, Fukushima K, Minor LB. 2012. The Vestibular System, The Vestibular System: A Sixth Sense. Oxford University Press. doi:10.1093/acprof:oso/9780195167085.001.0001</p><p>Heisenberg M, Wolf R. 1986. Vision in <italic>Drosophila</italic>. Genetics in Microbehavior. Studies of Brain Function, Volume 12. M. Heisenberg , R. Wolf. <italic>Q Rev Biol</italic> 61:141–141. doi:10.1086/414849</p><p>Kathman ND, Fox JL. 2019. Representation of Haltere Oscillations and Integration with Visual Inputs in the Fly Central Complex. <italic>J Neurosci</italic> 39:4100–4112. doi:10.1523/JNEUROSCI.1707-18.2019</p><p>Kim AJ, Fenk LM, Lyu C, Maimon G. 2017. Quantitative Predictions Orchestrate Visual Signaling in <italic>Drosophila</italic>. <italic>Cell</italic> 168:280-294.e12. doi:10.1016/j.cell.2016.12.005</p><p>Lehmann F-O, Bartussek J. 2017. Neural control and precision of flight muscle activation in <italic>Drosophila</italic>. <italic>J Comp Physiol A</italic> 203:1–14. doi:10.1007/s00359-016-1133-9</p><p>Milde JJ, Seyan HS, Strausfeld NJ. 1987. The neck motor system of the fly Calliphora erythrocephala – II. Sensory organization. <italic>J Comp Physiol A</italic> 160:225–238. doi:10.1007/BF00609728</p><p>Mureli S, Fox JL. 2015. Haltere mechanosensory influence on tethered flight behavior in <italic>Drosophila</italic>. <italic>J Exp Biol</italic> 218:2528–2537. doi:10.1242/jeb.121863</p><p>Mureli S, Thanigaivelan I, Schaffer ML, Fox JL. 2017. Cross-modal influence of mechanosensory input on gaze responses to visual motion in <italic>Drosophila</italic>. <italic>J Exp Biol</italic> 220:2218–2227. doi:10.1242/jeb.146282</p><p>Roth E, Hall RW, Daniel TL, Sponberg S. 2016. Integration of parallel mechanosensory and visual pathways resolved through sensory conflict. <italic>Proc Natl Acad Sci</italic> 113:12832–12837. doi:10.1073/pnas.1522419113</p><p>Roth E, Sponberg S, Cowan N. 2014. A comparative approach to closed-loop computation. <italic>Curr Opin Neurobiol</italic> 25:54–62. doi:10.1016/j.conb.2013.11.005</p><p>Roth E, Zhuang K, Stamper SA, Fortune ES, Cowan NJ. 2011. Stimulus predictability mediates a switch in locomotor smooth pursuit performance for Eigenmannia virescens. <italic>J Exp Biol</italic> 214:1170–1180. doi:10.1242/jeb.048124</p><p>Sane SP, Dieudonné A, Willis MA, Daniel TL. 2007. Antennal Mechanosensors Mediate Flight Control in Moths. <italic>Science (80- )</italic> 315:863–866. doi:10.1126/science.1133598</p><p>Schweigart G, Mergner T, Evdokimidis I, Morand S, Becker W. 1997. Gaze Stabilization by Optokinetic Reflex (OKR) and Vestibulo-ocular Reflex (VOR) During Active Head Rotation in Man. <italic>Vision Res</italic> 37:1643–1652. doi:10.1016/S0042-6989(96)00315-X</p><p>Sherman A, Dickinson MH. 2004. Summation of visual and mechanosensory feedback in <italic>Drosophila</italic> flight control. <italic>J Exp Biol</italic> 207:133–142. doi:10.1242/jeb.00731</p><p>Sherman A, Dickinson MH. 2003. A comparison of visual and haltere-mediated equilibrium reflexes in the fruit fly <italic>Drosophila melanogaster</italic>. <italic>J Exp Biol</italic> 206:295–302. doi:10.1242/jeb.00075</p><p>Sponberg S, Dyhr JP, Hall RW, Daniel TL. 2015. Luminance-dependent visual processing enables moth flight in low light. <italic>Science (80- )</italic> 348:1245–1248. doi:10.1126/science.aaa3042</p><p>Stöckl AL, Kihlström K, Chandler S, Sponberg S. 2017. Comparative system identification of flower tracking performance in three hawkmoth species reveals adaptations for dim light vision. <italic>Philos Trans R Soc B Biol Sci</italic> 372:20160078. doi:10.1098/rstb.2016.0078</p><p>Strausfeld NJ, Seyan HS. 1985. Convergence of visual, haltere, and prosternai inputs at neck motor neurons of Calliphora erythrocephala. <italic>Cell Tissue Res</italic> 240:601–615. doi:10.1007/BF00216350</p><p>Van Breugel F Van, Kutz JN, Brunton BW. 2020. Numerical Differentiation of Noisy Data: A Unifying Multi-Objective Optimization Framework. <italic>IEEE Access</italic> 8:196865–196877. doi:10.1109/ACCESS.2020.3034077</p><p>Windsor SP, Taylor GK. 2017. Head movements quadruple the range of speeds encoded by the insect motion vision system in hawkmoths. <italic>Proc R Soc B Biol Sci</italic> 284:20171622. doi:10.1098/rspb.2017.1622</p><p>Wolf R, Voss A, Hein S, Heisenberg M, Sullivan GD. 1992. Can a fly ride a bicycle? <italic>Philos Trans R Soc London Ser B Biol Sci</italic> 337:261–269. doi:10.1098/rstb.1992.0104</p></body></sub-article></article>