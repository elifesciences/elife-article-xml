<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98701</article-id><article-id pub-id-type="doi">10.7554/eLife.98701</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98701.5</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical tracking of hierarchical rhythms orchestrates the multisensory processing of biological motion</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Li</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6088-7892</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Shuo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Yuhao</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wang</surname><given-names>Ying</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5756-2480</contrib-id><email>wangying@psych.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Jiang</surname><given-names>Yi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5746-7301</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>State Key Laboratory of Cognitive Science and Mental Health, Institute of Psychology, Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>Department of Psychology, University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>05</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP98701</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-23"><day>23</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-04-25"><day>25</day><month>04</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.23.590751"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-16"><day>16</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98701.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-27"><day>27</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98701.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-21"><day>21</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98701.3"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-24"><day>24</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98701.4"/></event></pub-history><permissions><copyright-statement>© 2024, Shen et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Shen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98701-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98701-figures-v1.pdf"/><abstract><p>When observing others’ behaviors, we continuously integrate their movements with the corresponding sounds to enhance perception and develop adaptive responses. However, how the human brain integrates these complex audiovisual cues based on their natural temporal correspondence remains unclear. Using electroencephalogram (EEG), we demonstrated that rhythmic cortical activity tracked the hierarchical rhythmic structures in audiovisually congruent human walking movements and footstep sounds. Remarkably, the cortical tracking effects exhibit distinct multisensory integration modes at two temporal scales: an additive mode in a lower-order, narrower temporal integration window (step cycle) and a super-additive enhancement in a higher-order, broader temporal window (gait cycle). Furthermore, while neural responses at the lower-order timescale reflect a domain-general audiovisual integration process, cortical tracking at the higher-order timescale is exclusively engaged in the integration of biological motion cues. In addition, only this higher-order, domain-specific cortical tracking effect correlates with individuals’ autistic traits, highlighting its potential as a neural marker for autism spectrum disorder. These findings unveil the multifaceted mechanism whereby rhythmic cortical activity supports the multisensory integration of human motion, shedding light on how neural coding of hierarchical temporal structures orchestrates the processing of complex, natural stimuli across multiple timescales.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multisensory integration</kwd><kwd>biological motion</kwd><kwd>rhythm</kwd><kwd>cortical tracking</kwd><kwd>autistic traits</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology of the People's Republic of China</institution></institution-wrap></funding-source><award-id>2021ZD0204200</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Ying</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology of the People's Republic of China</institution></institution-wrap></funding-source><award-id>2021ZD0203800</award-id><principal-award-recipient><name><surname>Jiang</surname><given-names>Yi</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32171059</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Ying</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32430043</award-id><principal-award-recipient><name><surname>Jiang</surname><given-names>Yi</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004739</institution-id><institution>Youth Innovation Promotion Association of the Chinese Academy of Sciences</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wang</surname><given-names>Ying</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002858</institution-id><institution>China Postdoctoral Science Foundation</institution></institution-wrap></funding-source><award-id>2024M170993</award-id><principal-award-recipient><name><surname>Shen</surname><given-names>Li</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002858</institution-id><institution>China Postdoctoral Science Foundation</institution></institution-wrap></funding-source><award-id>2024M753476</award-id><principal-award-recipient><name><surname>Shen</surname><given-names>Li</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>The Key Research and Development Program of Guangdong, China</institution></institution-wrap></funding-source><award-id>2023B0303010004</award-id><principal-award-recipient><name><surname>Jiang</surname><given-names>Yi</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution>Interdisciplinary Innovation Team of the Chinese Academy of Sciences</institution></institution-wrap></funding-source><award-id>JCTD-2021-06</award-id><principal-award-recipient><name><surname>Jiang</surname><given-names>Yi</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Jiang</surname><given-names>Yi</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Cortical tracking of multi-scale temporal structures supports audiovisual integration of human motion via distinct modes, with superadditive integration at higher-order timescale linked to biological motion processing and autistic traits.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The perception of biological motion (BM), the movements of living creatures, is a fundamental ability of the human visual system. Extensive evidence shows that humans can readily perceive BM from a visual display depicting just a handful of light dots attached to the head and major joints of a moving person (<xref ref-type="bibr" rid="bib8">Blake and Shiffrar, 2007</xref>). Nevertheless, in real life, BM perception often occurs in multisensory contexts. For instance, one may simultaneously hear footstep sounds while seeing others walking. The integration of these visual and auditory BM cues facilitates the detection, discrimination, and attentional processing of BM (<xref ref-type="bibr" rid="bib34">Mendonça et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Shen et al., 2023a</xref>; <xref ref-type="bibr" rid="bib52">Thomas and Shiffrar, 2013</xref>; <xref ref-type="bibr" rid="bib55">van der Zwan et al., 2009</xref>). Notably, such benefits are diminished when the visual BM is deprived of characteristic kinematic cues but not low-level motion attributes (<xref ref-type="bibr" rid="bib10">Brooks et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Shen et al., 2023a</xref>; <xref ref-type="bibr" rid="bib51">Thomas and Shiffrar, 2010</xref>), and the temporal windows of perceptual audiovisual synchrony are different between BM and non-BM stimuli (<xref ref-type="bibr" rid="bib1">Arrighi et al., 2006</xref>; <xref ref-type="bibr" rid="bib43">Saygin et al., 2008</xref>), highlighting the specificity of audiovisual BM processing. This specificity may relate to the evolutionary significance of BM and its pivotal role in social situations. In particular, integrating multisensory BM cues is foundational for perceiving and attending to other people and developing further social interaction. Such ability is usually compromised in people with social deficits, such as individuals with autism spectrum disorder (ASD) (<xref ref-type="bibr" rid="bib16">Falck-Ytter et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Feldman et al., 2018</xref>). These findings underline the unique contribution of multisensory BM processing to human perception and social cognition. However, despite the behavioral evidence, the neural coding of audiovisual BM cues and its possible link with individuals’ social cognitive capability remains largely unexplored.</p><p>An intrinsic property of human movements (such as walking and running) is that they are rhythmic and accompanied by frequency-congruent sounds. The audiovisual integration (AVI) of such rhythmic stimuli may involve a process whereby brain activity aligns with and tracks external rhythms, revealed by increased power or phase coherence of neural oscillations at corresponding frequencies (<xref ref-type="bibr" rid="bib3">Bauer et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Obleser and Kayser, 2019</xref>). Studies based on simple or discrete stimuli show that temporal congruency in auditory and visual rhythms significantly enhances the cortical tracking of rhythmic stimulations in both modalities (<xref ref-type="bibr" rid="bib11">Covic et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Keitel and Müller, 2016</xref>; <xref ref-type="bibr" rid="bib38">Nozaradan et al., 2012b</xref>). Unlike these stimuli, BM conveys complex hierarchical rhythmic structures corresponding to integration windows at multiple temporal scales. For example, the human locomotion movement has a narrower integration window consisting of each step (i.e. step cycle) and a broader integration window incorporating the opponent motion of the two feet (i.e. gait cycle). A recent study suggests that neural tracking of these nested kinematic structures contributes to the spatiotemporal integration of visual BM cues in different manners (<xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>). However, it remains open whether and how the cortical tracking of hierarchical rhythmic structures underpins the AVI of BM information.</p><p>To tackle this issue, we recorded electroencephalogram (EEG) signals from participants who viewed rhythmic point-light walkers or/and listened to the corresponding footstep sounds under visual (V), auditory (A), and audiovisual (AV) conditions in Experiments 1a and 1b (<xref ref-type="fig" rid="fig1">Figure 1</xref>). An enhanced cortical tracking effect in the AV condition compared to each unisensory condition will indicate significant multisensory gains. Moreover, we adopted an additive model to classify multisensory integration based on the AV vs. A+V comparison. This model assumes independence between inputs from each sensory modality and distinguishes among sub-additive (AV&lt;A +V), additive (AV = A +V), and super-additive (AV&gt;A +V) response modes (see a review by <xref ref-type="bibr" rid="bib50">Stevenson et al., 2014</xref>). The additive mode represents a linear combination between two modalities. In contrast, the super-additive and sub-additive modes indicate non-linear interaction processing, either with potentiated neural activation to facilitate the perception or detection of near-threshold signals (super-additive) or a deactivation mechanism to minimize the processing of redundant information cross-modally (sub-additive) (<xref ref-type="bibr" rid="bib30">Laurienti et al., 2005</xref>; <xref ref-type="bibr" rid="bib35">Metzger et al., 2020</xref>; <xref ref-type="bibr" rid="bib48">Stanford et al., 2005</xref>; <xref ref-type="bibr" rid="bib60">Wright et al., 2003</xref>). Distinguishing among these integration modes may help elucidate the neural mechanism underlying AVI in specific contexts.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Illustrations of audiovisual stimuli and experimental procedures.</title><p>The illustration was based on stimuli with a gait-cycle frequency of 1Hz. (<bold>a</bold>) Visual stimuli. The left panel depicts the static schematic of upright and inverted point-light walkers. The right panel shows the keyframes from a gait cycle of the biological motion (BM) sequence. The colors of dots and lines between dots are for illustration only and are not shown in the experiments. (<bold>b</bold>) Auditory stimuli. The auditory sequences contain periodic impulses of footstep sounds whose peak amplitudes occur around the points when the foot strikes the ground. The duration of two successive impulses defines the gait cycle of footstep sounds, which is temporally congruent (Con) or incongruent (InC) with the visual stimuli. (<bold>c</bold>) Experimental procedure and design. The color of the visual stimuli changed one or two times within 6s in the catch trials but did not change in the experimental trials. Participants were required to report the number of changes when the point-light stimulus was replaced by a red fixation. In Experiment 1, participants viewed rhythmic point-light walkers or/and listened to the corresponding footstep sounds under visual (V),auditory (A),and audiovisual (AV) conditions. The visual stimulus was the BM sequence in the V and AV conditions but a static frame from the sequence in the A condition. Experiment 2 included only the AV condition with different stimulus orientations (upright vs. inverted) and audiovisual congruency (congruent vs. incongruent).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98701-fig1-v1.tif"/></fig><p>Experiment 2 examined to what extent the AVI effect was specific to the multisensory processing of BM by using non-BM (inverted visual stimuli) as a control. Inversion disrupts the unique, gravity-compatible kinematic features of BM but not the rhythmic signals generated by low-level motion cues (<xref ref-type="bibr" rid="bib31">Ma et al., 2022</xref>; <xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>; <xref ref-type="bibr" rid="bib47">Simion et al., 2008</xref>; <xref ref-type="bibr" rid="bib53">Troje and Westhoff, 2006</xref>; <xref ref-type="bibr" rid="bib59">Wang et al., 2022</xref>), thus is expected to interfere with the BM-specific neural processing. Participants perceived visual BM stimuli accompanied by temporally congruent or incongruent BM sounds. Comparing the congruency effect in neural responses between the upright and inverted conditions allowed us to verify whether the AVI of BM involves a mechanism distinct from that underlies the AVI of non-BM. In addition, we further explored the possible linkage between the BM-specific neural tracking effect and observers’ autistic traits. Previous behavioral studies found reduced orienting to audiovisually synchronized BM stimuli in ASD (<xref ref-type="bibr" rid="bib17">Falck-Ytter et al., 2018</xref>). Since individuals with varying social cognitive abilities lie on a continuum extending from clinical to nonclinical populations with different levels of autistic traits, we investigated whether cortical tracking of audiovisual BM correlates with individuals’ autistic traits, as measured by the Autism-Spectrum Quotient (AQ) (<xref ref-type="bibr" rid="bib2">Baron-Cohen et al., 2001</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In all experiments, 17–23% of the trials were randomly selected as catch trials, in which the color of the walker changed one or two times throughout the trial, and there was no color change in other trials. Participants were required to detect the color change of visual stimuli (zero to two times during one trial) to maintain attention. Behavioral analysis on all trials showed that their performances for the task were generally high and equally well in all conditions of Experiment 1a (mean accuracy&gt;98%; <italic>F</italic> (2, 46)=0.814, p=0.450, <italic>ƞ<sub>p</sub></italic><sup>2</sup>=20.034), Experiment 1b (mean accuracy&gt;98%; <italic>F</italic> (2, 46)=0.615, p=0.545, <italic>ƞ<sub>p</sub></italic><sup>2</sup>=20.026), and Experiment 2 (mean accuracy&gt;98%; <italic>F</italic> (3, 69)=0.493, p=0.688, <italic>ƞ<sub>p</sub></italic><sup>2</sup>=20.021), indicating comparable attention state across conditions. The catch trials were excluded from the following EEG analysis.</p><sec id="s2-1"><title>Cortical tracking of rhythmic structures in audiovisual BM reveals AVI</title><sec id="s2-1-1"><title>Experiment 1a</title><p>In Experiment 1a, we examined the cortical tracking of rhythmic BM information under V, A, and AV conditions (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). We were interested in two critical rhythmic structures in the walking motion sequence, i.e., the gait cycle and the step cycle (<xref ref-type="fig" rid="fig1">Figure 1a and b</xref>). During walking, each step of the left or right foot occurs alternatively to form a step cycle, and the antiphase oscillations of limbs during two steps characterize a gait cycle (<xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>). In Experiment 1a, the frequency of a full gait cycle is 1Hz, and the step-cycle frequency is 2Hz. The strength of the cortical tracking effect was quantified by the amplitude peaks emerging from the EEG spectra at these frequencies.</p><p>As shown in the grand average amplitude spectra (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), both the responses in three conditions showed clear peaks at step-cycle frequency (2Hz; V: <italic>t</italic> (23)=6.963, p&lt;0.001; A: <italic>t</italic> (23)=6.073, p&lt;0.001; AV: <italic>t</italic> (23)=7.054, p&lt;0.001; FDR corrected). In contrast, at gait-cycle frequency (1Hz), only the response to AV stimulation showed significant peaks (V: <italic>t</italic> (23)=–2.072, p=0.975; A: <italic>t</italic> (23)=–0.054, p=0.521; AV: <italic>t</italic> (23)=4.059, p&lt;0.001; FDR corrected). Besides, we also observed significant peaks at 4Hz in all three conditions (ps&lt;0.001, FDR corrected), which showed a similar audiovisual integration mode as 2Hz (see more details in Appendix and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Cortical tracking of visual (V),auditory (A),and audiovisual (AV) biological motion (BM) signals at gait-cycle and step-cycle frequencies.</title><p>(<bold>a</bold>) and (<bold>d</bold>) The amplitude spectra of electroencephalogram (EEG) responses in three conditions in Experiment 1a and Experiment 1b, respectively. The solid lines show the grand average amplitude over all electrodes and subjects. The shaded regions depict standard errors of the group mean. Asterisks indicate significant spectra peaks (one-sample t-test against zero; p&lt;0.05, FDR corrected). (<bold>b</bold>) and (<bold>e</bold>) The normalized amplitude at gait-cycle frequency in the AV condition exceeded the arithmetical sum of those in V and A conditions (AV&gt;A +V, paired t-test), (<bold>c</bold>) and (<bold>f</bold>) but the normalized amplitude at step-cycle frequency in the AV condition was comparable to the sum of V and A (AV = A +V, paired t-test). Colored dots represent individual data in each condition. Error bars represent±1 standard error of means (N = 24). *: p&lt;0.05; **: p&lt;0.01; ***: p&lt;0.001; m.s.: 0.05&lt;p&lt;0.10; n.s<italic>.:</italic> p&gt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98701-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Cortical tracking of audiovisual biological motion (BM) information at different frequencies.</title><p>Colored dots represent individual data in each condition. Error bars represent±1 standard error of means. N=48 (24 in Experiment 1a and 24 in Experiment 1b). Paired t-test. *: p&lt;0.05; **: p&lt;0.01; ***: p&lt;0.001; n.s.: p&gt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98701-fig2-figsupp1-v1.tif"/></fig></fig-group><p>Furthermore, we directly compared the cortical tracking effects between different conditions via a two-tailed paired t-test. At both 1Hz (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and 2Hz (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), the amplitude in the AV condition was greater than that in the V condition (1Hz: <italic>t</italic> (23)=4.664, p&lt;0.001, Cohen’s <italic>d</italic>=0.952; 2Hz: <italic>t</italic> (23)=5.132, p&lt;0.001, Cohen’s <italic>d</italic>=1.048) and the A condition (1Hz: <italic>t</italic> (23)=2.391, p=0.025, Cohen’s <italic>d</italic>=0.488; 2Hz: <italic>t</italic> (23)=3.808, p&lt;0.001, Cohen’s <italic>d</italic>=0.777), respectively, suggesting multisensory gains. More importantly, at 1Hz, the amplitude in the AV condition was significantly larger than the algebraic sum of those in the A and V conditions (<italic>t</italic> (23)=3.028, p=0.006, Cohen’s <italic>d</italic>=0.618), indicating a super-additive audiovisual integration effect. While at 2Hz, the amplitude in the AV condition was comparable to the unisensory sum (<italic>t</italic> (23)=–0.623, p=0.539, Cohen’s <italic>d</italic>=–0.127), indicating additive audiovisual integration.</p></sec><sec id="s2-1-2"><title>Experiment 1b</title><p>To further test whether such cortical tracking effect can apply to stimuli with a different speed, Experiment 1b altered the frequencies of the gait cycle and the corresponding step cycle to 0.83Hz and 1.67Hz while adopting the same paradigm as Experiment 1a. Consistent with Experiment 1a, the frequency-domain analysis revealed significant cortical tracking of the audiovisual stimuli at the new speeds. As shown in <xref ref-type="fig" rid="fig2">Figure 2d</xref>, both the responses to V, A, and AV stimuli showed clear peaks at step-cycle frequency (1.67Hz; V: <italic>t</italic> (23)=3.473, p=0.001; A: <italic>t</italic> (23)=9.194, p&lt;0.001; AV: <italic>t</italic> (23)=8.756, p&lt;0.001; FDR corrected) and its harmonics (3.33Hz, ps&lt;0.001, FDR corrected). In contrast, at gait-cycle frequency (0.83Hz), only the response to AV stimuli showed significant peaks (V: <italic>t</italic> (23)=–1.125, p=0.846; A: <italic>t</italic> (23)=–2.449, p=0.989; AV: <italic>t</italic> (23)=3.052, p=0.003; FDR corrected).</p><p>At both 0.83Hz (<xref ref-type="fig" rid="fig2">Figure 2e</xref>) and 1.67Hz (<xref ref-type="fig" rid="fig2">Figure 2f</xref>), the amplitude in the AV condition was stronger or marginally stronger than that in the V condition (0.83Hz: <italic>t</italic> (23)=2.665, p=0.014, Cohen’s <italic>d</italic>=0.544; 1.67Hz: <italic>t</italic> (23)=6.380, p&lt;0.001, Cohen’s <italic>d</italic>=1.302) and the A condition (0.83Hz: <italic>t</italic> (23)=3.625, p&lt;0.001, Cohen’s <italic>d</italic>=0.740; 1.67Hz: <italic>t</italic> (23)=1.752, p=0.093, Cohen’s <italic>d</italic>=0.358), respectively, suggesting multisensory gains. More importantly, at 0.83Hz, the amplitude in the AV condition was significantly larger than the sum of those in the A and V conditions (<italic>t</italic> (23)=3.240, p=0.004, Cohen’s <italic>d</italic>=0.661), indicating a super-additive audiovisual integration effect. By contrast, at 1.67Hz, the amplitude in the AV condition was comparable to the unisensory sum (<italic>t</italic> (23)=–0.735, p=0.470, Cohen’s <italic>d</italic>=–0.150), indicating linear audiovisual integration. Significant peaks were also observed at 3.33Hz in all three conditions (ps&lt;0.001, FDR corrected), which showed similar audiovisual integration mode as 1.67Hz (see more details in Appendix and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>In summary, results from Experiments 1a and 1b consistently showed that the cortical tracking of the audiovisual signals at different temporal scales exhibit distinct audiovisual integration modes, i.e., the super-additive effect at gait-cycle frequency and the additive effect at step-cycle frequency, indicating that the cortical tracking effects at the two temporal scales might be driven by functionally dissociable mechanisms.</p></sec></sec><sec id="s2-2"><title>Cortical tracking of higher-order rhythmic structure contributes to the AVI of BM</title><p>To further explore whether and how the cortical tracking of rhythmic structures contributes to the specialized audiovisual processing of BM, we adopted both upright and inverted BM stimuli in Experiment 2. The task and the frequencies of visual stimuli in Experiment 2 were same as Experiment 1a. Specifically, participants were required to perform the change detection task when perceiving upright and inverted visual BM sequences (1Hz for gait-cycle frequency and 2Hz for step-cycle frequency) accompanied by frequency congruent (1Hz) or incongruent (0.6Hz and 1.4Hz) footstep sounds (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). The audiovisual congruency effect, characterized by stronger neural responses in the audiovisual congruent condition compared with the incongruent condition, can be taken as an index of AVI (<xref ref-type="bibr" rid="bib19">Fleming et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Maddox et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Wuerger et al., 2012a</xref>). A stronger congruency effect in the upright condition relative to the inverted condition characterizes an AVI process specific to BM information.</p><p>We contrasted the congruency effect between the upright and inverted conditions to search for clusters showing a significant difference, which equaled identifying an interaction effect, using a cluster-based permutation test over all electrodes (n=1000, alpha = 0.05; see Materials and methods). At 1Hz, the congruency effect in the upright condition was significantly stronger than that in the inverted condition in a cluster at the right hemisphere (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, lower panel, p=0.029; C2, CPz, CP2, CP4, CP6, Pz, P2, P4, P6). Then, we averaged the amplitude of electrodes within the significant cluster and performed two-tailed paired t-tests to examine whether the congruency effect was significant in the upright and the inverted conditions, respectively. Results showed that (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) audiovisual congruency enhanced the oscillatory amplitude only for upright BM (<italic>t</italic> (23)=4.632, p&lt;0.001, Cohen’s <italic>d</italic>=0.945) but not when visual BM was inverted (<italic>t</italic> (23)=0.480, p=0.635, Cohen’s <italic>d</italic>=0.098). Together, these findings suggest that cortical tracking of the high-order gait cycles involves a domain-specific process exclusively engaged in the AVI of BM.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cortical tracking at gait-cycle rather than step-cycle frequency contributes to the biological motion (BM)-specific audiovisual integration (AVI) effect.</title><p>The lower panels in (<bold>a</bold>) and (<bold>d</bold>) depict the topographic maps of the BM-specific AVI effect, measured by the difference of congruency effects between the upright and inverted conditions at 1Hz and 2Hz, respectively. A significantly enhanced congruency effect in the upright condition relative to the inverted condition was observed at 1Hz (marked by black dots) but not at 2Hz (cluster-based permutation test; n = 1000, alpha = 0.05). The amplitude at these significant electrodes was averaged to quantify the congruency effect for the upright and inverted conditions at 1Hz (<bold>b</bold>) and 2Hz (<bold>e</bold>). Error bars represent±1 standard error of means. N = 24. Paired t-test. *: p&lt;0.05; **: p&lt;0.01; ***: p&lt;0.001; n.s<italic>.:</italic> p&gt;0.05. Individuals’ autistic traits correlated with the BM-specific AVI effect at 1Hz (<bold>c</bold>) but not at 2Hz (<bold>f</bold>). Shaded regions indicate the 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98701-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Control analysis at step-cycle frequency.</title><p>(<bold>a</bold>) The amplitude at the electrodes marked by solid black dots was averaged to quantify the cortical tracking effect at 2 Hz under the upright and inverted conditions, respectively. The congruency effect was not significantly different between these conditions at the group level. (<bold>b</bold>) The individual congruency effect (upright versus inverted) was not significantly correlated with the Autism-Spectrum Quotient (AQ) score. N = 24. Paired t-test. *: <italic>p</italic> &lt; .05; **: <italic>p</italic> &lt; .01; ***: <italic>p</italic> &lt; .001; <italic>n.s.</italic>: <italic>p</italic> &gt; .05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98701-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>The biological motion (BM)-specific cortical tracking effect for high and low Autism-Spectrum Quotient (AQ) groups at 1Hz (N = 23) and 2Hz (N = 24).</title><p>Independent sample t-test.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98701-fig3-figsupp2-v1.tif"/></fig></fig-group><p>In contrast, at 2Hz, no cluster showed a significantly different congruency effect between the upright and inverted conditions (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). We then conducted further analysis based on the electrodes yielded by 1Hz as marked in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. Results showed that both upright and inverted stimuli induced a significant congruency effect at 2Hz (<xref ref-type="fig" rid="fig3">Figure 3e</xref>; upright: <italic>t</italic> (23)=3.096, p=0.005, Cohen’s <italic>d</italic>=0.632; inverted: <italic>t</italic> (23)=2.672, p=0.014, Cohen’s <italic>d</italic>=0.545). These findings suggest that neural tracking of the lower-order step cycles is associated with a domain-general AVI process mostly driven by temporal correspondence in physical stimuli.</p></sec><sec id="s2-3"><title>BM-specific cortical tracking correlates with autistic traits</title><p>Furthermore, we examined the link between individuals’ autistic traits and the neural responses underpinning the AVI of BM, measured by the difference of congruency effect between the upright and the inverted BM conditions, using Pearson correlation analysis. After removing one outlier (whose neural response exceeded 3 SD from the group mean), we observed an evident negative correlation between individuals’ AQ scores and their neural responses at 1Hz (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, <italic>r</italic>=–0.493, p=0.017) but not at 2Hz (<xref ref-type="fig" rid="fig3">Figure 3f</xref>, <italic>r</italic>=–0.158, p=0.460). The lack of significant results at 2Hz was not attributable to electrode selection bias based on the significant cluster at 1Hz, as similar results were observed when we performed analyses on clusters showing non-selective significant congruency effects at 2Hz (see the control analysis in Appendix and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Besides, we split the participants based on their median AQ score and found that, compared with the high AQ group, the low AQ group showed a greater BM-specific cortical tracking effect at 1Hz but not at 2Hz. These findings provide further support to the possible linkage between social cognition and cortical tracking of BM as well as its dissociation at the two temporal scales (see more details in Appendix and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The current study investigated the neural implementation for the AVI of human BM information. We found that, even under a motion-irrelevant color detection task, observers’ neural activity tracked the temporally corresponding audiovisual BM signals at the frequencies of two rhythmic structures, i.e., the higher-order structure of gait cycle at a larger integration window and the basic-level structure of step cycle at a smaller integration window. The strength of these cortical tracking effects was enhanced under the audiovisual condition than in the visual-only or auditory-only condition, indicating multisensory gains. More crucially, although the cortical tracking of both gait-cycle and step-cycle gain benefits from multisensory correspondence, the mechanisms underlying these two processes appear to be different. At step-cycle frequency, the cortical tracking effect in the AV condition equaled the additive sum of the unisensory conditions. Such linear integration may result from concurrent, independent processing of unisensory inputs without additional interaction of them (<xref ref-type="bibr" rid="bib49">Stein et al., 2009</xref>). In contrast, at gait-cycle frequency, the congruent audiovisual signals led to a super-additive multisensory enhancement over the linear combination of auditory and visual conditions (AV&gt;A +V), despite that there was no evident cortical tracking effect in the visual condition, different from previous findings obtained with a motion-relevant change detection task (<xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>). This super-additive multisensory enhancement may bring about decreased thresholds of detection and identification (<xref ref-type="bibr" rid="bib48">Stanford et al., 2005</xref>), allowing people to achieve a more clear and stable perception of the external environment and detect weak stimulus changes in time and respond adaptively.</p><p>Furthermore, results from Experiment 2 demonstrated that the cortical tracking of rhythmic structure corresponding to the gait cycle rather than the step cycle underlies the specialized processing of audiovisual BM information. In particular, the AVI effect at step-cycle frequency was significant for both upright and inverted BM signals and comparable between the two conditions, while the AVI effect at gait-cycle frequency was only significant in the upright condition and was greater than that in the inverted condition. The inversion effect has long been regarded as a marker of the specificity of BM processing in numerous behavioral and neuroimaging studies (<xref ref-type="bibr" rid="bib20">Grossman and Blake, 2001</xref>; <xref ref-type="bibr" rid="bib31">Ma et al., 2022</xref>; <xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>; <xref ref-type="bibr" rid="bib47">Simion et al., 2008</xref>; <xref ref-type="bibr" rid="bib53">Troje and Westhoff, 2006</xref>; <xref ref-type="bibr" rid="bib54">Vallortigara and Regolin, 2006</xref>; <xref ref-type="bibr" rid="bib58">Wang et al., 2014</xref>; <xref ref-type="bibr" rid="bib57">Wang and Jiang, 2012</xref>; <xref ref-type="bibr" rid="bib59">Wang et al., 2022</xref>). Our current findings of the inversion effect in the cortical tracking of audiovisual BM at the gait-cycle frequency suggest that the neural encoding of the higher-order rhythmic structure reflects the AVI of BM and contributes to the specialized processing of BM information. In contrast, the cortical tracking of the step cycle may reflect the integration of basic motion signals and corresponding sounds. Together, these results reveal that the neural tracking of rhythmic structures at different temporal scales plays distinct roles in the AVI of BM, which may result from the interplay of stimulus-driven and domain-specific mechanisms.</p><p>Besides the temporal dynamics of neural activity revealed by the cortical tracking process, we found that the BM-specific AVI effect was associated with neural activity in the right temporoparietal electrodes. This finding likely relates to the activation of the right posterior superior temporal sulcus (pSTS), a region responding to both auditory and visual BM information and being causally involved in BM perception (<xref ref-type="bibr" rid="bib7">Bidet-Caulet et al., 2005</xref>; <xref ref-type="bibr" rid="bib21">Grossman et al., 2005</xref>; <xref ref-type="bibr" rid="bib59">Wang et al., 2022</xref>). While previous fMRI studies have observed STS activation when processing spatial or semantic correspondence between audiovisual BM (<xref ref-type="bibr" rid="bib36">Meyer et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Wuerger et al., 2012b</xref>), whether this region also engages in the audiovisual processing of BM signals based on temporal correspondence remains unknown. The current study provides preliminary evidence for such a possibility, inviting future research to localize the exact source of the multisensory integration processes based on imaging data with high spatial and temporal resolutions, such as MEG.</p><p>Cortical tracking of external rhythms is also described as cortical entrainment in a broad sense (<xref ref-type="bibr" rid="bib13">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Obleser and Kayser, 2019</xref>). Controversy remains regarding the involvements of endogenous neural oscillations and stimulus-evoked responses in these processes (<xref ref-type="bibr" rid="bib15">Duecker et al., 2024</xref>), as it is challenging to fully dissociate these components due to their intricate interplay (<xref ref-type="bibr" rid="bib22">Herrmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Hosseinian et al., 2021</xref>). Despite the complexity of the neuronal mechanisms, previous research suggests that cortical tracking or entrainment plays a role in the multisensory processing of simple or discrete rhythmic signals (<xref ref-type="bibr" rid="bib11">Covic et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Keitel and Müller, 2016</xref>; <xref ref-type="bibr" rid="bib38">Nozaradan et al., 2012b</xref>). These findings may partially explain the non-selective AVI effect at the step cycle in the current study. However, we found that the cortical tracking of the higher-order rhythmic structure formed by spatiotemporal integration of meaningful BM information (i.e. the gait cycle of upright walkers rather than inverted walkers) is selectively engaged in the AVI of BM, suggesting that the multisensory processing of natural continuous stimuli may involve unique mechanisms besides the purely stimulus-driven AVI process. These findings advance our understanding of the recently proposed view that multi-timescale neural processes coordinate multisensory integration (<xref ref-type="bibr" rid="bib44">Senkowski and Engel, 2024</xref>), especially from the perspective of natural stimuli processing. Similar to BM, other natural rhythmic stimuli, like speech and music, also convey hierarchical structures that can entrain neural oscillations at different temporal scales, both in unisensory (<xref ref-type="bibr" rid="bib13">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Doelling and Poeppel, 2015</xref>) and multisensory contexts (<xref ref-type="bibr" rid="bib6">Biau et al., 2022</xref>; <xref ref-type="bibr" rid="bib12">Crosse et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Nozaradan et al., 2016</xref>). Possibly, the audiovisual processing of these stimuli engages multi-scale neural coding mechanisms that play distinct functions in perception. Investigating this issue and comparing the results with BM studies will help complete the picture of how the human brain integrates complex, rhythmic information sampled from different sensory modalities to orchestrate perception in natural scenarios.</p><p>Since the scope of the current study mainly focused on the neural processing of BM, the task we employed was unrelated to audiovisual correspondence and not for establishing a direct link between neural responses and behavior. This is a limitation of the current study. A recent study demonstrated that listening to frequency-congruent footstep sounds, compared with incongruent sounds, enhanced the visual search for human walkers but not for non-BM stimuli containing the same rhythmic signals, indicating that audiovisual correspondence specifically enhances the perceptual and attentional processing of BM (<xref ref-type="bibr" rid="bib45">Shen et al., 2023a</xref>). Future research could examine whether the cortical tracking of rhythmic structures plays a functional role in such behaviorally relevant tasks. They could also apply advanced neuromodulation techniques to elucidate the causal relevance of the cortical tracking effect to BM perception (e.g. <xref ref-type="bibr" rid="bib29">Kösem et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Kösem et al., 2018</xref>).</p><p>Last but not least, our study demonstrated that the selective cortical tracking of higher-level rhythmic structure in audiovisually congruent BM signals negatively correlated with individual autistic traits. This finding highlights the critical role of the neural tracking of audiovisual BM signals in social cognition. It also offers the first evidence that differences in audiovisual BM processing are already present in nonclinical individuals at the neural level and associated with their autistic traits, extending previous behavioral evidence for atypical audiovisual BM processing in ASD populations (<xref ref-type="bibr" rid="bib16">Falck-Ytter et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Klin et al., 2009</xref>). Meanwhile, given that impaired audiovisual BM processing at the early stage may influence social development and result in cascading consequences for lifetime impairments in social interaction (<xref ref-type="bibr" rid="bib17">Falck-Ytter et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Klin et al., 2005</xref>), it is worth exploring neural tracking of audiovisual BM signals in children, which may pave the way for utilizing it as a potential early neural marker for ASD.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Seventy-two participants (mean age± SD = 22.4±2.6years, 35females) took part in the study, 24 for each of Experiment 1a, Experiment 1b, and Experiment 2. All of them had normal or corrected-to-normal vision and reported no history of neurological, psychiatric, or hearing disorders. They were naïve to the purpose of the study and gave informed consent according to procedures and protocols approved by the institutional review board of the Institute of Psychology, Chinese Academy of Sciences (reference number for approval: H21041).</p></sec><sec id="s4-2"><title>Stimuli</title><sec id="s4-2-1"><title>Visual stimuli</title><p>The visual stimuli (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, left panel) consisted of 13 point-light dots attached to the head and major joints of a human walker (<xref ref-type="bibr" rid="bib56">Vanrie and Verfaillie, 2004</xref>). The point-light walker was presented at the center of the screen without translational motion. It conveys rhythmic structures specified by recurrent forward motions of bilateral limbs (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, right panel). Each step, regardless of left or right foot, occurs recurrently to form a step cycle. The antiphase oscillations of limbs during two steps characterize a gait cycle (<xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>). In Experiment 1a, a full gait cycle took 1s and was repeated six times to form a 6s walking sequence. That is, the gait-cycle frequency is 1Hz and the step-cycle frequency is 2Hz. In Experiment 1b, the gait-cycle frequency was 0.83Hz and the step-cycle frequency was 1.67Hz. The gait cycle was repeated six times to form a 7.2s walking sequence. The stimuli in Experiment 2 were the same as that in Experiment 1a. Meanwhile, the point-light BM was flipped vertically to generate inverted BM (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, left panel), which preserves the temporal structure of the stimuli but distorts its distinctive kinematic features, such as movement that is compatible with the effect of gravity (<xref ref-type="bibr" rid="bib46">Shen et al., 2023b</xref>; <xref ref-type="bibr" rid="bib53">Troje and Westhoff, 2006</xref>; <xref ref-type="bibr" rid="bib59">Wang et al., 2022</xref>).</p></sec><sec id="s4-2-2"><title>Auditory stimuli</title><p>Auditory stimuli were continuous footstep sounds (6s) with a sampling rate of 44,100Hz. As shown in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, in Experiments 1a and 2, the gait-cycle frequency of congruent sounds was 1Hz, which consisted of two steps or two impulses generated by each foot striking the ground within one gait cycle. The incongruent sounds included a faster (1.4Hz) and a slower (0.60Hz) sound. Both congruent and incongruent sounds were generated by manipulating the temporal interval between two successive impulses based on the same auditory stimuli. In Experiment 1b, the gait-cycle frequency of sound was 0.83Hz.</p></sec><sec id="s4-2-3"><title>Stimuli presentation</title><p>The visual stimuli were rendered white against a gray background and displayed on a CRT (cathode ray tube) monitor. Participants sat 60cm from the computer screen (1280×1024 at 60Hz; high: 37.5cm; width: 30cm), with their heads held stationary on a chinrest. The auditory stimuli were presented binaurally over insert earphones. All stimuli were generated and presented using MATLAB together with the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib9">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib42">Pelli, 1997</xref>).</p></sec></sec><sec id="s4-3"><title>Procedure and task</title><sec id="s4-3-1"><title>Experiment 1a</title><p>The experiment was conducted in an acoustically dampened and electromagnetically shielded chamber. Participants completed the task under three conditions (visual: V; auditory: A; audiovisual: AV) with the same procedure (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) except for the stimuli. In the V condition, each trial began with a white fixation cross (0.42°×0.42°) displayed at the center of a gray background for a random duration (0.8–1 s). Subsequently, a 6s point-light walker (3.05°×5.47°) walked toward the left or right at a constant walking cycle frequency (1Hz). To maintain observers’ attention, 17–23% of the trials were randomly selected as catch trials, in which the color of the walker changed (the RGB values changed from [255 255 255] to [207 207 207]) one or two times throughout the trial. Each change lasted 0.5s. Observers were required to report the number of changes (0, 1, or 2) via keypresses as accurately as possible after the point-light display was replaced by a red fixation. The next trial started 2–3s after the response. In the A condition, the 6 s stimuli were replaced by a visually static BM figure accompanied by continuous footstep sounds. The frequency of footstep sounds was congruent with the frequency of visual BM in the V condition. In the AV condition, the stimuli were temporally congruent visual BM sequences (as in the V condition) and footstep sounds (as in the A condition). Three conditions were conducted in separate blocks. V condition was performed in the middle of A and AV conditions. The order of A and AV conditions was counterbalanced across participants. Each participant completed 40 experimental trials without changes and 10–15 catch trials in each condition, resulting in a total of 150–165 trials. In each condition, participants completed a practice session with three trials to get familiar with the task before the formal EEG experiment.</p></sec><sec id="s4-3-2"><title>Experiment 1b</title><p>The procedure of Experiment 1b was the same as that for Experiment 1a but with two exceptions. First, to test if the cortical tracking effect can apply to stimuli with a different speed, we altered the frequencies of gait and step cycles to 0.83Hz and 1.67Hz. Second, we presented the three conditions (V, A, and AV) in a completely random order to eliminate the influence of presentation order. To minimize the potential influence of condition switch, we increased the trial number in the practice session from 3 to 14 for each condition.</p></sec><sec id="s4-3-3"><title>Experiment 2</title><p>The procedure in Experiment 2 was similar to the AV condition in Experiment 1a, except that the visually displayed BM was accompanied by frequency congruent (1Hz) or incongruent (0.6 Hz or 1.4Hz) footstep sounds. Each participant completed a total of 76 experiment trials, consisting of 36 congruent-trials, 20 incongruent-trials with faster sounds (1.4Hz), and 20 incongruent-trials with slower sounds (0.6Hz). These trials were assigned to three blocks based on the frequency of the footstep sounds, with the order of the three frequencies balanced across participants. Besides, an inverted BM was used as a control to investigate whether there is a specialized mechanism tuned to the AVI of life motion signals. The order of upright and inverted conditions was balanced across participants. Meanwhile, we measured the participants’ autistic traits by using the Autism-Spectrum Quotient, or AQ questionnaire (<xref ref-type="bibr" rid="bib2">Baron-Cohen et al., 2001</xref>). Higher AQ scores indicate a higher level of autistic traits.</p></sec></sec><sec id="s4-4"><title>EEG recording and analysis</title><p>EEG was recorded at 1000Hz using a SynAmps<sup>2</sup> NeuroScan amplifier System with 64 electrodes placed on the scalp according to the international 10–20 system. Horizontal and vertical eye movements were measured via four additional electrodes placed on the outer canthus of each eye and the inferior and superior areas of the left orbit. Impedances were kept below 5kΩ for all electrodes.</p><sec id="s4-4-1"><title>Preprocessing</title><p>The catch trials were excluded from EEG analysis. All preprocessing and further analyses were performed using the FieldTrip toolbox (<xref ref-type="bibr" rid="bib41">Oostenveld et al., 2011</xref>; <ext-link ext-link-type="uri" xlink:href="http://fieldtriptoolbox.org">http://fieldtriptoolbox.org</ext-link>) in the MATLAB environment. EEG recordings were pass-filtered between 0.1 Hz and 30Hz, and down-sampled to 100Hz. Then the continuous EEG data were cut into epochs ranging from –1s to 6 gait cycles (7.2s in Experiment 1b and 6s in other experiments) time-locked to the onset of the visual point-light stimuli. The epochs were visually inspected, and trials contaminated with excessive noise were excluded from the analysis. After the trial rejection, eye and cardiac artifacts were removed via independent component analysis based on the Runica algorithm (<xref ref-type="bibr" rid="bib4">Bell and Sejnowski, 1995</xref>; <xref ref-type="bibr" rid="bib24">Jung et al., 2000</xref>; <xref ref-type="bibr" rid="bib33">Makeig, 2002</xref>). Then the cleaned data were re-referenced to the average mastoids (M1 and M2). To minimize the influence of stimulus-onset evoked activity on EEG spectral decomposition, the EEG recording before the onset of the stimulus and the first cycle (1s in Experiments 1a and 2; 1.2s in Experiment 1b) of each trial was excluded (<xref ref-type="bibr" rid="bib37">Nozaradan et al., 2012a</xref>). After that, the EEG epochs were averaged across trials for each participant and condition.</p></sec><sec id="s4-4-2"><title>Frequency-domain analysis and statistics</title><p>A fast Fourier transform (FFT) with zero padding (1200) was used to convert the averaged EEG signals from the temporal domain to the spectral domain, resulting in a frequency resolution of 0.083Hz, i.e., 1/12Hz, which is sufficient for observing neural responses around the frequency of the rhythmic BM structures in all experiments. When performing FFT, a Hanning window was adopted to minimize spectral leakage. Then, to remove the 1/f trend of the response amplitude spectrum and identify spectral peaks, the response amplitude at each frequency was normalized by subtracting the average amplitude measured at the neighboring frequency bins (two bins on each side) (<xref ref-type="bibr" rid="bib37">Nozaradan et al., 2012a</xref>). We calculated the normalized amplitude separately for each electrode (except for electrooculogram electrodes, CB1, and CB2), participant, and condition.</p><p>In Experiment 1, the normalized amplitude in all electrodes was averaged and a right-tailed one-sample t-test against zero was performed on the grand average amplitude to test whether the neural response in each frequency bin showed a significant tracking effect or spectral peak. This test was applied to all frequency bins below 5.33Hz and multiple comparisons were controlled by false discovery rate (FDR) correction at p&lt;0.05 (<xref ref-type="bibr" rid="bib5">Benjamini and Hochberg, 1995</xref>). In Experiment 2, to further identify the BM-specific AVI process, the audiovisual congruency effect was compared between the upright and inverted conditions using a cluster-based permutation test over all electrodes (1000 iterations, requiring a cluster size of at least two significant neighbors, a two-sided t-test at p&lt;0.05 on the clustered data) (<xref ref-type="bibr" rid="bib41">Oostenveld et al., 2011</xref>; <ext-link ext-link-type="uri" xlink:href="http://fieldtriptoolbox.org">http://fieldtriptoolbox.org</ext-link>). This allowed us to identify the spatial distribution of the BM-specific congruency effect.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Writing – review and editing, Funding acquisition</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Helsinki Declaration of 1975, as revised in 2008. Written informed consent, and consent to publish, was obtained from participants. The institutional review board of the Institute of Psychology, Chinese Academy of Sciences has approved this study (reference number for approval: H21041).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98701-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and code accompanying this study are made available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.57760/sciencedb.psych.00144">https://doi.org/10.57760/sciencedb.psych.00144</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Data from: Cortical tracking of hierarchical rhythms orchestrates the multisensory processing of biological motion</data-title><source>Science Data Bank</source><pub-id pub-id-type="doi">10.57760/sciencedb.psych.00144</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arrighi</surname><given-names>R</given-names></name><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perceptual synchrony of audiovisual streams for natural and artificial motion sequences</article-title><source>Journal of Vision</source><volume>6</volume><fpage>260</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1167/6.3.6</pub-id><pub-id pub-id-type="pmid">16643094</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname><given-names>S</given-names></name><name><surname>Wheelwright</surname><given-names>S</given-names></name><name><surname>Skinner</surname><given-names>R</given-names></name><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Clubley</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The autism-spectrum quotient (AQ): evidence from Asperger syndrome/high-functioning autism, males and females, scientists and mathematicians</article-title><source>Journal of Autism and Developmental Disorders</source><volume>31</volume><fpage>5</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1023/a:1005653411471</pub-id><pub-id pub-id-type="pmid">11439754</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>AKR</given-names></name><name><surname>Debener</surname><given-names>S</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Synchronisation of neural oscillations and cross-modal influences</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>481</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.03.003</pub-id><pub-id pub-id-type="pmid">32317142</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>An information-maximization approach to blind separation and blind deconvolution</article-title><source>Neural Computation</source><volume>7</volume><fpage>1129</fpage><lpage>1159</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id><pub-id pub-id-type="pmid">7584893</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biau</surname><given-names>E</given-names></name><name><surname>Schultz</surname><given-names>BG</given-names></name><name><surname>Gunter</surname><given-names>TC</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Left Motor δ oscillations reflect asynchrony detection in multisensory speech Perception</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>2313</fpage><lpage>2326</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2965-20.2022</pub-id><pub-id pub-id-type="pmid">35086905</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidet-Caulet</surname><given-names>A</given-names></name><name><surname>Voisin</surname><given-names>J</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Fonlupt</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Listening to a walking human activates the temporal biological motion area</article-title><source>NeuroImage</source><volume>28</volume><fpage>132</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.06.018</pub-id><pub-id pub-id-type="pmid">16027008</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>R</given-names></name><name><surname>Shiffrar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Perception of human motion</article-title><source>Annual Review of Psychology</source><volume>58</volume><fpage>47</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.57.102904.190152</pub-id><pub-id pub-id-type="pmid">16903802</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brooks</surname><given-names>A</given-names></name><name><surname>van der Zwan</surname><given-names>R</given-names></name><name><surname>Billard</surname><given-names>A</given-names></name><name><surname>Petreska</surname><given-names>B</given-names></name><name><surname>Clarke</surname><given-names>S</given-names></name><name><surname>Blanke</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Auditory motion affects visual biological motion processing</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>523</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.12.012</pub-id><pub-id pub-id-type="pmid">16504220</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Covic</surname><given-names>A</given-names></name><name><surname>Keitel</surname><given-names>C</given-names></name><name><surname>Porcu</surname><given-names>E</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name><name><surname>Müller</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Audio-visual synchrony and spatial attention enhance processing of dynamic visual stimulation independently and in parallel: A frequency-tagging study</article-title><source>NeuroImage</source><volume>161</volume><fpage>32</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.022</pub-id><pub-id pub-id-type="pmid">28802870</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Butler</surname><given-names>JS</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Congruent visual speech enhances cortical entrainment to continuous auditory speech in noise-free conditions</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>14195</fpage><lpage>14204</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1829-15.2015</pub-id><pub-id pub-id-type="pmid">26490860</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id><pub-id pub-id-type="pmid">26642090</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical entrainment to music and its modulation by expertise</article-title><source>PNAS</source><volume>112</volume><fpage>E6233</fpage><lpage>E42</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508431112</pub-id><pub-id pub-id-type="pmid">26504238</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duecker</surname><given-names>K</given-names></name><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Breska</surname><given-names>A</given-names></name><name><surname>Coffey</surname><given-names>EBJ</given-names></name><name><surname>Sivarao</surname><given-names>DV</given-names></name><name><surname>Zoefel</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Challenges and approaches in the study of neural entrainment</article-title><source>The Journal of Neuroscience</source><volume>44</volume><elocation-id>e1234242024</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1234-24.2024</pub-id><pub-id pub-id-type="pmid">39358026</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falck-Ytter</surname><given-names>T</given-names></name><name><surname>Rehnberg</surname><given-names>E</given-names></name><name><surname>Bölte</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Lack of visual orienting to biological motion and audiovisual synchrony in 3-year-olds with autism</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e68816</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0068816</pub-id><pub-id pub-id-type="pmid">23861945</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falck-Ytter</surname><given-names>T</given-names></name><name><surname>Nyström</surname><given-names>P</given-names></name><name><surname>Gredebäck</surname><given-names>G</given-names></name><name><surname>Gliga</surname><given-names>T</given-names></name><name><surname>Bölte</surname><given-names>S</given-names></name><collab>EASE team</collab></person-group><year iso-8601-date="2018">2018</year><article-title>Reduced orienting to audiovisual synchrony in infancy predicts autism diagnosis at 3 years of age</article-title><source>Journal of Child Psychology and Psychiatry, and Allied Disciplines</source><volume>59</volume><fpage>872</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1111/jcpp.12863</pub-id><pub-id pub-id-type="pmid">29359802</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>JI</given-names></name><name><surname>Dunham</surname><given-names>K</given-names></name><name><surname>Cassidy</surname><given-names>M</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Woynaroski</surname><given-names>TG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Audiovisual multisensory integration in individuals with autism spectrum disorder: A systematic review and meta-analysis</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>95</volume><fpage>220</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.09.020</pub-id><pub-id pub-id-type="pmid">30287245</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>JT</given-names></name><name><surname>Noyce</surname><given-names>AL</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Audio-visual spatial alignment improves integration in the presence of a competing audio-visual stimulus</article-title><source>Neuropsychologia</source><volume>146</volume><elocation-id>107530</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107530</pub-id><pub-id pub-id-type="pmid">32574616</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>ED</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Brain activity evoked by inverted and imagined biological motion</article-title><source>Vision Research</source><volume>41</volume><fpage>1475</fpage><lpage>1482</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(00)00317-5</pub-id><pub-id pub-id-type="pmid">11322987</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>ED</given-names></name><name><surname>Battelli</surname><given-names>L</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Repetitive TMS over posterior STS disrupts perception of biological motion</article-title><source>Vision Research</source><volume>45</volume><fpage>2847</fpage><lpage>2853</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.05.027</pub-id><pub-id pub-id-type="pmid">16039692</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>CS</given-names></name><name><surname>Murray</surname><given-names>MM</given-names></name><name><surname>Ionta</surname><given-names>S</given-names></name><name><surname>Hutt</surname><given-names>A</given-names></name><name><surname>Lefebvre</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shaping intrinsic neural oscillations with periodic stimulation</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>5328</fpage><lpage>5337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0236-16.2016</pub-id><pub-id pub-id-type="pmid">27170129</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosseinian</surname><given-names>T</given-names></name><name><surname>Yavari</surname><given-names>F</given-names></name><name><surname>Biagi</surname><given-names>MC</given-names></name><name><surname>Kuo</surname><given-names>MF</given-names></name><name><surname>Ruffini</surname><given-names>G</given-names></name><name><surname>Nitsche</surname><given-names>MA</given-names></name><name><surname>Jamil</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>External induction and stabilization of brain oscillations in the human</article-title><source>Brain Stimulation</source><volume>14</volume><fpage>579</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1016/j.brs.2021.03.011</pub-id><pub-id pub-id-type="pmid">33781955</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>TP</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name><name><surname>Westerfield</surname><given-names>M</given-names></name><name><surname>Townsend</surname><given-names>J</given-names></name><name><surname>Courchesne</surname><given-names>E</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects</article-title><source>Clinical Neurophysiology</source><volume>111</volume><fpage>1745</fpage><lpage>1758</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(00)00386-2</pub-id><pub-id pub-id-type="pmid">11018488</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>C</given-names></name><name><surname>Müller</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Audio-visual synchrony and feature-selective attention co-amplify early visual processing</article-title><source>Experimental Brain Research</source><volume>234</volume><fpage>1221</fpage><lpage>1231</lpage><pub-id pub-id-type="doi">10.1007/s00221-015-4392-8</pub-id><pub-id pub-id-type="pmid">26226930</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Klin</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>W</given-names></name><name><surname>Schultz</surname><given-names>RT</given-names></name><name><surname>Volkmar</surname><given-names>FR</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>The enactive mind-from actions to cognition: lessons from autism</chapter-title><person-group person-group-type="editor"><name><surname>Klin</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>W</given-names></name><name><surname>Schultz</surname><given-names>RT</given-names></name><name><surname>Volkmar</surname><given-names>FR</given-names></name></person-group><source>Handbook of Autism and Pervasive Developmental Disorders: Diagnosis, Development, Neurobiology, and Behavior</source><edition>3rd ed</edition><publisher-name>John Wiley &amp; Sons Inc</publisher-name><fpage>682</fpage><lpage>703</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klin</surname><given-names>A</given-names></name><name><surname>Lin</surname><given-names>DJ</given-names></name><name><surname>Gorrindo</surname><given-names>P</given-names></name><name><surname>Ramsay</surname><given-names>G</given-names></name><name><surname>Jones</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Two-year-olds with autism orient to non-social contingencies rather than biological motion</article-title><source>Nature</source><volume>459</volume><fpage>257</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1038/nature07868</pub-id><pub-id pub-id-type="pmid">19329996</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname><given-names>A</given-names></name><name><surname>Bosker</surname><given-names>HR</given-names></name><name><surname>Takashima</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural entrainment determines the words we hear</article-title><source>Current Biology</source><volume>28</volume><fpage>2867</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.023</pub-id><pub-id pub-id-type="pmid">30197083</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname><given-names>A</given-names></name><name><surname>Bosker</surname><given-names>HR</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Riecke</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Biasing the perception of spoken words with transcranial alternating current stimulation</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>1428</fpage><lpage>1437</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01579</pub-id><pub-id pub-id-type="pmid">32427072</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Perrault</surname><given-names>TJ</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>On the use of superadditivity as a metric for characterizing multisensory integration in functional neuroimaging studies</article-title><source>Experimental Brain Research</source><volume>166</volume><fpage>289</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2370-2</pub-id><pub-id pub-id-type="pmid">15988597</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Yuan</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Gravity-dependent animacy perception in zebrafish</article-title><source>Research</source><volume>2022</volume><elocation-id>9829016</elocation-id><pub-id pub-id-type="doi">10.34133/2022/9829016</pub-id><pub-id pub-id-type="pmid">36128180</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>RK</given-names></name><name><surname>Atilgan</surname><given-names>H</given-names></name><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Lee</surname><given-names>AKC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners</article-title><source>eLife</source><volume>4</volume><elocation-id>e04995</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04995</pub-id><pub-id pub-id-type="pmid">25654748</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Response: event-related brain dynamics -- unifying brain electrophysiology</article-title><source>Trends in Neurosciences</source><volume>25</volume><elocation-id>390</elocation-id><pub-id pub-id-type="doi">10.1016/s0166-2236(02)02198-7</pub-id><pub-id pub-id-type="pmid">12127749</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendonça</surname><given-names>C</given-names></name><name><surname>Santos</surname><given-names>JA</given-names></name><name><surname>López-Moliner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The benefit of multisensory integration with biological motion signals</article-title><source>Experimental Brain Research</source><volume>213</volume><fpage>185</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1007/s00221-011-2620-4</pub-id><pub-id pub-id-type="pmid">21424256</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzger</surname><given-names>BA</given-names></name><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Nesbitt</surname><given-names>E</given-names></name><name><surname>Karas</surname><given-names>PJ</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Responses to visual speech in human posterior superior temporal gyrus examined with iEEG deconvolution</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>6938</fpage><lpage>6948</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0279-20.2020</pub-id><pub-id pub-id-type="pmid">32727820</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>GF</given-names></name><name><surname>Greenlee</surname><given-names>M</given-names></name><name><surname>Wuerger</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Interactions between auditory and visual semantic stimulus classes: evidence for common processing networks for speech and body actions</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>2291</fpage><lpage>2308</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21593</pub-id><pub-id pub-id-type="pmid">20954938</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Selective neuronal entrainment to the beat and meter embedded in a musical rhythm</article-title><source>The Journal of Neuroscience</source><volume>32</volume><elocation-id>e122012</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3203-12.2012</pub-id><pub-id pub-id-type="pmid">23223281</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Steady-state evoked potentials as an index of multisensory temporal binding</article-title><source>NeuroImage</source><volume>60</volume><fpage>21</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.11.065</pub-id><pub-id pub-id-type="pmid">22155324</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Schönwiesner</surname><given-names>M</given-names></name><name><surname>Caron-Desrochers</surname><given-names>L</given-names></name><name><surname>Lehmann</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Enhanced brainstem and cortical encoding of sound during synchronized movement</article-title><source>NeuroImage</source><volume>142</volume><fpage>231</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.015</pub-id><pub-id pub-id-type="pmid">27397623</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural entrainment and attentional selection in the listening brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>913</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.08.004</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897x00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saygin</surname><given-names>AP</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name><name><surname>de Sa</surname><given-names>VR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>In the footsteps of biological motion and multisensory perception: judgments of audiovisual temporal relations are enhanced for upright walkers</article-title><source>Psychological Science</source><volume>19</volume><fpage>469</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02111.x</pub-id><pub-id pub-id-type="pmid">18466408</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Multi-timescale neural dynamics for multisensory integration</article-title><source>Nature Reviews. Neuroscience</source><volume>25</volume><fpage>625</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1038/s41583-024-00845-7</pub-id><pub-id pub-id-type="pmid">39090214</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>L</given-names></name><name><surname>Lu</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023a</year><article-title>Audiovisual correspondence facilitates the visual search for biological motion</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>30</volume><fpage>2272</fpage><lpage>2281</lpage><pub-id pub-id-type="doi">10.3758/s13423-023-02308-z</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>L</given-names></name><name><surname>Lu</surname><given-names>X</given-names></name><name><surname>Yuan</surname><given-names>X</given-names></name><name><surname>Hu</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023b</year><article-title>Cortical encoding of rhythmic kinematic structures in biological motion</article-title><source>NeuroImage</source><volume>268</volume><elocation-id>119893</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.119893</pub-id><pub-id pub-id-type="pmid">36693597</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simion</surname><given-names>F</given-names></name><name><surname>Regolin</surname><given-names>L</given-names></name><name><surname>Bulf</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A predisposition for biological motion in the newborn baby</article-title><source>PNAS</source><volume>105</volume><fpage>809</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1073/pnas.0707021105</pub-id><pub-id pub-id-type="pmid">18174333</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Quessy</surname><given-names>S</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Evaluating the operations underlying multisensory integration in the cat superior colliculus</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>6499</fpage><lpage>6508</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5095-04.2005</pub-id><pub-id pub-id-type="pmid">16014711</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Ramachandran</surname><given-names>R</given-names></name><name><surname>Perrault</surname><given-names>TJ</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Challenges in quantifying multisensory integration: alternative criteria, models, and inverse effectiveness</article-title><source>Experimental Brain Research</source><volume>198</volume><fpage>113</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1880-8</pub-id><pub-id pub-id-type="pmid">19551377</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>RA</given-names></name><name><surname>Ghose</surname><given-names>D</given-names></name><name><surname>Fister</surname><given-names>JK</given-names></name><name><surname>Sarko</surname><given-names>DK</given-names></name><name><surname>Altieri</surname><given-names>NA</given-names></name><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Kurela</surname><given-names>LR</given-names></name><name><surname>Siemann</surname><given-names>JK</given-names></name><name><surname>James</surname><given-names>TW</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Identifying and quantifying multisensory integration: A tutorial review</article-title><source>Brain Topography</source><volume>27</volume><fpage>707</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1007/s10548-014-0365-7</pub-id><pub-id pub-id-type="pmid">24722880</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>JP</given-names></name><name><surname>Shiffrar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>I can see you better if I can hear you coming: action-consistent sounds facilitate the visual detection of human gait</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/10.12.14</pub-id><pub-id pub-id-type="pmid">21047746</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>JP</given-names></name><name><surname>Shiffrar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Meaningful sounds enhance visual sensitivity to human gait regardless of synchrony</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>13148</elocation-id><pub-id pub-id-type="doi">10.1167/13.14.8</pub-id><pub-id pub-id-type="pmid">24317426</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troje</surname><given-names>NF</given-names></name><name><surname>Westhoff</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The inversion effect in biological motion perception: evidence for a “life detector”?</article-title><source>Current Biology</source><volume>16</volume><fpage>821</fpage><lpage>824</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.03.022</pub-id><pub-id pub-id-type="pmid">16631591</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallortigara</surname><given-names>G</given-names></name><name><surname>Regolin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Gravity bias in the interpretation of biological motion by inexperienced chicks</article-title><source>Current Biology</source><volume>16</volume><fpage>R279</fpage><lpage>R80</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.03.052</pub-id><pub-id pub-id-type="pmid">16631570</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Zwan</surname><given-names>R</given-names></name><name><surname>Machatch</surname><given-names>C</given-names></name><name><surname>Kozlowski</surname><given-names>D</given-names></name><name><surname>Troje</surname><given-names>NF</given-names></name><name><surname>Blanke</surname><given-names>O</given-names></name><name><surname>Brooks</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gender bending: auditory cues affect visual judgements of gender in biological motion displays</article-title><source>Experimental Brain Research</source><volume>198</volume><fpage>373</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1800-y</pub-id><pub-id pub-id-type="pmid">19396433</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanrie</surname><given-names>J</given-names></name><name><surname>Verfaillie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Perception of biological motion: A stimulus set of human point-light actions</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>36</volume><fpage>625</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.3758/BF03206542</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Life motion signals lengthen perceived temporal duration</article-title><source>PNAS</source><volume>109</volume><fpage>E673</fpage><lpage>E677</lpage><pub-id pub-id-type="doi">10.1073/pnas.1115515109</pub-id><pub-id pub-id-type="pmid">22215595</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The feet have it: local biological motion cues trigger reflexive attentional orienting in the brain</article-title><source>NeuroImage</source><volume>84</volume><fpage>217</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.041</pub-id><pub-id pub-id-type="pmid">23994124</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><name><surname>Xu</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Zhou</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Modulation of biological motion perception in humans by gravity</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>e2765</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-30347-y</pub-id><pub-id pub-id-type="pmid">35589705</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>TM</given-names></name><name><surname>Pelphrey</surname><given-names>KA</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>McKeown</surname><given-names>MJ</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Polysensory interactions along lateral temporal regions evoked by audiovisual speech</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1034</fpage><lpage>1043</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.10.1034</pub-id><pub-id pub-id-type="pmid">12967920</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wuerger</surname><given-names>SM</given-names></name><name><surname>Crocker-Buque</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>GF</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Evidence for auditory-visual processing specific to biological motion</article-title><source>Seeing and Perceiving</source><volume>25</volume><fpage>15</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1163/187847611X620892</pub-id><pub-id pub-id-type="pmid">22353566</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wuerger</surname><given-names>SM</given-names></name><name><surname>Parkes</surname><given-names>L</given-names></name><name><surname>Lewis</surname><given-names>PA</given-names></name><name><surname>Crocker-Buque</surname><given-names>A</given-names></name><name><surname>Rutschmann</surname><given-names>R</given-names></name><name><surname>Meyer</surname><given-names>GF</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Premotor cortex is sensitive to auditory-visual congruence for biological motion</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>575</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00173</pub-id><pub-id pub-id-type="pmid">22126670</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Results on other peaks in Experiment 1</title><p>As shown in <xref ref-type="fig" rid="fig2">Figure 2a and d</xref>, the audiovisual BM signals induced significant amplitude peaks at 1f (1/0.83Hz), 2f (2/1.67Hz), and 4f (4/3.33Hz) relative to the gait cycle frequency (ps&lt;0.001; FDR corrected). To further test the roles of the neural activity at different frequencies, we analyzed the AVI modes at each frequency, by comparing the neural responses in the AV condition with the sum of those in the A and V conditions. Given that Experiments 1a and 1b yielded similar results, we collapsed the data and presented the results as follows.</p><p>As shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, at 4f, the amplitude of neural responses showed significant peaks in all three conditions (V: <italic>t</italic> (47)=6.869, p&lt;0.001; A: <italic>t</italic> (47)=7.938, p&lt;0.001; AV: <italic>t</italic> (47)=8.303, p&lt;0.001). Moreover, the amplitude in the AV condition was larger than that in the V condition (<italic>t</italic> (47)=4.855, p&lt;0.001, Cohen’s <italic>d</italic>=0.701) and the A condition (<italic>t</italic> (47)=3.080, p=0.003, Cohen’s <italic>d</italic>=0.445), respectively, suggesting multisensory gains. In addition, the amplitude in the AV condition was comparable to the unisensory sum (<italic>t</italic> (47)=–1.049, p=0.300, Cohen’s <italic>d</italic>=–0.151), indicating linear AVI. These results were similar to those observed at 2f but different from those at 1f, as reported in the main text. Together, these results show a similar additive AVI mode at 2f and 4f and a super-additive integration mode only at 1f, suggesting that the cortical tracking effects at 2f and 4f may be functionally linked but independent of that at 1f.</p></sec><sec sec-type="appendix" id="s9"><title>Control analysis of correlation in Experiment 2</title><p>The control analysis mainly aims to eliminate the potential bias due to electrode selection. As reported in the main text, both correlation analyses at 1Hz and 2Hz were performed based on electrodes in the significant cluster observed at 1Hz because there was no significant cluster at 2Hz (<xref ref-type="fig" rid="fig3">Figure 3a and d</xref>, lower panel). There is a possibility that these electrodes did not show a significant congruency effect at 2Hz, either in the upright or the inverted condition, thus were not able to capture the correlation between the variance in neural responses and that in autistic traits. To rule out such a possibility, we conducted a control analysis based on electrodes showing a significant congruency effect at 2Hz, for the upright (p=0.004, cluster-based permutation test) and inverted (p=0.002, cluster-based permutation test) conditions, respectively. As shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>, the difference of congruency effect between upright and inverted conditions is still not significant in the group level (<italic>t</italic> (23)=–0.689, p=498, Cohen’s <italic>d</italic>=–0.141), while it shows individual variance (SD = 0.079, range: [–0.173 0.153]) larger than that for the 1Hz condition (SD = 0.041, range: [–0.023 0.135]), which allows us to identify a correlation if existing. Analysis of these data showed a non-significant correlation (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1b</xref>, <italic>r</italic>=–0.091, p=0.674), similar to the results illustrated in <xref ref-type="fig" rid="fig3">Figure 3f</xref>.</p></sec><sec sec-type="appendix" id="s10"><title>Additional analysis in Experiment 2</title><p>To further examine the linkage between autistic traits and the BM-specific cortical tracking effect, we split the participants into high (above 20) and low (below or equal to 20) AQ groups by the median AQ score (20) of this sample. Similar to correlation analysis, one outlier, whose BM-specific audiovisual congruency effect (upright – inverted) in neural responses at 1Hz exceeds 3 SD from the group mean, was removed from the following analysis. As shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, at 1Hz, participants with low AQ showed a greater cortical tracking effect compared with high AQ participants (<italic>t</italic> (21)=2.127, p=0.045). At 2Hz, low and high AQ participants showed comparable neural responses (<italic>t</italic> (22)=0.946, p=0.354). These results are in line with the correlation analysis, providing further support to the relevance between social cognition and cortical tracking of BM as well as its dissociation at the two temporal scales.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98701.5.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>Wang et al. presented visual (dot) motion and/or the sound of a walking person and found <bold>solid</bold> evidence that EEG activity tracks the step rhythm, as well as the gait (2-step cycle) rhythm, with some demonstration that the gait rhythm is tracked superadditively (power for A+V condition is higher than the sum of the A-only and V-only condition). The <bold>valuable</bold> findings will be of wide interest to those examining biological motion perception and oscillatory processes more broadly.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98701.5.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Shen et al. conducted three experiments to study the cortical tracking of the natural rhythms involved in biological motion (BM), and whether these involve audiovisual integration (AVI). They presented participants with visual (dot) motion and/or the sound of a walking person. They found that EEG activity tracks the step rhythm, as well as the gait (2-step cycle) rhythm. The gait rhythm specifically is tracked superadditively (power for A+V condition is higher than the sum of the A-only and V-only condition, Experiments 1a/b), which is independent of the specific step frequency (Experiment 1b). Furthermore, audiovisual integration during tracking of gait was specific to BM, as it was absent (that is, the audiovisual congruency effect) when the walking dot motion was vertically inverted (Experiment 2). Finally, the study shows that an individual's autistic traits are negatively correlated with the BM-AVI congruency effect.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98701.5.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors evaluate spectral changes in electroencephalography (EEG) data as a function of the congruency of audio and visual information associated with biological motion (BM) or non-biological motion. The results show supra-additive power gains in the neural response to gait dynamics, with trials in which audio and visual information was presented simultaneously producing higher average amplitude than the combined average power for auditory and visual conditions alone. Further analyses suggest that such supra-additivity is specific to BM and emerges from temporoparietal areas. The authors also find that the BM-specific supra-additivity is negatively correlated with autism traits.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98701.5.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Li</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Shuo</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Yuhao</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Ying</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Jiang</surname><given-names>Yi</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>Shen et al. conducted three experiments to study the cortical tracking of the natural rhythms involved in biological motion (BM), and whether these involve audiovisual integration (AVI). They presented participants with visual (dot) motion and/or the sound of a walking person. They found that EEG activity tracks the step rhythm, as well as the gait (2-step cycle) rhythm. The gait rhythm specifically is tracked superadditively (power for A+V condition is higher than the sum of the A-only and V-only condition,Experiments 1a/b), which is independent of the specific step frequency (Experiment 1b). Furthermore, audiovisual integration during tracking of gait was specific to BM, as it was absent (that is, the audiovisual congruency effect) when the walking dot motion was vertically inverted (Experiment 2). Finally, the study shows that an individual's autistic traits are negatively correlated with the BM-AVI congruency effect.</p><p>Strengths:</p><p>The three experiments are well designed and the various conditions are well controlled. The rationale of the study is clear, and the manuscript is pleasant to read. The analysis choices are easy to follow, and mostly appropriate.</p><p>Weaknesses:</p><p>On revision, the authors are careful not to overinterpret an analysis where the statistical test is not independent from the data (channel) selection criterion.</p></disp-quote><p>Thanks for the suggestion and we have done this according to your recommendations below.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>Re: the double-dipping concern: I appreciate the revision. Just to clarify: my concern rests with the selection of *electrodes* based on the interaction test for the 1Hz condition. The 2Hz condition analogous test yields no significant electrodes. You perform subsequent tests (t-tests and 3-way interaction) on the data averaged across the electrodes that were significant for the 1Hz condition. Therefore, these tests will be biased to find a pattern reflecting an interaction at 1Hz, while no similar bias exists for an effect at 2Hz. Therefore, there is a bias to observe a 3-way interaction, and simple effects compatible with a 2-way interaction only for 1Hz, not for 2Hz (which is exactly what you found). There is no good statistical alternative here, I appreciate that, but the bias exists nonetheless. I think the wording is improved in this revision, and the evidence is convincing even in light of this bias.</p></disp-quote><p>We are grateful for your thoughtful comments on the analytical methods. We appreciate your concerns regarding the potential bias of examining 3-way interaction based on electrodes yielding a 2-way interaction effect. To address this issue, we have conducted a bias-free analysis based on electrodes across the whole brain. The results showed a similar pattern of 3-way interaction as previously reported (p = 0.051), suggesting that the previous findings might not be caused by electrode selection. Given that the main results of Experiment 2 were not based on whole-brain analysis, we did not involve this analysis in the main text, and we have removed the three-way interaction results based on selected electrodes from the manuscript to reduce potential concerns. It is also noteworthy that, when performing analyses based on channels independent of the interaction effect at 1 Hz (i.e., significant congruency effects in the upright and inverted conditions, respectively, at 2Hz), we got similar results as reported in the main text (i.e., non-significant interaction and correlation at 2 Hz). These results were presented in the supplementary file in previous versions and mentioned in the correlation part of the Results section (see Fig. S2). Once again, we sincerely appreciate your careful review of our research. We hope the abovementioned points adequately address your concern.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The authors evaluate spectral changes in electroencephalography (EEG) data as a function of the congruency of audio and visual information associated with biological motion (BM) or non-biological motion. The results show supra-additive power gains in the neural response to gait dynamics, with trials in which audio and visual information was presented simultaneously producing higher average amplitude than the combined average power for auditory and visual conditions alone. Further analyses suggest that such supra-additivity is specific to BM and emerges from temporoparietal areas. The authors also find that the BM-specific supra-additivity is negatively correlated with autism traits.</p><p>Strengths:</p><p>The manuscript is well-written, with a concise and clear writing style. The visual presentation is largely clear. The study involves multiple experiments with different participant groups. Each experiment involves specific considered changes to the experimental paradigm that both replicate the previous experiment's finding yet extend it in a relevant manner.</p><p>In the first revisions of the paper, the manuscript better relays the results and anticipates analyses, and this version adequately resolves some concerns I had about analysis details. In a further revision, it is clarified better how the results relate to the various competing hypotheses on how biological motion is processed.</p><p>Weaknesses:</p><p>Still, it is my view that the findings of the study are basic neural correlate results that offer only minimal constraint towards the question of how the brain realizes the integration of multisensory information in the service of biological motion perception, and the data do not address the causal relevance of observed neural effects towards behavior and cognition. The presence of an inversion effect suggests that the supraadditivity is related to cognition, but that leaves open whether any detected neural pattern is actually consequential for multi-sensory integration (i.e., correlation is not causation). In other words, the fact that frequency-specific neural responses to the [audio &amp; visual] condition are stronger than those to [audio] and [visual] combined does not mean this has implications for behavioral performance. While the correlation to autism traits could suggest some relation to behavior and is interesting in its own right, this correlation is a highly indirect way of assessing behavioral relevance. It would be helpful to test the relevance of supra-additive cortical tracking on a behavioral task directly related to the processing of biological motion to justify the claim that inputs are being integrated in the service of behavior. Under either framework, cortical tracking or entrainment, the causal relevance of neural findings toward cognition is lacking.</p><p>Overall, I believe this study finds neural correlates of biological motion that offer some constraint toward mechanism, and it is possible that the effects are behaviorally relevant, but based on the current task and associated analyses this has not been shown (or could not have been, given the paradigm).</p><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>Thank you for your revisions; I have updated the Strengths section, and reworded the weaknesses section. I now concede that the neural effects observed offer some constraint towards what the neural mechanisms for AV integration for BM are, whereas in my previous review, I said too strongly that these results do not offer any information about mechanism.</p></disp-quote><p>Thank you again for your insightful thoughts and comments on our research. They have contributed greatly to enhancing the discussion of the article and provided valuable inspiration for future exploration of causal mechanisms.</p></body></sub-article></article>